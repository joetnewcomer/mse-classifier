,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,complexity of matrix multiplication,complexity of matrix multiplication,,"For $n\times n$ dimensional matrices, it is known that calculating $\operatorname{tr}\{AB\}$ needs $n^2$ scalar multiplications. How many scalar multiplications are needed to calculate $\operatorname{tr}\{ABCD\}$? Note that $\operatorname{tr}$ means the trace of a matrix.","For $n\times n$ dimensional matrices, it is known that calculating $\operatorname{tr}\{AB\}$ needs $n^2$ scalar multiplications. How many scalar multiplications are needed to calculate $\operatorname{tr}\{ABCD\}$? Note that $\operatorname{tr}$ means the trace of a matrix.",,"['linear-algebra', 'matrices', 'computational-complexity']"
1,"For any three vectors $x,y,z\in\mathbb{R}^d$, we have $ \|y-z\|\cdot\|x\|\leq\|x-y\|\cdot\|z\|+\|z-x\|\cdot\|y\|$","For any three vectors , we have","x,y,z\in\mathbb{R}^d  \|y-z\|\cdot\|x\|\leq\|x-y\|\cdot\|z\|+\|z-x\|\cdot\|y\|","Does anyone know a proof of the following problem? Problem: Show that for any three vectors ${\bf x}, {\bf y}, {\bf z}\in \mathbb{R}^d$ the following holds, $$ \|{\bf y} - {\bf z}\|\cdot \|{\bf x}\| \leq \|{\bf x} - {\bf y}\|\cdot \|{\bf z}\| + \|{\bf z} - {\bf x}\|\cdot\|{\bf y}\|.$$ All of the norms are Euclidean 2-norm.","Does anyone know a proof of the following problem? Problem: Show that for any three vectors ${\bf x}, {\bf y}, {\bf z}\in \mathbb{R}^d$ the following holds, $$ \|{\bf y} - {\bf z}\|\cdot \|{\bf x}\| \leq \|{\bf x} - {\bf y}\|\cdot \|{\bf z}\| + \|{\bf z} - {\bf x}\|\cdot\|{\bf y}\|.$$ All of the norms are Euclidean 2-norm.",,"['linear-algebra', 'geometry', 'normed-spaces']"
2,"Checking if one ""special"" kind of block matrix is Hurwitz","Checking if one ""special"" kind of block matrix is Hurwitz",,"I have the next block matrix $$ J = \begin{bmatrix}A & B \\ K &0\end{bmatrix} $$ all matrices are square, where $A < 0$ (definite negative), $B$ has all its eigenvalues with positive real part being $A = - (B + B^T)$, and $K$ is a diagonal matrix. What I see from numerical simulation, is that if $$K < 0 \iff J \text{ is Hurwitz}$$. Any clue about how can I prove this? Another question, I have seen many times that $$ M = \begin{bmatrix}A & B \\ -B^T &0\end{bmatrix} $$ is Hurwitz if $A < 0$, but I can not find the proof for it. I guess it is really related to the former question. Many thanks in advance. Edit More ideas related to the second question. $M$ is Hurwitz and its characteristic polynomial is det$(\lambda^2I - A\lambda + BB^T)=0$. The cp of J is det$(\lambda^2I + (B + B^T)\lambda - BK) = 0$. Looking at the two cps, $-A > 0$ and $B+B^T > 0$, and $BB^T > 0$ and $-BK$ has its Eigenvalues with positive real part $\iff BK + K^TB^T < 0$. Is this fact related to being Hurwitz ? I mean, the block matrix is Hurwitz if its cp det$(\lambda^2I + V\lambda + W)=0$ has $V > 0$ and $W > 0$ ? Edit 2 Thank you very much for your response in Answer1. You are right, and also I have found several counterexamples to this conjecture. However changing the condition (which has been proved wrong with a counter example) I have not found a counter example (yet). Let $K = -cI$, with $c>0$ being a scalar. In other words,  $$BK + K^TB^T < 0 \iff J \text{ is Hurwitz}$$. Finally, I have found a counter example (third comment to the answer 1). So this conjecture is wrong too. However, it seems that for $c$ sufficiently small, $J$ is Hurwitz, now I have to found the condition in $c$ for that. Any clues or suggestions? Edit 3 Finally, it has been found with counterexamples, that the last conjecture is false too. Then, the only way (as far as I know) that I have for assessing the stability of $J$ is to check the next linear matrix inequality. $$ J \quad\text{is Hurwitz if} \quad \exists K \quad \text{s.t.} \quad J+J^T \prec 0  $$","I have the next block matrix $$ J = \begin{bmatrix}A & B \\ K &0\end{bmatrix} $$ all matrices are square, where $A < 0$ (definite negative), $B$ has all its eigenvalues with positive real part being $A = - (B + B^T)$, and $K$ is a diagonal matrix. What I see from numerical simulation, is that if $$K < 0 \iff J \text{ is Hurwitz}$$. Any clue about how can I prove this? Another question, I have seen many times that $$ M = \begin{bmatrix}A & B \\ -B^T &0\end{bmatrix} $$ is Hurwitz if $A < 0$, but I can not find the proof for it. I guess it is really related to the former question. Many thanks in advance. Edit More ideas related to the second question. $M$ is Hurwitz and its characteristic polynomial is det$(\lambda^2I - A\lambda + BB^T)=0$. The cp of J is det$(\lambda^2I + (B + B^T)\lambda - BK) = 0$. Looking at the two cps, $-A > 0$ and $B+B^T > 0$, and $BB^T > 0$ and $-BK$ has its Eigenvalues with positive real part $\iff BK + K^TB^T < 0$. Is this fact related to being Hurwitz ? I mean, the block matrix is Hurwitz if its cp det$(\lambda^2I + V\lambda + W)=0$ has $V > 0$ and $W > 0$ ? Edit 2 Thank you very much for your response in Answer1. You are right, and also I have found several counterexamples to this conjecture. However changing the condition (which has been proved wrong with a counter example) I have not found a counter example (yet). Let $K = -cI$, with $c>0$ being a scalar. In other words,  $$BK + K^TB^T < 0 \iff J \text{ is Hurwitz}$$. Finally, I have found a counter example (third comment to the answer 1). So this conjecture is wrong too. However, it seems that for $c$ sufficiently small, $J$ is Hurwitz, now I have to found the condition in $c$ for that. Any clues or suggestions? Edit 3 Finally, it has been found with counterexamples, that the last conjecture is false too. Then, the only way (as far as I know) that I have for assessing the stability of $J$ is to check the next linear matrix inequality. $$ J \quad\text{is Hurwitz if} \quad \exists K \quad \text{s.t.} \quad J+J^T \prec 0  $$",,"['linear-algebra', 'matrices']"
3,Good properties of weakly commuting matrices?,Good properties of weakly commuting matrices?,,"Let $A$ and $B$ be $n\times n$ matrices over $\mathbb{C}$. If $AB=BA$, we know that we can simultaneously diagonalize $A$ and $B$ (or make them Jordan canonical form). What if they are weakly commutative in the sense that $AB=cBA$ for some $c\in \mathbb{C}^{\times}$? What can we say about $A$ and $B$? I am sorry being a bit vague, but I came across with this kind of matrices in my little project and wonder what we can say about them.","Let $A$ and $B$ be $n\times n$ matrices over $\mathbb{C}$. If $AB=BA$, we know that we can simultaneously diagonalize $A$ and $B$ (or make them Jordan canonical form). What if they are weakly commutative in the sense that $AB=cBA$ for some $c\in \mathbb{C}^{\times}$? What can we say about $A$ and $B$? I am sorry being a bit vague, but I came across with this kind of matrices in my little project and wonder what we can say about them.",,"['linear-algebra', 'abstract-algebra']"
4,A problem on skew-symmetric matrix,A problem on skew-symmetric matrix,,"If $A∈M(n;\mathbb{R})$, let $A^t$ denote its transpose. A matrix $S\in M(n;\mathbb{R})$ is said to be skew-symmetric if $S^t = −S$. Pick out the true statements: a. If S ∈ $M(n;\mathbb{R})$ is skew-symmetric and non-singular, then $n$ is even. b. Let $G = \{T ∈ GL(n;\mathbb{R})\mid T^t ST = S$, for all skew-symmetric $S ∈ M(n;\mathbb{R}$}. Then $G$ is a subgroup of $GL(n;\mathbb{R})$. c. Let $I_n$ and $O_n$ denote the $n \times n$ identity and null matrices respectively. Let $S$ be the $2n \times 2n$ matrix given in block form by $\left[\matrix{O_n&I_n\cr -I_n&O_n}\right]$. If $X$ is a $2n×2n$ matrix such that $X^t S+SX = 0$, then the trace of $X$ is zero. Please help anyone to solve the problem. My thinking as far:- (a) is true as every skew symmetric matrix of odd order is singular. For(b) & (c) no idea. Thanks.","If $A∈M(n;\mathbb{R})$, let $A^t$ denote its transpose. A matrix $S\in M(n;\mathbb{R})$ is said to be skew-symmetric if $S^t = −S$. Pick out the true statements: a. If S ∈ $M(n;\mathbb{R})$ is skew-symmetric and non-singular, then $n$ is even. b. Let $G = \{T ∈ GL(n;\mathbb{R})\mid T^t ST = S$, for all skew-symmetric $S ∈ M(n;\mathbb{R}$}. Then $G$ is a subgroup of $GL(n;\mathbb{R})$. c. Let $I_n$ and $O_n$ denote the $n \times n$ identity and null matrices respectively. Let $S$ be the $2n \times 2n$ matrix given in block form by $\left[\matrix{O_n&I_n\cr -I_n&O_n}\right]$. If $X$ is a $2n×2n$ matrix such that $X^t S+SX = 0$, then the trace of $X$ is zero. Please help anyone to solve the problem. My thinking as far:- (a) is true as every skew symmetric matrix of odd order is singular. For(b) & (c) no idea. Thanks.",,"['linear-algebra', 'matrices']"
5,Alternating two-form,Alternating two-form,,"Let $V$ be a real vector space of dimension $2$, and let $\langle\ \ ,\ \ \rangle$ be an inner product on $V$. Define $f:V^4 \to \mathbb{R}$ by $$f(x,y,z,w):=\langle x,y \rangle \langle z,w \rangle- \langle x,w \rangle \langle z,y \rangle$$ Show there's a skew-symmetric bilinear form $g:V^2 \to \mathbb{R}$ such that $$f(x,y,z,w)=g(x,z)g(y,w)$$ My thought: Let $z=y,w=x$ to get $f(x,y,y,x)= \langle x,y \rangle ^2-\langle x,x \rangle \langle y,y \rangle$. If $g$ exists, then $f(x,y,y,x)=g(x,y)g(y,x)=-g(x,y)^2$ (notice that $g$ is skew-symetric). Equating both identities we get $g(x,y)^2= -\langle x,y \rangle ^2+\langle x,x \rangle \langle y,y \rangle$. Here is where I can't proceed any more...","Let $V$ be a real vector space of dimension $2$, and let $\langle\ \ ,\ \ \rangle$ be an inner product on $V$. Define $f:V^4 \to \mathbb{R}$ by $$f(x,y,z,w):=\langle x,y \rangle \langle z,w \rangle- \langle x,w \rangle \langle z,y \rangle$$ Show there's a skew-symmetric bilinear form $g:V^2 \to \mathbb{R}$ such that $$f(x,y,z,w)=g(x,z)g(y,w)$$ My thought: Let $z=y,w=x$ to get $f(x,y,y,x)= \langle x,y \rangle ^2-\langle x,x \rangle \langle y,y \rangle$. If $g$ exists, then $f(x,y,y,x)=g(x,y)g(y,x)=-g(x,y)^2$ (notice that $g$ is skew-symetric). Equating both identities we get $g(x,y)^2= -\langle x,y \rangle ^2+\langle x,x \rangle \langle y,y \rangle$. Here is where I can't proceed any more...",,"['linear-algebra', 'abstract-algebra', 'inner-products']"
6,What is the standard proof that $\dim(k^{\mathbb N})$ is uncountable?,What is the standard proof that  is uncountable?,\dim(k^{\mathbb N}),"This is my (silly) proof to a claim on top of p. 54 of Rotman's ""Homological algebra"". For $k$ an infinite field (the finite case is trivial) prove that $k^\mathbb{N}$, the $k$-space of functions from the positive integers $\mathbb{N}$ to $k$, has uncountable dimension. Lemma. There is an uncountable family $(A_r)_{r \in \mathbb{R}}$ of almost disjoint infinite subsets of $\mathbb{N}$, i.e., $|A_r \cap A_s| < \infty$ for $r \neq s$. The proof is standard, let $f : \mathbb{Q} \stackrel{\sim}{\to} \mathbb{N}$ and $A_r := \{f(r_1), f(r_2), \ldots\}$ for $(r_j)_{j \in \mathbb{N}}$ a sequence of distinct rationals whose limit is $r$. Of course, these must be chosen simultaneously for all $r$, but any choice will work. Now the family $f_r : \mathbb{N} \to k$, $f_r(x) = 1$ for $x \in A_r$ and $0$ elsewhere is linearly independent, since $a_1f_{r_1} + \cdots + a_kf_{r_k} = 0$ yields $a_1 = 0$ if applied to $x \in A_{r_1} \backslash (A_{r_2} \cup \cdots \cup A_{r_k})$, etc. Curiously, this shows that $\dim(k^\mathbb{N}) \ge |\mathbb{R}|$, which is ""a bit more"". I'd like to see the folklore trivial ""one-line argument"", since I don't remember to have learned about it. Thanks in advance. Also, greetings to stackexchange (this being my first topic here).","This is my (silly) proof to a claim on top of p. 54 of Rotman's ""Homological algebra"". For $k$ an infinite field (the finite case is trivial) prove that $k^\mathbb{N}$, the $k$-space of functions from the positive integers $\mathbb{N}$ to $k$, has uncountable dimension. Lemma. There is an uncountable family $(A_r)_{r \in \mathbb{R}}$ of almost disjoint infinite subsets of $\mathbb{N}$, i.e., $|A_r \cap A_s| < \infty$ for $r \neq s$. The proof is standard, let $f : \mathbb{Q} \stackrel{\sim}{\to} \mathbb{N}$ and $A_r := \{f(r_1), f(r_2), \ldots\}$ for $(r_j)_{j \in \mathbb{N}}$ a sequence of distinct rationals whose limit is $r$. Of course, these must be chosen simultaneously for all $r$, but any choice will work. Now the family $f_r : \mathbb{N} \to k$, $f_r(x) = 1$ for $x \in A_r$ and $0$ elsewhere is linearly independent, since $a_1f_{r_1} + \cdots + a_kf_{r_k} = 0$ yields $a_1 = 0$ if applied to $x \in A_{r_1} \backslash (A_{r_2} \cup \cdots \cup A_{r_k})$, etc. Curiously, this shows that $\dim(k^\mathbb{N}) \ge |\mathbb{R}|$, which is ""a bit more"". I'd like to see the folklore trivial ""one-line argument"", since I don't remember to have learned about it. Thanks in advance. Also, greetings to stackexchange (this being my first topic here).",,"['linear-algebra', 'cardinals']"
7,Find the equation of a line which is perpendicular to a given vector and passing through a known point,Find the equation of a line which is perpendicular to a given vector and passing through a known point,,"There is given a vector $2 \vec i + \vec j - 3 \vec k$ and now I want to find the equation of a line that is perpendicular to the given vector and passing through a known point $(1,1,1)$. How can I solve this?","There is given a vector $2 \vec i + \vec j - 3 \vec k$ and now I want to find the equation of a line that is perpendicular to the given vector and passing through a known point $(1,1,1)$. How can I solve this?",,"['linear-algebra', 'vector-analysis']"
8,A scalar product in the space of oriented volumes?,A scalar product in the space of oriented volumes?,,"Let $L\colon \mathbb{R}^n \to \mathbb{R}^N$ be an injective linear map. By the Cauchy-Binet formula, $\det(L^TL)$ equals the sum of the squares of all minors of $L$ of order $n$: this looks just like a norm. Also, the square root of this number has a geometrical interpretation as the scale factor by which $L$ maps $n$-volumes, which makes Grassmann's exterior product come into mind. Question If we let $\Lambda^n(\mathbb{R}^N)$ denote the vector space spanned by $n$-vectors $$v_1\wedge \ldots \wedge v_n,\qquad v_j \in \mathbb{R}^N, $$ does there exist a scalar product $\langle ,\rangle$ on it such that    $$\det(L^TL)=\langle Le_1\wedge \ldots \wedge Le_n, Le_1\wedge \ldots \wedge Le_n\rangle ?^{(\star)}$$ If the answer is affirmative, is this scalar product geometrically related to the concept of    ""oriented $n$-volume in $\mathbb{R}^N$""? And finally, is it possible to   generalize all this to an arbitrary Riemannian manifold? Bibliographical references as answers are fine. Thank you. (*) $e_1\ldots e_n$ denotes the standard basis of $\mathbb{R}^n$.","Let $L\colon \mathbb{R}^n \to \mathbb{R}^N$ be an injective linear map. By the Cauchy-Binet formula, $\det(L^TL)$ equals the sum of the squares of all minors of $L$ of order $n$: this looks just like a norm. Also, the square root of this number has a geometrical interpretation as the scale factor by which $L$ maps $n$-volumes, which makes Grassmann's exterior product come into mind. Question If we let $\Lambda^n(\mathbb{R}^N)$ denote the vector space spanned by $n$-vectors $$v_1\wedge \ldots \wedge v_n,\qquad v_j \in \mathbb{R}^N, $$ does there exist a scalar product $\langle ,\rangle$ on it such that    $$\det(L^TL)=\langle Le_1\wedge \ldots \wedge Le_n, Le_1\wedge \ldots \wedge Le_n\rangle ?^{(\star)}$$ If the answer is affirmative, is this scalar product geometrically related to the concept of    ""oriented $n$-volume in $\mathbb{R}^N$""? And finally, is it possible to   generalize all this to an arbitrary Riemannian manifold? Bibliographical references as answers are fine. Thank you. (*) $e_1\ldots e_n$ denotes the standard basis of $\mathbb{R}^n$.",,"['linear-algebra', 'reference-request', 'euclidean-geometry', 'riemannian-geometry', 'exterior-algebra']"
9,Defining the determinant of linear transformations as multilinear alternating form,Defining the determinant of linear transformations as multilinear alternating form,,"Here is what our professor showed us in our linear algebra class to introduce the idea of determinants: Suppose we have an $n$-dimensional vector space $V$. Then we can create a function from $V^n$ to $\mathbb{R}$ called $vol$ (for ""volume"") satisfying these properties: $vol$ is multilinear $vol$ is alternating (i.e. if any two of $v_1, \ldots, v_n$ are the same, then $vol(v_1, \ldots, v_n) = 0$) From these two properties, we can see that if $e_1, \ldots, e_n$ is a basis of $V$, then the $vol$ function is completely defined by the value $vol(e_1, \ldots, e_n)$. Thus if $T$ is a linear operator on $V$, the ratio: $\dfrac{vol(Te_1, \ldots, Te_n)}{vol(e_1, \ldots, e_n)}$ is the same for any (multilinear and alternating) $vol$ function. However, I am having trouble understanding why the ratio is also independent of the basis $e_1, \ldots, e_n$. This is what I am asking for help with. I can see that this invariance implies that intuitively, every $n$-parallelotope is stretched by the same amount by the operator $T$. (Our professor then defined the determinant of $T$ as that ratio.)","Here is what our professor showed us in our linear algebra class to introduce the idea of determinants: Suppose we have an $n$-dimensional vector space $V$. Then we can create a function from $V^n$ to $\mathbb{R}$ called $vol$ (for ""volume"") satisfying these properties: $vol$ is multilinear $vol$ is alternating (i.e. if any two of $v_1, \ldots, v_n$ are the same, then $vol(v_1, \ldots, v_n) = 0$) From these two properties, we can see that if $e_1, \ldots, e_n$ is a basis of $V$, then the $vol$ function is completely defined by the value $vol(e_1, \ldots, e_n)$. Thus if $T$ is a linear operator on $V$, the ratio: $\dfrac{vol(Te_1, \ldots, Te_n)}{vol(e_1, \ldots, e_n)}$ is the same for any (multilinear and alternating) $vol$ function. However, I am having trouble understanding why the ratio is also independent of the basis $e_1, \ldots, e_n$. This is what I am asking for help with. I can see that this invariance implies that intuitively, every $n$-parallelotope is stretched by the same amount by the operator $T$. (Our professor then defined the determinant of $T$ as that ratio.)",,['linear-algebra']
10,Diagonal submatrices of the inverse of a $p \times p$ block matrix,Diagonal submatrices of the inverse of a  block matrix,p \times p,"Let $X$ be a square, symmetric, positive definite matrix that can be decomposed into $p\times p$ block matrices: $$X = \begin{bmatrix} X_{11} & X_{12} & \ldots & X_{1p}\\ X_{21} & X_{22} & \ldots & X_{2p}\\ \vdots & \vdots & \ddots & \vdots \\ X_{p1} & X_{p2} & \ldots & X_{pp} \end{bmatrix}$$ Let $Y$ denote the inverse of $X$ that consists of $p \times p$ submatrices: $$Y = X^{-1} = \begin{bmatrix} Y_{11} & Y_{12} & \ldots & Y_{1p}\\ Y_{21} & Y_{22} & \ldots & Y_{2p}\\ \vdots & \vdots & \ddots & \vdots \\ Y_{p1} & Y_{p2} & \ldots & Y_{pp} \end{bmatrix}$$ I would like to write the diagonal submatrices $Y_{ii}$ for $1 \leq i \leq p$ as a function of the submatrices in $X$ . $\bf{p =2}$ case: Since $X_{11}, X_{22}$ are positive definite (and invertible), then $$Y = \begin{bmatrix} (X_{11} - X_{12}X_{22}^{-1}X_{21})^{-1} & -(X_{11}-X_{12}X_{22}^{-1} X_{21})^{-1}X_{12}X_{22}^{-1} \\ -(X_{22} - X_{21} X_{11}^{-1} X_{12})^{-1} X_{21}X_{11}^{-1} & (X_{22}-X_{21} X_{11}^{-1} X_{12})^{-1} \end{bmatrix}$$ so $Y_{11} = (X_{11} - X_{12}X_{22}^{-1}X_{21})^{-1}$ $Y_{22} = (X_{22}-X_{21} X_{11}^{-1} X_{12})^{-1}$ $\bf{p =3}$ case: Since $X_{11}, X_{22}, X_{33}$ are positive definite (and invertible), then (if my math is right) we have the following $Y_{11} = ((X_{11} - X_{13}X^{-1}_{11}X_{31}) - (X_{12}-X_{13}X^{-1}_{33}X_{32})(X_{22}-X_{23}X^{-1}_{33}X_{32})^{-1}(X_{21}-X_{23}X^{-1}_{33}X_{31}))^{-1}$ $Y_{22} = ((X_{22} - X_{21}X^{-1}_{11}X_{12}) - (X_{23}-X_{21}X^{-1}_{11}X_{13})(X_{33}-X_{31}X^{-1}_{11}X_{13})^{-1}(X_{32} - X_{31}X^{-1}_{11}X_{12}))^{-1}$ $Y_{33} = ((X_{33}-X_{31}X^{-1}_{11}X_{13}) - (X_{32}-X_{31}X^{-1}_{11}X_{12})(X_{22}-X_{21}X^{-1}_{11}X_{12})^{-1}(X_{23}-X_{21}X^{-1}_{11}X_{13}))^{-1}$ For the general $p \times p$ case, is it possible to write $$Y_{ii} = (Z_{ii})^{-1}$$ where $Z_{ii}$ can be written as a function of the submatrices in $X$ ? Would $Z_{ii}$ be a Schur complement of some sort?","Let be a square, symmetric, positive definite matrix that can be decomposed into block matrices: Let denote the inverse of that consists of submatrices: I would like to write the diagonal submatrices for as a function of the submatrices in . case: Since are positive definite (and invertible), then so case: Since are positive definite (and invertible), then (if my math is right) we have the following For the general case, is it possible to write where can be written as a function of the submatrices in ? Would be a Schur complement of some sort?","X p\times p X = \begin{bmatrix} X_{11} & X_{12} & \ldots & X_{1p}\\
X_{21} & X_{22} & \ldots & X_{2p}\\
\vdots & \vdots & \ddots & \vdots \\
X_{p1} & X_{p2} & \ldots & X_{pp}
\end{bmatrix} Y X p \times p Y = X^{-1} = \begin{bmatrix} Y_{11} & Y_{12} & \ldots & Y_{1p}\\
Y_{21} & Y_{22} & \ldots & Y_{2p}\\
\vdots & \vdots & \ddots & \vdots \\
Y_{p1} & Y_{p2} & \ldots & Y_{pp}
\end{bmatrix} Y_{ii} 1 \leq i \leq p X \bf{p =2} X_{11}, X_{22} Y = \begin{bmatrix} (X_{11} - X_{12}X_{22}^{-1}X_{21})^{-1} & -(X_{11}-X_{12}X_{22}^{-1} X_{21})^{-1}X_{12}X_{22}^{-1} \\
-(X_{22} - X_{21} X_{11}^{-1} X_{12})^{-1} X_{21}X_{11}^{-1} & (X_{22}-X_{21} X_{11}^{-1} X_{12})^{-1} \end{bmatrix} Y_{11} = (X_{11} - X_{12}X_{22}^{-1}X_{21})^{-1} Y_{22} = (X_{22}-X_{21} X_{11}^{-1} X_{12})^{-1} \bf{p =3} X_{11}, X_{22}, X_{33} Y_{11} = ((X_{11} - X_{13}X^{-1}_{11}X_{31}) - (X_{12}-X_{13}X^{-1}_{33}X_{32})(X_{22}-X_{23}X^{-1}_{33}X_{32})^{-1}(X_{21}-X_{23}X^{-1}_{33}X_{31}))^{-1} Y_{22} = ((X_{22} - X_{21}X^{-1}_{11}X_{12}) - (X_{23}-X_{21}X^{-1}_{11}X_{13})(X_{33}-X_{31}X^{-1}_{11}X_{13})^{-1}(X_{32} - X_{31}X^{-1}_{11}X_{12}))^{-1} Y_{33} = ((X_{33}-X_{31}X^{-1}_{11}X_{13}) - (X_{32}-X_{31}X^{-1}_{11}X_{12})(X_{22}-X_{21}X^{-1}_{11}X_{12})^{-1}(X_{23}-X_{21}X^{-1}_{11}X_{13}))^{-1} p \times p Y_{ii} = (Z_{ii})^{-1} Z_{ii} X Z_{ii}","['linear-algebra', 'matrices', 'positive-definite', 'block-matrices']"
11,"Classify all matrices $A,B$ such that $AB\ne BA$, but there is some $n>1$ such that $(AB)^n=(BA)^n$","Classify all matrices  such that , but there is some  such that","A,B AB\ne BA n>1 (AB)^n=(BA)^n","Recently when teaching the basics of matrices, a student asked if there are matrices $A,B$ for which $AB\ne BA$ , but for some $n>1$ we have $(AB)^n = (BA)^n$ ? I was able to come up with a few cases where the answer is yes. If both $AB$ and $BA$ are nilpotent, then the result is obvious; take $n$ to be the maximum power that kills both. Another nice case is if $A,B$ are rotation matrices in dimension $d\ge 3$ (so they don't commute) and are both rotations by rational multiples of $2\pi$ . In particular, the case where $d=3$ and $A$ is a rotation about the $x$ -axis by $\pi/2$ and $B$ is a rotation about the $y$ -axis by $\pi/2$ readily gives $AB\ne BA$ but $(AB)^3=(BA)^3=I$ . The student is a fan of Rubik's Cubes so I think this will make a nice example. My gut says other geometric-flavored matrices (i.e., reflections) will also have this property. However, I want to ask a follow-up question, namely to classify all such matrices with this property. Writing $A=PS P^{-1}$ and $B=PT P^{-1}$ for some matrix $P$ is intractable, as we have $$ (AB)^n = P (ST)^n P^{-1} ; (BA)^n = P(TS)^n P^{-1}, $$ which gets us nowhere. Is there some nice classification of these matrices?","Recently when teaching the basics of matrices, a student asked if there are matrices for which , but for some we have ? I was able to come up with a few cases where the answer is yes. If both and are nilpotent, then the result is obvious; take to be the maximum power that kills both. Another nice case is if are rotation matrices in dimension (so they don't commute) and are both rotations by rational multiples of . In particular, the case where and is a rotation about the -axis by and is a rotation about the -axis by readily gives but . The student is a fan of Rubik's Cubes so I think this will make a nice example. My gut says other geometric-flavored matrices (i.e., reflections) will also have this property. However, I want to ask a follow-up question, namely to classify all such matrices with this property. Writing and for some matrix is intractable, as we have which gets us nowhere. Is there some nice classification of these matrices?","A,B AB\ne BA n>1 (AB)^n = (BA)^n AB BA n A,B d\ge 3 2\pi d=3 A x \pi/2 B y \pi/2 AB\ne BA (AB)^3=(BA)^3=I A=PS P^{-1} B=PT P^{-1} P 
(AB)^n = P (ST)^n P^{-1} ; (BA)^n = P(TS)^n P^{-1},
","['linear-algebra', 'matrices', 'rotations']"
12,"Prove that for an orthogonal matrix $A$ and a certain reflection $R$, the $\mathbb C$-linear part of $RA$ is invertible","Prove that for an orthogonal matrix  and a certain reflection , the -linear part of  is invertible",A R \mathbb C RA,"Hello: I need help with this problem: Let $V = (V,b)$ be a finite-dimensional vector space equipped with a symmetric and positive definite bilinear form $b$ . And let $\{e_1,…,e_n\}$ be a orthonormal basis for the subspace $\ker((P_A)^t)$ ( $P_A$ is defined below). For a matrix $A \in \mathrm{O}(V)$ , let $\mathrm{O}_*(V)$ the subset of $\mathrm{O}(V)$ such that be the matrix $P_A:=\frac{A-JAJ}{2}$ is invertible, where $J$ is a complex structure (a matrix such that $J^2=-1$ and $J^t=J^{-1}=-J$ ). Let $n=\dim \ker(P_A)$ . For every $j \in \{1,…,n\}$ we define the reflexions $r_j$ such that $r_j(e_j)=-Je_j$ , $r(Je_j)=-e_j$ and $r_j(v)=v$ for and $v \in V$ such that $b(v,e_j)=b(v,Je_j)=0$ . Finally, let $$R:=r_1r_2\dots r_n \in \mathrm{O}(V).$$ I need to prove that $$RA \in \mathrm{SO}_*(V),$$ where similarly as $\mathrm{O}(V)$ : $\mathrm{SO}_*(V)$ is the subset of $\mathrm{SO}(V)$ such that $P_B:=\frac{B-JBJ}{2}$ is invertible. I already proved that $RA \in \mathrm{SO}(V)$ ; the only thing that I haven’t been able to figure out is to prove that $\frac{1}{2}(RA-JRAJ)$ is invertible, since $n$ can be even or odd. Also, $P_{r_j}$ is not invertible since $\det(r_j)=-1$ . ¿What is good and optimized approach to deal with the product of reflections $$R=r_1r_2\cdots r_n?$$ In summary: I’m trying to prove that $RA$ (where $R$ the product of reflections $r_1r_2\dots r_n$ ) is a ortogonal matrix with $\det(RA)=+1$ and that the matrix $\frac{1}{2}(J-J(RA)J)$ is invertible. Any help will be greatly appreciated. Thanks :). UPDATE: Two things: I made a typo, $\{e_1,…,e_n\}$ be a orthonormal basis for the subspace $\ker((P_A)^t)$ , not on $\ker(P_A)$ . On the main reference I'm using, the author establishes the following: This operators (each reflection $r_j$ , with $j \in \{ 1,\dots, n \}$ ) has the identity restricted to the subspace ortogonal to $e_j$ as $p_{r_j}$ . Let $R:=r_1\dots r_n$ , then the operator $R$ is in block form, its lower right corner being the identity on $(\ker((P_A)^t))^{\perp}$ . And so $RA \in \mathrm{SO}_*(V)$ . And that's it, I think the author's argument has many gaps or things that I'm not getting :(. He says he's following this article , but I have read any multiply times and I don't see anything like what I'm trying to prove (or at least eith this notation).","Hello: I need help with this problem: Let be a finite-dimensional vector space equipped with a symmetric and positive definite bilinear form . And let be a orthonormal basis for the subspace ( is defined below). For a matrix , let the subset of such that be the matrix is invertible, where is a complex structure (a matrix such that and ). Let . For every we define the reflexions such that , and for and such that . Finally, let I need to prove that where similarly as : is the subset of such that is invertible. I already proved that ; the only thing that I haven’t been able to figure out is to prove that is invertible, since can be even or odd. Also, is not invertible since . ¿What is good and optimized approach to deal with the product of reflections In summary: I’m trying to prove that (where the product of reflections ) is a ortogonal matrix with and that the matrix is invertible. Any help will be greatly appreciated. Thanks :). UPDATE: Two things: I made a typo, be a orthonormal basis for the subspace , not on . On the main reference I'm using, the author establishes the following: This operators (each reflection , with ) has the identity restricted to the subspace ortogonal to as . Let , then the operator is in block form, its lower right corner being the identity on . And so . And that's it, I think the author's argument has many gaps or things that I'm not getting :(. He says he's following this article , but I have read any multiply times and I don't see anything like what I'm trying to prove (or at least eith this notation).","V = (V,b) b \{e_1,…,e_n\} \ker((P_A)^t) P_A A \in \mathrm{O}(V) \mathrm{O}_*(V) \mathrm{O}(V) P_A:=\frac{A-JAJ}{2} J J^2=-1 J^t=J^{-1}=-J n=\dim \ker(P_A) j \in \{1,…,n\} r_j r_j(e_j)=-Je_j r(Je_j)=-e_j r_j(v)=v v \in V b(v,e_j)=b(v,Je_j)=0 R:=r_1r_2\dots r_n \in \mathrm{O}(V). RA \in \mathrm{SO}_*(V), \mathrm{O}(V) \mathrm{SO}_*(V) \mathrm{SO}(V) P_B:=\frac{B-JBJ}{2} RA \in \mathrm{SO}(V) \frac{1}{2}(RA-JRAJ) n P_{r_j} \det(r_j)=-1 R=r_1r_2\cdots r_n? RA R r_1r_2\dots r_n \det(RA)=+1 \frac{1}{2}(J-J(RA)J) \{e_1,…,e_n\} \ker((P_A)^t) \ker(P_A) r_j j \in \{ 1,\dots, n \} e_j p_{r_j} R:=r_1\dots r_n R (\ker((P_A)^t))^{\perp} RA \in \mathrm{SO}_*(V)","['linear-algebra', 'matrices', 'linear-transformations', 'orthogonal-matrices', 'reflection']"
13,The trace as an integral over the projective space,The trace as an integral over the projective space,,"Let $(E,h)$ be a Hermitian vector space of dimension $n$ and $u\in End(E)$ . We have an expression of the trace of $u$ as the integral $$Tr(u)=\frac{n}{A}\int_{S}\langle v,uv\rangle d\mu$$ where $S$ is the unit sphere in $E$ and $A$ its volume (see Integral around unit sphere of inner product ). Is there a similar expression but using the projective space instead of the sphere? That is some equality that looks like $$Tr(u)=C\int_{\mathbb{P}E}\frac{\langle v,uv\rangle}{||v||^2} d\mu_{FS}$$ for some constant $C$ and $d\mu_{FS}$ being the Fubini-Study volume form.",Let be a Hermitian vector space of dimension and . We have an expression of the trace of as the integral where is the unit sphere in and its volume (see Integral around unit sphere of inner product ). Is there a similar expression but using the projective space instead of the sphere? That is some equality that looks like for some constant and being the Fubini-Study volume form.,"(E,h) n u\in End(E) u Tr(u)=\frac{n}{A}\int_{S}\langle v,uv\rangle d\mu S E A Tr(u)=C\int_{\mathbb{P}E}\frac{\langle v,uv\rangle}{||v||^2} d\mu_{FS} C d\mu_{FS}","['linear-algebra', 'differential-geometry', 'trace', 'projective-space']"
14,Reasoning behind choosing appropriate decomposition when solving linear equation.,Reasoning behind choosing appropriate decomposition when solving linear equation.,,"Now I am reading the book ""Introduction to linear algebra"" by Gilbert Strang. I understand all the technical details regarding LU, QR and SVD decompositions, but I get completely confused when it comes to choosing between them for a particular case. I also know the following time complexities: LU decomposition: $\mathcal O\left(n^3\right)$ for square matrix of size $n \times n$ ; QR decomposition: $\mathcal O\left(mn^3\right)$ for rectangular matrix of size $m \times n$ ; SVD decomposition: $\mathcal O\left(n^3\right)$ for square matrix of size $n \times n$ . Also, time complexity for solving $Ax = b$ as $x = A^{-1}b$ is $\mathcal O\left(n^2\right)$ if we already know $A^{-1}$ . But if you already have your matrices $L$ and $U$ , time complexity for solving $LUx = b$ is the same. So, as I know, in this case using LU decomposition is only considered better for numerical reasons. QR decomposition doesn't speed up the process either. QR decomposition can make calculation of least squares solution simpler, since the equation becomes $\hat x = R^{-1}Q^{\mathrm T}b$ . But taking into account the fact you first need to calculate QR decomposition itself and only after that apply least squares, then this QR decomposition doesn't speed up anything. I also know it's a good idea to use LU or QR, when you always have the same matrix $A$ and different $b$ 's. But not considering this use case, are all these decompositions used only for numerical stability? Because I don't actually see how they can speed up calculations. But it seemed to me those are just little pieces of a big picture, since I am not sure what decomposition to choose when it comes to a real problem. I encountered a lot of related questions on this site, but they all seemed to ask about some particular detail regarding some particular decomposition whereas I'm asking about all them at once. Could someone make it clear when one should use each decomposition and why? Some examples of use cases would be highly appreciated.","Now I am reading the book ""Introduction to linear algebra"" by Gilbert Strang. I understand all the technical details regarding LU, QR and SVD decompositions, but I get completely confused when it comes to choosing between them for a particular case. I also know the following time complexities: LU decomposition: for square matrix of size ; QR decomposition: for rectangular matrix of size ; SVD decomposition: for square matrix of size . Also, time complexity for solving as is if we already know . But if you already have your matrices and , time complexity for solving is the same. So, as I know, in this case using LU decomposition is only considered better for numerical reasons. QR decomposition doesn't speed up the process either. QR decomposition can make calculation of least squares solution simpler, since the equation becomes . But taking into account the fact you first need to calculate QR decomposition itself and only after that apply least squares, then this QR decomposition doesn't speed up anything. I also know it's a good idea to use LU or QR, when you always have the same matrix and different 's. But not considering this use case, are all these decompositions used only for numerical stability? Because I don't actually see how they can speed up calculations. But it seemed to me those are just little pieces of a big picture, since I am not sure what decomposition to choose when it comes to a real problem. I encountered a lot of related questions on this site, but they all seemed to ask about some particular detail regarding some particular decomposition whereas I'm asking about all them at once. Could someone make it clear when one should use each decomposition and why? Some examples of use cases would be highly appreciated.",\mathcal O\left(n^3\right) n \times n \mathcal O\left(mn^3\right) m \times n \mathcal O\left(n^3\right) n \times n Ax = b x = A^{-1}b \mathcal O\left(n^2\right) A^{-1} L U LUx = b \hat x = R^{-1}Q^{\mathrm T}b A b,"['linear-algebra', 'optimization', 'numerical-linear-algebra', 'computational-complexity', 'matrix-decomposition']"
15,Do harmonic functions span the space of functions on manifolds?,Do harmonic functions span the space of functions on manifolds?,,"If one considers the Laplace (or Helmholtz) equation in two dimensions, then through separation of variables in plane polar coordinates, the azimuthal dependence is seen to be of the form of a harmonic $e^{in\phi}$ , where $n$ is an integer. The set of harmonics form a basis for any (reasonable) function on a circle i.e. any function with $2\pi$ periodicity (the Fourier series). Similarly, if one considers the Laplace (or Helmholtz) equation in three dimensions, then through separation of variables in spherical polar coordinates, the angular dependence is seen to be of the form of a spherical harmonic $Y_{l}^{m}(\theta,\phi)$ , where $l$ and $m$ are integers with $|m|<l$ . The set of spherical harmonics form a basis for any (reasonable) function on a sphere. Is there a reason why solutions to Laplace's equation form a basis for functions on manifolds of the space? I suspect there is a theorem for this but I've either never come across it or it has completely slipped my mind. Thanks in advance for any help.","If one considers the Laplace (or Helmholtz) equation in two dimensions, then through separation of variables in plane polar coordinates, the azimuthal dependence is seen to be of the form of a harmonic , where is an integer. The set of harmonics form a basis for any (reasonable) function on a circle i.e. any function with periodicity (the Fourier series). Similarly, if one considers the Laplace (or Helmholtz) equation in three dimensions, then through separation of variables in spherical polar coordinates, the angular dependence is seen to be of the form of a spherical harmonic , where and are integers with . The set of spherical harmonics form a basis for any (reasonable) function on a sphere. Is there a reason why solutions to Laplace's equation form a basis for functions on manifolds of the space? I suspect there is a theorem for this but I've either never come across it or it has completely slipped my mind. Thanks in advance for any help.","e^{in\phi} n 2\pi Y_{l}^{m}(\theta,\phi) l m |m|<l","['linear-algebra', 'partial-differential-equations', 'harmonic-analysis', 'harmonic-functions', 'laplacian']"
16,The set of irreducible representations (over C) determines a finite group,The set of irreducible representations (over C) determines a finite group,,"I'm trying to understand group representations, and I think that this statement is correct (where determines means up to isomorphism ), although I couldn't find a proof online (without references to more complicated things like ""Tannaka duality""), so I tried to prove it, but I may have overlooked something: Let $G = \{x_1, ..., x_n\}$ , $H = \{y_1, ..., y_n\}$ be two finite groups of same cardinality. Suppose G and H have the same set of irreducible representations in the following sense : there is a finite set of morphisms $g_i$ (resp. $h_i$ ), each one from G (resp. H) to some subgroup of $GL(n_i, \Bbb{C})$ and we can write $g_i(x_j) = P_i h_i(y_j) P_i^{-1}$ for some invertible matrix $P_i$ and all $j$ . Then if we consider the regular representation of G, we can write it as a sum of the $g_i$ representations (with $n_i$ giving the multiplicity) and do the same for H. That shows that the two regular representations are isomorphic as we can build a new base using a block-diagonal transition matrix using $(P_i)$ . As they are also faithful (isomorphic to the groups themselves), then G and H are isomorphic. Is that correct ? EDIT: I updated the ""same set of irreducible representations"" following the comments (that also requires the additional assumption of same cardinality)","I'm trying to understand group representations, and I think that this statement is correct (where determines means up to isomorphism ), although I couldn't find a proof online (without references to more complicated things like ""Tannaka duality""), so I tried to prove it, but I may have overlooked something: Let , be two finite groups of same cardinality. Suppose G and H have the same set of irreducible representations in the following sense : there is a finite set of morphisms (resp. ), each one from G (resp. H) to some subgroup of and we can write for some invertible matrix and all . Then if we consider the regular representation of G, we can write it as a sum of the representations (with giving the multiplicity) and do the same for H. That shows that the two regular representations are isomorphic as we can build a new base using a block-diagonal transition matrix using . As they are also faithful (isomorphic to the groups themselves), then G and H are isomorphic. Is that correct ? EDIT: I updated the ""same set of irreducible representations"" following the comments (that also requires the additional assumption of same cardinality)","G = \{x_1, ..., x_n\} H = \{y_1, ..., y_n\} g_i h_i GL(n_i, \Bbb{C}) g_i(x_j) = P_i h_i(y_j) P_i^{-1} P_i j g_i n_i (P_i)","['linear-algebra', 'group-theory', 'finite-groups', 'representation-theory']"
17,Diagonalizing a nearly-diagonal matrix,Diagonalizing a nearly-diagonal matrix,,"Take a (kind of) arrowhead real-symmetric matrix of the general form $$ M =  \begin{bmatrix} a_{11} & a_{12} & a_{13} & a_{14} & a_{15} & a_{16} \\ a_{12} & a_{22} & a_{23} & a_{24} & a_{25} & a_{26} \\ a_{13} & a_{23} & a_{33} & 0 & 0 & 0 \\ a_{14} & a_{24} & 0 & a_{44} & 0 & 0 \\ a_{15} & a_{25} & 0 & 0 & a_{55} & 0 \\ a_{16} & a_{26} & 0 & 0 & 0 & a_{66} \\ \end{bmatrix} $$ where the size of the blocks may vary, however in general, the diagonal submatrix will be of dimension close to that of the entire matrix. Is there a method to diagonalise this matrix which takes advantage of this largely diagonal structure? My desire is computational efficiency, i.e. compared to dgemm . I require all of the eigenvalues and eigenvectors of this matrix, i.e. $V^{-1}MV = W$ where $V$ are the eigenvectors of $M$ , and $W$ a diagonal matrix containing the eigenvalues.","Take a (kind of) arrowhead real-symmetric matrix of the general form where the size of the blocks may vary, however in general, the diagonal submatrix will be of dimension close to that of the entire matrix. Is there a method to diagonalise this matrix which takes advantage of this largely diagonal structure? My desire is computational efficiency, i.e. compared to dgemm . I require all of the eigenvalues and eigenvectors of this matrix, i.e. where are the eigenvectors of , and a diagonal matrix containing the eigenvalues.","
M = 
\begin{bmatrix}
a_{11} & a_{12} & a_{13} & a_{14} & a_{15} & a_{16} \\
a_{12} & a_{22} & a_{23} & a_{24} & a_{25} & a_{26} \\
a_{13} & a_{23} & a_{33} & 0 & 0 & 0 \\
a_{14} & a_{24} & 0 & a_{44} & 0 & 0 \\
a_{15} & a_{25} & 0 & 0 & a_{55} & 0 \\
a_{16} & a_{26} & 0 & 0 & 0 & a_{66} \\
\end{bmatrix}
 V^{-1}MV = W V M W","['linear-algebra', 'matrices', 'eigenvalues-eigenvectors', 'numerical-linear-algebra', 'diagonalization']"
18,Proving a matrix is invertible given equation (without identity matrix),Proving a matrix is invertible given equation (without identity matrix),,"I'm given a square matrix ${A}$ (3×3) and the following equation ${A}^3-2017{A}^2 + {A} = {0}$ and I have to find if ${A}$ is invertible in some cases, no cases, or all cases. I can find ${A}=0$ as an answer for the non invertible case, but I can't seem to solve the equation. In most other examples I've found, there was an identity matrix, which made it easy to find the invertible of ${A}$ like this: ${A}*invertible=I$ but this is not the case here. I've tried doing this: $A*(A*(-A+2017*I))=A*I$, but I don't think I can divide both parts by ${A}$ because I haven't proven that ${A}$ is invertible.","I'm given a square matrix ${A}$ (3×3) and the following equation ${A}^3-2017{A}^2 + {A} = {0}$ and I have to find if ${A}$ is invertible in some cases, no cases, or all cases. I can find ${A}=0$ as an answer for the non invertible case, but I can't seem to solve the equation. In most other examples I've found, there was an identity matrix, which made it easy to find the invertible of ${A}$ like this: ${A}*invertible=I$ but this is not the case here. I've tried doing this: $A*(A*(-A+2017*I))=A*I$, but I don't think I can divide both parts by ${A}$ because I haven't proven that ${A}$ is invertible.",,"['linear-algebra', 'matrix-equations']"
19,Does $V/(\ker f \cap \ker g) \cong \text{im} f +\text{im} g$?,Does ?,V/(\ker f \cap \ker g) \cong \text{im} f +\text{im} g,Let $k$ be a field and $V$ be a finite dimensional $k$-vector space. Let $f$ and $g$ be two $k$-linear endomorphisms of $V$ such that $f\circ g=g\circ f$. Do we have an isomorphism of $k$-vector spaces $V/(\ker f \cap \ker g) \cong \text{im} f +\text{im} g$ ? Many thanks!,Let $k$ be a field and $V$ be a finite dimensional $k$-vector space. Let $f$ and $g$ be two $k$-linear endomorphisms of $V$ such that $f\circ g=g\circ f$. Do we have an isomorphism of $k$-vector spaces $V/(\ker f \cap \ker g) \cong \text{im} f +\text{im} g$ ? Many thanks!,,"['linear-algebra', 'matrix-rank', 'exact-sequence']"
20,Roots of random polynomials.,Roots of random polynomials.,,"Assume $P(x)$ is a random polynomial of degree $d$, where its coefficients are picked uniformly at random from $\mathbb{F}_p$, and $p$ is a large prime number. So the polynomial is defined over $\mathbb{F}_p$. Question 1: What is the probability that polynomial $P(x)$ has at least one root? Question 2: Are roots of $P(x)$ random values in $\mathbb{F}_p$?","Assume $P(x)$ is a random polynomial of degree $d$, where its coefficients are picked uniformly at random from $\mathbb{F}_p$, and $p$ is a large prime number. So the polynomial is defined over $\mathbb{F}_p$. Question 1: What is the probability that polynomial $P(x)$ has at least one root? Question 2: Are roots of $P(x)$ random values in $\mathbb{F}_p$?",,"['linear-algebra', 'polynomials', 'finite-fields', 'factoring']"
21,"Change of basis - Definition, matrix and relation to diagonalization","Change of basis - Definition, matrix and relation to diagonalization",,"Change of basis and diagonalization is a hassle for anyone new to the world of linear algebra; anyway for me. I was thinking I could post my full interpretation and then you guys could correct me if I'm wrong in fill in the gaps if I've missed anything etc. Maybe it also could work as future reference for others. Current questions: First, are my interpretations below correct? Anything that's vague or indefinite? I'm especially unsure about the use of diagonalization for system of equations. Does it really have the same equal solution or is it dependent on the ""new"" basis or something like that? What are the further uses of diagonalization? How would one go about to find the change of basis matrix from $\mathcal A$ to $\mathcal B$? From e.g. base $\mathcal C$ to the standard basis it's simple, the transformation matrix consists of the basis vectors of $\mathcal C$ (although expressed in the standardbasis I guess, but it's simpler to grasp). Must one ""go through"" the standard basis if I want to go from $\mathcal A$ to $\mathcal B$? Change of basis Let $\mathcal A = \{\overrightarrow{a_1},\cdots,\overrightarrow{a_n}\}$ and $\mathcal B = \{\overrightarrow{b_1},\cdots,\overrightarrow{b_n}\}$ be two bases in $\mathbb R^n$. Then, the change of basis matrix from $\mathcal A$ to $\mathcal B$ is the matrix $T_{\mathcal A \rightarrow \mathcal B}$ which columns is the $\mathcal A$-basis vectors expressed in $\mathcal B$: $$T_{\mathcal A \rightarrow \mathcal B} = \begin{bmatrix}[\overrightarrow{a_1}]_\mathcal B&\cdots&[\overrightarrow{a_n}]_\mathcal B\end{bmatrix}$$ Then, for all vectors $\overrightarrow{x}$ in $\mathbb R^n$ the following applies: $$T_{\mathcal A \rightarrow \mathcal B}[\overrightarrow{x}]_\mathcal A=[\overrightarrow{x}]_\mathcal B$$ I.e. the change of basis matrix T that transforms a vector $\overrightarrow{x}$ from $\mathcal A$ to $\mathcal B$ is obtained by inserting $\mathcal A$'s base vectors expressed in $\mathcal B$ as columns. ""The old base is expressed in the new"". Notes $T_{\mathcal A \rightarrow \mathcal B} = (T_{\mathcal B \rightarrow \mathcal A})^{-1}$ (invers) $T_{\mathcal A \rightarrow \mathcal C} = T_{\mathcal B \rightarrow \mathcal C}T_{\mathcal A \rightarrow \mathcal B}$ (multiple basis changes, note the order) The above is in some sense also true for a linear transformation, since the transformation matrix have the images of its basis as columns. Though, a linear transformation can be from $\mathbb R^n$ to $\mathbb R^k$ and is not always invertible. Diagonalization A square matrix $A$ can be decomposed to special form $A=SDS^{-1}$ (or the more commonly written and equal $D=S^{-1}AS$), where $S$ is the matrix consisting of eigenvectors to $A$ and $D$ is the diagonal matrix with its corresponding eigenvalues. To do this one must find the eigenvalues and eigenvectors and if there are as many linear independent eigenvectors as the basis of the space which $A$ exists in, diagonalization is possible. In other words, to diagonlize a matrix $A$ one must find a new basis which consists of its eigenvectors. So, diagonalization is a way of expressing the same linear transformation but in another basis, which is easier to understand and visualize since they are mapped on themselves by a scalar of their corresponding eigenvalues. Use in system of equations An initial equation $AX=Y$ can then be written as: $$SDS^{-1}X=Y$$ By multiplying both sides with $S^{-1}$ we get: $$DS^{-1}X=S^{-1}Y$$ Since the same linear transformation $S^{-1}$ is applied to both $X$ and $Y$ the initial equation is equivalent to the transformed one: $$DX'=Y'$$ Where $X'=S^{-1}X$ and $Y'=S^{-1}Y$ So, to solve for $X$ one must first obtain $X'$ from $DX'=Y'$, which becomes easy since $D$ is diagonal. $X$ is then obtained from $X=SX'$ Therefore diagonalization is a way solving a system of equations.","Change of basis and diagonalization is a hassle for anyone new to the world of linear algebra; anyway for me. I was thinking I could post my full interpretation and then you guys could correct me if I'm wrong in fill in the gaps if I've missed anything etc. Maybe it also could work as future reference for others. Current questions: First, are my interpretations below correct? Anything that's vague or indefinite? I'm especially unsure about the use of diagonalization for system of equations. Does it really have the same equal solution or is it dependent on the ""new"" basis or something like that? What are the further uses of diagonalization? How would one go about to find the change of basis matrix from $\mathcal A$ to $\mathcal B$? From e.g. base $\mathcal C$ to the standard basis it's simple, the transformation matrix consists of the basis vectors of $\mathcal C$ (although expressed in the standardbasis I guess, but it's simpler to grasp). Must one ""go through"" the standard basis if I want to go from $\mathcal A$ to $\mathcal B$? Change of basis Let $\mathcal A = \{\overrightarrow{a_1},\cdots,\overrightarrow{a_n}\}$ and $\mathcal B = \{\overrightarrow{b_1},\cdots,\overrightarrow{b_n}\}$ be two bases in $\mathbb R^n$. Then, the change of basis matrix from $\mathcal A$ to $\mathcal B$ is the matrix $T_{\mathcal A \rightarrow \mathcal B}$ which columns is the $\mathcal A$-basis vectors expressed in $\mathcal B$: $$T_{\mathcal A \rightarrow \mathcal B} = \begin{bmatrix}[\overrightarrow{a_1}]_\mathcal B&\cdots&[\overrightarrow{a_n}]_\mathcal B\end{bmatrix}$$ Then, for all vectors $\overrightarrow{x}$ in $\mathbb R^n$ the following applies: $$T_{\mathcal A \rightarrow \mathcal B}[\overrightarrow{x}]_\mathcal A=[\overrightarrow{x}]_\mathcal B$$ I.e. the change of basis matrix T that transforms a vector $\overrightarrow{x}$ from $\mathcal A$ to $\mathcal B$ is obtained by inserting $\mathcal A$'s base vectors expressed in $\mathcal B$ as columns. ""The old base is expressed in the new"". Notes $T_{\mathcal A \rightarrow \mathcal B} = (T_{\mathcal B \rightarrow \mathcal A})^{-1}$ (invers) $T_{\mathcal A \rightarrow \mathcal C} = T_{\mathcal B \rightarrow \mathcal C}T_{\mathcal A \rightarrow \mathcal B}$ (multiple basis changes, note the order) The above is in some sense also true for a linear transformation, since the transformation matrix have the images of its basis as columns. Though, a linear transformation can be from $\mathbb R^n$ to $\mathbb R^k$ and is not always invertible. Diagonalization A square matrix $A$ can be decomposed to special form $A=SDS^{-1}$ (or the more commonly written and equal $D=S^{-1}AS$), where $S$ is the matrix consisting of eigenvectors to $A$ and $D$ is the diagonal matrix with its corresponding eigenvalues. To do this one must find the eigenvalues and eigenvectors and if there are as many linear independent eigenvectors as the basis of the space which $A$ exists in, diagonalization is possible. In other words, to diagonlize a matrix $A$ one must find a new basis which consists of its eigenvectors. So, diagonalization is a way of expressing the same linear transformation but in another basis, which is easier to understand and visualize since they are mapped on themselves by a scalar of their corresponding eigenvalues. Use in system of equations An initial equation $AX=Y$ can then be written as: $$SDS^{-1}X=Y$$ By multiplying both sides with $S^{-1}$ we get: $$DS^{-1}X=S^{-1}Y$$ Since the same linear transformation $S^{-1}$ is applied to both $X$ and $Y$ the initial equation is equivalent to the transformed one: $$DX'=Y'$$ Where $X'=S^{-1}X$ and $Y'=S^{-1}Y$ So, to solve for $X$ one must first obtain $X'$ from $DX'=Y'$, which becomes easy since $D$ is diagonal. $X$ is then obtained from $X=SX'$ Therefore diagonalization is a way solving a system of equations.",,"['linear-algebra', 'matrices', 'systems-of-equations', 'diagonalization', 'change-of-basis']"
22,Why do we care about normal matrices/operators?,Why do we care about normal matrices/operators?,,"We know that normal operators are ""nice"".  In the finite dimensional case, the spectral theorem tells us everything we need to know.  In the infinite dimensional case, we can define a continuous functional calculus over the (bounded) normal operators (admittedly I don't know much about the unbounded case).  However, is there a context in which we should ""expect"" an operator to be normal?  That is: Is there a common application of linear algebra (or operator theory) in which we would expect the operator we're working with to be an arbitrary normal operator? There are many contexts in which we might expect the operator in question to be self-adjoint (the Hessian matrix for instance), and there are certainly some in which we might expect the operator to be unitary/orthogonal (the orthogonal Procrustes problem comes to mind).  However, I'm looking for a ""natural"" application in which we might expect a normal operator of any sort , but not expect a non-normal operator. I found this MO thread about applications of the spectral theorem, but that hasn't led to any satisfying leads, unfortunately. Thanks for any feedback.","We know that normal operators are ""nice"".  In the finite dimensional case, the spectral theorem tells us everything we need to know.  In the infinite dimensional case, we can define a continuous functional calculus over the (bounded) normal operators (admittedly I don't know much about the unbounded case).  However, is there a context in which we should ""expect"" an operator to be normal?  That is: Is there a common application of linear algebra (or operator theory) in which we would expect the operator we're working with to be an arbitrary normal operator? There are many contexts in which we might expect the operator in question to be self-adjoint (the Hessian matrix for instance), and there are certainly some in which we might expect the operator to be unitary/orthogonal (the orthogonal Procrustes problem comes to mind).  However, I'm looking for a ""natural"" application in which we might expect a normal operator of any sort , but not expect a non-normal operator. I found this MO thread about applications of the spectral theorem, but that hasn't led to any satisfying leads, unfortunately. Thanks for any feedback.",,"['linear-algebra', 'matrices', 'functional-analysis', 'operator-theory', 'applications']"
23,Finding the dimension of the space of symmetric $r-$tensors,Finding the dimension of the space of symmetric tensors,r-,"Let $T^r(V^*)$ denote the space of $r-$tensors on $V$ a vector space of dimension $n$. Basis of $V$ is denoted $\{u_1, \dots u_n $} and basis of $T^r(V^*)$ is $\{\tilde{u}^{i_1} \otimes \dots \otimes \tilde{u}^{i_r}  : 1\leq i_1 \leq n, \dots , 1\leq i_r \leq n\}$ Let $\Sigma^r(V^*)$ be the (sub)space of symmetric $r-$tensors where if $\sigma \in S_r$ is a permutation in the symmetric group and $\alpha \in \Sigma^r(V^*)$, then:  $$\alpha^\sigma (v_1, \dots , v_r) = \alpha (v_{\sigma(1)}, \dots , v_{\sigma(r)}) $$ What I am trying to do is determine the dimension of  $\Sigma^k(V^*)$. I know that there is a projection mapping $\mathcal{S}$ called the symmetrizer from $T^r(V^*)$ onto  $\Sigma^r(V^*)$ defined by: $$\mathcal{S}(\alpha) = \dfrac{1}{r!}\sum_{\sigma \in S_r} \alpha^\sigma$$. I also (from some searching) already know what the desired answer is: $$\text{dim}\Sigma^k(V^*) = \binom{n + r -1}{r} = \dfrac{(n + r -1)!}{r! (n-1)!}$$ However, I am unable to derive the general expression myself. My current approach, which has worked for $V = \mathbb{R}^4$ and $r =2, 3$ has been to consider the basis of $T^r(V^*)$ and see which elements give the same element of $\Sigma^k(V^*)$ upon symmetrization. For example for the $r=2$ case, I notice that $\mathcal{S}(\tilde{e}^1 \otimes \tilde{e}^2) = \mathcal{S}(\tilde{e}^2 \otimes \tilde{e}^1)$ ; $\mathcal{S}(\tilde{e}^1 \otimes \tilde{e}^3) = \mathcal{S}(\tilde{e}^3 \otimes \tilde{e}^1)$ ; $\mathcal{S}(\tilde{e}^1 \otimes \tilde{e}^4) = \mathcal{S}(\tilde{e}^4 \otimes \tilde{e}^1)$ ; $\mathcal{S}(\tilde{e}^3 \otimes \tilde{e}^2) = \mathcal{S}(\tilde{e}^2 \otimes \tilde{e}^3)$; $\mathcal{S}(\tilde{e}^4 \otimes \tilde{e}^2) = \mathcal{S}(\tilde{e}^2 \otimes \tilde{e}^4)$ ;  and $\mathcal{S}(\tilde{e}^3 \otimes \tilde{e}^4) = \mathcal{S}(\tilde{e}^4 \otimes \tilde{e}^3)$. To these $6$ terms, I add the symmertrizations of $\tilde{e}^i \otimes \tilde{e}^i$ for $i \in \{1,2,3,4\}$ to get dimension 10, as required.","Let $T^r(V^*)$ denote the space of $r-$tensors on $V$ a vector space of dimension $n$. Basis of $V$ is denoted $\{u_1, \dots u_n $} and basis of $T^r(V^*)$ is $\{\tilde{u}^{i_1} \otimes \dots \otimes \tilde{u}^{i_r}  : 1\leq i_1 \leq n, \dots , 1\leq i_r \leq n\}$ Let $\Sigma^r(V^*)$ be the (sub)space of symmetric $r-$tensors where if $\sigma \in S_r$ is a permutation in the symmetric group and $\alpha \in \Sigma^r(V^*)$, then:  $$\alpha^\sigma (v_1, \dots , v_r) = \alpha (v_{\sigma(1)}, \dots , v_{\sigma(r)}) $$ What I am trying to do is determine the dimension of  $\Sigma^k(V^*)$. I know that there is a projection mapping $\mathcal{S}$ called the symmetrizer from $T^r(V^*)$ onto  $\Sigma^r(V^*)$ defined by: $$\mathcal{S}(\alpha) = \dfrac{1}{r!}\sum_{\sigma \in S_r} \alpha^\sigma$$. I also (from some searching) already know what the desired answer is: $$\text{dim}\Sigma^k(V^*) = \binom{n + r -1}{r} = \dfrac{(n + r -1)!}{r! (n-1)!}$$ However, I am unable to derive the general expression myself. My current approach, which has worked for $V = \mathbb{R}^4$ and $r =2, 3$ has been to consider the basis of $T^r(V^*)$ and see which elements give the same element of $\Sigma^k(V^*)$ upon symmetrization. For example for the $r=2$ case, I notice that $\mathcal{S}(\tilde{e}^1 \otimes \tilde{e}^2) = \mathcal{S}(\tilde{e}^2 \otimes \tilde{e}^1)$ ; $\mathcal{S}(\tilde{e}^1 \otimes \tilde{e}^3) = \mathcal{S}(\tilde{e}^3 \otimes \tilde{e}^1)$ ; $\mathcal{S}(\tilde{e}^1 \otimes \tilde{e}^4) = \mathcal{S}(\tilde{e}^4 \otimes \tilde{e}^1)$ ; $\mathcal{S}(\tilde{e}^3 \otimes \tilde{e}^2) = \mathcal{S}(\tilde{e}^2 \otimes \tilde{e}^3)$; $\mathcal{S}(\tilde{e}^4 \otimes \tilde{e}^2) = \mathcal{S}(\tilde{e}^2 \otimes \tilde{e}^4)$ ;  and $\mathcal{S}(\tilde{e}^3 \otimes \tilde{e}^4) = \mathcal{S}(\tilde{e}^4 \otimes \tilde{e}^3)$. To these $6$ terms, I add the symmertrizations of $\tilde{e}^i \otimes \tilde{e}^i$ for $i \in \{1,2,3,4\}$ to get dimension 10, as required.",,"['linear-algebra', 'combinatorics', 'tensor-products', 'differential-forms', 'tensors']"
24,Behavior of the spectral radius of $A_1^k\ldots A_j^k$ when $k\to \infty$,Behavior of the spectral radius of  when,A_1^k\ldots A_j^k k\to \infty,"First formulation of my problem : Let $A_1,\cdots,A_j$ be hermitian definite positive matrices of dimension $n$ with all their eigenvalues in $(0;1]$. We also add the condition that $\|A_1\cdots A_j\|_2^2<1$. I am wondering if the quantity $$\frac{-1}{k}\log\left(\rho\left(A_1^k\cdots A_j^k\right)\right)$$ has a (positive and finite) limit when $k$ goes to infinity. Here $\rho(M)$ denote the spectral radius of the matrix $M$. Second formulation : The same problem can be reformulated in terms of ordinary differential equation : let $a : [0;1]\to \mathscr H_n^{+}$ be a bounded, piecewise continuous function from $[0;1]$ to the space of hermitian semi definite positive matrices and define $G_a :[0;1]\to \mathscr{M}_n(\mathbf C)$ as the solution of the following differential equation $$ \left\{  \begin{array}{rcl} G_a(0)&=&\mathrm{Id}\\ G_a'(t)&=&-a(t)G(t). \end{array} \right. $$ We moreover assume that $\|G_a(1)\|_2^2<1$. I want to know if the quantity  $$\frac{-1}{k}\log\big(\rho\left(G_{ka}(1)\right)\big)$$ has a finite positive limit when $k\to \infty$. This is indeed a reformulation of the first problem if we take $a$ to be piecewise constant on every interval $[\frac{i-1}{j};\frac{i}{j})$ and such that $\exp(-a(i-1)/j)=A_i$. By an argument of density the two problems are equivalent. What I already know : Note that in dimension $1$ ( ie the matrices are just real numbers) the answer to my question is trivially yes. The few numerical simulations i have done (and my intuition) seems to indicate that the answer to my question is yes : there is a positive finite limit. Finally, i was able to prove an upper and a lower bound for the first formulation of the problem, these bounds are not sharp and i will only sketch the proof of the bounds : first bound : All the $A_i^k$ are contractions of $\mathbf C^n$ so $\rho\left(A_1^k\cdots A_j^k\right)<1$ and thus $\det\left(A_1^k\cdots A_j^k\right)<1$ but $\det\left(A_1^k\cdots A_j^k\right)=\det\left(A_1\cdots A_j\right)^k$ and thus $\rho\left(A_1^k\cdots A_j^k\right)\geq \det\left(A_1\cdots A_j\right)^{k}$, giving us the first bound. second bound : One can show that all the coefficients of $A_1^k$ are of the form $C(\alpha+o(1))^k$ for some $C$ and $\alpha$. The product and the sum of two of this expressions can still be exprimed in that way. From that we deduce that the coefficients of the characteristic polynomial of $A_1^k\cdots A_j^k$ are also of the form $C(\alpha+o(1))^k$, so it's an exponentially small perturbation of the polynomial $X^n$. Since the roots of a polynomial (of degree $n$) are $1/n$-Hölder functions of the coefficients we get the other bound on $\rho\left(A_1\cdots A_j\right)^k$. This second bound suggest to concider the matrices $A_1^k\cdots A_j^k$ as small perturbations of the null matrix, maybe this approach is fruitfull but i don't know much about perturbation theory.","First formulation of my problem : Let $A_1,\cdots,A_j$ be hermitian definite positive matrices of dimension $n$ with all their eigenvalues in $(0;1]$. We also add the condition that $\|A_1\cdots A_j\|_2^2<1$. I am wondering if the quantity $$\frac{-1}{k}\log\left(\rho\left(A_1^k\cdots A_j^k\right)\right)$$ has a (positive and finite) limit when $k$ goes to infinity. Here $\rho(M)$ denote the spectral radius of the matrix $M$. Second formulation : The same problem can be reformulated in terms of ordinary differential equation : let $a : [0;1]\to \mathscr H_n^{+}$ be a bounded, piecewise continuous function from $[0;1]$ to the space of hermitian semi definite positive matrices and define $G_a :[0;1]\to \mathscr{M}_n(\mathbf C)$ as the solution of the following differential equation $$ \left\{  \begin{array}{rcl} G_a(0)&=&\mathrm{Id}\\ G_a'(t)&=&-a(t)G(t). \end{array} \right. $$ We moreover assume that $\|G_a(1)\|_2^2<1$. I want to know if the quantity  $$\frac{-1}{k}\log\big(\rho\left(G_{ka}(1)\right)\big)$$ has a finite positive limit when $k\to \infty$. This is indeed a reformulation of the first problem if we take $a$ to be piecewise constant on every interval $[\frac{i-1}{j};\frac{i}{j})$ and such that $\exp(-a(i-1)/j)=A_i$. By an argument of density the two problems are equivalent. What I already know : Note that in dimension $1$ ( ie the matrices are just real numbers) the answer to my question is trivially yes. The few numerical simulations i have done (and my intuition) seems to indicate that the answer to my question is yes : there is a positive finite limit. Finally, i was able to prove an upper and a lower bound for the first formulation of the problem, these bounds are not sharp and i will only sketch the proof of the bounds : first bound : All the $A_i^k$ are contractions of $\mathbf C^n$ so $\rho\left(A_1^k\cdots A_j^k\right)<1$ and thus $\det\left(A_1^k\cdots A_j^k\right)<1$ but $\det\left(A_1^k\cdots A_j^k\right)=\det\left(A_1\cdots A_j\right)^k$ and thus $\rho\left(A_1^k\cdots A_j^k\right)\geq \det\left(A_1\cdots A_j\right)^{k}$, giving us the first bound. second bound : One can show that all the coefficients of $A_1^k$ are of the form $C(\alpha+o(1))^k$ for some $C$ and $\alpha$. The product and the sum of two of this expressions can still be exprimed in that way. From that we deduce that the coefficients of the characteristic polynomial of $A_1^k\cdots A_j^k$ are also of the form $C(\alpha+o(1))^k$, so it's an exponentially small perturbation of the polynomial $X^n$. Since the roots of a polynomial (of degree $n$) are $1/n$-Hölder functions of the coefficients we get the other bound on $\rho\left(A_1\cdots A_j\right)^k$. This second bound suggest to concider the matrices $A_1^k\cdots A_j^k$ as small perturbations of the null matrix, maybe this approach is fruitfull but i don't know much about perturbation theory.",,"['linear-algebra', 'matrices', 'ordinary-differential-equations', 'limits', 'perturbation-theory']"
25,Find the rank of the following matrix depending on $\lambda\in\Bbb R$,Find the rank of the following matrix depending on,\lambda\in\Bbb R,Find the rank of the following matrix depending on $\lambda\in\Bbb R$. $$A=\begin{pmatrix} 1&2&3&4\\ 2&\lambda&6&7\\ 3&6&8&9\\ 4&7&9&10 \end{pmatrix}$$ My attempt: $$\begin{pmatrix} 1&2&3&4\\ 2&\lambda&6&7\\ 3&6&8&9\\ 4&7&9&10 \end{pmatrix}\sim\begin{pmatrix} 1&2&3&4\\ 0&\lambda-4&0&-1\\ 0&0&-1&-3\\ 0&-1&-3&-6 \end{pmatrix}\sim\begin{pmatrix} 1&0&-3&-8\\ 0&\lambda-4&0&-1\\ 0&0&-1&-3\\ 0&-1&-3&-6\\ \end{pmatrix}$$ For $\lambda=4$ we have:  $$\begin{pmatrix} 1 &0&-3&-8\\ 0&0&0&-1\\ 0&0&-1&-3\\ 0&-1&-3&-6 \end{pmatrix}\sim\begin{pmatrix} 1&0&0&1\\ 0&0&0&-1\\ 0&0&-1&-3\\ 0&-1&0&3\end{pmatrix}\sim\begin{pmatrix} 1&0&0&0\\ 0&0&0&-1\\ 0&0&-1&0\\ 0&-1&0&0\\ \end{pmatrix}$$ $\Rightarrow r(A)=4$ For $\lambda\neq 4$ we have: $$\begin{pmatrix} 1&0&0&1\\ 0&\lambda-4&0&-1\\ 0&0&-1&-3\\ 0&-1&0&3\\ \end{pmatrix}\sim\begin{pmatrix} 1&0&0&1\\ 0&0&0&3\lambda-13\\ 0&0&-1&-3\\ 0&-1&0&3 \end{pmatrix}$$ For$\lambda=\frac{13}{3}\Rightarrow r(A)=3$ and for $\lambda\neq \frac{13}{3} \Rightarrow r(A)=4$ Is this correct? Thanks!,Find the rank of the following matrix depending on $\lambda\in\Bbb R$. $$A=\begin{pmatrix} 1&2&3&4\\ 2&\lambda&6&7\\ 3&6&8&9\\ 4&7&9&10 \end{pmatrix}$$ My attempt: $$\begin{pmatrix} 1&2&3&4\\ 2&\lambda&6&7\\ 3&6&8&9\\ 4&7&9&10 \end{pmatrix}\sim\begin{pmatrix} 1&2&3&4\\ 0&\lambda-4&0&-1\\ 0&0&-1&-3\\ 0&-1&-3&-6 \end{pmatrix}\sim\begin{pmatrix} 1&0&-3&-8\\ 0&\lambda-4&0&-1\\ 0&0&-1&-3\\ 0&-1&-3&-6\\ \end{pmatrix}$$ For $\lambda=4$ we have:  $$\begin{pmatrix} 1 &0&-3&-8\\ 0&0&0&-1\\ 0&0&-1&-3\\ 0&-1&-3&-6 \end{pmatrix}\sim\begin{pmatrix} 1&0&0&1\\ 0&0&0&-1\\ 0&0&-1&-3\\ 0&-1&0&3\end{pmatrix}\sim\begin{pmatrix} 1&0&0&0\\ 0&0&0&-1\\ 0&0&-1&0\\ 0&-1&0&0\\ \end{pmatrix}$$ $\Rightarrow r(A)=4$ For $\lambda\neq 4$ we have: $$\begin{pmatrix} 1&0&0&1\\ 0&\lambda-4&0&-1\\ 0&0&-1&-3\\ 0&-1&0&3\\ \end{pmatrix}\sim\begin{pmatrix} 1&0&0&1\\ 0&0&0&3\lambda-13\\ 0&0&-1&-3\\ 0&-1&0&3 \end{pmatrix}$$ For$\lambda=\frac{13}{3}\Rightarrow r(A)=3$ and for $\lambda\neq \frac{13}{3} \Rightarrow r(A)=4$ Is this correct? Thanks!,,"['linear-algebra', 'matrices', 'matrix-rank']"
26,What are the conditions that ensure that a linear operator on a separable Hilbert space has a discrete spectrum and its eigenvectors form a basis?,What are the conditions that ensure that a linear operator on a separable Hilbert space has a discrete spectrum and its eigenvectors form a basis?,,"I am particularly interesed in answers such as: symmetric, bounded, positive. self-adjoint and bounded essentialy self-adjoint and positive. (Those were invented) I'm interested in this question in the frame of Quantum mechanics. Thanks a lot!","I am particularly interesed in answers such as: symmetric, bounded, positive. self-adjoint and bounded essentialy self-adjoint and positive. (Those were invented) I'm interested in this question in the frame of Quantum mechanics. Thanks a lot!",,"['linear-algebra', 'hilbert-spaces', 'linear-transformations', 'quantum-mechanics']"
27,$\operatorname{Adj} (\mathbf I_n x-\mathbf A)$ when $\operatorname{rank}(\mathbf A)\le n-2$,when,\operatorname{Adj} (\mathbf I_n x-\mathbf A) \operatorname{rank}(\mathbf A)\le n-2,"Let $\mathbf B$ denote an $n \times n$ matrix with $r\equiv\operatorname{rank}(\mathbf B)$. I need to prove the following conjecture: If $r \le n - 2$, then there exists a polynomial matrix $\mathbf P(x)$ such that   $$ \operatorname{Adj} (\mathbf I_n x-\mathbf B)= x^{n-r-1}\mathbf P(x).  $$","Let $\mathbf B$ denote an $n \times n$ matrix with $r\equiv\operatorname{rank}(\mathbf B)$. I need to prove the following conjecture: If $r \le n - 2$, then there exists a polynomial matrix $\mathbf P(x)$ such that   $$ \operatorname{Adj} (\mathbf I_n x-\mathbf B)= x^{n-r-1}\mathbf P(x).  $$",,"['linear-algebra', 'matrix-rank', 'adjoint-operators']"
28,"Why are $S$, $Z$ and $M$ used to denote the Conductor, Cyclic subspace and Annihilator in linear algebra?","Why are ,  and  used to denote the Conductor, Cyclic subspace and Annihilator in linear algebra?",S Z M,"In the text Linear Algebra (by Hoffman and Kunze), there are notations S, Z, M. What are these short for – that is, why are these particular three letters used for the following concepts? (i) S . Let $W$ be an invariant subspace for $T$ and let $\alpha$ be a vector in $V$ . The $T$ - conductor of $\alpha$ into $W$ is the set $S_{T}(\alpha ; W)$ , which consists of all polynomials $g$ (over the scalar field) such that $g(T)\alpha$ is in $W$ . (ii) Z . If $\alpha$ is any vector in $V$ , the $T$ - cyclic subspace generated by $\alpha$ is the subspace $Z(\alpha ;T)$ of all vectors of the form $g(T)\alpha$ , $g$ in $F[x]$ . (iii) M . If $\alpha$ is any vector in $V$ , the $T$ -annihilator of $\alpha$ is the ideal $M(\alpha ; T)$ in $F[x]$ consisting of all polynomials $g$ over $F$ such that $g(T)\alpha = 0$ .","In the text Linear Algebra (by Hoffman and Kunze), there are notations S, Z, M. What are these short for – that is, why are these particular three letters used for the following concepts? (i) S . Let be an invariant subspace for and let be a vector in . The - conductor of into is the set , which consists of all polynomials (over the scalar field) such that is in . (ii) Z . If is any vector in , the - cyclic subspace generated by is the subspace of all vectors of the form , in . (iii) M . If is any vector in , the -annihilator of is the ideal in consisting of all polynomials over such that .",W T \alpha V T \alpha W S_{T}(\alpha ; W) g g(T)\alpha W \alpha V T \alpha Z(\alpha ;T) g(T)\alpha g F[x] \alpha V T \alpha M(\alpha ; T) F[x] g F g(T)\alpha = 0,"['linear-algebra', 'notation']"
29,Compute a diagonalizable matrix close in matrix exponential,Compute a diagonalizable matrix close in matrix exponential,,"It is known that for any matrix $A$, one can perturb $A$ slightly so that the resulting $A(\epsilon)$ is diagonalizable. I am wondering whether for any matrix $A$, $\epsilon>0$, there is an algorithm to compute a diagonalizable matrix $A(\epsilon)$ such that $$\|e^A - e^{A(\epsilon)}\|_2\leq \epsilon$$ Here $e^A$ is the matrix exponential, and $\|\cdot\|_2$ is the 2-norm.","It is known that for any matrix $A$, one can perturb $A$ slightly so that the resulting $A(\epsilon)$ is diagonalizable. I am wondering whether for any matrix $A$, $\epsilon>0$, there is an algorithm to compute a diagonalizable matrix $A(\epsilon)$ such that $$\|e^A - e^{A(\epsilon)}\|_2\leq \epsilon$$ Here $e^A$ is the matrix exponential, and $\|\cdot\|_2$ is the 2-norm.",,"['linear-algebra', 'matrices', 'numerical-linear-algebra', 'diagonalization', 'matrix-exponential']"
30,The matrix square root is not differentiable on the boundary of the manifold of positive semi-definite matrices?,The matrix square root is not differentiable on the boundary of the manifold of positive semi-definite matrices?,,"$\newcommand{\psym}{\operatorname{P}_{\ge 0}}$ $\newcommand{\Sig }{\Sigma}$ Let $\psym$ denote the subset of symmetric positive semi-definite matrices. Let $S:\psym \setminus \{0\} \to \psym \setminus \{0\}$, where $S(A)=\sqrt{A}$ is the unique positive semi-definite square root of  $A$. $\psym \setminus \{0\}$ is a manifold with boundary. I am trying to prove $S$ is not differentiable at every point in $\{A \in \psym | \, \, \det(A)=0 \}$ (i.e, on the boundary). Am I correct about this claim? and its proof? Here is my attempt: Assume it was differentiable at such an $A$. Since $S^{-1}(A)=A^2$ is differentiable everywhere, we would get: $$ Id=d(S^{-1} \circ S)_A = dS^{-1}_\sqrt{A} \circ dS_A \Rightarrow dS^{-1}_\sqrt{A} \text{ is invertible}$$ But this is false since $dS^{-1}_A(X)=AX+XA$ is non-invertible. Let's see why. First, note that $dS^{-1}_A:\operatorname{sym}_n \to \operatorname{sym}_n$ (where $\operatorname{sym}_n$ is the space of symmetric matrices). We want to show there is a symmetric, non-zero $X$ such that $AX+XA=0$. Since $A$ is symmetric we can orthogonally diagonalize it: $A=V \Sig V^T, V \in O_n$ Then $AX+XA=V \Sig V^T X+XV \Sig V^T=0 \iff \Sig (V^TXV) + (V^TXV) \Sig =0$ Since $X$ is non-zero and symmetric $\iff$ $V^TXV$ is non-zero and symmetric, it is enough to show this for non-zero diagonal matrices* $\Sig$ such that $\det(\Sig)=0$. In this case, the equation becomes $X_{ij}(\sigma_i+\sigma_j)=0$. Assume without loss of generality $\sigma_1 = 0$ (since $\det(\Sig)=0$), and choose $X_{11}=1$ and all the other $X_{ij}$ to be zero. Is this proof true? Is there an easier argument? *There are other, perhaps easier ways to see it is enough to consider the diagonal case only. (For example, using the fact that taking squares commutes with conjugation, and orthogonal conjugation is a self-diffeomorphism)","$\newcommand{\psym}{\operatorname{P}_{\ge 0}}$ $\newcommand{\Sig }{\Sigma}$ Let $\psym$ denote the subset of symmetric positive semi-definite matrices. Let $S:\psym \setminus \{0\} \to \psym \setminus \{0\}$, where $S(A)=\sqrt{A}$ is the unique positive semi-definite square root of  $A$. $\psym \setminus \{0\}$ is a manifold with boundary. I am trying to prove $S$ is not differentiable at every point in $\{A \in \psym | \, \, \det(A)=0 \}$ (i.e, on the boundary). Am I correct about this claim? and its proof? Here is my attempt: Assume it was differentiable at such an $A$. Since $S^{-1}(A)=A^2$ is differentiable everywhere, we would get: $$ Id=d(S^{-1} \circ S)_A = dS^{-1}_\sqrt{A} \circ dS_A \Rightarrow dS^{-1}_\sqrt{A} \text{ is invertible}$$ But this is false since $dS^{-1}_A(X)=AX+XA$ is non-invertible. Let's see why. First, note that $dS^{-1}_A:\operatorname{sym}_n \to \operatorname{sym}_n$ (where $\operatorname{sym}_n$ is the space of symmetric matrices). We want to show there is a symmetric, non-zero $X$ such that $AX+XA=0$. Since $A$ is symmetric we can orthogonally diagonalize it: $A=V \Sig V^T, V \in O_n$ Then $AX+XA=V \Sig V^T X+XV \Sig V^T=0 \iff \Sig (V^TXV) + (V^TXV) \Sig =0$ Since $X$ is non-zero and symmetric $\iff$ $V^TXV$ is non-zero and symmetric, it is enough to show this for non-zero diagonal matrices* $\Sig$ such that $\det(\Sig)=0$. In this case, the equation becomes $X_{ij}(\sigma_i+\sigma_j)=0$. Assume without loss of generality $\sigma_1 = 0$ (since $\det(\Sig)=0$), and choose $X_{11}=1$ and all the other $X_{ij}$ to be zero. Is this proof true? Is there an easier argument? *There are other, perhaps easier ways to see it is enough to consider the diagonal case only. (For example, using the fact that taking squares commutes with conjugation, and orthogonal conjugation is a self-diffeomorphism)",,"['linear-algebra', 'differential-geometry', 'proof-verification', 'smooth-manifolds', 'matrix-calculus']"
31,The lattice points in the real cone of some semigroups are just the integer cone of that semigroup.,The lattice points in the real cone of some semigroups are just the integer cone of that semigroup.,,"I'm trying to solve an exercise in Fulton's book on toric varieties, and have reduced it to the following: Let $M$ be a lattice of rank $n$ with $M \otimes \mathbb{R} = V$ , and $S$ be a finitely generated semigroup of $M$ , such that the following conditions hold: i) S is saturated. (If $v \in M, n \in \mathbb{Z}_{> 0}$ with $nv \in S$ , then $v \in S$ .) ii) S generates M as a group. Then the lattice points inside the real cone on $S$ are simply the integer cone on $S$ . In other words, given a saturated semigroup $S=\langle v_1,\dots,v_k\rangle_{\mathbb{Z}_{\geq 0}}$ with $S=\langle v_1,\dots,v_k\rangle_{\mathbb{Z}} = M$ , then $\langle v_1,\dots,v_k\rangle_{\mathbb{R}_{\geq 0}}\cap M = S$ . This seems geometrically obvious, but I can't find a way to prove it. I can prove the result if $\langle v_1,\dots,v_k\rangle_{\mathbb{R}_{\geq 0}}\cap M=\langle v_1,\dots,v_k\rangle_{\mathbb{Q}_{\geq 0}}\cap M$ using the saturatedness of $S$ , the problem is showing that any positve real linear combination of the $v_i$ lying in the lattice can also be written as a positive rational combination. Thinking about the different ways to represent the same vector, we see that finding $r=(r_1,\dots,r_k)$ such that $\sum r_iv_i = v$ is equivalent to saying that $r$ is a solution to the matrix equation: $$\bigg(v_1 \dots v_k \bigg)x=v $$ The solutions to this equation are either an empty set, of an affine subspace of $\mathbb{R}^k$ . But if $v$ is in the intersection we're lookinh at, we know that the solution space contains a ""positive"" point, i.e. a point of $(\mathbb{R}_{\geq 0})^k$ , and an ""integer"" point, i.e. a point of $\mathbb{Z}^k$ . I'd like to use this somehow to show that it also contains a positive rational point, which is easy enough if the positive point lies in the strict interior of the set of positive points, but seems like it could be impossible in some examples, such as if the solution space is an affine line running tangent to the set of positive points, so I'm not sure if this approach will work...","I'm trying to solve an exercise in Fulton's book on toric varieties, and have reduced it to the following: Let be a lattice of rank with , and be a finitely generated semigroup of , such that the following conditions hold: i) S is saturated. (If with , then .) ii) S generates M as a group. Then the lattice points inside the real cone on are simply the integer cone on . In other words, given a saturated semigroup with , then . This seems geometrically obvious, but I can't find a way to prove it. I can prove the result if using the saturatedness of , the problem is showing that any positve real linear combination of the lying in the lattice can also be written as a positive rational combination. Thinking about the different ways to represent the same vector, we see that finding such that is equivalent to saying that is a solution to the matrix equation: The solutions to this equation are either an empty set, of an affine subspace of . But if is in the intersection we're lookinh at, we know that the solution space contains a ""positive"" point, i.e. a point of , and an ""integer"" point, i.e. a point of . I'd like to use this somehow to show that it also contains a positive rational point, which is easy enough if the positive point lies in the strict interior of the set of positive points, but seems like it could be impossible in some examples, such as if the solution space is an affine line running tangent to the set of positive points, so I'm not sure if this approach will work...","M n M \otimes \mathbb{R} = V S M v \in M, n \in \mathbb{Z}_{> 0} nv \in S v \in S S S S=\langle v_1,\dots,v_k\rangle_{\mathbb{Z}_{\geq 0}} S=\langle v_1,\dots,v_k\rangle_{\mathbb{Z}} = M \langle v_1,\dots,v_k\rangle_{\mathbb{R}_{\geq 0}}\cap M = S \langle v_1,\dots,v_k\rangle_{\mathbb{R}_{\geq 0}}\cap M=\langle v_1,\dots,v_k\rangle_{\mathbb{Q}_{\geq 0}}\cap M S v_i r=(r_1,\dots,r_k) \sum r_iv_i = v r \bigg(v_1 \dots v_k \bigg)x=v  \mathbb{R}^k v (\mathbb{R}_{\geq 0})^k \mathbb{Z}^k","['linear-algebra', 'geometry', 'semigroups', 'integer-lattices', 'toric-geometry']"
32,'Sign' of normalized eigenvector for singular value decomposition,'Sign' of normalized eigenvector for singular value decomposition,,"I'm working on an SV decomposition script in Python. I am getting incorrect results because of the 'indeterminacy' associated with normalizing the singular vectors. I understand that the sign of the vectors does not matter in terms of their behaviour as eigenvectors, but it does give incorrect results for SV decomposition. My example is this matrix: $$ A = \begin{bmatrix} 3 & 2\\ 1 & -1\\ \end{bmatrix} $$ When I use numpy.linalg.eig to calculate the normalized eigenvectors for the singular vectors, some of them are the opposite sign to the singular vectors returned by numpy.linalg.svd (i.e. negative of each other) - if I understand correctly both should be valid normalized eigenvectors. In all other respects my algorithm's results are the same as Numpy's. When I expand the factorisation, a lot of the time mine is incorrect while Numpy's always is. I believe the problem is that numpy.linalg.eig just happens to return the 'wrongly signed' eigenvectors. For singular value decomposition, is there any easy/deterministic way to check which 'sign' your singular vectors need to have?","I'm working on an SV decomposition script in Python. I am getting incorrect results because of the 'indeterminacy' associated with normalizing the singular vectors. I understand that the sign of the vectors does not matter in terms of their behaviour as eigenvectors, but it does give incorrect results for SV decomposition. My example is this matrix: $$ A = \begin{bmatrix} 3 & 2\\ 1 & -1\\ \end{bmatrix} $$ When I use numpy.linalg.eig to calculate the normalized eigenvectors for the singular vectors, some of them are the opposite sign to the singular vectors returned by numpy.linalg.svd (i.e. negative of each other) - if I understand correctly both should be valid normalized eigenvectors. In all other respects my algorithm's results are the same as Numpy's. When I expand the factorisation, a lot of the time mine is incorrect while Numpy's always is. I believe the problem is that numpy.linalg.eig just happens to return the 'wrongly signed' eigenvectors. For singular value decomposition, is there any easy/deterministic way to check which 'sign' your singular vectors need to have?",,"['linear-algebra', 'eigenvalues-eigenvectors', 'matrix-decomposition', 'svd']"
33,Why is the trivial vector space the smallest vector space?,Why is the trivial vector space the smallest vector space?,,My book (Elementary Linear Algebra by Andrilli) says: The set $\mathcal{{V}}$ = {${\mathbb {0}}$} is a vector space AND is   the smallest vector space. Then the book asks why $\mathcal{{V}}$ is the smallest vector space. I have no idea where to even start to explain why $\mathcal{{V}}$ is the smallest space. It seems like an odd question to ask.,My book (Elementary Linear Algebra by Andrilli) says: The set $\mathcal{{V}}$ = {${\mathbb {0}}$} is a vector space AND is   the smallest vector space. Then the book asks why $\mathcal{{V}}$ is the smallest vector space. I have no idea where to even start to explain why $\mathcal{{V}}$ is the smallest space. It seems like an odd question to ask.,,[]
34,Bijection that preserve lines must be linear,Bijection that preserve lines must be linear,,"There have been some past posts on this topic, but with no complete answer provided. Namely, if T is a bijection of the Euclidean plane that maps line segments to line segments (setwise) then T is an affine transformation, in the sense of linear algebra. What is the most elementary proof of this fact?","There have been some past posts on this topic, but with no complete answer provided. Namely, if T is a bijection of the Euclidean plane that maps line segments to line segments (setwise) then T is an affine transformation, in the sense of linear algebra. What is the most elementary proof of this fact?",,"['linear-algebra', 'geometry', 'analytic-geometry']"
35,Is the Laplace transform a vector space isomorphism? And what space is it isomorphic to?,Is the Laplace transform a vector space isomorphism? And what space is it isomorphic to?,,"The laplace transform is a linear transformation, $\mathcal{L}: \mathcal{M} \rightarrow?$, where $\mathcal{M}$ is the set of exponentially bounded functions on $\mathbb{R},$since $\mathcal{L}(af(x)+bg(x))=a \mathcal{L}(f(x))+b\mathcal{g(x)}$   for $a,b\in \mathbb{R}$ and $f,g \in \mathcal{M}$.  It seems to be injective since $\operatorname{Ker}(\mathcal{L})=0$ unless I've missed something. Therefore by the rank-nullity theorem $\mathcal{L}$ must surjective and so it is an isomorphism. So my questions are 1) Is this proof outline correct? and 2) what set is the laplace transform mapping into?","The laplace transform is a linear transformation, $\mathcal{L}: \mathcal{M} \rightarrow?$, where $\mathcal{M}$ is the set of exponentially bounded functions on $\mathbb{R},$since $\mathcal{L}(af(x)+bg(x))=a \mathcal{L}(f(x))+b\mathcal{g(x)}$   for $a,b\in \mathbb{R}$ and $f,g \in \mathcal{M}$.  It seems to be injective since $\operatorname{Ker}(\mathcal{L})=0$ unless I've missed something. Therefore by the rank-nullity theorem $\mathcal{L}$ must surjective and so it is an isomorphism. So my questions are 1) Is this proof outline correct? and 2) what set is the laplace transform mapping into?",,"['linear-algebra', 'vector-spaces', 'laplace-transform', 'linear-transformations']"
36,Floating point arithmetic operations when row reducing matrices,Floating point arithmetic operations when row reducing matrices,,"A numerical note in my linear algebra text states the following: ""In general, the forward phase of row reduction takes much longer than the backward phase. An algorithm for solving a system is usually measured in flops (or floating point operations). A flop is one arithmetic operation $(+,-,*,/)$ on two real floating point numbers. For an $n\times(n+1)$ matrix, the reduction to echelon form can take $$ \frac{2}{3}n^3+\frac{1}{2}n^2-\frac{7}{6}n\tag{1} $$ flops (which is approximately $2n^3/3$ flops when $n$ is moderately large--say, $n\geq 30$). In contrast, further reduction to reduced echelon form needs at most $n^2$ flops."" There is no explanation at all how the author came up with the expression in $(1)$. There is obviously a reason for that expression and the $n^2$ mentioned at the end of the quote. Can someone with knowledge of linear algebra or computer architecture, etc., explain why these expressions are the way there? It seems like the author just pulled them out of nowhere.","A numerical note in my linear algebra text states the following: ""In general, the forward phase of row reduction takes much longer than the backward phase. An algorithm for solving a system is usually measured in flops (or floating point operations). A flop is one arithmetic operation $(+,-,*,/)$ on two real floating point numbers. For an $n\times(n+1)$ matrix, the reduction to echelon form can take $$ \frac{2}{3}n^3+\frac{1}{2}n^2-\frac{7}{6}n\tag{1} $$ flops (which is approximately $2n^3/3$ flops when $n$ is moderately large--say, $n\geq 30$). In contrast, further reduction to reduced echelon form needs at most $n^2$ flops."" There is no explanation at all how the author came up with the expression in $(1)$. There is obviously a reason for that expression and the $n^2$ mentioned at the end of the quote. Can someone with knowledge of linear algebra or computer architecture, etc., explain why these expressions are the way there? It seems like the author just pulled them out of nowhere.",,"['linear-algebra', 'numerical-methods', 'numerical-linear-algebra']"
37,Linear maps for $\Bbb{R}^n$ to $\Bbb{R}^m$?,Linear maps for  to ?,\Bbb{R}^n \Bbb{R}^m,"This question is related to: What is $\Bbb{R}^n$? The basis of a matrix representation I am still confused about the topics in these questions and am going to ask another question that will hopefully clarify this for me. Consider the following theorems: Theorem 1 Let $A$ be an $m\times n$ matrix with components in the field $\Bbb{F}$ then the map: $$\Bbb{F}^n\rightarrow \Bbb{F}^m$$   $$x \mapsto  Ax$$   is linear. Conversely, if $f:\Bbb{F}^n \rightarrow \Bbb{F}^m$ is linear , there exists a unique $m \times n$ matrix $A$  with components in the field $\Bbb{F}$  for which $f(x)=Ax$. Hence one can interoperate the $m \times n$ matrices as the linear map from $\Bbb{F}^n$ to $\Bbb{F}^n$ This was taken from ""Linear Algebra"" by Janich, Kalus (with minor changes). Now consider the similar theorem from the same book. Theorem 2 Let $f:V\rightarrow W$ be a linear map between vector spaces over $\Bbb{F}$, and let $(v_1,...,v_n)$ and $(w_1,...,w_n)$ be bases for $V$ and $W$ respectively. Then the $m \times n$ matrix $B$ determined by the commutator diagram: $$ \require{AMScd} \begin{CD} V @>{f}>> W\\ @VVV @VVV \\ \mathbb{F}^n @>{B}>> \mathbb{F}^n \end{CD} $$   is called the matrix associated to $f$ relative to the two chosen bases. The matrix $B$ is associated with the basis $v= (v_1,...,v_n)$ and $w=(w_1,...,w_n)$, and we could write $B$ in terms of another basis which will change its components and be representing $f$ in the new basis. It is thus not theorem 2 that I have a problem understanding. It is rather theorem 1. Example Consider the linear map $$f(         \begin{bmatrix}         x \\ y         \end{bmatrix})=\begin{bmatrix}         2x+y \\ y         \end{bmatrix}$$   Clearly here the vector  $$\begin{bmatrix}         x \\ y         \end{bmatrix}$$ is in $\Bbb{R}^n$ (taking $x,y\in \Bbb{R}$) and is not a coordinate vector , but an arbitrary vector in $\Bbb{R}^n$. We could write the linear map $f$ as follows:   $$f(         \begin{bmatrix}         x \\ y         \end{bmatrix})= \begin{pmatrix}         2 & 1\\ 0&1         \end{pmatrix} \begin{bmatrix}         x \\ y         \end{bmatrix} $$   In this case the matrix $A$ in theorem 1 is given by:   $$A=\begin{pmatrix}         2 & 1\\ 0&1         \end{pmatrix}$$ The thing that I am confused about is simply which bias the matrix $A$ is in as described above. My view (which goes against the answers in the linked questions) is that $A$ should not be associated with a basis as described above. The reasoning behind this is that $A$ performers exactly the same operation on a vector $x$ as $f$ does with $x$ not been a coordinate vector. If we however say that $A$ is in a basis (by in a basis I mean like the matrix $B$ in theorem 2, so that is represents $f$ with respect to one (or two, if the bases of the domain and codomain are different) bases). Then it follows that $x$ also has to be a coordinate vector in this same basis (the basis of the domain). But the linear map $f$ does not act on coordinate vectors, it is not associated with a basis and therefore $x$ is not a coordiante vector. So my conclusion is that: The matrix $A$ is not a representation of $f$ in a particular basis but represents $f$ in general and performing $A$ on an arbitrary vector in $\Bbb{R}^n$ is equivalent to performing the linear map $f$ on it. And both $A$ operate on vectors in $\Bbb{R}^n$ which themselves are not coordinate vectors. So the matrix $A$ is not the same as $B$ i.e. $B$ represents its linear map in with respect to a particular set of basis and acts on coordinate vectors only. Whilst the matrix $A$ is not related to a basis and acts on any vectors, as $f$ does in $\Bbb{R}^n$. Please could you either confirm that this argument/analysis is right or wrong. I am really confused about this topic, so if you could give sources in you questions, this would be a great help. p.s. I note that this question is very similar to my other questions, I have however tried to make it slightly more detailed and asking a different question at the end. I have asked this one since I am still confused about the subject and want a detailed answer, preferably backed up by sources.","This question is related to: What is $\Bbb{R}^n$? The basis of a matrix representation I am still confused about the topics in these questions and am going to ask another question that will hopefully clarify this for me. Consider the following theorems: Theorem 1 Let $A$ be an $m\times n$ matrix with components in the field $\Bbb{F}$ then the map: $$\Bbb{F}^n\rightarrow \Bbb{F}^m$$   $$x \mapsto  Ax$$   is linear. Conversely, if $f:\Bbb{F}^n \rightarrow \Bbb{F}^m$ is linear , there exists a unique $m \times n$ matrix $A$  with components in the field $\Bbb{F}$  for which $f(x)=Ax$. Hence one can interoperate the $m \times n$ matrices as the linear map from $\Bbb{F}^n$ to $\Bbb{F}^n$ This was taken from ""Linear Algebra"" by Janich, Kalus (with minor changes). Now consider the similar theorem from the same book. Theorem 2 Let $f:V\rightarrow W$ be a linear map between vector spaces over $\Bbb{F}$, and let $(v_1,...,v_n)$ and $(w_1,...,w_n)$ be bases for $V$ and $W$ respectively. Then the $m \times n$ matrix $B$ determined by the commutator diagram: $$ \require{AMScd} \begin{CD} V @>{f}>> W\\ @VVV @VVV \\ \mathbb{F}^n @>{B}>> \mathbb{F}^n \end{CD} $$   is called the matrix associated to $f$ relative to the two chosen bases. The matrix $B$ is associated with the basis $v= (v_1,...,v_n)$ and $w=(w_1,...,w_n)$, and we could write $B$ in terms of another basis which will change its components and be representing $f$ in the new basis. It is thus not theorem 2 that I have a problem understanding. It is rather theorem 1. Example Consider the linear map $$f(         \begin{bmatrix}         x \\ y         \end{bmatrix})=\begin{bmatrix}         2x+y \\ y         \end{bmatrix}$$   Clearly here the vector  $$\begin{bmatrix}         x \\ y         \end{bmatrix}$$ is in $\Bbb{R}^n$ (taking $x,y\in \Bbb{R}$) and is not a coordinate vector , but an arbitrary vector in $\Bbb{R}^n$. We could write the linear map $f$ as follows:   $$f(         \begin{bmatrix}         x \\ y         \end{bmatrix})= \begin{pmatrix}         2 & 1\\ 0&1         \end{pmatrix} \begin{bmatrix}         x \\ y         \end{bmatrix} $$   In this case the matrix $A$ in theorem 1 is given by:   $$A=\begin{pmatrix}         2 & 1\\ 0&1         \end{pmatrix}$$ The thing that I am confused about is simply which bias the matrix $A$ is in as described above. My view (which goes against the answers in the linked questions) is that $A$ should not be associated with a basis as described above. The reasoning behind this is that $A$ performers exactly the same operation on a vector $x$ as $f$ does with $x$ not been a coordinate vector. If we however say that $A$ is in a basis (by in a basis I mean like the matrix $B$ in theorem 2, so that is represents $f$ with respect to one (or two, if the bases of the domain and codomain are different) bases). Then it follows that $x$ also has to be a coordinate vector in this same basis (the basis of the domain). But the linear map $f$ does not act on coordinate vectors, it is not associated with a basis and therefore $x$ is not a coordiante vector. So my conclusion is that: The matrix $A$ is not a representation of $f$ in a particular basis but represents $f$ in general and performing $A$ on an arbitrary vector in $\Bbb{R}^n$ is equivalent to performing the linear map $f$ on it. And both $A$ operate on vectors in $\Bbb{R}^n$ which themselves are not coordinate vectors. So the matrix $A$ is not the same as $B$ i.e. $B$ represents its linear map in with respect to a particular set of basis and acts on coordinate vectors only. Whilst the matrix $A$ is not related to a basis and acts on any vectors, as $f$ does in $\Bbb{R}^n$. Please could you either confirm that this argument/analysis is right or wrong. I am really confused about this topic, so if you could give sources in you questions, this would be a great help. p.s. I note that this question is very similar to my other questions, I have however tried to make it slightly more detailed and asking a different question at the end. I have asked this one since I am still confused about the subject and want a detailed answer, preferably backed up by sources.",,['linear-algebra']
38,How do I find a constant for a polynomial so its roots are reflective around a linear function?,How do I find a constant for a polynomial so its roots are reflective around a linear function?,,"How can I find all complex numbers $w$ so that the roots of the following polynomial are reflected around a linear function $f(x)$ $$p(q) = q^2-4q+w = 0$$ If I want to find all the complex numbers $w$ , for which the roots of the polynomials are reflective around, say $f(x) = -x+2$ , how would I go about doing that? Mark my words! The text below is a bit confusing, just showing my work if it lights any spark in someone's mind, hopefully it does! I really want to solve this problem, seems like something that would be very useful to know how to solve. I have been playing around and noticed some few things. If we have a function $g(x)$ which is perpendicular to $f(x)$ then all roots in a polynomial must either come in pair as points on $g(x)$ line OR a root will be sitting on the $f(x)$ line [double root?] ; this is the requirement for the condition in the title of this question This means in my case that all roots will be in the form of $\alpha=r\cdot e^{i\cdot(\pi/4+k\pi)}+ z$ where $z$ is the point at which $f(x) = g(x)$ . And $r=\sqrt{(\alpha_1-\Re z)^2+(\alpha_1-\Im z)^2}$ , where $\alpha_1$ is a root. The reason behind this is that the perpendicular function $g$ would have a negated slope from $f$ which in my case had the slope of $-1\cdot x\implies g(x) = x + m$ where $m$ would be chosen according to a root found. Drawing this, one easily sees the roots must be found at $\theta=\pi/4\ or\ -3\pi/4$ with mid-point in $z$ , and a radius of $r$ . I have also tried using Vietas formulas with no luck as Vietas formulas requires all coefficients to be known for a full solution. (Obviously) Edit 1. I might add! I'm still looking for a complete solution for all possible $w$ where the roots of $p$ are reflective around $f(x) = -x+2$","How can I find all complex numbers so that the roots of the following polynomial are reflected around a linear function If I want to find all the complex numbers , for which the roots of the polynomials are reflective around, say , how would I go about doing that? Mark my words! The text below is a bit confusing, just showing my work if it lights any spark in someone's mind, hopefully it does! I really want to solve this problem, seems like something that would be very useful to know how to solve. I have been playing around and noticed some few things. If we have a function which is perpendicular to then all roots in a polynomial must either come in pair as points on line OR a root will be sitting on the line [double root?] ; this is the requirement for the condition in the title of this question This means in my case that all roots will be in the form of where is the point at which . And , where is a root. The reason behind this is that the perpendicular function would have a negated slope from which in my case had the slope of where would be chosen according to a root found. Drawing this, one easily sees the roots must be found at with mid-point in , and a radius of . I have also tried using Vietas formulas with no luck as Vietas formulas requires all coefficients to be known for a full solution. (Obviously) Edit 1. I might add! I'm still looking for a complete solution for all possible where the roots of are reflective around",w f(x) p(q) = q^2-4q+w = 0 w f(x) = -x+2 g(x) f(x) g(x) f(x) \alpha=r\cdot e^{i\cdot(\pi/4+k\pi)}+ z z f(x) = g(x) r=\sqrt{(\alpha_1-\Re z)^2+(\alpha_1-\Im z)^2} \alpha_1 g f -1\cdot x\implies g(x) = x + m m \theta=\pi/4\ or\ -3\pi/4 z r w p f(x) = -x+2,"['linear-algebra', 'algebra-precalculus', 'complex-analysis', 'polynomials', 'complex-numbers']"
39,Geometry question pertaining to $4$ points in the plane where $90$ degree projectors are on each point and we must illuminate the whole plane.,Geometry question pertaining to  points in the plane where  degree projectors are on each point and we must illuminate the whole plane.,4 90,"Suppose we have $4$ points that can be positioned anywhere in $\mathbb{R}^2$. Now imagine each point has $90$ degree projectors coming out of them and you can rotate these projectors any way you would like. How can one prove that no matter where the four points are you can rotate the projectors so that the whole plane will be illuminated entirely? I would like to elaborate on illuminate. You see, if you can imagine to lines shooting out of the points where they form $90$ degree angles then the space contained inbetween the lines can be thought of as being illuminated. And so obviously if the points are positioned such that they look to be corners of a square, we can just rotate the projectors so that they look to be the sides of a square. And so then extending the projectors off to infinity we can illuminate the whole plane. This is just one case of course. I was not sure how to prove it no matter where the points are. My thought was wherever the points are try to draw an $xy$ axis such that each point is in a quadrant. Then one can just proceed to make the projectors so that they form a square like figure again. Its an interesting problem. Heres a bonus question. Suppose there are $n$ projectors located at $n$ points where each projector illuminates $\displaystyle\frac{360}{n}$ degrees. Then one can rotate them until the whole plane is illuminated.","Suppose we have $4$ points that can be positioned anywhere in $\mathbb{R}^2$. Now imagine each point has $90$ degree projectors coming out of them and you can rotate these projectors any way you would like. How can one prove that no matter where the four points are you can rotate the projectors so that the whole plane will be illuminated entirely? I would like to elaborate on illuminate. You see, if you can imagine to lines shooting out of the points where they form $90$ degree angles then the space contained inbetween the lines can be thought of as being illuminated. And so obviously if the points are positioned such that they look to be corners of a square, we can just rotate the projectors so that they look to be the sides of a square. And so then extending the projectors off to infinity we can illuminate the whole plane. This is just one case of course. I was not sure how to prove it no matter where the points are. My thought was wherever the points are try to draw an $xy$ axis such that each point is in a quadrant. Then one can just proceed to make the projectors so that they form a square like figure again. Its an interesting problem. Heres a bonus question. Suppose there are $n$ projectors located at $n$ points where each projector illuminates $\displaystyle\frac{360}{n}$ degrees. Then one can rotate them until the whole plane is illuminated.",,"['linear-algebra', 'geometry']"
40,"""Easy"" (maybe not) question about dual spaces (Lineal Algebra).","""Easy"" (maybe not) question about dual spaces (Lineal Algebra).",,"Hi everyone is my first time reading about dual spaces and in one part of the notes that I read, says: The dual of the quotient space $V/U$ is naturally a subspace of $V$, namely the annihilators of $U$ in $V$. I have doubt about this when says a naturally subspace does not actually mean which there is a natural identification of $(V/U)^*$ to the set of all the annihilators $U$ in $V$ under a map more than a subspace? Clearly exists an epimorphism $f:  V \twoheadrightarrow V/U $ and we can associate the map $f^t: (V/U)^* \rightarrow V^*$ by $h\mapsto h\circ f$, where $h\in (V/U)^*$, so $f^t[\,(V/U)^*]= \{\text{the annihilators of U in V} \}$, or there exist a natural identification of  $(V/U)^*$ to the set of all the annihilators $U$ in $V$ under $f^t$?  Am I completely off track or my intuition is correct? Claim:  Let $f, f^t$ as define above then  $f^t[\,(V/U)^*]= \{\text{the annihilators of U in V} \}$ Proof of claim: ($\Rightarrow$)  $T\in f^t[\,(V/U)^*]$, so $T=h\circ f$ for some $h\in (V/U)^*$, i.e., $h: V/U \rightarrow \mathbb{F}$. If $x\in U$, so $f(x)= x+U= U$ and then $h(f(x))= h(U)=0$. ($\Leftarrow$) Let $T\in \{\text{the annihilators of U in V} \}$, i.e., $T(x)=0$ whenever $x\in U$. Let define $\overline{T}:  V/U \rightarrow \mathbb{F}$ by the formula $\overline{T}(x+U)=T(x)$, we claim that $\overline{T}\circ f=T$, let $x\in V$ so $(\overline{T}\circ f) (x)=\overline{T}(f(x))=\overline{T}(x+U)=T(x)$. Then $\overline{T}\circ f= f^t(\overline{T})=T$, i.e., $T\in f^t[\,(V/U)^*]$. One more thing is very natural in the literature read that if $V$ is a vector space (a finite dimensional vector space) then is the dual of other space, that doesn't really mean that $V$ is isomorphic to the dual of other space instead of is the dual of some other space because not all the vector spaces are the set of linear functionals? Thanks in advance. Edit: If $U$ is a subspace of $V$ (and $V$ is finite dimensional vector space), clearly we have $V= U \oplus U'$  and $U'\cong V/U$ ($U'\hookrightarrow V \twoheadrightarrow V/U$) then we can conclude that  $(V/U)^* \cong (U')^*$ and is not difficult to show that $(U')^*$  contain all the annihilators of $U$ in $V$ is in that way in which  as says in the book ""the dual of the quotient space $V/U$ is naturally a subspace of $V$, namely the annihilators of $U$ in $V$"" because we can identified naturally with $(U')^*$? Edit: Other thing: In other part says 'If we choose a basis of $V$, and use it to identify elements of V with “column vectors” of length n, then elements of $V^*$ correspond to “row vectors” of the same length'. Why is this true? Clearly if we set $\mathcal{B}= \{v_1,..v_n\}$ be a basis for $V$, any element of $V$, says $v\in V$ can be expresses uniquely as $v=\sum_i a_i v_i$, so we can associate $v\mapsto [V]^\mathcal{B}$ which is the vector column. Now if $\mathcal{B^*}= \{v_1^*..v_n^*\}$ is the dual basis for $V^*$, so for any $f\in V^*$ we have $f= \sum_i f(v_i)v_i^*$  but I can't see in which sense we can associate it to a row vector $[f]_{B^*}$, naturally? Any comment it would be great. Thanks :)","Hi everyone is my first time reading about dual spaces and in one part of the notes that I read, says: The dual of the quotient space $V/U$ is naturally a subspace of $V$, namely the annihilators of $U$ in $V$. I have doubt about this when says a naturally subspace does not actually mean which there is a natural identification of $(V/U)^*$ to the set of all the annihilators $U$ in $V$ under a map more than a subspace? Clearly exists an epimorphism $f:  V \twoheadrightarrow V/U $ and we can associate the map $f^t: (V/U)^* \rightarrow V^*$ by $h\mapsto h\circ f$, where $h\in (V/U)^*$, so $f^t[\,(V/U)^*]= \{\text{the annihilators of U in V} \}$, or there exist a natural identification of  $(V/U)^*$ to the set of all the annihilators $U$ in $V$ under $f^t$?  Am I completely off track or my intuition is correct? Claim:  Let $f, f^t$ as define above then  $f^t[\,(V/U)^*]= \{\text{the annihilators of U in V} \}$ Proof of claim: ($\Rightarrow$)  $T\in f^t[\,(V/U)^*]$, so $T=h\circ f$ for some $h\in (V/U)^*$, i.e., $h: V/U \rightarrow \mathbb{F}$. If $x\in U$, so $f(x)= x+U= U$ and then $h(f(x))= h(U)=0$. ($\Leftarrow$) Let $T\in \{\text{the annihilators of U in V} \}$, i.e., $T(x)=0$ whenever $x\in U$. Let define $\overline{T}:  V/U \rightarrow \mathbb{F}$ by the formula $\overline{T}(x+U)=T(x)$, we claim that $\overline{T}\circ f=T$, let $x\in V$ so $(\overline{T}\circ f) (x)=\overline{T}(f(x))=\overline{T}(x+U)=T(x)$. Then $\overline{T}\circ f= f^t(\overline{T})=T$, i.e., $T\in f^t[\,(V/U)^*]$. One more thing is very natural in the literature read that if $V$ is a vector space (a finite dimensional vector space) then is the dual of other space, that doesn't really mean that $V$ is isomorphic to the dual of other space instead of is the dual of some other space because not all the vector spaces are the set of linear functionals? Thanks in advance. Edit: If $U$ is a subspace of $V$ (and $V$ is finite dimensional vector space), clearly we have $V= U \oplus U'$  and $U'\cong V/U$ ($U'\hookrightarrow V \twoheadrightarrow V/U$) then we can conclude that  $(V/U)^* \cong (U')^*$ and is not difficult to show that $(U')^*$  contain all the annihilators of $U$ in $V$ is in that way in which  as says in the book ""the dual of the quotient space $V/U$ is naturally a subspace of $V$, namely the annihilators of $U$ in $V$"" because we can identified naturally with $(U')^*$? Edit: Other thing: In other part says 'If we choose a basis of $V$, and use it to identify elements of V with “column vectors” of length n, then elements of $V^*$ correspond to “row vectors” of the same length'. Why is this true? Clearly if we set $\mathcal{B}= \{v_1,..v_n\}$ be a basis for $V$, any element of $V$, says $v\in V$ can be expresses uniquely as $v=\sum_i a_i v_i$, so we can associate $v\mapsto [V]^\mathcal{B}$ which is the vector column. Now if $\mathcal{B^*}= \{v_1^*..v_n^*\}$ is the dual basis for $V^*$, so for any $f\in V^*$ we have $f= \sum_i f(v_i)v_i^*$  but I can't see in which sense we can associate it to a row vector $[f]_{B^*}$, naturally? Any comment it would be great. Thanks :)",,"['linear-algebra', 'soft-question', 'intuition', 'self-learning']"
41,Intuition behind $\ker(T)=\ker(T^*)$ for $T$ a normal operator,Intuition behind  for  a normal operator,\ker(T)=\ker(T^*) T,"Let $T : V \to V$ be a normal operator and $V$ a finite-dimensional vector space. Show that $\ker(T)=  \ker(T^*)$ and $\text{im}(T) = \text{im}(T^*)$. I know how to rigorously show this, but I'm curious if anyone has an intuitive way of understanding why this has to be the case.","Let $T : V \to V$ be a normal operator and $V$ a finite-dimensional vector space. Show that $\ker(T)=  \ker(T^*)$ and $\text{im}(T) = \text{im}(T^*)$. I know how to rigorously show this, but I'm curious if anyone has an intuitive way of understanding why this has to be the case.",,"['linear-algebra', 'adjoint-operators']"
42,definition of determinant in Artin,definition of determinant in Artin,,"In Michael Artin's Algebra , the discussion on determinant starts from the standard recursive expansion by minors. Artin defines determinant as a function $\delta$ from a square matrix to a real number. Then Artin lists three characteristics for this function in Theorem 1.4.7 (page 20, second edition) as quoted below. ""Theorem 1.4.7 Uniqueness of the Determinant. There is a unique function $\delta$ on the space of $n\times n$ matrices with the properties below, namely the determinant. With $I$ denoting the identity matrix, $\delta(I)=1$ . $\delta$ is linear in the rows of the matrix $A$ . If two adjacent rows of a matrix $A$ are equal, then $\delta(A)$ =0."" In his book, Artin does not explain why $\delta$ should have these properties. I suppose in history people went through a period of trial and error before such abstract concept was proposed and accepted. Can anyone refer me to any source revealing how these properties were thought of, especially, the second and the third property. Thank you! Regards.","In Michael Artin's Algebra , the discussion on determinant starts from the standard recursive expansion by minors. Artin defines determinant as a function from a square matrix to a real number. Then Artin lists three characteristics for this function in Theorem 1.4.7 (page 20, second edition) as quoted below. ""Theorem 1.4.7 Uniqueness of the Determinant. There is a unique function on the space of matrices with the properties below, namely the determinant. With denoting the identity matrix, . is linear in the rows of the matrix . If two adjacent rows of a matrix are equal, then =0."" In his book, Artin does not explain why should have these properties. I suppose in history people went through a period of trial and error before such abstract concept was proposed and accepted. Can anyone refer me to any source revealing how these properties were thought of, especially, the second and the third property. Thank you! Regards.",\delta \delta n\times n I \delta(I)=1 \delta A A \delta(A) \delta,"['linear-algebra', 'soft-question', 'math-history']"
43,Criterion for positive semidefinite matrices,Criterion for positive semidefinite matrices,,"Is there a criterion for positive semidefiniteness of a matrix in terms of dimension reduction, i.e, such that positive semi-definiteness of $n \times n$ matrix is expressed as  positive semidefiniteness of smaller matrices and possibly some additional condition? Could anyone give me hint? Thanks for replies. UPD: besides the version of Sylvester's criterion for semi-definite case.","Is there a criterion for positive semidefiniteness of a matrix in terms of dimension reduction, i.e, such that positive semi-definiteness of $n \times n$ matrix is expressed as  positive semidefiniteness of smaller matrices and possibly some additional condition? Could anyone give me hint? Thanks for replies. UPD: besides the version of Sylvester's criterion for semi-definite case.",,"['linear-algebra', 'matrices', 'positive-semidefinite']"
44,Algebraic Proof of Stone-Weierstrass,Algebraic Proof of Stone-Weierstrass,,"I have seen a few proofs of the Stone-Weierstrass theorem today for the first time. While some were completely analytic, the proof given in Rudin's Principles of Mathematical Analysis hinted at an algebraic approach. He defines a sequence of normalized polynomials which he convolutes with the arbitrary continuous function $f(x)$ to obtain polynomial approximations. I am aware that convolution may be thought of as an inner product, hence was curious if there is a proof which uses algebraic language and minimizes the analysis. Of course, I do not expect the analysis to disappear, but perhaps it can be marginalized. For example, while the easiest proofs of the fundamental theorem of algebra are entirely analytic, there are mostly algebraic proofs which only use analysis to assert that odd degree polynomials have a real root. I seek such a proof because 1) I think it would be interesting, and 2) My intuition works best with algebra.","I have seen a few proofs of the Stone-Weierstrass theorem today for the first time. While some were completely analytic, the proof given in Rudin's Principles of Mathematical Analysis hinted at an algebraic approach. He defines a sequence of normalized polynomials which he convolutes with the arbitrary continuous function $f(x)$ to obtain polynomial approximations. I am aware that convolution may be thought of as an inner product, hence was curious if there is a proof which uses algebraic language and minimizes the analysis. Of course, I do not expect the analysis to disappear, but perhaps it can be marginalized. For example, while the easiest proofs of the fundamental theorem of algebra are entirely analytic, there are mostly algebraic proofs which only use analysis to assert that odd degree polynomials have a real root. I seek such a proof because 1) I think it would be interesting, and 2) My intuition works best with algebra.",,"['linear-algebra', 'analysis', 'functional-analysis']"
45,Coercive bilinear form on Hilbert space,Coercive bilinear form on Hilbert space,,"I need to show the two following results. If true, it must be a simple proof but I do not seem to be able to make it work. Thank you in advance. Consider a continuous symmetric bilinear form $B$ on a real Hilbert space $H$. Let $D$ be a closed subspace of that Hilbert space over which the form is coercive, i.e, there exists $\alpha >0 \in \mathbb{R}$ such that $B(d,d)\geq \alpha \|d\|^2$ for all $d\in D$. Take the new closed subspace of $H$ generated by $D$ and a one dimensional subspace. Assume $B$ is definite positive on the new subspace. Then I need to show that $B$ is also coercive on the new subspace. It seems like the logical thing to do is to use the Lax-Milgram theorem but I am not exactly sure how to do this.","I need to show the two following results. If true, it must be a simple proof but I do not seem to be able to make it work. Thank you in advance. Consider a continuous symmetric bilinear form $B$ on a real Hilbert space $H$. Let $D$ be a closed subspace of that Hilbert space over which the form is coercive, i.e, there exists $\alpha >0 \in \mathbb{R}$ such that $B(d,d)\geq \alpha \|d\|^2$ for all $d\in D$. Take the new closed subspace of $H$ generated by $D$ and a one dimensional subspace. Assume $B$ is definite positive on the new subspace. Then I need to show that $B$ is also coercive on the new subspace. It seems like the logical thing to do is to use the Lax-Milgram theorem but I am not exactly sure how to do this.",,"['linear-algebra', 'functional-analysis', 'operator-theory', 'quadratic-forms']"
46,Why does this matrix have 3 nonzero distinct eigenvalues,Why does this matrix have 3 nonzero distinct eigenvalues,,"Consider the $n \times n$ matrix $$A=\left[  \begin{array}{cccc} 0 & 1 & ... & 1 \\  1 & 0 &  & 0 \\  \vdots  &  & \ddots  &  \\  1 & 0 &  & 0% \end{array}% \right] $$ (A has $n-1$ ones in the first row, $n-1$ ones in the first column, and zeros anywhere else), and let $$G=A(I_n-\theta A)^{-1},$$ where $\theta$ is a scalar such that  $I_n-\theta A$ is positive definite. Let $W$ be a nontrivial subspace of $\mathbb{R}^{n}$ including the vector of all ones, and no eigenvectors of $A$. Let $M$ be the orthogonal projector onto the orthogonal complement of $W$. Let $$Q=G-\frac{1}{n}\mathrm{trace}(G) I_{n}.$$ Show that $$MQ+QM$$ has exactly 3 nonzero distinct eigenvalues (for any $n$, any $\theta$ such that  $I_n-\theta A$ is positive definite, any $W \subset \mathbb{R}^{n}$ including the vector of all ones).","Consider the $n \times n$ matrix $$A=\left[  \begin{array}{cccc} 0 & 1 & ... & 1 \\  1 & 0 &  & 0 \\  \vdots  &  & \ddots  &  \\  1 & 0 &  & 0% \end{array}% \right] $$ (A has $n-1$ ones in the first row, $n-1$ ones in the first column, and zeros anywhere else), and let $$G=A(I_n-\theta A)^{-1},$$ where $\theta$ is a scalar such that  $I_n-\theta A$ is positive definite. Let $W$ be a nontrivial subspace of $\mathbb{R}^{n}$ including the vector of all ones, and no eigenvectors of $A$. Let $M$ be the orthogonal projector onto the orthogonal complement of $W$. Let $$Q=G-\frac{1}{n}\mathrm{trace}(G) I_{n}.$$ Show that $$MQ+QM$$ has exactly 3 nonzero distinct eigenvalues (for any $n$, any $\theta$ such that  $I_n-\theta A$ is positive definite, any $W \subset \mathbb{R}^{n}$ including the vector of all ones).",,"['linear-algebra', 'graph-theory', 'eigenvalues-eigenvectors']"
47,How to diagonalize this matrix?,How to diagonalize this matrix?,,"Consider the $n\times m$ matrix $M=[M_1, \ldots, M_m]$ where the $i$-th column reads $$ M_i= \,^t(\underbrace{1,\ldots,1}_{a_i},0,\ldots,0) $$ where the $a_i$'s are given positive natural numbers. Is it possible to compute the  singular values of $M$ in terms of the $a_i$'s ? Maybe it is simpler if one choose the $a_i$'s non-decreasing. I am interested  in this problem since it somehow generalize the case where  $n=m$ and $a_i=n$ for all $i=1,\ldots,n$, and for which the singular values are $0$ with multiplicity $n-1$ and $n^2$ with multiplicity $1$. This comes from an easy kernel size plus trace argument. And I was wondering if it could be applied to this general case, but I didn't succeed ...","Consider the $n\times m$ matrix $M=[M_1, \ldots, M_m]$ where the $i$-th column reads $$ M_i= \,^t(\underbrace{1,\ldots,1}_{a_i},0,\ldots,0) $$ where the $a_i$'s are given positive natural numbers. Is it possible to compute the  singular values of $M$ in terms of the $a_i$'s ? Maybe it is simpler if one choose the $a_i$'s non-decreasing. I am interested  in this problem since it somehow generalize the case where  $n=m$ and $a_i=n$ for all $i=1,\ldots,n$, and for which the singular values are $0$ with multiplicity $n-1$ and $n^2$ with multiplicity $1$. This comes from an easy kernel size plus trace argument. And I was wondering if it could be applied to this general case, but I didn't succeed ...",,"['linear-algebra', 'matrices', 'eigenvalues-eigenvectors']"
48,Grassman formula for vector space dimensions,Grassman formula for vector space dimensions,,"If $U$ and $W$ are subspaces of a finite dimensional vector space, $$ \dim U + \dim W = \dim(U\cap W) + \dim(U + W)$$ Proof : let $B_{U\cap W} = \{v_1,\ldots,v_m\}$ be a base of $U\cap W$. If we extend the basis to $B_U = \{v_1,\ldots,v_m, u_{m+1}, \ldots, u_r\}$ and $B_W = \{v_1,\ldots,v_m, w_{m+1},\ldots,w_s\}$ then $S:=\{v_1,\ldots,v_m,u_{m+1},\ldots,u_r, w_{m+1},\ldots,w_s\}$ is a generating set of $U+W$. Now I have to prove that $S$ is linearly independent: $$ 0 = \sum_{i=1}^m a_i v_i + \sum_{j=m+1}^r b_j u_j + \sum_{k=r+1}^s c_k w_k  \implies v = \sum_{i = 1}^m a_i v_i+\sum_{j=m+1}^r b_j u_j = -\sum_{k=r+1}^m c_k w_k $$ is a vector of $U\cap W$ and $b_j = 0$ since $B_U$ is independent. Therefore $0 = \sum_{i=1}^m a_i v_i + \sum_{k=m+1}^s c_k w_k$ and, since $B_W$ is independent we have that $a_i = c_k = 0$, and $\dim(U+W) = \dim U + \dim W - \dim(U\cap W)$. Question : I can't understand why $v\in U\cap W$, since we have expressed it as linear combination of bases of $U$. Thanks in advance!","If $U$ and $W$ are subspaces of a finite dimensional vector space, $$ \dim U + \dim W = \dim(U\cap W) + \dim(U + W)$$ Proof : let $B_{U\cap W} = \{v_1,\ldots,v_m\}$ be a base of $U\cap W$. If we extend the basis to $B_U = \{v_1,\ldots,v_m, u_{m+1}, \ldots, u_r\}$ and $B_W = \{v_1,\ldots,v_m, w_{m+1},\ldots,w_s\}$ then $S:=\{v_1,\ldots,v_m,u_{m+1},\ldots,u_r, w_{m+1},\ldots,w_s\}$ is a generating set of $U+W$. Now I have to prove that $S$ is linearly independent: $$ 0 = \sum_{i=1}^m a_i v_i + \sum_{j=m+1}^r b_j u_j + \sum_{k=r+1}^s c_k w_k  \implies v = \sum_{i = 1}^m a_i v_i+\sum_{j=m+1}^r b_j u_j = -\sum_{k=r+1}^m c_k w_k $$ is a vector of $U\cap W$ and $b_j = 0$ since $B_U$ is independent. Therefore $0 = \sum_{i=1}^m a_i v_i + \sum_{k=m+1}^s c_k w_k$ and, since $B_W$ is independent we have that $a_i = c_k = 0$, and $\dim(U+W) = \dim U + \dim W - \dim(U\cap W)$. Question : I can't understand why $v\in U\cap W$, since we have expressed it as linear combination of bases of $U$. Thanks in advance!",,"['linear-algebra', 'vector-spaces']"
49,Find area of pentagon using determinants,Find area of pentagon using determinants,,"Find the area of the pentagon of the five vertices $(1,2), (4,1), (5,3), (3,7),  (2,6)$ . Please, use the way of using determinant. My idea is to cut the pentagon into some triangles, then calculate each triangle, then sum them. I wonder if there is some other way to directly calculate it using a bigger matrix calculating its determinant?","Find the area of the pentagon of the five vertices $(1,2), (4,1), (5,3), (3,7),  (2,6)$ . Please, use the way of using determinant. My idea is to cut the pentagon into some triangles, then calculate each triangle, then sum them. I wonder if there is some other way to directly calculate it using a bigger matrix calculating its determinant?",,['linear-algebra']
50,What would be a good method for finding the submatrix with the largest sum?,What would be a good method for finding the submatrix with the largest sum?,,This question is from an ongoing contest which ends in 4 days.  It is this problem from the October Challenge . Given:A Matrix (Not necessarily square) filled with negative and positive integers.What will be good way of finding the sub matrix with the largest sum  and this sub matrix may not be contiguous such that you can select columns 1 and 3 and rows 1 and 3 and leave out column 2 and row 2? Any hints/suggestions/tricks will help a lot.,This question is from an ongoing contest which ends in 4 days.  It is this problem from the October Challenge . Given:A Matrix (Not necessarily square) filled with negative and positive integers.What will be good way of finding the sub matrix with the largest sum  and this sub matrix may not be contiguous such that you can select columns 1 and 3 and rows 1 and 3 and leave out column 2 and row 2? Any hints/suggestions/tricks will help a lot.,,"['linear-algebra', 'algorithms', 'numerical-linear-algebra']"
51,How to show a Determinantal inequality,How to show a Determinantal inequality,,"If $A, B$ and $C$ are $n\times n$ positive semidefinite matrices. How to show that $$\det(A + B) + \det(A + C)\le \det A + \det(A + B + C)?$$","If $A, B$ and $C$ are $n\times n$ positive semidefinite matrices. How to show that $$\det(A + B) + \det(A + C)\le \det A + \det(A + B + C)?$$",,"['linear-algebra', 'inequality', 'determinant']"
52,Solve for integer matrix such that $R^gu=v$ given $u$ and $v$,Solve for integer matrix such that  given  and,R^gu=v u v,"Given non-negative integer $n$-vectors $u$ and $v$, how does one find all $n \times n$ non-negative integer matrices $R$ and powers $g$ such that $R^gu=v$?","Given non-negative integer $n$-vectors $u$ and $v$, how does one find all $n \times n$ non-negative integer matrices $R$ and powers $g$ such that $R^gu=v$?",,['linear-algebra']
53,"Given k real n x n matrices with a common eigenvector, is there some nontrivial polynomial equation the entries of the matrices satisfy?","Given k real n x n matrices with a common eigenvector, is there some nontrivial polynomial equation the entries of the matrices satisfy?",,"The following problem has come up in my research and I don't have the tools (i.e., I don't know Algebraic Geometry, especially over $\mathbb{R}$) to solve it. Consider two subsets $X$ and $Y$ of $M_n(\mathbb{R})^k$ with $k\geq 3$.  The subset $X$ consists of all $k$ tuples of n x n matrices $(A_1,..., A_k)$ such that for any two $A_i$ and $A_j$, there is a vector $v_{ij}$ which is simultaneously an eigenvector for $A_i$ and $A_j$ (but perhaps with different eigenvalues). The subset $Y$ consists of all those elements of $X$ such that $v_{ij}$ can be chosen independently from $i$ and $j$ - that is, if $(A_1,.., A_k)\in Y$ then all $k$ matrices have a common eigenvector.  (Of course, when $k=1$ or $k=2$, $X = Y$, hence the above restriction on $k$). Now, I have not been able to prove that $X$ is Zariski closed (though I have not tried that hard - it's not so important for my purposes), but I can prove that it's contained in a proper Zariski closed subset of $M_n(\mathbb{R})^k$ (thought of as $\mathbb{R}^{kn^2}$):  we have $f_{12} = det(A_1A_2 - A_2A_1) = 0$ since $A_1A_2 v_{12} = A_2A_1 v_{12}$.  Or, since we're worker over $\mathbb{R}$, we can put all the $f_{ij}$ into one big polynomial equation $$\sum_{1\leq i < j\leq k} f_{ij}^2 = 0.$$ What I'd like to know is Is there a Zariski closed subset $F$ with $Y\subseteq F\subsetneq X$? Said another way Is there a polynomial which is simultaneously satisfied by all k-tuples of matrices sharing a common eigenvector but for which there are elements in $X$ which do not solve it? Finally, in case it helps, the case I'm most interested in is $n=3$ and $k = 5$, but I imagine the choice of $n$ won't affect the answer greatly and $k=3$ probably contains all the insight necessary to tackle the larger $k$ values. Thank you in advance for your help.","The following problem has come up in my research and I don't have the tools (i.e., I don't know Algebraic Geometry, especially over $\mathbb{R}$) to solve it. Consider two subsets $X$ and $Y$ of $M_n(\mathbb{R})^k$ with $k\geq 3$.  The subset $X$ consists of all $k$ tuples of n x n matrices $(A_1,..., A_k)$ such that for any two $A_i$ and $A_j$, there is a vector $v_{ij}$ which is simultaneously an eigenvector for $A_i$ and $A_j$ (but perhaps with different eigenvalues). The subset $Y$ consists of all those elements of $X$ such that $v_{ij}$ can be chosen independently from $i$ and $j$ - that is, if $(A_1,.., A_k)\in Y$ then all $k$ matrices have a common eigenvector.  (Of course, when $k=1$ or $k=2$, $X = Y$, hence the above restriction on $k$). Now, I have not been able to prove that $X$ is Zariski closed (though I have not tried that hard - it's not so important for my purposes), but I can prove that it's contained in a proper Zariski closed subset of $M_n(\mathbb{R})^k$ (thought of as $\mathbb{R}^{kn^2}$):  we have $f_{12} = det(A_1A_2 - A_2A_1) = 0$ since $A_1A_2 v_{12} = A_2A_1 v_{12}$.  Or, since we're worker over $\mathbb{R}$, we can put all the $f_{ij}$ into one big polynomial equation $$\sum_{1\leq i < j\leq k} f_{ij}^2 = 0.$$ What I'd like to know is Is there a Zariski closed subset $F$ with $Y\subseteq F\subsetneq X$? Said another way Is there a polynomial which is simultaneously satisfied by all k-tuples of matrices sharing a common eigenvector but for which there are elements in $X$ which do not solve it? Finally, in case it helps, the case I'm most interested in is $n=3$ and $k = 5$, but I imagine the choice of $n$ won't affect the answer greatly and $k=3$ probably contains all the insight necessary to tackle the larger $k$ values. Thank you in advance for your help.",,"['linear-algebra', 'algebraic-geometry']"
54,Kalman Filter -- handling large covariance matrices with principal-component-like structures,Kalman Filter -- handling large covariance matrices with principal-component-like structures,,"I understand that the estimation of the covariance matrices are the important part of the Kalman filter. However in my use case my covariance matrices are really big but with a pretty neat factor model (PCA-like) structure. Borrowing from Wikipedia convention , what I mean is that our $Q$ and $R$ matrices can be written as some sort of reduced form $$Q =B\Sigma_{Q}B^{T}+\Sigma_{qq}$$ where $\Sigma_{qq}$ is a diagonal matrix and the dimension of $\Sigma_{Q}$ is significantly smaller than the dimension of $Q$ and similarly $$R =B\Sigma_{R} B^{T}+\Sigma_{rr}$$ For simplicity we can even assume $\Sigma_{Q} = c \Sigma_{R}$ , $\Sigma_{qq} = c \Sigma_{rr}$ . i.e they are scaled by a constant. Different from PCA or SVD, $\Sigma_Q$ or $\Sigma_R$ are not diagonal matrices, as in different ""factors/components"" have correlations, but it's enough to reduce the complexity of $R$ and $Q$ by quite a bit. Now, can the ""reduced form"" of the covariance matrices be carried into the rest of the update steps (again borrowed from wikipedia ), such as $P$ ? If I still have to operate on a full $P$ , it'd defeat the purposes of the reduced forms I developed for $R$ and $Q$ . We can further assume $F=I$ in my use case, if that helps, but not much we can assume about $H$ . If necessarily we can also assume I have no observation noise (i.e. $R$ is 0). I'd like to have a more general case without making assumption on $F$ or $R$ if possible for my own intellectual curiosity. \begin{aligned} \mathbf {P} _{k\mid k-1}&=\mathbf {F} _{k}\mathbf {P} _{k-1\mid k-1}\mathbf {F} _{k}^{\textsf {T}}+\mathbf {Q} _{k},\\ \mathbf {S} _{k}&=\mathbf {H} _{k}\mathbf {P} _{k\mid k-1}\mathbf {H} _{k}^{\textsf {T}}+\mathbf {R} _{k},\\ \mathbf {K} _{k}&=\mathbf {P} _{k\mid k-1}\mathbf {H} _{k}^{\textsf {T}}\mathbf {S} _{k}^{-1},\\ \mathbf {P} _{k|k}&=\left(\mathbf {I} -\mathbf {K} _{k}\mathbf {H} _{k}\right)\mathbf {P} _{k|k-1}. \end{aligned}","I understand that the estimation of the covariance matrices are the important part of the Kalman filter. However in my use case my covariance matrices are really big but with a pretty neat factor model (PCA-like) structure. Borrowing from Wikipedia convention , what I mean is that our and matrices can be written as some sort of reduced form where is a diagonal matrix and the dimension of is significantly smaller than the dimension of and similarly For simplicity we can even assume , . i.e they are scaled by a constant. Different from PCA or SVD, or are not diagonal matrices, as in different ""factors/components"" have correlations, but it's enough to reduce the complexity of and by quite a bit. Now, can the ""reduced form"" of the covariance matrices be carried into the rest of the update steps (again borrowed from wikipedia ), such as ? If I still have to operate on a full , it'd defeat the purposes of the reduced forms I developed for and . We can further assume in my use case, if that helps, but not much we can assume about . If necessarily we can also assume I have no observation noise (i.e. is 0). I'd like to have a more general case without making assumption on or if possible for my own intellectual curiosity.","Q R Q =B\Sigma_{Q}B^{T}+\Sigma_{qq} \Sigma_{qq} \Sigma_{Q} Q R =B\Sigma_{R} B^{T}+\Sigma_{rr} \Sigma_{Q} = c \Sigma_{R} \Sigma_{qq} = c \Sigma_{rr} \Sigma_Q \Sigma_R R Q P P R Q F=I H R F R \begin{aligned}
\mathbf {P} _{k\mid k-1}&=\mathbf {F} _{k}\mathbf {P} _{k-1\mid k-1}\mathbf {F} _{k}^{\textsf {T}}+\mathbf {Q} _{k},\\
\mathbf {S} _{k}&=\mathbf {H} _{k}\mathbf {P} _{k\mid k-1}\mathbf {H} _{k}^{\textsf {T}}+\mathbf {R} _{k},\\
\mathbf {K} _{k}&=\mathbf {P} _{k\mid k-1}\mathbf {H} _{k}^{\textsf {T}}\mathbf {S} _{k}^{-1},\\
\mathbf {P} _{k|k}&=\left(\mathbf {I} -\mathbf {K} _{k}\mathbf {H} _{k}\right)\mathbf {P} _{k|k-1}.
\end{aligned}","['linear-algebra', 'numerical-methods', 'numerical-linear-algebra', 'control-theory', 'kalman-filter']"
55,Does this type of tensor appear anywhere?,Does this type of tensor appear anywhere?,,"Antisymmetric, or skew-symmetric tensors on a subset of indices are those that get multiplied by $-1$ when any of the indices from the subset are transposed. This type of tensor is widely used in physics and mathematics. Now imagine I have, say, a tensor of type $(3, 0)$ that gets multiplied by $e^{2\pi i\over3}$ when I apply a cyclic permutation to the indices. More generally, let $h$ be an element of order $k$ in the multiplicative group of the underlying field, and say there is a tensor that gets multiplied by $h$ when a certain permutation of order $k$ is applied to its indices. This property does not seem to depend on the choice of basis. Do such tensors appear anywhere in physics or mathematics? I'm sorry if this question isn't well-motivated, I'm just curious.","Antisymmetric, or skew-symmetric tensors on a subset of indices are those that get multiplied by when any of the indices from the subset are transposed. This type of tensor is widely used in physics and mathematics. Now imagine I have, say, a tensor of type that gets multiplied by when I apply a cyclic permutation to the indices. More generally, let be an element of order in the multiplicative group of the underlying field, and say there is a tensor that gets multiplied by when a certain permutation of order is applied to its indices. This property does not seem to depend on the choice of basis. Do such tensors appear anywhere in physics or mathematics? I'm sorry if this question isn't well-motivated, I'm just curious.","-1 (3, 0) e^{2\pi i\over3} h k h k","['linear-algebra', 'representation-theory', 'tensors']"
56,Linear transformation of a function vector space,Linear transformation of a function vector space,,"I'm having a bit of trouble solving a homework problem. I've given the specific problem below, but general answers are 100% welcome: Consider the following three bases for a vector space 𝑉, which is a subspace of $C^\infty(-\infty,\infty):$ $$B_1 = \{ 1, e^x, e^{2x}\}$$ $$B_2 = \{ 1, 1+e^{2x}, e^x -e^{2x}\}$$ $$B_3 = \{ 1+e^x, 1+e^{2x}, 1+e^x+e^{2x}\}$$ Obtain the following transition matrices: $$P_{B_1 \to B_2}, P_{B_2 \to B_1}, P_{B_2 \to B_3}$$ I've read about change of basis and done problems for questions like this in finite vector spaces ( $\mathbb{R}^n$ and $M_{nm}$ , etc). But the way we're given to solve equations involves utilizing the inverse of the basis, which I'm not certain on calculating. At first observation, I can see that I could pull out $\vec{x} = \{1,e^x,e^{2x}\}$ in order to at least simplify what I'm looking at: $$B_1 = A_1\vec{x} = \begin{bmatrix} 1&0&0\\0&1&0\\0&0&1 \end{bmatrix} \begin{bmatrix} 1\\e^x\\e^{2x} \end{bmatrix}$$ $$B_2 = A_2\vec{x} = \begin{bmatrix} 1&0&0\\1&0&1\\0&1&-1 \end{bmatrix} \begin{bmatrix} 1\\e^x\\e^{2x} \end{bmatrix}$$ $$B_3 = A_3\vec{x} = \begin{bmatrix} 1&1&0\\1&0&1\\1&1&1 \end{bmatrix} \begin{bmatrix} 1\\e^x\\e^{2x} \end{bmatrix}$$ This allows me to view each basis at least in the sense of something which isn't one dimensional. I know from my text that a transition vector can be expressed as $P_{A \to B} = [B]^{-1}[A]$ (where A and B are bases), as well as: $$AA^{-1} = I\\ IA = A\\ (AB)^{-1} = B^{-1}A^{-1}\\ [B] = [\vec{u}_1| \vec{u}_2| ...| \vec{u}_n]\\ A\vec{x}=\vec{b}\\ \vec{x}=A^{-1}\vec{b}$$ With the final two holding true if A is invertible. I've expressed all the bases in the form $\vec{b} = A\vec{x}$ , so following the above logic (since every A from our bases is invertible), I believe the following should hold true: $$ P_{B_1 \to B_2} = B_2^{-1}B_1\\ ... = (A_2\vec{x})^{-1}(A_1\vec{x})\\ ... = (\vec{x})^{-1}A_2^{-1}A_1\vec{x}\\ \vec{x}P_{B_1 \to B_2} = A_2^{-1}A_1\vec{x}$$ However, I'm having trouble getting rid of or simplifying that $\vec{x}$ around. I'm also not certain if it's even appropriate to denote a non-square $\vec{x}^{-1}$ . But I feel like there must be a way to simplify $(\vec{x})^{-1}M\vec{x} \to M$ . Failing that, I also tried: $$rank(B_{1...3}) = 3\\ \therefore dim(V) = 3\\ \therefore \forall \vec{v} \in V, \vec{v} \in \mathbb{R}^3$$ So by this, I believe we can determine that while $C^\infty$ is an infinite-dimensional vector space, $V$ is actually a 3-dimensional subspace described by our bases. But I suppose this also leaves me a littlle confused, as it would mean e.g. $1 + e^x$ (from $B_{3,1}$ ) is actually a third-dimensional vector, which is what lead me to breaking down the bases given based on the variable inputs over addition. In short: Is my approach flawed or incorrect? If so, how? I'm a bit short on terminology since I'm learning, is there a name for what I did in that first step? Most importantly: How do I generate the transition vector for this subspace?","I'm having a bit of trouble solving a homework problem. I've given the specific problem below, but general answers are 100% welcome: Consider the following three bases for a vector space 𝑉, which is a subspace of Obtain the following transition matrices: I've read about change of basis and done problems for questions like this in finite vector spaces ( and , etc). But the way we're given to solve equations involves utilizing the inverse of the basis, which I'm not certain on calculating. At first observation, I can see that I could pull out in order to at least simplify what I'm looking at: This allows me to view each basis at least in the sense of something which isn't one dimensional. I know from my text that a transition vector can be expressed as (where A and B are bases), as well as: With the final two holding true if A is invertible. I've expressed all the bases in the form , so following the above logic (since every A from our bases is invertible), I believe the following should hold true: However, I'm having trouble getting rid of or simplifying that around. I'm also not certain if it's even appropriate to denote a non-square . But I feel like there must be a way to simplify . Failing that, I also tried: So by this, I believe we can determine that while is an infinite-dimensional vector space, is actually a 3-dimensional subspace described by our bases. But I suppose this also leaves me a littlle confused, as it would mean e.g. (from ) is actually a third-dimensional vector, which is what lead me to breaking down the bases given based on the variable inputs over addition. In short: Is my approach flawed or incorrect? If so, how? I'm a bit short on terminology since I'm learning, is there a name for what I did in that first step? Most importantly: How do I generate the transition vector for this subspace?","C^\infty(-\infty,\infty): B_1 = \{ 1, e^x, e^{2x}\} B_2 = \{ 1, 1+e^{2x}, e^x -e^{2x}\} B_3 = \{ 1+e^x, 1+e^{2x}, 1+e^x+e^{2x}\} P_{B_1 \to B_2}, P_{B_2 \to B_1}, P_{B_2 \to B_3} \mathbb{R}^n M_{nm} \vec{x} = \{1,e^x,e^{2x}\} B_1 = A_1\vec{x} = \begin{bmatrix} 1&0&0\\0&1&0\\0&0&1 \end{bmatrix} \begin{bmatrix} 1\\e^x\\e^{2x} \end{bmatrix} B_2 = A_2\vec{x} = \begin{bmatrix} 1&0&0\\1&0&1\\0&1&-1 \end{bmatrix} \begin{bmatrix} 1\\e^x\\e^{2x} \end{bmatrix} B_3 = A_3\vec{x} = \begin{bmatrix} 1&1&0\\1&0&1\\1&1&1 \end{bmatrix} \begin{bmatrix} 1\\e^x\\e^{2x} \end{bmatrix} P_{A \to B} = [B]^{-1}[A] AA^{-1} = I\\
IA = A\\
(AB)^{-1} = B^{-1}A^{-1}\\
[B] = [\vec{u}_1| \vec{u}_2| ...| \vec{u}_n]\\
A\vec{x}=\vec{b}\\
\vec{x}=A^{-1}\vec{b} \vec{b} = A\vec{x} 
P_{B_1 \to B_2} = B_2^{-1}B_1\\
... = (A_2\vec{x})^{-1}(A_1\vec{x})\\
... = (\vec{x})^{-1}A_2^{-1}A_1\vec{x}\\
\vec{x}P_{B_1 \to B_2} = A_2^{-1}A_1\vec{x} \vec{x} \vec{x}^{-1} (\vec{x})^{-1}M\vec{x} \to M rank(B_{1...3}) = 3\\
\therefore dim(V) = 3\\
\therefore \forall \vec{v} \in V, \vec{v} \in \mathbb{R}^3 C^\infty V 1 + e^x B_{3,1}","['linear-algebra', 'matrices', 'change-of-basis']"
57,Writing $\int_\Omega \nabla u^T M \nabla v$ in terms of $H^1$ inner product of $u$ with another function,Writing  in terms of  inner product of  with another function,\int_\Omega \nabla u^T M \nabla v H^1 u,"Let $\Omega \subset \mathbb{R}^n$ be a smooth domain, $u,v \in H^1_0(\Omega)$ with the usual inner product and let $M=M(x)$ be a $n\times n$ matrix with entries $m_{ij}\colon \Omega \to \mathbb{R}$ which are as smooth as necessary. Furthermore, $M(x)$ is positive-definite and invertible for every $x$ . Is it true that there exist $\varphi, \phi \in H^1_0(\Omega)$ such that $$\int_\Omega \nabla u^T M \nabla v = \int_\Omega \nabla u^T \nabla \varphi + \int_\Omega u\phi?$$ If so how we can relate $\varphi$ and $\phi$ with $M$ and $v$ ? Outside of when $M$ is a multiple of the identity, I don't know.","Let be a smooth domain, with the usual inner product and let be a matrix with entries which are as smooth as necessary. Furthermore, is positive-definite and invertible for every . Is it true that there exist such that If so how we can relate and with and ? Outside of when is a multiple of the identity, I don't know.","\Omega \subset \mathbb{R}^n u,v \in H^1_0(\Omega) M=M(x) n\times n m_{ij}\colon \Omega \to \mathbb{R} M(x) x \varphi, \phi \in H^1_0(\Omega) \int_\Omega \nabla u^T M \nabla v = \int_\Omega \nabla u^T \nabla \varphi + \int_\Omega u\phi? \varphi \phi M v M","['linear-algebra', 'matrices', 'functional-analysis', 'sobolev-spaces']"
58,"If $AB=A$, does B have to be the identity matrix?","If , does B have to be the identity matrix?",AB=A,"Suppose $A$ and $B$ are square matrices and that $AB=A$ with $B \neq I$ . What does this say about the invertibility of $A$ ? This question showed up on an exam I took this past spring. I got stuck on it, but I thought about it for a while and think I figured it out. Here's something similar to what I got: Suppose $A$ is invertible. Then: $$\begin{align} AB &= A \\ A^{-1}AB &= A^{-1}A \\ IB &= I \\ B &= I \end{align}$$ This shows that if $AB=A$ , then $B$ must be an identity matrix if $A$ is invertible. Conclusion: If $AB=A$ and $B \neq I$ , then $A$ must be singular. An obvious example would be making $A$ a zero matrix. Is what I've got correct?","Suppose and are square matrices and that with . What does this say about the invertibility of ? This question showed up on an exam I took this past spring. I got stuck on it, but I thought about it for a while and think I figured it out. Here's something similar to what I got: Suppose is invertible. Then: This shows that if , then must be an identity matrix if is invertible. Conclusion: If and , then must be singular. An obvious example would be making a zero matrix. Is what I've got correct?","A B AB=A B \neq I A A \begin{align}
AB &= A \\
A^{-1}AB &= A^{-1}A \\
IB &= I \\
B &= I
\end{align} AB=A B A AB=A B \neq I A A",['linear-algebra']
59,"For $A\in\mathbb{R}^{n\times n}$ and $B\in\mathbb{R}^{n\times k}$, does $(I_{n}-aA)^{-1}Bb$ determine $a\in\mathbb{R}$ and $b\in\mathbb{R}^k$?","For  and , does  determine  and ?",A\in\mathbb{R}^{n\times n} B\in\mathbb{R}^{n\times k} (I_{n}-aA)^{-1}Bb a\in\mathbb{R} b\in\mathbb{R}^k,"Let $A$ be a real $n\times n$ matrix, and $B$ be a real $n\times k$ matrix of rank $k<n$, with $\mathrm{col}(B)$ (the column space of $B$) not an invariant subspace of $A$. We assume $A\neq0,I_n$. Consider $$f(a,b)=(I_{n}-aA)^{-1}Bb,$$ for real scalar $a\in S$ and $b\in\mathbb{R}^k$. $S$ is the set of values of $a$ such that $(I_{n}-aA)$ is invertible. We say that $f(a,b)$ identifies $a$ and $b$ over $\Omega$ if $f(a,b)=f(a^{\ast },b^{\ast})$ implies $(a,b)=(a^{\ast},b^{\ast})$ for any two pairs $(a,b),(a^{\ast},b^{\ast})\in\Omega$. The problem is to describe the set over which $f(a,b)$ does not identify $a$ and $b$. What I have done so far: Rewrite $f(a,b)=f(a^{\ast},b^{\ast})$ as $$ B(b-b^{\ast})+AB(ab^{\ast}-a^{\ast}b)=0.\tag{1}\label{1} $$ Thus, $f(a,b)$ identifies $a$ and $b$ over $\Omega$ if $\eqref{1}$ implies $(a,b)=(a^{\ast},b^{\ast})$ for all $(a,b),(a^{\ast},b^{\ast})\in\Omega$. So far I understand only two particular cases: If $\mathrm{rank}(B,AB)=2k$, then $\eqref{1}$ is satisfied iff $b-b^{\ast}=0$ and $ab^{\ast}-a^{\ast}b=0$, from which $(a,b)=(a^{\ast},b^{\ast})$ provided that $b\neq0$. Hence, in this case $f(a,b)$ identifies $a$ and $b$ over $S\times\mathbb{R}^{k}/\{0\}$. Partition $B$ as $(B_{1},B_{2})$ where $B_{1}$ is $n\times k_{1}$ and $B_{2}$ is $n\times k_{2}$, with $0<k_{1}<k$, and suppose that $AB_{1}=B_{1}L$ for some $k_{1}\times k_{1}$ matrix $L$ ($\mathrm{col}(B_{1})$ is an invariant subspace of $A$), and that $\mathrm{rank}(B,AB)=k+k_{2}.$ Then, letting $\left(  b_{1}^{\prime},b_{2}^{\prime}\right)  $ be the partition of $b^{\prime}$ conformable with that of $B,$ $\eqref{1}$ becomes $$ B_{1}(b_{1}-b_{1}^{\ast}+L(ab_{1}^{\ast}-a^{\ast}b_{1}))+B_{2}(b_{2}% -b_{2}^{\ast})+AB_{2}(ab_{2}^{\ast}-a^{\ast}b_{2})=0. $$ Since the columns of $(B_{1},B_{2},AB_{2})$ are linearly independent, $\eqref{1}$ is satisfied if and only if $b_{1}-b_{1}^{\ast}+L(ab_{1}^{\ast }-a^{\ast}b_{1})=0$, $b_{2}-b_{2}^{\ast}=0$, and $ab_{2}^{\ast}-a^{\ast}% b_{2}=0$. Combining the second and third equalities, gives $b_{2}=b_{2}^{\ast }$ and $a=a^{\ast}$, provided that $b_{2}\neq0.$ Hence the first equality becomes $\left(  I_{k_{1}}-aL\right)  \left(  b_{1}-b_{1}^{\ast}\right)  =0$, which is equivalent to $b_{1}-b_{1}^{\ast}$ because $I_{k_{1}}-aL$ is invertible for any $a\in S$. This means that $f(a,b)$ identifies $(a,b)$ provided that not all entries of $b$ associated to columns of $B$ not in $\mathrm{col}(A)$ are zero (if they were we would essentially be in the case when $\mathrm{col}(B)$ is an invariant subspace of $A$--in that case it is clear that $\left(  a,b\right)  $ cannot be identified from $f(a,b)$) Update: Let's look at the case $k=2.$ Write $B=(B_{1},B_{2})$, so $B_1$ and $B_2$ are vectors. Assume $\mathrm{rank}(B,AB)=\mathrm{rank}(B,AB_{2})=k+1$ (the case $\mathrm{rank}(B,AB)=2k$ is trivial, see case 1 above, and so is the case $\mathrm{rank}(B,AB)=k$), and write $$AB_{1}% =l_{1}B_{1}+l_{2}B_{2}+l_{3}AB_{2},$$ for $l_{1},l_{2},l_{3}\in\mathbb{R}$. From $\eqref{1}$, \begin{equation} \left[  \beta_{1}-\beta_{1}^{\ast}+l_{1}(a\beta_{1}^{\ast}-a^{\ast}\beta _{1})\right]  B_{1}+\left[  \beta_{2}-\beta_{2}^{\ast}+l_{2}(a\beta_{1}^{\ast }-a^{\ast}\beta_{1})\right]  B_{2}+\left[  a\beta_{2}^{\ast}-a^{\ast}\beta _{2}+l_{3}(a\beta_{1}^{\ast}-a^{\ast}\beta_{1})\right]  AB_{2}=0. \end{equation} Since $\mathrm{rank}(B,AB_{2})=k+1,$ this equality is satisfied iff $$\left\{ \begin{array} [c]{c}% b_{1}-b_{1}^{\ast}+l_{1}(ab_{1}^{\ast}-a^{\ast}b_{1})=0\\ b_{2}-b_{2}^{\ast}+l_{2}(ab_{1}^{\ast}-a^{\ast}b_{1})=0\\ ab_{2}^{\ast}-a^{\ast}b_{2}+l_{3}(ab_{1}^{\ast}-a^{\ast}b_{1})=0 \end{array} \right.$$ As a linear system in the unknowns $b_{1}^{\ast},b_{2}^{\ast},a^{\ast}$, this is $$C\left[ \begin{array} [c]{c}% b_{1}^{\ast}\\ b_{2}^{\ast}\\ a^{\ast}% \end{array} \right]  =\left[ \begin{array} [c]{c}% b_{1}\\ b_{2}\\ 0 \end{array} \right],\tag{1}\label{2}$$ where $$C=\left[ \begin{array} [c]{ccc}% 1-l_{1}a & 0 & l_{1}b_{1}\\ -l_{2}a & 1 & l_{2}b_{1}\\ -l_{3}a & a & l_{3}b_{1}+b_{2}% \end{array} \right]  .$$ If $\mathrm{rank}(C)=3$, the only solution to the system is $\left(  b^{\ast },a^{\ast}\right)  =\left(  b,a\right)  $. If $\mathrm{rank}(C)<3$, there may be other solutions. Note that $$ \det(C)=\left(  l_{1}a-1\right)  b_{2}-\left(  l_{2}a+l_{3}\right)  b_{1}. $$ Note that for fixed $a$, the set of $(b_1,b_2)$ such that $\det(C)=0$ is a line in $\mathbb{R}^2$. Incidentally note that case 2. above (with $k=2$) obtains when $l_{2}=l_{3}=0.$ In that case, $\det(C)=\left(  l_{1}a-1\right)  b_{2}$, so $\det(C)=0$ only if $b_{2}=0$ ($l_{1}a-1=0$ is impossible as $l_{1}$ must be an eigenvalue of $A$, so $l_{1}a-1=0$ would contradict invertibility of $(I_n-aA)$). That is, in that case, we find again the result that $b,a$ is identifiable provided that $b_{2}\neq0$.","Let $A$ be a real $n\times n$ matrix, and $B$ be a real $n\times k$ matrix of rank $k<n$, with $\mathrm{col}(B)$ (the column space of $B$) not an invariant subspace of $A$. We assume $A\neq0,I_n$. Consider $$f(a,b)=(I_{n}-aA)^{-1}Bb,$$ for real scalar $a\in S$ and $b\in\mathbb{R}^k$. $S$ is the set of values of $a$ such that $(I_{n}-aA)$ is invertible. We say that $f(a,b)$ identifies $a$ and $b$ over $\Omega$ if $f(a,b)=f(a^{\ast },b^{\ast})$ implies $(a,b)=(a^{\ast},b^{\ast})$ for any two pairs $(a,b),(a^{\ast},b^{\ast})\in\Omega$. The problem is to describe the set over which $f(a,b)$ does not identify $a$ and $b$. What I have done so far: Rewrite $f(a,b)=f(a^{\ast},b^{\ast})$ as $$ B(b-b^{\ast})+AB(ab^{\ast}-a^{\ast}b)=0.\tag{1}\label{1} $$ Thus, $f(a,b)$ identifies $a$ and $b$ over $\Omega$ if $\eqref{1}$ implies $(a,b)=(a^{\ast},b^{\ast})$ for all $(a,b),(a^{\ast},b^{\ast})\in\Omega$. So far I understand only two particular cases: If $\mathrm{rank}(B,AB)=2k$, then $\eqref{1}$ is satisfied iff $b-b^{\ast}=0$ and $ab^{\ast}-a^{\ast}b=0$, from which $(a,b)=(a^{\ast},b^{\ast})$ provided that $b\neq0$. Hence, in this case $f(a,b)$ identifies $a$ and $b$ over $S\times\mathbb{R}^{k}/\{0\}$. Partition $B$ as $(B_{1},B_{2})$ where $B_{1}$ is $n\times k_{1}$ and $B_{2}$ is $n\times k_{2}$, with $0<k_{1}<k$, and suppose that $AB_{1}=B_{1}L$ for some $k_{1}\times k_{1}$ matrix $L$ ($\mathrm{col}(B_{1})$ is an invariant subspace of $A$), and that $\mathrm{rank}(B,AB)=k+k_{2}.$ Then, letting $\left(  b_{1}^{\prime},b_{2}^{\prime}\right)  $ be the partition of $b^{\prime}$ conformable with that of $B,$ $\eqref{1}$ becomes $$ B_{1}(b_{1}-b_{1}^{\ast}+L(ab_{1}^{\ast}-a^{\ast}b_{1}))+B_{2}(b_{2}% -b_{2}^{\ast})+AB_{2}(ab_{2}^{\ast}-a^{\ast}b_{2})=0. $$ Since the columns of $(B_{1},B_{2},AB_{2})$ are linearly independent, $\eqref{1}$ is satisfied if and only if $b_{1}-b_{1}^{\ast}+L(ab_{1}^{\ast }-a^{\ast}b_{1})=0$, $b_{2}-b_{2}^{\ast}=0$, and $ab_{2}^{\ast}-a^{\ast}% b_{2}=0$. Combining the second and third equalities, gives $b_{2}=b_{2}^{\ast }$ and $a=a^{\ast}$, provided that $b_{2}\neq0.$ Hence the first equality becomes $\left(  I_{k_{1}}-aL\right)  \left(  b_{1}-b_{1}^{\ast}\right)  =0$, which is equivalent to $b_{1}-b_{1}^{\ast}$ because $I_{k_{1}}-aL$ is invertible for any $a\in S$. This means that $f(a,b)$ identifies $(a,b)$ provided that not all entries of $b$ associated to columns of $B$ not in $\mathrm{col}(A)$ are zero (if they were we would essentially be in the case when $\mathrm{col}(B)$ is an invariant subspace of $A$--in that case it is clear that $\left(  a,b\right)  $ cannot be identified from $f(a,b)$) Update: Let's look at the case $k=2.$ Write $B=(B_{1},B_{2})$, so $B_1$ and $B_2$ are vectors. Assume $\mathrm{rank}(B,AB)=\mathrm{rank}(B,AB_{2})=k+1$ (the case $\mathrm{rank}(B,AB)=2k$ is trivial, see case 1 above, and so is the case $\mathrm{rank}(B,AB)=k$), and write $$AB_{1}% =l_{1}B_{1}+l_{2}B_{2}+l_{3}AB_{2},$$ for $l_{1},l_{2},l_{3}\in\mathbb{R}$. From $\eqref{1}$, \begin{equation} \left[  \beta_{1}-\beta_{1}^{\ast}+l_{1}(a\beta_{1}^{\ast}-a^{\ast}\beta _{1})\right]  B_{1}+\left[  \beta_{2}-\beta_{2}^{\ast}+l_{2}(a\beta_{1}^{\ast }-a^{\ast}\beta_{1})\right]  B_{2}+\left[  a\beta_{2}^{\ast}-a^{\ast}\beta _{2}+l_{3}(a\beta_{1}^{\ast}-a^{\ast}\beta_{1})\right]  AB_{2}=0. \end{equation} Since $\mathrm{rank}(B,AB_{2})=k+1,$ this equality is satisfied iff $$\left\{ \begin{array} [c]{c}% b_{1}-b_{1}^{\ast}+l_{1}(ab_{1}^{\ast}-a^{\ast}b_{1})=0\\ b_{2}-b_{2}^{\ast}+l_{2}(ab_{1}^{\ast}-a^{\ast}b_{1})=0\\ ab_{2}^{\ast}-a^{\ast}b_{2}+l_{3}(ab_{1}^{\ast}-a^{\ast}b_{1})=0 \end{array} \right.$$ As a linear system in the unknowns $b_{1}^{\ast},b_{2}^{\ast},a^{\ast}$, this is $$C\left[ \begin{array} [c]{c}% b_{1}^{\ast}\\ b_{2}^{\ast}\\ a^{\ast}% \end{array} \right]  =\left[ \begin{array} [c]{c}% b_{1}\\ b_{2}\\ 0 \end{array} \right],\tag{1}\label{2}$$ where $$C=\left[ \begin{array} [c]{ccc}% 1-l_{1}a & 0 & l_{1}b_{1}\\ -l_{2}a & 1 & l_{2}b_{1}\\ -l_{3}a & a & l_{3}b_{1}+b_{2}% \end{array} \right]  .$$ If $\mathrm{rank}(C)=3$, the only solution to the system is $\left(  b^{\ast },a^{\ast}\right)  =\left(  b,a\right)  $. If $\mathrm{rank}(C)<3$, there may be other solutions. Note that $$ \det(C)=\left(  l_{1}a-1\right)  b_{2}-\left(  l_{2}a+l_{3}\right)  b_{1}. $$ Note that for fixed $a$, the set of $(b_1,b_2)$ such that $\det(C)=0$ is a line in $\mathbb{R}^2$. Incidentally note that case 2. above (with $k=2$) obtains when $l_{2}=l_{3}=0.$ In that case, $\det(C)=\left(  l_{1}a-1\right)  b_{2}$, so $\det(C)=0$ only if $b_{2}=0$ ($l_{1}a-1=0$ is impossible as $l_{1}$ must be an eigenvalue of $A$, so $l_{1}a-1=0$ would contradict invertibility of $(I_n-aA)$). That is, in that case, we find again the result that $b,a$ is identifiable provided that $b_{2}\neq0$.",,"['linear-algebra', 'matrices', 'invariant-subspace']"
60,Vandermonde matrices nullspaces and notation,Vandermonde matrices nullspaces and notation,,"I'm following the book Mimetic Discretization Methods by Castillo and Miranda, reading the page 209-211, it states the following: Let $V(m;G)$ be a Vandermonde matrix , of order $m$ with generator $G = \{ g_1, g_2, \ldots, g_n \}$. Where the order implies the highest power of Vandermonde matrix, therefore $V(m;G)$ has $(m+1)$ rows and $n$ columns. Let  \begin{align} G_1 &= \{ -1/2, 1/2, 3/2, 5/2, 7/2, 9/2 \} \\ G_2 &= \{ -3/2, -1/2, 1/2, 3/2, 5/2, 7/2 \} \\ G_3 &= \{ -5/2, -3/2, -1/2, 1/2, 3/2, 5/2 \} \\ G_4 &= \{ -7/2, -5/2, -3/2, -1/2, 1/2, 3/2 \} \end{align} Generators of the following matrices \begin{align} V_1 &=V(4;G_1) \\ V_2 &=V(4;G_2) \\ V_3 &= V(4;G_3) \\ V_4 &= V(4;G_4) \end{align} Just for sake of example $V_1$ looks like: \begin{align} V_1 = \begin{bmatrix} 1 & 1 & 1 & 1 & 1 & 1 \\ -1/2 & 1/2 & 3/2 & 5/2 & 7/2 & 9/2 \\ (-1/2)^2 & (1/2)^2 & (3/2)^2 & (5/2)^2 & (7/2)^2 & (9/2)^2 \\ (-1/2)^3 & (1/2)^3 & (3/2)^3 & (5/2)^3 & (7/2)^3 & (9/2)^3 \\ (-1/2)^4 & (1/2)^4 & (3/2)^4 & (5/2)^4 & (7/2)^4 & (9/2)^4 \\ \end{bmatrix} \end{align} The null space of this matrix is given by $Ker(V_1) =[-1 \; 5 \; -10\; 10\; -5\; 1]^T$, that is easily verifiable just by $V_1 \cdot Ker(V_1)$ that results in the null vector, as expected. The other Vandermonde matrices $V_2, V_3, V_4$ share the same null space. The book then states: In general, the null space of a $k$-th order Vandermonde matrix has dimension $\frac{1}{2}k -1$. My first question : There is no references or proofs to support this claim. Is this true? How can I prove that? What are conditions to that be true. I could not find anywhere this affirmation. The text continues, and we need to solve the following system \begin{equation} V_i a_i^T = b \end{equation} Where $V_i$ are the Vandermonde matrices above, $a_i$ is a unknown vector and $b = [ 0 \;1 \;0 \;0\; 0 ]$ Lets only focus in $i=1$ since the construction to other matrices are equivalent. We then need to solve the following: \begin{equation} V_1 a_1^T = b \end{equation} completely written as \begin{equation} \begin{bmatrix} 1 & 1 & 1 & 1 & 1 & 1 \\ -1/2 & 1/2 & 3/2 & 5/2 & 7/2 & 9/2 \\ (-1/2)^2 & (1/2)^2 & (3/2)^2 & (5/2)^2 & (7/2)^2 & (9/2)^2 \\ (-1/2)^3 & (1/2)^3 & (3/2)^3 & (5/2)^3 & (7/2)^3 & (9/2)^3 \\ (-1/2)^4 & (1/2)^4 & (3/2)^4 & (5/2)^4 & (7/2)^4 & (9/2)^4 \\ \end{bmatrix}\begin{bmatrix} a_{11} \\a_{12} \\a_{13} \\a_{14} \\a_{15} \\a_{16} \end{bmatrix} = \begin{bmatrix} 0 \\ 1 \\ 0 \\ 0 \\ 0 \end{bmatrix} \end{equation} Then the text declares that \begin{align} a_1 = \left[ -\frac{11}{12} \, \frac{17}{24} \, \frac{3}{8} \, -\frac{5}{24} \,  \frac{1}{24} \, 0 \right]  + \alpha_1 Ker(V_1) \end{align} for some $\alpha_1 \in \mathbb{R}$. My second question is: where that vector in first term came from? Is that true? How can I show that? I'm able to solve that system using least squares, but could not find a correlation. Some tries: (EDIT) Definition ( Null Space ): $Ker(V)$ is a null space of $V$ if an only if $V \cdot \alpha Ker(V) = \vec{0}$ for any $\alpha \in \mathbb{R}$. Lets say by hypothesys that \begin{align} a &= x + \alpha Ker(V) \end{align} Since we are looking for (I will drop the indexes for sake of notation and typing): \begin{align} V \cdot a &= b \\ a &= V^{-1} b \end{align} by the hypothesis \begin{align} x + \alpha Ker(V) &= V^{-1} b \\ Vx + V \left( \alpha Ker(V) \right) &= b \end{align} by Null Space definition \begin{align} Vx &= b \end{align} We are just back where we started, seems a tautology in my view. Anyway I also know that \begin{align} a &= x + \alpha Ker(V) \\ x &= a - \alpha Ker(V) \end{align} I can find $a$ by least squares, and have $Ker(V)$ by the null space analysis. Therefore  I just need to find $\alpha$ to uniquely identify $x$.","I'm following the book Mimetic Discretization Methods by Castillo and Miranda, reading the page 209-211, it states the following: Let $V(m;G)$ be a Vandermonde matrix , of order $m$ with generator $G = \{ g_1, g_2, \ldots, g_n \}$. Where the order implies the highest power of Vandermonde matrix, therefore $V(m;G)$ has $(m+1)$ rows and $n$ columns. Let  \begin{align} G_1 &= \{ -1/2, 1/2, 3/2, 5/2, 7/2, 9/2 \} \\ G_2 &= \{ -3/2, -1/2, 1/2, 3/2, 5/2, 7/2 \} \\ G_3 &= \{ -5/2, -3/2, -1/2, 1/2, 3/2, 5/2 \} \\ G_4 &= \{ -7/2, -5/2, -3/2, -1/2, 1/2, 3/2 \} \end{align} Generators of the following matrices \begin{align} V_1 &=V(4;G_1) \\ V_2 &=V(4;G_2) \\ V_3 &= V(4;G_3) \\ V_4 &= V(4;G_4) \end{align} Just for sake of example $V_1$ looks like: \begin{align} V_1 = \begin{bmatrix} 1 & 1 & 1 & 1 & 1 & 1 \\ -1/2 & 1/2 & 3/2 & 5/2 & 7/2 & 9/2 \\ (-1/2)^2 & (1/2)^2 & (3/2)^2 & (5/2)^2 & (7/2)^2 & (9/2)^2 \\ (-1/2)^3 & (1/2)^3 & (3/2)^3 & (5/2)^3 & (7/2)^3 & (9/2)^3 \\ (-1/2)^4 & (1/2)^4 & (3/2)^4 & (5/2)^4 & (7/2)^4 & (9/2)^4 \\ \end{bmatrix} \end{align} The null space of this matrix is given by $Ker(V_1) =[-1 \; 5 \; -10\; 10\; -5\; 1]^T$, that is easily verifiable just by $V_1 \cdot Ker(V_1)$ that results in the null vector, as expected. The other Vandermonde matrices $V_2, V_3, V_4$ share the same null space. The book then states: In general, the null space of a $k$-th order Vandermonde matrix has dimension $\frac{1}{2}k -1$. My first question : There is no references or proofs to support this claim. Is this true? How can I prove that? What are conditions to that be true. I could not find anywhere this affirmation. The text continues, and we need to solve the following system \begin{equation} V_i a_i^T = b \end{equation} Where $V_i$ are the Vandermonde matrices above, $a_i$ is a unknown vector and $b = [ 0 \;1 \;0 \;0\; 0 ]$ Lets only focus in $i=1$ since the construction to other matrices are equivalent. We then need to solve the following: \begin{equation} V_1 a_1^T = b \end{equation} completely written as \begin{equation} \begin{bmatrix} 1 & 1 & 1 & 1 & 1 & 1 \\ -1/2 & 1/2 & 3/2 & 5/2 & 7/2 & 9/2 \\ (-1/2)^2 & (1/2)^2 & (3/2)^2 & (5/2)^2 & (7/2)^2 & (9/2)^2 \\ (-1/2)^3 & (1/2)^3 & (3/2)^3 & (5/2)^3 & (7/2)^3 & (9/2)^3 \\ (-1/2)^4 & (1/2)^4 & (3/2)^4 & (5/2)^4 & (7/2)^4 & (9/2)^4 \\ \end{bmatrix}\begin{bmatrix} a_{11} \\a_{12} \\a_{13} \\a_{14} \\a_{15} \\a_{16} \end{bmatrix} = \begin{bmatrix} 0 \\ 1 \\ 0 \\ 0 \\ 0 \end{bmatrix} \end{equation} Then the text declares that \begin{align} a_1 = \left[ -\frac{11}{12} \, \frac{17}{24} \, \frac{3}{8} \, -\frac{5}{24} \,  \frac{1}{24} \, 0 \right]  + \alpha_1 Ker(V_1) \end{align} for some $\alpha_1 \in \mathbb{R}$. My second question is: where that vector in first term came from? Is that true? How can I show that? I'm able to solve that system using least squares, but could not find a correlation. Some tries: (EDIT) Definition ( Null Space ): $Ker(V)$ is a null space of $V$ if an only if $V \cdot \alpha Ker(V) = \vec{0}$ for any $\alpha \in \mathbb{R}$. Lets say by hypothesys that \begin{align} a &= x + \alpha Ker(V) \end{align} Since we are looking for (I will drop the indexes for sake of notation and typing): \begin{align} V \cdot a &= b \\ a &= V^{-1} b \end{align} by the hypothesis \begin{align} x + \alpha Ker(V) &= V^{-1} b \\ Vx + V \left( \alpha Ker(V) \right) &= b \end{align} by Null Space definition \begin{align} Vx &= b \end{align} We are just back where we started, seems a tautology in my view. Anyway I also know that \begin{align} a &= x + \alpha Ker(V) \\ x &= a - \alpha Ker(V) \end{align} I can find $a$ by least squares, and have $Ker(V)$ by the null space analysis. Therefore  I just need to find $\alpha$ to uniquely identify $x$.",,"['linear-algebra', 'numerical-linear-algebra']"
61,Trace cauchy schwarz inequality,Trace cauchy schwarz inequality,,"This was an interesting result I found while reading Holevo's Quantum Information theory book. They call it the non-commutative operator Cauchy Schwarz. Let $S$ be a state i.e. a non negative operator with unit trace on a Hilbert space $\mathcal{H}$ in $\mathbb{C}$. Then for arbitrary operators $X$ and $Y$ on $\mathcal{H}$,  \begin{equation}  |Tr(SX^*Y)|^2 \leq Tr(SX^*X)Tr(SY^*Y)\label{Thi} \end{equation} I was able to show this by mirroring the usual Cauchy Schwarz inequality (I assumed RHS is finite). Then a friend suggested that it might follow easily if we show that $Tr(SX^*Y)$ is an inner product on the operator space. I was able to show that it is a semi inner product. To prove it is an inner product, I needed to show  $$Tr(SX^*X) = 0 \Rightarrow X=0$$ I wasn't successful with this one. I was able to show it if $S$ is a strictly positive operator (at least I think I did) but not for non negative. My guess is that in general, $Tr(SX^*Y)$ is a seminorm but not necessarily a norm. But I was unable to furnish an $X$ such that $X\ne 0$ but $Tr(SX^*X) = 0$. I seek such an example. Kindly give me some ideas. We could think in terms of matrices here. Update: Upon further inspection, I realized that my ""proof that mirrors cauchy schwarz"" was in fact flawed. I assumed implicitly that both terms in RHS were $>0$. In one of the steps, I divide by the square root of RHS, which would be illegal if it were $0$. This wasn't a problem in usual Cauchy schwarz because we were given that LHS is square of mod of an inner product. This changes my perception and I now believe that the above is an inner product but I need to show the final step. Update: I'll show my work so far. Let $\langle X,Y\rangle_T \triangleq Tr(SX^*Y)$. Then Linearity in second argument (In Quantum theory, it is the second argument) and Conjugation: Easy to show. Non-Negative for equal arguments: We have  $$Tr(SX^*X) = Tr(XSX^*)$$ Since $S \geq 0$, $XSX^* \geq 0$ and hence $Tr(XSX^*) \geq 0$. Also if $X=0$, then $\langle X,X\rangle_T = 0$. Need to show if $Tr(SX^*X) = 0$ then $X=0$. If $S$ is strictly positive definite, then for any orthonormal basis $e_i$ $$ 0 = \langle X,X\rangle_T = Tr(\sqrt{S}X^*X\sqrt{S}) = \sum_{i=1}^d \langle e_i,\sqrt{S}X^*X\sqrt{S}e_i\rangle \\    = \sum_{i=1}^d \langle X\sqrt{S}e_i,X\sqrt{S}e_i\rangle = \sum_{i=1}^d \|X\sqrt{S}e_i\|^2$$ Hence $\|X\sqrt{S}e_i\| =0 $ for every $i$ and this further implies $X\sqrt{S} = 0$. Since $\sqrt{S}$ is invertible owing to strict positivity, we get $X=0$.This completes the proof when $S$ is strictly positive definite. For non-negative definite, I don't know how to tackle it as $S$ may have a non-trivial kernel/nullspace.","This was an interesting result I found while reading Holevo's Quantum Information theory book. They call it the non-commutative operator Cauchy Schwarz. Let $S$ be a state i.e. a non negative operator with unit trace on a Hilbert space $\mathcal{H}$ in $\mathbb{C}$. Then for arbitrary operators $X$ and $Y$ on $\mathcal{H}$,  \begin{equation}  |Tr(SX^*Y)|^2 \leq Tr(SX^*X)Tr(SY^*Y)\label{Thi} \end{equation} I was able to show this by mirroring the usual Cauchy Schwarz inequality (I assumed RHS is finite). Then a friend suggested that it might follow easily if we show that $Tr(SX^*Y)$ is an inner product on the operator space. I was able to show that it is a semi inner product. To prove it is an inner product, I needed to show  $$Tr(SX^*X) = 0 \Rightarrow X=0$$ I wasn't successful with this one. I was able to show it if $S$ is a strictly positive operator (at least I think I did) but not for non negative. My guess is that in general, $Tr(SX^*Y)$ is a seminorm but not necessarily a norm. But I was unable to furnish an $X$ such that $X\ne 0$ but $Tr(SX^*X) = 0$. I seek such an example. Kindly give me some ideas. We could think in terms of matrices here. Update: Upon further inspection, I realized that my ""proof that mirrors cauchy schwarz"" was in fact flawed. I assumed implicitly that both terms in RHS were $>0$. In one of the steps, I divide by the square root of RHS, which would be illegal if it were $0$. This wasn't a problem in usual Cauchy schwarz because we were given that LHS is square of mod of an inner product. This changes my perception and I now believe that the above is an inner product but I need to show the final step. Update: I'll show my work so far. Let $\langle X,Y\rangle_T \triangleq Tr(SX^*Y)$. Then Linearity in second argument (In Quantum theory, it is the second argument) and Conjugation: Easy to show. Non-Negative for equal arguments: We have  $$Tr(SX^*X) = Tr(XSX^*)$$ Since $S \geq 0$, $XSX^* \geq 0$ and hence $Tr(XSX^*) \geq 0$. Also if $X=0$, then $\langle X,X\rangle_T = 0$. Need to show if $Tr(SX^*X) = 0$ then $X=0$. If $S$ is strictly positive definite, then for any orthonormal basis $e_i$ $$ 0 = \langle X,X\rangle_T = Tr(\sqrt{S}X^*X\sqrt{S}) = \sum_{i=1}^d \langle e_i,\sqrt{S}X^*X\sqrt{S}e_i\rangle \\    = \sum_{i=1}^d \langle X\sqrt{S}e_i,X\sqrt{S}e_i\rangle = \sum_{i=1}^d \|X\sqrt{S}e_i\|^2$$ Hence $\|X\sqrt{S}e_i\| =0 $ for every $i$ and this further implies $X\sqrt{S} = 0$. Since $\sqrt{S}$ is invertible owing to strict positivity, we get $X=0$.This completes the proof when $S$ is strictly positive definite. For non-negative definite, I don't know how to tackle it as $S$ may have a non-trivial kernel/nullspace.",,"['linear-algebra', 'functional-analysis', 'operator-theory', 'trace', 'cauchy-schwarz-inequality']"
62,"Find a sufficient and necessay condition on $I$ so that $\{M \in M_n(\mathbb R), \ rank(M) \in I \}$ is connected.",Find a sufficient and necessay condition on  so that  is connected.,"I \{M \in M_n(\mathbb R), \ rank(M) \in I \}","Let $n \in \mathbb N$ and $ I \subset [| 1, n |]$. Find a necessary and sufficient condition on $I$ so that $\{M \in M_n(\mathbb R), \ rank(M) \in I \}$ is connected. I know that $\{M \in M_n(\mathbb R), \ rank(M) =n\}$ is not connected since $\det$ is continuous while $\mathbb R^*$ is not connected. Do you have a hint for this ? Thank you.","Let $n \in \mathbb N$ and $ I \subset [| 1, n |]$. Find a necessary and sufficient condition on $I$ so that $\{M \in M_n(\mathbb R), \ rank(M) \in I \}$ is connected. I know that $\{M \in M_n(\mathbb R), \ rank(M) =n\}$ is not connected since $\det$ is continuous while $\mathbb R^*$ is not connected. Do you have a hint for this ? Thank you.",,"['linear-algebra', 'general-topology']"
63,Weird Invariant of Finite Reflection Groups,Weird Invariant of Finite Reflection Groups,,"I am looking for a proof of the following: Let $V$ be a real vector space of dimension $n$ over $\mathbb R$ endowed with an inner product $\langle \,, \rangle$ . Fix a basis $e_1, \cdots, e_n$ for $V$ and the dual basis $x_1, \cdots, x_n$ for $V^*$ . Let $G$ be a finite reflection group acting on $V$ , and thus acting on the ring $\mathbb R[x_1,\cdots, x_n]$ as $g\cdot P(v)= P(g^{-1} \cdot v)$ . By definition $\langle \,, \rangle$ is $G$ invariant. Let $P(v),Q(v) \in R[x_1,\cdots, x_n]$ homogeneous and $G$ -invariant polynomials, i.e. $g \cdot P(v) = P(v)$ for every $g \in G$ , for every $v \in V$ . Then $$\sum_{i,j=1}^n \frac{\partial P}{\partial x_i}\frac{\partial Q}{\partial x_j}\langle e_i,e_j\rangle$$ is $G$ -invariant. Example: Let's consider the root system $\Phi=B_2$ and its associated group $G$ ; pick an hortonormal basis for $V=\mathbb R^2$ . A basis for the ring of invariants $\mathbb R[x,y]^G$ is given by $$P(x,y)=x^2 + y^2$$ $$Q(x,y)=x^2y^2.$$ Then we compute $$\sum_{i,j=1}^n \frac{\partial P}{\partial x_i}\frac{\partial P}{\partial x_j}\langle e_i,e_j\rangle = 2x(2x) + 2y(2y) = 2(x^2 + y^2) = 2P(x,y).$$ $$\sum_{i,j=1}^n \frac{\partial P}{\partial x_i}\frac{\partial Q}{\partial x_j}\langle e_i,e_j\rangle = 2x(2xy^2) + 2y(2x^2y) = 8x^2y^2 = 8Q(x,y)$$ $$\sum_{i,j=1}^n \frac{\partial Q}{\partial x_i}\frac{\partial Q}{\partial x_j}\langle e_i,e_j\rangle = (2xy^2)^2 + (2x^2y)^2 = 4x^2y^2(x^2+y^2) = 4Q(x,y)P(x,y).$$ These are all polynomials in $P$ and $Q$ , and thus invariants. Thoughts: when the basis is orthonormal, this is just the inner product between gradients, so a geometric interpretation of this can tell us that in any fixed point $v$ the angle between the two gradients is not going to change, so it is invariant. I don't know how to produce an algebraic proof in the general setting.","I am looking for a proof of the following: Let be a real vector space of dimension over endowed with an inner product . Fix a basis for and the dual basis for . Let be a finite reflection group acting on , and thus acting on the ring as . By definition is invariant. Let homogeneous and -invariant polynomials, i.e. for every , for every . Then is -invariant. Example: Let's consider the root system and its associated group ; pick an hortonormal basis for . A basis for the ring of invariants is given by Then we compute These are all polynomials in and , and thus invariants. Thoughts: when the basis is orthonormal, this is just the inner product between gradients, so a geometric interpretation of this can tell us that in any fixed point the angle between the two gradients is not going to change, so it is invariant. I don't know how to produce an algebraic proof in the general setting.","V n \mathbb R \langle \,, \rangle e_1, \cdots, e_n V x_1, \cdots, x_n V^* G V \mathbb R[x_1,\cdots, x_n] g\cdot P(v)= P(g^{-1} \cdot v) \langle \,, \rangle G P(v),Q(v) \in R[x_1,\cdots, x_n] G g \cdot P(v) = P(v) g \in G v \in V \sum_{i,j=1}^n \frac{\partial P}{\partial x_i}\frac{\partial Q}{\partial x_j}\langle e_i,e_j\rangle G \Phi=B_2 G V=\mathbb R^2 \mathbb R[x,y]^G P(x,y)=x^2 + y^2 Q(x,y)=x^2y^2. \sum_{i,j=1}^n \frac{\partial P}{\partial x_i}\frac{\partial P}{\partial x_j}\langle e_i,e_j\rangle = 2x(2x) + 2y(2y) = 2(x^2 + y^2) = 2P(x,y). \sum_{i,j=1}^n \frac{\partial P}{\partial x_i}\frac{\partial Q}{\partial x_j}\langle e_i,e_j\rangle = 2x(2xy^2) + 2y(2x^2y) = 8x^2y^2 = 8Q(x,y) \sum_{i,j=1}^n \frac{\partial Q}{\partial x_i}\frac{\partial Q}{\partial x_j}\langle e_i,e_j\rangle = (2xy^2)^2 + (2x^2y)^2 = 4x^2y^2(x^2+y^2) = 4Q(x,y)P(x,y). P Q v","['linear-algebra', 'abstract-algebra', 'group-theory', 'polynomials', 'finite-groups']"
64,"If $\{u_1,...,u_n,v_1,...,v_k\}$ is a basis does it necessarily follow that $\{u_1,...,u_n\}$ is linearly independent?",If  is a basis does it necessarily follow that  is linearly independent?,"\{u_1,...,u_n,v_1,...,v_k\} \{u_1,...,u_n\}","We are given that $\{u_1,...,u_n,v_1,...,v_k\}$ is a basis. My question is does it necessarily follow that $\{u_1,...,u_n\}$ or $\{v_1,...,v_k\}$ are linearly independent? I suspect the answer is yes, but I think I'm missing something in my argument. So: if $\{u_1,...,u_n,v_1,...,v_k\}$ is a basis, then this list of vectors is linearly independent. That is, $a_1 u_1 + \cdots + a_n u_n + b_1 v_1 + \cdots + b_k v_k = 0 \implies a_1 = \cdots = a_n = b_1 = \cdots = b_k = 0$. Then I want to say the following: Since all the $b_i \,'s$ are $0$, we have $a_1 u_1 + \cdots + a_n u_n = 0 \implies a_1 = \cdots = a_n = 0$, meaning that {$u_1,...,u_n$} is linearly independent. With a similar argument, we can also conclude that {$v_1,...,v_k$} is linearly independent. Am I missing something though? I feel like my argument is kind of making a bit of a stretch. Thanks for your help!!","We are given that $\{u_1,...,u_n,v_1,...,v_k\}$ is a basis. My question is does it necessarily follow that $\{u_1,...,u_n\}$ or $\{v_1,...,v_k\}$ are linearly independent? I suspect the answer is yes, but I think I'm missing something in my argument. So: if $\{u_1,...,u_n,v_1,...,v_k\}$ is a basis, then this list of vectors is linearly independent. That is, $a_1 u_1 + \cdots + a_n u_n + b_1 v_1 + \cdots + b_k v_k = 0 \implies a_1 = \cdots = a_n = b_1 = \cdots = b_k = 0$. Then I want to say the following: Since all the $b_i \,'s$ are $0$, we have $a_1 u_1 + \cdots + a_n u_n = 0 \implies a_1 = \cdots = a_n = 0$, meaning that {$u_1,...,u_n$} is linearly independent. With a similar argument, we can also conclude that {$v_1,...,v_k$} is linearly independent. Am I missing something though? I feel like my argument is kind of making a bit of a stretch. Thanks for your help!!",,"['linear-algebra', 'proof-verification']"
65,Find solutions near $I$ of $P^2=I$,Find solutions near  of,I P^2=I,"The questions is: Using the exponential, find all solutions near $I$ of the equation $P^2=I$. I did some try with the exponential: $$e^A=I+\frac{A}{1!}+\frac{A^2}{2!}+\cdots$$ $e^A=I$ while $A=0$, and let $P=e^X$, as $e^A=P^2$, $X=\frac{A}{2}$, which is also $0$. Then, how can I get the solutions?","The questions is: Using the exponential, find all solutions near $I$ of the equation $P^2=I$. I did some try with the exponential: $$e^A=I+\frac{A}{1!}+\frac{A^2}{2!}+\cdots$$ $e^A=I$ while $A=0$, and let $P=e^X$, as $e^A=P^2$, $X=\frac{A}{2}$, which is also $0$. Then, how can I get the solutions?",,"['linear-algebra', 'matrices']"
66,Natural isomorphism are base independent isomorphisms?,Natural isomorphism are base independent isomorphisms?,,"In Linear Algebra courses, often one hears the term ""natural isomorphism"" to designate isomorphism which behave like $V \cong V^{**}$ (in the finite dimensional case). Usually, one comes across the rather coarse definition: an isomorphism $\phi: V \to W$ between vector spaces is said to be natural (or canonical) when its expression does not depend on a particular choice of basis. It seems like a weird definition, to say the least, so it is tempting to look for a more rigorous one in a Category Theory book. Maclane, for example, defines firstly a natural transformation between two functors. After that, it is possible to show that if we consider the category $Vect_{K}$ of finite dimesional vector spaces, and the functors $Id, dDual: Vect_{K} \to Vect_{K}$, the first one being the identity functor, and the second one being the functor that takes vector spaces to their double dual, and linear transformations to their double dual tranformations, then we can find a natural transformation between them.  The questions therefore are: Is it true that a (collecion of, maybe) basis independent isomomorphism(s) ""induces"" a functor $\tau$ on $Vect_{K}$ in such a manner that there exists a natural transformation between $\tau$ and $Id$? Given a natural transformation between a functor $\tau$ on $Vect_{K}$ and the identity functor, is it true that we can find a collection of basis independent isomorphism beween vector spaces? The second question seems stupid, but I wanted to pose the complete problem. Sorry for the long post.","In Linear Algebra courses, often one hears the term ""natural isomorphism"" to designate isomorphism which behave like $V \cong V^{**}$ (in the finite dimensional case). Usually, one comes across the rather coarse definition: an isomorphism $\phi: V \to W$ between vector spaces is said to be natural (or canonical) when its expression does not depend on a particular choice of basis. It seems like a weird definition, to say the least, so it is tempting to look for a more rigorous one in a Category Theory book. Maclane, for example, defines firstly a natural transformation between two functors. After that, it is possible to show that if we consider the category $Vect_{K}$ of finite dimesional vector spaces, and the functors $Id, dDual: Vect_{K} \to Vect_{K}$, the first one being the identity functor, and the second one being the functor that takes vector spaces to their double dual, and linear transformations to their double dual tranformations, then we can find a natural transformation between them.  The questions therefore are: Is it true that a (collecion of, maybe) basis independent isomomorphism(s) ""induces"" a functor $\tau$ on $Vect_{K}$ in such a manner that there exists a natural transformation between $\tau$ and $Id$? Given a natural transformation between a functor $\tau$ on $Vect_{K}$ and the identity functor, is it true that we can find a collection of basis independent isomorphism beween vector spaces? The second question seems stupid, but I wanted to pose the complete problem. Sorry for the long post.",,"['linear-algebra', 'category-theory']"
67,Relation between Adjacency Matrix and Incidence Matrix,Relation between Adjacency Matrix and Incidence Matrix,,"Let the Adjacency matrix be A, and Incidence Matrix be B;  'd' represents degree of given vertex How do we prove $B.B^T=A+\begin{bmatrix}d(V_1) & 0 &\dots \\ 0 & d(V_2) & 0 & \dots\\ 0 &0 & d(V_3)&0&\dots \\ \vdots & & & \ddots \\ 0 &\dots & & & d(V_n) \end{bmatrix}$","Let the Adjacency matrix be A, and Incidence Matrix be B;  'd' represents degree of given vertex How do we prove $B.B^T=A+\begin{bmatrix}d(V_1) & 0 &\dots \\ 0 & d(V_2) & 0 & \dots\\ 0 &0 & d(V_3)&0&\dots \\ \vdots & & & \ddots \\ 0 &\dots & & & d(V_n) \end{bmatrix}$",,"['linear-algebra', 'graph-theory']"
68,The significance of the composition of an operator and its adjoint,The significance of the composition of an operator and its adjoint,,"As I read the literature, I have noticed that the composition $T^*T$ of a linear operator $T:H\to H$ and its adjoint frequently turns up in all kind of places. I am aware that it is Hermitian (at least when $T$ is bounded), that $||Tx||^2=\langle Tx,Tx\rangle=\langle x,T^*Tx\rangle$ and other basic stuff like that. However, I don't quite ""feel"" what the notion $T^*T$ really is and why is it so ubiquitous. I know that this might seem vague but can anyone give me a general idea of how I should view $T^*T$? What dose it do to a vector and what are its important properties? I am particularly struck by the fact that $||A||_2^2=\rho(A^*A)$ when $A$ is a matrix representing a finite dimensional operator and that $I+T^*T$ is a bijection.","As I read the literature, I have noticed that the composition $T^*T$ of a linear operator $T:H\to H$ and its adjoint frequently turns up in all kind of places. I am aware that it is Hermitian (at least when $T$ is bounded), that $||Tx||^2=\langle Tx,Tx\rangle=\langle x,T^*Tx\rangle$ and other basic stuff like that. However, I don't quite ""feel"" what the notion $T^*T$ really is and why is it so ubiquitous. I know that this might seem vague but can anyone give me a general idea of how I should view $T^*T$? What dose it do to a vector and what are its important properties? I am particularly struck by the fact that $||A||_2^2=\rho(A^*A)$ when $A$ is a matrix representing a finite dimensional operator and that $I+T^*T$ is a bijection.",,"['linear-algebra', 'functional-analysis', 'soft-question', 'motivation']"
69,Basis of Quaternion Algebras,Basis of Quaternion Algebras,,"I was reading some notes in which the definition of quaternion algebra is given as : For $a,b \in k^{*}$ ,we define $k$-algebra by generators and relations as follows: it has two genrators $i$ and $j$ and is subject to the relations $i^2 = a,  j^2 = b, ij=-ij$.Any k-algebra isomophic to this is called a quaternion algebra. Now,to prove that $ {{1,i,j,ij}}$ is the basis for the quaternion algebra,it is easy to check that this is the spanning set.But to show the $k$-linear independence,they say that it is enough to check that the $k$-algebra with ${1,i,j,ij}$ as basis is associative. I am not getting this that how will the associativity of the algebra will imply the linear independence.(it might be simple but I am stuck at this).","I was reading some notes in which the definition of quaternion algebra is given as : For $a,b \in k^{*}$ ,we define $k$-algebra by generators and relations as follows: it has two genrators $i$ and $j$ and is subject to the relations $i^2 = a,  j^2 = b, ij=-ij$.Any k-algebra isomophic to this is called a quaternion algebra. Now,to prove that $ {{1,i,j,ij}}$ is the basis for the quaternion algebra,it is easy to check that this is the spanning set.But to show the $k$-linear independence,they say that it is enough to check that the $k$-algebra with ${1,i,j,ij}$ as basis is associative. I am not getting this that how will the associativity of the algebra will imply the linear independence.(it might be simple but I am stuck at this).",,"['linear-algebra', 'abstract-algebra', 'number-theory']"
70,Closed form of a recursive relation,Closed form of a recursive relation,,"A sequence $\langle a_n\rangle$ is defined recursively by $a_1=0$, $a_2=1$ and for $n\ge 3$, $$a_n=\frac 12 na_{n-1}+\frac 12n(n-1)a_{n-2}+(-1)^n\left(1-\frac n2\right).$$ Find a closed form expression for $$f_n=a_n+2\binom n1a_{n-1}+3\binom n2a_{n-2}+\cdots+(n-1)\binom n{n-2}a_2+n\binom n{n-1}a_1.$$ I substituted $b_k=\binom n{n-k}a_k$ which reduces the given recursion to $$b_n=\frac {b_{n-1}}2+b_{n-2}+(-1)^n\left(1-\frac n2\right)\\ \Longrightarrow2b_n-b_{n-1}-2b_{n-2}=(-1)^n(2-n)\\ \Longrightarrow 2b_{n-1}-b_{n-2}-2b_{n-3}=-(-1)^n(3-n)\\ \Longrightarrow2b_{n-1}-b_{n-2}-2b_{n-3}=-(-1)^n(3-n)\\ \Longrightarrow 2b_{n-2}-b_{n-3}-2b_{n-4}=(-1)^n(4-n)$$ Adding the last four equations, we get $$2b_n+3b_{n-1}-2b_{n-2}-5b_{n-3}-2b_{n-4}=0$$ Now using the standard way of solving such recursions, we set $b_k=\lambda^k$, which gives $$2\lambda^4+3\lambda^3-2\lambda^2-5\lambda-2=0$$ We have to find $f_n=a_n+2\binom n1a_{n-1}+3\binom n2a_{n-2}+\cdots+(n-1)\binom n{n-2}a_2+n\binom n{n-1}a_1\\=b_n+2b_{n-1}+3b_{n-2}+\cdots+(n-1)b_2+nb_1\\=\lambda^n+2\lambda^{n-1}+3\lambda^{n-2}+\cdots+(n-1)\lambda^2+n\lambda$ This is where I got stuck. What should I do after this?","A sequence $\langle a_n\rangle$ is defined recursively by $a_1=0$, $a_2=1$ and for $n\ge 3$, $$a_n=\frac 12 na_{n-1}+\frac 12n(n-1)a_{n-2}+(-1)^n\left(1-\frac n2\right).$$ Find a closed form expression for $$f_n=a_n+2\binom n1a_{n-1}+3\binom n2a_{n-2}+\cdots+(n-1)\binom n{n-2}a_2+n\binom n{n-1}a_1.$$ I substituted $b_k=\binom n{n-k}a_k$ which reduces the given recursion to $$b_n=\frac {b_{n-1}}2+b_{n-2}+(-1)^n\left(1-\frac n2\right)\\ \Longrightarrow2b_n-b_{n-1}-2b_{n-2}=(-1)^n(2-n)\\ \Longrightarrow 2b_{n-1}-b_{n-2}-2b_{n-3}=-(-1)^n(3-n)\\ \Longrightarrow2b_{n-1}-b_{n-2}-2b_{n-3}=-(-1)^n(3-n)\\ \Longrightarrow 2b_{n-2}-b_{n-3}-2b_{n-4}=(-1)^n(4-n)$$ Adding the last four equations, we get $$2b_n+3b_{n-1}-2b_{n-2}-5b_{n-3}-2b_{n-4}=0$$ Now using the standard way of solving such recursions, we set $b_k=\lambda^k$, which gives $$2\lambda^4+3\lambda^3-2\lambda^2-5\lambda-2=0$$ We have to find $f_n=a_n+2\binom n1a_{n-1}+3\binom n2a_{n-2}+\cdots+(n-1)\binom n{n-2}a_2+n\binom n{n-1}a_1\\=b_n+2b_{n-1}+3b_{n-2}+\cdots+(n-1)b_2+nb_1\\=\lambda^n+2\lambda^{n-1}+3\lambda^{n-2}+\cdots+(n-1)\lambda^2+n\lambda$ This is where I got stuck. What should I do after this?",,"['linear-algebra', 'recurrence-relations']"
71,Rational matrix having roots of every degree,Rational matrix having roots of every degree,,"As the result of another question , now deleted, I am interested in the following problem. Problem. Let $A\in M_n(\mathbb Q)$ be an invertible matrix with the property that the equation $X^k=A$ has solutions (in $M_n(\mathbb Q)$) for any $k\ge 1$. Prove all eigenvalues of $A$ are equal to $1$. (This is the proposed problem 398 at page 36 of this journal .) Remarks. 1. The question is the $\mathbb Q$-version of a well known contest problem for matrices in $M_n(\mathbb Z)$. (In that case the conclusion is stronger, and we have $A=I_n$.) 2. The converse also holds. I do not know a proof of the main problem, nor do I know of a proof of the remarks. Remarks on any of these would be appreciated.","As the result of another question , now deleted, I am interested in the following problem. Problem. Let $A\in M_n(\mathbb Q)$ be an invertible matrix with the property that the equation $X^k=A$ has solutions (in $M_n(\mathbb Q)$) for any $k\ge 1$. Prove all eigenvalues of $A$ are equal to $1$. (This is the proposed problem 398 at page 36 of this journal .) Remarks. 1. The question is the $\mathbb Q$-version of a well known contest problem for matrices in $M_n(\mathbb Z)$. (In that case the conclusion is stronger, and we have $A=I_n$.) 2. The converse also holds. I do not know a proof of the main problem, nor do I know of a proof of the remarks. Remarks on any of these would be appreciated.",,"['linear-algebra', 'abstract-algebra', 'matrices', 'eigenvalues-eigenvectors', 'contest-math']"
72,Decomposition of shear matrix into rotation & scaling,Decomposition of shear matrix into rotation & scaling,,How can I decompose the affine transformation: $$ \begin{bmatrix}1&\text{shear}_x\\\text{shear}_y&1\end{bmatrix}$$ into rotation and scaling primitives? $$ \begin{bmatrix}\cos\theta&-\sin\theta\\\sin\theta&\cos\theta\end{bmatrix}$$ \begin{bmatrix}\text{scale}_x&0\\0&\text{scale}_y\end{bmatrix},How can I decompose the affine transformation: $$ \begin{bmatrix}1&\text{shear}_x\\\text{shear}_y&1\end{bmatrix}$$ into rotation and scaling primitives? $$ \begin{bmatrix}\cos\theta&-\sin\theta\\\sin\theta&\cos\theta\end{bmatrix}$$ \begin{bmatrix}\text{scale}_x&0\\0&\text{scale}_y\end{bmatrix},,"['linear-algebra', 'affine-geometry']"
73,Solving Cubic Equations Using Origami,Solving Cubic Equations Using Origami,,"I have to write a research paper on a mathematical topic for my class; I chose the above topic. I understand that a parabola can be formed using a focus and directrix, both created by origami folds, and that Axiom 6 of Origami-Folding (Given two points $P_1$ and $P_2$ and two lines $L_1$ and $L_2$, there is a fold that places $P_1$ onto $L_1$ and $P_2$ onto $L_2$) can be used to solve a cubic equation. But some of this explanation of why confuses me: Now, let's solve the cubic equation $x^3+ax^2+bx+c=0$ with origami. Let two points $P_1$ and $P_2$ have the coordinates $(a,1)$ and $(c,b)$, respectively. Also let two lines $L_1$ and $L_2$ have the equations $y+1=0$ and $x+c=0$, respectively. Fold a line placing $P_1$ onto $L_1$ and placing $P_2$ onto $L_2$, and the slope of the crease is the solution of $x^3+ax^2+bx+c=0$. I will explain why. Let p1 be a parabola having the focus $P_1$ and the directrix $L_1$. Since the crease is not parallel to the $y$-axis, we can let the crease have the equation $y=tx+u$. Let the crease be tangent to $P_1$ at $(x_1,y_1)$, and $(x_1 -a)^2=4y_1$. Because the crease has the equation $(x_1 -a)(x-x_1)=2(y-y_1)$, we get $t=\frac {x_1-a}{2}$ and $u= y_1-\frac {x_1(x_1-a)}{2}$. From these equations, we get $u=-t^2-at$. Specifically, I do not understand where the equation  $(x_1 -a)^2=4y_1$ is coming from. I would greatly appreciate someone helping to explain. [this explanation comes from http://origami.ousaan.com/library/conste.html if you want a look at the entire thing]","I have to write a research paper on a mathematical topic for my class; I chose the above topic. I understand that a parabola can be formed using a focus and directrix, both created by origami folds, and that Axiom 6 of Origami-Folding (Given two points $P_1$ and $P_2$ and two lines $L_1$ and $L_2$, there is a fold that places $P_1$ onto $L_1$ and $P_2$ onto $L_2$) can be used to solve a cubic equation. But some of this explanation of why confuses me: Now, let's solve the cubic equation $x^3+ax^2+bx+c=0$ with origami. Let two points $P_1$ and $P_2$ have the coordinates $(a,1)$ and $(c,b)$, respectively. Also let two lines $L_1$ and $L_2$ have the equations $y+1=0$ and $x+c=0$, respectively. Fold a line placing $P_1$ onto $L_1$ and placing $P_2$ onto $L_2$, and the slope of the crease is the solution of $x^3+ax^2+bx+c=0$. I will explain why. Let p1 be a parabola having the focus $P_1$ and the directrix $L_1$. Since the crease is not parallel to the $y$-axis, we can let the crease have the equation $y=tx+u$. Let the crease be tangent to $P_1$ at $(x_1,y_1)$, and $(x_1 -a)^2=4y_1$. Because the crease has the equation $(x_1 -a)(x-x_1)=2(y-y_1)$, we get $t=\frac {x_1-a}{2}$ and $u= y_1-\frac {x_1(x_1-a)}{2}$. From these equations, we get $u=-t^2-at$. Specifically, I do not understand where the equation  $(x_1 -a)^2=4y_1$ is coming from. I would greatly appreciate someone helping to explain. [this explanation comes from http://origami.ousaan.com/library/conste.html if you want a look at the entire thing]",,"['linear-algebra', 'geometry', 'polynomials', 'cubics', 'origami']"
74,Matrix with determinant 0,Matrix with determinant 0,,"If $A \in M_3(\mathbb{R})$ is a $3 \times 3$ matrix with $\det(A)=0$ and the square of each element equals its cofactor , do we necessarily have $A=0_3$? $a_{ij}^2=A_{ij}$, where $A_{ij}=(-1)^{i+j}M_{ij}$ and $M_{ij}$ is the minor of $a_{ij}$.","If $A \in M_3(\mathbb{R})$ is a $3 \times 3$ matrix with $\det(A)=0$ and the square of each element equals its cofactor , do we necessarily have $A=0_3$? $a_{ij}^2=A_{ij}$, where $A_{ij}=(-1)^{i+j}M_{ij}$ and $M_{ij}$ is the minor of $a_{ij}$.",,"['linear-algebra', 'matrices', 'determinant']"
75,How to prove that this $X_0$ is nilpotent,How to prove that this  is nilpotent,X_0,"Let $V=M_n(\mathbb C)$ and $A\subseteq B$ are subspaces of $V$. Let also $$ M=\{X\in V:\ \forall Y\in B,\ XY-YX\in A\}. $$ Suppose $X_0\in M$ enjoys the property that $\operatorname{tr}(ZX_0)=0$ for any $Z\in M$. Show that $X_0$ is nilpotent. My attempt: I know that $$\operatorname{tr}(XY-YX)=0$$ but I cannot continue from there. Thank you for any help. The problem appears on a Chinese bulletin board. A soluton can be found in section 4.3 of James E. Humphreys, Introduction to Lie Algebras and Representation Theory , but I want to know if this problem can be solved by other methods. Thank you.","Let $V=M_n(\mathbb C)$ and $A\subseteq B$ are subspaces of $V$. Let also $$ M=\{X\in V:\ \forall Y\in B,\ XY-YX\in A\}. $$ Suppose $X_0\in M$ enjoys the property that $\operatorname{tr}(ZX_0)=0$ for any $Z\in M$. Show that $X_0$ is nilpotent. My attempt: I know that $$\operatorname{tr}(XY-YX)=0$$ but I cannot continue from there. Thank you for any help. The problem appears on a Chinese bulletin board. A soluton can be found in section 4.3 of James E. Humphreys, Introduction to Lie Algebras and Representation Theory , but I want to know if this problem can be solved by other methods. Thank you.",,[]
76,Spinor representation and Clifford modules,Spinor representation and Clifford modules,,"Let $V$ be an even-dimensional real inner product space. We denote the Clifford algebra of $V$ by $C(V)$ and the spinor representation by $S$. For a finite-dimensional $\mathbb Z_2$-graded complex Clifford module $E$ the following facts are known. Denote by $W$ the trivial Clifford module $\mathrm{Hom}_{C(V)}(S,E)$. $E$ is isomorphic to $W \otimes S$ as a Clifford module. $\mathrm{End}(W)$ is isomorphic to $\mathrm{End}_{C(V)}(E)$. Question: Why do the above statements hold? The statements can be found in [BGV,§3.2] and [R,4.12] without any details. According to [BGV], the isomorphism $W \otimes S \to E$ is given by the evaluation. This is obviously a homomorphism of Clifford modules, but it is not obvious that the evaluation map is bijective. References [BGV] Berline-Getzler-Vergne, Heat Kernels and Dirac operators [R] Roe, Elliptic operators, topology and asymptotic methods","Let $V$ be an even-dimensional real inner product space. We denote the Clifford algebra of $V$ by $C(V)$ and the spinor representation by $S$. For a finite-dimensional $\mathbb Z_2$-graded complex Clifford module $E$ the following facts are known. Denote by $W$ the trivial Clifford module $\mathrm{Hom}_{C(V)}(S,E)$. $E$ is isomorphic to $W \otimes S$ as a Clifford module. $\mathrm{End}(W)$ is isomorphic to $\mathrm{End}_{C(V)}(E)$. Question: Why do the above statements hold? The statements can be found in [BGV,§3.2] and [R,4.12] without any details. According to [BGV], the isomorphism $W \otimes S \to E$ is given by the evaluation. This is obviously a homomorphism of Clifford modules, but it is not obvious that the evaluation map is bijective. References [BGV] Berline-Getzler-Vergne, Heat Kernels and Dirac operators [R] Roe, Elliptic operators, topology and asymptotic methods",,"['linear-algebra', 'representation-theory', 'modules', 'clifford-algebras', 'spin-geometry']"
77,"Prove that $\{ \sin(x), \sin^2(x), \sin^3(x)\}$ is linearly independent in $F(\Bbb R)$ [duplicate]",Prove that  is linearly independent in  [duplicate],"\{ \sin(x), \sin^2(x), \sin^3(x)\} F(\Bbb R)","This question already has answers here : Are the functions $\sin^n(x)$ linearly independent? (3 answers) Closed 10 years ago . Prove that $\{ \sin(x), \sin^2(x), \sin^3(x)\}$ is linearly independent in $F(\Bbb R)$. I tried plugging in $\left\{ 0, \frac {\pi} {2}, \pi, \frac {3\pi}{2}\right\}$ but that doesn't work. How should I prove this?","This question already has answers here : Are the functions $\sin^n(x)$ linearly independent? (3 answers) Closed 10 years ago . Prove that $\{ \sin(x), \sin^2(x), \sin^3(x)\}$ is linearly independent in $F(\Bbb R)$. I tried plugging in $\left\{ 0, \frac {\pi} {2}, \pi, \frac {3\pi}{2}\right\}$ but that doesn't work. How should I prove this?",,['linear-algebra']
78,GP 1.4.4 An extension of partial converse of preimage theorem.,GP 1.4.4 An extension of partial converse of preimage theorem.,,"This is exercise 1.4.4 on Guillemin and Pollack's Differential Topology Suppose that $Z \subset X \subset Y$ are manifolds, and $z \in Z$. Then there exist independent functions $g_1, \dots, g_l$, on a neighborhood $W$ of $z$ in $Y$ such that   $$Z \cap W = \{y \in W : g_1(y) = 0, \dots, g_l(y) = 0\},$$   $$X \cap W = \{y \in W : g_i(y) = 0, \dots , g_m(y) = 0\},$$   where $l-m$ is the codimension of $Z$ in $X$. I tried to set up the proof as following: Suppose that $Z \subset X \subset Y$ are manifolds, and $z \in Z$.  Let $Z$ and $X$ have codimensions $l$ and $m$ in $Y$, $Z$ has codimension $l-m$ in $X$. From the partial converse to the preimage theorem, there exist independent functions $f_1, \dots f_m$ on a neighborhood $U$ of $z$ in $Y$ such that $X \cap U$ is the common vanishing set of the $f_i$. We also know that there exist independent functions $h_{m+1}, \dots, h_l$ on a neighborhood $V$ of $z$ in $X$ such that $Z \cap V$ is the common vanishing set of the $h_i$. And then I don't know why $h_i$s are smooth, and how should I continue. Any ideas? Thank you.","This is exercise 1.4.4 on Guillemin and Pollack's Differential Topology Suppose that $Z \subset X \subset Y$ are manifolds, and $z \in Z$. Then there exist independent functions $g_1, \dots, g_l$, on a neighborhood $W$ of $z$ in $Y$ such that   $$Z \cap W = \{y \in W : g_1(y) = 0, \dots, g_l(y) = 0\},$$   $$X \cap W = \{y \in W : g_i(y) = 0, \dots , g_m(y) = 0\},$$   where $l-m$ is the codimension of $Z$ in $X$. I tried to set up the proof as following: Suppose that $Z \subset X \subset Y$ are manifolds, and $z \in Z$.  Let $Z$ and $X$ have codimensions $l$ and $m$ in $Y$, $Z$ has codimension $l-m$ in $X$. From the partial converse to the preimage theorem, there exist independent functions $f_1, \dots f_m$ on a neighborhood $U$ of $z$ in $Y$ such that $X \cap U$ is the common vanishing set of the $f_i$. We also know that there exist independent functions $h_{m+1}, \dots, h_l$ on a neighborhood $V$ of $z$ in $X$ such that $Z \cap V$ is the common vanishing set of the $h_i$. And then I don't know why $h_i$s are smooth, and how should I continue. Any ideas? Thank you.",,"['linear-algebra', 'general-topology', 'differential-topology']"
79,Do these two sets of matrices form groups?,Do these two sets of matrices form groups?,,"Stimulated by some Physics backgrounds , consider the following two sets of matrices. Notations and definitions:Let $A,B$ be two complex $n\times n$ matrices, then $\left [ A,B \right ]\overset{\underset{\mathrm{def}}{}}{=}AB-BA$ and $ e^A\overset{\underset{\mathrm{def}}{}}{=} \sum_{m=0}^{\infty }\frac{A^m}{m!}. $ Let $M_1,M_2,M_3$ be three $n\times n$ Hermitian matrices, and they satisfy $[M_1,M_2]=iM_3,[M_2,M_3]=iM_1,[M_3,M_1]=iM_2$ identities, where $i=\sqrt{-1}$. Now define a set $X$ of unitary matrices:$X=\left \{ e^{i\alpha M_3}e^{i\beta  M_2}e^{i\gamma M_3} :\alpha,\beta,\gamma \in \mathbb{R}\right \},$ and another set $Y$ of unitary matrices:$Y=\left \{ e^{i(\alpha M_1+\beta  M_2+\gamma M_3)} :\alpha,\beta,\gamma \in \mathbb{R}\right \},$ where $i=\sqrt{-1}$ ( Note that the number indices of the Hermitian matrices $M$ are different in $X$ and $Y$ ). And my questions are as follows: (1) Is $X=Y$ ? (2) If (1) is true, is the set $X$ a group ? (3) If both (1) and (2) are true, is $X\cong SU(2)$ true ? Supplements: For concreteness, let's take a look at the following simple example. Consider the Physically called spin-$\frac{1}{2}$ ""Pauli"" matrices $M_1=\frac{1}{2}\bigl(\begin{smallmatrix}  0& 1\\   1&0  \end{smallmatrix}\bigr),M_2=\frac{1}{2}\bigl(\begin{smallmatrix}  0& -i\\   i&0  \end{smallmatrix}\bigr)$ and $M_3=\frac{1}{2}\bigl(\begin{smallmatrix}  1& 0\\   0&-1  \end{smallmatrix}\bigr),$ and it's easy to find that $M_1^2+M_2^2+M_3^2=\frac{1}{2}(\frac{1}{2}+1)\mathbb{I}$, where $\mathbb{I}$ is a $2\times2$ identity matrix. In the above example, direct calculation of matrices $e^{i\alpha M_3}e^{i\beta  M_2}e^{i\gamma M_3}$ in $X$ shows that $X=SU(2)$ (then $X$ is a group), and it's also easy to verify that $Y\subseteq SU(2)$. So now the question is, is $SU(2)\subseteq Y$ too ? Thanks in advance.","Stimulated by some Physics backgrounds , consider the following two sets of matrices. Notations and definitions:Let $A,B$ be two complex $n\times n$ matrices, then $\left [ A,B \right ]\overset{\underset{\mathrm{def}}{}}{=}AB-BA$ and $ e^A\overset{\underset{\mathrm{def}}{}}{=} \sum_{m=0}^{\infty }\frac{A^m}{m!}. $ Let $M_1,M_2,M_3$ be three $n\times n$ Hermitian matrices, and they satisfy $[M_1,M_2]=iM_3,[M_2,M_3]=iM_1,[M_3,M_1]=iM_2$ identities, where $i=\sqrt{-1}$. Now define a set $X$ of unitary matrices:$X=\left \{ e^{i\alpha M_3}e^{i\beta  M_2}e^{i\gamma M_3} :\alpha,\beta,\gamma \in \mathbb{R}\right \},$ and another set $Y$ of unitary matrices:$Y=\left \{ e^{i(\alpha M_1+\beta  M_2+\gamma M_3)} :\alpha,\beta,\gamma \in \mathbb{R}\right \},$ where $i=\sqrt{-1}$ ( Note that the number indices of the Hermitian matrices $M$ are different in $X$ and $Y$ ). And my questions are as follows: (1) Is $X=Y$ ? (2) If (1) is true, is the set $X$ a group ? (3) If both (1) and (2) are true, is $X\cong SU(2)$ true ? Supplements: For concreteness, let's take a look at the following simple example. Consider the Physically called spin-$\frac{1}{2}$ ""Pauli"" matrices $M_1=\frac{1}{2}\bigl(\begin{smallmatrix}  0& 1\\   1&0  \end{smallmatrix}\bigr),M_2=\frac{1}{2}\bigl(\begin{smallmatrix}  0& -i\\   i&0  \end{smallmatrix}\bigr)$ and $M_3=\frac{1}{2}\bigl(\begin{smallmatrix}  1& 0\\   0&-1  \end{smallmatrix}\bigr),$ and it's easy to find that $M_1^2+M_2^2+M_3^2=\frac{1}{2}(\frac{1}{2}+1)\mathbb{I}$, where $\mathbb{I}$ is a $2\times2$ identity matrix. In the above example, direct calculation of matrices $e^{i\alpha M_3}e^{i\beta  M_2}e^{i\gamma M_3}$ in $X$ shows that $X=SU(2)$ (then $X$ is a group), and it's also easy to verify that $Y\subseteq SU(2)$. So now the question is, is $SU(2)\subseteq Y$ too ? Thanks in advance.",,"['linear-algebra', 'group-theory', 'representation-theory', 'mathematical-physics', 'quantum-mechanics']"
80,Maximal subset with rank $k$,Maximal subset with rank,k,"I'm trying to solve the following problem for an algorithm I'm trying to develop and I couldn't find anything helpful in scholar google. Here is the question: Suppose I have a set of $N$ vectors $V=\{x_1,...,x_N\}$ and a positive integer $k$. I would like to find the largest subset from V (as many vectors as possible) that has dimension $k$ (the rank of the matrix of those vectors will be $k$). Any ideas on how to solve this redundancy problem? I was thinking about pivoted QR or Interpolative Decomposition, but I got a dead end. If this helps, I don't mind that instead of rank smaller than $k$ I will obtain a solution that gives nuclear norm smaller than some given positive number (i.e. the sum of singular values). Thanks, Gil","I'm trying to solve the following problem for an algorithm I'm trying to develop and I couldn't find anything helpful in scholar google. Here is the question: Suppose I have a set of $N$ vectors $V=\{x_1,...,x_N\}$ and a positive integer $k$. I would like to find the largest subset from V (as many vectors as possible) that has dimension $k$ (the rank of the matrix of those vectors will be $k$). Any ideas on how to solve this redundancy problem? I was thinking about pivoted QR or Interpolative Decomposition, but I got a dead end. If this helps, I don't mind that instead of rank smaller than $k$ I will obtain a solution that gives nuclear norm smaller than some given positive number (i.e. the sum of singular values). Thanks, Gil",,"['linear-algebra', 'algorithms', 'computer-science']"
81,Linear Algebra and Trig Identity Proof,Linear Algebra and Trig Identity Proof,,"I am working on the following question. It involves finding a proof for a trig identity using linear algebra. The proof is one involving $sin(\alpha +\theta)$ and $cos(\alpha +\theta)$, as you will see. I will go through where I am up to, progressing through each part of the question. Let $T_{\alpha}$ be the linear transformation from $\mathbb{R}^2$ to $\mathbb{R}^2$     which is the rotation counterclockwiseby $\alpha$, and $T_{\theta}$ the counterclockwise rotation by θ. (A) Write down the standard matrices for $T_{\alpha}$ and $T_{\theta}$, explain your reasoning. Let the standard matrix for $T_{\alpha}$ be $A$ and let the standard matrix for $T_{\theta}$ be $B$, then $$A=\begin{bmatrix}        \cos (\alpha) & -\sin (\alpha) \\        \sin (\alpha) & \cos (\alpha) \\      \end{bmatrix}$$ and $$B=\begin{bmatrix}        \cos (\theta) & -\sin (\theta) \\        \sin (\theta) & \cos (\theta) \\      \end{bmatrix}$$ On to the next part of the queston. (B) Explain what the linear transformation $T_{\alpha} \circ T_{\theta}$ does to $\mathbb{R}^2$. It first rotates a given point by $\alpha$ degrees and then rotates the given point by $\theta$ degrees. (C) Compute the matrix for $T_{\alpha} \circ T_{\theta}$ by multiplying the matrices for $T_{\alpha}$ and $T_{\theta}$ So, $$AB = \begin{bmatrix}        \cos (\alpha)\cos (\theta) - \sin (\alpha) \sin (\theta) & -\cos (\alpha)\sin (\theta) - \sin (\alpha) \cos (\theta) \\        \sin (\alpha)\cos (\theta) + \cos (\alpha) \cos (\theta) & -\sin (\alpha)\sin (\theta) + \cos (\alpha) \cos (\theta) \\      \end{bmatrix}$$ (D) On the other hand, from the description in part (b), you can directly write down     the matrix for $T_{\alpha} \circ T_{\theta}$. What is that matrix? If that matrix is $C$, then $$C=\begin{bmatrix}        \cos (\alpha + \theta) & -\sin (\alpha + \theta) \\        \sin (\alpha + \theta) & \cos (\alpha + \theta) \\      \end{bmatrix}$$ (E) Since the matrices from parts (c) and (d) are describe the same linear transformation, they must be equal. What identities among sin and cos must therefore be     true? So I must set $AB=C$. Then $$\begin{bmatrix}        \cos (\alpha)\cos (\theta) - \sin (\alpha) \sin (\theta) & -\cos (\alpha)\sin (\theta) - \sin (\alpha) \cos (\theta) \\        \sin (\alpha)\cos (\theta) + \cos (\alpha) \cos (\theta) & -\sin (\alpha)\sin (\theta) + \cos (\alpha) \cos (\theta) \\      \end{bmatrix}=\begin{bmatrix}        \cos (\alpha + \theta) & -\sin (\alpha + \theta) \\        \sin (\alpha + \theta) & \cos (\alpha + \theta) \\      \end{bmatrix}$$ These are the identities I was looking for! Now the next part has me worried. Using a similar idea, ﬁnd formulas for $\sin(3\theta)$ and $\cos(3\theta)$ in terms of $\sin(\theta)$ and $\cos(\theta)$. Now I am not completely hopeless - I was able to come up with this next bit. But I am not sure if I have done things correctly. I'll use the transformation $T_{\theta}$ from before. The standard matrix is $$A=\begin{bmatrix}        \cos (\alpha) & -\sin (\alpha) \\        \sin (\alpha) & \cos (\alpha) \\      \end{bmatrix}$$ I thought that maybe if I transformed a point three times it would rotate it $3\theta$ degrees, i.e with a standard matrix $AAA$. The result was $$AAA=B$$ $$=$$$$ \begin{bmatrix}        \cos^{3}(\theta)-\cos(\theta)\sin^2(\theta)-2\cos(\theta)\sin^2(\theta) & -\sin(\theta)\cos^2(\theta)+\sin^3(\theta)-2\cos^2(\theta)\sin(\theta) \\        2\sin(\theta)\cos^2(\theta)-\sin^3(\theta)+\cos^2(\theta)\sin(\theta)  & -2\sin^3(\theta)\cos(\theta)-\sin^2(\theta)cos(\theta)+\cos^3(\theta)\ \\      \end{bmatrix}$$ As before I would say that $AAA$ is equal to a transformation matrix $$A'=\begin{bmatrix}        \cos (3\alpha) & -\sin (3\alpha) \\        \sin (3\alpha) & \cos (3\alpha) \\      \end{bmatrix}$$ Then setting $A'=B$ would result in some identities. They just seem quite long and I am not sure if what I have done is correct or checks out. Any help would be appreciated.","I am working on the following question. It involves finding a proof for a trig identity using linear algebra. The proof is one involving $sin(\alpha +\theta)$ and $cos(\alpha +\theta)$, as you will see. I will go through where I am up to, progressing through each part of the question. Let $T_{\alpha}$ be the linear transformation from $\mathbb{R}^2$ to $\mathbb{R}^2$     which is the rotation counterclockwiseby $\alpha$, and $T_{\theta}$ the counterclockwise rotation by θ. (A) Write down the standard matrices for $T_{\alpha}$ and $T_{\theta}$, explain your reasoning. Let the standard matrix for $T_{\alpha}$ be $A$ and let the standard matrix for $T_{\theta}$ be $B$, then $$A=\begin{bmatrix}        \cos (\alpha) & -\sin (\alpha) \\        \sin (\alpha) & \cos (\alpha) \\      \end{bmatrix}$$ and $$B=\begin{bmatrix}        \cos (\theta) & -\sin (\theta) \\        \sin (\theta) & \cos (\theta) \\      \end{bmatrix}$$ On to the next part of the queston. (B) Explain what the linear transformation $T_{\alpha} \circ T_{\theta}$ does to $\mathbb{R}^2$. It first rotates a given point by $\alpha$ degrees and then rotates the given point by $\theta$ degrees. (C) Compute the matrix for $T_{\alpha} \circ T_{\theta}$ by multiplying the matrices for $T_{\alpha}$ and $T_{\theta}$ So, $$AB = \begin{bmatrix}        \cos (\alpha)\cos (\theta) - \sin (\alpha) \sin (\theta) & -\cos (\alpha)\sin (\theta) - \sin (\alpha) \cos (\theta) \\        \sin (\alpha)\cos (\theta) + \cos (\alpha) \cos (\theta) & -\sin (\alpha)\sin (\theta) + \cos (\alpha) \cos (\theta) \\      \end{bmatrix}$$ (D) On the other hand, from the description in part (b), you can directly write down     the matrix for $T_{\alpha} \circ T_{\theta}$. What is that matrix? If that matrix is $C$, then $$C=\begin{bmatrix}        \cos (\alpha + \theta) & -\sin (\alpha + \theta) \\        \sin (\alpha + \theta) & \cos (\alpha + \theta) \\      \end{bmatrix}$$ (E) Since the matrices from parts (c) and (d) are describe the same linear transformation, they must be equal. What identities among sin and cos must therefore be     true? So I must set $AB=C$. Then $$\begin{bmatrix}        \cos (\alpha)\cos (\theta) - \sin (\alpha) \sin (\theta) & -\cos (\alpha)\sin (\theta) - \sin (\alpha) \cos (\theta) \\        \sin (\alpha)\cos (\theta) + \cos (\alpha) \cos (\theta) & -\sin (\alpha)\sin (\theta) + \cos (\alpha) \cos (\theta) \\      \end{bmatrix}=\begin{bmatrix}        \cos (\alpha + \theta) & -\sin (\alpha + \theta) \\        \sin (\alpha + \theta) & \cos (\alpha + \theta) \\      \end{bmatrix}$$ These are the identities I was looking for! Now the next part has me worried. Using a similar idea, ﬁnd formulas for $\sin(3\theta)$ and $\cos(3\theta)$ in terms of $\sin(\theta)$ and $\cos(\theta)$. Now I am not completely hopeless - I was able to come up with this next bit. But I am not sure if I have done things correctly. I'll use the transformation $T_{\theta}$ from before. The standard matrix is $$A=\begin{bmatrix}        \cos (\alpha) & -\sin (\alpha) \\        \sin (\alpha) & \cos (\alpha) \\      \end{bmatrix}$$ I thought that maybe if I transformed a point three times it would rotate it $3\theta$ degrees, i.e with a standard matrix $AAA$. The result was $$AAA=B$$ $$=$$$$ \begin{bmatrix}        \cos^{3}(\theta)-\cos(\theta)\sin^2(\theta)-2\cos(\theta)\sin^2(\theta) & -\sin(\theta)\cos^2(\theta)+\sin^3(\theta)-2\cos^2(\theta)\sin(\theta) \\        2\sin(\theta)\cos^2(\theta)-\sin^3(\theta)+\cos^2(\theta)\sin(\theta)  & -2\sin^3(\theta)\cos(\theta)-\sin^2(\theta)cos(\theta)+\cos^3(\theta)\ \\      \end{bmatrix}$$ As before I would say that $AAA$ is equal to a transformation matrix $$A'=\begin{bmatrix}        \cos (3\alpha) & -\sin (3\alpha) \\        \sin (3\alpha) & \cos (3\alpha) \\      \end{bmatrix}$$ Then setting $A'=B$ would result in some identities. They just seem quite long and I am not sure if what I have done is correct or checks out. Any help would be appreciated.",,['linear-algebra']
82,Symmetrizing matrix properties,Symmetrizing matrix properties,,"A symmetrizer $P$ is a $n\times n$ symmetric matrix such that for a $n\times n$ matrix $A$ it holds that $AP=PA^T$. There exists a symmetrizer for any square matrix, and in general it is not unique. Furthermore, if $A$ has complex eigenvalues, then there does not exist a positive definite $P$ that symmetrizes $A$. I am looking for more information (or literature) on existence and properties of symmetrizers. Particularly: Are there conditions on $A$ that ensure that a positive (semi-)definite $P$ exists? I am particularly interested in the case $A\in\mathbb{R}^{n\times n}$. Are there conditions on the existence of symmetrizer, that symmetrizes two distinct matrices $A_1, A_2 \in \mathbb{R}^{n\times n}$? What can be said about the spectrum of $PA$?","A symmetrizer $P$ is a $n\times n$ symmetric matrix such that for a $n\times n$ matrix $A$ it holds that $AP=PA^T$. There exists a symmetrizer for any square matrix, and in general it is not unique. Furthermore, if $A$ has complex eigenvalues, then there does not exist a positive definite $P$ that symmetrizes $A$. I am looking for more information (or literature) on existence and properties of symmetrizers. Particularly: Are there conditions on $A$ that ensure that a positive (semi-)definite $P$ exists? I am particularly interested in the case $A\in\mathbb{R}^{n\times n}$. Are there conditions on the existence of symmetrizer, that symmetrizes two distinct matrices $A_1, A_2 \in \mathbb{R}^{n\times n}$? What can be said about the spectrum of $PA$?",,"['linear-algebra', 'matrices']"
83,Concentration of measure on spheres with respect to a unitary of trace approximately zero,Concentration of measure on spheres with respect to a unitary of trace approximately zero,,"This question arose out of my attempt to understand how a unitary of trace approximately zero acts on the unit sphere of a $n$ -dimensional Hilbert space. First, some context: We note that, by concentration of measures for spheres, we have the following: Let $S^{n-1} = \{x \in \mathbb{R}^n: \|x\|_2 = 1\}$ denote the unit sphere of $\mathbb{R}^n$ . Let $v_n \in S^{n-1}$ be randomly chosen according to the canonical probability measure on $S^{n-1}$ . For any fixed $\epsilon > 0$ , as $n \to \infty$ , the probability that $v_n$ lies $\epsilon$ -close to any given equator goes to $1$ . See, for example, here . Reframing this in terms of orthogonal matrices, we see that an equator is exactly the the intersection of the unit sphere with the invariant subspace of an orthogonal matrix whose eigenvalues are all $1$ except one $-1$ with multiplicity $1$ . That is to say, we have the following: Let $v_n \in S^{n-1}$ be randomly chosen according to the canonical probability measure on $S^{n-1}$ . Fix $\epsilon > 0$ , and fix a sequence of matrices $T_n \in \mathbb{M}_n(\mathbb{R})$ s.t. $T_n$ is an orthogonal matrix with eigenvalue $1$ of multiplicity $n-1$ and eigenvalue $-1$ of multiplicity $1$ for all $n$ . Then the probability that $\|T_nv_n - v_n\|_2 < \epsilon$ goes to $1$ as $n \to \infty$ . By essentially the same proof, the complex version of this also holds, namely, the above result still holds if $v_n$ is instead randomly chosen on the unit sphere of $\mathbb{C}^n$ and $T_n \in \mathbb{M}_n(\mathbb{C})$ are unitary matrices with the same condition on eigenvalues. Now, I’m interested in analogues of this for unitary matrices with (normalized) trace close to $0$ . Drawing from the intuition from real Hilbert spaces and orthogonal matrices, it would seem that such matrices should have a large region on the unit sphere where the vectors in the region are sent to vectors approximately orthogonal to the original one. Then by a concentration of measure style argument, it should be the case that the probability that a uniformly random vector on the unit sphere will be sent to a vector orthogonal to it goes to $1$ as the dimension goes to $\infty$ . I’ve been unable to prove it, however. The following is the precise question statement: Let $v_n \in S^{2n-1} \subset \mathbb{C}^n$ be randomly chosen according to the canonical probability measure on $S^{2n-1}$ . Fix $\epsilon > 0$ , and fix a sequence of matrices $T_n \in \mathbb{M}_n(\mathbb{C})$ s.t. $T_n$ is a unitary matrix with $\frac{1}{n}\mathrm{Tr}(T_n) < \frac{\epsilon}{2}$ for all $n$ . Is it the case then that the probability that $|\langle T_nv_n, v_n \rangle| < \epsilon$ goes to $1$ as $n \to \infty$ ? Some thoughts on the matter: Again, this seems intuitively plausible. We also observe that the stronger condition that $\langle T_nv_n, v_n \rangle = 0$ is equivalent to (after diagonalizing $T_n$ ) $\sum_{i=1}^n \lambda_{ni}|(v_n)_i|^2 = 0$ , where $\lambda_{ni}$ are the eigenvalues of $T_n$ and $(v_n)_i$ is the $i$ -th coordinate of $v_n$ . This is a complex co-dimension $1$ condition, same as the equator example in the complex case. Furthermore, this condition is certainly satisfiable. Indeed, $\frac{1}{n}\mathrm{Tr}(T_n) < \frac{\epsilon}{2}$ implies that any $v_n$ with $|(v_n)_i| = \frac{1}{\sqrt{n}}$ for all $i$ satisfies the desired condition. And furthermore any $v$ that is $\frac{\epsilon}{4}$ -close to such an $v_n$ would satisfy the desired condition as well, again similar to the equator example. However, I’ve been unable to make much progress on this as I’m not quite familiar with the intricacies of the concentration of measure, and the same method as in the equator example doesn’t exactly apply. Any help on this is highly appreciated!","This question arose out of my attempt to understand how a unitary of trace approximately zero acts on the unit sphere of a -dimensional Hilbert space. First, some context: We note that, by concentration of measures for spheres, we have the following: Let denote the unit sphere of . Let be randomly chosen according to the canonical probability measure on . For any fixed , as , the probability that lies -close to any given equator goes to . See, for example, here . Reframing this in terms of orthogonal matrices, we see that an equator is exactly the the intersection of the unit sphere with the invariant subspace of an orthogonal matrix whose eigenvalues are all except one with multiplicity . That is to say, we have the following: Let be randomly chosen according to the canonical probability measure on . Fix , and fix a sequence of matrices s.t. is an orthogonal matrix with eigenvalue of multiplicity and eigenvalue of multiplicity for all . Then the probability that goes to as . By essentially the same proof, the complex version of this also holds, namely, the above result still holds if is instead randomly chosen on the unit sphere of and are unitary matrices with the same condition on eigenvalues. Now, I’m interested in analogues of this for unitary matrices with (normalized) trace close to . Drawing from the intuition from real Hilbert spaces and orthogonal matrices, it would seem that such matrices should have a large region on the unit sphere where the vectors in the region are sent to vectors approximately orthogonal to the original one. Then by a concentration of measure style argument, it should be the case that the probability that a uniformly random vector on the unit sphere will be sent to a vector orthogonal to it goes to as the dimension goes to . I’ve been unable to prove it, however. The following is the precise question statement: Let be randomly chosen according to the canonical probability measure on . Fix , and fix a sequence of matrices s.t. is a unitary matrix with for all . Is it the case then that the probability that goes to as ? Some thoughts on the matter: Again, this seems intuitively plausible. We also observe that the stronger condition that is equivalent to (after diagonalizing ) , where are the eigenvalues of and is the -th coordinate of . This is a complex co-dimension condition, same as the equator example in the complex case. Furthermore, this condition is certainly satisfiable. Indeed, implies that any with for all satisfies the desired condition. And furthermore any that is -close to such an would satisfy the desired condition as well, again similar to the equator example. However, I’ve been unable to make much progress on this as I’m not quite familiar with the intricacies of the concentration of measure, and the same method as in the equator example doesn’t exactly apply. Any help on this is highly appreciated!","n S^{n-1} = \{x \in \mathbb{R}^n: \|x\|_2 = 1\} \mathbb{R}^n v_n \in S^{n-1} S^{n-1} \epsilon > 0 n \to \infty v_n \epsilon 1 1 -1 1 v_n \in S^{n-1} S^{n-1} \epsilon > 0 T_n \in \mathbb{M}_n(\mathbb{R}) T_n 1 n-1 -1 1 n \|T_nv_n - v_n\|_2 < \epsilon 1 n \to \infty v_n \mathbb{C}^n T_n \in \mathbb{M}_n(\mathbb{C}) 0 1 \infty v_n \in S^{2n-1} \subset \mathbb{C}^n S^{2n-1} \epsilon > 0 T_n \in \mathbb{M}_n(\mathbb{C}) T_n \frac{1}{n}\mathrm{Tr}(T_n) < \frac{\epsilon}{2} n |\langle T_nv_n, v_n \rangle| < \epsilon 1 n \to \infty \langle T_nv_n, v_n \rangle = 0 T_n \sum_{i=1}^n \lambda_{ni}|(v_n)_i|^2 = 0 \lambda_{ni} T_n (v_n)_i i v_n 1 \frac{1}{n}\mathrm{Tr}(T_n) < \frac{\epsilon}{2} v_n |(v_n)_i| = \frac{1}{\sqrt{n}} i v \frac{\epsilon}{4} v_n","['linear-algebra', 'probability', 'measure-theory', 'operator-theory', 'concentration-of-measure']"
84,Subspaces with common images,Subspaces with common images,,"Let $X$ and $Y$ be finite dimensional vector spaces over $\mathbb{C}$ , and let $S,T:X\to Y$ be linear transformations. Is there a method for determining all subspaces $V\subseteq X$ such that $S(V)=T(V)$ (as subspaces not necessarily pointwise). I am especially interested in the case where $\text{dim} X \geq \text{dim} Y$ .","Let and be finite dimensional vector spaces over , and let be linear transformations. Is there a method for determining all subspaces such that (as subspaces not necessarily pointwise). I am especially interested in the case where .","X Y \mathbb{C} S,T:X\to Y V\subseteq X S(V)=T(V) \text{dim} X \geq \text{dim} Y","['linear-algebra', 'abstract-algebra', 'algorithms', 'linear-transformations']"
85,Inequality involving minors of a hermitian matrix,Inequality involving minors of a hermitian matrix,,"Let $A$ be an $n \times n$ hermitian matrix with $n \geq 3$ . I am trying to prove the following inequality involving its minors $$\left| \sum_{k=3}^n A_{3k} A_{[12k],[123]} \right| \leq \sqrt{\sum_{i < j} |A_{[13],[ij]}|^2}\sqrt{\sum_{i < j} |A_{[23],[ij]}|^2},$$ where the square brackets in subscripts contain the numbers of rows and columns involved in the given minor. The inequality holds trivially (and saturates) for diagonal matrices and passes all numerical tests with flying colours, but I am unable to crack it in the general case. It looks Cauchy-Schwarz-esque, but that resemblance did not lead me anywhere. Let me add that the above inequality can be written equivalently as $$|\langle u_1 \otimes u_2 \otimes A u_3, A u_1 \wedge A u_2 \wedge A u_3 \rangle | \leq \tfrac{1}{2} \|A u_1 \wedge A u_3 \| \|A u_2 \wedge A u_3 \|$$ for any hermitian $A \in B(\mathbb{C}^n)$ and any pairwise orthonormal $u_1,u_2,u_3 \in \mathbb{C}^n$ , where the wedge products are not normalized, i.e. $u \wedge v = u \otimes v - v \otimes u$ and similarly for the triple wedge product. The inner product on $(\mathbb{C}^n)^{\otimes 3}$ is of course the natural one. Any hint would be much appreciated!","Let be an hermitian matrix with . I am trying to prove the following inequality involving its minors where the square brackets in subscripts contain the numbers of rows and columns involved in the given minor. The inequality holds trivially (and saturates) for diagonal matrices and passes all numerical tests with flying colours, but I am unable to crack it in the general case. It looks Cauchy-Schwarz-esque, but that resemblance did not lead me anywhere. Let me add that the above inequality can be written equivalently as for any hermitian and any pairwise orthonormal , where the wedge products are not normalized, i.e. and similarly for the triple wedge product. The inner product on is of course the natural one. Any hint would be much appreciated!","A n \times n n \geq 3 \left| \sum_{k=3}^n A_{3k} A_{[12k],[123]} \right| \leq \sqrt{\sum_{i < j} |A_{[13],[ij]}|^2}\sqrt{\sum_{i < j} |A_{[23],[ij]}|^2}, |\langle u_1 \otimes u_2 \otimes A u_3, A u_1 \wedge A u_2 \wedge A u_3 \rangle | \leq \tfrac{1}{2} \|A u_1 \wedge A u_3 \| \|A u_2 \wedge A u_3 \| A \in B(\mathbb{C}^n) u_1,u_2,u_3 \in \mathbb{C}^n u \wedge v = u \otimes v - v \otimes u (\mathbb{C}^n)^{\otimes 3}","['linear-algebra', 'inequality', 'tensor-products', 'cauchy-schwarz-inequality', 'hermitian-matrices']"
86,Automorphisms for direct products of finite commutative nilpotent rings.,Automorphisms for direct products of finite commutative nilpotent rings.,,"Let $(R, +, \cdot)$ be an associative commutative nilpotent ring of cardinality $2^n$ such that $$ r^2 = 0, $$ for every $r\in R$ . Also $(V, +)$ is a vector space over $\mathbb{F}_2$ .  Let $\operatorname{Aut}(R)$ be a group of ring automorphisms. That is a group of bijective maps $\phi: R\to R $ such taht for every $r,s\in R$ and $*\in \{+, \cdot\}$ $$ \phi(r*s) = \phi(r)*\phi(s). $$ I want to know how the group of automorphisms for the direct product $$ \underbrace{R\oplus R \oplus\ldots \oplus R}_{k} $$ is structured. Of course it contains trivial diagonal automorphisms of the form $$ \Phi(r_1, \ldots, r_k) = (\phi_1(r_1), \ldots, \phi_k(r_k)), $$ where $\phi_i\in \operatorname{Aut}(R)$ . Also there are automorphisms acting as permutations, that is for $\sigma \in S_k$ $$ \Psi_{\sigma}(r_1, \ldots, r_k) = (r_{\sigma(1)}, \ldots, r_{\sigma(k)}). $$ But what about more subtle maps? It is interesting how the whole group of automorphisms for $\underbrace{R\oplus R \oplus\ldots \oplus R}_{k}$ is structured. My attempts. I'm trying to start from investigation for $$ R\oplus R $$ automorphisms. Let $\phi$ be such automorphism. Then since $\phi$ is $+$ automorphism there exist $\alpha, \beta, \gamma, \delta \in \operatorname{Hom_+}(R,R)$ such that $$ \phi(x,y) = (\alpha(x) + \beta(y), \gamma(x) + \delta(y)), $$ (this is usual matrix product). Then since $\phi$ preserves multiplication we have $$ \phi(xz,yt) = \phi(x,y)\phi(z,t), $$ for arbitrary $x,y,z,t\in R$ . Combining this with the above equality and using the fact that we working in vector space over $\mathbb{F}_2$ we can obtain that $\alpha,\beta, \gamma, \delta$ are also multiplicative homomorphisms. Then $\phi$ is automorphism if and only if for all $x,y,z,t\in R$ we have $$ \begin{array}{l} \alpha(x)\beta(t) = \beta(y)\alpha(z) \\ \gamma(x)\delta(t) = \delta(y)\gamma(z) \\ \end{array} $$ and $\phi$ is bijection. Next, I'm stuck.","Let be an associative commutative nilpotent ring of cardinality such that for every . Also is a vector space over .  Let be a group of ring automorphisms. That is a group of bijective maps such taht for every and I want to know how the group of automorphisms for the direct product is structured. Of course it contains trivial diagonal automorphisms of the form where . Also there are automorphisms acting as permutations, that is for But what about more subtle maps? It is interesting how the whole group of automorphisms for is structured. My attempts. I'm trying to start from investigation for automorphisms. Let be such automorphism. Then since is automorphism there exist such that (this is usual matrix product). Then since preserves multiplication we have for arbitrary . Combining this with the above equality and using the fact that we working in vector space over we can obtain that are also multiplicative homomorphisms. Then is automorphism if and only if for all we have and is bijection. Next, I'm stuck.","(R, +, \cdot) 2^n 
r^2 = 0,
 r\in R (V, +) \mathbb{F}_2 \operatorname{Aut}(R) \phi: R\to R  r,s\in R *\in \{+, \cdot\} 
\phi(r*s) = \phi(r)*\phi(s).
 
\underbrace{R\oplus R \oplus\ldots \oplus R}_{k}
 
\Phi(r_1, \ldots, r_k) = (\phi_1(r_1), \ldots, \phi_k(r_k)),
 \phi_i\in \operatorname{Aut}(R) \sigma \in S_k 
\Psi_{\sigma}(r_1, \ldots, r_k) = (r_{\sigma(1)}, \ldots, r_{\sigma(k)}).
 \underbrace{R\oplus R \oplus\ldots \oplus R}_{k} 
R\oplus R
 \phi \phi + \alpha, \beta, \gamma, \delta \in \operatorname{Hom_+}(R,R) 
\phi(x,y) = (\alpha(x) + \beta(y), \gamma(x) + \delta(y)),
 \phi 
\phi(xz,yt) = \phi(x,y)\phi(z,t),
 x,y,z,t\in R \mathbb{F}_2 \alpha,\beta, \gamma, \delta \phi x,y,z,t\in R 
\begin{array}{l}
\alpha(x)\beta(t) = \beta(y)\alpha(z) \\
\gamma(x)\delta(t) = \delta(y)\gamma(z) \\
\end{array}
 \phi","['linear-algebra', 'abstract-algebra', 'ring-theory', 'finite-groups', 'automorphism-group']"
87,Encoding primes via ranks of sign matrices,Encoding primes via ranks of sign matrices,,"Crossposted at MathOverflow Recently I came across a very simply defined family of matrices: for $n \in \mathbb{N}$ , set $A_n := (a_{ij})_{1 \le i, j \le n}$ , where $$\displaystyle a_{ij} := (-1)^{\big\lfloor \dfrac{2(i-1)(j-1)}{n} \big\rfloor}$$ These are normalized $\pm 1$ symmetric $n \times n$ matrices. The first few are: $$ A_2 = \begin{bmatrix}      1&1\\      1&-1      \end{bmatrix}, A_3 = \begin{bmatrix}      1&1&1\\      1&1&-1\\      1&-1&1      \end{bmatrix}, A_4 = \begin{bmatrix}      1&1&1&1\\      1&1&-1&-1\\      1&-1&1&-1\\      1&-1&-1&1      \end{bmatrix}, \ldots $$ Computing $\operatorname{rank}(A_n)$ for small $n$ quickly suggests a pattern: $$\operatorname{rank}(A_n) = \sigma_0(n) + \Big\lfloor \frac{n-1}{2} \Big\rfloor$$ where $\sigma_0(n)$ is the number (= sum of $0^\text{th}$ powers) of divisors of $n$ . My question is: Is this formula for $\operatorname{rank}(A_n)$ true for all $n$ ? If so, then since the minimal value of $\sigma_0$ is $2$ , which occurs exactly for prime $n$ , one would have $\operatorname{rank}(A_n) = \big\lfloor \frac{n+3}{2} \big\rfloor$ is minimal $\iff n$ is prime. (This would, in my opinion, be an interesting encoding of the primes in a purely linear-algebraic fashion.) I have tested this up to $n = 30$ . To save some trouble, this is A361003 in OEIS (coincidentally just added last week!). A combinatorial proof e.g. via A361001 would be fine. If anyone knows more about this family of matrices I would be happy to read more.","Crossposted at MathOverflow Recently I came across a very simply defined family of matrices: for , set , where These are normalized symmetric matrices. The first few are: Computing for small quickly suggests a pattern: where is the number (= sum of powers) of divisors of . My question is: Is this formula for true for all ? If so, then since the minimal value of is , which occurs exactly for prime , one would have is minimal is prime. (This would, in my opinion, be an interesting encoding of the primes in a purely linear-algebraic fashion.) I have tested this up to . To save some trouble, this is A361003 in OEIS (coincidentally just added last week!). A combinatorial proof e.g. via A361001 would be fine. If anyone knows more about this family of matrices I would be happy to read more.","n \in \mathbb{N} A_n := (a_{ij})_{1 \le i, j \le n} \displaystyle a_{ij} := (-1)^{\big\lfloor \dfrac{2(i-1)(j-1)}{n} \big\rfloor} \pm 1 n \times n 
A_2 = \begin{bmatrix}
     1&1\\
     1&-1
     \end{bmatrix},
A_3 = \begin{bmatrix}
     1&1&1\\
     1&1&-1\\
     1&-1&1
     \end{bmatrix},
A_4 = \begin{bmatrix}
     1&1&1&1\\
     1&1&-1&-1\\
     1&-1&1&-1\\
     1&-1&-1&1
     \end{bmatrix}, \ldots
 \operatorname{rank}(A_n) n \operatorname{rank}(A_n) = \sigma_0(n) + \Big\lfloor \frac{n-1}{2} \Big\rfloor \sigma_0(n) 0^\text{th} n \operatorname{rank}(A_n) n \sigma_0 2 n \operatorname{rank}(A_n) = \big\lfloor \frac{n+3}{2} \big\rfloor \iff n n = 30","['linear-algebra', 'combinatorics', 'matrices', 'number-theory', 'elementary-number-theory']"
88,Is there a subgroup of the general linear group isomorphic to the general linear group?,Is there a subgroup of the general linear group isomorphic to the general linear group?,,"Let $\mathbb{V}$ be a finite-dimensional vector space over some field $K$ . (Let us consider the case $K$ is characteristic 0 and the dimension of V is greater than one ) Is there a proper subgroup of $GL( \mathbb{V})$ isomorphic to $GL( \mathbb{V})$ ? Also, the same question for $SL( \mathbb{V})$ . If you do not know the answer, please give any idea/reference on approaching to the problem. My intuition is the answer is no, but I could not show it.","Let be a finite-dimensional vector space over some field . (Let us consider the case is characteristic 0 and the dimension of V is greater than one ) Is there a proper subgroup of isomorphic to ? Also, the same question for . If you do not know the answer, please give any idea/reference on approaching to the problem. My intuition is the answer is no, but I could not show it.",\mathbb{V} K K GL( \mathbb{V}) GL( \mathbb{V}) SL( \mathbb{V}),"['linear-algebra', 'group-theory', 'infinite-groups']"
89,"In a Clifford algebra over an $n$-dimensional non-degenerate space, is a product of any number of vectors a product of at most $n$ vectors?","In a Clifford algebra over an -dimensional non-degenerate space, is a product of any number of vectors a product of at most  vectors?",n n,"In a Clifford algebra over an $n$ -dimensional vector space $V$ with a non-degenerate quadratic/bilinear form, can any product of vectors $a_1,a_2,\cdots,a_m\in V$ be written as $$a_1a_2\cdots a_m=a_1'a_2'\cdots a_k'$$ for some $k\leq n$ and $a_1',a_2',\cdots,a_k'\in V$ ? Due to the grading on the algebra, necessarily $k\equiv m\bmod2$ (unless the product vanishes). It suffices to prove that a product of $n+1$ vectors can be reduced to a product of $n-1$ vectors. If the vectors are all invertible ( $a^{-1}=\frac{a}{a\cdot a}$ ), then they represent reflections ( $v\mapsto-ava^{-1}=v-2\frac{v\cdot a}{a\cdot a}a$ ), and the product represents an isometry of $V$ . According to the Cartan-Dieudonne theorem, any isometry is the composition of at most $n$ reflections, so we get the desired result. (I could make the correspondence between multivectors and isometries more explicit, but that's beside the point of this question.) So, suppose some of the vectors are null ( $a\cdot a=0$ ), and thus don't represent isometries. Non-degeneracy means, for $a\neq0$ , there's some vector $v\in V$ such that $a\cdot v\neq0$ . To see that this condition is necessary, consider $V=\mathbb R^3$ with the degenerate quadratic form $v=(v_1e_1+v_2e_2+v_3e_3)\mapsto v\cdot v=v_1^2-v_2^2+0v_3^2$ . (This space may be denoted $\mathbb R^{1,1,1}$ .) The product of four unit vectors $$e_1(e_1+e_3)e_2(-e_2+e_3)=1+(e_1+e_2)e_3$$ cannot be written as a product of two vectors; their wedge product would have to be $(e_1+e_2)e_3$ , so they'd be in the span of the null vectors $e_1+e_2$ and $e_3$ , but then their dot product would be $0$ , not $1$ as required. I've proven this for $n=2,3,4$ ( $n=1$ requires a scalar in front of the empty product). See the Degenerate Cartan-Dieudonne chat . I also thought through a proof for $n=5$ , but I didn't write it down, and it's a bit complicated. Anyway, the methods I used won't work for $n\geq6$ . I relied on the fact that the product of vectors, $P=a_1a_2\cdots a_{n+1}$ , has only grades $\equiv n+1\bmod2$ , and has $P\tilde P$ and $\tilde PP$ being scalars. These latter properties alone imply that $P$ is a product of vectors, only in low dimensions. Over $\mathbb R^6$ , the odd multivector $P=e_1e_2e_3+e_4e_5e_6$ has $P\tilde P=\tilde PP=2$ , but $P$ is not a product of vectors, because $Pe_1\tilde P$ is not a vector (it's a $5$ -vector).","In a Clifford algebra over an -dimensional vector space with a non-degenerate quadratic/bilinear form, can any product of vectors be written as for some and ? Due to the grading on the algebra, necessarily (unless the product vanishes). It suffices to prove that a product of vectors can be reduced to a product of vectors. If the vectors are all invertible ( ), then they represent reflections ( ), and the product represents an isometry of . According to the Cartan-Dieudonne theorem, any isometry is the composition of at most reflections, so we get the desired result. (I could make the correspondence between multivectors and isometries more explicit, but that's beside the point of this question.) So, suppose some of the vectors are null ( ), and thus don't represent isometries. Non-degeneracy means, for , there's some vector such that . To see that this condition is necessary, consider with the degenerate quadratic form . (This space may be denoted .) The product of four unit vectors cannot be written as a product of two vectors; their wedge product would have to be , so they'd be in the span of the null vectors and , but then their dot product would be , not as required. I've proven this for ( requires a scalar in front of the empty product). See the Degenerate Cartan-Dieudonne chat . I also thought through a proof for , but I didn't write it down, and it's a bit complicated. Anyway, the methods I used won't work for . I relied on the fact that the product of vectors, , has only grades , and has and being scalars. These latter properties alone imply that is a product of vectors, only in low dimensions. Over , the odd multivector has , but is not a product of vectors, because is not a vector (it's a -vector).","n V a_1,a_2,\cdots,a_m\in V a_1a_2\cdots a_m=a_1'a_2'\cdots a_k' k\leq n a_1',a_2',\cdots,a_k'\in V k\equiv m\bmod2 n+1 n-1 a^{-1}=\frac{a}{a\cdot a} v\mapsto-ava^{-1}=v-2\frac{v\cdot a}{a\cdot a}a V n a\cdot a=0 a\neq0 v\in V a\cdot v\neq0 V=\mathbb R^3 v=(v_1e_1+v_2e_2+v_3e_3)\mapsto v\cdot v=v_1^2-v_2^2+0v_3^2 \mathbb R^{1,1,1} e_1(e_1+e_3)e_2(-e_2+e_3)=1+(e_1+e_2)e_3 (e_1+e_2)e_3 e_1+e_2 e_3 0 1 n=2,3,4 n=1 n=5 n\geq6 P=a_1a_2\cdots a_{n+1} \equiv n+1\bmod2 P\tilde P \tilde PP P \mathbb R^6 P=e_1e_2e_3+e_4e_5e_6 P\tilde P=\tilde PP=2 P Pe_1\tilde P 5","['linear-algebra', 'quadratic-forms', 'clifford-algebras']"
90,"Axler ""Linear Algebra Done Right"" Exercise 6.B.13","Axler ""Linear Algebra Done Right"" Exercise 6.B.13",,"This exercise appears in Section 6.B ""Orthonormal Bases"" in Linear Algebra Done Right by Sheldon Axler.  Inner product spaces, norms, orthogonality, and orthonormal bases have been introduced. The unofficial solution manual I am using presents a somewhat involved proof of this exercise that involves induction and the Gram-Schmidt procedure, which I will not reproduce here.  However, I arrived at an alternative proof that is much simpler in my opinion.  I would like to check if my proof is correct. $(V, \langle \cdot, \cdot \rangle)$ is an inner product space over the field $\mathbb{F}$ , which stands for either $\mathbb{R}$ or $\mathbb{C}$ .  (Axler, pp. 4, 167) Suppose $v_1, \dotsc, v_m$ is a linearly independent list in $V$ .  Show that there exists $w \in V$ such that $\langle w, v_j \rangle > 0$ for all $j \in \{1, \dotsc, m\}$ .  (Axler, p. 191) Proof.  Let $U = \operatorname{span}(v_1, \dotsc, v_m)$ .  Consider the linear map $\phi : U \to \mathbb{F}^m$ defined by $$ \phi(u) = (\langle u, v_1 \rangle, \dotsc, \langle u, v_m \rangle). $$ We show that $\phi$ is injective.  Suppose that $u \in U$ and $\phi(u) = 0$ .  Then, $$ \langle u, v_1 \rangle = \dotsb = \langle u, v_m \rangle = 0. $$ Since $u \in \operatorname{span}(v_1, \dotsc, v_m)$ , we have $\langle u, u \rangle = 0$ , so $u = 0$ .  Hence, $\phi$ is injective.  But $\dim U = \dim \mathbb{F}^m = m$ , so $\phi$ is surjective.  Choose $w \in U$ such that $$ \phi(w) = (1, \dotsc, 1). $$ Therefore, $\langle w, v_j \rangle = 1 > 0$ for all $j \in \{1, \dotsc, m\}$ .","This exercise appears in Section 6.B ""Orthonormal Bases"" in Linear Algebra Done Right by Sheldon Axler.  Inner product spaces, norms, orthogonality, and orthonormal bases have been introduced. The unofficial solution manual I am using presents a somewhat involved proof of this exercise that involves induction and the Gram-Schmidt procedure, which I will not reproduce here.  However, I arrived at an alternative proof that is much simpler in my opinion.  I would like to check if my proof is correct. is an inner product space over the field , which stands for either or .  (Axler, pp. 4, 167) Suppose is a linearly independent list in .  Show that there exists such that for all .  (Axler, p. 191) Proof.  Let .  Consider the linear map defined by We show that is injective.  Suppose that and .  Then, Since , we have , so .  Hence, is injective.  But , so is surjective.  Choose such that Therefore, for all .","(V, \langle \cdot, \cdot \rangle) \mathbb{F} \mathbb{R} \mathbb{C} v_1, \dotsc, v_m V w \in V \langle w, v_j \rangle > 0 j \in \{1, \dotsc, m\} U = \operatorname{span}(v_1, \dotsc, v_m) \phi : U \to \mathbb{F}^m 
\phi(u) = (\langle u, v_1 \rangle, \dotsc, \langle u, v_m \rangle).
 \phi u \in U \phi(u) = 0 
\langle u, v_1 \rangle = \dotsb = \langle u, v_m \rangle = 0.
 u \in \operatorname{span}(v_1, \dotsc, v_m) \langle u, u \rangle = 0 u = 0 \phi \dim U = \dim \mathbb{F}^m = m \phi w \in U 
\phi(w) = (1, \dotsc, 1).
 \langle w, v_j \rangle = 1 > 0 j \in \{1, \dotsc, m\}","['linear-algebra', 'solution-verification', 'vector-spaces', 'linear-transformations', 'inner-products']"
91,How many ways are there to prove Cayley-Hamilton Theorem?,How many ways are there to prove Cayley-Hamilton Theorem?,,"I see many proofs for the Cayley-Hamilton Theorem in textbooks and net, so I want to know how many proofs are there for this important and applicable theorem?","I see many proofs for the Cayley-Hamilton Theorem in textbooks and net, so I want to know how many proofs are there for this important and applicable theorem?",,"['linear-algebra', 'abstract-algebra']"
92,Harmonic Numbers' Numerators Divisible by a Prime $p$,Harmonic Numbers' Numerators Divisible by a Prime,p,"For a prime $p$ , I am trying to determine the set of all $n$ for which the numerator of $H_n$ is divisible by $p$ , with $H_n$ being the $n$ 'th harmonic number. After going through a lot of literature, this turns out to be a difficult thing to do. The most promising paper regarding my question seems to be this paper by David W. Boyd . (PDFs easily accessible over the internet). Because the problem is difficult, I am trying to focus on primes $p<550$ as suggested by the paper. Using similiar notation to the paper, let $J_p$ be the set of those $n$ for which the numerator of $H_n$ is divisible by $p$ . They give in the paper: $J_3= \{2,7,22\}$ $J_5= \{4,20,24\}$ And I ran a computer program for quite some time to find: $J_7 = \{6,42,48,295,299,337,341,2096,2390,14675,16731,16735,102728...\}$ But I don't know if that's a complete list. EDIT: this turns out to be a complete list I believe this is the core of their technique / algorithm: Similiarly define, for a given $p$ : $G_m=\{p^{m-1}\leq n < p^m : p \mid H_n\} \quad  G_0=\{0\}$ Because it is known that $p \mid H_{p-1}$ , it must be that $p-1 \in G_1$ But I don't understand the next formulas that follow in the paper: $H_n=ap + O(p^2) , \quad H_{pn+k} = a + H_k + O(p) , \quad a + H_k = O(p)$ Does it mean that given the other parameters in the RHS of the equation, I can get the LHS in $O(p)$ time? Also, what is the variable $a$ exactly? It's not mentioned in the paper beforehand. How do I find it? Looking at the values in the given $J_p$ , it seems the solutions come in ""subsets"", so that solutions in subset $s_{i+1} \in S_{i+1}$ are formed from solutions $s \in S_i$ such that every $ps \leq s_{i+1} < ps + p$ for some $s$ . For example, looking at $J_7$ , then $42$ can generate solutions in the range $[7\cdot 42 , 7 \cdot 42 + 7]=[294,301]$ , similiarly $48$ generates solutions in the range $[7\cdot 48 , 7 \cdot 48 + 7]=[336,343]$ . And they both generate $2$ solutions each. This means I would only need to check divisibility of numerators in a small range (assuming $p$ is small), resulting in $O(mp)$ algorithm, with $m$ defined as above. Is that observation correct? I think it is similiar to how they define $G_m$ . But How can I quickly obtain $H_n$ mod $p$ for some $n$ using previous values? Only a few values need to be checked, but as $m$ grows, the numerators become extremely large, about $p^m$ . Another paper I found gives a more detailed example of the technique: p-Integral harmonic sums Answers regarding any of my questions will be great. The best answer I can hope for is an explanation of their technique, what they do exactly and how they do it. A sketch for their technique revolving around $J_7$ as an example would be great, going through each of the $m$ 's before $G_m$ becomes empty.","For a prime , I am trying to determine the set of all for which the numerator of is divisible by , with being the 'th harmonic number. After going through a lot of literature, this turns out to be a difficult thing to do. The most promising paper regarding my question seems to be this paper by David W. Boyd . (PDFs easily accessible over the internet). Because the problem is difficult, I am trying to focus on primes as suggested by the paper. Using similiar notation to the paper, let be the set of those for which the numerator of is divisible by . They give in the paper: And I ran a computer program for quite some time to find: But I don't know if that's a complete list. EDIT: this turns out to be a complete list I believe this is the core of their technique / algorithm: Similiarly define, for a given : Because it is known that , it must be that But I don't understand the next formulas that follow in the paper: Does it mean that given the other parameters in the RHS of the equation, I can get the LHS in time? Also, what is the variable exactly? It's not mentioned in the paper beforehand. How do I find it? Looking at the values in the given , it seems the solutions come in ""subsets"", so that solutions in subset are formed from solutions such that every for some . For example, looking at , then can generate solutions in the range , similiarly generates solutions in the range . And they both generate solutions each. This means I would only need to check divisibility of numerators in a small range (assuming is small), resulting in algorithm, with defined as above. Is that observation correct? I think it is similiar to how they define . But How can I quickly obtain mod for some using previous values? Only a few values need to be checked, but as grows, the numerators become extremely large, about . Another paper I found gives a more detailed example of the technique: p-Integral harmonic sums Answers regarding any of my questions will be great. The best answer I can hope for is an explanation of their technique, what they do exactly and how they do it. A sketch for their technique revolving around as an example would be great, going through each of the 's before becomes empty.","p n H_n p H_n n p<550 J_p n H_n p J_3= \{2,7,22\} J_5= \{4,20,24\} J_7 = \{6,42,48,295,299,337,341,2096,2390,14675,16731,16735,102728...\} p G_m=\{p^{m-1}\leq n < p^m : p \mid H_n\} \quad  G_0=\{0\} p \mid H_{p-1} p-1 \in G_1 H_n=ap + O(p^2) , \quad H_{pn+k} = a + H_k + O(p) , \quad a + H_k = O(p) O(p) a J_p s_{i+1} \in S_{i+1} s \in S_i ps \leq s_{i+1} < ps + p s J_7 42 [7\cdot 42 , 7 \cdot 42 + 7]=[294,301] 48 [7\cdot 48 , 7 \cdot 48 + 7]=[336,343] 2 p O(mp) m G_m H_n p n m p^m J_7 m G_m","['linear-algebra', 'number-theory', 'discrete-mathematics', 'algorithms', 'computer-science']"
93,Uniqueness of a constrained system of linear equations,Uniqueness of a constrained system of linear equations,,"I would like to determine whether there exists a solution (and if so, check uniqueness) to the following system of linear equations (with respect to $\eta = (\eta_1,...,\eta_J))$ : $$\begin{aligned} \sum_{j=1}^J (\psi_i -y_j)a_{ij}\eta_j &= 0, \hspace{1em} \forall i=1,...,I,  \\ \sum_{j=1}^J \eta_j &= 1, \\ \eta_j &\geq 0, \hspace{1em} \forall j=1,...,J \end{aligned},$$ where there are the following constraints: $a_{ij} \in [0,1] \hspace{1em} \forall i,j$ if we rearrange y's in an increasing order $y_1 < y_2 < ... < y_J$ , then the following inequalities hold $y_1 \leq \psi_i \leq y_J, \hspace{1em} \forall i=1,...,I$ . Is there a way to determine whether solution exists in general case for any $I$ and $J$ ? And if there are solutions - how to determine uniqueness? I solved the most simple case for $I=J=2$ . I ""deleted"" the $I$ th equation and solved the following system: $$\begin{aligned}(\psi_1 - y_1)a_{11}\eta_1 + (\psi_1-y_2)a_{12}\eta_2 &= 0, \\ \eta_1+\eta_2 &= 1, \\ \eta_1 &\geq 0, \\ \eta_2 &\geq 0.      \end{aligned}$$ Assuming without loss of generality that $y_1 < y_2$ we obtain the following: $$\begin{aligned}\eta_1 &= \frac{a_{12}(y_2-\psi_1)}{a_{11}(\psi_1-y_1) + a_{12}(y_2-\psi_1)}, \\ \eta_2 &= \frac{a_{11}(\psi_1-y_1)}{a_{11}(\psi_1-y_1) + a_{12}(y_2-\psi_1)}.     \end{aligned}$$ $\eta$ 's must be nonnegative, so this implies that either of the following two conditions must hold: $a_{12}(y_2-\psi_1) \geq 0$ , $a_{11}(\psi_1-y_1) \geq 0$ , and $a_{11}(\psi_1-y_1) + a_{12}(y_2-\psi_1) \geq 0 ;$ $a_{12}(y_2-\psi_1) \leq 0$ , $a_{11}(\psi_1-y_1) \leq 0$ , and $a_{11}(\psi_1-y_1) + a_{12}(y_2-\psi_1) \leq 0 ;$ Which due to the constraints on $a_{ij}$ 's lead to the equivalent conditions: $y_1 \leq \psi_1 \leq y_2$ and $a_{11}(\psi_1 - y_1) + a_{12}(y_2-\psi_1) \geq 0$ ; $y_2 \leq \psi_1 \leq y_1$ and $a_{11}(\psi_1 - y_1) + a_{12}(y_2-\psi_1) \leq 0$ . And now it is easily seen that the first case occurs when $y_1 \leq \psi_1 \leq y_2$ and the second case cannot occur, since $y_2 > y_1$ . If this solution does not satisfy the ""deleted"" equation, then there is no solution. But if it does, then the solution is unique.","I would like to determine whether there exists a solution (and if so, check uniqueness) to the following system of linear equations (with respect to : where there are the following constraints: if we rearrange y's in an increasing order , then the following inequalities hold . Is there a way to determine whether solution exists in general case for any and ? And if there are solutions - how to determine uniqueness? I solved the most simple case for . I ""deleted"" the th equation and solved the following system: Assuming without loss of generality that we obtain the following: 's must be nonnegative, so this implies that either of the following two conditions must hold: , , and , , and Which due to the constraints on 's lead to the equivalent conditions: and ; and . And now it is easily seen that the first case occurs when and the second case cannot occur, since . If this solution does not satisfy the ""deleted"" equation, then there is no solution. But if it does, then the solution is unique.","\eta = (\eta_1,...,\eta_J)) \begin{aligned} \sum_{j=1}^J (\psi_i -y_j)a_{ij}\eta_j &= 0, \hspace{1em} \forall i=1,...,I,  \\ \sum_{j=1}^J \eta_j &= 1, \\ \eta_j &\geq 0, \hspace{1em} \forall j=1,...,J \end{aligned}, a_{ij} \in [0,1] \hspace{1em} \forall i,j y_1 < y_2 < ... < y_J y_1 \leq \psi_i \leq y_J, \hspace{1em} \forall i=1,...,I I J I=J=2 I \begin{aligned}(\psi_1 - y_1)a_{11}\eta_1 + (\psi_1-y_2)a_{12}\eta_2 &= 0, \\ \eta_1+\eta_2 &= 1, \\ \eta_1 &\geq 0, \\ \eta_2 &\geq 0.      \end{aligned} y_1 < y_2 \begin{aligned}\eta_1 &= \frac{a_{12}(y_2-\psi_1)}{a_{11}(\psi_1-y_1) + a_{12}(y_2-\psi_1)}, \\ \eta_2 &= \frac{a_{11}(\psi_1-y_1)}{a_{11}(\psi_1-y_1) + a_{12}(y_2-\psi_1)}.     \end{aligned} \eta a_{12}(y_2-\psi_1) \geq 0 a_{11}(\psi_1-y_1) \geq 0 a_{11}(\psi_1-y_1) + a_{12}(y_2-\psi_1) \geq 0 ; a_{12}(y_2-\psi_1) \leq 0 a_{11}(\psi_1-y_1) \leq 0 a_{11}(\psi_1-y_1) + a_{12}(y_2-\psi_1) \leq 0 ; a_{ij} y_1 \leq \psi_1 \leq y_2 a_{11}(\psi_1 - y_1) + a_{12}(y_2-\psi_1) \geq 0 y_2 \leq \psi_1 \leq y_1 a_{11}(\psi_1 - y_1) + a_{12}(y_2-\psi_1) \leq 0 y_1 \leq \psi_1 \leq y_2 y_2 > y_1","['linear-algebra', 'algebra-precalculus', 'systems-of-equations']"
94,2 questions on Page 6 of Hoffman Kunze ( Linear Algebra),2 questions on Page 6 of Hoffman Kunze ( Linear Algebra),,"While studying Linear Algebra from Hoffman Kunze I have following two questions : I.3. Matrices and Elementary Row Operations One cannot fail to notice that in forming linear combinations of linear equations there is no need to continue writing the ""unknowns' $x_{1}, \ldots, x_{n},$ since one actually computes only with the coefficients $A_{i j}$ and the scalars $y_{i} .$ We shall now abbreviate the system (1-1) by $$AX=Y$$ where $$A=\begin{bmatrix}A_{11}&\cdots&A_{1n}\\\vdots&&\vdots\\A_{m1}&\cdots&A_{mn}\end{bmatrix}\\X=\begin{bmatrix}x_1\\\vdots \\x_n\end{bmatrix}\text { and }Y=\begin{bmatrix}y_1\\\vdots\\y_m\end{bmatrix}$$ We call $A$ the matrix of coefficients of the system. Strictly speaking, the rectangular array displayed above is not a matrix, but is a representation of a matrix. An $m \times n$ matrix over the field $F$ is a function $A$ from the set of pairs of integers $(i, j), 1 \leq i \leq m, 1 \leq j \leq n,$ into the field $F$ . The entries of the matrix $A$ are the scalars $A(i, j)=A_{i j},$ and quite often it is most convenient to describe the matrix by displaying its entries in a rectangular array having $m$ rows and $n$ columns, as above. Thus $X$ (above) is, or defines, an $n \times 1$ matrix and $Y$ is an $m \times 1$ matrix. For the time being, $A X=Y$ is nothing more than a shorthand notation for our system of linear equations. Later, when we have defined a multiplication for matrices, it will mean that $Y$ is the product of $A$ and $X$ Questions-> (1) Why in 2nd paragraph author wrote that the rectangular array displayed above is not a matrix? (2) In 2nd line of 2nd paragraph author wrote the definition of matrix. How is a matrix a function from set of pair of integers into the field? To me this definition given contradicts the definition given on Wikipedia and I can't understand it. Definition on Wikipedia: https://en.m.wikipedia.org/wiki/Matrix_(mathematics) Can kindly anyone explain why I am confused .","While studying Linear Algebra from Hoffman Kunze I have following two questions : I.3. Matrices and Elementary Row Operations One cannot fail to notice that in forming linear combinations of linear equations there is no need to continue writing the ""unknowns' since one actually computes only with the coefficients and the scalars We shall now abbreviate the system (1-1) by where We call the matrix of coefficients of the system. Strictly speaking, the rectangular array displayed above is not a matrix, but is a representation of a matrix. An matrix over the field is a function from the set of pairs of integers into the field . The entries of the matrix are the scalars and quite often it is most convenient to describe the matrix by displaying its entries in a rectangular array having rows and columns, as above. Thus (above) is, or defines, an matrix and is an matrix. For the time being, is nothing more than a shorthand notation for our system of linear equations. Later, when we have defined a multiplication for matrices, it will mean that is the product of and Questions-> (1) Why in 2nd paragraph author wrote that the rectangular array displayed above is not a matrix? (2) In 2nd line of 2nd paragraph author wrote the definition of matrix. How is a matrix a function from set of pair of integers into the field? To me this definition given contradicts the definition given on Wikipedia and I can't understand it. Definition on Wikipedia: https://en.m.wikipedia.org/wiki/Matrix_(mathematics) Can kindly anyone explain why I am confused .","x_{1}, \ldots, x_{n}, A_{i j} y_{i} . AX=Y A=\begin{bmatrix}A_{11}&\cdots&A_{1n}\\\vdots&&\vdots\\A_{m1}&\cdots&A_{mn}\end{bmatrix}\\X=\begin{bmatrix}x_1\\\vdots \\x_n\end{bmatrix}\text { and }Y=\begin{bmatrix}y_1\\\vdots\\y_m\end{bmatrix} A m \times n F A (i, j), 1 \leq i \leq m, 1 \leq j \leq n, F A A(i, j)=A_{i j}, m n X n \times 1 Y m \times 1 A X=Y Y A X",['linear-algebra']
95,Where am I wrong in this try to prove Stokes' theorem for a parallelogram?,Where am I wrong in this try to prove Stokes' theorem for a parallelogram?,,"So in order to understand Stokes' theorem, I tried to prove it for a parallelogram $P$ with parameterized boundary $R(t)=(x(t),y(t))$ spanned by two vectors $\mathbf{u}$ and $\mathbf{v}$ . The parallelogram : So that for some covector field ""1-form"" $\omega$ : $$\int_{\partial P}\omega = \int_{P}d\omega$$ With the edges having $t=0,1,2,3$ I think this reduces to: $$\omega_{R(0)}(\mathbf{u})+\omega_{R(1)}(\mathbf{v})-\omega_{R(2)}(\mathbf{u})-\omega_{R(3)}(\mathbf{v})=d\omega(\mathbf{u}\wedge\mathbf{v})$$ And this is my try: $$\mathbf{u}=R(1)-R(0)=R(2)-R(3)\\ \mathbf{v}=R(2)-R(1)=R(3)-R(0)$$ Let $ω_t=ω_{R(t)}$ so: $$\omega_{0}(\mathbf{u})+\omega_{1}(\mathbf{v})-\omega_{2}(\mathbf{u})-\omega_{3}(\mathbf{v})=(ω_0-ω_2)(\mathbf{u})+(ω_1-ω_3)(\mathbf{v})$$ Let: $$ω_1-ω_0=ω_2-ω_3= \Delta_{\mathbf{u}}~ω~,~ω_2-ω_1=ω_3-ω_0= \Delta_{\mathbf{v}}~ω~,\\\Delta_{\mathbf{u}}~ω(\mathbf{v})=\Deltaω(\mathbf{u},\mathbf{v})$$ So: $$(ω_0-ω_2)(\mathbf{u})+(ω_1-ω_3)(\mathbf{v})=\\(-\Delta_{\mathbf{u}} ~ω-\Delta_{\mathbf{v}}~ω)(\mathbf{u})+(\Delta_{\mathbf{u}} ~ω-\Delta_{\mathbf{v}}~ω)(\mathbf{v})=\\ -\Delta_{\mathbf{u}} ~ω(\mathbf{u})-\Delta_{\mathbf{v}}~ω(\mathbf{u})+\Delta_{\mathbf{u}} ~ω(\mathbf{v})-\Delta_{\mathbf{v}}~ω(\mathbf{v})=\\- \Deltaω(\mathbf{u},{\mathbf{u}})-\Deltaω(\mathbf{v},{\mathbf{u}})+\Delta ω(\mathbf{u},{\mathbf{v}})-\Deltaω(\mathbf{v},{\mathbf{v}})$$ I'm not quite sure how to continue from here but if $-\Deltaω(\mathbf{v},{\mathbf{v}})-\Deltaω(\mathbf{u},{\mathbf{u}}) =0$ , then let: $$dω(\mathbf{u},\mathbf{v})=\Delta ω(\mathbf{u},{\mathbf{v}})-\Deltaω(\mathbf{v},{\mathbf{u}})$$ It's easy to proof that $d\omega$ is multi-linear and anti-symmetric so: $$d\omega(\mathbf{u},\mathbf{v})=d\omega(\mathbf{u}\wedge \mathbf{v})$$ So I know I probably made a lot of mistakes but is the core of this way to prove it right? Are there any papers who proof Stokes' Theorem in general by proving it for a parallelogram or parallelepiped spanned by vectors? Because it makes much sense and it's a beautiful approach, but what are my exact mistakes?","So in order to understand Stokes' theorem, I tried to prove it for a parallelogram with parameterized boundary spanned by two vectors and . The parallelogram : So that for some covector field ""1-form"" : With the edges having I think this reduces to: And this is my try: Let so: Let: So: I'm not quite sure how to continue from here but if , then let: It's easy to proof that is multi-linear and anti-symmetric so: So I know I probably made a lot of mistakes but is the core of this way to prove it right? Are there any papers who proof Stokes' Theorem in general by proving it for a parallelogram or parallelepiped spanned by vectors? Because it makes much sense and it's a beautiful approach, but what are my exact mistakes?","P R(t)=(x(t),y(t)) \mathbf{u} \mathbf{v} \omega \int_{\partial P}\omega = \int_{P}d\omega t=0,1,2,3 \omega_{R(0)}(\mathbf{u})+\omega_{R(1)}(\mathbf{v})-\omega_{R(2)}(\mathbf{u})-\omega_{R(3)}(\mathbf{v})=d\omega(\mathbf{u}\wedge\mathbf{v}) \mathbf{u}=R(1)-R(0)=R(2)-R(3)\\ \mathbf{v}=R(2)-R(1)=R(3)-R(0) ω_t=ω_{R(t)} \omega_{0}(\mathbf{u})+\omega_{1}(\mathbf{v})-\omega_{2}(\mathbf{u})-\omega_{3}(\mathbf{v})=(ω_0-ω_2)(\mathbf{u})+(ω_1-ω_3)(\mathbf{v}) ω_1-ω_0=ω_2-ω_3= \Delta_{\mathbf{u}}~ω~,~ω_2-ω_1=ω_3-ω_0= \Delta_{\mathbf{v}}~ω~,\\\Delta_{\mathbf{u}}~ω(\mathbf{v})=\Deltaω(\mathbf{u},\mathbf{v}) (ω_0-ω_2)(\mathbf{u})+(ω_1-ω_3)(\mathbf{v})=\\(-\Delta_{\mathbf{u}} ~ω-\Delta_{\mathbf{v}}~ω)(\mathbf{u})+(\Delta_{\mathbf{u}} ~ω-\Delta_{\mathbf{v}}~ω)(\mathbf{v})=\\
-\Delta_{\mathbf{u}} ~ω(\mathbf{u})-\Delta_{\mathbf{v}}~ω(\mathbf{u})+\Delta_{\mathbf{u}} ~ω(\mathbf{v})-\Delta_{\mathbf{v}}~ω(\mathbf{v})=\\- \Deltaω(\mathbf{u},{\mathbf{u}})-\Deltaω(\mathbf{v},{\mathbf{u}})+\Delta ω(\mathbf{u},{\mathbf{v}})-\Deltaω(\mathbf{v},{\mathbf{v}}) -\Deltaω(\mathbf{v},{\mathbf{v}})-\Deltaω(\mathbf{u},{\mathbf{u}}) =0 dω(\mathbf{u},\mathbf{v})=\Delta ω(\mathbf{u},{\mathbf{v}})-\Deltaω(\mathbf{v},{\mathbf{u}}) d\omega d\omega(\mathbf{u},\mathbf{v})=d\omega(\mathbf{u}\wedge \mathbf{v})","['linear-algebra', 'differential-geometry', 'stokes-theorem']"
96,When does $v_0\wedge\dots\wedge v_{k-1}=0$ when working over a ring that's not a field?,When does  when working over a ring that's not a field?,v_0\wedge\dots\wedge v_{k-1}=0,"Let $M$ be a module over a commutative ring $R$ , and let $v_0,\dots,v_{k-1}$ be elements of $M$ . If $R$ is a field then $v_0\wedge\dots\wedge v_{k-1}$ is equal to $0$ if and only if $v_0,\dots,v_{k-1}$ are linearly dependent. But if $R$ isn't a field then this needn't be true. For example if we view $\frac{\mathbb{Z}}{2\mathbb{Z}}$ as a $\mathbb Z$ -module then $1\in\frac{\mathbb{Z}}{2\mathbb{Z}}$ is linearly dependent on its own, because $2.1=0$ , but it doesn't get sent to $0$ in $\Lambda^1\left(\frac{\mathbb{Z}}{2\mathbb{Z}}\right)$ . Is there a nice characterisation of which lists of vectors do get sent to $0$ ?","Let be a module over a commutative ring , and let be elements of . If is a field then is equal to if and only if are linearly dependent. But if isn't a field then this needn't be true. For example if we view as a -module then is linearly dependent on its own, because , but it doesn't get sent to in . Is there a nice characterisation of which lists of vectors do get sent to ?","M R v_0,\dots,v_{k-1} M R v_0\wedge\dots\wedge v_{k-1} 0 v_0,\dots,v_{k-1} R \frac{\mathbb{Z}}{2\mathbb{Z}} \mathbb Z 1\in\frac{\mathbb{Z}}{2\mathbb{Z}} 2.1=0 0 \Lambda^1\left(\frac{\mathbb{Z}}{2\mathbb{Z}}\right) 0","['linear-algebra', 'abstract-algebra', 'ring-theory', 'modules', 'exterior-algebra']"
97,Geometric proof/evidence for the 3x3 matrix determinant's formula?,Geometric proof/evidence for the 3x3 matrix determinant's formula?,,"I'm struggling to find a visually intuitive proof for the formula of a 3x3 matrix. I searched it online for hours and it seems impossible to find a source that attempt at least to explain where does it come from. I'm aware that a very similiar question has been asked but it's a very old question and it probably didn't get the deserved attention (I'm referring to this ). Moreover, I think that the increased popularity of the site since then may offer this  question the possibility to get  a better and more satisfactory answer which would turn helpful for a lot of students. This what a visual explanation for the determinant's formual of a 2x2 matrix: This makes it very clear why determinant for a 2x2 matrix is a d-b c and it visually explains how determinant is linked to the area of a parallelogram. I'm not looking necessarly for this kind of ""geometric"" proof. It would be helpful any intuitive explanation for the formula.","I'm struggling to find a visually intuitive proof for the formula of a 3x3 matrix. I searched it online for hours and it seems impossible to find a source that attempt at least to explain where does it come from. I'm aware that a very similiar question has been asked but it's a very old question and it probably didn't get the deserved attention (I'm referring to this ). Moreover, I think that the increased popularity of the site since then may offer this  question the possibility to get  a better and more satisfactory answer which would turn helpful for a lot of students. This what a visual explanation for the determinant's formual of a 2x2 matrix: This makes it very clear why determinant for a 2x2 matrix is a d-b c and it visually explains how determinant is linked to the area of a parallelogram. I'm not looking necessarly for this kind of ""geometric"" proof. It would be helpful any intuitive explanation for the formula.",,"['linear-algebra', 'linear-transformations', 'determinant']"
98,How do fractional tensor products work?,How do fractional tensor products work?,,"In this blog post , Terry Tao discusses the $n$ -fold tensor product of a one-dimensional vector space $V^L$ ( $L$ is just a non-numeric label, not an exponent). He claims that With a bit of additional effort (and taking full advantage of the one-dimensionality of the vector spaces), one can also define spaces with fractional exponents; for instance, one can define $V^{L^{1/2}}$ as the space of formal signed square roots $\pm l^{1/2}$ of non-negative elements $l$ in $V^L$ , with a rather complicated but explicitly definable rule for addition and scalar multiplication. ... However, when working with vector-valued quantities in two and higher dimensions, there are representation-theoretic obstructions to taking arbitrary fractional powers of units. What is the ""rather complicated but explicitly definable rule for addition and scalar multiplication""? Is it easy to see why it doesn't work in higher than one dimension? Could one extend the construction to include irrational exponents? And what properties does the field need to satisfy in order for this construction to work? (Tao claims that the 1D vector space needs to be totally ordered. I'm not sure if this is exactly the same requirement as the field's being totally ordered. Presumably this construction doesn't work for arbitrary ordered fields, because you certainly can't define a square root function $\mathbb{Q} \to \mathbb{Q}$ . Does it only work for real vector spaces?)","In this blog post , Terry Tao discusses the -fold tensor product of a one-dimensional vector space ( is just a non-numeric label, not an exponent). He claims that With a bit of additional effort (and taking full advantage of the one-dimensionality of the vector spaces), one can also define spaces with fractional exponents; for instance, one can define as the space of formal signed square roots of non-negative elements in , with a rather complicated but explicitly definable rule for addition and scalar multiplication. ... However, when working with vector-valued quantities in two and higher dimensions, there are representation-theoretic obstructions to taking arbitrary fractional powers of units. What is the ""rather complicated but explicitly definable rule for addition and scalar multiplication""? Is it easy to see why it doesn't work in higher than one dimension? Could one extend the construction to include irrational exponents? And what properties does the field need to satisfy in order for this construction to work? (Tao claims that the 1D vector space needs to be totally ordered. I'm not sure if this is exactly the same requirement as the field's being totally ordered. Presumably this construction doesn't work for arbitrary ordered fields, because you certainly can't define a square root function . Does it only work for real vector spaces?)",n V^L L V^{L^{1/2}} \pm l^{1/2} l V^L \mathbb{Q} \to \mathbb{Q},"['linear-algebra', 'tensor-products', 'dimensional-analysis']"
99,Optimal orthonormal basis to represent a Gaussian,Optimal orthonormal basis to represent a Gaussian,,"I am looking at representing a set of Gaussians, of the form $\exp(-\frac{(r-r_i)^2}{2 \sigma^2})$, on a 1D domain. I do not know $r_i$ and $\sigma$ prior to defining the basis $\{ \phi_k(r) \}_{k=1}^n$. For the representation, I want to use a basis which can be defined based on four criteria: The basis is as complete as possible given a target number $n$ of basis functions. The basis functions are fully defined from the domain size, $r \in (0,r_\text{cut})$, and $n$. That is, the basis functions depend on $r$ and, parametrically, on $r_\text{cut}$ and $n$. The basis is orthonormal. The basis is optimally suited for representation of Gaussian functions. This means that I can obtain the expansion coefficients analytically. Basically, the end result is an approximant to my original function: $\exp(-\frac{(r-r_i)^2}{2 \sigma^2}) \approx \sum\limits_{k=1}^n w_k \phi_k (r)$ where, as said, I'm aiming at being able to obtain the $w_k$ analytically. These expansion coefficients can depend parametrically on $r_i$, $\sigma$, $r_\text{cut}$ and $n$. Is there any basis suited for this problem? Why I want to do this My final application requires a 3D representation of a series of points. The radial representation is done using Gaussians along the radial direction; the angular representation is done using spherical harmonics. The whole goal of doing a representation in a basis is to use the expansion coefficients. These expansion coefficients can be used to build a rotationally-invariant discrete representation of the set of points. For this to be computationally efficient, the basis needs to be as small as possible. To represent the radial part, I want a fixed basis which is optimally suited to represent 1D Gaussians. I cannot use the Gaussians themselves because I need to represent literally millions of Gaussians using a fixed-size basis, which needs to be always the same.","I am looking at representing a set of Gaussians, of the form $\exp(-\frac{(r-r_i)^2}{2 \sigma^2})$, on a 1D domain. I do not know $r_i$ and $\sigma$ prior to defining the basis $\{ \phi_k(r) \}_{k=1}^n$. For the representation, I want to use a basis which can be defined based on four criteria: The basis is as complete as possible given a target number $n$ of basis functions. The basis functions are fully defined from the domain size, $r \in (0,r_\text{cut})$, and $n$. That is, the basis functions depend on $r$ and, parametrically, on $r_\text{cut}$ and $n$. The basis is orthonormal. The basis is optimally suited for representation of Gaussian functions. This means that I can obtain the expansion coefficients analytically. Basically, the end result is an approximant to my original function: $\exp(-\frac{(r-r_i)^2}{2 \sigma^2}) \approx \sum\limits_{k=1}^n w_k \phi_k (r)$ where, as said, I'm aiming at being able to obtain the $w_k$ analytically. These expansion coefficients can depend parametrically on $r_i$, $\sigma$, $r_\text{cut}$ and $n$. Is there any basis suited for this problem? Why I want to do this My final application requires a 3D representation of a series of points. The radial representation is done using Gaussians along the radial direction; the angular representation is done using spherical harmonics. The whole goal of doing a representation in a basis is to use the expansion coefficients. These expansion coefficients can be used to build a rotationally-invariant discrete representation of the set of points. For this to be computationally efficient, the basis needs to be as small as possible. To represent the radial part, I want a fixed basis which is optimally suited to represent 1D Gaussians. I cannot use the Gaussians themselves because I need to represent literally millions of Gaussians using a fixed-size basis, which needs to be always the same.",,"['linear-algebra', 'normal-distribution', 'change-of-basis']"
