,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,Are the group-like elements of a finite dimensional Hopf algebra finite?,Are the group-like elements of a finite dimensional Hopf algebra finite?,,Let $H$ be a finite dimensional Hopf algebra over a field $k$ . Let $G(H)$ be the set of group-like elements of $H$ . Is $G(H)$ finite?,Let be a finite dimensional Hopf algebra over a field . Let be the set of group-like elements of . Is finite?,H k G(H) H G(H),"['abstract-algebra', 'hopf-algebras', 'grouplike-elements']"
1,Suppose $G$ is a finite group and $|G:H|=n$ I want to show that $|H:H\cap H^g|\leq n$ for all $g\in G$.,Suppose  is a finite group and  I want to show that  for all .,G |G:H|=n |H:H\cap H^g|\leq n g\in G,"Suppose $G$ is a finite group and $|G:H|=n$ I want to show that $|H:H\cap H^g|\leq n$ for all $g\in G$ . If I let $g\in H$ then we know $H^g \leq N_G(H)$ which then we can apply the first isomorphism theorem and conclude $HH^g \leq G$ , $H\trianglelefteq HH^g$ and $H \cap H^g\trianglelefteq H$ and hence $$G/ H \geq HH^g/ H \trianglerighteq H/H\cap H^g$$ But for $g\notin H$ I have no clue. Since $g\notin H$ , I guess I can argue $H^g\leq G$ and hence $H^g\cap H \leq G$ and also $H\cap H^g \leq H$ but then these left cosets arent a group so I can't really say much. Maybe I can argue $H\cap H^g \subset H$ and hence $G/ H\cap H^g \supset H/ H\cap H^g$ and conlude it there? Any hint would be great appreciated. I want to fill the part knowledge I'm missing to complete this.","Suppose is a finite group and I want to show that for all . If I let then we know which then we can apply the first isomorphism theorem and conclude , and and hence But for I have no clue. Since , I guess I can argue and hence and also but then these left cosets arent a group so I can't really say much. Maybe I can argue and hence and conlude it there? Any hint would be great appreciated. I want to fill the part knowledge I'm missing to complete this.",G |G:H|=n |H:H\cap H^g|\leq n g\in G g\in H H^g \leq N_G(H) HH^g \leq G H\trianglelefteq HH^g H \cap H^g\trianglelefteq H G/ H \geq HH^g/ H \trianglerighteq H/H\cap H^g g\notin H g\notin H H^g\leq G H^g\cap H \leq G H\cap H^g \leq H H\cap H^g \subset H G/ H\cap H^g \supset H/ H\cap H^g,"['abstract-algebra', 'group-theory', 'solution-verification', 'group-isomorphism']"
2,Help understanding proof involving Schur's Lemma.,Help understanding proof involving Schur's Lemma.,,"In the book Representation Theory, A First Course by William Fulton and Joe Harris, they state the following: Proposition 1.8 For any representation $V$ of a finite group $G$ , there is a decomposition $$ V=V_1^{\oplus a_1}\oplus\cdots\oplus V_k^{\oplus a_k}, $$ where the $V_i$ are distinct irreducible representations. The decomposition of $V$ into a direct sum of the $k$ factors is unique, as are the $V_i$ that occur and their multiplicities $a_i$ . Proof. It follows from Schur's lemma that if W is another representation of G, with a decomposition $W=\oplus W_j^{\oplus b_j}$ , and $\phi:V\rightarrow W$ is a map of representations, then $\phi$ must map the factor $V_i^{\oplus a_i}$ into that factor $W_j^{\oplus b_j}$ for which $W_j\cong V_i$ ; when applied to the identity map of $V$ to $V$ , the stated uniqueness follows. I don't quite follow the proof. I think for the first part, he uses that if $\phi$ is a linear map between representations, then $f$ restricted to $W_i$ is a map between irreducible representations and Schur's lemma tells us that those representations are isomorphic (correct?). Also, I am not sure about the last part ""when applied to the identity map of $V$ to $V$ , the stated uniqueness follows"". Edit: I found another proof in REPRESENTATION THEORY FOR FINITE GROUPS by Shaun Tan, but I also do not find it easy to understand that one. Specifically, I do not understand why he says that ""If $j=i$ , then $\phi\left(V_{i}^{\oplus a_{i}}\right) \neq 0$ for any $i$ ."" Theorem 4.3. For any finite-dimensional representation $(\rho, V)$ of a finite group $G$ there is a unique decomposition $V=V_{1}^{\oplus a_{1}} \oplus V_{2}^{\oplus a_{2}} \oplus \ldots \oplus V_{i}^{\oplus a_{i}}$ where the $V_{i}$ are inequivalent and irreducible with unique multiplicities $a_{i}$ . Proof. We suppose $V=W_{1}^{\oplus b_{1}} \oplus W_{2}^{\oplus b_{2}} \oplus \ldots \oplus W_{j}^{\oplus b_{j}}$ . Then we let $\phi: V \rightarrow V$ be the identity map. We use Schur's Lemma. For each irreducible $V_{i}^{\oplus a_{i}}$ , we restrict the domain of $\phi$ to that component. Then, either $\phi=0$ or $\phi$ is an isomorphism. If $j=i$ , then $\phi\left(V_{i}^{\oplus a_{i}}\right) \neq 0$ for any $i$ . For each component, $\phi$ is an isomorphism such that $V_{i}^{\oplus a_{i}}$ maps to $W_{j}^{\oplus b_{j}}$ where $V_{i}$ is isomorphic to $W_{j}$ .","In the book Representation Theory, A First Course by William Fulton and Joe Harris, they state the following: Proposition 1.8 For any representation of a finite group , there is a decomposition where the are distinct irreducible representations. The decomposition of into a direct sum of the factors is unique, as are the that occur and their multiplicities . Proof. It follows from Schur's lemma that if W is another representation of G, with a decomposition , and is a map of representations, then must map the factor into that factor for which ; when applied to the identity map of to , the stated uniqueness follows. I don't quite follow the proof. I think for the first part, he uses that if is a linear map between representations, then restricted to is a map between irreducible representations and Schur's lemma tells us that those representations are isomorphic (correct?). Also, I am not sure about the last part ""when applied to the identity map of to , the stated uniqueness follows"". Edit: I found another proof in REPRESENTATION THEORY FOR FINITE GROUPS by Shaun Tan, but I also do not find it easy to understand that one. Specifically, I do not understand why he says that ""If , then for any ."" Theorem 4.3. For any finite-dimensional representation of a finite group there is a unique decomposition where the are inequivalent and irreducible with unique multiplicities . Proof. We suppose . Then we let be the identity map. We use Schur's Lemma. For each irreducible , we restrict the domain of to that component. Then, either or is an isomorphism. If , then for any . For each component, is an isomorphism such that maps to where is isomorphic to .","V G 
V=V_1^{\oplus a_1}\oplus\cdots\oplus V_k^{\oplus a_k},
 V_i V k V_i a_i W=\oplus W_j^{\oplus b_j} \phi:V\rightarrow W \phi V_i^{\oplus a_i} W_j^{\oplus b_j} W_j\cong V_i V V \phi f W_i V V j=i \phi\left(V_{i}^{\oplus a_{i}}\right) \neq 0 i (\rho, V) G V=V_{1}^{\oplus a_{1}} \oplus V_{2}^{\oplus a_{2}} \oplus \ldots \oplus V_{i}^{\oplus a_{i}} V_{i} a_{i} V=W_{1}^{\oplus b_{1}} \oplus W_{2}^{\oplus b_{2}} \oplus \ldots \oplus W_{j}^{\oplus b_{j}} \phi: V \rightarrow V V_{i}^{\oplus a_{i}} \phi \phi=0 \phi j=i \phi\left(V_{i}^{\oplus a_{i}}\right) \neq 0 i \phi V_{i}^{\oplus a_{i}} W_{j}^{\oplus b_{j}} V_{i} W_{j}","['abstract-algebra', 'group-theory', 'vector-spaces', 'modules', 'representation-theory']"
3,"What is known about this structure with idempotence, commutativity, cancellation, and another unnamed property?","What is known about this structure with idempotence, commutativity, cancellation, and another unnamed property?",,"I'm interested in an algebraic structure $(S,\cdot)$ satisfying the following axioms: Idempotence: $a \cdot a = a$ for all $a \in S$ Commutativity: $a \cdot b = b \cdot a$ for all $a, b \in S$ Cancellation: if $a \cdot b = a \cdot c$ then $b = c$ for all $a, b, c \in S$ Unknown: $(a \cdot b) \cdot (c \cdot d) = (a \cdot c) \cdot (b \cdot d)$ for all $a, b, c, d \in S$ I know of essentially just two examples that aren't also associative: Fix a set of variables $\{x_1, \dots, x_n\}$ . Define $S$ to be the set of convex combinations of $x_i$ with dyadic rational coefficients, and define $a \cdot b = \frac{a + b}{2}$ . This is easy to visualize by letting the variables represent $n$ points that span $(n-1)$ -dimensional space with a binary operation that returns the midpoint. We can also relax the coefficients from dyadic rationals to rationals or even reals. $S = \{a, b, c\}$ with binary operation $a \cdot b = c$ , $a \cdot c = b$ , and $b \cdot c = a$ Here's a couple very basic properties I do know: From idempotence and (4) we can easily derive $a \cdot (b \cdot c) = (a \cdot b) \cdot (a \cdot c)$ , meaning that $\cdot$ distributes over itself. From idempotence and cancellation, we can see that $a \cdot b = a$ implies that $b = a$ . I don't know much else about it, and I'm curious if it has a name, other non-associative examples, or any interesting properties. I'm also happy to hear about similar algebraic structures, especially non-associative ones with property (4). There's no application for this exactly, just something I came up with while drawing diagrams related to the first example above.","I'm interested in an algebraic structure satisfying the following axioms: Idempotence: for all Commutativity: for all Cancellation: if then for all Unknown: for all I know of essentially just two examples that aren't also associative: Fix a set of variables . Define to be the set of convex combinations of with dyadic rational coefficients, and define . This is easy to visualize by letting the variables represent points that span -dimensional space with a binary operation that returns the midpoint. We can also relax the coefficients from dyadic rationals to rationals or even reals. with binary operation , , and Here's a couple very basic properties I do know: From idempotence and (4) we can easily derive , meaning that distributes over itself. From idempotence and cancellation, we can see that implies that . I don't know much else about it, and I'm curious if it has a name, other non-associative examples, or any interesting properties. I'm also happy to hear about similar algebraic structures, especially non-associative ones with property (4). There's no application for this exactly, just something I came up with while drawing diagrams related to the first example above.","(S,\cdot) a \cdot a = a a \in S a \cdot b = b \cdot a a, b \in S a \cdot b = a \cdot c b = c a, b, c \in S (a \cdot b) \cdot (c \cdot d) = (a \cdot c) \cdot (b \cdot d) a, b, c, d \in S \{x_1, \dots, x_n\} S x_i a \cdot b = \frac{a + b}{2} n (n-1) S = \{a, b, c\} a \cdot b = c a \cdot c = b b \cdot c = a a \cdot (b \cdot c) = (a \cdot b) \cdot (a \cdot c) \cdot a \cdot b = a b = a",['abstract-algebra']
4,Examples of rings where every left ideal is two-sided but not every right ideal,Examples of rings where every left ideal is two-sided but not every right ideal,,"Is there an example of a ring where every left-ideal is two-sided but not every right ideal? WHAT FOLLOWS IS A FAILED EXAMPLE As an argument but not a proof that the example fails, consider: $ f(a) \;a\; f(a) = 1 f(a) = f(a) $ Thus $f(a) (a \; f(a)) = f(a) \times 1$ This suggests that $(a \; f(a))$ must also be $1$ (it certainly can't equal $f(a)$ since $f(a)$ can't be idempotent), but I'm not sure how to make this an actual argument. Let $F$ be the completely free algebra in the signature $(+, \times, f, \mathbb{Z}[w])$ where $+$ and $\times$ are binary functions, $f$ is a unary, and $\mathbb{Z}$ is a set of constants. Additionally, let the function $d : F \to \mathbb{N}$ be defined as follows: $d(\mathbb{Z})$ is 0 $d(f(x))$ is $1 + d(x)$ for all $x$ . $d(x+y)$ is the maximum of $d(x)$ and $d(y)$ for all $x$ and $y$ . $d(x\times y) = d(x+y)$ Next, I will build a set of equations, $\Delta$ . The intent of this construction is to make $f(x)$ be a left inverse but not a right inverse of $x$ for every $x$ . Let $\Sigma_0$ consist of the axioms of non-commutative rings, all variable-free true equations (i.e. of the form $t_1(\varnothing) = t_2(\varnothing)$ ) in $\mathbb{Z}$ , and the variable-free equation $f(0) = 0$ . Note that $\Delta_0$ is $f$ -free except for a single sentence. Let $\Delta_0$ be the equational deductive closure of $\Sigma_0$ . Note that $\Delta_0$ does not include the non-theorem $xy = yx$ because we can't conclude that it's true. We know that $xy = yx$ holds of all integers $\mathbb{Z}[w]$ , but can't conclude that only integers exist. Let $\Sigma_1$ be the union of $\Delta_0$ , sentences $f(x)x = 1$ for each $x$ such that $d(x) = 0$ and $x \neq 0$ is in $\Delta_0$ , and sentences $f(x)x = 0$ for each $x$ such that $d(x) = 0$ and $x = 0$ is in $\Delta_0$ . Let $\Delta_1$ be the equational deductive closure of $\Sigma_1$ . Analogously, let $\Sigma_{n+1}$ be the union of $\Delta_n$ , sentences $f(x)x = 1$ for each $x$ such that $d(x) = n$ and $x \neq 0$ is in $\Delta_n$ , and sentences $f(x)x = 0$ for each $x$ such that $d(x) = n$ and $x = 0$ is in $\Delta_n$ . Let $\Delta_{n+1}$ be the equational deductive closure of $\Sigma_{n+1}$ . I define $\Delta$ as $\cup_{n \in \mathbb{N}}\Delta_n$ . I define $A$ as $F/\Delta$ , which is well-defined since $F$ is a completely-free algebra and $\Delta$ is a set of equations. I claim that $A$ has precisely two left ideals. Let $I$ be a left ideal of $A$ . Suppose $I$ is $\{0\}$ , then $I$ is closed under left multiplication by arbitrary elements of $A$ . Suppose $I$ is not $\{0\}$ . It follows that $I$ has a nonzero element $c$ . $f(c)$ is a left inverse of $c$ and thus left multiplication by $rf(c)$ will hit the arbitrarily chosen ring element $r$ since $f(c)c = 1$ and thus $(rf(c))c = r(f(c)c) = r1 = r$ . Thus, if $I$ is not $(0)$ , then it is $(1)$ . Every left ideal of $A$ is two-sided since $(0)$ and $(1)$ are both two-sided ideals.","Is there an example of a ring where every left-ideal is two-sided but not every right ideal? WHAT FOLLOWS IS A FAILED EXAMPLE As an argument but not a proof that the example fails, consider: Thus This suggests that must also be (it certainly can't equal since can't be idempotent), but I'm not sure how to make this an actual argument. Let be the completely free algebra in the signature where and are binary functions, is a unary, and is a set of constants. Additionally, let the function be defined as follows: is 0 is for all . is the maximum of and for all and . Next, I will build a set of equations, . The intent of this construction is to make be a left inverse but not a right inverse of for every . Let consist of the axioms of non-commutative rings, all variable-free true equations (i.e. of the form ) in , and the variable-free equation . Note that is -free except for a single sentence. Let be the equational deductive closure of . Note that does not include the non-theorem because we can't conclude that it's true. We know that holds of all integers , but can't conclude that only integers exist. Let be the union of , sentences for each such that and is in , and sentences for each such that and is in . Let be the equational deductive closure of . Analogously, let be the union of , sentences for each such that and is in , and sentences for each such that and is in . Let be the equational deductive closure of . I define as . I define as , which is well-defined since is a completely-free algebra and is a set of equations. I claim that has precisely two left ideals. Let be a left ideal of . Suppose is , then is closed under left multiplication by arbitrary elements of . Suppose is not . It follows that has a nonzero element . is a left inverse of and thus left multiplication by will hit the arbitrarily chosen ring element since and thus . Thus, if is not , then it is . Every left ideal of is two-sided since and are both two-sided ideals."," f(a) \;a\; f(a) = 1 f(a) = f(a)  f(a) (a \; f(a)) = f(a) \times 1 (a \; f(a)) 1 f(a) f(a) F (+, \times, f, \mathbb{Z}[w]) + \times f \mathbb{Z} d : F \to \mathbb{N} d(\mathbb{Z}) d(f(x)) 1 + d(x) x d(x+y) d(x) d(y) x y d(x\times y) = d(x+y) \Delta f(x) x x \Sigma_0 t_1(\varnothing) = t_2(\varnothing) \mathbb{Z} f(0) = 0 \Delta_0 f \Delta_0 \Sigma_0 \Delta_0 xy = yx xy = yx \mathbb{Z}[w] \Sigma_1 \Delta_0 f(x)x = 1 x d(x) = 0 x \neq 0 \Delta_0 f(x)x = 0 x d(x) = 0 x = 0 \Delta_0 \Delta_1 \Sigma_1 \Sigma_{n+1} \Delta_n f(x)x = 1 x d(x) = n x \neq 0 \Delta_n f(x)x = 0 x d(x) = n x = 0 \Delta_n \Delta_{n+1} \Sigma_{n+1} \Delta \cup_{n \in \mathbb{N}}\Delta_n A F/\Delta F \Delta A I A I \{0\} I A I \{0\} I c f(c) c rf(c) r f(c)c = 1 (rf(c))c = r(f(c)c) = r1 = r I (0) (1) A (0) (1)","['abstract-algebra', 'ring-theory', 'examples-counterexamples', 'noncommutative-algebra']"
5,Sum of an irreducible representation.,Sum of an irreducible representation.,,"I am trying to prove the following statement: Let $\mathfrak{X}$ be an irreducible $F$ -representation of $G$ over an arbitrary field. Show that $\sum\limits_{g\in G}\mathfrak{X}(g) = 0$ unless $\mathfrak{X}$ is the principal representation. I know that there are many different proofs of this fact, but I wanted to prove this statement on my own, and I have not yet seen a proof similar to mine (although I may be wrong). But is this proof correct? Unfortunately, as it has already turned out, this proof doesn't work for an arbitrary field, but it is probably true for a field with characteristic $0$ !!! Proof: Let $\mathfrak{X}$ be the principal representation, then by definition $\forall g\in G\Rightarrow \mathfrak{X}(g) = 1_{F}$ . Then $$\sum\limits_{g\in G}\mathfrak{X}(g) = \sum\limits_{g\in G}1_{F} =|G|\cdot 1_{F}$$ Now let $\mathfrak{X}$ not be the principal representation. Let $T = \sum\limits_{g\in G}\mathfrak{X}(g)$ . Note that $\forall a \in G$ $$\mathfrak{X}(a)T = \mathfrak{X}(a)\sum\limits_{g\in G}\mathfrak{X}(g) = \sum\limits_{g\in G}\mathfrak{X}(ag) = T.$$ Then it follows that $T^2 = |G|\cdot T$ . Then the polynomial $x^2 -|G|x$ is annulling for $T$ . Then the Jordan form of $T$ (up to permutations of Jordan cells) can take the following forms: Either $|G|\cdot I$ ; or zero matrix; or a matrix with several units diagonally (not completely filling it), and the rest of all values are zero, and multiplied by $|G|$ . Note that only the zero matrix among all these types of matrices has a trace equal to zero! Let $\chi$ be the character associated with $\mathfrak{X}$ . Consider the following expression $$S = \sum\limits_{g\in G}\chi(g) = [\chi, 1_G] = 0.$$ The last equality holds because $\chi\in Irr(G)$ and $\chi \neq 1_G$ . Since $\chi(g) = tr(\mathfrak{X}(g))$ and $tr(A + B) = tr(A) + tr(B)$ for any matrices $A$ and $B$ , we get that $S = tr(T) = 0$ . But only the zero matrix among the possible candidates for the role of $T$ had a zero trace. So $T = 0$ , which was required to be proved. $\blacksquare$","I am trying to prove the following statement: Let be an irreducible -representation of over an arbitrary field. Show that unless is the principal representation. I know that there are many different proofs of this fact, but I wanted to prove this statement on my own, and I have not yet seen a proof similar to mine (although I may be wrong). But is this proof correct? Unfortunately, as it has already turned out, this proof doesn't work for an arbitrary field, but it is probably true for a field with characteristic !!! Proof: Let be the principal representation, then by definition . Then Now let not be the principal representation. Let . Note that Then it follows that . Then the polynomial is annulling for . Then the Jordan form of (up to permutations of Jordan cells) can take the following forms: Either ; or zero matrix; or a matrix with several units diagonally (not completely filling it), and the rest of all values are zero, and multiplied by . Note that only the zero matrix among all these types of matrices has a trace equal to zero! Let be the character associated with . Consider the following expression The last equality holds because and . Since and for any matrices and , we get that . But only the zero matrix among the possible candidates for the role of had a zero trace. So , which was required to be proved.","\mathfrak{X} F G \sum\limits_{g\in G}\mathfrak{X}(g) = 0 \mathfrak{X} 0 \mathfrak{X} \forall g\in G\Rightarrow \mathfrak{X}(g) = 1_{F} \sum\limits_{g\in G}\mathfrak{X}(g) = \sum\limits_{g\in G}1_{F} =|G|\cdot 1_{F} \mathfrak{X} T = \sum\limits_{g\in G}\mathfrak{X}(g) \forall a \in G \mathfrak{X}(a)T = \mathfrak{X}(a)\sum\limits_{g\in G}\mathfrak{X}(g) = \sum\limits_{g\in G}\mathfrak{X}(ag) = T. T^2 = |G|\cdot T x^2 -|G|x T T |G|\cdot I |G| \chi \mathfrak{X} S = \sum\limits_{g\in G}\chi(g) = [\chi, 1_G] = 0. \chi\in Irr(G) \chi \neq 1_G \chi(g) = tr(\mathfrak{X}(g)) tr(A + B) = tr(A) + tr(B) A B S = tr(T) = 0 T T = 0 \blacksquare","['abstract-algebra', 'group-theory', 'solution-verification', 'representation-theory']"
6,Radical in polynomial ring over $\mathbb{Q}$ implies radical in polynomial ring over $\mathbb{C}$?,Radical in polynomial ring over  implies radical in polynomial ring over ?,\mathbb{Q} \mathbb{C},"Let $f_1,\ldots,f_s\in\mathbb{Q}[x_1,\ldots,x_n]$ be polynomials such that the ideal $\langle f_1,\ldots,f_s\rangle$ is radical. Viewing each $f_i$ as an element of $\mathbb{C}[x_1,\ldots,x_n]$ , is it true that $\langle f_1,\ldots,f_s\rangle$ generated as an ideal in $\mathbb{C}[x_1,\ldots,x_n]$ is also radical? For example, the ideal generated by $x^2+1$ in $\mathbb{Q}[x]$ is radical (it is prime) and even though the ideal generated by $x^2+1$ in $\mathbb{C}[x]$ is not prime, it is still radical . Indeed, if $f^n\in\langle x^2+1\rangle\subset\mathbb{C}[x]$ then $f^n$ has $x^2+1$ as a factor, so it has $x-i$ and $x+i$ as factors, which is only possible if $f$ had $x-i$ and $x+i$ as factors to begin with. Hence $f$ has $x^2+1$ as a factor so that $f\in\langle x^2+1\rangle\subset\mathbb{C}[x]$ .","Let be polynomials such that the ideal is radical. Viewing each as an element of , is it true that generated as an ideal in is also radical? For example, the ideal generated by in is radical (it is prime) and even though the ideal generated by in is not prime, it is still radical . Indeed, if then has as a factor, so it has and as factors, which is only possible if had and as factors to begin with. Hence has as a factor so that .","f_1,\ldots,f_s\in\mathbb{Q}[x_1,\ldots,x_n] \langle f_1,\ldots,f_s\rangle f_i \mathbb{C}[x_1,\ldots,x_n] \langle f_1,\ldots,f_s\rangle \mathbb{C}[x_1,\ldots,x_n] x^2+1 \mathbb{Q}[x] x^2+1 \mathbb{C}[x] f^n\in\langle x^2+1\rangle\subset\mathbb{C}[x] f^n x^2+1 x-i x+i f x-i x+i f x^2+1 f\in\langle x^2+1\rangle\subset\mathbb{C}[x]","['abstract-algebra', 'ring-theory', 'commutative-algebra', 'ideals']"
7,"$G = HN$ a group, $N$ normal and $N \cap H =1$. If the conjugacy of $H$ on $N$ has a orbit with all of the non trivial elements: $|N|$ is a prime","a group,  normal and . If the conjugacy of  on  has a orbit with all of the non trivial elements:  is a prime",G = HN N N \cap H =1 H N |N|,"Let $G$ be a finite group, $H, N \leqslant G $ with $N$ normal in $G$ such that $G=HN$ and $N \cap H =\{1\}$ . Suppose that all of the non-trivial elements of $N$ are in a single orbit for the conjugacy action of $H$ on $N$ , then: exists a prime $p$ such that $\forall x \in N$ , $x \neq 1$ we have $|x|=p$ . So I can deduce that the conjugacy action of $H$ on $N$ has two orbits: $\{1\}$ and $N \setminus \{1\}$ and if $x \in N \setminus \{1\}$ , for the orbit-stabilizer theorem we have that $|N|-1 = [H:H_x]$ , so $|N|-1$ divides the order of $H$ , hence $|G| = |N|(|N|-1)k$ . Now, to show that $|x| = p\,\,$ , I can imagine that the strategy would be to show that $|N|$ is the prime $p$ . Any hint will be appreciated since I am stuck here, thank you.","Let be a finite group, with normal in such that and . Suppose that all of the non-trivial elements of are in a single orbit for the conjugacy action of on , then: exists a prime such that , we have . So I can deduce that the conjugacy action of on has two orbits: and and if , for the orbit-stabilizer theorem we have that , so divides the order of , hence . Now, to show that , I can imagine that the strategy would be to show that is the prime . Any hint will be appreciated since I am stuck here, thank you.","G H, N \leqslant G  N G G=HN N \cap H =\{1\} N H N p \forall x \in N x \neq 1 |x|=p H N \{1\} N \setminus \{1\} x \in N \setminus \{1\} |N|-1 = [H:H_x] |N|-1 H |G| = |N|(|N|-1)k |x| = p\,\, |N| p","['abstract-algebra', 'group-theory', 'finite-groups', 'group-actions', 'normal-subgroups']"
8,Irreducibility of polynomials in $\mathbb{F}_p[x][y]$,Irreducibility of polynomials in,\mathbb{F}_p[x][y],"Suppose we have a polynomial $P(x,y)$ in $\mathbb{F}_p[x][y]$ that is irreducible as a polynomial in $y$ . I guess that $P(x^p,y)$ is also irreducible as a polynomial in $y$ . How can I prove it? Thank you in advance! Edit, after Kenta's comment: What if I add the condition that there exists an $n$ not divisible by $p$ such that the coefficient of $y^n$ in $P$ is nonzero?","Suppose we have a polynomial in that is irreducible as a polynomial in . I guess that is also irreducible as a polynomial in . How can I prove it? Thank you in advance! Edit, after Kenta's comment: What if I add the condition that there exists an not divisible by such that the coefficient of in is nonzero?","P(x,y) \mathbb{F}_p[x][y] y P(x^p,y) y n p y^n P","['abstract-algebra', 'irreducible-polynomials']"
9,How do I show that the ideal $(2)$ of $\Bbb{Z}/4\Bbb{Z}$ is not flat?,How do I show that the ideal  of  is not flat?,(2) \Bbb{Z}/4\Bbb{Z},Let $R=\Bbb{Z}/4\Bbb{Z}$ and let me take the ideal $(2)\subset R$ . I want to show that $(2)$ is not flat in $R$ . We only had the following definition: A module $M\subset R$ is flat if $M\otimes_R-$ is left exact. Using this definition I want to show it. My idea was the following. Since we know that tensoring is already right exact we only need to worry about left exactness. The problem is that I don't see what left exact sequence I need to consider such that after tensoring it isn't left exact anymore. Could someone give me a hint?,Let and let me take the ideal . I want to show that is not flat in . We only had the following definition: A module is flat if is left exact. Using this definition I want to show it. My idea was the following. Since we know that tensoring is already right exact we only need to worry about left exactness. The problem is that I don't see what left exact sequence I need to consider such that after tensoring it isn't left exact anymore. Could someone give me a hint?,R=\Bbb{Z}/4\Bbb{Z} (2)\subset R (2) R M\subset R M\otimes_R-,"['abstract-algebra', 'commutative-algebra', 'tensor-products', 'exact-sequence']"
10,Some questions on polynomial $f(x)=x^4+x^2+4$ over $\mathbb{Q}$,Some questions on polynomial  over,f(x)=x^4+x^2+4 \mathbb{Q},"1-) Firstly, I have shown that $f(x)$ is irreducible over $\mathbb{Q}$ . To show, I first use the theorem that says: $$\text{Suppose there exist } r \in \mathbb{Q} \text{ such that } f(r)=0.\text{ Then } r\in\mathbb{Z} \text{ and }r|a_{0} \text{, where } a_0 \text{ constant coefficient of the polynomial.}$$ $$f(1)\neq0,\;f(-1)\neq0,\;f(2)\neq0,\;f(-2)\neq0,\;f(4)\neq0,\;f(-4)\neq0$$ But, since $\deg(f(x))=4$ , we cannot say anything about roots. Clearly, we cannot use Eisenstein Criterion either. Now, we can check whether it has any non-linear factors. Fortunately, it has! $$x^4+x^2+4=(x^2+x\sqrt{3}+2)(x^2-x\sqrt{3}+2)$$ But, $\sqrt{3} \notin \mathbb{Q}$ .Therefore $x^4+x^2+4$ is irreducible over $\mathbb{Q}$ . Here is the question. Can I say $f(x)$ is irreducible after all these calculations? 2-) I defined $$f_{1}(x)=x^2+x\sqrt{3}+2, \;f_{2}(x)=x^2-x\sqrt{3}+2$$ We can say $f=f_{1}\cdot f_{2}$ is reducible over $\mathbb{Q(\sqrt{3})}$ since $\sqrt{3} \in \mathbb{Q(\sqrt{3})}$ . Even $f(x)$ is reducible over extension field $\mathbb{Q(\sqrt3)}$ , $f_{1}$ and $f_{2}$ are irreducible over $\mathbb{Q(\sqrt3)}$ For $f_{1}(x)=x^2+x\sqrt{3}+2$ we have $x_{1,2}=\dfrac{-\sqrt{3}\pm \sqrt{-5}}{2}$ and also for $x_{3,4}=\dfrac{\sqrt{3}\pm\sqrt{-5}}{2}$ . We know that neither $\sqrt{-5} \notin \mathbb{Q}$ nor $\sqrt{-5} \notin \mathbb{Q({\sqrt{3}})}$ . Therefore, we need some new splitting field and it is, clearly, $\mathbb{Q(\sqrt{3},\sqrt{-5})}$ . I think there is no problem. But I have to ask to be sure, Can I extend the field like I did to split the polynomial? 3-) There is extension $\mathbb{Q(\sqrt{3},\sqrt{-5})}$ over $\mathbb{Q(\sqrt{3})}$ . There is also extension $\mathbb{Q(\sqrt{3})}$ over $\mathbb{Q}$ . For the field extension $\mathbb{Q(\sqrt{3},\sqrt{-5})}$ over $\mathbb{Q}$ we have the structure such as: $$\mathbb{Q(\sqrt{3},\sqrt{-5})}=\lbrace r+s\sqrt{3}+t\sqrt{-5}+z\sqrt{-15}:r,s,t,z\in \mathbb{Q}\rbrace$$ So, for the basis, dimension or degree of minimal polynomial we have $4$ and this basis is $\lbrace1,\sqrt{3},\sqrt{-5},\sqrt{-15}\rbrace$ For, the minimal polynomial of $\mathbb{Q(\sqrt{3},\sqrt{-5})}$ over $\mathbb{Q(\sqrt{3})}$ we have $P_{\sqrt{-5}}(x)=x^2+5$ this polynomial is irreducible since $\sqrt{-5}\notin\mathbb{Q(\sqrt{3})}$ . For the minimal polynomial of $\mathbb{Q(\sqrt{3})}$ over $\mathbb{Q}$ we have $P_{\sqrt{3}}(x)=x^2-3$ and it is irreducible over $\mathbb{Q}$ by Eisenstein Criterion. 4-) Clearly $\mathbb{Q(\sqrt{3},\sqrt{-5})}$ is splitting field of $x^4+x^2+2$ over $\mathbb{Q}$ and since it is finite extension, we can say it is a normal extension and it is also separable extension since all the roots of polynomial is simple roots. My whole solving strategies makes sense? Thanks in advance!","1-) Firstly, I have shown that is irreducible over . To show, I first use the theorem that says: But, since , we cannot say anything about roots. Clearly, we cannot use Eisenstein Criterion either. Now, we can check whether it has any non-linear factors. Fortunately, it has! But, .Therefore is irreducible over . Here is the question. Can I say is irreducible after all these calculations? 2-) I defined We can say is reducible over since . Even is reducible over extension field , and are irreducible over For we have and also for . We know that neither nor . Therefore, we need some new splitting field and it is, clearly, . I think there is no problem. But I have to ask to be sure, Can I extend the field like I did to split the polynomial? 3-) There is extension over . There is also extension over . For the field extension over we have the structure such as: So, for the basis, dimension or degree of minimal polynomial we have and this basis is For, the minimal polynomial of over we have this polynomial is irreducible since . For the minimal polynomial of over we have and it is irreducible over by Eisenstein Criterion. 4-) Clearly is splitting field of over and since it is finite extension, we can say it is a normal extension and it is also separable extension since all the roots of polynomial is simple roots. My whole solving strategies makes sense? Thanks in advance!","f(x) \mathbb{Q} \text{Suppose there exist } r \in \mathbb{Q} \text{ such that } f(r)=0.\text{ Then } r\in\mathbb{Z} \text{ and }r|a_{0} \text{, where } a_0 \text{ constant coefficient of the polynomial.} f(1)\neq0,\;f(-1)\neq0,\;f(2)\neq0,\;f(-2)\neq0,\;f(4)\neq0,\;f(-4)\neq0 \deg(f(x))=4 x^4+x^2+4=(x^2+x\sqrt{3}+2)(x^2-x\sqrt{3}+2) \sqrt{3} \notin \mathbb{Q} x^4+x^2+4 \mathbb{Q} f(x) f_{1}(x)=x^2+x\sqrt{3}+2, \;f_{2}(x)=x^2-x\sqrt{3}+2 f=f_{1}\cdot f_{2} \mathbb{Q(\sqrt{3})} \sqrt{3} \in \mathbb{Q(\sqrt{3})} f(x) \mathbb{Q(\sqrt3)} f_{1} f_{2} \mathbb{Q(\sqrt3)} f_{1}(x)=x^2+x\sqrt{3}+2 x_{1,2}=\dfrac{-\sqrt{3}\pm \sqrt{-5}}{2} x_{3,4}=\dfrac{\sqrt{3}\pm\sqrt{-5}}{2} \sqrt{-5} \notin \mathbb{Q} \sqrt{-5} \notin \mathbb{Q({\sqrt{3}})} \mathbb{Q(\sqrt{3},\sqrt{-5})} \mathbb{Q(\sqrt{3},\sqrt{-5})} \mathbb{Q(\sqrt{3})} \mathbb{Q(\sqrt{3})} \mathbb{Q} \mathbb{Q(\sqrt{3},\sqrt{-5})} \mathbb{Q} \mathbb{Q(\sqrt{3},\sqrt{-5})}=\lbrace r+s\sqrt{3}+t\sqrt{-5}+z\sqrt{-15}:r,s,t,z\in \mathbb{Q}\rbrace 4 \lbrace1,\sqrt{3},\sqrt{-5},\sqrt{-15}\rbrace \mathbb{Q(\sqrt{3},\sqrt{-5})} \mathbb{Q(\sqrt{3})} P_{\sqrt{-5}}(x)=x^2+5 \sqrt{-5}\notin\mathbb{Q(\sqrt{3})} \mathbb{Q(\sqrt{3})} \mathbb{Q} P_{\sqrt{3}}(x)=x^2-3 \mathbb{Q} \mathbb{Q(\sqrt{3},\sqrt{-5})} x^4+x^2+2 \mathbb{Q}","['abstract-algebra', 'polynomials', 'solution-verification', 'irreducible-polynomials', 'splitting-field']"
11,Is $S_2 \wr S_k$ contained in $(S_a \times S_b) \wr S_{k-1}$?,Is  contained in ?,S_2 \wr S_k (S_a \times S_b) \wr S_{k-1},"I'm working on my thesis and I want to prove a theorem but I need the following to be true: $S_2 \wr S_k$ is not isomorphic to a subgroup of $(S_a \times S_b) \wr S_{k-1}$ where $a,b < 2k$ . Does anybody have an idea if this is true? EDIT:I am using this as a lemma to prove that every graph $G$ with automorphismgroup equal to $S_2 \wr S_k$ has at least $k$ disjunct edges. It is not hard to prove that this holds for $k=2$ but for $k=3$ there is a counterexapmle since $S_2 \wr S_3 \cong S_2 \times S_4$ , it seems to me that it probably would hold if $k$ is high enough, though I don't know how to proof that. (P.S. The counterexample for $k=3$ also gives a counterexample to the graph theorem I'm trying to prove since you can just take an empty graph on 4 vertices with the disjunct union of a single edge.)","I'm working on my thesis and I want to prove a theorem but I need the following to be true: is not isomorphic to a subgroup of where . Does anybody have an idea if this is true? EDIT:I am using this as a lemma to prove that every graph with automorphismgroup equal to has at least disjunct edges. It is not hard to prove that this holds for but for there is a counterexapmle since , it seems to me that it probably would hold if is high enough, though I don't know how to proof that. (P.S. The counterexample for also gives a counterexample to the graph theorem I'm trying to prove since you can just take an empty graph on 4 vertices with the disjunct union of a single edge.)","S_2 \wr S_k (S_a \times S_b) \wr S_{k-1} a,b < 2k G S_2 \wr S_k k k=2 k=3 S_2 \wr S_3 \cong S_2 \times S_4 k k=3","['abstract-algebra', 'group-theory', 'symmetric-groups', 'wreath-product']"
12,Under what circumstances are the relative Kahler differentials a flat module?,Under what circumstances are the relative Kahler differentials a flat module?,,"First let us recall one construction of the relative Kahler differentials. Let $k$ be a ring and $R$ a $k$ -algebra. The relative Kahler differentials $\Omega_{R/k}$ are the $R$ -module satisfying the following universal property: For any $R$ -module $M$ and $k$ -linear derivation $\delta:R\to M$ (an $R$ -module homomorphism satisfying the Leibniz rule), there exists a universal derivation $d:R\to\Omega_{R/k}$ and a unique $R$ -module homomorphism $\varphi:\Omega_{R/k}\to M$ such that $\varphi\circ d=\delta$ . You can use this universal property, along with some direct constructions and short exact sequences, to compute specific examples. Explicitly, suppose $R=k[x_{1},\ldots,x_{n}]/(f_{1},\ldots,f_{s})$ is a $k$ -algebra of finite type. Then one can show that $\Omega_{R/k}=(Rdx_{1}\oplus\cdots\oplus Rdx_{n})/(df_{1},\ldots,df_{s})$ , and $d:R\to\Omega_{R/k}$ is given by $f\mapsto df$ . Namely, the Kahler differentials are the cokernel of the Jacobian matrix $\begin{bmatrix}\frac{\partial f_{i}}{\partial x_{j}}\end{bmatrix}:R^{s}\to R^{n}$ . I'm curious as to what hypotheses are known under which $\Omega_{R/k}$ is a flat $R$ -module. For example, by our calculation above, $\Omega_{R/k}$ is free if $R$ is a polynomial $k$ -algebra, and by Proposition 3.9 of these linked notes by Akhil Mathew, $\Omega_{R/k}$ is projective of rank $\dim R$ if and only if $R$ is regular; hence, $\Omega_{R/k}$ is flat in both of these circumstances. My question: What else can be said if we only need $\Omega_{R/k}$ to be a flat $R$ -module? Even better, are there necessary and sufficient hypotheses on $R$ that guarantee flatness of $\Omega_{R/k}$ , like is the case for projectiveness?","First let us recall one construction of the relative Kahler differentials. Let be a ring and a -algebra. The relative Kahler differentials are the -module satisfying the following universal property: For any -module and -linear derivation (an -module homomorphism satisfying the Leibniz rule), there exists a universal derivation and a unique -module homomorphism such that . You can use this universal property, along with some direct constructions and short exact sequences, to compute specific examples. Explicitly, suppose is a -algebra of finite type. Then one can show that , and is given by . Namely, the Kahler differentials are the cokernel of the Jacobian matrix . I'm curious as to what hypotheses are known under which is a flat -module. For example, by our calculation above, is free if is a polynomial -algebra, and by Proposition 3.9 of these linked notes by Akhil Mathew, is projective of rank if and only if is regular; hence, is flat in both of these circumstances. My question: What else can be said if we only need to be a flat -module? Even better, are there necessary and sufficient hypotheses on that guarantee flatness of , like is the case for projectiveness?","k R k \Omega_{R/k} R R M k \delta:R\to M R d:R\to\Omega_{R/k} R \varphi:\Omega_{R/k}\to M \varphi\circ d=\delta R=k[x_{1},\ldots,x_{n}]/(f_{1},\ldots,f_{s}) k \Omega_{R/k}=(Rdx_{1}\oplus\cdots\oplus Rdx_{n})/(df_{1},\ldots,df_{s}) d:R\to\Omega_{R/k} f\mapsto df \begin{bmatrix}\frac{\partial f_{i}}{\partial x_{j}}\end{bmatrix}:R^{s}\to R^{n} \Omega_{R/k} R \Omega_{R/k} R k \Omega_{R/k} \dim R R \Omega_{R/k} \Omega_{R/k} R R \Omega_{R/k}","['abstract-algebra', 'algebraic-geometry', 'commutative-algebra', 'modules', 'flatness']"
13,Ring structure of the representation ring of a finite group,Ring structure of the representation ring of a finite group,,"I'm currently taking a course in representation theory, and I'm not entirely sure I understand the structure of the representation ring of a finite group. For a finite group $G$ with irreducible representations $(\rho_{i}, V_{i})_{i=1}^{n}$ over a field $F$ , we denote $$R_{F}[G] = \Big \{\sum_{i = 1}^{n}n_{i}V_{i} : n_{i} \in \mathbb{Z} \Big \}$$ to be the representation ring of the group $G$ with addition defined as direct addition of coefficients and multiplication defined by tensor product? I'm not entirely sure I understand the ring structure here. I understand that since we take coefficients in $\mathbb{Z}$ , we clearly have an abelian group by directing taking sums of coefficients of $V_{i}$ , but I'm not entirely sure I understand the multiplicative aspect of thing ring. From what I understand, it has to do with tensor products, but how is the tensor product actually taken? Do we distribute the tensor over all elements and then have to reduce the tensored representations back down to recover a ring element of this form? Would the multiplicative identity of this ring then just be the trivial representation with coefficient 1? This ring seems very odd to me, although it does seem to come up for instance in Brauer theorem. This question is somewhat similar to this one , but reading through the question and answer here doesn't really answer the question to me.","I'm currently taking a course in representation theory, and I'm not entirely sure I understand the structure of the representation ring of a finite group. For a finite group with irreducible representations over a field , we denote to be the representation ring of the group with addition defined as direct addition of coefficients and multiplication defined by tensor product? I'm not entirely sure I understand the ring structure here. I understand that since we take coefficients in , we clearly have an abelian group by directing taking sums of coefficients of , but I'm not entirely sure I understand the multiplicative aspect of thing ring. From what I understand, it has to do with tensor products, but how is the tensor product actually taken? Do we distribute the tensor over all elements and then have to reduce the tensored representations back down to recover a ring element of this form? Would the multiplicative identity of this ring then just be the trivial representation with coefficient 1? This ring seems very odd to me, although it does seem to come up for instance in Brauer theorem. This question is somewhat similar to this one , but reading through the question and answer here doesn't really answer the question to me.","G (\rho_{i}, V_{i})_{i=1}^{n} F R_{F}[G] = \Big \{\sum_{i = 1}^{n}n_{i}V_{i} : n_{i} \in \mathbb{Z} \Big \} G \mathbb{Z} V_{i}","['abstract-algebra', 'ring-theory', 'representation-theory']"
14,Questions regarding rings where every injective modules over them are also flat modules.,Questions regarding rings where every injective modules over them are also flat modules.,,"Let $R$ be a ring such that every left injective $M$ module over $R$ is also a left flat module over $R$ . An example that springs to my mind to illustrate this type of rings are the regular rings, since it is well known that every module over a regular ring is flat, particularly all injective modules over a regular ring are flat. First question is: Are there any more examples of these type of rings, where every injective module over this particular ring is also a flat module over the same ring? Second question is if $R$ is ring such that every left injective $M$ module over $R$ is also a left flat module over $R$ and every right injective $M$ module over $R$ is also a right flat module over $R$ , then is it possible to prove that $R$ is a coherent ring? That is, where every submodule of a finitely generated module over $R$ is finitely presented. I've run out of ideas proving this last statement but maybe my affirmation is not true.","Let be a ring such that every left injective module over is also a left flat module over . An example that springs to my mind to illustrate this type of rings are the regular rings, since it is well known that every module over a regular ring is flat, particularly all injective modules over a regular ring are flat. First question is: Are there any more examples of these type of rings, where every injective module over this particular ring is also a flat module over the same ring? Second question is if is ring such that every left injective module over is also a left flat module over and every right injective module over is also a right flat module over , then is it possible to prove that is a coherent ring? That is, where every submodule of a finitely generated module over is finitely presented. I've run out of ideas proving this last statement but maybe my affirmation is not true.",R M R R R M R R M R R R R,"['abstract-algebra', 'ring-theory', 'modules', 'homological-algebra', 'noncommutative-algebra']"
15,"Show that the Lorentz boosts, $\begin{pmatrix}\gamma & -\beta\gamma\\ -\beta\gamma & \gamma\end{pmatrix}$ form a one-parameter Lie group.","Show that the Lorentz boosts,  form a one-parameter Lie group.",\begin{pmatrix}\gamma & -\beta\gamma\\ -\beta\gamma & \gamma\end{pmatrix},"In the following post, I am questioning certain parts of the author's solution for the beginning of a proof that the Lorentz boosts form a one-parameter Lie group: Show that the Lorentz boosts, \begin{pmatrix}\gamma & -\beta\gamma\\  -\beta\gamma & \gamma\end{pmatrix} form a one-parameter Lie group. Is this group Abelian? Here is the author's solution: The Lorentz boosts, $\Lambda$ , are defined by $$\Lambda(\beta)=\begin{pmatrix}\gamma & -\beta\gamma\\  -\beta\gamma & \gamma\end{pmatrix}=\gamma(\beta)\begin{pmatrix}1 & -\beta\\  -\beta & 1\end{pmatrix}$$ where $\gamma(\beta)=\frac{1}{\sqrt{1-{\beta}^2}},\quad \beta=\frac{v}{c}, \quad$ and $c$ is the speed of light, so $\lvert v \rvert\lt c$ . To show that these matrices form a Lie group, we must demonstrate that these matrices under matrix multiplication satisfy the group axioms. Closure . The product of two boosts $\Lambda(\beta )$ and $\Lambda({\beta}^{\prime})$ is $$\begin{align}\Lambda(\beta )\Lambda({\beta}^{\prime})&=\gamma({\beta})\gamma({\beta}^{\prime})\begin{pmatrix}1 & -\beta\\  -\beta & 1\end{pmatrix}\begin{pmatrix}1 & -{\beta}^\prime\\  -{\beta}^\prime & 1\end{pmatrix}\\&=\gamma({\beta})\gamma({\beta}^{\prime})\begin{pmatrix}1+\beta{\beta}^\prime & -\beta-{\beta}^\prime\\  -\beta-{\beta}^\prime & 1+\beta{\beta}^\prime\end{pmatrix}\end{align}$$ For this product to have the form $$\Lambda(\beta^{\prime\prime})=\gamma(\beta^{\prime\prime})\begin{pmatrix}1 & -{\beta}^{\prime\prime}\\  -{\beta}^{\prime\prime} & 1\end{pmatrix}\tag{1}$$ we must require that $$\color{red}{\gamma(\beta^{\prime\prime})=\gamma(\beta)\gamma({\beta}^\prime)\left(1+\beta{\beta}^\prime\right)}\tag{2}$$ $${\begin{align}\color{red}{{\beta}^{\prime\prime}\gamma({\beta}^{\prime\prime})=\gamma(\beta)\gamma({\beta}^\prime)\left(\beta+{\beta}^\prime\right)}\tag{3}&\\=[\cdots]\end{align}}$$ I would like to understand the reasoning behind the need for the red equations, $(2)$ and $(3)$ . Taking equation $(2)$ for now, and plugging it back into $(1)$ , $$\begin{align}\Lambda(\beta^{\prime\prime})&=\gamma(\beta^{\prime\prime})\begin{pmatrix}1 & -{\beta}^{\prime\prime}\\  -{\beta}^{\prime\prime} & 1\end{pmatrix}\\&=\gamma(\beta)\gamma({\beta}^\prime)\left(1+\beta{\beta}^\prime\right)\begin{pmatrix}1 & -{\beta}^{\prime\prime}\\  -{\beta}^{\prime\prime} & 1\end{pmatrix}\\&=\gamma(\beta)\gamma({\beta}^\prime)\begin{pmatrix}1 +\beta{\beta}^\prime & -\left(1 +\beta{\beta}^\prime\right){\beta}^{\prime\prime}\\  -\left(1 +\beta{\beta}^\prime\right){\beta}^{\prime\prime} & 1 +\beta{\beta}^\prime\end{pmatrix}\\&=\gamma(\beta)\gamma({\beta}^\prime)\left(1+\beta{\beta}^\prime\right)\begin{pmatrix}1 & -{\beta}^{\prime\prime}\\  -{\beta}^{\prime\prime} & 1\end{pmatrix}\end{align}$$ So I see why the factor of $\color{red}{\left(1+\beta{\beta}^\prime\right)}$ was needed in equation $(2)$ , but there was no way I could intuit the need for this $\color{red}{\left(1+\beta{\beta}^\prime\right)}$ factor without direct substitution as I did above. Is there some deeper technique going on here that eludes me, like matrix diagonalization? The other equation (in red) I am questioning is $(3)$ and I really don't understand what the author is trying to do with this. So, if anyone has any insights or thought's on why this is being done please let me know. N.B . There is more to equation $(3)$ than I have written above, but I didn't want to ask too many questions at once, so for now I am just questioning the need for $\color{red}{{\beta}^{\prime\prime}\gamma({\beta}^{\prime\prime})=\gamma(\beta)\gamma({\beta}^\prime)\left(\beta+{\beta}^\prime\right)}$ .","In the following post, I am questioning certain parts of the author's solution for the beginning of a proof that the Lorentz boosts form a one-parameter Lie group: Show that the Lorentz boosts, form a one-parameter Lie group. Is this group Abelian? Here is the author's solution: The Lorentz boosts, , are defined by where and is the speed of light, so . To show that these matrices form a Lie group, we must demonstrate that these matrices under matrix multiplication satisfy the group axioms. Closure . The product of two boosts and is For this product to have the form we must require that I would like to understand the reasoning behind the need for the red equations, and . Taking equation for now, and plugging it back into , So I see why the factor of was needed in equation , but there was no way I could intuit the need for this factor without direct substitution as I did above. Is there some deeper technique going on here that eludes me, like matrix diagonalization? The other equation (in red) I am questioning is and I really don't understand what the author is trying to do with this. So, if anyone has any insights or thought's on why this is being done please let me know. N.B . There is more to equation than I have written above, but I didn't want to ask too many questions at once, so for now I am just questioning the need for .","\begin{pmatrix}\gamma & -\beta\gamma\\  -\beta\gamma & \gamma\end{pmatrix} \Lambda \Lambda(\beta)=\begin{pmatrix}\gamma & -\beta\gamma\\  -\beta\gamma & \gamma\end{pmatrix}=\gamma(\beta)\begin{pmatrix}1 & -\beta\\  -\beta & 1\end{pmatrix} \gamma(\beta)=\frac{1}{\sqrt{1-{\beta}^2}},\quad \beta=\frac{v}{c}, \quad c \lvert v \rvert\lt c \Lambda(\beta ) \Lambda({\beta}^{\prime}) \begin{align}\Lambda(\beta )\Lambda({\beta}^{\prime})&=\gamma({\beta})\gamma({\beta}^{\prime})\begin{pmatrix}1 & -\beta\\  -\beta & 1\end{pmatrix}\begin{pmatrix}1 & -{\beta}^\prime\\  -{\beta}^\prime & 1\end{pmatrix}\\&=\gamma({\beta})\gamma({\beta}^{\prime})\begin{pmatrix}1+\beta{\beta}^\prime & -\beta-{\beta}^\prime\\  -\beta-{\beta}^\prime & 1+\beta{\beta}^\prime\end{pmatrix}\end{align} \Lambda(\beta^{\prime\prime})=\gamma(\beta^{\prime\prime})\begin{pmatrix}1 & -{\beta}^{\prime\prime}\\  -{\beta}^{\prime\prime} & 1\end{pmatrix}\tag{1} \color{red}{\gamma(\beta^{\prime\prime})=\gamma(\beta)\gamma({\beta}^\prime)\left(1+\beta{\beta}^\prime\right)}\tag{2} {\begin{align}\color{red}{{\beta}^{\prime\prime}\gamma({\beta}^{\prime\prime})=\gamma(\beta)\gamma({\beta}^\prime)\left(\beta+{\beta}^\prime\right)}\tag{3}&\\=[\cdots]\end{align}} (2) (3) (2) (1) \begin{align}\Lambda(\beta^{\prime\prime})&=\gamma(\beta^{\prime\prime})\begin{pmatrix}1 & -{\beta}^{\prime\prime}\\  -{\beta}^{\prime\prime} & 1\end{pmatrix}\\&=\gamma(\beta)\gamma({\beta}^\prime)\left(1+\beta{\beta}^\prime\right)\begin{pmatrix}1 & -{\beta}^{\prime\prime}\\  -{\beta}^{\prime\prime} & 1\end{pmatrix}\\&=\gamma(\beta)\gamma({\beta}^\prime)\begin{pmatrix}1 +\beta{\beta}^\prime & -\left(1 +\beta{\beta}^\prime\right){\beta}^{\prime\prime}\\  -\left(1 +\beta{\beta}^\prime\right){\beta}^{\prime\prime} & 1 +\beta{\beta}^\prime\end{pmatrix}\\&=\gamma(\beta)\gamma({\beta}^\prime)\left(1+\beta{\beta}^\prime\right)\begin{pmatrix}1 & -{\beta}^{\prime\prime}\\  -{\beta}^{\prime\prime} & 1\end{pmatrix}\end{align} \color{red}{\left(1+\beta{\beta}^\prime\right)} (2) \color{red}{\left(1+\beta{\beta}^\prime\right)} (3) (3) \color{red}{{\beta}^{\prime\prime}\gamma({\beta}^{\prime\prime})=\gamma(\beta)\gamma({\beta}^\prime)\left(\beta+{\beta}^\prime\right)}","['abstract-algebra', 'matrices', 'group-theory', 'representation-theory', 'lie-groups']"
16,A map homotopic to a $\mathbb{Z}_2$-equivariant map is $\mathbb{Z}_2$-equivariant.,A map homotopic to a -equivariant map is -equivariant.,\mathbb{Z}_2 \mathbb{Z}_2,"Assume an antipodal homeomorphism $\nu_n: S^n \rightarrow S^n$ mapping $x$ to $-x$ , (it can be seen as an action of $\mathbb{Z}_2$ on $S^n$ ). What I want to prove or reject is: Suppose $h:S^2 \rightarrow S^1$ is a $\mathbb{Z}_2$ -equivariant map. If $g:S^2 \rightarrow S^1$ is homotopic to $h$ , then $g$ is a $\mathbb{Z}_2$ -equivariant map too. This claim might be true in general for any $G$ -equivariant map on any topological space. The general statement would be: Suppose $X$ and $Y$ are two topological spaces equipped with an action of the group $G$ (i.e. $X$ and $Y$ are $G$ -spaces). If $h:X \rightarrow Y$ is a $G$ -equivariant map homotopic to the map $g:X \rightarrow Y$ , then $g$ is $G$ -equivariant too. I am trying to prove the statement for $G=\mathbb{Z}_2$ , $X=S^2$ , and $Y=S^1$ but it might be wrong. So, either a proof or a counter example would help me. I know equivariant homotopy theory is interested in studying the homotopy classes of equivariant maps between $X$ and $Y$ but I guess my question does not fit into this subject because we do not know yet if the map $g$ is $G$ -equivariant or not. I have not seen this question anywhere. This is my own thought. I would appreciate any hint or help.","Assume an antipodal homeomorphism mapping to , (it can be seen as an action of on ). What I want to prove or reject is: Suppose is a -equivariant map. If is homotopic to , then is a -equivariant map too. This claim might be true in general for any -equivariant map on any topological space. The general statement would be: Suppose and are two topological spaces equipped with an action of the group (i.e. and are -spaces). If is a -equivariant map homotopic to the map , then is -equivariant too. I am trying to prove the statement for , , and but it might be wrong. So, either a proof or a counter example would help me. I know equivariant homotopy theory is interested in studying the homotopy classes of equivariant maps between and but I guess my question does not fit into this subject because we do not know yet if the map is -equivariant or not. I have not seen this question anywhere. This is my own thought. I would appreciate any hint or help.",\nu_n: S^n \rightarrow S^n x -x \mathbb{Z}_2 S^n h:S^2 \rightarrow S^1 \mathbb{Z}_2 g:S^2 \rightarrow S^1 h g \mathbb{Z}_2 G X Y G X Y G h:X \rightarrow Y G g:X \rightarrow Y g G G=\mathbb{Z}_2 X=S^2 Y=S^1 X Y g G,"['abstract-algebra', 'general-topology', 'algebraic-topology']"
17,Rotman's Algebraic Topology Theorem 8.24,Rotman's Algebraic Topology Theorem 8.24,,"I'm trying to understand the proof of Theorem 8.24 of Rotman's Introduction to Algebraic Topology. Two steps in his proof don't seem very obvious to me: (blue highlight) Why do we need the ""X is Hausdorff"" statement to show that $\bar{e} = \Phi^n_{\alpha}(D^n)$ for some $\alpha$ ? (green highlight) The inductive hypothesis says that $X^{n-1}$ has a weak topology determined by $\{\bar{e}: dim(e) \leq n-1\}$ , but I am not sure how I can proceed onward by applying Lemma 8.16. Thanks everyone in advance! Here is the main theorem: and here is his proof: Lemma 8.15: Lemma 8.16:","I'm trying to understand the proof of Theorem 8.24 of Rotman's Introduction to Algebraic Topology. Two steps in his proof don't seem very obvious to me: (blue highlight) Why do we need the ""X is Hausdorff"" statement to show that for some ? (green highlight) The inductive hypothesis says that has a weak topology determined by , but I am not sure how I can proceed onward by applying Lemma 8.16. Thanks everyone in advance! Here is the main theorem: and here is his proof: Lemma 8.15: Lemma 8.16:",\bar{e} = \Phi^n_{\alpha}(D^n) \alpha X^{n-1} \{\bar{e}: dim(e) \leq n-1\},"['abstract-algebra', 'algebraic-topology']"
18,"Ring homomorphisms between $C([0,1], \mathbb{R})$ and $C(\text{Cantor set},\mathbb{R})$",Ring homomorphisms between  and,"C([0,1], \mathbb{R}) C(\text{Cantor set},\mathbb{R})","Let $K \subseteq [0,1]$ be the Cantor set. $C([0,1], \mathbb{R})$ be the ring of continuous functions from $[0,1]$ to $\mathbb{R}$ . $C(K,\mathbb{R})$ be the ring of continuous functions from $K$ to $\mathbb{R}$ . Then, what can we say about the homomorphisms from $C([0,1], \mathbb{R})$ to $C(K,\mathbb{R})$ ? How many of them are injective ? How many of them are surjective ?","Let be the Cantor set. be the ring of continuous functions from to . be the ring of continuous functions from to . Then, what can we say about the homomorphisms from to ? How many of them are injective ? How many of them are surjective ?","K \subseteq [0,1] C([0,1], \mathbb{R}) [0,1] \mathbb{R} C(K,\mathbb{R}) K \mathbb{R} C([0,1], \mathbb{R}) C(K,\mathbb{R})","['abstract-algebra', 'ring-theory', 'continuity', 'cantor-set']"
19,"Is $(x_1x_2+x_3x_4, 2x_1x_2+x_5x_6)$ a prime ideal?",Is  a prime ideal?,"(x_1x_2+x_3x_4, 2x_1x_2+x_5x_6)","I'm studying for my Qualifying exam and I found the following question in an old question bank. Let $\mathbb{K}$ be an algebraically closed field (char $\mathbb{K}\not=2$ ). Is $\mathbb{K}[x_1,x_2,x_3,x_4,x_5,x_6]/(x_1x_2+x_3x_4, 2x_1x_2+x_5x_6)$ an integral domain? I have proven that each of the generators of the ideal is irreducible, and hence prime and I thought ideal generated by prime elements will be a prime ideal, but it turns out that's not true. TBH, I don't really know any tricks to prove an ideal is a prime ideal, other than the definition and the integral domain condition.","I'm studying for my Qualifying exam and I found the following question in an old question bank. Let be an algebraically closed field (char ). Is an integral domain? I have proven that each of the generators of the ideal is irreducible, and hence prime and I thought ideal generated by prime elements will be a prime ideal, but it turns out that's not true. TBH, I don't really know any tricks to prove an ideal is a prime ideal, other than the definition and the integral domain condition.","\mathbb{K} \mathbb{K}\not=2 \mathbb{K}[x_1,x_2,x_3,x_4,x_5,x_6]/(x_1x_2+x_3x_4, 2x_1x_2+x_5x_6)","['abstract-algebra', 'algebraic-geometry', 'commutative-algebra']"
20,If $F \leq\overline{\mathbb{Q}}(x)$ then either $F/\mathbb{Q}$ or $\overline{\mathbb{Q}}(x)/F$ is algebraic.,If  then either  or  is algebraic.,F \leq\overline{\mathbb{Q}}(x) F/\mathbb{Q} \overline{\mathbb{Q}}(x)/F,"Suppose that $\overline{\mathbb{Q}}$ is the algebraic closure of $\mathbb{Q}$ , and let $F \leq \overline{\mathbb{Q}}(x)$ be a subfield. Then I want to show that either $F/\mathbb{Q}$ or $\overline{\mathbb{Q}}(x)/F$ is an algebraic extension. I guess I get the idea somewhat- If $F/\mathbb{Q}$ is not algebraic then that must mean that in some form $F = K(x)$ where $K \leq \mathbb{Q}$ , and then obviously we do get that $\overline{\mathbb{Q}}(x)/K(x)$ is algebraic. That is the intuitive answer, but how does one formalize this? If $F/\mathbb{Q}$ is not algebraic, then there is $y \in F \subseteq \overline{\mathbb{Q}}(x)$ that is transcendental. Does this directly imply that $x \in F$ ? I'm sort of confused here. And if we do get that $x \in F$ , we get that $F \geq \mathbb{Q}(x)$ , but can one say that $F = K(x)$ for some $K \geq \mathbb{Q}$ ?","Suppose that is the algebraic closure of , and let be a subfield. Then I want to show that either or is an algebraic extension. I guess I get the idea somewhat- If is not algebraic then that must mean that in some form where , and then obviously we do get that is algebraic. That is the intuitive answer, but how does one formalize this? If is not algebraic, then there is that is transcendental. Does this directly imply that ? I'm sort of confused here. And if we do get that , we get that , but can one say that for some ?",\overline{\mathbb{Q}} \mathbb{Q} F \leq \overline{\mathbb{Q}}(x) F/\mathbb{Q} \overline{\mathbb{Q}}(x)/F F/\mathbb{Q} F = K(x) K \leq \mathbb{Q} \overline{\mathbb{Q}}(x)/K(x) F/\mathbb{Q} y \in F \subseteq \overline{\mathbb{Q}}(x) x \in F x \in F F \geq \mathbb{Q}(x) F = K(x) K \geq \mathbb{Q},"['abstract-algebra', 'field-theory', 'galois-theory', 'extension-field', 'transcendental-numbers']"
21,"Given $H\subseteq G$ with $G$ a group, then $HH^{-1} =H \implies aH = H \ \forall a \in H$.","Given  with  a group, then .",H\subseteq G G HH^{-1} =H \implies aH = H \ \forall a \in H,"I'm solving a problem which deals with equivalent definitions of a subgroup. The problem states that: Given a subset $H \neq \emptyset$ of a group $G$ , prove that the following are equivalent: $H$ is a subgroup of $G$ . $H = H^2$ and $H^{-1}=H$ $HH^{-1}=H$ $aH = H, \quad  \forall a \in H$ where $XY = \left\{xy \ \vert \ x \in X, y \in Y\right\}$ , $H^2 =HH$ , $H^{-1} = \{h^{-1} \ |\  h \in h\}$ and $aH = \{ah \ | \ h \in H\}$ . I've seen that in problems of this type a common strategy is to make an ""implication chain"", i.e., show that $1 \implies 2, 2 \implies 3, 3 \implies 4, 4\implies 1$ and this proves equivalence between all the statements. I attempted to follow this idea in my proof. I've managed to show that $1 \implies 2$ and $2 \implies 3$ , however, I got stuck on the proof that $3 \implies 4$ . My attempt went something like this. We need to show that given any $x \in H$ and $y \in H^{-1}$ we can write $xy = ah$ for some $h \in H$ . The problem is that I couldn't find a way to relate $y \in H$ without using that $H$ is a subgroup (which is what I want to prove, so I can't assume this). I've looked at proofs of $1 \implies 4$ like the one here , but every proof I've seen explicitly uses the fact that $H$ is a subgroup to assert that $h^{-1} \in H$ , but in $3 \implies 4$ I cannot assume that, only that $h^{-1} \in G$ . Does anyone know how I could prove $3 \implies 4$ ? Thank you!","I'm solving a problem which deals with equivalent definitions of a subgroup. The problem states that: Given a subset of a group , prove that the following are equivalent: is a subgroup of . and where , , and . I've seen that in problems of this type a common strategy is to make an ""implication chain"", i.e., show that and this proves equivalence between all the statements. I attempted to follow this idea in my proof. I've managed to show that and , however, I got stuck on the proof that . My attempt went something like this. We need to show that given any and we can write for some . The problem is that I couldn't find a way to relate without using that is a subgroup (which is what I want to prove, so I can't assume this). I've looked at proofs of like the one here , but every proof I've seen explicitly uses the fact that is a subgroup to assert that , but in I cannot assume that, only that . Does anyone know how I could prove ? Thank you!","H \neq \emptyset G H G H = H^2 H^{-1}=H HH^{-1}=H aH = H, \quad  \forall a \in H XY = \left\{xy \ \vert \ x \in X, y \in Y\right\} H^2 =HH H^{-1} = \{h^{-1} \ |\  h \in h\} aH = \{ah \ | \ h \in H\} 1 \implies 2, 2 \implies 3, 3 \implies 4, 4\implies 1 1 \implies 2 2 \implies 3 3 \implies 4 x \in H y \in H^{-1} xy = ah h \in H y \in H H 1 \implies 4 H h^{-1} \in H 3 \implies 4 h^{-1} \in G 3 \implies 4","['abstract-algebra', 'group-theory']"
22,Do the Moufang identities *themselves* imply diassociativity / Moufang's theorem / Artin's theorem?,Do the Moufang identities *themselves* imply diassociativity / Moufang's theorem / Artin's theorem?,,"A Moufang loop is a loop satisfying the Moufang identities .  Famously, these are diassociative -- the subloop generated by any two elements is associative (is a group) -- and more generally, they satisfy Moufang's theorem, that if any three elements associate, then so do anything they generate (i.e. they generate a group). Separately, if you have an alternative algebra or ring , then multiplication in it also satisfies the Moufang identities, is also diassociative, and also satisfies the analogue of Moufang's theorem. Now, in the case where every nonzero element has an inverse (as in the octonions), you could prove the latter by just appealing to Moufang's theorem for loops.  But in general you can't do that. So, something is going on here -- the Moufang identities, together with inverses, imply Moufang's theorem; and the Moufang identities, together with the existence of an addition operation that our multiplication distributes over, implies Moufang's theorem. It seems really funny to me that in both these cases, these identities imply the same result, but in each case, we need a different auxiliary assumption to make it work. So: Do the Moufang identities themselves imply Moufang's theorem?  That is to say, if we have a magma (and let's say it has an identity because we may as well), and it satisfies the Moufang identities, does it automatically satisfy the analogue of Moufang's theorem, including being diassocative ?  Or is there some counterexample to this? (And if the theorem doesn't hold in this setting, is there some simple additional assumption we could make, that would make it true, while also covering both the cases above?) I'm really wondering about this because this seems like an obvious question to ask, whether we can unify these two settings, but I haven't seen an answer stated anywhere. Thank you all!","A Moufang loop is a loop satisfying the Moufang identities .  Famously, these are diassociative -- the subloop generated by any two elements is associative (is a group) -- and more generally, they satisfy Moufang's theorem, that if any three elements associate, then so do anything they generate (i.e. they generate a group). Separately, if you have an alternative algebra or ring , then multiplication in it also satisfies the Moufang identities, is also diassociative, and also satisfies the analogue of Moufang's theorem. Now, in the case where every nonzero element has an inverse (as in the octonions), you could prove the latter by just appealing to Moufang's theorem for loops.  But in general you can't do that. So, something is going on here -- the Moufang identities, together with inverses, imply Moufang's theorem; and the Moufang identities, together with the existence of an addition operation that our multiplication distributes over, implies Moufang's theorem. It seems really funny to me that in both these cases, these identities imply the same result, but in each case, we need a different auxiliary assumption to make it work. So: Do the Moufang identities themselves imply Moufang's theorem?  That is to say, if we have a magma (and let's say it has an identity because we may as well), and it satisfies the Moufang identities, does it automatically satisfy the analogue of Moufang's theorem, including being diassocative ?  Or is there some counterexample to this? (And if the theorem doesn't hold in this setting, is there some simple additional assumption we could make, that would make it true, while also covering both the cases above?) I'm really wondering about this because this seems like an obvious question to ask, whether we can unify these two settings, but I haven't seen an answer stated anywhere. Thank you all!",,"['abstract-algebra', 'universal-algebra', 'magma', 'quasigroups', 'nonassociative-algebras']"
23,Chevalley-Eilenberg complexes of curved dg Lie algebras,Chevalley-Eilenberg complexes of curved dg Lie algebras,,"Let $k$ be a field of characterisitic $0$ . A curved dg Lie algebra (curved dgla) is a triple $(\mathfrak{g},d,w)$ where $\mathfrak{g}$ is a graded Lie algebra, $d$ is a derivation with degree $1$ and $w \in \mathfrak{g} _2$ such that $d(w) = 0$ and $d^2(x) = [w,x]$ . The element $w$ s called the curvature. For any curved dgla $(\mathfrak{g}, d, w)$ , we can construct a cochain complex CE $(\mathfrak{g},d,w)$ called Chevalley-Eilenberg complex. CE $(\mathfrak{g},d,w)$ is isomorphic to Sym $(\mathfrak{g}[1])^*$ as a graded algebra. The differential $d'$ of CE $(\mathfrak{g},d,w)$ is a sum $d' = d'_0 + d'_1 +d'_2$ where the restriction is given by $d'_0$ : $(\mathfrak{g}[1])^* \rightarrow k$ is given by the evaluation on $w$ the curvature of $\mathfrak{g}$ . $d'_1$ : $(\mathfrak{g}[1])^* \rightarrow (\mathfrak{g}[1])^*$ is given by the differential on $g$ . $d'_2$ : $(\mathfrak{g}[1])^* \rightarrow S^2(\mathfrak{g}[1])^*$ is given by the bracket on $\mathfrak{g}$ . It seems that the evaluation of $w$ can be defined only on an element of $(\mathfrak{g}[1]_1)^*$ for me. How is define the evaluation of $w$ on an element of $(\mathfrak{g}[1]_i)^*$ $(i \neq 1)$ ? And, is $d_0$ of degree $-1$ ?","Let be a field of characterisitic . A curved dg Lie algebra (curved dgla) is a triple where is a graded Lie algebra, is a derivation with degree and such that and . The element s called the curvature. For any curved dgla , we can construct a cochain complex CE called Chevalley-Eilenberg complex. CE is isomorphic to Sym as a graded algebra. The differential of CE is a sum where the restriction is given by : is given by the evaluation on the curvature of . : is given by the differential on . : is given by the bracket on . It seems that the evaluation of can be defined only on an element of for me. How is define the evaluation of on an element of ? And, is of degree ?","k 0 (\mathfrak{g},d,w) \mathfrak{g} d 1 w \in \mathfrak{g} _2 d(w) = 0 d^2(x) = [w,x] w (\mathfrak{g}, d, w) (\mathfrak{g},d,w) (\mathfrak{g},d,w) (\mathfrak{g}[1])^* d' (\mathfrak{g},d,w) d' = d'_0 + d'_1 +d'_2 d'_0 (\mathfrak{g}[1])^* \rightarrow k w \mathfrak{g} d'_1 (\mathfrak{g}[1])^* \rightarrow (\mathfrak{g}[1])^* g d'_2 (\mathfrak{g}[1])^* \rightarrow S^2(\mathfrak{g}[1])^* \mathfrak{g} w (\mathfrak{g}[1]_1)^* w (\mathfrak{g}[1]_i)^* (i \neq 1) d_0 -1","['abstract-algebra', 'lie-algebras', 'homological-algebra']"
24,Uniqueness of Semidirect Product $(\mathbb{F}_p\times\mathbb{F}_p)\rtimes\mathbb{F}_p$,Uniqueness of Semidirect Product,(\mathbb{F}_p\times\mathbb{F}_p)\rtimes\mathbb{F}_p,"Let $p$ be an odd prime. For a non-abelian group $G$ of order $p^3$ in which no element is of order $p^2$ , it is isomorphic to a semidirect product $$(\mathbb{F}_p\times\mathbb{F}_p)\rtimes\mathbb{F}_p.$$ Here I am trying to see if such semidirect product is unique up to isomorphism. Note that $$\operatorname{Aut}(\mathbb{F}_p\times\mathbb{F}_p)\cong\operatorname{GL}(2,\mathbb{F}_p),$$ so for convenience, we shall consider a nontrivial group homomorphism $\phi:\mathbb{F}_p\to\operatorname{GL}(2,\mathbb{F}_p)$ . Since $\phi$ is nontrivial, it must be injective in this case, so $\phi(1)=:A$ is of order $p$ . It has been shown from this post that $A$ has Jordan canonical form $J:=J_2(1)=\begin{bmatrix} 1 & 1 \\ 0 & 1 \end{bmatrix}$ . That is, $A=BJB^{-1}$ for some $B\in\operatorname{GL}(2,\mathbb{F}_p)$ . Now let $\psi:\mathbb{F}_p\to\operatorname{GL}(2,\mathbb{F}_p)$ be another nontrivial group homomorphism. Then $\psi(1)=CJC^{-1}$ for some $C\in\operatorname{GL}(2,\mathbb{F}_p)$ . By the following theorem: Theorem. Let $H$ and $K$ be two groups and $\phi,\psi:K\to\operatorname{Aut}(H)$ be group homomorphisms. If $\psi=\phi\circ f$ for some $f\in\operatorname{Aut}(K)$ , then $$H\rtimes_\phi K\cong H\rtimes_\psi K.$$ it suffices to find some $f\in\operatorname{Aut}(\mathbb{F}_p)$ such that $\psi=\phi\circ f$ . Suppose $f(1)=k$ . Then $$(\phi\circ f)(1)=\phi(f(1))=\phi(k)=\phi(1)^k=(BJB^{-1})^k=BJ^kB^{-1}.$$ If $\psi=\phi\circ f$ , then we shall have $$CJC^{-1}=\psi(1)=(\phi\circ f)(1)=BJ^kB^{-1}\implies (B^{-1}C)J=J^k(B^{-1}C).$$ Let $S:=B^{-1}C=\begin{bmatrix} a & b \\ c & d \end{bmatrix}$ . It follows that $$\begin{bmatrix}          a & a+b \\ c & c+d      \end{bmatrix}=\begin{bmatrix}          a & b \\ c & d      \end{bmatrix}\begin{bmatrix}          1 & 1 \\ 0 & 1      \end{bmatrix}=SJ=J^kS=\begin{bmatrix}          1 & k \\ 0 & 1      \end{bmatrix}\begin{bmatrix}          a & b \\ c & d      \end{bmatrix}=\begin{bmatrix}          a+kc & b+kd \\ c & d      \end{bmatrix}.$$ Then we have $c=0$ and $a=kd$ , which are not necessarily satisfied since $B$ and $C$ are arbitrary. Then I got stuck with this step. I'd appreciate it if anyone has good ideas on this.","Let be an odd prime. For a non-abelian group of order in which no element is of order , it is isomorphic to a semidirect product Here I am trying to see if such semidirect product is unique up to isomorphism. Note that so for convenience, we shall consider a nontrivial group homomorphism . Since is nontrivial, it must be injective in this case, so is of order . It has been shown from this post that has Jordan canonical form . That is, for some . Now let be another nontrivial group homomorphism. Then for some . By the following theorem: Theorem. Let and be two groups and be group homomorphisms. If for some , then it suffices to find some such that . Suppose . Then If , then we shall have Let . It follows that Then we have and , which are not necessarily satisfied since and are arbitrary. Then I got stuck with this step. I'd appreciate it if anyone has good ideas on this.","p G p^3 p^2 (\mathbb{F}_p\times\mathbb{F}_p)\rtimes\mathbb{F}_p. \operatorname{Aut}(\mathbb{F}_p\times\mathbb{F}_p)\cong\operatorname{GL}(2,\mathbb{F}_p), \phi:\mathbb{F}_p\to\operatorname{GL}(2,\mathbb{F}_p) \phi \phi(1)=:A p A J:=J_2(1)=\begin{bmatrix} 1 & 1 \\ 0 & 1 \end{bmatrix} A=BJB^{-1} B\in\operatorname{GL}(2,\mathbb{F}_p) \psi:\mathbb{F}_p\to\operatorname{GL}(2,\mathbb{F}_p) \psi(1)=CJC^{-1} C\in\operatorname{GL}(2,\mathbb{F}_p) H K \phi,\psi:K\to\operatorname{Aut}(H) \psi=\phi\circ f f\in\operatorname{Aut}(K) H\rtimes_\phi K\cong H\rtimes_\psi K. f\in\operatorname{Aut}(\mathbb{F}_p) \psi=\phi\circ f f(1)=k (\phi\circ f)(1)=\phi(f(1))=\phi(k)=\phi(1)^k=(BJB^{-1})^k=BJ^kB^{-1}. \psi=\phi\circ f CJC^{-1}=\psi(1)=(\phi\circ f)(1)=BJ^kB^{-1}\implies (B^{-1}C)J=J^k(B^{-1}C). S:=B^{-1}C=\begin{bmatrix} a & b \\ c & d \end{bmatrix} \begin{bmatrix} 
        a & a+b \\ c & c+d 
    \end{bmatrix}=\begin{bmatrix} 
        a & b \\ c & d 
    \end{bmatrix}\begin{bmatrix} 
        1 & 1 \\ 0 & 1 
    \end{bmatrix}=SJ=J^kS=\begin{bmatrix} 
        1 & k \\ 0 & 1 
    \end{bmatrix}\begin{bmatrix} 
        a & b \\ c & d 
    \end{bmatrix}=\begin{bmatrix} 
        a+kc & b+kd \\ c & d 
    \end{bmatrix}. c=0 a=kd B C","['abstract-algebra', 'group-theory', 'finite-groups', 'semidirect-product']"
25,Rings with primal term reducts,Rings with primal term reducts,,"This question is a follow-up to this one . Say that a term reduct of a ring $\mathcal{R}=(R; +,\times,0,1)$ is a magma $\mathcal{M}$ whose domain is $R$ and whose magma operation is $(x,y)\mapsto t(x,y)$ for some (parameter-free, two-variable) term in the language of rings (denote this magma by "" $\mathcal{R}_t$ ""). For example, if $\mathcal{S}=\mathbb{Z}/2\mathbb{Z}$ and $u(x,y)=(x+1)(y+1)$ then $\mathcal{S}_u$ is isomorphic the magma with domain $\{0,1\}$ and operation $(x,y)\mapsto \max\{1-x,1-y\}$ . I'm interested in the number of terms, or at least unary terms, of term reducts of rings. Specifically: An algebra is primal iff every function on the algebra is represented by a term. An algebra is unary-rich iff every unary function on the algebra is represented by a term. For example, the $\mathcal{S}_u$ above is unary-rich (and I think primal, but I'm not certain about that). My question is: Which rings have primal (or at least unary-rich) term reducts? Even the $\mathbb{Z}/p\mathbb{Z}$ s seem nontrivial here. Note that no magma with more than one element which has an idempotent can be unary-rich, so we need to look at terms more complicated than just $+$ or $\times$ .","This question is a follow-up to this one . Say that a term reduct of a ring is a magma whose domain is and whose magma operation is for some (parameter-free, two-variable) term in the language of rings (denote this magma by "" ""). For example, if and then is isomorphic the magma with domain and operation . I'm interested in the number of terms, or at least unary terms, of term reducts of rings. Specifically: An algebra is primal iff every function on the algebra is represented by a term. An algebra is unary-rich iff every unary function on the algebra is represented by a term. For example, the above is unary-rich (and I think primal, but I'm not certain about that). My question is: Which rings have primal (or at least unary-rich) term reducts? Even the s seem nontrivial here. Note that no magma with more than one element which has an idempotent can be unary-rich, so we need to look at terms more complicated than just or .","\mathcal{R}=(R; +,\times,0,1) \mathcal{M} R (x,y)\mapsto t(x,y) \mathcal{R}_t \mathcal{S}=\mathbb{Z}/2\mathbb{Z} u(x,y)=(x+1)(y+1) \mathcal{S}_u \{0,1\} (x,y)\mapsto \max\{1-x,1-y\} \mathcal{S}_u \mathbb{Z}/p\mathbb{Z} + \times","['abstract-algebra', 'model-theory', 'universal-algebra', 'magma']"
26,can there be a prime ideal between a minimal containment of homogeneous primes?,can there be a prime ideal between a minimal containment of homogeneous primes?,,"If $\mathfrak p_0 \subsetneq \mathfrak p_1\subset k[x_0,...,x_n]$ are homogeneous prime ideals such that there is no homogeneous prime ideal strictly between them, could there be a (non-homogeneous) prime ideal strictly between them? Part of me thinks the answer is probably no, but I couldn't come up with a counterexample. Part of me thinks the answer might be yes since the dimension of a projective variety seems like it should be one less than that of its cone, although idk if that makes any sense or if I'm even using any of those terms correctly. Entertaining the possibility that it's true, my only progress has been to note that if such a prime ideal $\frak q$ exists, then the set $\mathfrak q - \mathfrak p_0$ must not contain any homogeneous elements, since if one such $f$ exists, then $\mathfrak q$ is a minimal prime over $(f)+\mathfrak p_0$ , which implies $\mathfrak q$ must be homogeneous.","If are homogeneous prime ideals such that there is no homogeneous prime ideal strictly between them, could there be a (non-homogeneous) prime ideal strictly between them? Part of me thinks the answer is probably no, but I couldn't come up with a counterexample. Part of me thinks the answer might be yes since the dimension of a projective variety seems like it should be one less than that of its cone, although idk if that makes any sense or if I'm even using any of those terms correctly. Entertaining the possibility that it's true, my only progress has been to note that if such a prime ideal exists, then the set must not contain any homogeneous elements, since if one such exists, then is a minimal prime over , which implies must be homogeneous.","\mathfrak p_0 \subsetneq \mathfrak p_1\subset k[x_0,...,x_n] \frak q \mathfrak q - \mathfrak p_0 f \mathfrak q (f)+\mathfrak p_0 \mathfrak q","['abstract-algebra', 'commutative-algebra']"
27,"Why is the ring homomorphism $\Gamma: D^{o}\rightarrow \text{End}_{M_n(D)}(D^n)$, where $D$ is a division ring, a bijection?","Why is the ring homomorphism , where  is a division ring, a bijection?",\Gamma: D^{o}\rightarrow \text{End}_{M_n(D)}(D^n) D,"It is said in this answer that for any division ring $D$ , the following ring homomorphism is a bijection: $\Gamma: D^{o}\rightarrow \text{End}_{M_n(D)}(D^n)=\{M_n(D)\text{-module endomorphisms of }D^n\}\\d\mapsto \theta_d\quad \text{s.t.}\quad \theta_d(x_1,\ldots,x_n)=(x_1d,\ldots,x_nd)$ I can see why it is an injective homomorphism (if $d\neq d$ , $(x_1 d,\ldots,x_n d) \neq (x_1 d,\ldots,x_n d)$ ) but Im not sure about how to prove it is surjective too. How can I write every element of $\text{End}_{M_n(D)}(D^n)$ in that form? Oh, I also wanted to know what does the notation $D^o$ exactly mean here.","It is said in this answer that for any division ring , the following ring homomorphism is a bijection: I can see why it is an injective homomorphism (if , ) but Im not sure about how to prove it is surjective too. How can I write every element of in that form? Oh, I also wanted to know what does the notation exactly mean here.","D \Gamma: D^{o}\rightarrow \text{End}_{M_n(D)}(D^n)=\{M_n(D)\text{-module endomorphisms of }D^n\}\\d\mapsto \theta_d\quad \text{s.t.}\quad \theta_d(x_1,\ldots,x_n)=(x_1d,\ldots,x_nd) d\neq d (x_1 d,\ldots,x_n d) \neq (x_1 d,\ldots,x_n d) \text{End}_{M_n(D)}(D^n) D^o","['abstract-algebra', 'matrices', 'ring-theory', 'modules']"
28,Distributions of a group scheme as differential operators.,Distributions of a group scheme as differential operators.,,"I'm trying to understand the distributions on an affine group scheme act as differential operators. Let $X$ be a scheme and $G$ a group scheme both affine over some commutative ring $k$ . Suppose $\alpha:G\times X\to X$ is an action on $X$ . Then there is a morphism between the coordinate algebras $\Delta:k[X] \to k[G]\otimes k[X]$ , which makes $k[X]$ a comodule. Let $I$ be the kernel of the augmentation map $\varepsilon:k[G]\to k$ . A distribution of order $\leq n$ on $G$ is a $k$ -linear map $\mu:\frac{k[G]}{I^{n+1}} \to k$ . Each distribution induces a $k$ -linear endomorphism on $k[X]$ via $$D_{\mu}: k[X] \to_\Delta k[G]\otimes k[X] \to _{\mu\otimes \text{id}} k\otimes k[X] \to_\cong k[X] $$ Jantzen claims in chapter 7 of Representation of Algebraic Groups , that this endomorphism $D_{\mu}$ is a differential operator on $k[X]$ . He cites a book by Demazure and Gabriel, which I've also looked at, but they don't provide a proof either. I've been trying to prove it for quite a while now, but haven't made any progress. Does anybody know a proof of this or a resource to find one?","I'm trying to understand the distributions on an affine group scheme act as differential operators. Let be a scheme and a group scheme both affine over some commutative ring . Suppose is an action on . Then there is a morphism between the coordinate algebras , which makes a comodule. Let be the kernel of the augmentation map . A distribution of order on is a -linear map . Each distribution induces a -linear endomorphism on via Jantzen claims in chapter 7 of Representation of Algebraic Groups , that this endomorphism is a differential operator on . He cites a book by Demazure and Gabriel, which I've also looked at, but they don't provide a proof either. I've been trying to prove it for quite a while now, but haven't made any progress. Does anybody know a proof of this or a resource to find one?",X G k \alpha:G\times X\to X X \Delta:k[X] \to k[G]\otimes k[X] k[X] I \varepsilon:k[G]\to k \leq n G k \mu:\frac{k[G]}{I^{n+1}} \to k k k[X] D_{\mu}: k[X] \to_\Delta k[G]\otimes k[X] \to _{\mu\otimes \text{id}} k\otimes k[X] \to_\cong k[X]  D_{\mu} k[X],"['abstract-algebra', 'algebraic-geometry', 'algebraic-groups', 'differential-operators', 'hopf-algebras']"
29,Definition of Clifford Algebras,Definition of Clifford Algebras,,"I'm currently studying algebras, and in particular Clifford Algebras. Primarily I've been looking at this paper , and on page 7 and 8 the definition of a Clifford Algebra is given. I have a broad understanding of tensor algebras, but I'm struggling to understand the definition of Clifford algebra by the quotient algebra $T(V)$ with the ideal $I_q$ where $$I_q = < v \otimes v + q(x)1_{T(V)}>$$ In particular, it isn't clear to me why $I_q$ is a proper ideal, or how to interpret its elements, and further the elements of $T(V)/I_q$ . I know this might be a lot of questions all in one, but any insights would be greatly appreciated!","I'm currently studying algebras, and in particular Clifford Algebras. Primarily I've been looking at this paper , and on page 7 and 8 the definition of a Clifford Algebra is given. I have a broad understanding of tensor algebras, but I'm struggling to understand the definition of Clifford algebra by the quotient algebra with the ideal where In particular, it isn't clear to me why is a proper ideal, or how to interpret its elements, and further the elements of . I know this might be a lot of questions all in one, but any insights would be greatly appreciated!",T(V) I_q I_q = < v \otimes v + q(x)1_{T(V)}> I_q T(V)/I_q,"['abstract-algebra', 'tensors', 'clifford-algebras']"
30,Bijective function in a commutative diagram,Bijective function in a commutative diagram,,"Let $X,Y, Z$ be sets and $p_x:Z\rightarrow X$ and $p_y:Z \rightarrow Y$ so that for every Triple $(M, f_x, f_y)$ exactly one function $g:M\rightarrow Z$ exists, which makes following diagram commutative (Commutative $\Leftrightarrow f_x=g\circ p_x$ and $f_y=g\circ p_y$ ). Show that for $(M, f_x, f_y)=(X\times Y, \pi_x, \pi_y)$ , the received function $g:X\times Y \rightarrow Z$ is a Bijection. $\pi_x(x,y) = x$ and $\pi_y(x,y) = y$ My attempt: Since the diagram is commutative, it follows... $$x = \pi_x(x,y) = (p_x \circ g)(x,y) \Rightarrow x = (p_x \circ g)(x,y)$$ $$y = \pi_y(x,y) = (p_y \circ g)(x,y) \Rightarrow y = (p_y \circ g)(x,y)$$ So I think in order to get the desired values, $g$ must be... $$g(x,y) = (p_x^{-1} \circ \pi_x)(x) \cap (p_y^{-1}\circ \pi_y)(y)$$ I'm unsure what to do next. I could also say... $$ g^{-1}(x,y) = (\pi_x^{-1}\circ p_x)(x) \cap (\pi_y^{-1}\circ p_y)(y)$$ But since I don't know if the functions $\pi, p$ themselves are bijective, I cannot conclude that $g^{-1}$ exists I'd like to have some tips on how to proceed.","Let be sets and and so that for every Triple exactly one function exists, which makes following diagram commutative (Commutative and ). Show that for , the received function is a Bijection. and My attempt: Since the diagram is commutative, it follows... So I think in order to get the desired values, must be... I'm unsure what to do next. I could also say... But since I don't know if the functions themselves are bijective, I cannot conclude that exists I'd like to have some tips on how to proceed.","X,Y, Z p_x:Z\rightarrow X p_y:Z \rightarrow Y (M, f_x, f_y) g:M\rightarrow Z \Leftrightarrow f_x=g\circ p_x f_y=g\circ p_y (M, f_x, f_y)=(X\times Y, \pi_x, \pi_y) g:X\times Y \rightarrow Z \pi_x(x,y) = x \pi_y(x,y) = y x = \pi_x(x,y) = (p_x \circ g)(x,y) \Rightarrow x = (p_x \circ g)(x,y) y = \pi_y(x,y) = (p_y \circ g)(x,y) \Rightarrow y = (p_y \circ g)(x,y) g g(x,y) = (p_x^{-1} \circ \pi_x)(x) \cap (p_y^{-1}\circ \pi_y)(y)  g^{-1}(x,y) = (\pi_x^{-1}\circ p_x)(x) \cap (\pi_y^{-1}\circ p_y)(y) \pi, p g^{-1}","['abstract-algebra', 'commutative-algebra']"
31,"Given a finite solvable group $G$, prove that a minimal normal subgroup $H$ is a $p$-group","Given a finite solvable group , prove that a minimal normal subgroup  is a -group",G H p,"Given a finite solvable group $G$ , and a minimal normal subgroup $H$ , prove that $H$ is a $p$ -subgroup for some prime $p$ . My Attempt: I am trying to write this proof without using the term ""characteristic subgroup"". I'm aware to the fact that by proving that $p$ -subgroup of $M$ is a characteristic subgroup will finish the proof. Let $p$ be a prime number such that $p$ is a divisor of the order of $M$ . By Sylow theorem, there exists a $p$ -subgroup of $M$ , let it be called $S$ . I'd like to prove that every element of $M$ is of order $p^n$ , for some $n \in \mathbb{N}$ . let $s\in S, \  g\in G: g^{-1}sg \in M$ , as $S\le M$ and $M$ is normal in $G$ . but why and how I can prove that $=g^{-1}sg$ is of order $p^m$ , for some $m\in \mathbb{N}: m\le n$ .","Given a finite solvable group , and a minimal normal subgroup , prove that is a -subgroup for some prime . My Attempt: I am trying to write this proof without using the term ""characteristic subgroup"". I'm aware to the fact that by proving that -subgroup of is a characteristic subgroup will finish the proof. Let be a prime number such that is a divisor of the order of . By Sylow theorem, there exists a -subgroup of , let it be called . I'd like to prove that every element of is of order , for some . let , as and is normal in . but why and how I can prove that is of order , for some .","G H H p p p M p p M p M S M p^n n \in \mathbb{N} s\in S, \  g\in G: g^{-1}sg \in M S\le M M G =g^{-1}sg p^m m\in \mathbb{N}: m\le n","['abstract-algebra', 'group-theory', 'finite-groups', 'normal-subgroups', 'sylow-theory']"
32,Intersection of subgroups of a finite abelian group,Intersection of subgroups of a finite abelian group,,"Let $G$ be a finite abelian group and let $g \in G$ be a non-trivial element. I want to show that the intersection over all subgroups $G'$ of $G$ such that $G/G'$ is cyclic  and $g \not \in G'$ is trivial. My attempt: By the Fundamental theorem of abelian groups, $G \cong \mathbb Z/n_1 \mathbb Z \times \cdots \times  \mathbb Z/n_m \mathbb Z$ . We prove by induction on the number of cyclic factors of $G$ . For the base case ( $m=1$ ), $G$ is cyclic. Then if we let $G'=\{e\}$ , $G/G'$ is cyclic and $g \not \in G'$ . Thus, the proposition is true in this case. Now for the general case, write $G = \mathbb Z/n_1 \mathbb Z \times \cdots \times  \mathbb Z/n_m \mathbb Z$ as before and let $g=(g_1, \ldots, g_m)$ . Since $g$ is non-trivial, at least of its components is not zero. WLOG, assume $g_1 \neq 0$ . Then $h:=(g_1, \ldots, g_{m-1})$ is a non-zero element of $H:=\mathbb Z/n_1 \mathbb Z \times \cdots \times  \mathbb Z/n_{m-1} \mathbb Z$ . By the inudction hypothesis, the intersection over all subgroups $H'$ of $H$ such that $H/H'$ is cyclic  and $h \not \in H'$ is trivial. I am not sure how to proceed from here and I will be very grateful for any advice on this attempt or some ideas for an alternative attempt.","Let be a finite abelian group and let be a non-trivial element. I want to show that the intersection over all subgroups of such that is cyclic  and is trivial. My attempt: By the Fundamental theorem of abelian groups, . We prove by induction on the number of cyclic factors of . For the base case ( ), is cyclic. Then if we let , is cyclic and . Thus, the proposition is true in this case. Now for the general case, write as before and let . Since is non-trivial, at least of its components is not zero. WLOG, assume . Then is a non-zero element of . By the inudction hypothesis, the intersection over all subgroups of such that is cyclic  and is trivial. I am not sure how to proceed from here and I will be very grateful for any advice on this attempt or some ideas for an alternative attempt.","G g \in G G' G G/G' g \not \in G' G \cong \mathbb Z/n_1 \mathbb Z \times \cdots \times  \mathbb Z/n_m \mathbb Z G m=1 G G'=\{e\} G/G' g \not \in G' G = \mathbb Z/n_1 \mathbb Z \times \cdots \times  \mathbb Z/n_m \mathbb Z g=(g_1, \ldots, g_m) g g_1 \neq 0 h:=(g_1, \ldots, g_{m-1}) H:=\mathbb Z/n_1 \mathbb Z \times \cdots \times  \mathbb Z/n_{m-1} \mathbb Z H' H H/H' h \not \in H'","['abstract-algebra', 'group-theory', 'finite-groups', 'abelian-groups', 'cyclic-groups']"
33,Two definitions of representation of a topological group,Two definitions of representation of a topological group,,"Please compare the following two definitions: Definition 1 A representation of a topological group $G$ in a vector space ${\mathbb{V}}$ over complex numbers is a continuous map $$   A\, :\quad G\times {\mathbb{V}}\longrightarrow{\mathbb{V}}\;\,,\qquad (g\,,\;v)\longmapsto A(g)\,v\;\,,\;\;\;\;\; g\in G\,,\;\;v\in{\mathbb{V}}\;\; \label{1}\tag{1}   $$ with the following properties: (1) $\;\; A\,$ is a group action; (2) $\;\; A\,$ is linear, i.e. $$   A(g)\,(\alpha\, v+\beta)=\alpha\, A(g)\, v+ A(g)\, w\quad \mbox{for}\quad\forall g\in G\,,\;\; v,\; w\in {\mathbb{V}}\,,\;\; \alpha\in{\mathbb{C}}\,\;.   $$ Definition 2 A representation of a topological group $G$ in a vector space ${\mathbb{V}}$ is a homomorphism of $ G$ into the group of invertible linear transformations of ${\mathbb{V}}$ : $$  A\,:\quad G\longrightarrow GL({\mathbb{V}})\;\,. \label{2}\tag{2}  $$ $$ \, $$ Definition 2 $\;\;\Longrightarrow\;\;$ Definition 1 , Indeed, the $A$ from Definition 2 is linear and ensures $\,(g\,,\;v)\longmapsto A(g)\,v\,$ . Also, Definition 2 says that $A$ is a homomorphism $-$ which guarantees the continuity of $\,(g\,,\;v)\longmapsto A(g)\,v\;$ in Definition 1 . QED To show that Definition 1 $\;\Longrightarrow\;$ Definition 2 , we must demonstrate that Definition 1 ensures \eqref{2} being a homomorphism. How to do this?","Please compare the following two definitions: Definition 1 A representation of a topological group in a vector space over complex numbers is a continuous map with the following properties: (1) is a group action; (2) is linear, i.e. Definition 2 A representation of a topological group in a vector space is a homomorphism of into the group of invertible linear transformations of : Definition 2 Definition 1 , Indeed, the from Definition 2 is linear and ensures . Also, Definition 2 says that is a homomorphism which guarantees the continuity of in Definition 1 . QED To show that Definition 1 Definition 2 , we must demonstrate that Definition 1 ensures \eqref{2} being a homomorphism. How to do this?","G {\mathbb{V}} 
  A\, :\quad G\times {\mathbb{V}}\longrightarrow{\mathbb{V}}\;\,,\qquad (g\,,\;v)\longmapsto A(g)\,v\;\,,\;\;\;\;\; g\in G\,,\;\;v\in{\mathbb{V}}\;\; \label{1}\tag{1}
   \;\; A\, \;\; A\, 
  A(g)\,(\alpha\, v+\beta)=\alpha\, A(g)\, v+ A(g)\, w\quad \mbox{for}\quad\forall g\in G\,,\;\; v,\; w\in {\mathbb{V}}\,,\;\; \alpha\in{\mathbb{C}}\,\;.
   G {\mathbb{V}}  G {\mathbb{V}} 
 A\,:\quad G\longrightarrow GL({\mathbb{V}})\;\,. \label{2}\tag{2}
  
\,
 \;\;\Longrightarrow\;\; A \,(g\,,\;v)\longmapsto A(g)\,v\, A - \,(g\,,\;v)\longmapsto A(g)\,v\; \;\Longrightarrow\;","['abstract-algebra', 'group-theory', 'representation-theory', 'topological-groups']"
34,$1+i$ is a prime element of ring of Gaussian integers.,is a prime element of ring of Gaussian integers.,1+i,"I want to prove that $1+i$ is a prime element of ring of Gaussian integers $Z[i]$ . I started off with saying that if $1+i$ is prime then $1+i|(a+bi)(c+di)$ implies $1+i|a+bi$ or $1+i|c+di$ for $a+bi,c+di$ in $Z[i]$ . Taking conjugate, we get $1-i|(a-bi)(c-di)$ Next we get, $2|(a^2+b^2)(c^2+d^2)$ Since $2$ is a prime in $Z$ hence $2|(a^2+b^2)$ or $2|(c^2+d^2)$ . After this I am stuck. I do not know how to get $1+i|a+bi$ or $1+i|c+di$ from here. Please suggest.","I want to prove that is a prime element of ring of Gaussian integers . I started off with saying that if is prime then implies or for in . Taking conjugate, we get Next we get, Since is a prime in hence or . After this I am stuck. I do not know how to get or from here. Please suggest.","1+i Z[i] 1+i 1+i|(a+bi)(c+di) 1+i|a+bi 1+i|c+di a+bi,c+di Z[i] 1-i|(a-bi)(c-di) 2|(a^2+b^2)(c^2+d^2) 2 Z 2|(a^2+b^2) 2|(c^2+d^2) 1+i|a+bi 1+i|c+di","['abstract-algebra', 'prime-numbers', 'gaussian-integers']"
35,"Well definedness of $p:\mathbb{Z}/\gcd(m,n)\mathbb Z\rightarrow P$, universal property of the tensor product.","Well definedness of , universal property of the tensor product.","p:\mathbb{Z}/\gcd(m,n)\mathbb Z\rightarrow P","I am trying to get used to the universal property of the tensor product. I have tried to prove this common fact using the universal property $\mathbb{Z}/n\mathbb Z\otimes \mathbb{Z}/m\mathbb Z \cong \mathbb{Z}/\gcd(m,n)\mathbb Z$ After checking that there is well-defined bilinear map (which i will call $b$ ) from $\mathbb{Z}/n\mathbb Z\otimes \mathbb{Z}/m\mathbb Z$ to $\mathbb{Z}/\gcd(m,n)\mathbb Z$ we need to show that we can factor any other bilinear map of $\mathbb Z$ -modules $p:\mathbb{Z}/n\mathbb Z\otimes \mathbb{Z}/m\mathbb Z\rightarrow P$ over $b$ . The obvious thing to do is do define $i:\mathbb{Z}/\gcd(m,n)\mathbb Z\rightarrow P$ by $x\mod mn\mapsto p(x,1)$ . I am having difficulty checking that this is well defined. We need to show that $p(x,1)=p(x',1)$ where $x'=x+k\cdot \gcd(m,n)$ . But the only things we know are that $p(x,1)=p(1,x)$ $p(x+n,1)=p(x,1)$ $p(1,y)=p(1,y+m)$ I have not been able to use these to prove well definedness. This exercise has been asked many times on this website but I am asking specifically about why in a certain method a particular map is well defined. I don't want an answer using a different method.",I am trying to get used to the universal property of the tensor product. I have tried to prove this common fact using the universal property After checking that there is well-defined bilinear map (which i will call ) from to we need to show that we can factor any other bilinear map of -modules over . The obvious thing to do is do define by . I am having difficulty checking that this is well defined. We need to show that where . But the only things we know are that I have not been able to use these to prove well definedness. This exercise has been asked many times on this website but I am asking specifically about why in a certain method a particular map is well defined. I don't want an answer using a different method.,"\mathbb{Z}/n\mathbb Z\otimes \mathbb{Z}/m\mathbb Z \cong \mathbb{Z}/\gcd(m,n)\mathbb Z b \mathbb{Z}/n\mathbb Z\otimes \mathbb{Z}/m\mathbb Z \mathbb{Z}/\gcd(m,n)\mathbb Z \mathbb Z p:\mathbb{Z}/n\mathbb Z\otimes \mathbb{Z}/m\mathbb Z\rightarrow P b i:\mathbb{Z}/\gcd(m,n)\mathbb Z\rightarrow P x\mod mn\mapsto p(x,1) p(x,1)=p(x',1) x'=x+k\cdot \gcd(m,n) p(x,1)=p(1,x) p(x+n,1)=p(x,1) p(1,y)=p(1,y+m)","['abstract-algebra', 'tensor-products']"
36,Why is $A \rtimes B \simeq \mathbb Z \rtimes \mathbb Z/2\mathbb Z$,Why is,A \rtimes B \simeq \mathbb Z \rtimes \mathbb Z/2\mathbb Z,"Hello everyone I have a hard time trying to resolve this problem if anyone could help it would be a lot appreciated. Let $$f_1\colon\mathbb R\rightarrow \mathbb R,\,f_1(x)=-x,\quad f_2\colon\mathbb R\rightarrow \mathbb R,\,f_2(x) = -x +1$$ We define $D_\infty = \langle f_1,f_2 \rangle$ , the infinite dihedral group and $A = \langle f_2 \circ f_1 \rangle$ and $B=\langle f_2\rangle$ ,  then we have : $$D_\infty = A \rtimes B \simeq \mathbb Z \rtimes \mathbb Z/2\mathbb Z$$ How can we prove that statement ? Should we prove that $f_1 ^2=f_2 ^2=1$ , so I can use the following answer ? I also found here exactly what I want but how can I prove it using $f_1$ and $f_2$ . ( How can we also prove infinite order of $\langle f_2 \circ f_1\rangle$ , I thought maybe we have to compute $(f_2 \circ f_1)^k(x)$ ?)","Hello everyone I have a hard time trying to resolve this problem if anyone could help it would be a lot appreciated. Let We define , the infinite dihedral group and and ,  then we have : How can we prove that statement ? Should we prove that , so I can use the following answer ? I also found here exactly what I want but how can I prove it using and . ( How can we also prove infinite order of , I thought maybe we have to compute ?)","f_1\colon\mathbb R\rightarrow \mathbb R,\,f_1(x)=-x,\quad f_2\colon\mathbb R\rightarrow \mathbb R,\,f_2(x) = -x +1 D_\infty = \langle f_1,f_2 \rangle A = \langle f_2 \circ f_1 \rangle B=\langle f_2\rangle D_\infty = A \rtimes B \simeq \mathbb Z \rtimes \mathbb Z/2\mathbb Z f_1 ^2=f_2 ^2=1 f_1 f_2 \langle f_2 \circ f_1\rangle (f_2 \circ f_1)^k(x)","['abstract-algebra', 'group-theory']"
37,Multiplying a ring(or an ideal) by an element.,Multiplying a ring(or an ideal) by an element.,,"I was looking at a proof of the statement: Given a ring $R$ and an ideal $M$ , $R/M$ is a field $\iff$ M is maximal. The backwards direction (assume $M$ maximal) proof involved a statement where we take an $r\in R$ such that $r\notin M$ and take $M + r*R$ which is an ideal as $M$ and $R$ are ideals, which implies that this construction must be $R$ itself. I had two questions about this statement. First of all, I realized very quickly that for a given ring $r*R\neq R$ . Is this simply because $R$ is not a group under $*$ , i.e., if $R$ were a field, would $r*R=R$ , or is there something else going on? If it's because of the group assumption, what would the reasoning behind a proof be (I assume something to do with existence and uniqueness of inverses but not 100% sure)? The second was assuming that $r*R$ is an ideal. By definition of an ideal I know $r*R\in R$ , but by the above question there's no guarantee that $r*R=R$ . How then do we know that $r*R$ is an ideal?","I was looking at a proof of the statement: Given a ring and an ideal , is a field M is maximal. The backwards direction (assume maximal) proof involved a statement where we take an such that and take which is an ideal as and are ideals, which implies that this construction must be itself. I had two questions about this statement. First of all, I realized very quickly that for a given ring . Is this simply because is not a group under , i.e., if were a field, would , or is there something else going on? If it's because of the group assumption, what would the reasoning behind a proof be (I assume something to do with existence and uniqueness of inverses but not 100% sure)? The second was assuming that is an ideal. By definition of an ideal I know , but by the above question there's no guarantee that . How then do we know that is an ideal?",R M R/M \iff M r\in R r\notin M M + r*R M R R r*R\neq R R * R r*R=R r*R r*R\in R r*R=R r*R,"['abstract-algebra', 'group-theory', 'ring-theory', 'ideals']"
38,Ring is Noetherian if it admits a faithful finitely generated module with ACC on submodules generated by ideals,Ring is Noetherian if it admits a faithful finitely generated module with ACC on submodules generated by ideals,,"I have a (commutative unitary) ring $A$ and a faithful finitely generated module $M$ over $A$ which satisfies ACC on submodules of the form $IM$ , where $I\subset A$ is an ideal. I want to prove that $A$ is Noetherian. I have already proved that, because $M$ is finitely generated, if $M$ is a Noetherian $A$ -module, then $A/Ann(M)$ is a Noetherian ring. So, given my hypothesis, to prove that $A$ is Noetherian as a ring it suffices to prove that $M$ is Noetherian as $A$ -module. I've been given a sketch of proof, but I'm stuck with filling in the details. Here's the sketch: Proceed by contradiction and assume $M$ is not Noetherian. Consider the set $\{IM \mid I\subset A \text{ is an ideal and }M/IM \text{ is not Noetherian}\}$ and reduce to the case where $M$ is not Noetherian, but for every ideal $I\neq 0$ , $M/IM$ is Noetherian. Then consider the set $\Gamma = \{N\subset M \mid M/N \text{ is a faithful } A\text{-module}\}$ , find a maximal element in $\Gamma$ and conclude. Here's my attempt at using the hints I've been given: using ACC on submodules of the form $IM$ I found a maximal submodule $JM$ such that $M/JM$ is not Noetherian, so I thought substituting $M$ with $M/JM$ , but $M/JM$ is clearly not faithful so I don't think that's the right approach. Then I tried to assume the reduction to just $M$ not Noetherian had been made and tackled the set $\Gamma$ . I managed to find a maximal element (using Zorn's lemma), but I'm clueless on how to use that maximal element to conclude. I don't need the solution straight away, just some help to understand the hints better would be very much appreciated. (I hope I explained everything clearly cause english is not my first language.)","I have a (commutative unitary) ring and a faithful finitely generated module over which satisfies ACC on submodules of the form , where is an ideal. I want to prove that is Noetherian. I have already proved that, because is finitely generated, if is a Noetherian -module, then is a Noetherian ring. So, given my hypothesis, to prove that is Noetherian as a ring it suffices to prove that is Noetherian as -module. I've been given a sketch of proof, but I'm stuck with filling in the details. Here's the sketch: Proceed by contradiction and assume is not Noetherian. Consider the set and reduce to the case where is not Noetherian, but for every ideal , is Noetherian. Then consider the set , find a maximal element in and conclude. Here's my attempt at using the hints I've been given: using ACC on submodules of the form I found a maximal submodule such that is not Noetherian, so I thought substituting with , but is clearly not faithful so I don't think that's the right approach. Then I tried to assume the reduction to just not Noetherian had been made and tackled the set . I managed to find a maximal element (using Zorn's lemma), but I'm clueless on how to use that maximal element to conclude. I don't need the solution straight away, just some help to understand the hints better would be very much appreciated. (I hope I explained everything clearly cause english is not my first language.)",A M A IM I\subset A A M M A A/Ann(M) A M A M \{IM \mid I\subset A \text{ is an ideal and }M/IM \text{ is not Noetherian}\} M I\neq 0 M/IM \Gamma = \{N\subset M \mid M/N \text{ is a faithful } A\text{-module}\} \Gamma IM JM M/JM M M/JM M/JM M \Gamma,"['abstract-algebra', 'commutative-algebra', 'modules', 'noetherian', 'finitely-generated']"
39,Finite group generated by two different order $2$ elements is $\cong$ to $\mathbb{Z}_2^2$ or $D_n$,Finite group generated by two different order  elements is  to  or,2 \cong \mathbb{Z}_2^2 D_n,"I'm trying to solve this problem from my group theory course: Given $G$ finite group generated by two different order $2$ elements. Prove that $G\cong \mathbb{Z}_2^2$ or $G\cong D_n$ for some $n\geq 3$ (being $D_n$ the dihedral group of degree $n$ ). The first part is easy, since if I consider $a,b$ these two different order $2$ elements, and suppose $G$ abelian, then I get $$G=\{1,a,b,ab\},$$ and obviously this is isomorphic to $\mathbb{Z}_2^2$ . I'm having trouble with the second part. I understand the idea of two different reflections from $D_n$ generating the whole $D_n$ , but I don't know how to prove it. I know that $$D_n\cong\mathbb{Z}_n\rtimes_\phi \mathbb{Z}_2$$ for $\phi$ verifying that $\phi(\bar0)=id$ and $\phi(\bar1)$ being the inversion (correct me if I'm wrong), but I don't know if this is even useful here or how to use it in that case. How can I prove this? Any help will be appreciated, thanks in advance.","I'm trying to solve this problem from my group theory course: Given finite group generated by two different order elements. Prove that or for some (being the dihedral group of degree ). The first part is easy, since if I consider these two different order elements, and suppose abelian, then I get and obviously this is isomorphic to . I'm having trouble with the second part. I understand the idea of two different reflections from generating the whole , but I don't know how to prove it. I know that for verifying that and being the inversion (correct me if I'm wrong), but I don't know if this is even useful here or how to use it in that case. How can I prove this? Any help will be appreciated, thanks in advance.","G 2 G\cong \mathbb{Z}_2^2 G\cong D_n n\geq 3 D_n n a,b 2 G G=\{1,a,b,ab\}, \mathbb{Z}_2^2 D_n D_n D_n\cong\mathbb{Z}_n\rtimes_\phi \mathbb{Z}_2 \phi \phi(\bar0)=id \phi(\bar1)","['abstract-algebra', 'group-theory', 'finitely-generated', 'dihedral-groups', 'semidirect-product']"
40,Sum of odd powers of even number of complex numbers,Sum of odd powers of even number of complex numbers,,"Ive come across a result and Im having a bit of trouble seeing where it comes from, and it seems like it should be obvious why it is true. Say we are given n complex numbers $ \{z_1,...,z_n\}$ where n is even, such that the sum of their odd integer powers is zero for all powers less than $n$ . Ie; $$\sum_{i=1}^n z_i^k=0$$ for all k an odd integer less than n. Then if $m$ is an odd integer greater than n, we get that $$\sum_{i=1}^n z_i^m=0$$ Again I have a feeling that this is a well-known result, or at least a re-wording or special example of one, but I dont know where to start looking.","Ive come across a result and Im having a bit of trouble seeing where it comes from, and it seems like it should be obvious why it is true. Say we are given n complex numbers where n is even, such that the sum of their odd integer powers is zero for all powers less than . Ie; for all k an odd integer less than n. Then if is an odd integer greater than n, we get that Again I have a feeling that this is a well-known result, or at least a re-wording or special example of one, but I dont know where to start looking."," \{z_1,...,z_n\} n \sum_{i=1}^n z_i^k=0 m \sum_{i=1}^n z_i^m=0","['abstract-algebra', 'complex-analysis', 'polynomials']"
41,Find all the intermediate fields of the splitting field of $x^4 - 2$ over $\mathbb{Q}$,Find all the intermediate fields of the splitting field of  over,x^4 - 2 \mathbb{Q},"Okay, so I mostly worked this out, and I even created lattice diagrams as shown below. But I have a specific question about finding intermediate fields, which I will ask shortly. Let $\alpha = \sqrt[4]{2}$ and $\omega = e^{\frac{\pi}{4}i} = i$ .  Then $L = \mathbb{Q}(\alpha, i)$ is the splitting field of $x^4 -2$ over $\mathbb{Q}$ . Also, the Galois group $\Gamma_\mathbb{Q}(x^4 - 2) = D_8$ acts on the roots $\alpha, \alpha i, -\alpha,$ and $-\alpha i$ , and is generated by rotation $\sigma$ and reflection $\tau$ , where $\sigma(i) = i, \sigma(\alpha) = \alpha i$ and $\tau(\alpha) = \alpha, \tau(i) = -i$ . To find the intermediate fields between $L$ and $\mathbb{Q}$ , find the subgroups of $D_8$ instead with the idea that finding subgroups is easier and better understood than finding intermediate fields. Then from the subgroups, use the Galois correspondence to get all the intermediate fields. There are 10 subgroups of $D_8$ which must correspond to 10 intermediate fields.  Well, I pieced together 8 obvious candidates for intermediate fields, and in the end, I had to look up the other 2 which were $\mathbb{Q}(\alpha(1 + i))$ and $\mathbb{Q}(\alpha(1 - i))$ . Those two seemed strange until I realized that $\sqrt{8\alpha^2 i} = \alpha(1 + i)$ . Finally, I was able to check fixed fields to verify the exact correspondence, and come up with the diagrams. Question: Is there a systematic approach to finding and connecting up the corresponding intermediate fields once all the subgroups are known? I'm guessing, in general and maybe in this example with $D_8$ , there isn't a good, canonical way to anticipate and construct the field extensions?  The structure of groups and subgroups, as stated earlier, is easier and better understood than the structure of field extensions. Maybe this makes sense because the groups are finite and have only one operation, and fields are often infinite and have two operations. and UPDATE: In this lecture , Richard Borcherds clearly describes how to obtain the two nonobvious intermediate fields from the subgroups. Specifically, add the roots $\alpha$ and $\alpha i$ to fix by reflection one way, and then add the roots $\alpha$ and $-\alpha i$ to fix by reflection the other way.","Okay, so I mostly worked this out, and I even created lattice diagrams as shown below. But I have a specific question about finding intermediate fields, which I will ask shortly. Let and .  Then is the splitting field of over . Also, the Galois group acts on the roots and , and is generated by rotation and reflection , where and . To find the intermediate fields between and , find the subgroups of instead with the idea that finding subgroups is easier and better understood than finding intermediate fields. Then from the subgroups, use the Galois correspondence to get all the intermediate fields. There are 10 subgroups of which must correspond to 10 intermediate fields.  Well, I pieced together 8 obvious candidates for intermediate fields, and in the end, I had to look up the other 2 which were and . Those two seemed strange until I realized that . Finally, I was able to check fixed fields to verify the exact correspondence, and come up with the diagrams. Question: Is there a systematic approach to finding and connecting up the corresponding intermediate fields once all the subgroups are known? I'm guessing, in general and maybe in this example with , there isn't a good, canonical way to anticipate and construct the field extensions?  The structure of groups and subgroups, as stated earlier, is easier and better understood than the structure of field extensions. Maybe this makes sense because the groups are finite and have only one operation, and fields are often infinite and have two operations. and UPDATE: In this lecture , Richard Borcherds clearly describes how to obtain the two nonobvious intermediate fields from the subgroups. Specifically, add the roots and to fix by reflection one way, and then add the roots and to fix by reflection the other way.","\alpha = \sqrt[4]{2} \omega = e^{\frac{\pi}{4}i} = i L = \mathbb{Q}(\alpha, i) x^4 -2 \mathbb{Q} \Gamma_\mathbb{Q}(x^4 - 2) = D_8 \alpha, \alpha i, -\alpha, -\alpha i \sigma \tau \sigma(i) = i, \sigma(\alpha) = \alpha i \tau(\alpha) = \alpha, \tau(i) = -i L \mathbb{Q} D_8 D_8 \mathbb{Q}(\alpha(1 + i)) \mathbb{Q}(\alpha(1 - i)) \sqrt{8\alpha^2 i} = \alpha(1 + i) D_8 \alpha \alpha i \alpha -\alpha i","['abstract-algebra', 'field-theory', 'galois-theory', 'splitting-field']"
42,Prove that an automorphism of $M_n(k)$ where $k$ is an algebraically closed field must be an inner automorphism.,Prove that an automorphism of  where  is an algebraically closed field must be an inner automorphism.,M_n(k) k,Prove that an automorphism of $M_n(k)$ where $k$ is an algebraically closed field must be an inner automorphism. I'm a bit stuck on this one but I feel like I understand why it's true. Since $M_n(k)$ has a unique simple module (a column of lentgh $n$ ) really the only thing that can be done is to permute around the order of these simple summands (from a vector space perspective) but we need to leave them in their original order so we have to undo this. This would lead us to conjugation by an invertible matrix. Can someone provide some guidance please? Thanks! There is a relevant post here but I was hoping to avoid looking up the proof and just receive some guidance on how to continue. Automorphism of the matrix algebra is an inner automorphism,Prove that an automorphism of where is an algebraically closed field must be an inner automorphism. I'm a bit stuck on this one but I feel like I understand why it's true. Since has a unique simple module (a column of lentgh ) really the only thing that can be done is to permute around the order of these simple summands (from a vector space perspective) but we need to leave them in their original order so we have to undo this. This would lead us to conjugation by an invertible matrix. Can someone provide some guidance please? Thanks! There is a relevant post here but I was hoping to avoid looking up the proof and just receive some guidance on how to continue. Automorphism of the matrix algebra is an inner automorphism,M_n(k) k M_n(k) n,"['abstract-algebra', 'modules']"
43,Show that every group of order 15 is cyclic using class equation.,Show that every group of order 15 is cyclic using class equation.,,"I am willing to show that every group of order 15 is cyclic, using class equation. Let $G$ be a group of order 15. If $G$ is abelian, then $G=Z(G)$ and so for each $a\in G=Z(G)$ we have $cl(a)=\{a\}$ . Hence only possible class equation is $$15=1+1+1+\cdots+1+1(15~\text{times})$$ In this case, $G$ is isomorphic to either $\mathbb{Z}_{15}$ or the external direct product $\mathbb{Z}_3\times \mathbb{Z}_5$ . But since $\mathbb{Z}_{15}\simeq \mathbb{Z}_3\times \mathbb{Z}_5$ , it follows that $G$ is cyclic. We now show if $G$ is non-abelian then contradiction will appear. When $G$ is non-abelian, then $G\neq Z(G)$ . Here $|Z(G)|\in \{1,3,5\}$ . But if $|Z(G)|=3$ then $|G/Z(G)|=5$ a prime, so $G/Z(G)$ is cyclic and hence $G$ becomes Abelian, contradiction. Similalry $|Z(G)|\neq 5$ as well and so only possibility is $|Z(G)|=1$ . Then the class equation reads $$15=|G|=|Z(G)|+\sum |cl(a)|=1+\sum |cl(a)|$$ where the sum is taken over the orders of all non-singleton conjugacy classes $cl(a)$ in $G$ . Let there be $x_3$ and $x_5$ number of conjugacy classes of order 3 and 5 respectively in $G$ . Then we must have $$15=1+3x_3+5x_5\Rightarrow 14=3x_3+5x_5$$ which is satisfied by $x_3=3, x_5=1$ so that ultimately the class equation  becomes $$15=1+(3+3+3)+5$$ I do not know how to bring contradiction here. Any help? Thanks in well advance.","I am willing to show that every group of order 15 is cyclic, using class equation. Let be a group of order 15. If is abelian, then and so for each we have . Hence only possible class equation is In this case, is isomorphic to either or the external direct product . But since , it follows that is cyclic. We now show if is non-abelian then contradiction will appear. When is non-abelian, then . Here . But if then a prime, so is cyclic and hence becomes Abelian, contradiction. Similalry as well and so only possibility is . Then the class equation reads where the sum is taken over the orders of all non-singleton conjugacy classes in . Let there be and number of conjugacy classes of order 3 and 5 respectively in . Then we must have which is satisfied by so that ultimately the class equation  becomes I do not know how to bring contradiction here. Any help? Thanks in well advance.","G G G=Z(G) a\in G=Z(G) cl(a)=\{a\} 15=1+1+1+\cdots+1+1(15~\text{times}) G \mathbb{Z}_{15} \mathbb{Z}_3\times \mathbb{Z}_5 \mathbb{Z}_{15}\simeq \mathbb{Z}_3\times \mathbb{Z}_5 G G G G\neq Z(G) |Z(G)|\in \{1,3,5\} |Z(G)|=3 |G/Z(G)|=5 G/Z(G) G |Z(G)|\neq 5 |Z(G)|=1 15=|G|=|Z(G)|+\sum |cl(a)|=1+\sum |cl(a)| cl(a) G x_3 x_5 G 15=1+3x_3+5x_5\Rightarrow 14=3x_3+5x_5 x_3=3, x_5=1 15=1+(3+3+3)+5","['abstract-algebra', 'group-theory', 'finite-groups', 'cyclic-groups']"
44,Are there any geometrically meaningful/useful mixed-grade objects in geometric algebra other than rotors?,Are there any geometrically meaningful/useful mixed-grade objects in geometric algebra other than rotors?,,"While reading about geometric algebra, I have seen variables that are meant to represent blades, and variables that are meant to represent rotors, i.e. multivectors with a scalar and bivector component. But I have not seen any applications where a variable represents a mixed-grade object that is not the sum of a scalar and bivector. Are there examples of such objects being geometrically meaningful or useful?","While reading about geometric algebra, I have seen variables that are meant to represent blades, and variables that are meant to represent rotors, i.e. multivectors with a scalar and bivector component. But I have not seen any applications where a variable represents a mixed-grade object that is not the sum of a scalar and bivector. Are there examples of such objects being geometrically meaningful or useful?",,"['abstract-algebra', 'clifford-algebras', 'geometric-algebras']"
45,Does a ring isomorphic to its opposite always admit an involution?,Does a ring isomorphic to its opposite always admit an involution?,,"Let $R$ be a ring with identity such that $R\cong R^{\mathrm{op}}$ , then is it necessary that $R$ always admits an involution? An involution is an anti-homomorphism $l\colon R\to R$ such that $l\circ l=\mathrm{id}_R$ .","Let be a ring with identity such that , then is it necessary that always admits an involution? An involution is an anti-homomorphism such that .",R R\cong R^{\mathrm{op}} R l\colon R\to R l\circ l=\mathrm{id}_R,"['abstract-algebra', 'ring-theory']"
46,Let $1\in S\subseteq G$ such that the left cosets $aS$ with $a \in G$ partition the group $G$. Prove that $S\le G$.,Let  such that the left cosets  with  partition the group . Prove that .,1\in S\subseteq G aS a \in G G S\le G,"I am a complete beginner at group theory and I was looking at the following problem. Let $S$ be a subset of a group $G$ that contains the identity element $1$ and such that the left cosets $aS$ with $a \in G$ partition $G$ .Prove that $S$ a is a subgroup of $G$ . I tried the following: If we want to show that $S$ is a subgroup of $G$ , then we need to satisfy the following: $S \subseteq G$ $1 \in S$ $a,b\in S \implies ab \in S$ $\forall a\in S,\, \exists a^{-1} \in S,\, aa^{-1}=a^{-1}a=1$ I know that $S$ is a subset of $G$ so the first requirement is satisfied. It is also given that $1 \in S$ , so the second requirement is satisfied. To prove closure under composition (3), suppose $p,q \in S$ . Then $ap,aq \in aS$ . We want to show that $a(pq) \in aS$ as well, for some arbitrary $a \in S$ . If we let $b = ap$ and $c = aq$ , we have $a^{-1}b = p$ and $a^{-1}c = q$ . Left multiplying the equations gives $pq = a^{-1}ba^{-1}c$ . since $a^{-1} \in G$ , I can say that $pq$ is in another partition $a^{-1}S$ , and so it must be the case that $pq \in a^{-1}S$ . Now this is where I'm stuck because I want to get rid of that $a^{-1}$ in front of the $S$ , but I don't know how. I've also read this question here Let $S$ be a subset of a group $G$ that contains the identity element $1$ and such that the left cosets $aS$ with $a$ in $G$, partition $G$. , but I can't understand what the top answer is trying to argue at all, even after reading the comments below it. Can I please have some help with this problem?","I am a complete beginner at group theory and I was looking at the following problem. Let be a subset of a group that contains the identity element and such that the left cosets with partition .Prove that a is a subgroup of . I tried the following: If we want to show that is a subgroup of , then we need to satisfy the following: I know that is a subset of so the first requirement is satisfied. It is also given that , so the second requirement is satisfied. To prove closure under composition (3), suppose . Then . We want to show that as well, for some arbitrary . If we let and , we have and . Left multiplying the equations gives . since , I can say that is in another partition , and so it must be the case that . Now this is where I'm stuck because I want to get rid of that in front of the , but I don't know how. I've also read this question here Let $S$ be a subset of a group $G$ that contains the identity element $1$ and such that the left cosets $aS$ with $a$ in $G$, partition $G$. , but I can't understand what the top answer is trying to argue at all, even after reading the comments below it. Can I please have some help with this problem?","S G 1 aS a \in G G S G S G S \subseteq G 1 \in S a,b\in S \implies ab \in S \forall a\in S,\, \exists a^{-1} \in S,\, aa^{-1}=a^{-1}a=1 S G 1 \in S p,q \in S ap,aq \in aS a(pq) \in aS a \in S b = ap c = aq a^{-1}b = p a^{-1}c = q pq = a^{-1}ba^{-1}c a^{-1} \in G pq a^{-1}S pq \in a^{-1}S a^{-1} S","['abstract-algebra', 'group-theory']"
47,Proof that preimage of a subgroup to quotient group is a subgroup,Proof that preimage of a subgroup to quotient group is a subgroup,,"Sorry about the slight mess of a title. Let $G$ be a finite group and $N$ a normal subgroup of $G$ . If $H$ is a subgroup of $G/N$ , prove that $\phi^{-1}(H)$ is a subgroup in $G$ of order $|H| \cdot |N|$ , where $\phi : G \to G/N$ is the canonical homomorphism. Attempted solution: First of all, $\phi^{-1}(H) = \{ g \in G : \phi(g) \in H \}$ . To show that it is a subgroup in $G$ , it is sufficient to prove that the set is non-empty and that if $g, h \in \phi^{-1}(H)$ , then $gh^{-1} \in \phi^{-1}(H)$ . Clearly it is nonempty since $H < G/N$ which implies $H$ contains at least the identity. Let $g,h \in \phi^{-1}(H)$ . Then $\phi(gh^{-1}) = \phi(g) \phi(h^{-1}) = gNh^{-1}N = gh^{-1}N$ , since $N$ is normal in $G$ . Note that if $\phi(h) \in H$ , then so must $\phi(h^{-1}) \in H$ , since $H$ is a subgroup. This proves that $\phi^{-1}(H) < G$ . To prove that the order is $|H| \cdot |N|$ , I think it is enough to refer to the fact that $G/N$ contains disjoint subsets of $G$ each of order $N$ (since $G$ is finite) and it is ""obvious"" that we have $|H|$ such subsets so the order of $\phi^{-1}(H)$ is just the product $|H| \cdot |N|$ . However, I'm not sure this is so obvious. Is this proof actually correct?","Sorry about the slight mess of a title. Let be a finite group and a normal subgroup of . If is a subgroup of , prove that is a subgroup in of order , where is the canonical homomorphism. Attempted solution: First of all, . To show that it is a subgroup in , it is sufficient to prove that the set is non-empty and that if , then . Clearly it is nonempty since which implies contains at least the identity. Let . Then , since is normal in . Note that if , then so must , since is a subgroup. This proves that . To prove that the order is , I think it is enough to refer to the fact that contains disjoint subsets of each of order (since is finite) and it is ""obvious"" that we have such subsets so the order of is just the product . However, I'm not sure this is so obvious. Is this proof actually correct?","G N G H G/N \phi^{-1}(H) G |H| \cdot |N| \phi : G \to G/N \phi^{-1}(H) = \{ g \in G : \phi(g) \in H \} G g, h \in \phi^{-1}(H) gh^{-1} \in \phi^{-1}(H) H < G/N H g,h \in \phi^{-1}(H) \phi(gh^{-1}) = \phi(g) \phi(h^{-1}) = gNh^{-1}N = gh^{-1}N N G \phi(h) \in H \phi(h^{-1}) \in H H \phi^{-1}(H) < G |H| \cdot |N| G/N G N G |H| \phi^{-1}(H) |H| \cdot |N|","['abstract-algebra', 'group-theory', 'finite-groups']"
48,Relation: Module structure on the dual and braiding?,Relation: Module structure on the dual and braiding?,,"1. Context Let $H$ be a Hopf algebra over a field $\mathbb k$ . Let $(V, p)$ be a finite dimensional (left) $H$ -module. We want to endow its dual vector space $V^*$ with the structure of a (left) $H$ -module. For that end define the map $$   p' \colon H \xrightarrow{\enspace S \enspace} H \xrightarrow{\enspace p \enspace} \operatorname{End}(V) \xrightarrow{\enspace (-){^*}\enspace} \operatorname{End}(V^*) , $$ where $(-)^* \colon \operatorname{End}(V) \rightarrow \operatorname{End}(V^*)$ , $f \mapsto f^*$ . In a strict monoidal category we have a graphical calculus. In the following we consider a strictification of $\mathrm{vect}_{\mathbb k}$ . Then one can write down in string diagrams the definition of the above (left) $H$ -action on $V^*$ as follows: 2. Questions This picture seems to show that the braiding enters in the definition of the induced (left) $H$ -module structure on $V^\vee$ from the (left) $H$ -module structure on $V$ . Correct? How so?","1. Context Let be a Hopf algebra over a field . Let be a finite dimensional (left) -module. We want to endow its dual vector space with the structure of a (left) -module. For that end define the map where , . In a strict monoidal category we have a graphical calculus. In the following we consider a strictification of . Then one can write down in string diagrams the definition of the above (left) -action on as follows: 2. Questions This picture seems to show that the braiding enters in the definition of the induced (left) -module structure on from the (left) -module structure on . Correct? How so?","H \mathbb k (V, p) H V^* H 
  p' \colon H \xrightarrow{\enspace S \enspace} H \xrightarrow{\enspace p \enspace} \operatorname{End}(V) \xrightarrow{\enspace (-){^*}\enspace} \operatorname{End}(V^*) ,
 (-)^* \colon \operatorname{End}(V) \rightarrow \operatorname{End}(V^*) f \mapsto f^* \mathrm{vect}_{\mathbb k} H V^* H V^\vee H V","['abstract-algebra', 'modules', 'dual-spaces', 'monoidal-categories', 'hopf-algebras']"
49,Using partial information to factor $x^6+3x^5+5x^4+10x^3+13x^2+4x+1.$,Using partial information to factor,x^6+3x^5+5x^4+10x^3+13x^2+4x+1.,"I wish to find exact expressions for all roots of $p(x)=x^6+3x^5+5x^4+10x^3+13x^2+4x+1.$ By observing that for the roots $x_0 \pm iy_0, x_0 \approx -0.15883609808599033632, y_0 \approx 0.27511219196092896700,$ we have that $x_0$ is the unique real root of $r(x) = x^3+12x^2+8x+1,$ I was able to prove that all roots of the original sextic can be expressed in radicals. The process is as follows: Divide $p(x+iy)$ by $r(x)$ to get $\frac{1}{8}x^3 + \frac{3}{16}x^2 + x\left(\frac{7}{32}-\frac{15y^2}{8}\right) + \left(\frac{95}{32}-\frac{15y^2}{16}\right) + \frac{R(x,y)}{p(x)}$ where $R(x,y) = A(y)x^2 + B(y)x + C(y)$ and $A(y) = 15y^4 - \frac{15y^2}{4} - \frac{201}{16}, B(y) = 15y^8 - 30y^6 + 12y^4 + \frac{75y^2}{8} - \frac{767}{32}, C(y) = -y^6+5y^4-\frac{193y^2}{16}-\frac{63}{32}.$ The equation $R(x_0, y_0) = 0$ is a quartic in $y_0^2,$ which we can solve exactly to obtain $y_0^2$ and hence $y_0.$ Polynomial division reduces $p(x)$ to a quartic, and now we apply the quartic formula again to find the other $4$ roots. However, I don't want to perform the rest of the computations. Is there a cleaner way to use the observation that $r(x_0) = 0,$ perhaps in the realm of abstract algebra?","I wish to find exact expressions for all roots of By observing that for the roots we have that is the unique real root of I was able to prove that all roots of the original sextic can be expressed in radicals. The process is as follows: Divide by to get where and The equation is a quartic in which we can solve exactly to obtain and hence Polynomial division reduces to a quartic, and now we apply the quartic formula again to find the other roots. However, I don't want to perform the rest of the computations. Is there a cleaner way to use the observation that perhaps in the realm of abstract algebra?","p(x)=x^6+3x^5+5x^4+10x^3+13x^2+4x+1. x_0 \pm iy_0, x_0 \approx -0.15883609808599033632, y_0 \approx 0.27511219196092896700, x_0 r(x) = x^3+12x^2+8x+1, p(x+iy) r(x) \frac{1}{8}x^3 + \frac{3}{16}x^2 + x\left(\frac{7}{32}-\frac{15y^2}{8}\right) + \left(\frac{95}{32}-\frac{15y^2}{16}\right) + \frac{R(x,y)}{p(x)} R(x,y) = A(y)x^2 + B(y)x + C(y) A(y) = 15y^4 - \frac{15y^2}{4} - \frac{201}{16}, B(y) = 15y^8 - 30y^6 + 12y^4 + \frac{75y^2}{8} - \frac{767}{32}, C(y) = -y^6+5y^4-\frac{193y^2}{16}-\frac{63}{32}. R(x_0, y_0) = 0 y_0^2, y_0^2 y_0. p(x) 4 r(x_0) = 0,","['abstract-algebra', 'algebra-precalculus', 'polynomials', 'roots', 'cubics']"
50,Simple module over $\mathbb{Z}G$ has a $\mathbb{Z}N$ composition series when $N \triangleleft G$ is nilpotent and of finite-index,Simple module over  has a  composition series when  is nilpotent and of finite-index,\mathbb{Z}G \mathbb{Z}N N \triangleleft G,"Assume that $M$ is a simple $\mathbb{Z}G$ module with $G$ finitely-generated, virtually nilpotent group. Take $N\leq G$ to be a normal, finite index, nilpotent subgroup. The claim is that $M$ has a finite-length composition series as a $\mathbb{Z}N$ -module. How can I see this? One approach I've tried: I know that a module has a finite-length composition series if and only if the module is both Artinian and Noetherian. Now in fact I can prove (with some ""heavier"" theorems) that $M$ is Noetherian in this case (this follows because $N$ is also finitely generated, and nilpotent, hence polycyclic, in essence). But, this approach doesn't use the fact the $M$ is simple as a $\mathbb{Z}G$ -module and moreoever I can't prove the descending-chain-condition anyways.","Assume that is a simple module with finitely-generated, virtually nilpotent group. Take to be a normal, finite index, nilpotent subgroup. The claim is that has a finite-length composition series as a -module. How can I see this? One approach I've tried: I know that a module has a finite-length composition series if and only if the module is both Artinian and Noetherian. Now in fact I can prove (with some ""heavier"" theorems) that is Noetherian in this case (this follows because is also finitely generated, and nilpotent, hence polycyclic, in essence). But, this approach doesn't use the fact the is simple as a -module and moreoever I can't prove the descending-chain-condition anyways.",M \mathbb{Z}G G N\leq G M \mathbb{Z}N M N M \mathbb{Z}G,"['abstract-algebra', 'group-theory', 'ring-theory', 'modules', 'finitely-generated']"
51,Values of c for which the given quotient ring is a field.,Values of c for which the given quotient ring is a field.,,"I am stuck with the problem : Find all values of 'c' in $F_{5}=\frac{\mathbb{Z}}{5\mathbb{Z}}$ such that the quotient ring $\frac{F_{5}}{X^3 + 3X^2 + cX + 3}$ is a field. Justify your answer. My approach was, we've got a theorem for commutative ring R that if I is a maximal ideal in R then R/I is a field. Now to prove $X^3 + 3X^2 + cX + 3$ is a maximal ideal in the given field we need to show that this is irreducible. So, I think for the set of values of 'c' for which this polynomial is irreducible, will be the set for which the above quotient ring is a field. But I don't know how to find all the values of 'c' for which $X^3 + 3X^2 + cX + 3$ is irreducible except to try each value of 'c' individually and then use some irreducibility test. Is there a proper and simpler way to find such 'c'. Please help me in finding such values.","I am stuck with the problem : Find all values of 'c' in such that the quotient ring is a field. Justify your answer. My approach was, we've got a theorem for commutative ring R that if I is a maximal ideal in R then R/I is a field. Now to prove is a maximal ideal in the given field we need to show that this is irreducible. So, I think for the set of values of 'c' for which this polynomial is irreducible, will be the set for which the above quotient ring is a field. But I don't know how to find all the values of 'c' for which is irreducible except to try each value of 'c' individually and then use some irreducibility test. Is there a proper and simpler way to find such 'c'. Please help me in finding such values.",F_{5}=\frac{\mathbb{Z}}{5\mathbb{Z}} \frac{F_{5}}{X^3 + 3X^2 + cX + 3} X^3 + 3X^2 + cX + 3 X^3 + 3X^2 + cX + 3,"['abstract-algebra', 'ring-theory', 'field-theory', 'ideals']"
52,"Let $|G|=2^np$, $p$ an odd prime, $H\unlhd G$ a Sylow $2$-subgroup with $H\cong(\Bbb{Z}/2\Bbb{Z})^n$, $p\nmid 2^n-1$. Prove $Z(G)$ is nontrivial.","Let ,  an odd prime,  a Sylow -subgroup with , . Prove  is nontrivial.",|G|=2^np p H\unlhd G 2 H\cong(\Bbb{Z}/2\Bbb{Z})^n p\nmid 2^n-1 Z(G),"Let $G$ be a group with $|G|=2^np$ ( $p$ an odd prime). Let $H$ be a normal Sylow $2$ -subgroup such that $H\cong(\mathbb{Z}/2\mathbb{Z})^n$ . Prove that if $p$ does not divide $2^n-1$ , then $G$ has a non-trivial center. I think this may have something to do with the fact that $H$ can be written as a union of conjugacy classes and then apply the class equation, but I have no idea. Furthermore, we probably have to make use of the fact that since $H$ is normal it is the unique Sylow $2$ -subgroup of $G$ . Any hints?","Let be a group with ( an odd prime). Let be a normal Sylow -subgroup such that . Prove that if does not divide , then has a non-trivial center. I think this may have something to do with the fact that can be written as a union of conjugacy classes and then apply the class equation, but I have no idea. Furthermore, we probably have to make use of the fact that since is normal it is the unique Sylow -subgroup of . Any hints?",G |G|=2^np p H 2 H\cong(\mathbb{Z}/2\mathbb{Z})^n p 2^n-1 G H H 2 G,"['abstract-algebra', 'group-theory', 'normal-subgroups', 'sylow-theory']"
53,When an irreducible representation is an induced representation,When an irreducible representation is an induced representation,,"So I've been trying to answer this exercise to much of my frustration: Let $G$ be a finite group and $S$ a normal subgroup. Let $\rho$ be an irreducible representation of $G$ over $\mathbb{C}$ . Prove that either the restriction of $\rho$ to $S$ has all its irreducible components $S$ -isomorphic to each other, or there exists a proper subgroup $H$ of $G$ containing $S$ and an irreducible representation $\theta$ of $H$ such that $\rho \simeq \text{ind}_H^G(\theta)$ . Here is my progress so far: Let $E$ be a representation space for $\rho$ and $\chi_\rho$ the character. We also have the restriction $\text{res}_S^G(\rho)$ of our representation $\rho$ to $S$ . $\text{res}_S^G(E)$ is the representation space for this restriction. We pick a simple $S$ -submodule $F$ of $\text{res}_S^G(E)$ and realize that because $E$ is a simple $G$ -module, \begin{equation} \text{res}_S^G(E)=\sum_i \gamma_i F \end{equation} where $\{\gamma_i\}$ is a set of left coset representatives for $G/S$ . Also note that $S\trianglelefteq G$ implies that $\gamma_i F$ is an irreducible $S$ -submodule of $\text{res}_S^G(E)$ . If these submodules are $S$ -isomorphic to each other, then we have the first case. Now suppose otherwise. I am guessing that we are able to find a subgroup $S\subseteq H\subsetneq G$ and an irreducible character $\chi_\theta$ in the character decomposition of $\chi_{\text{res}_H^G(\rho)}$ such that $\text{ind}_H^G(\chi_\theta)$ is simple. We would then have \begin{equation} \langle\text{res}_H^G \chi_\rho,\chi_\theta\rangle=\langle\chi_{\text{res}_H^G(\rho)},\chi_\theta\rangle\geq 1. \end{equation} By Frobenius reciprocity, we would have $\langle \chi_\rho,\text{ind}_H^G(\theta) \rangle \geq 1$ , which implies $\langle \chi_\rho,\text{ind}_H^G(\chi_\theta) \rangle = 1$ since both characters are irreducible. If $\theta$ is a representation corresponding to $\chi_\theta$ , then $\rho\simeq \theta$ . Now it remains to find this subgroup $H$ . I have a hunch that $H=\{\sigma\in G: \sigma F \simeq F\text{ as an $S$-representation space}\}$ . It can be easily shown that $H\subsetneq G$ is a proper subgroup. However, I am not sure how to find such an irreducible character $\chi_\theta$ with our desired properties. Any hints are appreciated!","So I've been trying to answer this exercise to much of my frustration: Let be a finite group and a normal subgroup. Let be an irreducible representation of over . Prove that either the restriction of to has all its irreducible components -isomorphic to each other, or there exists a proper subgroup of containing and an irreducible representation of such that . Here is my progress so far: Let be a representation space for and the character. We also have the restriction of our representation to . is the representation space for this restriction. We pick a simple -submodule of and realize that because is a simple -module, where is a set of left coset representatives for . Also note that implies that is an irreducible -submodule of . If these submodules are -isomorphic to each other, then we have the first case. Now suppose otherwise. I am guessing that we are able to find a subgroup and an irreducible character in the character decomposition of such that is simple. We would then have By Frobenius reciprocity, we would have , which implies since both characters are irreducible. If is a representation corresponding to , then . Now it remains to find this subgroup . I have a hunch that . It can be easily shown that is a proper subgroup. However, I am not sure how to find such an irreducible character with our desired properties. Any hints are appreciated!","G S \rho G \mathbb{C} \rho S S H G S \theta H \rho \simeq \text{ind}_H^G(\theta) E \rho \chi_\rho \text{res}_S^G(\rho) \rho S \text{res}_S^G(E) S F \text{res}_S^G(E) E G \begin{equation}
\text{res}_S^G(E)=\sum_i \gamma_i F
\end{equation} \{\gamma_i\} G/S S\trianglelefteq G \gamma_i F S \text{res}_S^G(E) S S\subseteq H\subsetneq G \chi_\theta \chi_{\text{res}_H^G(\rho)} \text{ind}_H^G(\chi_\theta) \begin{equation}
\langle\text{res}_H^G \chi_\rho,\chi_\theta\rangle=\langle\chi_{\text{res}_H^G(\rho)},\chi_\theta\rangle\geq 1.
\end{equation} \langle \chi_\rho,\text{ind}_H^G(\theta) \rangle \geq 1 \langle \chi_\rho,\text{ind}_H^G(\chi_\theta) \rangle = 1 \theta \chi_\theta \rho\simeq \theta H H=\{\sigma\in G: \sigma F \simeq F\text{ as an S-representation space}\} H\subsetneq G \chi_\theta","['abstract-algebra', 'representation-theory']"
54,Explicit description of all extensions of $\mathbf{Z}/n\mathbf{Z}$ by $\mathbf{Z}$,Explicit description of all extensions of  by,\mathbf{Z}/n\mathbf{Z} \mathbf{Z},"In an exercise in my syllabus on homological algebra, I need to explicitly describe what are the $n$ isomorphism classes of extensions of $\mathbf{Z}/n\mathbf{Z}$ by $\mathbf{Z}$ for the cases $n=p$ prime, $n=pq$ with $p,q$ distinct primes and $n=4$ . (The group $\operatorname{Ext}_\mathbf{Z}^1 (\mathbf{Z}/n\mathbf{Z},\mathbf{Z})$ .) For $n=p$ , I succeeded in classifying these: there are $p$ short exact sequences of the form $$0\longrightarrow \mathbf Z\stackrel{\times p}{\longrightarrow} \mathbf Z\stackrel{f}{\longrightarrow} \mathbf Z/p\mathbf Z\longrightarrow 0$$ with $f:1\mapsto \overline{a}$ with $a\in \{1,\ldots,p-1\}$ and we have the split extension $$0\longrightarrow \mathbf Z\stackrel{}{\longrightarrow} \mathbf Z \oplus \mathbf Z/p\mathbf Z\stackrel{}{\longrightarrow} \mathbf Z/p\mathbf Z\longrightarrow 0$$ which are clearly distinct isomorphism classes. For $n=4$ , I can only find three: $$0\longrightarrow \mathbf Z\stackrel{\times 4}{\longrightarrow} \mathbf Z\stackrel{\pi_i}{\longrightarrow} \mathbf Z/4\mathbf Z\longrightarrow  0$$ where $\pi_1:x\mapsto \overline{x}$ and $\pi_2:x\mapsto \overline{-x}$ , and the split extension $$0\longrightarrow \mathbf Z\stackrel{}{\longrightarrow} \mathbf Z \oplus \mathbf Z/4\mathbf Z\stackrel{}{\longrightarrow} \mathbf Z/4\mathbf Z\longrightarrow  0.$$ For $n=pq$ , we again have the split extension and we can mimic what we did for $p$ prime to obtain $(p-1)(q-1)$ non-equivalent extensions of the form $$0\longrightarrow \mathbf Z\stackrel{\times pq}{\longrightarrow} \mathbf Z\stackrel{f}{\longrightarrow} \mathbf Z/pq\mathbf Z\longrightarrow 0$$ with $f:1\mapsto \overline{a}$ with $a\in \{1,\ldots,p-1\}\times \{1,\ldots,q-1 \}$ . Any help is appreciated!","In an exercise in my syllabus on homological algebra, I need to explicitly describe what are the isomorphism classes of extensions of by for the cases prime, with distinct primes and . (The group .) For , I succeeded in classifying these: there are short exact sequences of the form with with and we have the split extension which are clearly distinct isomorphism classes. For , I can only find three: where and , and the split extension For , we again have the split extension and we can mimic what we did for prime to obtain non-equivalent extensions of the form with with . Any help is appreciated!","n \mathbf{Z}/n\mathbf{Z} \mathbf{Z} n=p n=pq p,q n=4 \operatorname{Ext}_\mathbf{Z}^1 (\mathbf{Z}/n\mathbf{Z},\mathbf{Z}) n=p p 0\longrightarrow \mathbf Z\stackrel{\times p}{\longrightarrow} \mathbf Z\stackrel{f}{\longrightarrow} \mathbf Z/p\mathbf Z\longrightarrow 0 f:1\mapsto \overline{a} a\in \{1,\ldots,p-1\} 0\longrightarrow \mathbf Z\stackrel{}{\longrightarrow} \mathbf Z \oplus \mathbf Z/p\mathbf Z\stackrel{}{\longrightarrow} \mathbf Z/p\mathbf Z\longrightarrow 0 n=4 0\longrightarrow \mathbf Z\stackrel{\times 4}{\longrightarrow} \mathbf Z\stackrel{\pi_i}{\longrightarrow} \mathbf Z/4\mathbf Z\longrightarrow  0 \pi_1:x\mapsto \overline{x} \pi_2:x\mapsto \overline{-x} 0\longrightarrow \mathbf Z\stackrel{}{\longrightarrow} \mathbf Z \oplus \mathbf Z/4\mathbf Z\stackrel{}{\longrightarrow} \mathbf Z/4\mathbf Z\longrightarrow  0. n=pq p (p-1)(q-1) 0\longrightarrow \mathbf Z\stackrel{\times pq}{\longrightarrow} \mathbf Z\stackrel{f}{\longrightarrow} \mathbf Z/pq\mathbf Z\longrightarrow 0 f:1\mapsto \overline{a} a\in \{1,\ldots,p-1\}\times \{1,\ldots,q-1 \}","['abstract-algebra', 'group-theory', 'homological-algebra', 'exact-sequence']"
55,"Galois Extension, Chinese Remainder Theorem, and Quotients of Polynomial Rings","Galois Extension, Chinese Remainder Theorem, and Quotients of Polynomial Rings",,"I am trying to prove the following assertion (, which is an exercise from a section on tensor products in the book Algebra: Chapter 0 by P.Aluffi): Let $F=k(\alpha)\supset k$ be a finite simple extension such that $F\otimes_kF\cong F^{[F:k]}$ as a ring. Then the extension is Galois. A related question has been asked several times on this site, for example in here . Reading the answers to these questions, I learned that  the above assertion may be proved as follows: There is a canonical isomorphism $F\otimes_k F\cong F[x]/(m(x))$ of $k$ -algebras. Now $F^n$ is obviously reduced, so $F[x]/(m(x))$ is also reduced. It follows that $\alpha$ is separable over $k$ . (This step requires a little more argument, but I am OK with this step.) Therefore, to prove that $F/k$ is Galois, it suffices to show that the $\alpha$ splits over $F$ . Let $m(x)\in k[x]$ be the minimal polynomial of $\alpha$ over $k$ . But $F[x]/(m(x))\cong F^n$ as a ring by hypothesis. By the Chinese remainder theorem, this implies that $m(x)$ factors into linear factors over $F$ . I do not understand the bold faced part in step 3. Sure, the Chinese remainder theorem implies that if $m(x)$ splits over $F$ , then $F[x]/(m(x))\cong F^n$ as rings. However, we want to go in the other direction. How do we do this? Thanks in advance. In addition to the above question, I would greatly appreciate the answers to the following additional questions. ( But you do not have to answer these unless you want to. ) The question which I cited above actually assumed that the isomorphism $F\otimes_k F\cong F^{[F:k]}$ to be an $F$ -algebra isomorphism. In my book, this isomorphism is assumed to be merely a ring isomorphism. Maybe the author took it for granted that the isomorphism to be $F$ -linear? Or maybe we can do without the $F$ -linearity. In general, which quotient of $F[x]$ isomorphic to the direct product of $F$ as a ring (or as an $F$ -algebra)?","I am trying to prove the following assertion (, which is an exercise from a section on tensor products in the book Algebra: Chapter 0 by P.Aluffi): Let be a finite simple extension such that as a ring. Then the extension is Galois. A related question has been asked several times on this site, for example in here . Reading the answers to these questions, I learned that  the above assertion may be proved as follows: There is a canonical isomorphism of -algebras. Now is obviously reduced, so is also reduced. It follows that is separable over . (This step requires a little more argument, but I am OK with this step.) Therefore, to prove that is Galois, it suffices to show that the splits over . Let be the minimal polynomial of over . But as a ring by hypothesis. By the Chinese remainder theorem, this implies that factors into linear factors over . I do not understand the bold faced part in step 3. Sure, the Chinese remainder theorem implies that if splits over , then as rings. However, we want to go in the other direction. How do we do this? Thanks in advance. In addition to the above question, I would greatly appreciate the answers to the following additional questions. ( But you do not have to answer these unless you want to. ) The question which I cited above actually assumed that the isomorphism to be an -algebra isomorphism. In my book, this isomorphism is assumed to be merely a ring isomorphism. Maybe the author took it for granted that the isomorphism to be -linear? Or maybe we can do without the -linearity. In general, which quotient of isomorphic to the direct product of as a ring (or as an -algebra)?",F=k(\alpha)\supset k F\otimes_kF\cong F^{[F:k]} F\otimes_k F\cong F[x]/(m(x)) k F^n F[x]/(m(x)) \alpha k F/k \alpha F m(x)\in k[x] \alpha k F[x]/(m(x))\cong F^n m(x) F m(x) F F[x]/(m(x))\cong F^n F\otimes_k F\cong F^{[F:k]} F F F F[x] F F,"['abstract-algebra', 'galois-theory', 'extension-field', 'tensor-products']"
56,Generalizing the Quadratic Formula to a Field with Characteristic $\neq 2$,Generalizing the Quadratic Formula to a Field with Characteristic,\neq 2,"Here is a definition given by my book: Suppose $f(x)=ax^2+bx+c\in F[x]$ with $2a\in F^\times$ , and set $\Delta=b^2-4ac$ . 1) If there is a $\delta\in F$ such that $\delta^2=\Delta$ , then the roots of $f(x)$ are $\frac{-b\pm\delta}{2a}$ . 2) If there is no such $\delta\in F$ , then $f(x)$ has no roots in $F$ . Indeed, this definition seems familiar, and I know that, for polynomials in $\mathbb R[x]$ , this can be proven by completing the square. However, how can I generalize it from $\mathbb R[x]$ to $F[x]$ where $F$ can be any field with characteristic $\neq 2$ ? I'm unsure whether completing the square still works in this case, and the concepts of fields and polynomial arithmetic seem a bit abstract to the mathematically untalented me. Thanks in advance!","Here is a definition given by my book: Suppose with , and set . 1) If there is a such that , then the roots of are . 2) If there is no such , then has no roots in . Indeed, this definition seems familiar, and I know that, for polynomials in , this can be proven by completing the square. However, how can I generalize it from to where can be any field with characteristic ? I'm unsure whether completing the square still works in this case, and the concepts of fields and polynomial arithmetic seem a bit abstract to the mathematically untalented me. Thanks in advance!",f(x)=ax^2+bx+c\in F[x] 2a\in F^\times \Delta=b^2-4ac \delta\in F \delta^2=\Delta f(x) \frac{-b\pm\delta}{2a} \delta\in F f(x) F \mathbb R[x] \mathbb R[x] F[x] F \neq 2,"['abstract-algebra', 'polynomials', 'quadratics']"
57,Automorphisms of the Complex Field and Model Theory,Automorphisms of the Complex Field and Model Theory,,"A friend of mine has recently drawn my attention to a seemingly shocking result by the mathematician Joel David Hamkins The real numbers are not interpretable in the complex field about the possibility of interpret the real numbers in the complex field by using only the field structure of $\mathbb{C}$ . After a little thought the result seems not so strange to me anymore, but the ""elementary proof"" given by Hamkins in the quoted page contains two statements which I do not know. (I) For any $z \in \mathbb{C}$ , any two complex numbers transcendental over $Q(z)$ are automorphic in $\mathbb{C}$ by an automorphism fixing $z$ . (II) Any k-tuples $x \in \mathbb{C}^k$ and $y \in \mathbb{C}^k$ that exhibit the same algebraic equations over $\mathbb{Q}(p_1,\dots,p_n)$ will be automorphic by an automorphism fixing $(p_1,\dots,p_n)$ . My knowledge of abstract algebra is quite elementary, let us say at the level of Michael Artin's Algebra, and I cannot understand exactly the meaning of these two statements. Could someone give me some reference where I could find them and their proof? Thank you very much in advance for your great help.","A friend of mine has recently drawn my attention to a seemingly shocking result by the mathematician Joel David Hamkins The real numbers are not interpretable in the complex field about the possibility of interpret the real numbers in the complex field by using only the field structure of . After a little thought the result seems not so strange to me anymore, but the ""elementary proof"" given by Hamkins in the quoted page contains two statements which I do not know. (I) For any , any two complex numbers transcendental over are automorphic in by an automorphism fixing . (II) Any k-tuples and that exhibit the same algebraic equations over will be automorphic by an automorphism fixing . My knowledge of abstract algebra is quite elementary, let us say at the level of Michael Artin's Algebra, and I cannot understand exactly the meaning of these two statements. Could someone give me some reference where I could find them and their proof? Thank you very much in advance for your great help.","\mathbb{C} z \in \mathbb{C} Q(z) \mathbb{C} z x \in \mathbb{C}^k y \in \mathbb{C}^k \mathbb{Q}(p_1,\dots,p_n) (p_1,\dots,p_n)","['abstract-algebra', 'complex-numbers', 'field-theory', 'model-theory']"
58,Connection between an algebraic invariant and a maximisation issue,Connection between an algebraic invariant and a maximisation issue,,"I came to be interested by the following rational function : $$f(x)=\dfrac{(x^2-x+1)^3}{x^2(x-1)^2}\tag{1}$$ while writing this answer ; I discovered that $f$ is connected to rather deep features of ""abstract algebra"" (Klein $j$ -invariant (see paragraph ""sextic functions"") ; as well( https://hsm.stackexchange.com/q/5038/3730 ) ; see also Lurth theorem. Surprisingly, in this question and its second answer , one finds $f$ connected  to a maximisation/minimization issue ; precisely, the largest value of constant $M$ such that : $$(a^2+b^2+c^2-ab-bc-ca)^3 \geq M[((a-b)(b-c)(c-a)]^2\tag{2}$$ for all $a,b,c \geq 0$ , is the minimum of values taken by $f(a)$ . Remark : This solution is based on the change of variables $$(a,\ b, \ c) \rightarrow  \ \ (ka+p, \ \ kb+p, \ \ kc+p)\ $$ Beyond this result and its short proof, is there a ""higher level"" rationale for a connection between expression (1) and maximisation problem (2) ? The only feature I have noticed is that the RHS of (2) is the discriminant of polynomial $(x-a)(x-b)(x-c)$ . Another reference . Another one explaining what a j-invariant is Connection with elliptic curves","I came to be interested by the following rational function : while writing this answer ; I discovered that is connected to rather deep features of ""abstract algebra"" (Klein -invariant (see paragraph ""sextic functions"") ; as well( https://hsm.stackexchange.com/q/5038/3730 ) ; see also Lurth theorem. Surprisingly, in this question and its second answer , one finds connected  to a maximisation/minimization issue ; precisely, the largest value of constant such that : for all , is the minimum of values taken by . Remark : This solution is based on the change of variables Beyond this result and its short proof, is there a ""higher level"" rationale for a connection between expression (1) and maximisation problem (2) ? The only feature I have noticed is that the RHS of (2) is the discriminant of polynomial . Another reference . Another one explaining what a j-invariant is Connection with elliptic curves","f(x)=\dfrac{(x^2-x+1)^3}{x^2(x-1)^2}\tag{1} f j f M (a^2+b^2+c^2-ab-bc-ca)^3 \geq M[((a-b)(b-c)(c-a)]^2\tag{2} a,b,c \geq 0 f(a) (a,\ b, \ c) \rightarrow  \ \ (ka+p, \ \ kb+p, \ \ kc+p)\  (x-a)(x-b)(x-c)","['abstract-algebra', 'optimization', 'finite-groups', 'discriminant']"
59,"What makes two things (or two representations of a thing) ""the same""?","What makes two things (or two representations of a thing) ""the same""?",,"This is an extremely general question, so I'm going to use fields as a specific example and hopefully work from there. Keep in mind, though, that a similar ""re-encoding"" process can be performed for just about anything which is suitably algebra/structure-like: topological spaces, lattices, relational algebras, logics, even languages. Note: In this context $2\times F=\{0,1\}\times F=F\sqcup F=\cdots$ Let $\mathcal{F}=\langle F,+,\cdot\rangle$ be a field . Let $Cd(\mathcal{F})=\langle 2\times F,*\rangle$ , where $*:(2\times F)^2\to2\times F$ is defined as follows: $$x*y=\begin{cases}(0,x_2+y_2)&x_1=y_1=0\\(0,x_2\cdot y_2)&x_1\ne y_1\\(1,x_2\cdot y_2)&x_1=y_1=1\end{cases}$$ Observe that $Cd(\mathcal{F})$ satisfies the following axioms: Associativity: $$x_1=y_1=z_1\implies x*(y*z)=(x*y)*z$$ Commutativity: $$x*y=y*x$$ Distributivity: $$x_1=1\land y_1=z_1=0\implies x*(y*z)=(x*y)*(x*z)$$ Identity: $$x_1=i\implies x*e^i=e^i*x=x\quad:\quad i=1,2$$ Inverse: $$x_1=i\implies x*\overline{x}=\overline{x}*x=e^i\quad:\quad x\ne(1,0)$$ Clearly, $\mathcal{F}\ne Cd(\mathcal{F})$ , nor is $\mathcal{F}$ isomorphic to $Cd(\mathcal{F})$ ; $Cd(\mathcal{F})$ is not even a field! Yet $Cd(\mathcal{F})$ is similar to $\mathcal{F}$ in extremely obvious ways, to the extent that $\mathcal{F}$ and $Cd(\mathcal{F})$ are ""basically the same thing,"" or, at the very least, $\mathcal{F}$ and $Cd(\mathcal{F})$ ""encode"" the same thing. Of course, without clarification, any two things can be likened to one another, regardless of how disimilar they actually are. One could easily say that a group is ""basically the same"" as a Lie algebra and proceed to justify why this is the case - but such an arbitrary comparison is intuitively less reasonable than that made above. Since there are clearly ""reasonable"" and ""unreasonable"" comparisons, there ought to be a way to distinguish between them. This leads me to my question: how can I formalise the notion of ""sameness""? Is it possible to define a single ""sameness"" relation, or are there distinct classes of ""sameness""? Update: The notion which I am trying to capture is that $A$ and $B$ are the ""same"" if $A$ can encode $B$ and $B$ can encode $A$ . The relation in question ranges over [classes of] structures, theories, categories, or languages ( provided that ""language"" comes with rewrite rules and/or semantics); these can be used almost interchangeably because for any structure, there is a ""theory of that structure"" (up to isomorphism), for any theory, there is a ""category of that theory,"" etc. After doing some reading, I can confidently say that ""sameness,"" as presented in the example, is distinct from polynomial equivalence, term equivalence, and isotopy. This is because each of these relations apply only to algebras over the same set/universe. Even extending these notions to ""polynomial/term isomorphism"" - if that's a thing - does not account for the differences between $\mathcal{F}$ and $Cd(\mathcal{F})$ because any map between $F$ and $2\times F$ which would be suitably ""isomorphism-like"" would have to send each element of $F$ to two elements in $2\times F$ . It might be possible to bijectively map $n$ -tuples of elements and operations of $\mathcal{F}$ to those of $Cd(\mathcal{F})$ in a way that preserves the algebraic properties of $\mathcal{F}$ . At this point, I think it would be helpful to try and create a hierarchy (or partial order) of equivalence relations ordered by logical implication. My ""sameness"" relation ought to be ""that thing just above 'term-isomorphism'."" This seems like a more fruitful approach, since it would both clarify the particular relation I am talking about while and shine a light on the connection with other, ""nearby"" relations (like term-equivalence).","This is an extremely general question, so I'm going to use fields as a specific example and hopefully work from there. Keep in mind, though, that a similar ""re-encoding"" process can be performed for just about anything which is suitably algebra/structure-like: topological spaces, lattices, relational algebras, logics, even languages. Note: In this context Let be a field . Let , where is defined as follows: Observe that satisfies the following axioms: Associativity: Commutativity: Distributivity: Identity: Inverse: Clearly, , nor is isomorphic to ; is not even a field! Yet is similar to in extremely obvious ways, to the extent that and are ""basically the same thing,"" or, at the very least, and ""encode"" the same thing. Of course, without clarification, any two things can be likened to one another, regardless of how disimilar they actually are. One could easily say that a group is ""basically the same"" as a Lie algebra and proceed to justify why this is the case - but such an arbitrary comparison is intuitively less reasonable than that made above. Since there are clearly ""reasonable"" and ""unreasonable"" comparisons, there ought to be a way to distinguish between them. This leads me to my question: how can I formalise the notion of ""sameness""? Is it possible to define a single ""sameness"" relation, or are there distinct classes of ""sameness""? Update: The notion which I am trying to capture is that and are the ""same"" if can encode and can encode . The relation in question ranges over [classes of] structures, theories, categories, or languages ( provided that ""language"" comes with rewrite rules and/or semantics); these can be used almost interchangeably because for any structure, there is a ""theory of that structure"" (up to isomorphism), for any theory, there is a ""category of that theory,"" etc. After doing some reading, I can confidently say that ""sameness,"" as presented in the example, is distinct from polynomial equivalence, term equivalence, and isotopy. This is because each of these relations apply only to algebras over the same set/universe. Even extending these notions to ""polynomial/term isomorphism"" - if that's a thing - does not account for the differences between and because any map between and which would be suitably ""isomorphism-like"" would have to send each element of to two elements in . It might be possible to bijectively map -tuples of elements and operations of to those of in a way that preserves the algebraic properties of . At this point, I think it would be helpful to try and create a hierarchy (or partial order) of equivalence relations ordered by logical implication. My ""sameness"" relation ought to be ""that thing just above 'term-isomorphism'."" This seems like a more fruitful approach, since it would both clarify the particular relation I am talking about while and shine a light on the connection with other, ""nearby"" relations (like term-equivalence).","2\times F=\{0,1\}\times F=F\sqcup F=\cdots \mathcal{F}=\langle F,+,\cdot\rangle Cd(\mathcal{F})=\langle 2\times F,*\rangle *:(2\times F)^2\to2\times F x*y=\begin{cases}(0,x_2+y_2)&x_1=y_1=0\\(0,x_2\cdot y_2)&x_1\ne y_1\\(1,x_2\cdot y_2)&x_1=y_1=1\end{cases} Cd(\mathcal{F}) x_1=y_1=z_1\implies x*(y*z)=(x*y)*z x*y=y*x x_1=1\land y_1=z_1=0\implies x*(y*z)=(x*y)*(x*z) x_1=i\implies x*e^i=e^i*x=x\quad:\quad i=1,2 x_1=i\implies x*\overline{x}=\overline{x}*x=e^i\quad:\quad x\ne(1,0) \mathcal{F}\ne Cd(\mathcal{F}) \mathcal{F} Cd(\mathcal{F}) Cd(\mathcal{F}) Cd(\mathcal{F}) \mathcal{F} \mathcal{F} Cd(\mathcal{F}) \mathcal{F} Cd(\mathcal{F}) A B A B B A \mathcal{F} Cd(\mathcal{F}) F 2\times F F 2\times F n \mathcal{F} Cd(\mathcal{F}) \mathcal{F}","['abstract-algebra', 'category-theory', 'intuition', 'universal-algebra']"
60,Torsion in abelian groups,Torsion in abelian groups,,"If $A$ , $B$ and $C$ are finite abelian groups that obey the following exact sequence $$A\rightarrow B\rightarrow C\rightarrow1$$ and $$A[m]:=\{a\in A:a^m=1\}$$ is the following inequality true or false, why? $$|B[m]|\leq |A[m]||C[m]|$$ I know that it is false when I take away the finite condition. If it is false, is there some other nice upper bound on $|B[m]|$ ? One way to start is by restricting attention to $p$ primary invariant, but I am not sure how to proceed. Maybe taking the quotient of $A[p]$ and $f(A[p])$ will help where $f:A\rightarrow B$ is the homomorphism in the exact sequence, and then induction, but I am not sure if the details follow.","If , and are finite abelian groups that obey the following exact sequence and is the following inequality true or false, why? I know that it is false when I take away the finite condition. If it is false, is there some other nice upper bound on ? One way to start is by restricting attention to primary invariant, but I am not sure how to proceed. Maybe taking the quotient of and will help where is the homomorphism in the exact sequence, and then induction, but I am not sure if the details follow.",A B C A\rightarrow B\rightarrow C\rightarrow1 A[m]:=\{a\in A:a^m=1\} |B[m]|\leq |A[m]||C[m]| |B[m]| p A[p] f(A[p]) f:A\rightarrow B,"['abstract-algebra', 'group-theory', 'finite-groups', 'abelian-groups', 'exact-sequence']"
61,Short Exact Sequences as Fiber Bundles,Short Exact Sequences as Fiber Bundles,,"I am very much a visual thinker, and so to me, to feel that I have understood a concept, it is rather important for me to be able to ""see a picture of it in my head"". Now I recognize that for many concepts, this is bluntly impossible--I don't think that I ever will be able to think in 5-dimensional Euclidean space, for instance--but I want to at least be able to get an ""approximate"" or ""cartoony"" or ""conceptually accurate"" illustration in my mind. So for instance, take a fiber bundle. It was introduced to me in one lecture as $(E,B,\pi,F)$ : total space, base space, projection, and fiber, and that was it. As far as the lecturer was concerned, he had given a perfectly reasonable description of what a fiber bundle was. No need to dwell on it further than that. And sure, in a sense, I ""knew"" what a fiber bundle was after that lecture, but I feel that it was first when I got home later that day and googled and started seeing pictures of hairbrushes and Mbius stripes and other things that I actually knew what a fiber bundle was. So, a topic that has long confused me are short exact sequences . When they were first introduced to me, the concept, though straightforward, seemed eminently arbitrary: the image of one map is the kernel of the next. Why is that interesting? Sure, from differential geometry and what little algebraic topology I knew at the time, I was aware of that such sequences of structures and maps between them frequently occurred in mathematics, but I could not see why they were interesting structures in and of themselves . What was the concept that they encoded? I feel that my understanding is much better now, having had a look at some questions posted by other Stack Exchange users with the same stumbling block as me, such as What are exact sequences, metaphysically speaking? and Intuitive meaning of Exact Sequence . Nevertheless, I am still not 100% certain, and so I am posting this question in the same vein to see with more knowledgeable users if this understanding is correct: A way I've heard short exact sequences explained by many people is that they are meant to give an illustration of structures that are ""almost product spaces"", in the sense that $$0 \rightarrow A \rightarrow B \rightarrow C \rightarrow 0$$ along with the trivial $$0 \rightarrow A \rightarrow A \oplus C \rightarrow C \rightarrow 0$$ implies that $B$ is ""almost"" $A \oplus C$ . This makes me wonder, is it fair to think of short exact sequences as ""generalized fiber bundles"", or fiber bundles where the ""spaces"" are groups, rings, modules, etc., akin to how the Mbius stripe is ""almost"" a cylinder? Look forward to your responses!","I am very much a visual thinker, and so to me, to feel that I have understood a concept, it is rather important for me to be able to ""see a picture of it in my head"". Now I recognize that for many concepts, this is bluntly impossible--I don't think that I ever will be able to think in 5-dimensional Euclidean space, for instance--but I want to at least be able to get an ""approximate"" or ""cartoony"" or ""conceptually accurate"" illustration in my mind. So for instance, take a fiber bundle. It was introduced to me in one lecture as : total space, base space, projection, and fiber, and that was it. As far as the lecturer was concerned, he had given a perfectly reasonable description of what a fiber bundle was. No need to dwell on it further than that. And sure, in a sense, I ""knew"" what a fiber bundle was after that lecture, but I feel that it was first when I got home later that day and googled and started seeing pictures of hairbrushes and Mbius stripes and other things that I actually knew what a fiber bundle was. So, a topic that has long confused me are short exact sequences . When they were first introduced to me, the concept, though straightforward, seemed eminently arbitrary: the image of one map is the kernel of the next. Why is that interesting? Sure, from differential geometry and what little algebraic topology I knew at the time, I was aware of that such sequences of structures and maps between them frequently occurred in mathematics, but I could not see why they were interesting structures in and of themselves . What was the concept that they encoded? I feel that my understanding is much better now, having had a look at some questions posted by other Stack Exchange users with the same stumbling block as me, such as What are exact sequences, metaphysically speaking? and Intuitive meaning of Exact Sequence . Nevertheless, I am still not 100% certain, and so I am posting this question in the same vein to see with more knowledgeable users if this understanding is correct: A way I've heard short exact sequences explained by many people is that they are meant to give an illustration of structures that are ""almost product spaces"", in the sense that along with the trivial implies that is ""almost"" . This makes me wonder, is it fair to think of short exact sequences as ""generalized fiber bundles"", or fiber bundles where the ""spaces"" are groups, rings, modules, etc., akin to how the Mbius stripe is ""almost"" a cylinder? Look forward to your responses!","(E,B,\pi,F) 0 \rightarrow A \rightarrow B \rightarrow C \rightarrow 0 0 \rightarrow A \rightarrow A \oplus C \rightarrow C \rightarrow 0 B A \oplus C","['abstract-algebra', 'homological-algebra', 'exact-sequence', 'fiber-bundles']"
62,Control of fusion of p-elements in a subgroup containing a Sylow p-subgroup,Control of fusion of p-elements in a subgroup containing a Sylow p-subgroup,,"I need help on the following problem - my first attempt came to a dead end and I'm not sure where else to go. First, a definition: Definition (control of fusion) : Suppose $P \le H \le G$ . Then $H$ controls fusion in $P$ with respect to $G$ if whenever two elements of $P$ are conjugate in $G$ then they are already conjugate in $H$ . Another way to phrase this: whenever $x,g \in G$ satisfy $x,x^g \in P$ then $g=ch$ for some $c \in C_G(x)$ and $h \in H$ . Problem : Let $H \le G$ . Suppose that $H$ contains a Sylow $p$ -subgroup $P$ of $G$ and that $H$ controls fusion in $P$ with respect to $G$ . Then any two $p$ -elements of $H$ that are conjugate in $G$ are already conjugate in $H$ . Idea for proof : Since $H$ controls fusion in $P$ with respect to $G$ , this is already true for $p$ -elements in $P$ . So, consider $s \in H \setminus P$ where $s$ is a $p$ -element and let $g \in G$ be an element such that $s,s^g \in H$ . Since all Sylow $p$ -subgroups of $G$ are conjugate, there exists some $k \in G$ such that $s^k \in P$ and some $\ell$ such that $(s^g)^\ell \in P$ . Now I want to send both $s$ and $s^g$ into $P$ where we know $H$ controls fusion, use the second version of the definition above to rewrite that element as a product of an element of $C_G(s)$ and $H$ , then conjugate again to pull them back into $H \setminus P$ and show that this factorization still holds there. The problem is that there isn't necessarily one element that conjugates $s$ and $s^g$ into $P$ . Can this outline of an idea be made to work, and if not what other path can be taken toward a proof?","I need help on the following problem - my first attempt came to a dead end and I'm not sure where else to go. First, a definition: Definition (control of fusion) : Suppose . Then controls fusion in with respect to if whenever two elements of are conjugate in then they are already conjugate in . Another way to phrase this: whenever satisfy then for some and . Problem : Let . Suppose that contains a Sylow -subgroup of and that controls fusion in with respect to . Then any two -elements of that are conjugate in are already conjugate in . Idea for proof : Since controls fusion in with respect to , this is already true for -elements in . So, consider where is a -element and let be an element such that . Since all Sylow -subgroups of are conjugate, there exists some such that and some such that . Now I want to send both and into where we know controls fusion, use the second version of the definition above to rewrite that element as a product of an element of and , then conjugate again to pull them back into and show that this factorization still holds there. The problem is that there isn't necessarily one element that conjugates and into . Can this outline of an idea be made to work, and if not what other path can be taken toward a proof?","P \le H \le G H P G P G H x,g \in G x,x^g \in P g=ch c \in C_G(x) h \in H H \le G H p P G H P G p H G H H P G p P s \in H \setminus P s p g \in G s,s^g \in H p G k \in G s^k \in P \ell (s^g)^\ell \in P s s^g P H C_G(s) H H \setminus P s s^g P","['abstract-algebra', 'group-theory', 'finite-groups']"
63,Kernel of the following Substitution Homomorphism over $\mathbb{C}[x]$,Kernel of the following Substitution Homomorphism over,\mathbb{C}[x],"I'm hoping someone could review my proof for accuracy, thanks! There is another proof of this on stack exchange, but it uses quotient rings, which we haven't learned yet. Problem: Let $\phi: \mathbb{C}[x,y]$ -> $\mathbb{C}[t]$ , a the homomorphism that sends x -> $t + 1$ and $ y$ -> $t^3 - 1 $ . Determine the kernel $K$ of $\phi$ , and prove that every ideal of $\mathbb{C}[x,y]$ that contains $K$ can be generated by two elements. Proof: Note that $f = (x-1)^3 - 1 - y \in K$ . Suppose there is some $g \in K$ such that $f$ does not divide $g$ in $\mathbb{C}[x,y]$ . Then we have that $f$ is monic of degree $1$ when considered as a polynomial in the variable $y$ . Then we have $g = fq + r$ , where deg(r) $\lt$ 1 (with respect to the variable $y$ ). This implies that $r(x)$ is either a polynomial in $\mathbb{C}[x]$ that is non constant or is the $0$ polynomial. If it is nonconstant in $x$ then since r $\in$ K we must have that $x+1$ is a root of r. Part I think is wrong: But this implies that the polynomial $r$ has infinite roots, as $x$ is a variable that can range over all of $\mathbb{C}$ for example. Part I think may work better: Alternatively, plugging in x -> $x + 1$ will not send a non zero polynomial to zero as we are working in an integral domain and the substitution will maintain the degree of the polynomial as distributing $x+1$ will just produce extra terms of lower degree. Hence we have r(x) must be the zero polynomial and so f generates all of K and so K is principle and $K$ = $ ( (x-1)^3 - 1 - y )$ . Then Let $I$ be an ideal that contains K. If I = (1), then I contains K and is generated by 1 element. Suppose $I$ is a proper ideal then. New attempt: Then for any $g$ $\in$ $I$ we have $g = fq + r$ when dividing over $y$ . Then deg(r) $\lt$ $1$ and so it is either a constant or a polynomial over $x$ , call it $r(x)$ . Since $r(x)$ $\in$ $\mathbb{C}[x]$ then the set of all remainders is an ideal and is principle as it is polynomials in one variable over a field. Though Im not sure if we can claim the set of remainders is an ideal. It is certainly a subset of $I$ , but Im not sure if it is an ideal itself. ______________below this  is wrong__________ Then let $g$ $\in$ I and suppose $f$ does not divide $g$ . Then we can similarly divide $g$ by $f$ and conclude the remainder, $r(x)$ , must be degree 0 in the variable y which implies it may be of non zero degree of $x$ . Indeed it may be as considering $f$ as a function in the variable $x$ now we have that it is monic of degree 3. Then $g = fq + r$ is valid as an equation in the variable $y$ , and the variable $x$ . So we are dealing with the same $r(x)$ value as previously, and it must satisfy the additional constraint that the degree of $r(x)$ is less than $3$ . Hence any g $\notin$ K for g $\in$ I is of the form $g = fq + r$ , where $r(x)$ $\in$ $\mathbb{C}[x]$ . Then $r(x)$ $\in$ I also, as $I$ is closed under addition. Then the the lowest degree of $r$ is $0$ , $1$ or $2$ . $0$ implies $I$ is the whole ring. $1$ or $2$ implies we can generate $I$ with either $x$ or $x$$^2$ as $\mathbb{C}[x]$ is a ring with only ideals that are principle that are generated by lowest degree monic polynomials, and so $x$ or x $^2$ would suffice in that case. In any case we have I =  or , or <1>. Hence the claim is shown.","I'm hoping someone could review my proof for accuracy, thanks! There is another proof of this on stack exchange, but it uses quotient rings, which we haven't learned yet. Problem: Let -> , a the homomorphism that sends x -> and -> . Determine the kernel of , and prove that every ideal of that contains can be generated by two elements. Proof: Note that . Suppose there is some such that does not divide in . Then we have that is monic of degree when considered as a polynomial in the variable . Then we have , where deg(r) 1 (with respect to the variable ). This implies that is either a polynomial in that is non constant or is the polynomial. If it is nonconstant in then since r K we must have that is a root of r. Part I think is wrong: But this implies that the polynomial has infinite roots, as is a variable that can range over all of for example. Part I think may work better: Alternatively, plugging in x -> will not send a non zero polynomial to zero as we are working in an integral domain and the substitution will maintain the degree of the polynomial as distributing will just produce extra terms of lower degree. Hence we have r(x) must be the zero polynomial and so f generates all of K and so K is principle and = . Then Let be an ideal that contains K. If I = (1), then I contains K and is generated by 1 element. Suppose is a proper ideal then. New attempt: Then for any we have when dividing over . Then deg(r) and so it is either a constant or a polynomial over , call it . Since then the set of all remainders is an ideal and is principle as it is polynomials in one variable over a field. Though Im not sure if we can claim the set of remainders is an ideal. It is certainly a subset of , but Im not sure if it is an ideal itself. ______________below this  is wrong__________ Then let I and suppose does not divide . Then we can similarly divide by and conclude the remainder, , must be degree 0 in the variable y which implies it may be of non zero degree of . Indeed it may be as considering as a function in the variable now we have that it is monic of degree 3. Then is valid as an equation in the variable , and the variable . So we are dealing with the same value as previously, and it must satisfy the additional constraint that the degree of is less than . Hence any g K for g I is of the form , where . Then I also, as is closed under addition. Then the the lowest degree of is , or . implies is the whole ring. or implies we can generate with either or as is a ring with only ideals that are principle that are generated by lowest degree monic polynomials, and so or x would suffice in that case. In any case we have I =  or , or <1>. Hence the claim is shown.","\phi: \mathbb{C}[x,y] \mathbb{C}[t] t + 1  y t^3 - 1  K \phi \mathbb{C}[x,y] K f = (x-1)^3 - 1 - y \in K g \in K f g \mathbb{C}[x,y] f 1 y g = fq + r \lt y r(x) \mathbb{C}[x] 0 x \in x+1 r x \mathbb{C} x + 1 x+1 K  ( (x-1)^3 - 1 - y ) I I g \in I g = fq + r y \lt 1 x r(x) r(x) \in \mathbb{C}[x] I g \in f g g f r(x) x f x g = fq + r y x r(x) r(x) 3 \notin \in g = fq + r r(x) \in \mathbb{C}[x] r(x) \in I r 0 1 2 0 I 1 2 I x x^2 \mathbb{C}[x] x ^2","['abstract-algebra', 'proof-verification', 'ring-theory']"
64,Are projective modules related to projective spaces?,Are projective modules related to projective spaces?,,"I am aware of the possible motivations for calling ""projective"" a projective module (such as, for example, these ). However, I have been asked by a student if there is some connection between projective modules and projective spaces, since they share a common name. After a first moment in which I have been tempted to answer negatively, I realized that I actually don't know if this is the case or not. Does anybody have ever thought about this?","I am aware of the possible motivations for calling ""projective"" a projective module (such as, for example, these ). However, I have been asked by a student if there is some connection between projective modules and projective spaces, since they share a common name. After a first moment in which I have been tempted to answer negatively, I realized that I actually don't know if this is the case or not. Does anybody have ever thought about this?",,"['abstract-algebra', 'soft-question', 'projective-space', 'projective-module']"
65,Endomorphism ring of module quotient radical,Endomorphism ring of module quotient radical,,"Let $R$ be a ring of arbitrary characteristic and let $A$ be an $R$ -module. Denote $\mathrm{Rad}(A)$ as the radical of $A$ (i.e. the the intersection of maximal submodules). Then do we have $$  \mathrm{End}(A/\mathrm{Rad}(A)) \cong \mathrm{End}(A) /   \mathrm{Rad}\big(\mathrm{End}(A)\big), $$ when $A$ is not necessarily semisimple but $A/\mathrm{Rad}(A)$ is semisimple? If this is not generally true, then in what generality does this hold?","Let be a ring of arbitrary characteristic and let be an -module. Denote as the radical of (i.e. the the intersection of maximal submodules). Then do we have when is not necessarily semisimple but is semisimple? If this is not generally true, then in what generality does this hold?","R A R \mathrm{Rad}(A) A 
 \mathrm{End}(A/\mathrm{Rad}(A)) \cong \mathrm{End}(A) / 
 \mathrm{Rad}\big(\mathrm{End}(A)\big),
 A A/\mathrm{Rad}(A)","['abstract-algebra', 'ring-theory', 'commutative-algebra', 'modules', 'representation-theory']"
66,tale morphism induces isomorphism of tangent spaces,tale morphism induces isomorphism of tangent spaces,,"Let $(A, m) \to (B, n)$ be a flat map of local Noetherian rings with $mB = n$ , $B$ of finite type over $A$ , and $k(B) = B / n$ a finite separable field extension of $k(A) = A / m$ . Then, I want to show that the map $m / m^2 \to n / n^2$ induces an isomorphism of (base-changed) tangent spaces: $$\text{Hom}_{k(B)}(n/n^2, k(B)) \cong \text{Hom}_{k(A)}(m/m^2, k(B))$$ But I'm running into some problems trying to manipulate the objects in question... So, I of course have a short exact sequence $0 \to m^2 \to m \to m / m^2 \to 0$ of $A$ -modules, to which I apply the exact functor $- \otimes_A B$ to obtain $n / n^2 \cong (m / m^2) \otimes_A B \cong (m / m^2) \otimes_{k(A)} k(B)$ . But now in applying the tensor-hom adjunction, I get $$\text{Hom}_{k(B)}(n/n^2, k(B)) \cong \text{Hom}_{k(A)}(m/m^2, \text{Hom}_{k(A)}(k(B),k(B)))$$ which is a bigger module than I want. What's going wrong? EDIT: I finally see my issue. I was misapplying the tensor-hom adjunction; extension of scalars is left-adjoint simply to the forgetful functor of the scalar extension. The last line then comes out exactly as I would like it.","Let be a flat map of local Noetherian rings with , of finite type over , and a finite separable field extension of . Then, I want to show that the map induces an isomorphism of (base-changed) tangent spaces: But I'm running into some problems trying to manipulate the objects in question... So, I of course have a short exact sequence of -modules, to which I apply the exact functor to obtain . But now in applying the tensor-hom adjunction, I get which is a bigger module than I want. What's going wrong? EDIT: I finally see my issue. I was misapplying the tensor-hom adjunction; extension of scalars is left-adjoint simply to the forgetful functor of the scalar extension. The last line then comes out exactly as I would like it.","(A, m) \to (B, n) mB = n B A k(B) = B / n k(A) = A / m m / m^2 \to n / n^2 \text{Hom}_{k(B)}(n/n^2, k(B)) \cong \text{Hom}_{k(A)}(m/m^2, k(B)) 0 \to m^2 \to m \to m / m^2 \to 0 A - \otimes_A B n / n^2 \cong (m / m^2) \otimes_A B \cong (m / m^2) \otimes_{k(A)} k(B) \text{Hom}_{k(B)}(n/n^2, k(B)) \cong \text{Hom}_{k(A)}(m/m^2, \text{Hom}_{k(A)}(k(B),k(B)))","['abstract-algebra', 'algebraic-geometry', 'commutative-algebra', 'local-rings']"
67,Group presentations and isomorphism?,Group presentations and isomorphism?,,"I am reading Dummit and Foote, and they have only introduced group presentations very informally, so I am worried about the technicalities. I have to prove that the subgroup of $SL_2(\mathbb{F}_3)$ generated by $$A = \begin{bmatrix} 0 & -1\\ 1 & 0 \end{bmatrix} \hspace{2cm} B = \begin{bmatrix} 1 & 1\\ 1 & -1 \end{bmatrix} $$ is isomorphic to $Q_8$ . I know that one presentation of $Q_8$ is $$Q_8 = \langle -1, i, j, k|(-1)^2=1, i^2=j^2=k^2=ijk=-1 \rangle.$$ Now, if we identify $A$ with $i$ , $B$ with $j$ , $AB$ with $k$ , and $-1$ with $-I$ (where $I$ is the identity matrix, and I can prove that both $I$ and $-I$ are in my subgroup of matrices), then my matrices satisfy the same relations as the ones in my group presentation for $Q_8$ . My question is, am I done? Is this enough to show that the two groups are isomorphic? Or is there something I am missing?","I am reading Dummit and Foote, and they have only introduced group presentations very informally, so I am worried about the technicalities. I have to prove that the subgroup of generated by is isomorphic to . I know that one presentation of is Now, if we identify with , with , with , and with (where is the identity matrix, and I can prove that both and are in my subgroup of matrices), then my matrices satisfy the same relations as the ones in my group presentation for . My question is, am I done? Is this enough to show that the two groups are isomorphic? Or is there something I am missing?","SL_2(\mathbb{F}_3) A = \begin{bmatrix}
0 & -1\\
1 & 0
\end{bmatrix}
\hspace{2cm}
B = \begin{bmatrix}
1 & 1\\
1 & -1
\end{bmatrix}
 Q_8 Q_8 Q_8 = \langle -1, i, j, k|(-1)^2=1, i^2=j^2=k^2=ijk=-1 \rangle. A i B j AB k -1 -I I I -I Q_8","['abstract-algebra', 'group-theory', 'group-isomorphism', 'group-presentation']"
68,"Clarification: If $R$ is a ring, then $R^n\cong R^m$ as left $R$-modules if and only if they are also isomorphic as right $R$-modules.","Clarification: If  is a ring, then  as left -modules if and only if they are also isomorphic as right -modules.",R R^n\cong R^m R R,"I was working out the details of the following problem as I was preparing for a qualifying exam: Problem: Let $R$ be a unital ring (not necessarily commutative). Prove that if the left free $R$ -modules, $R^n$ and $R^m$ are isomorphic for some positive integers $n$ and $m$ , then $R^n$ and $R^m$ are isomorphic as right $R$ -modules. This question has been asked before , but the answer is very short and doesn't work out the details. While working out the details, I've encountered some confusion. Since the answer given by Lord Shark the Unknown is short, I'll reproduce it here before asking about the pieces I've found myself confused about. Lord Shark the Unknown's answer: If $\phi:R^m\to R^n$ is a left $R$ -module isomorphism,   and $\psi:R^n\to R^m$ is its inverse, then they correspond   to matrices $A$ and $B$ over $R$ with $AB=I_m$ and $BA=I_n$ .   But then $A$ and $B$ correspond to right $R$ -module maps $R^n\to R^m$ and $R^m\to R^n$ which are inverse to each other. My work: Minor comment, it appears that $\phi$ is intended to correspond to $A$ and $\psi$ to $B$ , so I would think that $AB$ should correspond to $\phi \circ \psi= 1_{R^n}$ . Thus I'll assume that $\phi$ should be $\phi:R^n\to R^m$ and $\psi:R^m\to R^n$ . It's quite possible that something weird happens with noncommutative rings, and this was correct as is, and I'm missing something.  ( Later comment : It's also possible Lord Shark the Unknown was working with the transposes of the matrices that I'm thinking of, in which case these dimensions make sense). Then let $e_1,\ldots,e_n$ be the standard basis for $R^n$ , $f_1,\ldots,f_m$ the standard basis for $R^m$ .  Let $A=[\phi]$ be defined by $$\phi(e_j)=\sum_i A_{ij}f_i,$$ and $B=[\psi]$ be defined by $$\psi(f_i)=\sum_j B_{ji}e_j.$$ Ignoring that $\phi\circ \psi = 1_{R^m}$ , $C:=[\phi\circ \psi]$ should be the  matrix such that $$\phi(\psi(f_i))=\sum_k C_{ki}f_k,\newcommand\of[1]{\left({#1}\right)}$$ but $$\phi(\psi(f_i)) = \phi\of{\sum_j B_{ji}e_j} = \sum_j B_{ji}\phi(e_j)  =\sum_j B_{ji} \sum_k A_{kj}f_k =\sum_k \of{\sum_j B_{ji}A_{kj}}f_k.$$ Thus $C_{ki} =\sum_j B_{ji}A_{kj}$ . Hence $B^TA^T = C^T$ . Alternatively, if we regard $A$ and $B$ as being matrices over $R^{\text{op}}$ , we get $AB=C$ , as claimed. Now over $R^{\text{op}}$ we get $AB=I_m$ , $BA=I_n$ , or over $R$ , we get $B^TA^T=I_m$ , and $A^TB^T=I_n$ . This suggests that we should use the  transposes to define the maps for the right modules, since right linear maps won't reverse the order of multiplication. (If $\phi(v)=ws$ , $\psi(w)=ur$ , then $\psi(\phi(v))=\psi(ws)=\psi(w)s=urs$ ). Then if we define $$\tilde{\phi}(e_j) =\sum_i f_i B_{ji}\text{, and } \tilde{\psi}(f_i) =\sum_j e_j A_{ij},$$ we can check that $$\tilde{\phi}(\tilde{\psi}(f_i))  = \tilde{\phi}\of{\sum_j e_j A_{ij} } = \sum_j \tilde{\phi}(e_j) A_{ij} = \sum_j \sum_k f_kB_{jk}A_{ij}  = \sum_k f_k \delta_{ik} = f_i, $$ and similarly, we get $\tilde{\psi}(\tilde{\phi}(e_j))=e_j$ , so $\tilde{\phi}$ and $\tilde{\psi}$ are inverse isomorphisms. Questions: Is this the standard way to handle matrices over noncommutative rings? I.e., for left modules do we usually take the entries to lie in $R^{\text{op}}$ ? For right modules it appears that the entries lie in $R$ . Then taking transposes  gives an isomorphism between $\newcommand\op{\text{op}}\newcommand\Mat{\mathrm{Mat}}\Mat_{n\times m}(R^{\text{op}})$ and $\Mat_{m\times n}(R)$ ? Is this correct, and is it the standard way to think about these things? If anyone could let me know if I've understood the intent of Lord Shark the Unknown's answer, or if I'm misunderstanding, that would be very helpful. It feels like there should be a more conceptual way of thinking about what's going on here, by translating the matrix argument into an argument about $\operatorname{Hom}$ functors/dualization. Something like the following: Let $\phi: R^n\to R^m$ and $\psi: R^m \to R^n$ be inverse isomorphisms. Let $*$ denote the functor $\newcommand\Hom{\operatorname{Hom}}\Hom(-,R)$ . Then $\psi^*:R^{n*}\to R^{m*}$ and $\phi^*:R^{m*}\to R^{n*}$ are inverse  isomorphisms. $R^{n*}$ has a natural right $R$ -module structure so that $R^{n*}\simeq R^n$ as right $R$ -modules. The natural right $R$ -module structure should be simply right multiplication by elements of $R$ . I.e., if $\alpha \in \Hom(R^n,R)$ , and $s\in R$ , then define $(\alpha s)(x) = \alpha(x)s$ . As for the natural isomorphism with $R^n$ , it should be given by $\alpha \mapsto (\alpha(e_i))_i$ . Right linearity follows from the definition of the right action of $R$ on $\Hom(R^n,R)$ , injectivity follows from the fact that the $e_i$ generate $R^n$ , and surjectivity follows from the existence of $f_j$ such that $f_j(e_i)=\delta_{ij}$ , since $R^n$ is free. Is this idea correct?","I was working out the details of the following problem as I was preparing for a qualifying exam: Problem: Let be a unital ring (not necessarily commutative). Prove that if the left free -modules, and are isomorphic for some positive integers and , then and are isomorphic as right -modules. This question has been asked before , but the answer is very short and doesn't work out the details. While working out the details, I've encountered some confusion. Since the answer given by Lord Shark the Unknown is short, I'll reproduce it here before asking about the pieces I've found myself confused about. Lord Shark the Unknown's answer: If is a left -module isomorphism,   and is its inverse, then they correspond   to matrices and over with and .   But then and correspond to right -module maps and which are inverse to each other. My work: Minor comment, it appears that is intended to correspond to and to , so I would think that should correspond to . Thus I'll assume that should be and . It's quite possible that something weird happens with noncommutative rings, and this was correct as is, and I'm missing something.  ( Later comment : It's also possible Lord Shark the Unknown was working with the transposes of the matrices that I'm thinking of, in which case these dimensions make sense). Then let be the standard basis for , the standard basis for .  Let be defined by and be defined by Ignoring that , should be the  matrix such that but Thus . Hence . Alternatively, if we regard and as being matrices over , we get , as claimed. Now over we get , , or over , we get , and . This suggests that we should use the  transposes to define the maps for the right modules, since right linear maps won't reverse the order of multiplication. (If , , then ). Then if we define we can check that and similarly, we get , so and are inverse isomorphisms. Questions: Is this the standard way to handle matrices over noncommutative rings? I.e., for left modules do we usually take the entries to lie in ? For right modules it appears that the entries lie in . Then taking transposes  gives an isomorphism between and ? Is this correct, and is it the standard way to think about these things? If anyone could let me know if I've understood the intent of Lord Shark the Unknown's answer, or if I'm misunderstanding, that would be very helpful. It feels like there should be a more conceptual way of thinking about what's going on here, by translating the matrix argument into an argument about functors/dualization. Something like the following: Let and be inverse isomorphisms. Let denote the functor . Then and are inverse  isomorphisms. has a natural right -module structure so that as right -modules. The natural right -module structure should be simply right multiplication by elements of . I.e., if , and , then define . As for the natural isomorphism with , it should be given by . Right linearity follows from the definition of the right action of on , injectivity follows from the fact that the generate , and surjectivity follows from the existence of such that , since is free. Is this idea correct?","R R R^n R^m n m R^n R^m R \phi:R^m\to R^n R \psi:R^n\to R^m A B R AB=I_m BA=I_n A B R R^n\to R^m R^m\to R^n \phi A \psi B AB \phi \circ \psi= 1_{R^n} \phi \phi:R^n\to R^m \psi:R^m\to R^n e_1,\ldots,e_n R^n f_1,\ldots,f_m R^m A=[\phi] \phi(e_j)=\sum_i A_{ij}f_i, B=[\psi] \psi(f_i)=\sum_j B_{ji}e_j. \phi\circ \psi = 1_{R^m} C:=[\phi\circ \psi] \phi(\psi(f_i))=\sum_k C_{ki}f_k,\newcommand\of[1]{\left({#1}\right)} \phi(\psi(f_i)) = \phi\of{\sum_j B_{ji}e_j} = \sum_j B_{ji}\phi(e_j) 
=\sum_j B_{ji} \sum_k A_{kj}f_k =\sum_k \of{\sum_j B_{ji}A_{kj}}f_k. C_{ki} =\sum_j B_{ji}A_{kj} B^TA^T = C^T A B R^{\text{op}} AB=C R^{\text{op}} AB=I_m BA=I_n R B^TA^T=I_m A^TB^T=I_n \phi(v)=ws \psi(w)=ur \psi(\phi(v))=\psi(ws)=\psi(w)s=urs \tilde{\phi}(e_j) =\sum_i f_i B_{ji}\text{, and }
\tilde{\psi}(f_i) =\sum_j e_j A_{ij}, \tilde{\phi}(\tilde{\psi}(f_i)) 
= \tilde{\phi}\of{\sum_j e_j A_{ij} }
= \sum_j \tilde{\phi}(e_j) A_{ij}
= \sum_j \sum_k f_kB_{jk}A_{ij} 
= \sum_k f_k \delta_{ik}
= f_i,
 \tilde{\psi}(\tilde{\phi}(e_j))=e_j \tilde{\phi} \tilde{\psi} R^{\text{op}} R \newcommand\op{\text{op}}\newcommand\Mat{\mathrm{Mat}}\Mat_{n\times m}(R^{\text{op}}) \Mat_{m\times n}(R) \operatorname{Hom} \phi: R^n\to R^m \psi: R^m \to R^n * \newcommand\Hom{\operatorname{Hom}}\Hom(-,R) \psi^*:R^{n*}\to R^{m*} \phi^*:R^{m*}\to R^{n*} R^{n*} R R^{n*}\simeq R^n R R R \alpha \in \Hom(R^n,R) s\in R (\alpha s)(x) = \alpha(x)s R^n \alpha \mapsto (\alpha(e_i))_i R \Hom(R^n,R) e_i R^n f_j f_j(e_i)=\delta_{ij} R^n","['abstract-algebra', 'ring-theory', 'modules', 'noncommutative-algebra', 'free-modules']"
69,Why does abstract algebra have just binary functions? [duplicate],Why does abstract algebra have just binary functions? [duplicate],,"This question already has answers here : Is abstract algebra (mostly?) restricted to $2$-ary operators? (4 answers) Closed 4 years ago . Are there operations/functions that take any other than 2 arguments in abstract algebra? If there are, then why are they not used or shown while teaching the topic? If there are not, then why is the subject constrained to just binary functions?","This question already has answers here : Is abstract algebra (mostly?) restricted to $2$-ary operators? (4 answers) Closed 4 years ago . Are there operations/functions that take any other than 2 arguments in abstract algebra? If there are, then why are they not used or shown while teaching the topic? If there are not, then why is the subject constrained to just binary functions?",,"['abstract-algebra', 'binary-operations']"
70,Show that a finite abelian group is cyclic if and only if every Sylow-p-subgroups are cyclic.,Show that a finite abelian group is cyclic if and only if every Sylow-p-subgroups are cyclic.,,"For the first part i used the result of ""subgroup of cyclic group is cyclic"", then it is clear that every sylow-p-subgroup of G is also cyclic. But in the converse i'm a little bit stuck.","For the first part i used the result of ""subgroup of cyclic group is cyclic"", then it is clear that every sylow-p-subgroup of G is also cyclic. But in the converse i'm a little bit stuck.",,"['abstract-algebra', 'finite-groups', 'abelian-groups', 'cyclic-groups', 'sylow-theory']"
71,Quotient field operations are well-defined: fleshing out Vinberg's sketch,Quotient field operations are well-defined: fleshing out Vinberg's sketch,,"Let $A$ be a non-trivial integral domain. Define the relation $\sim$ on the set of pairs $A \times A\setminus\{0_A\}$ as follows: $$(a_1,b_1) \sim (a_2,b_2) \overset{\text{def}}{\Longleftrightarrow} a_1b_2=a_2b_1.$$ It turns out that $\sim$ is an equivalence relation on $A \times A\setminus\{0_A\}$ . Addition and multiplication procedure is defined as follows. $$(a_1,b_1)+(a_2,b_2) \overset{\text{def}}{=} (a_1b_2+a_2b_1,b_1b_2)\\(a_1,b_1)\cdot(a_2,b_2)\overset{\text{def}}{=}(a_1a_2,b_1b_2).$$ If one wishes to define such operations similarly on the set of equivalence classes by $\sim$ , that is on the set $(A \times A\setminus\{0_A\})/\!\sim$ , one must prove the operations agree with the relation $\sim$ . In other words, it must be shown these procedures give a well-defined function, not depending on the choice of representative from an equivalence class. Here is how I would prove the result in the case of addition. Let $(a,b)\sim(a_1,b_1)$ and $(c,d) \sim (c_1,d_1)$ be any pairs in $A \times A\setminus\{0_A\}$ . We need to show that $(a,b)+(c,d)$ is $\sim$ -equivalent to $(a_1,b_1)+(c_1,d_1)$ , that is $(ad+bc)b_1d_1 = (a_1d_1+b_1c_1)bd.$ Hence, look at the expression $E:=(ad+bc) b_1d_1$ . Using distributivity in $A$ , we have $E=(ad)b_1d_1+(bc)b_1d_1$ . Using commutativity (and associativity) of multiplication, $E=(ab_1)dd_1+(cd_1)bb_1$ . But because $(a,b)\sim(a_1,b_1)$ and $(c,d) \sim (c_1,d_1)$ , we may replace $ab_1=a_1b$ , and $cd_1=c_1d$ . Therefore, $E=(a_1b)dd_1+(c_1d)bb_1$ . Again via distributivity (and commutativity, associativity), finally $E=(a_1d_1+b_1c_1)bd$ . QED Here is how E. B. Vinberg does it in A Course of Algebra , page 130. Define now addition and multiplication of pairs by the following rules: $$(a_1,b_1)+(a_2,b_2) = (a_1b_2+a_2b_1,b_1b_2)\\(a_1,b_1)(a_2,b_2)=(a_1a_2,b_1b_2).$$ We will prove that the equivalence relation defined above agrees with these operations. By the preceding discussion , it suffices to show that when we multiply both entries in one of the pairs $(a_1,b_1)$ or $(a_2,b_2)$ by the same element $c$ , their sum and product get replaced by equivalent pairs. But it is clear that when we do this, both entries in the sum and the product are multiplied by $c$ . (Emphasis added by me). Q: Why does it suffice to show only what Vinberg says? To emphasise, ""the preceding discussion"" is quoted in either my previous question in yellow quote boxes, or here in this post. The order of the book is preserved. I thought it would be a poor idea to again quote the full passage here due to length. Of course, I am willing to do so if necessary; in such a case, please leave an appropriate comment.","Let be a non-trivial integral domain. Define the relation on the set of pairs as follows: It turns out that is an equivalence relation on . Addition and multiplication procedure is defined as follows. If one wishes to define such operations similarly on the set of equivalence classes by , that is on the set , one must prove the operations agree with the relation . In other words, it must be shown these procedures give a well-defined function, not depending on the choice of representative from an equivalence class. Here is how I would prove the result in the case of addition. Let and be any pairs in . We need to show that is -equivalent to , that is Hence, look at the expression . Using distributivity in , we have . Using commutativity (and associativity) of multiplication, . But because and , we may replace , and . Therefore, . Again via distributivity (and commutativity, associativity), finally . QED Here is how E. B. Vinberg does it in A Course of Algebra , page 130. Define now addition and multiplication of pairs by the following rules: We will prove that the equivalence relation defined above agrees with these operations. By the preceding discussion , it suffices to show that when we multiply both entries in one of the pairs or by the same element , their sum and product get replaced by equivalent pairs. But it is clear that when we do this, both entries in the sum and the product are multiplied by . (Emphasis added by me). Q: Why does it suffice to show only what Vinberg says? To emphasise, ""the preceding discussion"" is quoted in either my previous question in yellow quote boxes, or here in this post. The order of the book is preserved. I thought it would be a poor idea to again quote the full passage here due to length. Of course, I am willing to do so if necessary; in such a case, please leave an appropriate comment.","A \sim A \times A\setminus\{0_A\} (a_1,b_1) \sim (a_2,b_2) \overset{\text{def}}{\Longleftrightarrow} a_1b_2=a_2b_1. \sim A \times A\setminus\{0_A\} (a_1,b_1)+(a_2,b_2) \overset{\text{def}}{=} (a_1b_2+a_2b_1,b_1b_2)\\(a_1,b_1)\cdot(a_2,b_2)\overset{\text{def}}{=}(a_1a_2,b_1b_2). \sim (A \times A\setminus\{0_A\})/\!\sim \sim (a,b)\sim(a_1,b_1) (c,d) \sim (c_1,d_1) A \times A\setminus\{0_A\} (a,b)+(c,d) \sim (a_1,b_1)+(c_1,d_1) (ad+bc)b_1d_1 = (a_1d_1+b_1c_1)bd. E:=(ad+bc) b_1d_1 A E=(ad)b_1d_1+(bc)b_1d_1 E=(ab_1)dd_1+(cd_1)bb_1 (a,b)\sim(a_1,b_1) (c,d) \sim (c_1,d_1) ab_1=a_1b cd_1=c_1d E=(a_1b)dd_1+(c_1d)bb_1 E=(a_1d_1+b_1c_1)bd (a_1,b_1)+(a_2,b_2) = (a_1b_2+a_2b_1,b_1b_2)\\(a_1,b_1)(a_2,b_2)=(a_1a_2,b_1b_2). (a_1,b_1) (a_2,b_2) c c","['abstract-algebra', 'proof-explanation', 'fractions', 'equivalence-relations']"
72,"Topics in Algebra - N. Herstein Exercise from Section 2.12, Question 16 (Page 103)","Topics in Algebra - N. Herstein Exercise from Section 2.12, Question 16 (Page 103)",,"Please help me with this Herstein exercise (Page 103,Sec 2.12, Ques 16). \begin{array} { l } { \text { If } G \text { is a finite group and its } p \text { -Sylow subgroup } P \text { lies in the center of } } \\ { G , \text { prove that there exists a normal subgroup } N \text { of } G \text { with } P \cap N = (e)} \\ {  \text {and } P N = G . }  \end{array} I got to know about more general theorems like Schur-Zassenhaus Theorem or Burnside's normal p-complement theorem from which this can be deduced as corollary. But, I want a solution which just uses theory built in Herstein's book. The question just before this is \begin{array} { l } { \text { Let } G \text { be a finite group in which } ( a b ) ^ { p } = a ^ { p } b ^ { p } \text { for every } a , b \in G , } \\ { \text { where } p \text { is a prime dividing } o ( G ) \text { . Prove } } \\ { \text { (a) The } p \text { -Sylow subgroup of } G \text { is normal in } G \text { . } } \\ { \text { (b) If } P \text { is the } p \text { -Sylow subgroup of } G , \text { then there exists a normal } } \\ { \text { subgroup } N \text { of } G \text { with } P \cap N = ( e ) \text { and } P N = G \text { . } } \\ { \text { (c) } G \text { has a nontrivial center. } } \end{array} I have solved it by first proving, for $p^n|o(G)$ and $p^{n+1} \not| o(G)$ , $$P=\{x\in G : x^{p^n}=e\}$$ is unique $p-Sylow$ sugroup of G and then taking a homomorphism $\phi:G\to G$ defined by $\phi(g)=g^{p^n}$ , where $p^n$ is order of $p-Sylow$ subgroup of G. Then $\phi(G)= N$","Please help me with this Herstein exercise (Page 103,Sec 2.12, Ques 16). I got to know about more general theorems like Schur-Zassenhaus Theorem or Burnside's normal p-complement theorem from which this can be deduced as corollary. But, I want a solution which just uses theory built in Herstein's book. The question just before this is I have solved it by first proving, for and , is unique sugroup of G and then taking a homomorphism defined by , where is order of subgroup of G. Then","\begin{array} { l } { \text { If } G \text { is a finite group and its } p \text { -Sylow subgroup } P \text { lies in the center of } } \\ { G , \text { prove that there exists a normal subgroup } N \text { of } G \text { with } P \cap N = (e)} \\ {  \text {and } P N = G . }  \end{array} \begin{array} { l } { \text { Let } G \text { be a finite group in which } ( a b ) ^ { p } = a ^ { p } b ^ { p } \text { for every } a , b \in G , } \\ { \text { where } p \text { is a prime dividing } o ( G ) \text { . Prove } } \\ { \text { (a) The } p \text { -Sylow subgroup of } G \text { is normal in } G \text { . } } \\ { \text { (b) If } P \text { is the } p \text { -Sylow subgroup of } G , \text { then there exists a normal } } \\ { \text { subgroup } N \text { of } G \text { with } P \cap N = ( e ) \text { and } P N = G \text { . } } \\ { \text { (c) } G \text { has a nontrivial center. } } \end{array} p^n|o(G) p^{n+1} \not| o(G) P=\{x\in G : x^{p^n}=e\} p-Sylow \phi:G\to G \phi(g)=g^{p^n} p^n p-Sylow \phi(G)= N","['abstract-algebra', 'group-theory', 'finite-groups', 'normal-subgroups', 'sylow-theory']"
73,Center of the dihedral group with odd and even number of vertices,Center of the dihedral group with odd and even number of vertices,,"I have posted a proof below, and would appreciate it if someone could review it for accuracy. Thanks! Problem: Let n $\in$ $\mathbb{Z}$ with $n$ $\ge$ 3. Prove the following: (a) Z(D $_{2n}$ ) = 1 if $n$ is odd. (b) Z(D $_{2n}$ ) = {1, r $^k$ } if $n$ = $2k$ . Note that $r$ and $s$ generate D $_{2n}$ with the group presentation { $r$ , $s$ | $r$$^n$ = $s$$^2$ = 1, $rs$ = $sr$$^{-1}$ } Proof: part (a) Let $n$ $\ge$ 3 where $n$ $\in$ $\mathbb{Z}$ and $n$ is odd. For any x $\in$ Z(D $_{2n}$ ), x must commute with both $s$ and $r$ , since both are in $D_{2n}$ . Note that if $x$ commutes with both $s$ an $r$ , then $x$ commutes with any element of $D_{2n}$ since $r$ and $s$ generate D $_{2n}$ . Then for any x $\in$ Z(D $_{2n}$ ), we have $xr$ = $rx$ , where x is of the form $x$ = s $^j$ r $^w$ (any such element can be arranged into this form via the relation $rs$ = $sr$$^{-1}$ and the fact that $r$ and $s$ generate D $_{2n}$ ). Note that since r and s have finite order, the exponents are modulo n and modulo 2 for $r$ and $s$ respectively. Then we have  s $^j$ r $^w$$r$ = $r$ s $^j$ r $^w$ .   Which implies s $^j$ r $^{w+1}$ = s $^j$ r $^{w +- 1}$ In the case where the power of r on the RHS is $w$ - 1, it is clear we cannot have equality unless |r| = 1, which it is not. Note that if $j$ is even then we have only the $w$ + 1 case. Hence $x$ may be of the form $1$$r$$^w$ = $r$$^w$ . Also we have that x must commute with $s$ , hence $s$$x$ = $x$$s$ . Then $s$$r$$^w$ = $r$$^w$$s$ .  Which implies $s$$r$$^w$ = $s$$r$$^{-w}$ . Applying s $^{-1}$ to both sides we arrive at the task of finding when r $^w$ = r $^{-w}$ i.e when $w$ = $-w$ . But since r has finite order we have: $$ \bar{w} = (n-1)*w $$ where $\bar{w}$ is the residue class of w modulo n. Hence for some a $\in$ $\mathbb{Z}$$_+$ $$ w + an = wn - w $$ $$ 2w = wn - an $$ $$ 2w = n(w-a) $$ Now since 0 $\lt$ w $\le$ n we have that the LHS is greater than 0. Also since n $\ge$ w then (w-a) $\le$ 2, since otherwise the RHS would be greater than the LHS. So since n is odd we must have that (w-a) is even and hence (w-a) = 2, implying that 2w = 2n and hence n = w. Hence x = $r$$^w$ = $r$$^n$ = 1. So Z(D $_{2n}$ ) = 1. part (b) From the argument above any x $\in$ Z(D $_{2n}$ ) must be of the form $s$$^j$$r$$^w$ for j even and j $\in$ $\mathbb{Z}$ . Then x is of the form $r$$^w$ for $0$ $\lt$ w $\le$ $n$ . Since $n$ is even we have the equation below is satisfied when $2w$ = $n$ or $w$ = $n$ , since (w-a) $\le$ 2 but also (w-a) $\ge$ 0 for (w-a) $\in$ $\mathbb{Z}$$_+$ . $$ 2w = n(w - a) $$ Hence $r$$^w$ $\in$ Z(D $_{2n}$ ) when $2w$ = $n$ , as desired.","I have posted a proof below, and would appreciate it if someone could review it for accuracy. Thanks! Problem: Let n with 3. Prove the following: (a) Z(D ) = 1 if is odd. (b) Z(D ) = {1, r } if = . Note that and generate D with the group presentation { , | = = 1, = } Proof: part (a) Let 3 where and is odd. For any x Z(D ), x must commute with both and , since both are in . Note that if commutes with both an , then commutes with any element of since and generate D . Then for any x Z(D ), we have = , where x is of the form = s r (any such element can be arranged into this form via the relation = and the fact that and generate D ). Note that since r and s have finite order, the exponents are modulo n and modulo 2 for and respectively. Then we have  s r = s r .   Which implies s r = s r In the case where the power of r on the RHS is - 1, it is clear we cannot have equality unless |r| = 1, which it is not. Note that if is even then we have only the + 1 case. Hence may be of the form = . Also we have that x must commute with , hence = . Then = .  Which implies = . Applying s to both sides we arrive at the task of finding when r = r i.e when = . But since r has finite order we have: where is the residue class of w modulo n. Hence for some a Now since 0 w n we have that the LHS is greater than 0. Also since n w then (w-a) 2, since otherwise the RHS would be greater than the LHS. So since n is odd we must have that (w-a) is even and hence (w-a) = 2, implying that 2w = 2n and hence n = w. Hence x = = = 1. So Z(D ) = 1. part (b) From the argument above any x Z(D ) must be of the form for j even and j . Then x is of the form for w . Since is even we have the equation below is satisfied when = or = , since (w-a) 2 but also (w-a) 0 for (w-a) . Hence Z(D ) when = , as desired.",\in \mathbb{Z} n \ge _{2n} n _{2n} ^k n 2k r s _{2n} r s r^n s^2 rs sr^{-1} n \ge n \in \mathbb{Z} n \in _{2n} s r D_{2n} x s r x D_{2n} r s _{2n} \in _{2n} xr rx x ^j ^w rs sr^{-1} r s _{2n} r s ^j ^wr r ^j ^w ^j ^{w+1} ^j ^{w +- 1} w j w x 1r^w r^w s sx xs sr^w r^ws sr^w sr^{-w} ^{-1} ^w ^{-w} w -w  \bar{w} = (n-1)*w  \bar{w} \in \mathbb{Z}_+  w + an = wn - w   2w = wn - an   2w = n(w-a)  \lt \le \ge \le r^w r^n _{2n} \in _{2n} s^jr^w \in \mathbb{Z} r^w 0 \lt \le n n 2w n w n \le \ge \in \mathbb{Z}_+  2w = n(w - a)  r^w \in _{2n} 2w n,"['abstract-algebra', 'group-theory', 'solution-verification', 'dihedral-groups']"
74,The Automorphism Group of Free Quandles?,The Automorphism Group of Free Quandles?,,"The following definitions are quoted from this article : My question is, do we know anything description the automorphism group of free quandles with relative smaller number of generating set $S$ ? For example, for each $2\leq |S|\leq 4$ , is $\mathrm{Aut}(FQ(S))$ isomorphic to any familiar group?","The following definitions are quoted from this article : My question is, do we know anything description the automorphism group of free quandles with relative smaller number of generating set ? For example, for each , is isomorphic to any familiar group?",S 2\leq |S|\leq 4 \mathrm{Aut}(FQ(S)),"['abstract-algebra', 'group-theory', 'automorphism-group']"
75,A projective ideal in an integral domain is finitely generated,A projective ideal in an integral domain is finitely generated,,"Let $R$ be an integral domain and $I$ a projective ideal. Then $I$ is finitely generated as an $R$ -module. This seems like it should be easy but I don't know what to argue, or where to bring in the integral domain condition. I tried localising at a prime $P$ containing $I$ , and we get that $I_P\subseteq PR_P\subseteq R_P$ is a projective $R_P$ module. Then if $0\to I_P\to R_P\to R_P/I_P\to 0$ is short exact, since projective modules are flat, we can tensor with $I_P$ , and the last term is $0$ . But there is a result that if $M$ is a flat module over a ring then if $J$ is an ideal, then $J\otimes M\cong JM$ . Hence, $I_P^2\cong I_P\otimes I_P\cong R_P\otimes I_P\cong I_P$ . But if $I$ , hence $I_P$ were finitely generated, then wouldn't that mean $I\subseteq I_P=0$ by Nakayama?","Let be an integral domain and a projective ideal. Then is finitely generated as an -module. This seems like it should be easy but I don't know what to argue, or where to bring in the integral domain condition. I tried localising at a prime containing , and we get that is a projective module. Then if is short exact, since projective modules are flat, we can tensor with , and the last term is . But there is a result that if is a flat module over a ring then if is an ideal, then . Hence, . But if , hence were finitely generated, then wouldn't that mean by Nakayama?",R I I R P I I_P\subseteq PR_P\subseteq R_P R_P 0\to I_P\to R_P\to R_P/I_P\to 0 I_P 0 M J J\otimes M\cong JM I_P^2\cong I_P\otimes I_P\cong R_P\otimes I_P\cong I_P I I_P I\subseteq I_P=0,"['abstract-algebra', 'ring-theory', 'commutative-algebra']"
76,Is there always a way to generate nontrivial finite extensions of a field?,Is there always a way to generate nontrivial finite extensions of a field?,,"Suppose I have a field $k,$ an algebraically closed field $L,$ and an embedding $k \hookrightarrow L.$ Then we know that for any algebraic extension of $k, E$ we can extend the embedding to $E \hookrightarrow L.$ However, if we take $L$ to be the algebraic closure of $k$ and $E$ to be a nontrivial finite extension of $L$ then $E$ is algebraic over $k$ and we have an embedding of $E \hookrightarrow L.$ So this would mean that $L$ and $E$ are infinite dimensional, otherwise, if $L$ and $E$ were finite, $E$ would have strictly greater dimension and such an embedding would not make sense. However, we have something like $\mathbb{C}$ that is a finite dimensional algebraic closure of $\mathbb{R}.$ So would this mean that there are no nontrivial finite extensions of algebraically closed fields in the first place? For finite fields, we can always take its polynomial rings and modulo out by some irreducible. For example, $\mathbb{R}[X]/X^2 + 1.$ However, this does not work for algebraically closed fields. But I feel that we should be able to make nontrivial extensions for any field. Is this not the case?","Suppose I have a field an algebraically closed field and an embedding Then we know that for any algebraic extension of we can extend the embedding to However, if we take to be the algebraic closure of and to be a nontrivial finite extension of then is algebraic over and we have an embedding of So this would mean that and are infinite dimensional, otherwise, if and were finite, would have strictly greater dimension and such an embedding would not make sense. However, we have something like that is a finite dimensional algebraic closure of So would this mean that there are no nontrivial finite extensions of algebraically closed fields in the first place? For finite fields, we can always take its polynomial rings and modulo out by some irreducible. For example, However, this does not work for algebraically closed fields. But I feel that we should be able to make nontrivial extensions for any field. Is this not the case?","k, L, k \hookrightarrow L. k, E E \hookrightarrow L. L k E L E k E \hookrightarrow L. L E L E E \mathbb{C} \mathbb{R}. \mathbb{R}[X]/X^2 + 1.","['abstract-algebra', 'field-theory']"
77,Trouble understanding converse of Tarski's theorem,Trouble understanding converse of Tarski's theorem,,"I am working through a paper entitled ""Elimination of quantifiers in algebraic structures,"" and am having a lot of trouble understanding the first theorem. The paper is available here and attempts to prove a partial converse of one of Tarski's theorems ( here is the particular theorem they prove the converse of). Namely, they want to show that if $K$ is an infinite field, whose $\mathcal{L}$ -theory admits quantifier elimination (QE), where $\mathcal{L}=\{=,0,1,+,-,\cdot\}$ (i.e. the language of rings with identity), then $K$ is algebraically closed. They attempt to do this by contradiction. From here on out I will copy out bits of the proof and walkthrough what is clear, and what is not. We argue by contradiction. Let $K$ have an algebraic extension $K(\alpha)$ of degree $n>1$ . I get the degree has to be $n>1$ otherwise, $\alpha\in K$ such that $K(\alpha)=K$ , contrary to assumption. Let $f(y,\bar{x})=y^n+x_{n-1}y^{n-1}+...+x_0$ . Then there is a quantifier free formula $\phi(\bar{x})$ which we may take as a disjunction of Type A (this is explained in Lemma 1 in the paper), which is equivalent in $K$ to $\forall y\,(f(y,\bar{x})\neq0)$ . As I understand this, I think they are saying that $f$ is the minimal polynomial for $\alpha$ . So clearly, $y\in K$ cannot be a root of it. Otherwise, it would not be minimal. Clearly since $K\vDash\forall y\,(f(y,\bar{x}))$ Now, the paper goes on to assume that $\alpha$ is seperable over $K$ . I'm not entirely sure why? Does the proof become obvious if $\alpha$ is not seperable? After this part, I get very confused about what the authors are trying to do. Any help clarifying how I should read this proof would be appreciated. I'm especially having a lot of trouble seeing the relationships between $f(y,\bar{x})$ and the big $F$ polynomial. I think the crucial part of the proof I don't understand is these couple of sentences somewhere in the middle: Now $\{F_j(\bar{z})\}$ is algebraically independent over $K$ and $K$ is infinite. Thus, there is no $K$ -Zariski closed proper subset $X$ of $K^n$ such that $X\supset\{\langle F_0(\bar{K},...,F_{n-1}(\bar{k}))\rangle|\text{$\bar{k}\in K^n$, $k_i\neq 0$ for some $i>1$}\}$ .","I am working through a paper entitled ""Elimination of quantifiers in algebraic structures,"" and am having a lot of trouble understanding the first theorem. The paper is available here and attempts to prove a partial converse of one of Tarski's theorems ( here is the particular theorem they prove the converse of). Namely, they want to show that if is an infinite field, whose -theory admits quantifier elimination (QE), where (i.e. the language of rings with identity), then is algebraically closed. They attempt to do this by contradiction. From here on out I will copy out bits of the proof and walkthrough what is clear, and what is not. We argue by contradiction. Let have an algebraic extension of degree . I get the degree has to be otherwise, such that , contrary to assumption. Let . Then there is a quantifier free formula which we may take as a disjunction of Type A (this is explained in Lemma 1 in the paper), which is equivalent in to . As I understand this, I think they are saying that is the minimal polynomial for . So clearly, cannot be a root of it. Otherwise, it would not be minimal. Clearly since Now, the paper goes on to assume that is seperable over . I'm not entirely sure why? Does the proof become obvious if is not seperable? After this part, I get very confused about what the authors are trying to do. Any help clarifying how I should read this proof would be appreciated. I'm especially having a lot of trouble seeing the relationships between and the big polynomial. I think the crucial part of the proof I don't understand is these couple of sentences somewhere in the middle: Now is algebraically independent over and is infinite. Thus, there is no -Zariski closed proper subset of such that .","K \mathcal{L} \mathcal{L}=\{=,0,1,+,-,\cdot\} K K K(\alpha) n>1 n>1 \alpha\in K K(\alpha)=K f(y,\bar{x})=y^n+x_{n-1}y^{n-1}+...+x_0 \phi(\bar{x}) K \forall y\,(f(y,\bar{x})\neq0) f \alpha y\in K K\vDash\forall y\,(f(y,\bar{x})) \alpha K \alpha f(y,\bar{x}) F \{F_j(\bar{z})\} K K K X K^n X\supset\{\langle F_0(\bar{K},...,F_{n-1}(\bar{k}))\rangle|\text{\bar{k}\in K^n, k_i\neq 0 for some i>1}\}","['abstract-algebra', 'logic', 'field-theory', 'model-theory']"
78,A problem about number of functions and homomorphism,A problem about number of functions and homomorphism,,"I'm trying to solve this problem. How many functions $f: \Bbb Z_{10}\rightarrow \Bbb Z_3$ are there such that $|f^{-1}([0]_3)| = 3$ or $|f^{-1}([1]_3)| = 4$ ? How many of them are such that the restriction to the multiplicative group of $Z_{10}$ is a homomorphism (basically the domain is the multiplicative group of $\Bbb Z_{10}$ and the codomain the multiplicative group of $\Bbb Z_{3}$ ). I start from the assumption that I'm not sure I understood the problem and that I have some difficulties for the second question. Anyway, this is my reasoning. Any help is welcome. Since $|f^{-1}([0]_3)| = 3$ we know that only three elements in $\Bbb Z_{10}$ are associated by the function to $[0]_3$ . So I exclude these three elements since they are already associated and for the definition of function, I can not associate them further. I exclude $[0]_3$ because otherwise if I could associate other elements $|f^{-1}([0]_3)| \neq 3$ . So basically I have now $7$ elements of $\Bbb Z_{10}$ which I can associate to $1,2 \in \Bbb Z_3$ . For this reason, there are $2^7$ possible function. Same reasoning for $|f^{-1}([1]_3)| = 4$ . Is that correct? For the second question, I found online that the number of homomorphism between two cyclic groups is the GCD of their order. First of all, can someone explain to me why the GCD? It is an information that I miss from the study of group theory or that I have neglected. So I need to know the order of the two multiplicative groups. I use the Euler totient function to do this and I obtain $4$ for the multiplicative group of $\Bbb Z_{10}$ and $2$ for $\Bbb Z_3$ . Now $gcd(2,4) = 2$ so there are two homomorphism (?). Is my reasoning correct?","I'm trying to solve this problem. How many functions are there such that or ? How many of them are such that the restriction to the multiplicative group of is a homomorphism (basically the domain is the multiplicative group of and the codomain the multiplicative group of ). I start from the assumption that I'm not sure I understood the problem and that I have some difficulties for the second question. Anyway, this is my reasoning. Any help is welcome. Since we know that only three elements in are associated by the function to . So I exclude these three elements since they are already associated and for the definition of function, I can not associate them further. I exclude because otherwise if I could associate other elements . So basically I have now elements of which I can associate to . For this reason, there are possible function. Same reasoning for . Is that correct? For the second question, I found online that the number of homomorphism between two cyclic groups is the GCD of their order. First of all, can someone explain to me why the GCD? It is an information that I miss from the study of group theory or that I have neglected. So I need to know the order of the two multiplicative groups. I use the Euler totient function to do this and I obtain for the multiplicative group of and for . Now so there are two homomorphism (?). Is my reasoning correct?","f: \Bbb Z_{10}\rightarrow \Bbb Z_3 |f^{-1}([0]_3)| = 3 |f^{-1}([1]_3)| = 4 Z_{10} \Bbb Z_{10} \Bbb Z_{3} |f^{-1}([0]_3)| = 3 \Bbb Z_{10} [0]_3 [0]_3 |f^{-1}([0]_3)| \neq 3 7 \Bbb Z_{10} 1,2 \in \Bbb Z_3 2^7 |f^{-1}([1]_3)| = 4 4 \Bbb Z_{10} 2 \Bbb Z_3 gcd(2,4) = 2","['abstract-algebra', 'group-theory']"
79,Any affine conic is isomorphic to $V(y-x^2)$ or $V(xy-1)$,Any affine conic is isomorphic to  or,V(y-x^2) V(xy-1),"I'm trying to solve the above exercise in Ghatmann's notes on algebraic geometry. I tried to consider the real case and I suspect that X1 should correspond to a plane that intersects with exactly one of the cones, whereas X2 is the case in which both cones are intersected (not at the origin). Next i tried to look at the general case of a quadratic polynomial in two variables to see what happens if I act on it with an affine change of coordinates(i.e. linear transformation followed by translation), this quickly got very complicated. My guess is that I need to figure out how the irreducibility condition affects the general equation, and maybe extend the intuition I gained for the real case to arbitrary fields(of characteristic $\ne$ 2). However, right now I'm kind of stuck, so any help would be appreciated. Thanks. Note that the field $K$ is assumed to be algebraically closed.","I'm trying to solve the above exercise in Ghatmann's notes on algebraic geometry. I tried to consider the real case and I suspect that X1 should correspond to a plane that intersects with exactly one of the cones, whereas X2 is the case in which both cones are intersected (not at the origin). Next i tried to look at the general case of a quadratic polynomial in two variables to see what happens if I act on it with an affine change of coordinates(i.e. linear transformation followed by translation), this quickly got very complicated. My guess is that I need to figure out how the irreducibility condition affects the general equation, and maybe extend the intuition I gained for the real case to arbitrary fields(of characteristic 2). However, right now I'm kind of stuck, so any help would be appreciated. Thanks. Note that the field is assumed to be algebraically closed.",\ne K,"['abstract-algebra', 'algebraic-geometry', 'commutative-algebra', 'conic-sections']"
80,Does there exist some sort of classification of finite verbally simple groups?,Does there exist some sort of classification of finite verbally simple groups?,,"Lets call a group verbally simple if it does not have any non-trivial verbal subgroup. Does there exist some sort of classification of finite verbally simple groups? $G^n$ , with $G$ being a finite simple group, is always verbally simple as it has no nontrivial characteristic subgroups and all verbal subgroups are characteristic. However those may be not the only examples... If $G$ is verbally simple it is either abelian or perfect, as commutator subgroup is verbal. If $G$ is abelian, then it is $C_p^n$ for some prime $p$ as for any abelian group $A$ $V_{x^q}(A)$ is a nontrivial proper subgroup for any $q$ that is a nontrivial proper divisor of $exp(A)$ . However, I do not know, how to deal with the case, when $G$ is perfect.","Lets call a group verbally simple if it does not have any non-trivial verbal subgroup. Does there exist some sort of classification of finite verbally simple groups? , with being a finite simple group, is always verbally simple as it has no nontrivial characteristic subgroups and all verbal subgroups are characteristic. However those may be not the only examples... If is verbally simple it is either abelian or perfect, as commutator subgroup is verbal. If is abelian, then it is for some prime as for any abelian group is a nontrivial proper subgroup for any that is a nontrivial proper divisor of . However, I do not know, how to deal with the case, when is perfect.",G^n G G G C_p^n p A V_{x^q}(A) q exp(A) G,"['abstract-algebra', 'group-theory', 'finite-groups', 'verbal-subgroups', 'characteristic-subgroups']"
81,Show that if $G$ is a finite group and $H_i$ are subgroups of $G$ with $[G:H_i]=2$ then $[G:\cap H_i]=$ some power of $2$,Show that if  is a finite group and  are subgroups of  with  then  some power of,G H_i G [G:H_i]=2 [G:\cap H_i]= 2,"Show that if $G$ is a finite group and $H_i$ are subgroups of $G$ with $[G:H_i]=2$ then $[G:\cap H_i]=$ some power of $2$ . My try : Let the number of subgroups of $G$ be $H_1,H_2,\ldots ,H_m$ Its clear that each $H_i$ is a normal subgroup of $G$ and every $H_i$ has exactly two left/right cosets. Let the left cosets of $H_1$ in $G$ be $H_1,g_1H_1$ ,that of $H_i$ in $G$ be $H_i,g_iH_i$ and so on. Let $H=\cap H_i$ Now we know that $[G:H\cap K]\le [G:H][G:K]$ for any two subgroups $H,K$ of $G$ . Thus we have $[G:H]\le 2^m$ Now I need to show only that $[G:H]\ge 2^m$ Now I understand that since $g_i\notin H_i\implies g_i\notin H$ hence we have at least $m$ cosets of $H$ in $G$ given by $g_1H,g_2H,\ldots g_mH$ But I need to find at least $2^m$ How can I do it? Please give some hints","Show that if is a finite group and are subgroups of with then some power of . My try : Let the number of subgroups of be Its clear that each is a normal subgroup of and every has exactly two left/right cosets. Let the left cosets of in be ,that of in be and so on. Let Now we know that for any two subgroups of . Thus we have Now I need to show only that Now I understand that since hence we have at least cosets of in given by But I need to find at least How can I do it? Please give some hints","G H_i G [G:H_i]=2 [G:\cap H_i]= 2 G H_1,H_2,\ldots ,H_m H_i G H_i H_1 G H_1,g_1H_1 H_i G H_i,g_iH_i H=\cap H_i [G:H\cap K]\le [G:H][G:K] H,K G [G:H]\le 2^m [G:H]\ge 2^m g_i\notin H_i\implies g_i\notin H m H G g_1H,g_2H,\ldots g_mH 2^m",['abstract-algebra']
82,Number of orbits and representatives of an action of $Gl_2(\mathbb{Z}_2)$ in $M_2(\mathbb{Z}_2)$ by conjugation,Number of orbits and representatives of an action of  in  by conjugation,Gl_2(\mathbb{Z}_2) M_2(\mathbb{Z}_2),"So I'm asked to find the number of orbits and representatives of that action, my idea was to find all the possible rationals forms induced by a polynomial of order 2 in $\mathbb{Z}_2[x]$ , thinking of it as modules over $\mathbb{Z}_2[x]$ , but i only seem to find 6, with the modules by $\mathbb{Z}_2[x]/x,\mathbb{Z}_2[x]/(x-1) \bigoplus \mathbb{Z}_2[x]/x,\mathbb{Z}_2[x]/(x-1),\mathbb{Z}_2[x]/(x+x+1),\mathbb{Z}_2[x]/(x-1)\bigoplus\mathbb{Z}_2[x]/x ,\mathbb{Z}_2[x]/x \bigoplus \mathbb{Z}_2[x]/x$ , but they are supposed to be 7 according to the solutions, can u help me out? Thanks.","So I'm asked to find the number of orbits and representatives of that action, my idea was to find all the possible rationals forms induced by a polynomial of order 2 in , thinking of it as modules over , but i only seem to find 6, with the modules by , but they are supposed to be 7 according to the solutions, can u help me out? Thanks.","\mathbb{Z}_2[x] \mathbb{Z}_2[x] \mathbb{Z}_2[x]/x,\mathbb{Z}_2[x]/(x-1) \bigoplus \mathbb{Z}_2[x]/x,\mathbb{Z}_2[x]/(x-1),\mathbb{Z}_2[x]/(x+x+1),\mathbb{Z}_2[x]/(x-1)\bigoplus\mathbb{Z}_2[x]/x ,\mathbb{Z}_2[x]/x \bigoplus \mathbb{Z}_2[x]/x","['abstract-algebra', 'group-actions']"
83,"Let $ R $ be a p.i.d. and $ A\in M_n(R) $. If $ \det(A)=1 $, prove or disprove that $ A $ can be expressed as products of elementary matrices.","Let  be a p.i.d. and . If , prove or disprove that  can be expressed as products of elementary matrices.", R   A\in M_n(R)   \det(A)=1   A ,"Let $ R $ be a p.i.d. and $ A\in M_n(R) $ . If $ \det(A)=1 $ , prove or disprove that $ A $ can be expressed as products of elementary matrices. I know that we can express $ A $ as products of elementary matrices when we require $ R $ to be Euclidean. Since we can get rid of matrices  whose left uppermost elements are like $$ \begin{pmatrix}x & s\\ y & t\end{pmatrix} .$$ (Note: Furthermore, I know that if we require $ R $ to be a field, and $ \det(A)=1 $ , we can even express $ A $ as products of transvection matrices. Transvection matrices generate SLn(R) ) Now back to this question, it seems to me that it suffices to prove that invertible matrices of type: $$ \begin{pmatrix}x & s\\ y & t\end{pmatrix} $$ where $ ax+by=d $ , $ \gcd(a, b)=d $ and $ s=bd^{-1}, t=-ad^{-1} $ (Here the inverse is obtained by cancellation in $ R $ ) can be expressed into products of elementary matrices. Since $ \det(A)=1 $ , then $ d=1 $ and we have: $ \gcd(a, b)=1, s=b, t=-a $ . Well how to move on? EDIT: I have changed my title and statement in a less misleading way since I find out that we can't express $ A $ in such a way under the given assumption generally.","Let be a p.i.d. and . If , prove or disprove that can be expressed as products of elementary matrices. I know that we can express as products of elementary matrices when we require to be Euclidean. Since we can get rid of matrices  whose left uppermost elements are like (Note: Furthermore, I know that if we require to be a field, and , we can even express as products of transvection matrices. Transvection matrices generate SLn(R) ) Now back to this question, it seems to me that it suffices to prove that invertible matrices of type: where , and (Here the inverse is obtained by cancellation in ) can be expressed into products of elementary matrices. Since , then and we have: . Well how to move on? EDIT: I have changed my title and statement in a less misleading way since I find out that we can't express in such a way under the given assumption generally."," R   A\in M_n(R)   \det(A)=1   A   A   R   \begin{pmatrix}x & s\\ y & t\end{pmatrix} .  R   \det(A)=1   A   \begin{pmatrix}x & s\\ y & t\end{pmatrix}   ax+by=d   \gcd(a, b)=d   s=bd^{-1}, t=-ad^{-1}   R   \det(A)=1   d=1   \gcd(a, b)=1, s=b, t=-a   A ","['abstract-algebra', 'matrices', 'principal-ideal-domains']"
84,Finding the number of subgroups of $\mathbb{Z}_{p^3} \oplus \mathbb{Z}_{p^2} $.,Finding the number of subgroups of .,\mathbb{Z}_{p^3} \oplus \mathbb{Z}_{p^2} ,What is the number of subgroups of order $p^2$ of $\mathbb{Z}_{p^3} \oplus \mathbb{Z}_{p^2} $ ? I'm not really sure how to figure it out. I tried seeing subgroups of each $\Bbb Z_{p^n}$ but I'm not sure I'm going to get them all.,What is the number of subgroups of order of ? I'm not really sure how to figure it out. I tried seeing subgroups of each but I'm not sure I'm going to get them all.,p^2 \mathbb{Z}_{p^3} \oplus \mathbb{Z}_{p^2}  \Bbb Z_{p^n},"['abstract-algebra', 'group-theory', 'finite-groups', 'p-groups']"
85,Understanding the structure of a group through its decomposition in normal subgroups,Understanding the structure of a group through its decomposition in normal subgroups,,"I am confused with a basic fact in group theory. ''Given a finite group $G,$ one can find a sequence of subgroups $$G=H_0\lhd H_1\lhd H_2 \cdot \cdot  \cdot \cdot \lhd H_r = \{e\}$$ and such that $H_{k+1}/H_k$ is simple.'' Since one talks about a group in general terms, then the statement must hold also for a simple group,whereby the sequence will be trivial. But in the case $G$ is not simple, by definition it must be solvable and the statement must hold again. But then we read another statement in algebra books: ''Given the finite group $G,$ it is solvable if one can find a sequence of subgroups $$G=H_0\lhd H_1\lhd H_2 \cdot \cdot  \cdot \cdot \lhd H_r = \{e\}$$ and such that $H_{k+1}/H_k$ is abelian.'' To my understanding, since the second statement is more specific than the first one, one would conclude that the abelian groups $H_{k+1}/H_k$ as stated in the second statement are simple (in which case they will be isomorphic to cyclic groups of prime order), something that is not stated in books. At the other hand, since the first statement must be true also for solvable groups, we must conclude that the simple group $H_{k+1}/H_k$ must be abelian in the case of a solvable group. Can somebody comment on my conclusions and say what is wrong or true. Which statement implies which one, or, are they equivalent in the case of solvable groups ? One also reads in algebra books (I quote here Lang's book) the following: ''such a sequence already gives information about $G$ . To get a full knowledge of $G$ , one would have to know how these factor groups are pieced together.'' Can somebody explain through an example, what knowledge of $G$ we get through such a sequence and how to proceed to piece the factor groups together. Many thanks.","I am confused with a basic fact in group theory. ''Given a finite group one can find a sequence of subgroups and such that is simple.'' Since one talks about a group in general terms, then the statement must hold also for a simple group,whereby the sequence will be trivial. But in the case is not simple, by definition it must be solvable and the statement must hold again. But then we read another statement in algebra books: ''Given the finite group it is solvable if one can find a sequence of subgroups and such that is abelian.'' To my understanding, since the second statement is more specific than the first one, one would conclude that the abelian groups as stated in the second statement are simple (in which case they will be isomorphic to cyclic groups of prime order), something that is not stated in books. At the other hand, since the first statement must be true also for solvable groups, we must conclude that the simple group must be abelian in the case of a solvable group. Can somebody comment on my conclusions and say what is wrong or true. Which statement implies which one, or, are they equivalent in the case of solvable groups ? One also reads in algebra books (I quote here Lang's book) the following: ''such a sequence already gives information about . To get a full knowledge of , one would have to know how these factor groups are pieced together.'' Can somebody explain through an example, what knowledge of we get through such a sequence and how to proceed to piece the factor groups together. Many thanks.","G, G=H_0\lhd H_1\lhd H_2 \cdot \cdot  \cdot \cdot \lhd H_r = \{e\} H_{k+1}/H_k G G, G=H_0\lhd H_1\lhd H_2 \cdot \cdot  \cdot \cdot \lhd H_r = \{e\} H_{k+1}/H_k H_{k+1}/H_k H_{k+1}/H_k G G G","['abstract-algebra', 'group-theory']"
86,Example of a ring with no minimal prime ideal,Example of a ring with no minimal prime ideal,,"I am a math student, in the course of abstract algebra we have shown that in a unitary commutative ring every ideal I possesses at least one minimal prime ideal. I am trying to find an example of ideal contents in a non-commutative (not unitary) ring that does not possess a minimal prime ideal.","I am a math student, in the course of abstract algebra we have shown that in a unitary commutative ring every ideal I possesses at least one minimal prime ideal. I am trying to find an example of ideal contents in a non-commutative (not unitary) ring that does not possess a minimal prime ideal.",,"['abstract-algebra', 'ideals', 'maximal-and-prime-ideals']"
87,How does group cohomology behaves where coefficient is direct sum?,How does group cohomology behaves where coefficient is direct sum?,,"Let $G$ be a finite group and $A_1$ and $A_2$ be $\mathbb ZG$ modules. Is it always true that for any $n\in \mathbb N$ , the cohomology group of $G$ with coefficients in $A_1\oplus A_2$ is the same as the following: $$H^n(G; A_1\oplus A_2)=H^n(G; A_1)\oplus H^n(G; A_2) ?$$ If not, then how to calculate $H^n(G; A_1\oplus A_2)$ in terms of $H^n(G; A_1)$ and $H^n(G; A_2)$ ? Thanks in advance.","Let be a finite group and and be modules. Is it always true that for any , the cohomology group of with coefficients in is the same as the following: If not, then how to calculate in terms of and ? Thanks in advance.",G A_1 A_2 \mathbb ZG n\in \mathbb N G A_1\oplus A_2 H^n(G; A_1\oplus A_2)=H^n(G; A_1)\oplus H^n(G; A_2) ? H^n(G; A_1\oplus A_2) H^n(G; A_1) H^n(G; A_2),"['abstract-algebra', 'group-theory', 'algebraic-topology', 'group-cohomology']"
88,"If a field $F$ is infinite, show that the ring homomorphism $\eta : F[x]\to C(F)$ is one-to-one.","If a field  is infinite, show that the ring homomorphism  is one-to-one.",F \eta : F[x]\to C(F),"Here is the question I am trying to attempt: I feel like there is a typo... it says let $p(x)=a_{n}x^{n}$ . Shouldn't it be $p(x) = a_{n}^{n}\cdots + a_{1}x+a_{0}$ ? I feel like I must use the roots of $p(x)$ in my answer, but I am not sure how and where. Any hints on what I should do? Edit: The duplicate answer did not help me because I am trying to prove injectivity...it has already been assumed that $\eta$ is a ring homomorphism.","Here is the question I am trying to attempt: I feel like there is a typo... it says let . Shouldn't it be ? I feel like I must use the roots of in my answer, but I am not sure how and where. Any hints on what I should do? Edit: The duplicate answer did not help me because I am trying to prove injectivity...it has already been assumed that is a ring homomorphism.",p(x)=a_{n}x^{n} p(x) = a_{n}^{n}\cdots + a_{1}x+a_{0} p(x) \eta,['abstract-algebra']
89,"Different roots of irreducible polynomial $p(x) \in F[x]$ generate isomorphic fields, yet $p(x)$ factors differently in each?","Different roots of irreducible polynomial  generate isomorphic fields, yet  factors differently in each?",p(x) \in F[x] p(x),"This seems counter intuitive to me. For example, the roots of $p(x)=x^3-2$ over $\mathbb{Q}$ are $2^{(1/3)},\gamma2^{(1/3)}, \gamma^22^{(1/3)}$ where $\gamma$ is a primitive third root of unity. So, $\mathbb{Q}(\gamma^22^{(1/3)})$$\cong$$\mathbb{Q}(2^{(1/3)})$ , okay, but something isn't quite clicking for me, and perhaps it's because i want to believe that two roots of $p(x)$ is in $\mathbb{Q}(\gamma^22^{(1/3)})$ when that's not true, I mean if it did have two roots then it would have to have all three and so that's obviously not true. So is it safe to say that in the splitting field generated by different roots of an irreducible polynomial, that irreducible polynomial will split in ""isormorphic"" ways in each respective splitting field? It must, it's just I've been discovering more and more places where i'm learing how ""isomorphic"" does not mean ""equal"" and so maybe that's why i'm so unsure. Sorry, i understand my phrasing is not perfect, but I hope that i'm getting my question across, and if not i hope someone just comes here and gives me general insight into the genereal area.","This seems counter intuitive to me. For example, the roots of over are where is a primitive third root of unity. So, , okay, but something isn't quite clicking for me, and perhaps it's because i want to believe that two roots of is in when that's not true, I mean if it did have two roots then it would have to have all three and so that's obviously not true. So is it safe to say that in the splitting field generated by different roots of an irreducible polynomial, that irreducible polynomial will split in ""isormorphic"" ways in each respective splitting field? It must, it's just I've been discovering more and more places where i'm learing how ""isomorphic"" does not mean ""equal"" and so maybe that's why i'm so unsure. Sorry, i understand my phrasing is not perfect, but I hope that i'm getting my question across, and if not i hope someone just comes here and gives me general insight into the genereal area.","p(x)=x^3-2 \mathbb{Q} 2^{(1/3)},\gamma2^{(1/3)}, \gamma^22^{(1/3)} \gamma \mathbb{Q}(\gamma^22^{(1/3)})\cong\mathbb{Q}(2^{(1/3)}) p(x) \mathbb{Q}(\gamma^22^{(1/3)})","['abstract-algebra', 'field-theory']"
90,Find number of invertible elements in $\mathbb{Z}[i]/(220+55i)\mathbb{Z}[i]$,Find number of invertible elements in,\mathbb{Z}[i]/(220+55i)\mathbb{Z}[i],"I was able to find the factorization $220+55i=11*(2+i)*(2-i)*(4+i)$ . Also know this famous question Quotient ring of Gaussian integers But how to apply it in this case? I'm confused, please help. Is the answer $$ (11^2-1)*(5-1)*(5-1)*(17-1) $$ correct? Why?","I was able to find the factorization . Also know this famous question Quotient ring of Gaussian integers But how to apply it in this case? I'm confused, please help. Is the answer correct? Why?","220+55i=11*(2+i)*(2-i)*(4+i) 
(11^2-1)*(5-1)*(5-1)*(17-1)
","['abstract-algebra', 'number-theory', 'factoring', 'quotient-group', 'gaussian-integers']"
91,Is it possible to prove that $M$ is an integer with $p+M \over x$ is always an integer?,Is it possible to prove that  is an integer with  is always an integer?,M p+M \over x,"Given a prime number $p$ and a set $S$ of $n$ rational numbers . Multiply all $n$ rational numbers we get a number $M$ . For each number $x$ in the set $S$ , we have $\frac{p+M}{x}$ is an integer . Is it possible to prove that $M$ is also an integer ? For $n = 2$ , let $S=\left\{x,y\right\}$ . Then $\frac{p+M}{x}=\frac{p}{x}+y$ , $\frac{p+M}{y}=\frac{p}{y}+x$ are integers, then multiply the two numbers we have $\frac{p^2}{M}+2p + M$ is an integer, thus $\frac{p^2}{M} + M=\alpha$ is an integer. If $M=\frac{A}{B}$ with $A$ , $B$ are coprime, then $B^2\times p^2+A^2=\alpha A B$ so $B|A^2$ then $B=1$ , thus $M$ is an integer. Is it true that for every $n>2$ , $M$ is an integer? If not, then what are the conditions of $n$ so that $M$ must be an integer? Edit: If $M$ is an integer, then is $M$ a power of $p$ ?","Given a prime number and a set of rational numbers . Multiply all rational numbers we get a number . For each number in the set , we have is an integer . Is it possible to prove that is also an integer ? For , let . Then , are integers, then multiply the two numbers we have is an integer, thus is an integer. If with , are coprime, then so then , thus is an integer. Is it true that for every , is an integer? If not, then what are the conditions of so that must be an integer? Edit: If is an integer, then is a power of ?","p S n n M x S \frac{p+M}{x} M n = 2 S=\left\{x,y\right\} \frac{p+M}{x}=\frac{p}{x}+y \frac{p+M}{y}=\frac{p}{y}+x \frac{p^2}{M}+2p + M \frac{p^2}{M} + M=\alpha M=\frac{A}{B} A B B^2\times p^2+A^2=\alpha A B B|A^2 B=1 M n>2 M n M M M p","['abstract-algebra', 'combinatorics', 'geometry', 'number-theory']"
92,On Abelian group product of a Free and a Finite group,On Abelian group product of a Free and a Finite group,,"I'd need help with this problem: Let $H$ be an abelian group. Let $T \leq H$ and $T' \leq H$ be finite subgroups of H. Let $F \leq H$ and $F' \leq H$ be free subgroups of H. Suppose $H = T \times F = T' \times F'$. Show that $T = T'$. Does the same conclusion hold for $F$? I don't really know how to approach the problem. I feel like I should use the Theorem of Classification of finitely generated abelian groups, but H isn't finitely generated. Thanks in advance for any help!","I'd need help with this problem: Let $H$ be an abelian group. Let $T \leq H$ and $T' \leq H$ be finite subgroups of H. Let $F \leq H$ and $F' \leq H$ be free subgroups of H. Suppose $H = T \times F = T' \times F'$. Show that $T = T'$. Does the same conclusion hold for $F$? I don't really know how to approach the problem. I feel like I should use the Theorem of Classification of finitely generated abelian groups, but H isn't finitely generated. Thanks in advance for any help!",,"['abstract-algebra', 'group-theory', 'abelian-groups', 'products']"
93,Coefficients such that linear combination lies in an ideal,Coefficients such that linear combination lies in an ideal,,"Let $R$ be a ring, $I$ an ideal, and $\langle g_1, \ldots, g_m \rangle$ a finitely generated ideal. Considering the intersection $I \cap \langle g_1, \ldots, g_m \rangle$, I became interested in the set of coefficients $c \in R^m$ such that $\sum_i c_i g_i \in I$. Question : is there an algorithm to calculate $\{ c \in R^m : \sum_i c_i g_i \in I \}$? It seems to be an $R$-module. It is easy to see that $\prod_i (I : (g_i))$ is contained in it (which contains $IR^m$). Does it have a name? Can it be expressed in terms of some other objects which can be computed? I am interested in the case $R = k[x_1,\ldots,x_n]$ where $k$ is a field, and (so) the ideal $I$ is also finitely generated. Maybe a Groebner basis of the intersection could help. Solution attempt : Suppose $I \cap \langle g_1, \ldots, g_m \rangle$ is finitely generated (e.g. $R$ is Noetherian). Say it is equal to $\langle h_1, \ldots, h_t \rangle$, and express $h_s = \sum_i b_{s,i} g_i$ with $b_{s,i} \in R$. Then $b_s = (b_{s,i}) \in R^m$ is contained in the set I'm interested in, for each $s=1,\ldots,t$. So the span $Rb_1+ \ldots+ Rb_t \subset R^m$ is contained in the set I'm interested in. Now the question is whether a different choice of basis for the intersection (or a different choice of coefficients) can give more elements in the span, and whether $Rb_1 + \ldots + Rb_t$ exhausts the set I'm interested in.","Let $R$ be a ring, $I$ an ideal, and $\langle g_1, \ldots, g_m \rangle$ a finitely generated ideal. Considering the intersection $I \cap \langle g_1, \ldots, g_m \rangle$, I became interested in the set of coefficients $c \in R^m$ such that $\sum_i c_i g_i \in I$. Question : is there an algorithm to calculate $\{ c \in R^m : \sum_i c_i g_i \in I \}$? It seems to be an $R$-module. It is easy to see that $\prod_i (I : (g_i))$ is contained in it (which contains $IR^m$). Does it have a name? Can it be expressed in terms of some other objects which can be computed? I am interested in the case $R = k[x_1,\ldots,x_n]$ where $k$ is a field, and (so) the ideal $I$ is also finitely generated. Maybe a Groebner basis of the intersection could help. Solution attempt : Suppose $I \cap \langle g_1, \ldots, g_m \rangle$ is finitely generated (e.g. $R$ is Noetherian). Say it is equal to $\langle h_1, \ldots, h_t \rangle$, and express $h_s = \sum_i b_{s,i} g_i$ with $b_{s,i} \in R$. Then $b_s = (b_{s,i}) \in R^m$ is contained in the set I'm interested in, for each $s=1,\ldots,t$. So the span $Rb_1+ \ldots+ Rb_t \subset R^m$ is contained in the set I'm interested in. Now the question is whether a different choice of basis for the intersection (or a different choice of coefficients) can give more elements in the span, and whether $Rb_1 + \ldots + Rb_t$ exhausts the set I'm interested in.",,"['abstract-algebra', 'ring-theory', 'commutative-algebra', 'ideals', 'groebner-basis']"
94,Determining the structure of a finite ring,Determining the structure of a finite ring,,"I'm working on the following problem and would like some guidance $R$ is a finite ring with $x^5 = x$ for every $x$ in $R$.  Determine the structure of $R$. My first thoughts were to factor and look at cases whether $R$ has zero divisors. If $R$ has no zero divisors, then $x^4 = 1$ for nonzero $x$, and so $R$ is a field, namely $\mathbb{Z}/5$ Then you could look at rings of the form $\mathbb{Z}/n$ and focus on the multiplicative group.  But this does not guarantee you'll find all commutative rings and it does not include non-commutative rings. So I think you have to use some ideas with the jacobson radical and semisimplicity. Source: Fall 1996","I'm working on the following problem and would like some guidance $R$ is a finite ring with $x^5 = x$ for every $x$ in $R$.  Determine the structure of $R$. My first thoughts were to factor and look at cases whether $R$ has zero divisors. If $R$ has no zero divisors, then $x^4 = 1$ for nonzero $x$, and so $R$ is a field, namely $\mathbb{Z}/5$ Then you could look at rings of the form $\mathbb{Z}/n$ and focus on the multiplicative group.  But this does not guarantee you'll find all commutative rings and it does not include non-commutative rings. So I think you have to use some ideas with the jacobson radical and semisimplicity. Source: Fall 1996",,"['abstract-algebra', 'ring-theory', 'noncommutative-algebra']"
95,Is $\Bbb Z[x]/(x^3-1)$ a principal ideal ring?,Is  a principal ideal ring?,\Bbb Z[x]/(x^3-1),Is every ideal in $\Bbb Z[x]/(x^3-1)$ principal? Edit: $\Bbb Z[x]/(x^3-1)$ is not isomorphic to $\Bbb Z[x]/(x-1) \oplus \Bbb Z[x]/(x^2+x+1)$  (by a modulo $3$ argument),Is every ideal in $\Bbb Z[x]/(x^3-1)$ principal? Edit: $\Bbb Z[x]/(x^3-1)$ is not isomorphic to $\Bbb Z[x]/(x-1) \oplus \Bbb Z[x]/(x^2+x+1)$  (by a modulo $3$ argument),,['abstract-algebra']
96,Why is the isomorphism of Artin-Wedderburn a ring isomorphism?,Why is the isomorphism of Artin-Wedderburn a ring isomorphism?,,"I'm somewhat confused on the following calculation in the proof of Artin-Wedderburn: let $R$ be a semisimple ring such that (since $R$ is f.g. over itself) we have $R \simeq \oplus_{i = 1}^r S_i^{n_i}$. Now, $$ R^{op} \stackrel{(1)}\simeq End_R(R) \simeq End_R(\oplus_{i = 1}^r S_i^{n_i}) \stackrel{(2)}\simeq \bigoplus_{i = i}^rEnd_R(S_i)^{n_i \times n_i}  $$ Now, I know that $(1)$ is a ring isomorphism between $R^{op}$ and the left $R$-module morphisms from $R$ to istelf, but why is $(2)$ a ring isomorphism? I know we have $Z(R)$-module isomorphism between $Hom_R(M \oplus N,P)$ and $Hom_R(M,P) \oplus Hom_R(N,P)$, so $(2)$ could be stated as a module isomorphism, but however $(1)$ is a ring homomorphism so it would not make sense in this context.","I'm somewhat confused on the following calculation in the proof of Artin-Wedderburn: let $R$ be a semisimple ring such that (since $R$ is f.g. over itself) we have $R \simeq \oplus_{i = 1}^r S_i^{n_i}$. Now, $$ R^{op} \stackrel{(1)}\simeq End_R(R) \simeq End_R(\oplus_{i = 1}^r S_i^{n_i}) \stackrel{(2)}\simeq \bigoplus_{i = i}^rEnd_R(S_i)^{n_i \times n_i}  $$ Now, I know that $(1)$ is a ring isomorphism between $R^{op}$ and the left $R$-module morphisms from $R$ to istelf, but why is $(2)$ a ring isomorphism? I know we have $Z(R)$-module isomorphism between $Hom_R(M \oplus N,P)$ and $Hom_R(M,P) \oplus Hom_R(N,P)$, so $(2)$ could be stated as a module isomorphism, but however $(1)$ is a ring homomorphism so it would not make sense in this context.",,"['abstract-algebra', 'ring-theory', 'modules']"
97,Definition of a primitive representation,Definition of a primitive representation,,"In this paper , a representation $\pi: G \to \operatorname{GL}(V)$  is said to be primitive if $\pi$ is irreducible and there exists no decomposition of $V$ as the direct sum of proper non-zero subspaces permuted by G. Could you please describe what 2. technically means? It would be helpful if you could write the condition as a formula. Right now, this just sounds like 1. where there is no proper non-zero subspace which is invariant under $G$. I thought being permuted by $G$ and being invariant under $G$ is the same. Thank you!","In this paper , a representation $\pi: G \to \operatorname{GL}(V)$  is said to be primitive if $\pi$ is irreducible and there exists no decomposition of $V$ as the direct sum of proper non-zero subspaces permuted by G. Could you please describe what 2. technically means? It would be helpful if you could write the condition as a formula. Right now, this just sounds like 1. where there is no proper non-zero subspace which is invariant under $G$. I thought being permuted by $G$ and being invariant under $G$ is the same. Thank you!",,"['abstract-algebra', 'representation-theory', 'terminology']"
98,Multiplicative group of $p$-adic numbers $\mathbb{Q}_p$,Multiplicative group of -adic numbers,p \mathbb{Q}_p,"$\DeclareMathOperator\Ker{Ker}\DeclareMathOperator\U{U}$I'm reading Serre's text, ""A course in arithmetic"". I have some problems with this argument. Let $\U=\mathbb{Z}_p^*$ the group of $p$-adic units. For every $n \le 1$ let $\U_n=1+p^n\mathbb{Z}_p$. I don't understand how to prove that $\U_n=\Ker(f_n)$ with $f_n:\U \to (\mathbb{Z}/p^n\mathbb{Z})^*$ morphism. In particular, I see that $\U_n \subseteq\Ker(f_n)$ but not the inverse direction. Then Serre says that the map $(1+p^nx) \to x \pmod p$ from $\U_n/\U_{n+1}$ to $\mathbb{Z}/p\mathbb{Z}$ is an isomorphism because$(1+p^nx)(1+p^ny)=1+p^n(x+y)$ modulo $p^{n+1}$. Also this point isn't clear to me. Is there anybody who has some suggestions? Thanks!","$\DeclareMathOperator\Ker{Ker}\DeclareMathOperator\U{U}$I'm reading Serre's text, ""A course in arithmetic"". I have some problems with this argument. Let $\U=\mathbb{Z}_p^*$ the group of $p$-adic units. For every $n \le 1$ let $\U_n=1+p^n\mathbb{Z}_p$. I don't understand how to prove that $\U_n=\Ker(f_n)$ with $f_n:\U \to (\mathbb{Z}/p^n\mathbb{Z})^*$ morphism. In particular, I see that $\U_n \subseteq\Ker(f_n)$ but not the inverse direction. Then Serre says that the map $(1+p^nx) \to x \pmod p$ from $\U_n/\U_{n+1}$ to $\mathbb{Z}/p\mathbb{Z}$ is an isomorphism because$(1+p^nx)(1+p^ny)=1+p^n(x+y)$ modulo $p^{n+1}$. Also this point isn't clear to me. Is there anybody who has some suggestions? Thanks!",,"['abstract-algebra', 'p-adic-number-theory']"
99,On the nilpotency class of certain wreath products,On the nilpotency class of certain wreath products,,"It is said in a paper of P. Hall that if you have a wreath product $S=A\wr B$ where $A$ is a cyclic group of order $p^r$ and $B$ is a cyclic group of order $p^s$, then the nilpotency class of $S$ is as follows: $$rp^s-(r-1)p^{s-1}.$$ He says that this can be directly established, however he does not say how a part from the easy case of $s=1$. Any ideas how to deduce it directly?","It is said in a paper of P. Hall that if you have a wreath product $S=A\wr B$ where $A$ is a cyclic group of order $p^r$ and $B$ is a cyclic group of order $p^s$, then the nilpotency class of $S$ is as follows: $$rp^s-(r-1)p^{s-1}.$$ He says that this can be directly established, however he does not say how a part from the easy case of $s=1$. Any ideas how to deduce it directly?",,"['abstract-algebra', 'group-theory']"
