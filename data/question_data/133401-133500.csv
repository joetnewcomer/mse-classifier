,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,Does this ODE have an exact or well-established approximate analytical solution?,Does this ODE have an exact or well-established approximate analytical solution?,,"The equation looks like this: $$\frac{\mathrm{d}y}{\mathrm{d}t} = A + B\sin\omega t - C y^n,$$ where $A$, $B$, $C$ are positive constants, and $n\ge1$ is an integer.  Actually I am mainly concerned with the $n=4$ case. The $n=1$ case is trivial.  Otherwise the only method I can think of is kind of an iterative approximation, in which a ""clean"" expression seems not easy to obtain. Just in case it is too ""easy"" for the experts, we may generalize it to the form $$\frac{\mathrm{d}y}{\mathrm{d}t} = f(t) - g(y).$$","The equation looks like this: $$\frac{\mathrm{d}y}{\mathrm{d}t} = A + B\sin\omega t - C y^n,$$ where $A$, $B$, $C$ are positive constants, and $n\ge1$ is an integer.  Actually I am mainly concerned with the $n=4$ case. The $n=1$ case is trivial.  Otherwise the only method I can think of is kind of an iterative approximation, in which a ""clean"" expression seems not easy to obtain. Just in case it is too ""easy"" for the experts, we may generalize it to the form $$\frac{\mathrm{d}y}{\mathrm{d}t} = f(t) - g(y).$$",,"['analysis', 'ordinary-differential-equations', 'approximation']"
1,Second-order nonlinear ODE with Dirac Delta,Second-order nonlinear ODE with Dirac Delta,,"Can anyone help me with the following differential equation? $$ 2x(t)x''(t) - x'(t)^2 + kx(t)^2\delta(t - a) =0, $$ where $\delta$ represents the Dirac Delta. I tried Mathematica but with no luck. And I really don't know where to start... Can anyone help me? Note: I first thought that such an equation wouldn't be solvable but If I search for a solution of $x(t)x''(t) - x'(t)^2 + kx(t)^2\delta(t - a) =0,$ mathematica can actually find a solution so maybe the above equation can also be solved.","Can anyone help me with the following differential equation? $$ 2x(t)x''(t) - x'(t)^2 + kx(t)^2\delta(t - a) =0, $$ where $\delta$ represents the Dirac Delta. I tried Mathematica but with no luck. And I really don't know where to start... Can anyone help me? Note: I first thought that such an equation wouldn't be solvable but If I search for a solution of $x(t)x''(t) - x'(t)^2 + kx(t)^2\delta(t - a) =0,$ mathematica can actually find a solution so maybe the above equation can also be solved.",,['ordinary-differential-equations']
2,Physics notation justified,Physics notation justified,,"Sometimes in physics they do things like this one: If $dq=f\left(x\right)\cdot dr$  then  $\frac{dq}{dt}=f\left(x\right)\cdot \frac{dr}{dt}$ Which mathematically is a wrong deduction. Is there any way to justify the required steps to make it a rigorous deduction? -for example using the inverse function theorem or so- Thanks a lot pals, Karan","Sometimes in physics they do things like this one: If $dq=f\left(x\right)\cdot dr$  then  $\frac{dq}{dt}=f\left(x\right)\cdot \frac{dr}{dt}$ Which mathematically is a wrong deduction. Is there any way to justify the required steps to make it a rigorous deduction? -for example using the inverse function theorem or so- Thanks a lot pals, Karan",,"['calculus', 'ordinary-differential-equations', 'physics', 'manifolds', 'mathematical-physics']"
3,Solution to a first order differential equation,Solution to a first order differential equation,,"I am solving a differential equation  $\displaystyle \frac {dy}{dt} =\frac {y+1}{t+1}$. I got the solution $y=c(t+1)-1$, $c$ a constant. But the handout by my professor says ""The solution is $y=c(t+1)-1$, $c$ a constant. where $t \neq -1$. But it does not mean that $y$ is not defined at $t \neq -1$. It means that $y$ can take any value at $t=-1$ as long as it satisfies $y=c(t+1)-1$."" It does not make much sense to me because if $y$ satisfies the equation $y=c(t+1)-1$ at $t=-1$, then the solution will be just $y=c(t+1)-1$, $c$ a constant. And I believe my solution indeed makes sense because $y=c(t+1)-1$, $c$ a constant is defined on $ \mathbb R$ and differentiable, $y'=c$ for any $t \in \mathbb R$. On the other hand, plugging this in to $\displaystyle \frac {y+1}{t+1}$ gives me $c$ for any $t \in \mathbb R$. Would you explain what the handout is saying?","I am solving a differential equation  $\displaystyle \frac {dy}{dt} =\frac {y+1}{t+1}$. I got the solution $y=c(t+1)-1$, $c$ a constant. But the handout by my professor says ""The solution is $y=c(t+1)-1$, $c$ a constant. where $t \neq -1$. But it does not mean that $y$ is not defined at $t \neq -1$. It means that $y$ can take any value at $t=-1$ as long as it satisfies $y=c(t+1)-1$."" It does not make much sense to me because if $y$ satisfies the equation $y=c(t+1)-1$ at $t=-1$, then the solution will be just $y=c(t+1)-1$, $c$ a constant. And I believe my solution indeed makes sense because $y=c(t+1)-1$, $c$ a constant is defined on $ \mathbb R$ and differentiable, $y'=c$ for any $t \in \mathbb R$. On the other hand, plugging this in to $\displaystyle \frac {y+1}{t+1}$ gives me $c$ for any $t \in \mathbb R$. Would you explain what the handout is saying?",,['ordinary-differential-equations']
4,"Solutions to Differential Equations - real, real repeating, and imaginary (for economics)","Solutions to Differential Equations - real, real repeating, and imaginary (for economics)",,"This is kind of a rambling question since I don't know how to ask it clearly. I'm doing some reading on modeling Supply and Demand from economics using a second order differential equation ( http://espin086.wordpress.com/2009/11/26/the-second-order-differential-equations-of-dynamic-market-equilibrium/ ). It's been a while since I've studied this (the DE part) and I have some questions. This article describes their solution to a differential equation that models price as having 3 solutions: A distinct real solution A repeating real solution, and A complex conjugate. I presume that if one wanted to use the solutions to the differential equations and calculate expected prices, one would use the distinct real solution. What's tripping me up though is the repeating real solution. Having plotted it (using graphsketch.com), I don't really see a repeating component in what they describe ($e^{-x}+x*e^{-x}$ is a generalization of the solution). Logically I don't understand how this fits in to a supply and demand context. Is it something that can be outright ignored? It is a solution after all so that would lead me to day no. The complex conjugate I presume can be discarded entirely since you can't have imaginary money. I remember from my electrical engineering days that imaginary components were very important in calculating magnitudes and this is adding to the confusion. Any help is appreciated. mj","This is kind of a rambling question since I don't know how to ask it clearly. I'm doing some reading on modeling Supply and Demand from economics using a second order differential equation ( http://espin086.wordpress.com/2009/11/26/the-second-order-differential-equations-of-dynamic-market-equilibrium/ ). It's been a while since I've studied this (the DE part) and I have some questions. This article describes their solution to a differential equation that models price as having 3 solutions: A distinct real solution A repeating real solution, and A complex conjugate. I presume that if one wanted to use the solutions to the differential equations and calculate expected prices, one would use the distinct real solution. What's tripping me up though is the repeating real solution. Having plotted it (using graphsketch.com), I don't really see a repeating component in what they describe ($e^{-x}+x*e^{-x}$ is a generalization of the solution). Logically I don't understand how this fits in to a supply and demand context. Is it something that can be outright ignored? It is a solution after all so that would lead me to day no. The complex conjugate I presume can be discarded entirely since you can't have imaginary money. I remember from my electrical engineering days that imaginary components were very important in calculating magnitudes and this is adding to the confusion. Any help is appreciated. mj",,['ordinary-differential-equations']
5,Finding a weighting function (Sturm-Liouville problem),Finding a weighting function (Sturm-Liouville problem),,"I have the problem: $$x^2y''+(1/4)y=\lambda y,  1<x<e$$ $$y(1)=y(e)=0$$ And I'm trying to find a weighting function for its solutions (I already calculated the solutions and eigenvalues). If we plug $x=e^t$ we can see the equation becomes: $$Y''-Y'+(1/4-\lambda)Y=0, 0<x<1$$ This seems like something I should be able to transform into Sturm-Liouville form, but since the coefficients of Y'' and $\lambda Y$ are different it doesn't seem feasible. Any suggestions? Thanks!","I have the problem: $$x^2y''+(1/4)y=\lambda y,  1<x<e$$ $$y(1)=y(e)=0$$ And I'm trying to find a weighting function for its solutions (I already calculated the solutions and eigenvalues). If we plug $x=e^t$ we can see the equation becomes: $$Y''-Y'+(1/4-\lambda)Y=0, 0<x<1$$ This seems like something I should be able to transform into Sturm-Liouville form, but since the coefficients of Y'' and $\lambda Y$ are different it doesn't seem feasible. Any suggestions? Thanks!",,['ordinary-differential-equations']
6,General solution of $\frac{d^{2}u}{d\rho^{2}}=\frac{l\left(l+1\right)}{\rho^{2}}u$,General solution of,\frac{d^{2}u}{d\rho^{2}}=\frac{l\left(l+1\right)}{\rho^{2}}u,"In his discussion of the radial wave function of hydrogen Griffiths ( Introduction to Quantum Mechanics, 2nd ed, p.146) gives the general solution of$$\frac{d^{2}u}{d\rho^{2}}=\frac{l\left(l+1\right)}{\rho^{2}}u$$   as$$u\left(\rho\right)=C\rho^{l+1}+D\rho^{-l}.$$   Why is this? There's not even a $u$   on the right-hand side! http://physicspages.com/2011/06/06/hydrogen-atom-radial-equation/ says this general solution can be verified by direct substitution, ie $$\frac{d^{2}u}{d\rho^{2}}=Cl\left(l+1\right)\rho^{l-1}+D\left(-l\right)\left(-l-1\right)\rho^{-l-2}=\frac{l\left(l+1\right)}{\rho^{2}}u.$$ I've fiddled around with this all afternoon (again there's no $u$   in the second expression, which I find ultra-confusing) but can't see how these expressions are equal.","In his discussion of the radial wave function of hydrogen Griffiths ( Introduction to Quantum Mechanics, 2nd ed, p.146) gives the general solution of$$\frac{d^{2}u}{d\rho^{2}}=\frac{l\left(l+1\right)}{\rho^{2}}u$$   as$$u\left(\rho\right)=C\rho^{l+1}+D\rho^{-l}.$$   Why is this? There's not even a $u$   on the right-hand side! http://physicspages.com/2011/06/06/hydrogen-atom-radial-equation/ says this general solution can be verified by direct substitution, ie $$\frac{d^{2}u}{d\rho^{2}}=Cl\left(l+1\right)\rho^{l-1}+D\left(-l\right)\left(-l-1\right)\rho^{-l-2}=\frac{l\left(l+1\right)}{\rho^{2}}u.$$ I've fiddled around with this all afternoon (again there's no $u$   in the second expression, which I find ultra-confusing) but can't see how these expressions are equal.",,"['quantum-mechanics', 'ordinary-differential-equations']"
7,Solution to the ODE $\qquad2\frac{\mathrm dI(a)}{\mathrm da}+aI(a)=0$,Solution to the ODE,\qquad2\frac{\mathrm dI(a)}{\mathrm da}+aI(a)=0,"I am working on this question Show that the function $$I(a)=\int\limits_0^\infty e^{-u^2}\cos(au)\,\mathrm du $$ satisfies the differential equation $$2\frac{\mathrm dI(a)}{\mathrm da}+aI(a)=0$$ Hence find an expression for $I(a)$ This is my working so far $$\begin{align*} \frac{\mathrm dI(a)}{\mathrm da}&=\int\limits_0^\infty \frac{\partial}{\partial a}e^{-u^2}\cos(au)\,\mathrm du\\ &=\int\limits_0^\infty -ue^{-u^2}\sin(au)\,\mathrm du\\ \\ \end{align*}$$ $$\begin{align*} 2\frac{\mathrm dI(a)}{\mathrm da}+aI(a) &=\int\limits_0^\infty -2ue^{-u^2}\sin(au)\,\mathrm du+\int\limits_0^\infty ae^{-u^2}\cos(au)\,\mathrm du\\ &=\int\limits_0^\infty\frac{\mathrm d }{\mathrm d u}\left(e^{-u^2}\sin(au)\right)\mathrm du\\ &=\left.e^{-u^2}\sin(au)\right|_0^\infty\\ &=0 \end{align*} $$ $$\begin{align*} I(0)&=\int\limits_0^\infty e^{-u^2}\,\mathrm du\\ \text{let }u=v^{1/2}\\ \mathrm du=\frac{v^{-1/2}}2\,\mathrm dv\\ &=\int\limits_0^\infty e^{-v}\frac{v^{-1/2}\,\mathrm dv}2\\ &=\frac12\int\limits_0^\infty {v^{-1/2}e^{-v}\,\mathrm dv}\\ &=\frac{\Gamma(\tfrac12)}2\\ &=\frac{\sqrt\pi}2 \end{align*}$$ I'm not sure what the question means by ""find an expression for $I(a)$"" How should I proceed?","I am working on this question Show that the function $$I(a)=\int\limits_0^\infty e^{-u^2}\cos(au)\,\mathrm du $$ satisfies the differential equation $$2\frac{\mathrm dI(a)}{\mathrm da}+aI(a)=0$$ Hence find an expression for $I(a)$ This is my working so far $$\begin{align*} \frac{\mathrm dI(a)}{\mathrm da}&=\int\limits_0^\infty \frac{\partial}{\partial a}e^{-u^2}\cos(au)\,\mathrm du\\ &=\int\limits_0^\infty -ue^{-u^2}\sin(au)\,\mathrm du\\ \\ \end{align*}$$ $$\begin{align*} 2\frac{\mathrm dI(a)}{\mathrm da}+aI(a) &=\int\limits_0^\infty -2ue^{-u^2}\sin(au)\,\mathrm du+\int\limits_0^\infty ae^{-u^2}\cos(au)\,\mathrm du\\ &=\int\limits_0^\infty\frac{\mathrm d }{\mathrm d u}\left(e^{-u^2}\sin(au)\right)\mathrm du\\ &=\left.e^{-u^2}\sin(au)\right|_0^\infty\\ &=0 \end{align*} $$ $$\begin{align*} I(0)&=\int\limits_0^\infty e^{-u^2}\,\mathrm du\\ \text{let }u=v^{1/2}\\ \mathrm du=\frac{v^{-1/2}}2\,\mathrm dv\\ &=\int\limits_0^\infty e^{-v}\frac{v^{-1/2}\,\mathrm dv}2\\ &=\frac12\int\limits_0^\infty {v^{-1/2}e^{-v}\,\mathrm dv}\\ &=\frac{\Gamma(\tfrac12)}2\\ &=\frac{\sqrt\pi}2 \end{align*}$$ I'm not sure what the question means by ""find an expression for $I(a)$"" How should I proceed?",,"['ordinary-differential-equations', 'gamma-function']"
8,How do I solve this problem? (Euler's Method),How do I solve this problem? (Euler's Method),,"I have to do this take home test for school before break ends and it is on Slope Fields and Euler's Method. I'm having trouble understanding and/or working with Euler's Method. These are the two problems I am stuck on: 2. $\text{}$ Use Euler's Method with $\Delta x=0.1$ , $\dfrac{dy}{dx}=2x-y$ and $y=0$ when $x=1$ to find the value of $y$ when $x=1.3$ . A) 0.6      B) 0.2      C) 0.4      D) 0.8 3. $\text{}$ Find the first three approximations ( by hand ) $y_1$ , $y_2$ , $y_3$ using Euler's Method for the initial value problem $$\frac{dy}{dx}=1+y,\quad y(0)=1.$$ ( original image ) Thanks for any help or tips in advance!","I have to do this take home test for school before break ends and it is on Slope Fields and Euler's Method. I'm having trouble understanding and/or working with Euler's Method. These are the two problems I am stuck on: 2. Use Euler's Method with , and when to find the value of when . A) 0.6      B) 0.2      C) 0.4      D) 0.8 3. Find the first three approximations ( by hand ) , , using Euler's Method for the initial value problem ( original image ) Thanks for any help or tips in advance!","\text{} \Delta x=0.1 \dfrac{dy}{dx}=2x-y y=0 x=1 y x=1.3 \text{} y_1 y_2 y_3 \frac{dy}{dx}=1+y,\quad y(0)=1.","['calculus', 'ordinary-differential-equations']"
9,What is Gamma density's differential equation?,What is Gamma density's differential equation?,,What is Gamma density's differential equation?,What is Gamma density's differential equation?,,"['ordinary-differential-equations', 'probability-distributions']"
10,How to solve this Ordinary Differential Equation (ODE)?,How to solve this Ordinary Differential Equation (ODE)?,,"By using the method of undetermined coeficiens, find the particular solution $y_p$ for this inhomogenous differential equation. $$y''+y= \sin x + x\cos x$$ I have find the roots which are $\pm i$. and the complementary function $y_c= (C_1 \cos x + C_2\sin x)$ The answer given is : $(x/4)[x\sin x-\cos x]$","By using the method of undetermined coeficiens, find the particular solution $y_p$ for this inhomogenous differential equation. $$y''+y= \sin x + x\cos x$$ I have find the roots which are $\pm i$. and the complementary function $y_c= (C_1 \cos x + C_2\sin x)$ The answer given is : $(x/4)[x\sin x-\cos x]$",,"['calculus', 'ordinary-differential-equations']"
11,Differential operators: elliptic vs strongly elliptic,Differential operators: elliptic vs strongly elliptic,,"This morning a collegue of mine came to me with the following question: does there exist any elliptic operator of order $2m$ with real (variable) coefficients that is not strongly elliptic? After some investigation, I wasn't able to find an answer. I found out some classical examples, but they all use the complex unit somewhere. I recall that $L=\sum_{|\beta|=2m}a_\beta (x) D^\beta$ is called elliptic if $\sum_{|\beta|=2m}a_\beta (x) \xi^{\beta} \neq 0$ whenever $\xi \neq 0$; strongly elliptic if $\sum_{|\beta|=2m}a_\beta (x) \xi^{\beta} \geq C(x) |\xi|^{2m}$ for some $C(x)>0$ and any $\xi$. The coefficients of $L$ may be taken smooth ""enough"" on a bounded domain $\Omega$.","This morning a collegue of mine came to me with the following question: does there exist any elliptic operator of order $2m$ with real (variable) coefficients that is not strongly elliptic? After some investigation, I wasn't able to find an answer. I found out some classical examples, but they all use the complex unit somewhere. I recall that $L=\sum_{|\beta|=2m}a_\beta (x) D^\beta$ is called elliptic if $\sum_{|\beta|=2m}a_\beta (x) \xi^{\beta} \neq 0$ whenever $\xi \neq 0$; strongly elliptic if $\sum_{|\beta|=2m}a_\beta (x) \xi^{\beta} \geq C(x) |\xi|^{2m}$ for some $C(x)>0$ and any $\xi$. The coefficients of $L$ may be taken smooth ""enough"" on a bounded domain $\Omega$.",,"['ordinary-differential-equations', 'operator-theory', 'differential-operators']"
12,Three ball-spring system,Three ball-spring system,,"So here is a crazy  problem for you all. Imagine there is a system of three balls in a line. The first and last balls have a larger mass M and the middle ball is a smaller mass m. Inbetwen the two larger balls and the middle ball are springs with spring constant of k. Now I would like to model this confusing heap of a system and solve the resulting eigenvalue problem . Trouble is, I don't k ow where to start. Normally, I would set up a differential relation, but since none of the balls are connected to walls, I don't know what is moving where? The problem doesn't even mention an external force. Also, what aould the eigenvalue solution to this problem even mean? I can't seem to grasp the meaning behind this problem. Please help.","So here is a crazy  problem for you all. Imagine there is a system of three balls in a line. The first and last balls have a larger mass M and the middle ball is a smaller mass m. Inbetwen the two larger balls and the middle ball are springs with spring constant of k. Now I would like to model this confusing heap of a system and solve the resulting eigenvalue problem . Trouble is, I don't k ow where to start. Normally, I would set up a differential relation, but since none of the balls are connected to walls, I don't know what is moving where? The problem doesn't even mention an external force. Also, what aould the eigenvalue solution to this problem even mean? I can't seem to grasp the meaning behind this problem. Please help.",,"['ordinary-differential-equations', 'physics', 'classical-mechanics']"
13,How to solve a differential equation containing an integral term?,How to solve a differential equation containing an integral term?,,"I'm asking this question on behalf of another person who is not as familiar as me with computers. He has the following problem to solve and unfortunately Mathcad can't solve such type of equations. He seeks help to find the function $u = u(x) = ?$ for $$ \frac{d^2u(x)}{dx^2} =\left[U(x) + \alpha^2\right]u(x) $$ where $$U(x) = \frac{2}{x}\left(\exp\left(-\frac{x}{\lambda}\right)-\int_0^x 4\pi\left(u(z)\right)^2\,dz\right)$$ The parameters $\lambda$ and $\alpha$ are real. Is another tool (i.e. Mathematica) able to solve it ? Do you have another suggestion ?","I'm asking this question on behalf of another person who is not as familiar as me with computers. He has the following problem to solve and unfortunately Mathcad can't solve such type of equations. He seeks help to find the function $u = u(x) = ?$ for $$ \frac{d^2u(x)}{dx^2} =\left[U(x) + \alpha^2\right]u(x) $$ where $$U(x) = \frac{2}{x}\left(\exp\left(-\frac{x}{\lambda}\right)-\int_0^x 4\pi\left(u(z)\right)^2\,dz\right)$$ The parameters $\lambda$ and $\alpha$ are real. Is another tool (i.e. Mathematica) able to solve it ? Do you have another suggestion ?",,"['integration', 'ordinary-differential-equations']"
14,Power series solution for a DE,Power series solution for a DE,,"I am trying to find the general solution of the DE $y''+y=x$ with a Maclaurin series. I know that I could solve this using the Principle of Superposition by finding a particular solution and the general solution to the homogeneous $y''+y=0$ (which yields the solution $y=acosx+bsinx+x$) but I am looking for what I am doing wrong. If we let $y=\sum_{n=0}^{\infty}c_nx^n$ then, by substituting into the original DE we can obtain a recursive relation. After substituting and rearranging I obtained:$$2c_2+c_0+[6c_3+c_1]x+\sum_{n=2}^{\infty}c_{n+1}(n+2)(n+1)+c_n=0+x+\sum_{n=2}^\infty0$$ This gives the recurrence relation $$c_{n+2}=-\frac{c_n}{(n+2)(n+1)}, n>1$$ and the relationships $2c_2+c_0=0$ and $6c_3+c_1=1$ which allows us to solve $c_2$ and $c_3$ in terms of $c_0$ and $c_1$: $c_2=-\frac{c_0}{2}$, $c_3$=$-\frac{c_1-1}{6}$. The solution for the recurrence relation is given by $$c_{2n}=(-1)^n\frac{c_0}{(2n)!}, c_{2n+1}=(-1)^n\frac{c_1-1}{(2n+1)!},n\in\mathbb{Z}^+$$ However, if I substitute these solutions into the assumed form $$\sum_{n=0}^{\infty}c_nx^n=\sum_{n=0}^{\infty}c_{2n}x^{2n}+\sum_{n=0}^{\infty}c_{2n+1}x^{2n+1}$$ we obtain the Maclaurin series for $cosx$ and $sinx$ but no $x$. What happened to the $x$? What did I do wrong?","I am trying to find the general solution of the DE $y''+y=x$ with a Maclaurin series. I know that I could solve this using the Principle of Superposition by finding a particular solution and the general solution to the homogeneous $y''+y=0$ (which yields the solution $y=acosx+bsinx+x$) but I am looking for what I am doing wrong. If we let $y=\sum_{n=0}^{\infty}c_nx^n$ then, by substituting into the original DE we can obtain a recursive relation. After substituting and rearranging I obtained:$$2c_2+c_0+[6c_3+c_1]x+\sum_{n=2}^{\infty}c_{n+1}(n+2)(n+1)+c_n=0+x+\sum_{n=2}^\infty0$$ This gives the recurrence relation $$c_{n+2}=-\frac{c_n}{(n+2)(n+1)}, n>1$$ and the relationships $2c_2+c_0=0$ and $6c_3+c_1=1$ which allows us to solve $c_2$ and $c_3$ in terms of $c_0$ and $c_1$: $c_2=-\frac{c_0}{2}$, $c_3$=$-\frac{c_1-1}{6}$. The solution for the recurrence relation is given by $$c_{2n}=(-1)^n\frac{c_0}{(2n)!}, c_{2n+1}=(-1)^n\frac{c_1-1}{(2n+1)!},n\in\mathbb{Z}^+$$ However, if I substitute these solutions into the assumed form $$\sum_{n=0}^{\infty}c_nx^n=\sum_{n=0}^{\infty}c_{2n}x^{2n}+\sum_{n=0}^{\infty}c_{2n+1}x^{2n+1}$$ we obtain the Maclaurin series for $cosx$ and $sinx$ but no $x$. What happened to the $x$? What did I do wrong?",,['ordinary-differential-equations']
15,"Verify for $f(x,y)$, homogeneous of degree $n$: $xf_x+yf_y=nf$","Verify for , homogeneous of degree :","f(x,y) n xf_x+yf_y=nf","$F(x,y)$ is homogenous of degree n if $f(tx,ty)=t^nf(x,y)$. Verify that $xf_x(x,y)+yf_y(x,y)=nf(x,y)$ $x^2f_{xx}(x,y)+2xyf_{xy}+y^2f_{yy}(x,y)=n(n-1)f(x,y)$ Looks like I need enlightenment again... Hopefully, its not another embarrassingly simple thing I missed out in another question What I tried: $f_x(x,y)=t^nf_x(x,y)$ $f_y(x,y)=t^nf_y(x,y)$ $xt^nf_x(x,y) + yt^nf_y(x,y) = t^n(xf_x(x,y) + yf_y(x,y))$ Doesn't look like I am doing the right thing?","$F(x,y)$ is homogenous of degree n if $f(tx,ty)=t^nf(x,y)$. Verify that $xf_x(x,y)+yf_y(x,y)=nf(x,y)$ $x^2f_{xx}(x,y)+2xyf_{xy}+y^2f_{yy}(x,y)=n(n-1)f(x,y)$ Looks like I need enlightenment again... Hopefully, its not another embarrassingly simple thing I missed out in another question What I tried: $f_x(x,y)=t^nf_x(x,y)$ $f_y(x,y)=t^nf_y(x,y)$ $xt^nf_x(x,y) + yt^nf_y(x,y) = t^n(xf_x(x,y) + yf_y(x,y))$ Doesn't look like I am doing the right thing?",,"['ordinary-differential-equations', 'integration']"
16,"$y''=y-y^3$, $y,y'\in L^2(\mathbb R)$",",","y''=y-y^3 y,y'\in L^2(\mathbb R)","I'm currently studying this non linear differential equation $$y''=y-y^3.$$ The assumption are that $y,y'$ are in $L^2(\mathbb R)$ , however no boundary conditions are assigned. I am asked to prove that $\mid y(x)\mid\le \sqrt 2.$ My attempt goes as follows: multiply both sides by $y'$ and integrate to obtain: $$(\clubsuit)\quad\mid y'(x)\mid=\sqrt{2\left(\frac{y^2}{2}-\frac{y^4}{4}+C_1\right)}.$$ Then what is inside the square root must be nonnegative, however I have no hypothesis on $C_1$ since no boundary conditions are provided. Another thing which should be useful is that $y^2\in L^1(\mathbb R)$ and, since $$\int {y^2}=2\int \mid yy'\mid\leq \left(\int y^2\right)^{1/2}\left(\int y'^2\right)^{1/2}< +\infty, $$ Then, by noticing that $$\mid y^2(t)-y^2(0)\mid=\mid\int_0^t (y^2)'\mid< C.$$ Hence $y$ is bounded. But nowhere from here. Hints? Finishing the exercise: As Julian pointed out, from the fact that $y\in L^2(\mathbb R)$ and it is bounded we conclude that $$\int y^4\leq \|y\|_\infty^2\int y^2< \infty,$$ so that $y\in L^4(\mathbb R).$ Squaring and integrating $(\clubsuit)$ one must conclude $C_1=0$ . The constant solutions are $$y=0.$$","I'm currently studying this non linear differential equation The assumption are that are in , however no boundary conditions are assigned. I am asked to prove that My attempt goes as follows: multiply both sides by and integrate to obtain: Then what is inside the square root must be nonnegative, however I have no hypothesis on since no boundary conditions are provided. Another thing which should be useful is that and, since Then, by noticing that Hence is bounded. But nowhere from here. Hints? Finishing the exercise: As Julian pointed out, from the fact that and it is bounded we conclude that so that Squaring and integrating one must conclude . The constant solutions are","y''=y-y^3. y,y' L^2(\mathbb R) \mid y(x)\mid\le \sqrt 2. y' (\clubsuit)\quad\mid y'(x)\mid=\sqrt{2\left(\frac{y^2}{2}-\frac{y^4}{4}+C_1\right)}. C_1 y^2\in L^1(\mathbb R) \int {y^2}=2\int \mid yy'\mid\leq \left(\int y^2\right)^{1/2}\left(\int y'^2\right)^{1/2}< +\infty,  \mid y^2(t)-y^2(0)\mid=\mid\int_0^t (y^2)'\mid< C. y y\in L^2(\mathbb R) \int y^4\leq \|y\|_\infty^2\int y^2< \infty, y\in L^4(\mathbb R). (\clubsuit) C_1=0 y=0.",['ordinary-differential-equations']
17,Using the Lambert W to express a solution of a differential equation.,Using the Lambert W to express a solution of a differential equation.,,I solved a differential equation some time ago and I need to solve for $y$. How can we solve for $y$ using the Lambert W function? $$C_1+x = e^y+Cy$$,I solved a differential equation some time ago and I need to solve for $y$. How can we solve for $y$ using the Lambert W function? $$C_1+x = e^y+Cy$$,,"['ordinary-differential-equations', 'special-functions']"
18,Backward Euler for a system of equations,Backward Euler for a system of equations,,"I have tried to solve a system of equations in the form: $$\eqalign{ {dy_1\over dt }&= y_1 +dt*f(y_1,y_2)\cr  {dy_2\over dt} &= y_2 +dt*g(y_1,y_2) } $$ using different schemes such as forward Euler and backward (implicit) Euler and Runge Kutta order 4. My results seem to be OK for the forward Euler scheme and Runge Kutta above a certain number of time steps (A). But for implicit Euler using A time steps, I get an error after a certain number of time steps (say A-100). I was wondering if anyone had any ideas of what the problem might be. I can't think of anything at all. Thanks! Sorry, the correct equations are: $$\eqalign{ {dy_1\over dt }&= f(y_1,y_2)\cr  {dy_2\over dt} &= g(y_1,y_2) } $$ where $y_1$ and $y_2$ have a size $n$ by $1$ ($n$ an integer greater than 1). I also just realized that if I use much smaller than that of explicit euler my reuslts become OK. I am now thinking it could be because I used forward euler to work out the initial RHS's at the new time step, to calculate the $y_1$ and $y_2$ at the new time steps.","I have tried to solve a system of equations in the form: $$\eqalign{ {dy_1\over dt }&= y_1 +dt*f(y_1,y_2)\cr  {dy_2\over dt} &= y_2 +dt*g(y_1,y_2) } $$ using different schemes such as forward Euler and backward (implicit) Euler and Runge Kutta order 4. My results seem to be OK for the forward Euler scheme and Runge Kutta above a certain number of time steps (A). But for implicit Euler using A time steps, I get an error after a certain number of time steps (say A-100). I was wondering if anyone had any ideas of what the problem might be. I can't think of anything at all. Thanks! Sorry, the correct equations are: $$\eqalign{ {dy_1\over dt }&= f(y_1,y_2)\cr  {dy_2\over dt} &= g(y_1,y_2) } $$ where $y_1$ and $y_2$ have a size $n$ by $1$ ($n$ an integer greater than 1). I also just realized that if I use much smaller than that of explicit euler my reuslts become OK. I am now thinking it could be because I used forward euler to work out the initial RHS's at the new time step, to calculate the $y_1$ and $y_2$ at the new time steps.",,"['ordinary-differential-equations', 'numerical-methods']"
19,separation of variables Solve and prove,separation of variables Solve and prove,,Using separation of variables Solve  $$\frac{dy}{dx} = 3y +1.$$ My final answer I got was: $$y = \pm C \frac{e^x}{3} - \frac{1}{3}.$$ But I don't how to take the derivative of that to get it back to my orig problem,Using separation of variables Solve  $$\frac{dy}{dx} = 3y +1.$$ My final answer I got was: $$y = \pm C \frac{e^x}{3} - \frac{1}{3}.$$ But I don't how to take the derivative of that to get it back to my orig problem,,"['calculus', 'ordinary-differential-equations']"
20,What is the domain of definition for the solution of DE?,What is the domain of definition for the solution of DE?,,"We have the following Differential Equation with the initial condition $$ \frac{dy}{dt}=\frac{1}{(y+1)(y-2)},~~ y(0)=0$$ By seperating the variables i got the general solution as follows $$ \frac{1}{3}y^3-\frac{1}{2}y^2-2y=t+C_1 \implies 2y^3-3y^2 -12y=6t+6C_1$$ (1) What is a domain of definition? (2) And what would be the domain of definition for the solution?","We have the following Differential Equation with the initial condition $$ \frac{dy}{dt}=\frac{1}{(y+1)(y-2)},~~ y(0)=0$$ By seperating the variables i got the general solution as follows $$ \frac{1}{3}y^3-\frac{1}{2}y^2-2y=t+C_1 \implies 2y^3-3y^2 -12y=6t+6C_1$$ (1) What is a domain of definition? (2) And what would be the domain of definition for the solution?",,['ordinary-differential-equations']
21,Negative damping ratio for second order system?,Negative damping ratio for second order system?,,"I was wondering what would you classify the damping state (under damped, over-damped, critically damped) a second-order ode system with a negative damping ratio? To me it doesn't make much sense since a negative damping ratio results in an unstable system. The system I have is: $G(s) = \frac{5}{s^2-6s+16}$ So solving for the natural frequency and damping ratio: $\omega_n = \sqrt{16} = 4$ $2 \zeta \omega_n = -6$ $\zeta = \frac{-6}{2 \omega_n} = -0.75$","I was wondering what would you classify the damping state (under damped, over-damped, critically damped) a second-order ode system with a negative damping ratio? To me it doesn't make much sense since a negative damping ratio results in an unstable system. The system I have is: $G(s) = \frac{5}{s^2-6s+16}$ So solving for the natural frequency and damping ratio: $\omega_n = \sqrt{16} = 4$ $2 \zeta \omega_n = -6$ $\zeta = \frac{-6}{2 \omega_n} = -0.75$",,['ordinary-differential-equations']
22,Minimizing the cost of a path in a dynamic system,Minimizing the cost of a path in a dynamic system,,"So suppose I want a path from 0 to $c>0$ on the real line, and I am going to use the function $S(t)$ to get there in (discrete) time $T$.  That is, my position at time 0 is 0, my position at time $T$ is $c$, and my position $P_t$ changes in the following way: $$ P_t = (1-\gamma) P_{t-1} + \gamma S(t)  $$ where $\gamma<1$ limits fast I can move.  Which can be written $$ \frac{P_t - P_{t-1}}{\gamma} = - P_{t-1} +S(t) $$ Sadly, I have a drift towards zero of $-P$, and $S$ has to drive me away from zero, towards $c$.  So $S(t)$ is the ""size"" of my push towards $c$ at date $t$.  The cost of $S$ is given by $$ C(T,S(t)) = \sum_1^{T} (S(t))^2 $$ So I am penalized for large movements in my position, and big pushes towards $c$.  I would like to calculate  $$ \min_{T,S(\cdot)} C(T,S(t))  $$ subject to $P_0 =0$, $P_T = c$. That is, if I am free to choose the number of steps  it takes to get there, and the path can be whatever I want, what is the cost minimizing route from 0 to $c$? Clearly, you could set $S(1) =  c / \gamma$, so I arrive immediately, at cost $( c/\gamma )^2$.  So that is an upper bound on the cost.  I could do it in two steps;  If I step $x$ the first period,  I will need to step  $$\frac{(c-(1-\gamma) \gamma x)}{\gamma}$$ the second period, for a total cost of of $$ x^2 + (\frac{(c-(1-\gamma) \gamma x)}{\gamma})^2, $$ Which is minimized for $x=\frac{c (1-\gamma)}{\gamma (2-2 \gamma+\gamma^2 )}$, which therefore costs  $$ \frac{c^2 (1-\gamma)^2}{\gamma^2 (2-2 \gamma+\gamma^2)^2}+\frac{(c+\frac{c (1-\gamma) (-1+\gamma)}{2-2 \gamma+\gamma^2})^2}{\gamma^2} $$ which is a lower cost than the one-step option, since $\gamma<1$.  It is not clear to me how to extend this analysis to large $T$ - is there a simple recursive form for the cost minimizing $k$-step path, and the problem reduces to choosing the best $k$?","So suppose I want a path from 0 to $c>0$ on the real line, and I am going to use the function $S(t)$ to get there in (discrete) time $T$.  That is, my position at time 0 is 0, my position at time $T$ is $c$, and my position $P_t$ changes in the following way: $$ P_t = (1-\gamma) P_{t-1} + \gamma S(t)  $$ where $\gamma<1$ limits fast I can move.  Which can be written $$ \frac{P_t - P_{t-1}}{\gamma} = - P_{t-1} +S(t) $$ Sadly, I have a drift towards zero of $-P$, and $S$ has to drive me away from zero, towards $c$.  So $S(t)$ is the ""size"" of my push towards $c$ at date $t$.  The cost of $S$ is given by $$ C(T,S(t)) = \sum_1^{T} (S(t))^2 $$ So I am penalized for large movements in my position, and big pushes towards $c$.  I would like to calculate  $$ \min_{T,S(\cdot)} C(T,S(t))  $$ subject to $P_0 =0$, $P_T = c$. That is, if I am free to choose the number of steps  it takes to get there, and the path can be whatever I want, what is the cost minimizing route from 0 to $c$? Clearly, you could set $S(1) =  c / \gamma$, so I arrive immediately, at cost $( c/\gamma )^2$.  So that is an upper bound on the cost.  I could do it in two steps;  If I step $x$ the first period,  I will need to step  $$\frac{(c-(1-\gamma) \gamma x)}{\gamma}$$ the second period, for a total cost of of $$ x^2 + (\frac{(c-(1-\gamma) \gamma x)}{\gamma})^2, $$ Which is minimized for $x=\frac{c (1-\gamma)}{\gamma (2-2 \gamma+\gamma^2 )}$, which therefore costs  $$ \frac{c^2 (1-\gamma)^2}{\gamma^2 (2-2 \gamma+\gamma^2)^2}+\frac{(c+\frac{c (1-\gamma) (-1+\gamma)}{2-2 \gamma+\gamma^2})^2}{\gamma^2} $$ which is a lower cost than the one-step option, since $\gamma<1$.  It is not clear to me how to extend this analysis to large $T$ - is there a simple recursive form for the cost minimizing $k$-step path, and the problem reduces to choosing the best $k$?",,"['calculus', 'ordinary-differential-equations', 'optimization', 'dynamical-systems']"
23,A differential equation,A differential equation,,"Think of $t$ and $r$ as two independent variables. Suppose $E$ be a function of $r$ and $V~$ be a function of $(t,r)$ such that both go to $0$ at $r=0$. There exists a positive function $M(r)$ such that $M(0)=0$ and $V(t,r) = -\dfrac{M(r)}{R(t,r)}$ where $R$ is another positive function such that $R(0,r)=r$. Let $p(r) = \dfrac{E(r)}{V(0,r)}$ be a function regular at $r=0$ such that $p(0) \in (-\infty,1)$. Also define a function $a$ of $r$ such that, $a(r) = \dfrac{M(r)}{\dfrac{4}{3}\pi r^3}$.  Then $a$ is also a positive definite function with a well-defined value at $r=0$. Define $\alpha = a(0)$ Now look at this differential equation, $$\frac{\dot{R}^2}{2} + V(t,r) = E(r)$$ Apparently this differential equation has a solution of the form, $$\frac{t}{t_0} = \sqrt{\frac{\alpha}{a(r)}}\frac{F(p(r))}{F(p(0))} \left [1 - \left ( \dfrac{R(t,r)}{r} \right)^{\dfrac{3}{2}}~\cdot~\dfrac{F\left(~~ \dfrac{p(r)R(t,r)}{r} \right) }{F(p(r))} \right ] $$ where $t_0 = \sqrt {\dfrac{3}{8\pi \alpha}} F(p(0))$ and the function $F$ is defined over the interval $(-\infty,1)$ as, $$F(x) = \left\{   \begin{array}{c c}    -\frac{\sqrt{1-x}}{x} - \frac{1}{(-x)^{\frac{3}{2}}} \tanh^{-1} \left [ \sqrt{\frac{x}{x-1}} \right ] & x&lt;0 \\      \frac{2}{3}  &  x =0 \\ \frac{1}{x^{\frac{3}{2}}}tan^{-1} \left [ \sqrt{\frac{x}{1-x}} \right ] - \frac{\sqrt{1-x}}{x}  & 0&lt;x&lt;1  \end {array}  \right. $$ How does one get the above solution?","Think of $t$ and $r$ as two independent variables. Suppose $E$ be a function of $r$ and $V~$ be a function of $(t,r)$ such that both go to $0$ at $r=0$. There exists a positive function $M(r)$ such that $M(0)=0$ and $V(t,r) = -\dfrac{M(r)}{R(t,r)}$ where $R$ is another positive function such that $R(0,r)=r$. Let $p(r) = \dfrac{E(r)}{V(0,r)}$ be a function regular at $r=0$ such that $p(0) \in (-\infty,1)$. Also define a function $a$ of $r$ such that, $a(r) = \dfrac{M(r)}{\dfrac{4}{3}\pi r^3}$.  Then $a$ is also a positive definite function with a well-defined value at $r=0$. Define $\alpha = a(0)$ Now look at this differential equation, $$\frac{\dot{R}^2}{2} + V(t,r) = E(r)$$ Apparently this differential equation has a solution of the form, $$\frac{t}{t_0} = \sqrt{\frac{\alpha}{a(r)}}\frac{F(p(r))}{F(p(0))} \left [1 - \left ( \dfrac{R(t,r)}{r} \right)^{\dfrac{3}{2}}~\cdot~\dfrac{F\left(~~ \dfrac{p(r)R(t,r)}{r} \right) }{F(p(r))} \right ] $$ where $t_0 = \sqrt {\dfrac{3}{8\pi \alpha}} F(p(0))$ and the function $F$ is defined over the interval $(-\infty,1)$ as, $$F(x) = \left\{   \begin{array}{c c}    -\frac{\sqrt{1-x}}{x} - \frac{1}{(-x)^{\frac{3}{2}}} \tanh^{-1} \left [ \sqrt{\frac{x}{x-1}} \right ] & x&lt;0 \\      \frac{2}{3}  &  x =0 \\ \frac{1}{x^{\frac{3}{2}}}tan^{-1} \left [ \sqrt{\frac{x}{1-x}} \right ] - \frac{\sqrt{1-x}}{x}  & 0&lt;x&lt;1  \end {array}  \right. $$ How does one get the above solution?",,['ordinary-differential-equations']
24,"If $f > 0$, $\ker(L)$ contains only constant functions, where $L = - \Delta + \nabla (- \log(f)) \cdot \nabla$ (Villani, Subsec. 7.6)","If ,  contains only constant functions, where  (Villani, Subsec. 7.6)",f > 0 \ker(L) L = - \Delta + \nabla (- \log(f)) \cdot \nabla,"In subsection(s) 7.5 (and 4.1) of Topics in Optimal Transportation , Cedric Villani states the following (I paraphrase): Take a probability measure $\mu \in \mathcal P_2(\mathbb R^n)$ with finite second moment and strictly positive density $f$ with respect to the Lebesgue measure on $\mathbb R^n$ . Consider the linear differential operator $$ L := - \Delta + \nabla (- \log(f))\cdot \nabla. $$ Then $L$ satisfies the following integration-by-parts formula with respect to $\mu$ : $$ \int_{\mathbb R^d} (L h_1)(x) h_2(x) \, \text{d}\mu(x) = \int_{\mathbb R^d} (L h_2)(x) h_1(x) \, \text{d}\mu(x)  = \int_{\mathbb R^d} \nabla h_1(x) \cdot \nabla h_2(x) \, \text{d}\mu(x)  $$ for all $h_1, h_2 \colon \mathbb R^d \to \mathbb R$ with certain regularity assumptions. In particular, $L$ is a non-negative self-adjoint operator on the set of those $h$ with respect to the $L^2(\mathbb R^d, d \mu)$ scalar product. Inspired by $\| h \|_{L^2(\mathbb R^d)}^2 = \int_{\mathbb R^d} | h(x) |^2 \, \text{d}x$ and $\| h \|_{\dot{H}^1(\mathbb R^d)}^2 = \int_{\mathbb R^d} \| \nabla h(x) \|_2^2 \, \text{d}x$ we define weighted square ""norms"" $$ \| h \|_{L^2(\mathbb R^d, d \mu)}^2 = \int_{\mathbb R^d} | h(x) |^2 \, \text{d}\mu(x) \quad \text{and} \quad \| h \|_{\dot{H}^1(\mathbb R^d, d\mu)}^2 = \int_{\mathbb R^d} h(x)(L h)(x) \|_2^2 \, \text{d}\mu(x). $$ Since $f > 0$ everywhere, $\ker(L)$ only contains constant functions. Under certain regularity conditions which we prefer not to discuss here, one can define the inverse $L^{-1}$ of $L$ on space of functions $h$ with $\int_{\mathbb R^d} h(x) \, \text{d}\mu(x) = 0$ and then establish \begin{align} \| h \|_{H^{-1}(\mathbb R^d, d \mu)} & := \sup\left\{ \int_{\mathbb R^d} h(x) k(x) \, \text{d}\mu(x): k \in \mathcal{C}_c^{\infty}(\mathbb R^n), \| k \|_{\dot{H}^1(\mathbb R^d; d\mu)} = 1 \right\} \\ & = \int_{\mathbb R^d} h(x) (L^{-1} h)(x) \, \text{d}\mu(x), \end{align} so $H^{-1}(\mathbb R^d, d \mu)$ is dual to $\dot{H}^1(\mathbb R^d, d \mu)$ . My work so far. I don't know how to show that $\ker(L)$ only contains constant functions, partly because I don't know on which space $L$ is defined. If we could take $f = 1$ (we can't since then $\mu$ would not be a probability measure), then $L = - \Delta$ and surely $\ker(L)$ also contains linear functions. If $f(x) = e^{-x^2} \in L^2(\mathbb R)$ up to a normalization constant, then $-\log(f) = x^2$ and thus $L h = - h''(x) + 2 x h'(x)$ and $L h = 0$ implies $h(x) = c_1 \text{erfi}(x) + c_2$ , where erfi is the imaginary error function, according to WolframAlpha, which is not constant. My question. I am wondering I what way "" $\ker(L)$ contains only constant functions"" is meant and how to show it.","In subsection(s) 7.5 (and 4.1) of Topics in Optimal Transportation , Cedric Villani states the following (I paraphrase): Take a probability measure with finite second moment and strictly positive density with respect to the Lebesgue measure on . Consider the linear differential operator Then satisfies the following integration-by-parts formula with respect to : for all with certain regularity assumptions. In particular, is a non-negative self-adjoint operator on the set of those with respect to the scalar product. Inspired by and we define weighted square ""norms"" Since everywhere, only contains constant functions. Under certain regularity conditions which we prefer not to discuss here, one can define the inverse of on space of functions with and then establish so is dual to . My work so far. I don't know how to show that only contains constant functions, partly because I don't know on which space is defined. If we could take (we can't since then would not be a probability measure), then and surely also contains linear functions. If up to a normalization constant, then and thus and implies , where erfi is the imaginary error function, according to WolframAlpha, which is not constant. My question. I am wondering I what way "" contains only constant functions"" is meant and how to show it.","\mu \in \mathcal P_2(\mathbb R^n) f \mathbb R^n 
L := - \Delta + \nabla (- \log(f))\cdot \nabla.
 L \mu 
\int_{\mathbb R^d} (L h_1)(x) h_2(x) \, \text{d}\mu(x)
= \int_{\mathbb R^d} (L h_2)(x) h_1(x) \, \text{d}\mu(x) 
= \int_{\mathbb R^d} \nabla h_1(x) \cdot \nabla h_2(x) \, \text{d}\mu(x) 
 h_1, h_2 \colon \mathbb R^d \to \mathbb R L h L^2(\mathbb R^d, d \mu) \| h \|_{L^2(\mathbb R^d)}^2 = \int_{\mathbb R^d} | h(x) |^2 \, \text{d}x \| h \|_{\dot{H}^1(\mathbb R^d)}^2 = \int_{\mathbb R^d} \| \nabla h(x) \|_2^2 \, \text{d}x 
\| h \|_{L^2(\mathbb R^d, d \mu)}^2 = \int_{\mathbb R^d} | h(x) |^2 \, \text{d}\mu(x)
\quad \text{and} \quad
\| h \|_{\dot{H}^1(\mathbb R^d, d\mu)}^2 = \int_{\mathbb R^d} h(x)(L h)(x) \|_2^2 \, \text{d}\mu(x).
 f > 0 \ker(L) L^{-1} L h \int_{\mathbb R^d} h(x) \, \text{d}\mu(x) = 0 \begin{align}
\| h \|_{H^{-1}(\mathbb R^d, d \mu)}
& := \sup\left\{ \int_{\mathbb R^d} h(x) k(x) \, \text{d}\mu(x): k \in \mathcal{C}_c^{\infty}(\mathbb R^n), \| k \|_{\dot{H}^1(\mathbb R^d; d\mu)} = 1 \right\} \\
& = \int_{\mathbb R^d} h(x) (L^{-1} h)(x) \, \text{d}\mu(x),
\end{align} H^{-1}(\mathbb R^d, d \mu) \dot{H}^1(\mathbb R^d, d \mu) \ker(L) L f = 1 \mu L = - \Delta \ker(L) f(x) = e^{-x^2} \in L^2(\mathbb R) -\log(f) = x^2 L h = - h''(x) + 2 x h'(x) L h = 0 h(x) = c_1 \text{erfi}(x) + c_2 \ker(L)","['ordinary-differential-equations', 'probability-distributions', 'linear-transformations', 'optimal-transport', 'elliptic-operators']"
25,Find the minimum positive value that $(f(x)-g(x)+3)$ may attain.,Find the minimum positive value that  may attain.,(f(x)-g(x)+3),"Let $f:\mathbb R\to \mathbb R$ be two non-constant differentiable functions. If $f'(x)-g'(x)=g'(x)(f(x))^2-f'(x)(g(x))^2$ for all $x\in\mathbb R$ and $f(1)=1, g(1)=\frac13$ and if there is no $x$ for which $f(x)=-2$ or $g(x)=2$ then find the minimum positive value that $(f(x)-g(x)+3)$ may attain. My Attempt: $f'(x)(1+(g(x)^2)=g'(x)(1+(f(x))^2)$ $\frac{f'(x)}{1+(f(x))^2}=\frac{g'(x)}{1+(g(x))^2}$ $\big[\arctan(f(x))\big]_1^x=\big[\arctan(g(x))\big]_1^x$ $\arctan(f(x))-\arctan1=\arctan(g(x))-\arctan\frac13$ $\arctan(f(x))-\arctan(g(x))=\arctan1-\arctan\frac13=\arctan\frac12$ Not sure what to do next. Perhaps the answer is $2√5-1$ .",Let be two non-constant differentiable functions. If for all and and if there is no for which or then find the minimum positive value that may attain. My Attempt: Not sure what to do next. Perhaps the answer is .,"f:\mathbb R\to \mathbb R f'(x)-g'(x)=g'(x)(f(x))^2-f'(x)(g(x))^2 x\in\mathbb R f(1)=1, g(1)=\frac13 x f(x)=-2 g(x)=2 (f(x)-g(x)+3) f'(x)(1+(g(x)^2)=g'(x)(1+(f(x))^2) \frac{f'(x)}{1+(f(x))^2}=\frac{g'(x)}{1+(g(x))^2} \big[\arctan(f(x))\big]_1^x=\big[\arctan(g(x))\big]_1^x \arctan(f(x))-\arctan1=\arctan(g(x))-\arctan\frac13 \arctan(f(x))-\arctan(g(x))=\arctan1-\arctan\frac13=\arctan\frac12 2√5-1","['calculus', 'ordinary-differential-equations', 'contest-math']"
26,Differential equation $f''-f'-f=1$,Differential equation,f''-f'-f=1,"Suppose $f:\Bbb R\rightarrow\Bbb R$ is a solution of $f''-f'-f=1$ and $f(0)=f(k)=0$ for some $k>0$ . Then, $f$ has positive and negative values over $(0,k)$ $f$ has only positive values over $(0,k)$ $f$ has only negative values over $(0,k)$ $f=-1$ on $(0,k)$ I have done a similar ques in which the differential equation was $f''-f'-1=0$ and I thought of it like there would be some $c\in (0,k)$ such that $f'(c)=0$ and at c the differential equation will give $f''(c)=1$ and that will be true for all stationary points and therefore there will be no maxima and we can conclude that the funtion will be below the x axis always. But i was not able to do same for this question.","Suppose is a solution of and for some . Then, has positive and negative values over has only positive values over has only negative values over on I have done a similar ques in which the differential equation was and I thought of it like there would be some such that and at c the differential equation will give and that will be true for all stationary points and therefore there will be no maxima and we can conclude that the funtion will be below the x axis always. But i was not able to do same for this question.","f:\Bbb R\rightarrow\Bbb R f''-f'-f=1 f(0)=f(k)=0 k>0 f (0,k) f (0,k) f (0,k) f=-1 (0,k) f''-f'-1=0 c\in (0,k) f'(c)=0 f''(c)=1","['ordinary-differential-equations', 'derivatives']"
27,Existence of unique solution of initial value problem,Existence of unique solution of initial value problem,,"Let us take initial value problem $y'+\frac{{2}}{{t}}y=4t$ , $y(1)=2$ where $p(t)=\frac{{2}}{{t}}$ and $q(t)=4t$ . Here $q(t)$ is continuous for all $t$ while $p(t)$ is continuous only for $t<0$ or $t>0$ . The interval $t>0$ contains the initial point; consequently theorem guarantees that the initial value problem has unique solution on the interval $0<t<\infty$ , but if the initial condition $y(1)=2$ is changed to $y(-1)=2$ the existence of solution will be on the interval $-\infty<t<0$ containing the initial point. Now my question is that is it necessary to look upon the interval which necessarily must contain initial point for the existence of solution of initial value problem? Is it valid or does there exist solution on those interval which doesn't contain initial point but those coefficients $p(t)$ and $q(t)$ are continuous for all the points contained in those interval?","Let us take initial value problem , where and . Here is continuous for all while is continuous only for or . The interval contains the initial point; consequently theorem guarantees that the initial value problem has unique solution on the interval , but if the initial condition is changed to the existence of solution will be on the interval containing the initial point. Now my question is that is it necessary to look upon the interval which necessarily must contain initial point for the existence of solution of initial value problem? Is it valid or does there exist solution on those interval which doesn't contain initial point but those coefficients and are continuous for all the points contained in those interval?",y'+\frac{{2}}{{t}}y=4t y(1)=2 p(t)=\frac{{2}}{{t}} q(t)=4t q(t) t p(t) t<0 t>0 t>0 0<t<\infty y(1)=2 y(-1)=2 -\infty<t<0 p(t) q(t),"['ordinary-differential-equations', 'initial-value-problems']"
28,Solving ODE Derived from Weird Problem: $\sqrt{k^2-{f'(t)}^2}f(t)-\sqrt{t^2-{f(t)}^2}f'(t)=t\sqrt{k^2-1}$,Solving ODE Derived from Weird Problem:,\sqrt{k^2-{f'(t)}^2}f(t)-\sqrt{t^2-{f(t)}^2}f'(t)=t\sqrt{k^2-1},"$$\large \text{Problem}$$ I'll give the question first and then the motivation. Is there a closed form solution to the differential equation $$\sqrt{k^2-{f'(t)}^2}f(t)-\sqrt{t^2-{f(t)}^2}f'(t)=t\sqrt{k^2-1}$$ with initial value $$f{\left(\frac{1}{\sqrt{k^2-1}}\right)}=\frac{1}{\sqrt{k^2-1}}?$$ What about $$\sqrt{k^2-{g'(t)}^2}(1-g(t))+\sqrt{t^2-{(1-g(t))}^2}g'(t)=t\sqrt{k^2-1}$$ with initial value $$g{\left(\frac{1}{\sqrt{k^2-1}}\right)}=1?$$ Alternatively, can we give the solution to one as a function of the solution to the other? These are increasingly tricky approaches to solving the problem given in the motivation. $$\large \text{Motivation}$$ I have been playing in my spare time with what happens when we have two ""blobs"" in $\mathbb{R}^2$ that radially expand from every point in the blob at a constant speed, starting as two points, but stop expanding anywhere they come into contact. More formally, we have sets $A_t$ and $B_t$ indexed by time $t\geq0$ such that $A_0=(0,0)$ and $B_0(1,0)$ . For all $x\in\mathbb{R}^2$ and times $s>t$ , if there exists a path from $A_t$ to $x$ of length $k(s-t)$ that does not intersect $B_r$ for any $r<s$ , the $x\in A_s$ . For all $x\in\mathbb{R}^2$ and times $s>t$ , if there exists a path from $A_t$ to $x$ of length $s-t$ that does not intersect $B_r$ for any $r<s$ , the $x\in A_s$ . The difference being the appearance of $k$ , the speed of $A_t$ 's expansion relative to that of $B_t$ . We suppose that $k>1$ , without loss of generality. For all $x\in A_s$ , there must be a path $\gamma(t)$ to $x$ such that $\gamma(t)$ 's length changes at a rate of at most $k$ , and for all $t<s$ , $\gamma(t)\in A_t\setminus B_t$ . Similarly for $B_s$ , but we change the rate of growth from $k$ to $1$ . For a point $x\in\mathbb{R}^2$ , if there exists $t$ such that $x\in A_t\cap B_t$ , then for all times $s$ , if $x\in A_s$ , then $x\in B_s$ . These conditions uniquely define $A_t$ and $B_t$ . I am interested in the final shapes of $A=\bigcup_{t\geq0}A_t$ , and $B=\bigcup_{t\geq0}B_t$ . We can show that $B$ is a closed set, and $A$ is the closure of its complement, so we can obviously just focus on $B$ . I know that $B$ is convex and bounded, and its boundary can be split into three curves. The first is easy. We consider the $x$ such that the distance from $x$ to $(0,0)$ is $k$ times the distance from $x$ to $(1,0)$ . If there is no issue with the straight line path between $x$ and $(0,0)$ intersecting with $B_t$ , or the straight line path between $x$ and $(1,0)$ intersecting with $A_t$ , then the paths clearly belong to $A$ and $B$ respectively, and they meet at these boundary points. The set of $x$ satisfying this distance equation is a certain circle, with $(1,0)$ on the inside and $(0,0)$ on the outside. Things are okay until the path from $(0,0)$ becomes tangent to the circle, at which point it's no longer obvious that it avoids $B_t$ , and indeed, it does not. There are two points on the circle which meet these tangent paths. These are the beginnings of the other two curves, which by symmetry, will obviously be mirror images in the line $y=0$ . So we might as well focus on the top. We can show that what needs to happen is that as we trace along the curve at speed $k$ , we just meet up with the expansion of $A_t$ . Now, we can show without much difficulty that $B_t$ is only ever going to expand via straight lines from $(1,0)$ . These lines are going to be growing radially from the points on the boundary of a circle of radius $t$ centred at $(1,0)$ which have not already been obstructed by the growth of $A_t$ . To get a little heuristic, locally, at a point $c(t)$ on the curve, we're going to be meeting an expanding wall of growth at unit speed, going up in parallel lines. What kind of motion is going to beat the wall? Well, we'll be moving at speed $k$ , so we need to pick an angle such that the movement in the direction of the wall is at unit speed. This is actually already satisfied by the derivative of the first curve at the starting point of $c$ . The curves will stop when they run into each other somewhere along $y=0$ . $$\text{To summarise}$$ the magnitude of $c$ 's derivative is $k$ . $c$ is always on a point of the circle of radius $t$ centred at $(1,0)$ . $c$ 's derivative forms a constant angle with the tangent line to this circle, which we can calculate from the initial value. We can also calculate the initial value of $t$ , where $A_t$ and $B_t$ no longer grow in lines colliding with each other, and the curve starts. Writing out the coordinates of $c$ and its derivative, and putting all this into the language of inner products, we find that we can solve for the $x$ -coordinates from the $y$ -coordinates and $t$ and vice versa. We then put the equations together to get the $y$ coordinate given by the function $f(t)$ above. $g(t)$ gives the $x$ coordinate. $$\text{EDIT}$$ It's been pointed out to me that the properties defining my differential equation are satisfied by logarithmic spirals (by the differential equation, this means that logarithmic spirals with given initial conditions uniquely satisfy them). Logarithmic spirals are however given in polar coordinates, so I suppose my original question becomes one of expressing a part of a logarithmic spiral as a closed-form Cartesian function. But I can't find this, so I suspect that there is none. Anyway, as far as characterising the set goes I'm satisfied. It's more natural to shift everything to the left by $1$ . Then the boundary is given by these curves: The part of circle with centre $\left(\frac{1}{k^2-1},0\right)$ and radius $\frac{k}{k^2-1}$ to the left of the $y$ -axis, The polar curve $r=\frac{1}{\sqrt{k^2-1}}e^\frac{2\theta-3\pi}{2\sqrt{k^2-1}}$ from $\theta=\frac{3\pi}{2}$ to $2\pi$ , and The reflection of that curve in the $x$ -axis. Here is an obligatory picture of boundaries for a family of $B$ 's for different $k$ 's, which is certainly what I wanted out of this.","I'll give the question first and then the motivation. Is there a closed form solution to the differential equation with initial value What about with initial value Alternatively, can we give the solution to one as a function of the solution to the other? These are increasingly tricky approaches to solving the problem given in the motivation. I have been playing in my spare time with what happens when we have two ""blobs"" in that radially expand from every point in the blob at a constant speed, starting as two points, but stop expanding anywhere they come into contact. More formally, we have sets and indexed by time such that and . For all and times , if there exists a path from to of length that does not intersect for any , the . For all and times , if there exists a path from to of length that does not intersect for any , the . The difference being the appearance of , the speed of 's expansion relative to that of . We suppose that , without loss of generality. For all , there must be a path to such that 's length changes at a rate of at most , and for all , . Similarly for , but we change the rate of growth from to . For a point , if there exists such that , then for all times , if , then . These conditions uniquely define and . I am interested in the final shapes of , and . We can show that is a closed set, and is the closure of its complement, so we can obviously just focus on . I know that is convex and bounded, and its boundary can be split into three curves. The first is easy. We consider the such that the distance from to is times the distance from to . If there is no issue with the straight line path between and intersecting with , or the straight line path between and intersecting with , then the paths clearly belong to and respectively, and they meet at these boundary points. The set of satisfying this distance equation is a certain circle, with on the inside and on the outside. Things are okay until the path from becomes tangent to the circle, at which point it's no longer obvious that it avoids , and indeed, it does not. There are two points on the circle which meet these tangent paths. These are the beginnings of the other two curves, which by symmetry, will obviously be mirror images in the line . So we might as well focus on the top. We can show that what needs to happen is that as we trace along the curve at speed , we just meet up with the expansion of . Now, we can show without much difficulty that is only ever going to expand via straight lines from . These lines are going to be growing radially from the points on the boundary of a circle of radius centred at which have not already been obstructed by the growth of . To get a little heuristic, locally, at a point on the curve, we're going to be meeting an expanding wall of growth at unit speed, going up in parallel lines. What kind of motion is going to beat the wall? Well, we'll be moving at speed , so we need to pick an angle such that the movement in the direction of the wall is at unit speed. This is actually already satisfied by the derivative of the first curve at the starting point of . The curves will stop when they run into each other somewhere along . the magnitude of 's derivative is . is always on a point of the circle of radius centred at . 's derivative forms a constant angle with the tangent line to this circle, which we can calculate from the initial value. We can also calculate the initial value of , where and no longer grow in lines colliding with each other, and the curve starts. Writing out the coordinates of and its derivative, and putting all this into the language of inner products, we find that we can solve for the -coordinates from the -coordinates and and vice versa. We then put the equations together to get the coordinate given by the function above. gives the coordinate. It's been pointed out to me that the properties defining my differential equation are satisfied by logarithmic spirals (by the differential equation, this means that logarithmic spirals with given initial conditions uniquely satisfy them). Logarithmic spirals are however given in polar coordinates, so I suppose my original question becomes one of expressing a part of a logarithmic spiral as a closed-form Cartesian function. But I can't find this, so I suspect that there is none. Anyway, as far as characterising the set goes I'm satisfied. It's more natural to shift everything to the left by . Then the boundary is given by these curves: The part of circle with centre and radius to the left of the -axis, The polar curve from to , and The reflection of that curve in the -axis. Here is an obligatory picture of boundaries for a family of 's for different 's, which is certainly what I wanted out of this.","\large \text{Problem} \sqrt{k^2-{f'(t)}^2}f(t)-\sqrt{t^2-{f(t)}^2}f'(t)=t\sqrt{k^2-1} f{\left(\frac{1}{\sqrt{k^2-1}}\right)}=\frac{1}{\sqrt{k^2-1}}? \sqrt{k^2-{g'(t)}^2}(1-g(t))+\sqrt{t^2-{(1-g(t))}^2}g'(t)=t\sqrt{k^2-1} g{\left(\frac{1}{\sqrt{k^2-1}}\right)}=1? \large \text{Motivation} \mathbb{R}^2 A_t B_t t\geq0 A_0=(0,0) B_0(1,0) x\in\mathbb{R}^2 s>t A_t x k(s-t) B_r r<s x\in A_s x\in\mathbb{R}^2 s>t A_t x s-t B_r r<s x\in A_s k A_t B_t k>1 x\in A_s \gamma(t) x \gamma(t) k t<s \gamma(t)\in A_t\setminus B_t B_s k 1 x\in\mathbb{R}^2 t x\in A_t\cap B_t s x\in A_s x\in B_s A_t B_t A=\bigcup_{t\geq0}A_t B=\bigcup_{t\geq0}B_t B A B B x x (0,0) k x (1,0) x (0,0) B_t x (1,0) A_t A B x (1,0) (0,0) (0,0) B_t y=0 k A_t B_t (1,0) t (1,0) A_t c(t) k c y=0 \text{To summarise} c k c t (1,0) c t A_t B_t c x y t y f(t) g(t) x \text{EDIT} 1 \left(\frac{1}{k^2-1},0\right) \frac{k}{k^2-1} y r=\frac{1}{\sqrt{k^2-1}}e^\frac{2\theta-3\pi}{2\sqrt{k^2-1}} \theta=\frac{3\pi}{2} 2\pi x B k","['ordinary-differential-equations', 'euclidean-geometry', 'plane-curves']"
29,Is a differential equation linear if the maximal solutions constitute a vector space?,Is a differential equation linear if the maximal solutions constitute a vector space?,,"Consider the differential equation $x'= F(t, x)$ , $F$ continuous, with the property of existence and uniqueness of solutions. Suppose that the maximal solutions are defined on all of $\mathbb{R}$ and constitute a vector space. Is it true that this differential equation is linear? Meaning that $F$ has the form $F(t, x) = A(t)x$ .","Consider the differential equation , continuous, with the property of existence and uniqueness of solutions. Suppose that the maximal solutions are defined on all of and constitute a vector space. Is it true that this differential equation is linear? Meaning that has the form .","x'= F(t, x) F \mathbb{R} F F(t, x) = A(t)x",['ordinary-differential-equations']
30,Solving a coupled system of first order homogeneous differential equations with variable coefficients,Solving a coupled system of first order homogeneous differential equations with variable coefficients,,Given the coupled system of first order homogeneous ordinary differential equations with variable coefficients. $$     \frac{dx}{dt} = a(t)z(t) $$ $$ \frac{dz}{dt} = -a(t)x(t) $$ How is the general solution below derived? $$ x(t) = c_1 \cos\left(\int_1^t a(\tau) d\tau \right)+c_2 \sin\left(\int_1^t a(\tau) d\tau\right) $$ $$ z(t) = -c_1 \sin\left(\int_1^t a(\tau) d\tau \right)+c_2 \cos\left(\int_1^t a(\tau) d\tau\right) $$ I thought it might be something like guessing the solution takes the form $$ x(t) = c_1 \cos\left(f(t)\right)+c_2 \sin\left(f(t)\right) $$ $$ z(t) = \frac{1}{\left(\frac{df}{dt}\right)} \frac{dx}{dt} = -c_1 \sin\left(f(t)\right)+c_2 \cos\left(f(t)\right) $$ And $\frac{df}{dt} = a(t)$ But I imagine there is a general approach to this type of problem?,Given the coupled system of first order homogeneous ordinary differential equations with variable coefficients. How is the general solution below derived? I thought it might be something like guessing the solution takes the form And But I imagine there is a general approach to this type of problem?,"    
\frac{dx}{dt} = a(t)z(t)
 
\frac{dz}{dt} = -a(t)x(t)
 
x(t) = c_1 \cos\left(\int_1^t a(\tau) d\tau \right)+c_2 \sin\left(\int_1^t a(\tau) d\tau\right)
 
z(t) = -c_1 \sin\left(\int_1^t a(\tau) d\tau \right)+c_2 \cos\left(\int_1^t a(\tau) d\tau\right)
 
x(t) = c_1 \cos\left(f(t)\right)+c_2 \sin\left(f(t)\right)
 
z(t) = \frac{1}{\left(\frac{df}{dt}\right)} \frac{dx}{dt} = -c_1 \sin\left(f(t)\right)+c_2 \cos\left(f(t)\right)
 \frac{df}{dt} = a(t)","['ordinary-differential-equations', 'systems-of-equations', 'circles']"
31,$g(x)=c$ solves the ODE $y'=f(y)$ iff $c$ is a critical point of the ODE,solves the ODE  iff  is a critical point of the ODE,g(x)=c y'=f(y) c,"Consider the autonomous ODE $y'=f(y)$ where $f$ is continuously differentiable. $g(x)=c$ (where $c$ is a constant) solves this ODE if and only if $c$ is a critical point of the ODE. Remember to prove in both directions. My attempt at the solution: First, I need to show that if $g(x)=c$ is a solution of the ODE then $c$ is a critical point of the ODE. So if $g(x)=c$ is a solution of $y'=f(y)$ then we have $y'=f(y)=f(c)$ by definition for all $x$ . Since $f(c)$ is a constant, then $y=c$ is also a solution of the ODE. If we do $\frac{d}{dy} f(y) = f'(y)$ at the point $y=c$ , then we have $f'(c)=0$ because the derivative of a constant function is always zero. Thus $c$ is a critical point of the ODE. Second, I need to show the converse is true. Namely, if $c$ is a critical point of the ODE, then $g(x)=c$ is a solution of the ODE. So if $c$ is a critical point, that means $f(c)=0$ . To show that $g(x)=c$ is a solution of the ODE, we need to show that $y'=f(y)=f(c)$ for all $x$ . Since $f(c)=0$ then $f(y)$ is constant at $y=c$ so when $y=c$ , $y'=f(y)=f(c)$ for all $x$ . Is this a correct proof?","Consider the autonomous ODE where is continuously differentiable. (where is a constant) solves this ODE if and only if is a critical point of the ODE. Remember to prove in both directions. My attempt at the solution: First, I need to show that if is a solution of the ODE then is a critical point of the ODE. So if is a solution of then we have by definition for all . Since is a constant, then is also a solution of the ODE. If we do at the point , then we have because the derivative of a constant function is always zero. Thus is a critical point of the ODE. Second, I need to show the converse is true. Namely, if is a critical point of the ODE, then is a solution of the ODE. So if is a critical point, that means . To show that is a solution of the ODE, we need to show that for all . Since then is constant at so when , for all . Is this a correct proof?",y'=f(y) f g(x)=c c c g(x)=c c g(x)=c y'=f(y) y'=f(y)=f(c) x f(c) y=c \frac{d}{dy} f(y) = f'(y) y=c f'(c)=0 c c g(x)=c c f(c)=0 g(x)=c y'=f(y)=f(c) x f(c)=0 f(y) y=c y=c y'=f(y)=f(c) x,['ordinary-differential-equations']
32,"Does there exist a unique solution for $y' = -y^{1/3}, y(0) = 0$?",Does there exist a unique solution for ?,"y' = -y^{1/3}, y(0) = 0","The given problem is to determine if the following initial value problems have a unique solution : $y' = y^{\frac13}, y(0) = 0,$ $y' = -y^{\frac13}, y(0) = 0,$ We can not apply Picard's theorem here. For the first one, the answer in this post tells that solutions are infinite. For the second one, I tried to construct a similar function given in the answer but was not successful. Since only the sign is different in the second one than the first one, I feel that the solutions are infinite but unable to prove or disprove them. Any help is higly appreciated.","The given problem is to determine if the following initial value problems have a unique solution : We can not apply Picard's theorem here. For the first one, the answer in this post tells that solutions are infinite. For the second one, I tried to construct a similar function given in the answer but was not successful. Since only the sign is different in the second one than the first one, I feel that the solutions are infinite but unable to prove or disprove them. Any help is higly appreciated.","y' = y^{\frac13}, y(0) = 0, y' = -y^{\frac13}, y(0) = 0,","['calculus', 'ordinary-differential-equations', 'initial-value-problems']"
33,Solve $x^2y''+xy'+y=0$ using the power series,Solve  using the power series,x^2y''+xy'+y=0,"Basically I have to find the power series of the ODE above. What I have so far: $y=\sum_{n=0}^{\infty }a_nx^n$ $y'=\sum_{n=1}^{\infty }na_nx^{n-1}$ $y''=\sum_{n=2}^{\infty }(n-1)(n)a_nx^{n-2}$ therefore, $xy'=\sum_{n=1}^{\infty }na_nx^{n}$ $x^2y''=\sum_{n=2}^{\infty }(n-1)(n)a_nx^{n}$ since the counters of the summations are different, I evaluate $y$ at $n=0,1$ and $xy'$ at $n=1$ I then get, ( $\sum_{n=2}^{\infty }(n-1)(n)a_nx^{n} )+(a_1x^1 + \sum_{n=2}^{\infty }na_nx^{n})+(a_0x^0+a_1x^1+\sum_{n=2}^{\infty }a_nx^n)=0$ This then leaves me with $a_0=0$ $2a_1=0; a_1 = 0$ $(n-1)(n)a_n+na_n+a_n=0$ I then get a recursion formula of: $a_n(n^2+1)=0$ which would make my $a_n$ equal to zero. Where am I wrong? Symbolab says that the answer to this ODE is $y=c_1\cos(\ln(x))+c_2\sin(\ln(x))$","Basically I have to find the power series of the ODE above. What I have so far: therefore, since the counters of the summations are different, I evaluate at and at I then get, ( This then leaves me with I then get a recursion formula of: which would make my equal to zero. Where am I wrong? Symbolab says that the answer to this ODE is","y=\sum_{n=0}^{\infty }a_nx^n y'=\sum_{n=1}^{\infty }na_nx^{n-1} y''=\sum_{n=2}^{\infty }(n-1)(n)a_nx^{n-2} xy'=\sum_{n=1}^{\infty }na_nx^{n} x^2y''=\sum_{n=2}^{\infty }(n-1)(n)a_nx^{n} y n=0,1 xy' n=1 \sum_{n=2}^{\infty }(n-1)(n)a_nx^{n} )+(a_1x^1 + \sum_{n=2}^{\infty }na_nx^{n})+(a_0x^0+a_1x^1+\sum_{n=2}^{\infty }a_nx^n)=0 a_0=0 2a_1=0; a_1 = 0 (n-1)(n)a_n+na_n+a_n=0 a_n(n^2+1)=0 a_n y=c_1\cos(\ln(x))+c_2\sin(\ln(x))","['ordinary-differential-equations', 'power-series']"
34,Using Laplace transform solve the following differential equation : $ y'+y = 2 + \delta(t-4)$,Using Laplace transform solve the following differential equation :, y'+y = 2 + \delta(t-4),"Using Laplace transform solve the following differential equation : $$ y'+y = 2 + \delta(t-4)$$ , $$y(0)=0$$ Express the solution $y(t)$ as a piecewise function about $t=4$ and tell us what happens to the graph of it at $t=4$ . My try : Taking Laplace transform on the both sides : $$ \big( s \cdot Y(s) - y(0) \big) + Y(s) = \frac{2}{s} + e^{-4s} \\  \implies Y(s) = \frac{2}{s(s+1)} + \frac{e^{-4s}}{(s+1)} \\ \implies Y(s) = 2\Big(\frac{1}{s}-\frac{1}{s+1}\Big) + \frac{e^{-4s}}{(s+1)} $$ Now I'm stuck here because I dont know how to take the inverse Laplace transform of $\frac{e^{-4s}}{(s+1)}$ . I haven't ever dealt with something involving exponential function. Can I get some help here please?","Using Laplace transform solve the following differential equation : , Express the solution as a piecewise function about and tell us what happens to the graph of it at . My try : Taking Laplace transform on the both sides : Now I'm stuck here because I dont know how to take the inverse Laplace transform of . I haven't ever dealt with something involving exponential function. Can I get some help here please?"," y'+y = 2 + \delta(t-4) y(0)=0 y(t) t=4 t=4  \big( s \cdot Y(s) - y(0) \big) + Y(s) = \frac{2}{s} + e^{-4s} \\
 \implies Y(s) = \frac{2}{s(s+1)} + \frac{e^{-4s}}{(s+1)} \\
\implies Y(s) = 2\Big(\frac{1}{s}-\frac{1}{s+1}\Big) + \frac{e^{-4s}}{(s+1)}
 \frac{e^{-4s}}{(s+1)}","['ordinary-differential-equations', 'laplace-transform']"
35,Finding all differentiable functions that satisfy an equation,Finding all differentiable functions that satisfy an equation,,"I have tried to solve a problem but it seems I have made a mistake and I don't know where. The problem: Find all differentiable functions $f : \mathbb{R} \to \mathbb{R}$ that satisfy the following condition: $f \circ f = f$ . My approach: $f \circ f = f \implies (f' \circ f)f'=f' \implies f'(x)=0$ or $(f' \circ f)(x)=1$ . If $f$ is invertible then we can muliply both sides by the inverse and get that $f(x)=x$ . If $f'(f(x))=1$ how can we continue. We also have to find $f(x)=x$ as a solution. I have tried using the differential notation but things got even messier: $\frac{df(f(x))}{dx}=\frac{df}{dx}$ . Using the chain rule, i.e.: $\frac{df(g(x))}{dx}=\frac{df}{dg}\frac{dg}{dx}$ we obtain $\frac{df}{df}\frac{df}{dx}=\frac{df}{dx}$ , which is just $\frac{df}{dx}=\frac{df}{dx}$ . In the meantime, I have found a way to prove that if $f(x)$ is not constant, then $f(x)=x$ , but I am not sure whether it is rigurous or not. Firstly suppose $f(x) \neq x$ . Then $f(x) < x$ or $f(x) > x$ . Assume $f(x) > x$ . Therefore, there exists $a : \mathbb{R} \to \mathbb{R}, a(x) > 0$ , such that $f(x)=x+a(x)$ . From the definition of the limit, $$f'(x)=\lim_{h \to 0} \frac{f(f(x)+h)-f(x)}{h}=\lim_{h \to 0} \frac{f(x+a(x)+h)-x-a(x)}{h}=\lim_{h \to 0} \frac{x+a(x)+h+a(x+a(x)+h)-x-a(x)}{h}=\lim_{h \to 0} \frac{h+a(x+h+a(x))}{h}$$ The last limit can exist if $\lim_{h \to 0}a(x+h+a(x))=0$ , but $a(x)$ is defined to be stictly positive. Analogously, for $f(x)<x$ .","I have tried to solve a problem but it seems I have made a mistake and I don't know where. The problem: Find all differentiable functions that satisfy the following condition: . My approach: or . If is invertible then we can muliply both sides by the inverse and get that . If how can we continue. We also have to find as a solution. I have tried using the differential notation but things got even messier: . Using the chain rule, i.e.: we obtain , which is just . In the meantime, I have found a way to prove that if is not constant, then , but I am not sure whether it is rigurous or not. Firstly suppose . Then or . Assume . Therefore, there exists , such that . From the definition of the limit, The last limit can exist if , but is defined to be stictly positive. Analogously, for .","f : \mathbb{R} \to \mathbb{R} f \circ f = f f \circ f = f \implies (f' \circ f)f'=f' \implies f'(x)=0 (f' \circ f)(x)=1 f f(x)=x f'(f(x))=1 f(x)=x \frac{df(f(x))}{dx}=\frac{df}{dx} \frac{df(g(x))}{dx}=\frac{df}{dg}\frac{dg}{dx} \frac{df}{df}\frac{df}{dx}=\frac{df}{dx} \frac{df}{dx}=\frac{df}{dx} f(x) f(x)=x f(x) \neq x f(x) < x f(x) > x f(x) > x a : \mathbb{R} \to \mathbb{R}, a(x) > 0 f(x)=x+a(x) f'(x)=\lim_{h \to 0} \frac{f(f(x)+h)-f(x)}{h}=\lim_{h \to 0} \frac{f(x+a(x)+h)-x-a(x)}{h}=\lim_{h \to 0} \frac{x+a(x)+h+a(x+a(x)+h)-x-a(x)}{h}=\lim_{h \to 0} \frac{h+a(x+h+a(x))}{h} \lim_{h \to 0}a(x+h+a(x))=0 a(x) f(x)<x","['real-analysis', 'calculus', 'ordinary-differential-equations', 'derivatives', 'continuity']"
36,Continuous function composed with itself is equal to propagation of a differential equation.,Continuous function composed with itself is equal to propagation of a differential equation.,,"This question has been bugging me for a while. It was given as the last question of a first year undergrad analysis exam and so should be solvable with little machinery, yet it seems to point straight at ODEs which have yet to be covered. Here is the question: Let $F:\mathbb{R} \to \mathbb{R}$ be a bounded $\mathscr{C}^1$ function and $\left(f_t:\mathbb{R}\to\mathbb{R}\right)_{t\in\mathbb{R}}$ a family of continuous functions with $f_0(x)=x$ such that $$\lim_{h\to 0} \frac{f_{t+h}(x)-f_t(x)}{h}=F(f_t(x))$$ Show that there exists a continuous function $g:\mathbb{R}\to\mathbb{R}$ such that $f_1 = g\circ g$ . I have attempted this with a classmate and we've come to various levels of understanding of the question, but the required conclusion keeps escaping us. Our main issues with the question are that we have very little understanding on continuous functions composed with themselves, and additionally we haven't managed to use the $\mathscr{C}^1$ and boundedness condition on $F$ . Note that the question also states that we are allowed to assume that the family of functions $(f_t)_{t\in\mathbb{R}}$ is uniquely determined by the given conditions: this again points to the domain of differential equations, which ideally we should not need to refer to in order to solve this.","This question has been bugging me for a while. It was given as the last question of a first year undergrad analysis exam and so should be solvable with little machinery, yet it seems to point straight at ODEs which have yet to be covered. Here is the question: Let be a bounded function and a family of continuous functions with such that Show that there exists a continuous function such that . I have attempted this with a classmate and we've come to various levels of understanding of the question, but the required conclusion keeps escaping us. Our main issues with the question are that we have very little understanding on continuous functions composed with themselves, and additionally we haven't managed to use the and boundedness condition on . Note that the question also states that we are allowed to assume that the family of functions is uniquely determined by the given conditions: this again points to the domain of differential equations, which ideally we should not need to refer to in order to solve this.",F:\mathbb{R} \to \mathbb{R} \mathscr{C}^1 \left(f_t:\mathbb{R}\to\mathbb{R}\right)_{t\in\mathbb{R}} f_0(x)=x \lim_{h\to 0} \frac{f_{t+h}(x)-f_t(x)}{h}=F(f_t(x)) g:\mathbb{R}\to\mathbb{R} f_1 = g\circ g \mathscr{C}^1 F (f_t)_{t\in\mathbb{R}},"['real-analysis', 'calculus', 'ordinary-differential-equations', 'function-and-relation-composition']"
37,Stuck while using Laplace transform to solve delayed DE.,Stuck while using Laplace transform to solve delayed DE.,,"I am studying delayed DE's, and while I've solved the following equation $$y'(t) = y(t-1)$$ $$y[-1,0] = y_0$$ where $y_0$ is a constant by using the method of steps to get $$y(t) = \sum y_0\frac{t^n}{n!} + c$$ on $[n,n+1]$ , I'm trying to now solve this using a Laplace transform. (Also, could someone confirm if the above is the correct answer? I remember following online notes, but it was a while ago, and I don't remember if this was supposed to be the final solution, since it's only defined on a specific interval?) Anyway, until now, I've worked out the following by applying a Laplace transform to both sides of the equation. (I'll be skipping some more simple steps while writing this down though) $$\int_0^\infty e^{-st}y'(t)dt = \int_0^\infty e^{-st} y(t-1)dt$$ $$sY(s) - Y(0) = \int_{-1}^\infty e^{-s(\tau+1)} y(\tau)d\tau$$ where $Y(s) = \mathcal{L}\{y(t)\}$ , and $t-1 = \tau$ . I then continue and get $$sY(s) - y_0 = e^{-s}\bigg[\int_{-1}^0 e^{-s\tau} y(\tau)d\tau + \int_{0}^\infty e^{-s\tau} y(\tau)d\tau \bigg]$$ $$sY(s) - y_0 = e^{-s}\bigg[y(\tau)\frac{e^{-s\tau}}{-s}\bigg|^0_{-1} +  \frac{1}{s}\int_{-1}^{0} y'(\tau)e^{-s\tau}d\tau +Y(s)\bigg]$$ where I used integration by parts to evaluate the first integral in the upper equation that's from $-1$ to $0$ . This is where I'm stuck, because in the last equation, while I can evaluate the first part, and obviously simplify the rest of the equation, I'm not sure how to solve the middle integral. I tried letting it be equal to a random integral $I$ to use integration by parts again and get a value, but I ended up with a $0=0$ equation lol. I might be fuzzy on my calculus skills, so can someone help me evaluate that center integral? Or basically just this following integral overall $$\int_{-1}^0 e^{-s\tau} y(\tau)d\tau$$ I feel like it's probably something really simple I'm missing, but I would be super helpful if someone could help point it out. Thank you!!","I am studying delayed DE's, and while I've solved the following equation where is a constant by using the method of steps to get on , I'm trying to now solve this using a Laplace transform. (Also, could someone confirm if the above is the correct answer? I remember following online notes, but it was a while ago, and I don't remember if this was supposed to be the final solution, since it's only defined on a specific interval?) Anyway, until now, I've worked out the following by applying a Laplace transform to both sides of the equation. (I'll be skipping some more simple steps while writing this down though) where , and . I then continue and get where I used integration by parts to evaluate the first integral in the upper equation that's from to . This is where I'm stuck, because in the last equation, while I can evaluate the first part, and obviously simplify the rest of the equation, I'm not sure how to solve the middle integral. I tried letting it be equal to a random integral to use integration by parts again and get a value, but I ended up with a equation lol. I might be fuzzy on my calculus skills, so can someone help me evaluate that center integral? Or basically just this following integral overall I feel like it's probably something really simple I'm missing, but I would be super helpful if someone could help point it out. Thank you!!","y'(t) = y(t-1) y[-1,0] = y_0 y_0 y(t) = \sum y_0\frac{t^n}{n!} + c [n,n+1] \int_0^\infty e^{-st}y'(t)dt = \int_0^\infty e^{-st} y(t-1)dt sY(s) - Y(0) = \int_{-1}^\infty e^{-s(\tau+1)} y(\tau)d\tau Y(s) = \mathcal{L}\{y(t)\} t-1 = \tau sY(s) - y_0 = e^{-s}\bigg[\int_{-1}^0 e^{-s\tau} y(\tau)d\tau +
\int_{0}^\infty e^{-s\tau} y(\tau)d\tau \bigg] sY(s) - y_0 = e^{-s}\bigg[y(\tau)\frac{e^{-s\tau}}{-s}\bigg|^0_{-1} + 
\frac{1}{s}\int_{-1}^{0} y'(\tau)e^{-s\tau}d\tau +Y(s)\bigg] -1 0 I 0=0 \int_{-1}^0 e^{-s\tau} y(\tau)d\tau","['integration', 'ordinary-differential-equations', 'laplace-transform', 'delay-differential-equations']"
38,linear 4×4 system of nonhomogeneous differential equations with repeated eigenvalues.,linear 4×4 system of nonhomogeneous differential equations with repeated eigenvalues.,,"I'm trying to find the general solution of this system of linear Nonhomogeneous differential equation: $$X'=\begin{bmatrix} -2 &1  &1 &-1\\  -1 & -1 & 1 &0\\  0 &0  &-1 &0\\ 0 &-1  &0 &0 \end{bmatrix}X +  \begin{bmatrix} e^t \\  2t \\  e^{-2t} \\ 0  \end{bmatrix} $$ as an aproach, I found the eigenvalue $-1$ with multiplicity of 4. And I found two eigenvectors: $$V^{(1)}=\begin{bmatrix} 1 \\  0 \\  1 \\ 0  \end{bmatrix}$$ and $$V^{(2)}=\begin{bmatrix} 0 \\  1 \\  0 \\ 1  \end{bmatrix}$$ so i got two answers for the Corresponding homogeneous equation: $$X^{(1)}=\begin{bmatrix} 1 \\  0 \\  1 \\ 0  \end{bmatrix}e^{-t}$$ and $$X^{(2)}=\begin{bmatrix} 0 \\  1 \\  0 \\ 1  \end{bmatrix}e^{-t}$$ since we have 2 eigenvectors and one eigenvalue with multiplicity of 4, I tried to find the other two answers by assuming them as $X=vte^{-t} + ηe^{-t}$ and I got: $X^{(3)}=\begin{bmatrix} 0 \\  1 \\  0 \\ 1  \end{bmatrix}te^{-t} + \begin{bmatrix} 0 \\  0 \\  1 \\ 1  \end{bmatrix}e^{-t}$ $X^{(4)}=\begin{bmatrix} 0 \\  1 \\  0 \\ 1  \end{bmatrix}te^{-t} + \begin{bmatrix} -1 \\  -1 \\  0 \\ 0  \end{bmatrix}e^{-t}$ I wanna know if the Answers I found are correct. if Yes, we have: $$X=\begin{bmatrix} e^{-t} &0  &0 &-e^{-t}\\  0 & e^{-t} & te^{-t} &te^{-t}-e^{-t}\\  e^{-t} &0  &e^{-t} &0\\ 0 &e^{-t}  &te^{-t}+e^{-t} &te^{-t} \end{bmatrix}$$ using the Variation of Parameters method, we have to inverse X to find the answer but A is singular $(detA=0)$ :( where am I going wrong? can someone help me?","I'm trying to find the general solution of this system of linear Nonhomogeneous differential equation: as an aproach, I found the eigenvalue with multiplicity of 4. And I found two eigenvectors: and so i got two answers for the Corresponding homogeneous equation: and since we have 2 eigenvectors and one eigenvalue with multiplicity of 4, I tried to find the other two answers by assuming them as and I got: I wanna know if the Answers I found are correct. if Yes, we have: using the Variation of Parameters method, we have to inverse X to find the answer but A is singular :( where am I going wrong? can someone help me?","X'=\begin{bmatrix}
-2 &1  &1 &-1\\ 
-1 & -1 & 1 &0\\ 
0 &0  &-1 &0\\
0 &-1  &0 &0
\end{bmatrix}X + 
\begin{bmatrix}
e^t \\ 
2t \\ 
e^{-2t} \\
0 
\end{bmatrix}
 -1 V^{(1)}=\begin{bmatrix}
1 \\ 
0 \\ 
1 \\
0 
\end{bmatrix} V^{(2)}=\begin{bmatrix}
0 \\ 
1 \\ 
0 \\
1 
\end{bmatrix} X^{(1)}=\begin{bmatrix}
1 \\ 
0 \\ 
1 \\
0 
\end{bmatrix}e^{-t} X^{(2)}=\begin{bmatrix}
0 \\ 
1 \\ 
0 \\
1 
\end{bmatrix}e^{-t} X=vte^{-t} + ηe^{-t} X^{(3)}=\begin{bmatrix}
0 \\ 
1 \\ 
0 \\
1 
\end{bmatrix}te^{-t} + \begin{bmatrix}
0 \\ 
0 \\ 
1 \\
1 
\end{bmatrix}e^{-t} X^{(4)}=\begin{bmatrix}
0 \\ 
1 \\ 
0 \\
1 
\end{bmatrix}te^{-t} + \begin{bmatrix}
-1 \\ 
-1 \\ 
0 \\
0 
\end{bmatrix}e^{-t} X=\begin{bmatrix}
e^{-t} &0  &0 &-e^{-t}\\ 
0 & e^{-t} & te^{-t} &te^{-t}-e^{-t}\\ 
e^{-t} &0  &e^{-t} &0\\
0 &e^{-t}  &te^{-t}+e^{-t} &te^{-t}
\end{bmatrix} (detA=0)","['ordinary-differential-equations', 'eigenvalues-eigenvectors', 'systems-of-equations']"
39,Is it possible to find the eigenfunction of the following differential operator?,Is it possible to find the eigenfunction of the following differential operator?,,Consider a differential operator of the form $\sum_{j=-\infty}^\infty(z_{j+1}\partial_{z_j}+z_j\partial_{z_{j+1}}-g z_j-g\partial_{z_j})$ Here g is a constant. The operator consisting up the first two terms $\sum_j(z_{j+1}\partial_{z_j}+z_j\partial_{z_{j+1}})$ has an eigenfunction of the form $\sum_m z_m e^{ikm}$ wth eigenvalue $2\cos k$ and the operator of the form $g(z_j+\partial_{z_j})$ has an eigenfunction of the form $e^{-z_j^2/2+Ez_j}$ with Eigenvalue $Eg$ . Is there a way to find an eigenfunction of the entire thing? Is there any ansatz that can help me proceed?,Consider a differential operator of the form Here g is a constant. The operator consisting up the first two terms has an eigenfunction of the form wth eigenvalue and the operator of the form has an eigenfunction of the form with Eigenvalue . Is there a way to find an eigenfunction of the entire thing? Is there any ansatz that can help me proceed?,\sum_{j=-\infty}^\infty(z_{j+1}\partial_{z_j}+z_j\partial_{z_{j+1}}-g z_j-g\partial_{z_j}) \sum_j(z_{j+1}\partial_{z_j}+z_j\partial_{z_{j+1}}) \sum_m z_m e^{ikm} 2\cos k g(z_j+\partial_{z_j}) e^{-z_j^2/2+Ez_j} Eg,"['ordinary-differential-equations', 'partial-differential-equations', 'eigenvalues-eigenvectors', 'operator-theory']"
40,What is the inverse of $f'(x)-f(x)$? [closed],What is the inverse of ? [closed],f'(x)-f(x),"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 2 years ago . Improve this question Consider the following function $P$ defined as $P(f(x))=f'(x)-f(x)$ What is the inverse of $P$ ? I can't figure this one out. A hint would be very helpful.","Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 2 years ago . Improve this question Consider the following function defined as What is the inverse of ? I can't figure this one out. A hint would be very helpful.",P P(f(x))=f'(x)-f(x) P,"['ordinary-differential-equations', 'analysis']"
41,Why does this non-stiff ode requires a stiff solver?,Why does this non-stiff ode requires a stiff solver?,,"This post is related to another one in physics.stackexchange. But it might be more of a quadrature than a physics problem. The following set of ordinary differential equations describe the motion of two mass points. They are coupled by a couple of geometric constraints yielding in the following equations in $x_1(t), z_2(t)$ (see Theoretische Physik , Bartelmann et al., 1e, 2015): $$ \begin{eqnarray} m_1 \ddot{x}_1 & = & -\frac{\dot{x}_1^2 + \dot{z}_2^2 - z_2g}{2(x_1^2/m_1 + z_2^2/m_2)}x_1, \\ m_2 \ddot{z}_2 & = & -\frac{\dot{x}_1^2 + \dot{z}_2^2 - z_2g}{2(x_1^2/m_1 + z_2^2/m_2)}z_2 - m_2g. \end{eqnarray} $$ Integrating with a Runge-Kutta scheme of order 4, and initial conditions chosen consistently with the constriants to be $\mathbf{x}_0 = (x_{10}, z_{20}) = (0, -L)$ , violates one of the geometric constraints $x_1^2 + z_2^2 - L^2 = 0$ for time $t > 0$ . If integrated with an implicit Runge-Kutta method of the Radau IIA family of order 5, considered a stiff solver, the error is reduced by an order of magnitude of 3. If I consider a characteristic of the system, stating that the expression $E := m/2(\dot{x}_1^2 + \dot{z}_2^2 + (x_1 + z_2)g)$ forms an invariant/is preserved, if $m_1 = m_2 = m$ , and considering the geometric constraint $x_1^2 + z_2^2 = L^2$ , the same system can be expressed as $$ \begin{eqnarray} m\ddot{x}_1 & = & -\frac{E - (x_1/2 + z_2)g}{L^2}x_1, \\ m\ddot{z}_2 & = & -\frac{E - (x_1/2 + z_2)g}{L^2}z_2, \end{eqnarray} $$ i.e. a system of ordinary differential equations which can be split into a decoupled linear, and a nonlinear part. Question: As can be seen from the linear part of the model, the eigenfrequencies are identical. Particularly in the case of $g=0$ , how come I need a stiff solver to account for numerical instabilities then?","This post is related to another one in physics.stackexchange. But it might be more of a quadrature than a physics problem. The following set of ordinary differential equations describe the motion of two mass points. They are coupled by a couple of geometric constraints yielding in the following equations in (see Theoretische Physik , Bartelmann et al., 1e, 2015): Integrating with a Runge-Kutta scheme of order 4, and initial conditions chosen consistently with the constriants to be , violates one of the geometric constraints for time . If integrated with an implicit Runge-Kutta method of the Radau IIA family of order 5, considered a stiff solver, the error is reduced by an order of magnitude of 3. If I consider a characteristic of the system, stating that the expression forms an invariant/is preserved, if , and considering the geometric constraint , the same system can be expressed as i.e. a system of ordinary differential equations which can be split into a decoupled linear, and a nonlinear part. Question: As can be seen from the linear part of the model, the eigenfrequencies are identical. Particularly in the case of , how come I need a stiff solver to account for numerical instabilities then?","x_1(t), z_2(t) 
\begin{eqnarray}
m_1 \ddot{x}_1 & = & -\frac{\dot{x}_1^2 + \dot{z}_2^2 - z_2g}{2(x_1^2/m_1 + z_2^2/m_2)}x_1, \\
m_2 \ddot{z}_2 & = & -\frac{\dot{x}_1^2 + \dot{z}_2^2 - z_2g}{2(x_1^2/m_1 + z_2^2/m_2)}z_2 - m_2g.
\end{eqnarray}
 \mathbf{x}_0 = (x_{10}, z_{20}) = (0, -L) x_1^2 + z_2^2 - L^2 = 0 t > 0 E := m/2(\dot{x}_1^2 + \dot{z}_2^2 + (x_1 + z_2)g) m_1 = m_2 = m x_1^2 + z_2^2 = L^2 
\begin{eqnarray}
m\ddot{x}_1 & = & -\frac{E - (x_1/2 + z_2)g}{L^2}x_1, \\
m\ddot{z}_2 & = & -\frac{E - (x_1/2 + z_2)g}{L^2}z_2,
\end{eqnarray}
 g=0","['ordinary-differential-equations', 'numerical-methods']"
42,"Euler's method, Multiple choice does not match my answer.","Euler's method, Multiple choice does not match my answer.",,"This is the original question. Use Euler's method with h=0.2 to estimate y when x =1 if $y' = (y^2-1) /2 $ and y(0) = 0 A. 7.690 B. 12.730 C. 13.504 D. 90.676 My answer follows. n= 5, h= 0.2 a= x_0= 0, b=1, y0= 0 I'm using the formula $y_{(n+1)} = y_n + h * y'$ and generated the following table. Note this table shows the answer as $-0.47141$ because $y_{n+1}$ is on the previous line of the table below. However, this doesn't match any of the given answer choices. This is an employment test covering Advanced Placement Calculus BC, so this is all the context I have. Where is the mistake? Are there other variations called ""Euler's method"" which generate different answers? x h y y^2 y' next y 0 0.2 0.00000 0.00000 -0.50000 -0.10000 0.2 0.2 -0.10000 0.01000 -0.49500 -0.19900 0.4 0.2 -0.19900 0.03960 -0.48020 -0.29504 0.6 0.2 -0.29504 0.08705 -0.45648 -0.38634 0.8 0.2 -0.38634 0.14925 -0.42537 *  -0.47141 * 1 0.2 -0.47141 0.22223 -0.38889 -0.54919","This is the original question. Use Euler's method with h=0.2 to estimate y when x =1 if and y(0) = 0 A. 7.690 B. 12.730 C. 13.504 D. 90.676 My answer follows. n= 5, h= 0.2 a= x_0= 0, b=1, y0= 0 I'm using the formula and generated the following table. Note this table shows the answer as because is on the previous line of the table below. However, this doesn't match any of the given answer choices. This is an employment test covering Advanced Placement Calculus BC, so this is all the context I have. Where is the mistake? Are there other variations called ""Euler's method"" which generate different answers? x h y y^2 y' next y 0 0.2 0.00000 0.00000 -0.50000 -0.10000 0.2 0.2 -0.10000 0.01000 -0.49500 -0.19900 0.4 0.2 -0.19900 0.03960 -0.48020 -0.29504 0.6 0.2 -0.29504 0.08705 -0.45648 -0.38634 0.8 0.2 -0.38634 0.14925 -0.42537 *  -0.47141 * 1 0.2 -0.47141 0.22223 -0.38889 -0.54919",y' = (y^2-1) /2  y_{(n+1)} = y_n + h * y' -0.47141 y_{n+1},"['calculus', 'ordinary-differential-equations', 'eulers-method']"
43,How to obtain discretized integral form of ODE initial value problem?,How to obtain discretized integral form of ODE initial value problem?,,"Given is the ODE initial value problem $$ \frac{d}{dt} x(t) = f(t, x(t)), \quad x(t_0) = x_0. $$ By integrating we obtain $$ x(t) = x_0 + F(t, x(t)) - F(t_0, x(t_0)) = x_0 + \int_{t_0}^{t} f(s, x(s))\,\mathrm{d}s. $$ In order to solve the ODE numerically on $[t_0, t_f]$ , we use a grid $G = \{t_0 < t_1 < \ldots < t_f \}$ , so we have $$ x(t_{i+1}) = x(t_0) + \int_{t_0}^{t_{i+1}} f(s, x(s)) \,\mathrm{d}s \tag{1} $$ and by applying a arbitrary quadrature rule for the integral, we obtain a numerical scheme. However, I noticed that in most books it reads $$ x(t_{i+1}) = x(t_i) + \int_{t_i}^{t_{i+1}} f(s, x(s)) \,\mathrm{d}s \tag{2} $$ instead. Did I miss something? Or can I derive (2) from (1)?","Given is the ODE initial value problem By integrating we obtain In order to solve the ODE numerically on , we use a grid , so we have and by applying a arbitrary quadrature rule for the integral, we obtain a numerical scheme. However, I noticed that in most books it reads instead. Did I miss something? Or can I derive (2) from (1)?","
\frac{d}{dt} x(t) = f(t, x(t)), \quad x(t_0) = x_0.
 
x(t) = x_0 + F(t, x(t)) - F(t_0, x(t_0)) = x_0 + \int_{t_0}^{t} f(s, x(s))\,\mathrm{d}s.
 [t_0, t_f] G = \{t_0 < t_1 < \ldots < t_f \} 
x(t_{i+1}) = x(t_0) + \int_{t_0}^{t_{i+1}} f(s, x(s)) \,\mathrm{d}s \tag{1}
 
x(t_{i+1}) = x(t_i) + \int_{t_i}^{t_{i+1}} f(s, x(s)) \,\mathrm{d}s \tag{2}
","['ordinary-differential-equations', 'numerical-methods', 'initial-value-problems']"
44,Prove that if $\mathbb Z \cap \{x(t)\ |\ t \in \mathbb R\}$ is non-empty then $x$ is constant.,Prove that if  is non-empty then  is constant.,\mathbb Z \cap \{x(t)\ |\ t \in \mathbb R\} x,"Let $f : \mathbb R \longrightarrow \mathbb R$ be a $C^{\infty}$ -function such that $f(x) = 0$ iff $x \in \mathbb Z.$ Suppose that the function $x : \mathbb R \longrightarrow \mathbb R$ satisfies $x'(t) = f(x(t)),$ for all $t \in \mathbb R.$ If $\mathbb Z \cap \{x(t)\ |\ t \in \mathbb R\}$ is non-empty then $x$ is a constant. This question appeared in one of the entrance examinations. It is clear that $x'(t_0) = 0,$ for some $t_0 \in \mathbb R.$ But how do I show that $x'(t) = 0,$ for all $t \in \mathbb R\ $ ? Any help would be much appreciated. Thanks!",Let be a -function such that iff Suppose that the function satisfies for all If is non-empty then is a constant. This question appeared in one of the entrance examinations. It is clear that for some But how do I show that for all ? Any help would be much appreciated. Thanks!,"f : \mathbb R \longrightarrow \mathbb R C^{\infty} f(x) = 0 x \in \mathbb Z. x : \mathbb R \longrightarrow \mathbb R x'(t) = f(x(t)), t \in \mathbb R. \mathbb Z \cap \{x(t)\ |\ t \in \mathbb R\} x x'(t_0) = 0, t_0 \in \mathbb R. x'(t) = 0, t \in \mathbb R\ ","['real-analysis', 'ordinary-differential-equations', 'smooth-functions']"
45,"Does a general procedure exist for reducing ${_2F_1}(a,b;c;z)$ when $a,b,c\in\Bbb Q$?",Does a general procedure exist for reducing  when ?,"{_2F_1}(a,b;c;z) a,b,c\in\Bbb Q","This question is related to a previous question of mine. A quick visit to the Wolfram Functions site reveals a rather extensive list of reduction formulae for the hypergeometric function ${_2F_1}(a,b;c;z)$ when $a,b,c$ are rational numbers. I am curious about how these reduction formulae are derived and if there is a general procedure for finding them? It was rather interesting that the link above includes reduction formulae for rational parameters when the parameters have denominators $1,2,3,4,5,6$ , and $8$ but not $7$ . Is there an interesting reason for this besides the list simply being incomplete? I know of one trick for the case where $c=b+1$ , which takes advantage of the differential formula $$ (z\partial_z+\beta_k-1){}_pF_q\left( \begin{array}{c}\alpha_1,\ldots,\alpha_p \\  \beta_1,\ldots,\beta_k,\ldots,\beta_q\end{array};z\right) =\left(\beta_k-1\right) {}_pF_q\left( \begin{array}{c}\alpha_1,\ldots,\alpha_p \\  \beta_1,\ldots,\beta_k-1,\ldots,\beta_q\end{array};z\right). \tag{1} $$ Take as an example $y(z)={_2F_1}(1,5/4;2;z)$ . Then using $(1)$ we can derive the ODE $$ (z\partial_z+1)y=(1-z)^{-5/4}. $$ Coupling this equation with the initial condition $y(0)=1$ and the product rule for derivatives gives the simple result $$ \partial_z(zy)=(1-z)^{-5/4},\quad y(0)=1, $$ which is easily solved by integrating and using the initial condition to determine the constant of integration. Doing so yields $$ {_2F_1}(1,5/4;2;z)=\frac{4}{z}((1-z)^{-1/4}-1). $$ Of course, this is a very specialized case of the general approach I am interested in. If a general procedure for arbitrary rational parameters does not exist, I would also be interested in procedures for families of parameters, e.g. a procedure for the case where all parameters have denominator of $2$ . Any references are also greatly appreciated.","This question is related to a previous question of mine. A quick visit to the Wolfram Functions site reveals a rather extensive list of reduction formulae for the hypergeometric function when are rational numbers. I am curious about how these reduction formulae are derived and if there is a general procedure for finding them? It was rather interesting that the link above includes reduction formulae for rational parameters when the parameters have denominators , and but not . Is there an interesting reason for this besides the list simply being incomplete? I know of one trick for the case where , which takes advantage of the differential formula Take as an example . Then using we can derive the ODE Coupling this equation with the initial condition and the product rule for derivatives gives the simple result which is easily solved by integrating and using the initial condition to determine the constant of integration. Doing so yields Of course, this is a very specialized case of the general approach I am interested in. If a general procedure for arbitrary rational parameters does not exist, I would also be interested in procedures for families of parameters, e.g. a procedure for the case where all parameters have denominator of . Any references are also greatly appreciated.","{_2F_1}(a,b;c;z) a,b,c 1,2,3,4,5,6 8 7 c=b+1 
(z\partial_z+\beta_k-1){}_pF_q\left(
\begin{array}{c}\alpha_1,\ldots,\alpha_p \\ 
\beta_1,\ldots,\beta_k,\ldots,\beta_q\end{array};z\right)
=\left(\beta_k-1\right)
{}_pF_q\left(
\begin{array}{c}\alpha_1,\ldots,\alpha_p \\ 
\beta_1,\ldots,\beta_k-1,\ldots,\beta_q\end{array};z\right).
\tag{1}
 y(z)={_2F_1}(1,5/4;2;z) (1) 
(z\partial_z+1)y=(1-z)^{-5/4}.
 y(0)=1 
\partial_z(zy)=(1-z)^{-5/4},\quad y(0)=1,
 
{_2F_1}(1,5/4;2;z)=\frac{4}{z}((1-z)^{-1/4}-1).
 2","['ordinary-differential-equations', 'reference-request', 'special-functions', 'hypergeometric-function']"
46,Example of a dynamical system,Example of a dynamical system,,"Consider a dynamical system defined by a non-autonomous ordinary differential equation $$ \dot z = X(z,t)$$ where $X$ is periodic with respect to time of period $T > 0$ . Consider the flow evaluated at $T$ , i.e., the map $\Phi^X_{T,0} : M \to M$ , where $M$ is a manifold (but let us take an open subset of $\mathbb R^n$ ) and $\Phi^X_{T,0}$ is the map that assigns to each $z \in M$ the value at time $T$ of the solution that starts from $z$ at time $0$ . This map can be interpreted as a Poincaré map in the extended quotient phase space $M \times S^1_T$ , $S^1_T$ being the one dimensional circle of length $T$ . Let us suppose that $\Phi^X_{T,0}$ has a fixed point $\bar z$ . I would like to find an explicit example of $X$ such that $\bar z$ is an equilibrium of the system. I have found the sufficient condition $X(\bar z,t)=0$ for every $t$ , but I am still missing the explicit example. Can someone help me?","Consider a dynamical system defined by a non-autonomous ordinary differential equation where is periodic with respect to time of period . Consider the flow evaluated at , i.e., the map , where is a manifold (but let us take an open subset of ) and is the map that assigns to each the value at time of the solution that starts from at time . This map can be interpreted as a Poincaré map in the extended quotient phase space , being the one dimensional circle of length . Let us suppose that has a fixed point . I would like to find an explicit example of such that is an equilibrium of the system. I have found the sufficient condition for every , but I am still missing the explicit example. Can someone help me?"," \dot z = X(z,t) X T > 0 T \Phi^X_{T,0} : M \to M M \mathbb R^n \Phi^X_{T,0} z \in M T z 0 M \times S^1_T S^1_T T \Phi^X_{T,0} \bar z X \bar z X(\bar z,t)=0 t","['ordinary-differential-equations', 'dynamical-systems', 'examples-counterexamples']"
47,Prove that all the eigenvalues of the following Sturm–Liouville problem are positive: $u'' + (\lambda -x^2)u = 0$ $\hspace{0.5cm}$ $0< x < \infty$,Prove that all the eigenvalues of the following Sturm–Liouville problem are positive:,u'' + (\lambda -x^2)u = 0 \hspace{0.5cm} 0< x < \infty,"Prove that all the eigenvalues of the following Sturm–Liouville problem are positive $u'' + (\lambda -x^2)u = 0$ $\hspace{0.5cm}$ $0< x < \infty$ $u'(0) = \lim_{x \to \infty}{u(x)} =0$ I'm trying to solve this Sturm-Liouville problem. But I haven't gotten a solution yet. I tried to solve it analogously to what I did in the following problem: $v'' + \lambda v= 0$ $\hspace{0.5cm} 0<x<L$ $v(0) = v(\pi) = 0$ In this case you can show that if $\lambda = 0$ , then $v(x) = 0$ . Then $\lambda = 0$ is not an eigenvalue of the problem. If $\lambda \neq 0$ , then the general solution to the problem is $v(x) = Ae^{i\sqrt{\lambda}x} + Be^{-i\sqrt{\lambda}x}$ . This solution satisfies the conditions of the problem if and only if \begin{cases} A +B =0\\ Ae^{i\sqrt{\lambda}\pi} + Be^{-i\sqrt{\lambda}\pi}=0 \end{cases} This system has a non-trivial solution if and only if $e^{-i\sqrt{\lambda}\pi} - e^{i\sqrt{\lambda}\pi}=0$ . From this it is obtained that the eigenvalues are $\lambda_n = n^2$ and since $B=-A$ , the eigenfunctions are $v_n{(x)} = 2iA\sin(nx)$ . However, my problem here is different in the sense that the coefficient $\lambda -x^2$ is variable by $x$ . I think that if $\lambda<0$ , let's say $\lambda = -k^2$ , then $u(x) = C_{1} e^{(\sqrt{k^2 + x^2})x} + C_{2}e^{-(\sqrt{k^2 + x^2})x}$ If $\lambda = 0$ , then $u'' -x^2 u = 0 \Rightarrow  u(x) =  C_{1} e^{(x)x} + C_{2}e^{-(x)x}$ If $\lambda> 0$ , let's say $\lambda = k^2$ , then $u(x) = C_{1} e^{(\sqrt{x^2 - k^2})x} + C_{2}e^{-(\sqrt{x^2 - k^2})x}$ Now I'm going to consider these cases and see which of them is possible, however I do not know if this form is correct. How can I solve this problem? I need some help or a way to solve this problem. I would like to know a solution. Any help is appreciated","Prove that all the eigenvalues of the following Sturm–Liouville problem are positive I'm trying to solve this Sturm-Liouville problem. But I haven't gotten a solution yet. I tried to solve it analogously to what I did in the following problem: In this case you can show that if , then . Then is not an eigenvalue of the problem. If , then the general solution to the problem is . This solution satisfies the conditions of the problem if and only if This system has a non-trivial solution if and only if . From this it is obtained that the eigenvalues are and since , the eigenfunctions are . However, my problem here is different in the sense that the coefficient is variable by . I think that if , let's say , then If , then If , let's say , then Now I'm going to consider these cases and see which of them is possible, however I do not know if this form is correct. How can I solve this problem? I need some help or a way to solve this problem. I would like to know a solution. Any help is appreciated",u'' + (\lambda -x^2)u = 0 \hspace{0.5cm} 0< x < \infty u'(0) = \lim_{x \to \infty}{u(x)} =0 v'' + \lambda v= 0 \hspace{0.5cm} 0<x<L v(0) = v(\pi) = 0 \lambda = 0 v(x) = 0 \lambda = 0 \lambda \neq 0 v(x) = Ae^{i\sqrt{\lambda}x} + Be^{-i\sqrt{\lambda}x} \begin{cases} A +B =0\\ Ae^{i\sqrt{\lambda}\pi} + Be^{-i\sqrt{\lambda}\pi}=0 \end{cases} e^{-i\sqrt{\lambda}\pi} - e^{i\sqrt{\lambda}\pi}=0 \lambda_n = n^2 B=-A v_n{(x)} = 2iA\sin(nx) \lambda -x^2 x \lambda<0 \lambda = -k^2 u(x) = C_{1} e^{(\sqrt{k^2 + x^2})x} + C_{2}e^{-(\sqrt{k^2 + x^2})x} \lambda = 0 u'' -x^2 u = 0 \Rightarrow  u(x) =  C_{1} e^{(x)x} + C_{2}e^{-(x)x} \lambda> 0 \lambda = k^2 u(x) = C_{1} e^{(\sqrt{x^2 - k^2})x} + C_{2}e^{-(\sqrt{x^2 - k^2})x},"['ordinary-differential-equations', 'eigenvalues-eigenvectors', 'eigenfunctions', 'sturm-liouville']"
48,How can I solve this differential equation $x(x+1)y' + y = \arctan(x)$?,How can I solve this differential equation ?,x(x+1)y' + y = \arctan(x),"I've been trying to solve this differential equation: $x(x+1)y' + y = \arctan(x)$ and that is a linear differential equation after a small reform: $y' + \frac{1}{x(x+1)} y = \frac{\arctan(x)}{x(x+1)}$ . What I did was try using an integrating factor $u(x) = \exp(\int{\frac{1}{x(x+1)}})$ and I found $u(x)=\exp( − \ln ( ∣ \frac{1}{x} + C|)$ the general solution of the differential equation is expressed as : $y = \frac{{\int {u\left( x \right)f\left( x \right)dx} + C}}{{u\left( x \right)}}$ , where $f(x)=\frac{\arctan(x)}{x(x+1)}$ and $C$ is an arbitrary constant. But I don't think I can do any more than this, is there a way I can get to find it ?","I've been trying to solve this differential equation: and that is a linear differential equation after a small reform: . What I did was try using an integrating factor and I found the general solution of the differential equation is expressed as : , where and is an arbitrary constant. But I don't think I can do any more than this, is there a way I can get to find it ?","x(x+1)y' + y = \arctan(x) y' + \frac{1}{x(x+1)} y = \frac{\arctan(x)}{x(x+1)} u(x) = \exp(\int{\frac{1}{x(x+1)}}) u(x)=\exp(
−
\ln
(
∣
\frac{1}{x}
+
C|) y = \frac{{\int {u\left( x \right)f\left( x \right)dx} + C}}{{u\left( x \right)}} f(x)=\frac{\arctan(x)}{x(x+1)} C","['calculus', 'ordinary-differential-equations']"
49,What is a counterexample to LaSalle invariance principle?,What is a counterexample to LaSalle invariance principle?,,"I realized I have never understood LaSalle's invariance principle because I have never seen a counterexample to it. LaSalle's invariance principle is usually stated as, Let $\Omega \subset D$ be a compact set that is positively invariant with respect to $\dot x = f(x)$ . Let $V: D \subset R$ be a continuously differentiable function such that $\dot V(x) \leq 0$ in $\Omega$ . Let $E$ be the set of all points in $\Omega$ where $\dot  V(x) = 0$ . let $M$ be the largest invariant set in $E$ . Then every solution starting in $\Omega$ approaches $M$ as $t\to \infty$ . For example, is it possible to come up with an example where the largest invariant set in $E$ is the empty set, hence we do not have conclusion? or as a corollary to it Let $x = 0$ be an equilibrium point of $\dot x = f(x)$ . Let $V : D ⊂ R ^n → R$ be a continuously differentiable, positive definite function on $D$ such that $\dot V (x) ≤ 0$ for all $x ∈ D$ . Assume that no solution, other than the trivial solution $x(t) ≡ 0$ , can stay identically in $\{x ∈ D | \dot V (x) = 0\}$ . Then $x = 0$ is asymptotically stable. http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.711.2224&rep=rep1&type=pdf http://www.cds.caltech.edu/archive/help/uploads/wiki/files/237/Lecture2_notes_CDS270.pdf For example, the statement ""Assume that no solution, other than the trivial solution $x(t) ≡ 0$ , can stay identically in $\{x ∈ D | \dot V (x) = 0\}$ ."" is quite vague to me. Can someone come up with an example where there does exist solution, other than $x(t) = 0$ that stays identically in $\{x ∈ D | \dot V (x) = 0\}$ , such that this results in a violation of the theorem? If not, any example that violates LaSalle invariance principle helps!","I realized I have never understood LaSalle's invariance principle because I have never seen a counterexample to it. LaSalle's invariance principle is usually stated as, Let be a compact set that is positively invariant with respect to . Let be a continuously differentiable function such that in . Let be the set of all points in where . let be the largest invariant set in . Then every solution starting in approaches as . For example, is it possible to come up with an example where the largest invariant set in is the empty set, hence we do not have conclusion? or as a corollary to it Let be an equilibrium point of . Let be a continuously differentiable, positive definite function on such that for all . Assume that no solution, other than the trivial solution , can stay identically in . Then is asymptotically stable. http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.711.2224&rep=rep1&type=pdf http://www.cds.caltech.edu/archive/help/uploads/wiki/files/237/Lecture2_notes_CDS270.pdf For example, the statement ""Assume that no solution, other than the trivial solution , can stay identically in ."" is quite vague to me. Can someone come up with an example where there does exist solution, other than that stays identically in , such that this results in a violation of the theorem? If not, any example that violates LaSalle invariance principle helps!","\Omega \subset D \dot x = f(x) V: D \subset R \dot V(x) \leq 0 \Omega E \Omega \dot
 V(x) = 0 M E \Omega M t\to \infty E x = 0 \dot x = f(x) V : D ⊂ R ^n → R D \dot V (x) ≤ 0 x ∈ D x(t) ≡ 0 \{x ∈ D
| \dot V (x) = 0\} x = 0 x(t) ≡ 0 \{x ∈ D | \dot V (x) = 0\} x(t) = 0 \{x ∈ D | \dot V (x) = 0\}","['ordinary-differential-equations', 'proof-explanation', 'dynamical-systems', 'examples-counterexamples', 'lyapunov-functions']"
50,Understanding algorithm to show $(x+\partial_x)e^{-x^2/2}=0$ using python?,Understanding algorithm to show  using python?,(x+\partial_x)e^{-x^2/2}=0,"I am given the following Python code which is supposed to verify numerically that $$(\partial_x + x)e^{-x^2/2}=0.$$ The algorithm does this, by transforming everything into a Fourier basis and then verifying it using the Fourier transform. However, I have some difficulties understanding the mathematical basis of this algorithm. In short, my question is: When the final result is computed as np.dot( D, yl ) , what is that actually in mathematical terms that we are computing here? import numpy as np   ## non-normalized gaussian with sigma=1  def gauss( x ): return np.exp( -x**2 / 2 )    ## interval on which the gaussian is evaluated  L = 10  ## number of sampling points  N = 21  ## sample rate  dl = L / N  ## highest frequency detectable  kmax= 1 / ( 2 * dl )   ## array of x values  xl = np.linspace( -L/2, L/2, N )  ## array of k values  kl = np.linspace( -kmax, kmax, N )   ## matrix of exponents ## the Fourier transform is defined via sum f * exp( -2 pi j k x)   ## i.e. the 2 pi is in the exponent  ## normalization is sqrt(N) where n is the number of sampling points  ## this definition makes it forward-backward symmetric  ## ""outer"" also exists in Matlab and basically does the same exponent = np.outer( -1j * 2 * np.pi * kl, xl )  ## linear operator for the standard Fourier transformation  A = np.exp( exponent ) / np.sqrt( N )   ## nth derivative is given via partial integration as  ( 2 pi j k)^n f(k)  ## every row needs to be multiplied by the according k  B = np.array( [ 1j * 2 * np.pi * kk * An for kk, An in zip( kl, A ) ] )   ## for the part with the linear term, every column needs to be multiplied ## by the according x or--as here---every row is multiplied element   ## wise with the x-vector C = np.array( [ xl * An for An in  A ] )   ## thats the according linear operator  D = B + C   ## the gaussian  yl = gauss( xl )   ## the transformation with the linear operator  print(  np.dot( D, yl ).round( decimals=9 ) )  ## ...results in a zero-vector, as expected","I am given the following Python code which is supposed to verify numerically that The algorithm does this, by transforming everything into a Fourier basis and then verifying it using the Fourier transform. However, I have some difficulties understanding the mathematical basis of this algorithm. In short, my question is: When the final result is computed as np.dot( D, yl ) , what is that actually in mathematical terms that we are computing here? import numpy as np   ## non-normalized gaussian with sigma=1  def gauss( x ): return np.exp( -x**2 / 2 )    ## interval on which the gaussian is evaluated  L = 10  ## number of sampling points  N = 21  ## sample rate  dl = L / N  ## highest frequency detectable  kmax= 1 / ( 2 * dl )   ## array of x values  xl = np.linspace( -L/2, L/2, N )  ## array of k values  kl = np.linspace( -kmax, kmax, N )   ## matrix of exponents ## the Fourier transform is defined via sum f * exp( -2 pi j k x)   ## i.e. the 2 pi is in the exponent  ## normalization is sqrt(N) where n is the number of sampling points  ## this definition makes it forward-backward symmetric  ## ""outer"" also exists in Matlab and basically does the same exponent = np.outer( -1j * 2 * np.pi * kl, xl )  ## linear operator for the standard Fourier transformation  A = np.exp( exponent ) / np.sqrt( N )   ## nth derivative is given via partial integration as  ( 2 pi j k)^n f(k)  ## every row needs to be multiplied by the according k  B = np.array( [ 1j * 2 * np.pi * kk * An for kk, An in zip( kl, A ) ] )   ## for the part with the linear term, every column needs to be multiplied ## by the according x or--as here---every row is multiplied element   ## wise with the x-vector C = np.array( [ xl * An for An in  A ] )   ## thats the according linear operator  D = B + C   ## the gaussian  yl = gauss( xl )   ## the transformation with the linear operator  print(  np.dot( D, yl ).round( decimals=9 ) )  ## ...results in a zero-vector, as expected",(\partial_x + x)e^{-x^2/2}=0.,"['ordinary-differential-equations', 'numerical-methods', 'matlab', 'gaussian']"
51,Proving that the Laguerre polynomials do indeed solve the differential equation,Proving that the Laguerre polynomials do indeed solve the differential equation,,"I am trying to show that the Laguerre differential equation, given in my homework problem as $xL''_n(x)+(1−x)L'_n(x)+ nL_n(x) = 0$ , is indeed solved by the Laguerre polynomials in their closed sum form: $L_n(x)=\sum_{k=0}^n \binom{n}{k}\frac{(-1)^k}{k!} x^k$ I've tried substituting the sum into the equation but I haven't been able to resolve it: $xL''_n(x)+(1−x)L'_n(x)+ nL_n(x)$ $ = x \sum_{k=0}^n \binom{n}{k}\frac{(-1)^k}{k!}k(k-1) x^{k-2} + (1-x)\sum_{k=0}^n \binom{n}{k}\frac{(-1)^k}{k!}k x^{k-1} +n\sum_{k=0}^n \binom{n}{k}\frac{(-1)^k}{k!} x^k$ $=\sum_{k=0}^n \binom{n}{k}\frac{(-1)^k}{k!} [xk(k-1)x^{k-2}+(1-x)kx^{k-1}+nx^k]$ $=\sum_{k=0}^n \binom{n}{k}\frac{(-1)^k}{k!} [(k^2-k)x^{k-1}+kx^{k-1}-kx^k+nx^k]$ $=\sum_{k=0}^n \binom{n}{k}\frac{(-1)^k}{k!} [k^2 x^{k-1}+(n-k)x^k]$ $=\sum_{k=0}^n \binom{n}{k}\frac{(-1)^k}{k!} [x^{k-1}(k^2+(n-k)x)]$ What am I doing wrong? How would I show that this equals zero? Or is this the completely wrong approach? I've seen various similar questions here but they always either work with a different version of the differential equation, or with recurrence relations - which I am hesitant to use, since my homework problem has not introduced the recurrence properties of the Laguerre polynomials so I'd have to derive and prove them before being allowed to use it in my proof. Beyond the closed sum form I am also given the Rodrigues form of the polynomials: ${\displaystyle L_{n}(x)={\frac {\mathrm {e} ^{x}}{n!}}{\frac {\mathrm {d} ^{n}}{\mathrm {d} x^{n}}}{\bigg (}x^{n}\mathrm {e} ^{-x}{\bigg )}}$ But substituting those into the differential equation only leads me to terms with $L_{n+1}$ and $L_{n+2}$ . Can anybody tell me how to approach this? Is there any way to prove that the polynomials in either of these two forms satisfy the above differential equation? Or is there no way around the recurrence relations and shall I prove these first, even though they were mentioned neither in my lecture nor the homework problem?","I am trying to show that the Laguerre differential equation, given in my homework problem as , is indeed solved by the Laguerre polynomials in their closed sum form: I've tried substituting the sum into the equation but I haven't been able to resolve it: What am I doing wrong? How would I show that this equals zero? Or is this the completely wrong approach? I've seen various similar questions here but they always either work with a different version of the differential equation, or with recurrence relations - which I am hesitant to use, since my homework problem has not introduced the recurrence properties of the Laguerre polynomials so I'd have to derive and prove them before being allowed to use it in my proof. Beyond the closed sum form I am also given the Rodrigues form of the polynomials: But substituting those into the differential equation only leads me to terms with and . Can anybody tell me how to approach this? Is there any way to prove that the polynomials in either of these two forms satisfy the above differential equation? Or is there no way around the recurrence relations and shall I prove these first, even though they were mentioned neither in my lecture nor the homework problem?",xL''_n(x)+(1−x)L'_n(x)+ nL_n(x) = 0 L_n(x)=\sum_{k=0}^n \binom{n}{k}\frac{(-1)^k}{k!} x^k xL''_n(x)+(1−x)L'_n(x)+ nL_n(x)  = x \sum_{k=0}^n \binom{n}{k}\frac{(-1)^k}{k!}k(k-1) x^{k-2} + (1-x)\sum_{k=0}^n \binom{n}{k}\frac{(-1)^k}{k!}k x^{k-1} +n\sum_{k=0}^n \binom{n}{k}\frac{(-1)^k}{k!} x^k =\sum_{k=0}^n \binom{n}{k}\frac{(-1)^k}{k!} [xk(k-1)x^{k-2}+(1-x)kx^{k-1}+nx^k] =\sum_{k=0}^n \binom{n}{k}\frac{(-1)^k}{k!} [(k^2-k)x^{k-1}+kx^{k-1}-kx^k+nx^k] =\sum_{k=0}^n \binom{n}{k}\frac{(-1)^k}{k!} [k^2 x^{k-1}+(n-k)x^k] =\sum_{k=0}^n \binom{n}{k}\frac{(-1)^k}{k!} [x^{k-1}(k^2+(n-k)x)] {\displaystyle L_{n}(x)={\frac {\mathrm {e} ^{x}}{n!}}{\frac {\mathrm {d} ^{n}}{\mathrm {d} x^{n}}}{\bigg (}x^{n}\mathrm {e} ^{-x}{\bigg )}} L_{n+1} L_{n+2},"['real-analysis', 'ordinary-differential-equations', 'polynomials', 'quantum-mechanics', 'laguerre-polynomials']"
52,"Question about the proof of Theorem D.5, Introduction to Smooth Manifolds by Lee","Question about the proof of Theorem D.5, Introduction to Smooth Manifolds by Lee",,"I am trying to understand the proof of the following theorem from Lee's ""Introduction to Smooth Manifolds"": The following statement is in the proof: Question: What I don't see is how $\frac{2\epsilon e^{CT}}{E}\cdot(e^{ET}-1)$ can be made as small as desired by choosing $h$ and $\tilde{h}$ sufficiently small. How does the expression depend on $h$ and $\tilde{h}$ ? To me it looks like an upper bound. The only idea I have is the following: There is the following statement Let $k\in\mathbb{N}$ such that $k>\frac{2e^{CT}}{E}\cdot(e^{ET}-1)$ and $\tilde{\epsilon}:=\frac{\epsilon}{k}$ ,  and $\epsilon$ as above. By the above statement I can find a $\tilde{\delta}$ such that $|y_{1}-y_{2}|<\tilde{\delta}$ implies \begin{align} |\frac{\partial V^{i}}{\partial y^{k}}(y_{1})-\frac{\partial V^{i}}{\partial y^{k}}(y_{2})|<\tilde{\epsilon}=\frac{\epsilon}{k} \end{align} Now the book says I would instead say ""Suppose that $h$ and $\tilde{h}$ are both less than $\frac{\tilde{\delta} e^{-CT}}{n}$ "". From here on I would proceed exactly as in the book to end up with the inequality \begin{align} |\Delta_{h}(t,x)-\Delta_{\tilde{h}}(t,x)|\leq...\leq\frac{2\tilde{\epsilon} e^{CT}}{E}\cdot(e^{ET}-1)=\frac{2\epsilon e^{CT}}{k\cdot E}\cdot(e^{ET}-1)<\epsilon \end{align} Does the statement ""choosing $h$ and $\tilde{h}$ sufficiently small  "" refer to the line "" $h$ and $\tilde{h}$ are both less than $\frac{\delta e^{-CT}}{n}$ "" ? I hope this makes any sense. Thank you very much in advance!","I am trying to understand the proof of the following theorem from Lee's ""Introduction to Smooth Manifolds"": The following statement is in the proof: Question: What I don't see is how can be made as small as desired by choosing and sufficiently small. How does the expression depend on and ? To me it looks like an upper bound. The only idea I have is the following: There is the following statement Let such that and ,  and as above. By the above statement I can find a such that implies Now the book says I would instead say ""Suppose that and are both less than "". From here on I would proceed exactly as in the book to end up with the inequality Does the statement ""choosing and sufficiently small  "" refer to the line "" and are both less than "" ? I hope this makes any sense. Thank you very much in advance!","\frac{2\epsilon e^{CT}}{E}\cdot(e^{ET}-1) h \tilde{h} h \tilde{h} k\in\mathbb{N} k>\frac{2e^{CT}}{E}\cdot(e^{ET}-1) \tilde{\epsilon}:=\frac{\epsilon}{k} \epsilon \tilde{\delta} |y_{1}-y_{2}|<\tilde{\delta} \begin{align}
|\frac{\partial V^{i}}{\partial y^{k}}(y_{1})-\frac{\partial V^{i}}{\partial y^{k}}(y_{2})|<\tilde{\epsilon}=\frac{\epsilon}{k}
\end{align} h \tilde{h} \frac{\tilde{\delta} e^{-CT}}{n} \begin{align}
|\Delta_{h}(t,x)-\Delta_{\tilde{h}}(t,x)|\leq...\leq\frac{2\tilde{\epsilon} e^{CT}}{E}\cdot(e^{ET}-1)=\frac{2\epsilon e^{CT}}{k\cdot E}\cdot(e^{ET}-1)<\epsilon
\end{align} h \tilde{h} h \tilde{h} \frac{\delta e^{-CT}}{n}","['ordinary-differential-equations', 'differential-geometry', 'manifolds']"
53,Can conservative systems have unstable equilibrium points?,Can conservative systems have unstable equilibrium points?,,"In general, can conservative systems have a equilibrium(fixed) point which is unstable? If so, I don't know why so in spite of these systems have conserved quantity.","In general, can conservative systems have a equilibrium(fixed) point which is unstable? If so, I don't know why so in spite of these systems have conserved quantity.",,"['ordinary-differential-equations', 'dynamical-systems']"
54,Solving non-autonomous system of differential equations,Solving non-autonomous system of differential equations,,"Given the following 2x2 system of differential equations, $\begin{cases} \dot{x} = 4x-y \\ \dot{y} = 2x+y \end{cases}$ A solution is easy to fins by computing eigenvalues and eigenvectors of the associated matrix A $A = \begin{bmatrix} 4 & -1 \\ 2 & 1 \end{bmatrix}$ . Therefore, knowing that eigenvalues are 2 and 3, and the related eigenvectors are $\begin{bmatrix} 1 \\ 2 \end{bmatrix}$ and $\begin{bmatrix} 1 \\ 1 \end{bmatrix}$ respectively, the solution is of the form $Ae^{2t}\begin{bmatrix} 1 \\ 2 \end{bmatrix}+Be^{3t}\begin{bmatrix} 1 \\ 1 \end{bmatrix}$ . What about if I make the initial system of equations non-autonomous? Resulting in something as follows $\begin{cases} \dot{x} = 4x-y \\ \dot{y} = 2x+y +f(t) \end{cases}$ where f(t) can be either a polynomial on t, an exponential ( $e^{at}$ ) or a trigonometric function ( $sin(at)$ or $cos(at)$ ). What is now the procedure to follow this system of differential equations? For example, let $f(t)=t^2$ . How do I solve the following system of differential equations? $\begin{cases} \dot{x} = 4x-y \\ \dot{y} = 2x+y+t^2 \end{cases}$","Given the following 2x2 system of differential equations, A solution is easy to fins by computing eigenvalues and eigenvectors of the associated matrix A . Therefore, knowing that eigenvalues are 2 and 3, and the related eigenvectors are and respectively, the solution is of the form . What about if I make the initial system of equations non-autonomous? Resulting in something as follows where f(t) can be either a polynomial on t, an exponential ( ) or a trigonometric function ( or ). What is now the procedure to follow this system of differential equations? For example, let . How do I solve the following system of differential equations?",\begin{cases} \dot{x} = 4x-y \\ \dot{y} = 2x+y \end{cases} A = \begin{bmatrix} 4 & -1 \\ 2 & 1 \end{bmatrix} \begin{bmatrix} 1 \\ 2 \end{bmatrix} \begin{bmatrix} 1 \\ 1 \end{bmatrix} Ae^{2t}\begin{bmatrix} 1 \\ 2 \end{bmatrix}+Be^{3t}\begin{bmatrix} 1 \\ 1 \end{bmatrix} \begin{cases} \dot{x} = 4x-y \\ \dot{y} = 2x+y +f(t) \end{cases} e^{at} sin(at) cos(at) f(t)=t^2 \begin{cases} \dot{x} = 4x-y \\ \dot{y} = 2x+y+t^2 \end{cases},"['ordinary-differential-equations', 'systems-of-equations']"
55,"Unique solution of $\dot{x} = f(x), x(0) = x_0$",Unique solution of,"\dot{x} = f(x), x(0) = x_0","I teach myself to solve IVP and could use some help regarding this exercise I found: Let $f:[0,+\infty) \to [0,+\infty)$ be continuous such that $0$ is the only zero of $f$ . Consider the differential equation $\dot{x} = f(x)$ with initial value $x(0) = x_0 \in [0, +\infty)$ . I am asked to show that $\int_{0}^{1} \frac{1}{f(x)}\ \mathrm{d}x = +\infty$ implies that the IVP has a unique solution for $x_0 = 0$ . It's obvious that $x(t) = 0$ is a solution for $x_0 = 0$ but I have no idea how to proof the statement. Are there any important theorems that might help here?",I teach myself to solve IVP and could use some help regarding this exercise I found: Let be continuous such that is the only zero of . Consider the differential equation with initial value . I am asked to show that implies that the IVP has a unique solution for . It's obvious that is a solution for but I have no idea how to proof the statement. Are there any important theorems that might help here?,"f:[0,+\infty) \to [0,+\infty) 0 f \dot{x} = f(x) x(0) = x_0 \in [0, +\infty) \int_{0}^{1} \frac{1}{f(x)}\ \mathrm{d}x = +\infty x_0 = 0 x(t) = 0 x_0 = 0","['calculus', 'ordinary-differential-equations', 'initial-value-problems']"
56,Maximal solution and its domain of $x'(t) = x(t)^{x(t)}$.,Maximal solution and its domain of .,x'(t) = x(t)^{x(t)},"How can I proof that the maximal solution to \begin{align} x'(t) = x(t)^{x(t)}, \quad x(0) = x_0, \end{align} where $x_0 > 0$ and $t \geq 0$ , is not (globally) defined on $\mathbb{R}_{0}^{+}$ ? I am given the hint that it might help to first look at $x_0 > 1$ . Unfortunately, that does not help. I am close to giving up on this one so any help is appreciated.","How can I proof that the maximal solution to where and , is not (globally) defined on ? I am given the hint that it might help to first look at . Unfortunately, that does not help. I am close to giving up on this one so any help is appreciated.","\begin{align}
x'(t) = x(t)^{x(t)}, \quad x(0) = x_0,
\end{align} x_0 > 0 t \geq 0 \mathbb{R}_{0}^{+} x_0 > 1","['ordinary-differential-equations', 'initial-value-problems']"
57,Maximizing the amount of fish caught,Maximizing the amount of fish caught,,"In one of my classes, our teacher introduced us to a software. Using this software, we had to model a lake with fish in it. The growth of the fish population would be governed according to two things: the natural logistic growth rate and fishing. The task was then to maximize the amount fished after a set period of time. This was to learn how to use the software, but just for fun, I wanted to find the absolute maximum. Let $r$ be the logistic growth rate, $k$ be the carrying capacity, $T$ be the total time, and $P_0$ be the starting fish population with $P_0 < k$ . All four of these values are fixed. Let $P(t)$ be the fish population at time $t$ and $f(t)$ be the fishing rate. Then I got that the differential equation for $P$ would be $$\frac{dP}{dt} = \underbrace{rP\left(1 - \frac{P}{k}\right)}_{\text{logistic growth}} - f \tag 1$$ while the amount fished (which is the value to be maximized) is $$\int_0^T f(t)dt \tag 2$$ The main condition is that if $P$ drops to $0$ , then $f$ would also have to drop to $0$ . You can't get fish if there's no fish left! Otherwise, you could make $f$ arbitrarily large. Also, $f$ must be nonnegative: no adding fish to the lake. Trying to solve $(1)$ for an explicit equation for $P$ seems impossible, but I'll show what I tried. Setting it up as $Mdt + NdP = 0$ makes it $$\left( rP \left( 1-\frac{P}{k} \right) - f \right) dt + (-1) dP = 0$$ Multiplying by an integrating factor $u$ , I get that $$r \left( 1- \frac{2P}{k} \right)u = -\frac{\partial u}{\partial t} - \left(  rP\left(1-\frac{P}{k}\right)-f \right) \frac{\partial u}{\partial P}$$ I'm pretty new to differential equations, and I don't know how to solve this (or if it even is solvable). Rearranging $(1)$ , I get that $f = rP\left(1-\frac{P}{k}\right) - \frac{dP}{dt}$ , which means that $(2)$ is $$\int_0^T \left( rP\left(1-\frac{P}{k}\right) - \frac{dP}{dt} \right) dt = r\int_0^T P\left(1-\frac{P}{k}\right)dt - P(T)+P_0$$ Intuitively, it is true that $P(T) = 0$ . If it was greater than $0$ , then more fish could be taken. I'm not sure what to do from here though. One of the challenges with this problem is that it's impossible  to work from the end. It might pay off to let the fish grow more at that instant so that you can fish more later. My questions: $1. $ What is the $f(t)$ that maximizes $(2)$ given that $P$ changes according to $(1)$ , $f(t) \ge 0$ , and if $P = 0$ , then $f = 0$ ? $2. $ Can the problem be modified in some way to get rid of $r, k, T,$ or $P_0$ (i.e. can it be transformed so that one of those can be fixed at $1$ [or any other value])? I think a linear transformation might be useful here, but I don't know. $3. $ I'm doubtful of an exact closed-form solution, so are there approximations of $f(t)$ ?","In one of my classes, our teacher introduced us to a software. Using this software, we had to model a lake with fish in it. The growth of the fish population would be governed according to two things: the natural logistic growth rate and fishing. The task was then to maximize the amount fished after a set period of time. This was to learn how to use the software, but just for fun, I wanted to find the absolute maximum. Let be the logistic growth rate, be the carrying capacity, be the total time, and be the starting fish population with . All four of these values are fixed. Let be the fish population at time and be the fishing rate. Then I got that the differential equation for would be while the amount fished (which is the value to be maximized) is The main condition is that if drops to , then would also have to drop to . You can't get fish if there's no fish left! Otherwise, you could make arbitrarily large. Also, must be nonnegative: no adding fish to the lake. Trying to solve for an explicit equation for seems impossible, but I'll show what I tried. Setting it up as makes it Multiplying by an integrating factor , I get that I'm pretty new to differential equations, and I don't know how to solve this (or if it even is solvable). Rearranging , I get that , which means that is Intuitively, it is true that . If it was greater than , then more fish could be taken. I'm not sure what to do from here though. One of the challenges with this problem is that it's impossible  to work from the end. It might pay off to let the fish grow more at that instant so that you can fish more later. My questions: What is the that maximizes given that changes according to , , and if , then ? Can the problem be modified in some way to get rid of or (i.e. can it be transformed so that one of those can be fixed at [or any other value])? I think a linear transformation might be useful here, but I don't know. I'm doubtful of an exact closed-form solution, so are there approximations of ?","r k T P_0 P_0 < k P(t) t f(t) P \frac{dP}{dt} = \underbrace{rP\left(1 - \frac{P}{k}\right)}_{\text{logistic growth}} - f \tag 1 \int_0^T f(t)dt \tag 2 P 0 f 0 f f (1) P Mdt + NdP = 0 \left( rP \left( 1-\frac{P}{k} \right) - f \right) dt + (-1) dP = 0 u r \left( 1- \frac{2P}{k} \right)u = -\frac{\partial u}{\partial t} - \left( 
rP\left(1-\frac{P}{k}\right)-f \right) \frac{\partial u}{\partial P} (1) f = rP\left(1-\frac{P}{k}\right) - \frac{dP}{dt} (2) \int_0^T \left( rP\left(1-\frac{P}{k}\right) - \frac{dP}{dt} \right) dt = r\int_0^T P\left(1-\frac{P}{k}\right)dt - P(T)+P_0 P(T) = 0 0 1.  f(t) (2) P (1) f(t) \ge 0 P = 0 f = 0 2.  r, k, T, P_0 1 3.  f(t)","['ordinary-differential-equations', 'multivariable-calculus', 'partial-differential-equations', 'optimization', 'calculus-of-variations']"
58,Meaning of a characteristic curve (introductory PDE),Meaning of a characteristic curve (introductory PDE),,"What is the meaning of a characteristic curve when solving PDE? For example, in solving $u_x + yu_y = 0 $ , we get $dy/dx = y/1$ and so solving this ODE, we obtain $y=Ce^x $ . Here, what does $y=Ce^x $ really mean? Does it mean that $x$ and $y$ have to be in the relation $y=Ce^x $ ? Drawing the characteristic curves $y=Ce^x $ , how does it help us determine the solution $u$ ?","What is the meaning of a characteristic curve when solving PDE? For example, in solving , we get and so solving this ODE, we obtain . Here, what does really mean? Does it mean that and have to be in the relation ? Drawing the characteristic curves , how does it help us determine the solution ?",u_x + yu_y = 0  dy/dx = y/1 y=Ce^x  y=Ce^x  x y y=Ce^x  y=Ce^x  u,"['ordinary-differential-equations', 'partial-differential-equations']"
59,Two methods are giving two different answers to the this differential equation : $\frac{dy}{dx}=\frac{1}{2} \frac{d(\sin ^{-1}(f(x))}{dx}$,Two methods are giving two different answers to the this differential equation :,\frac{dy}{dx}=\frac{1}{2} \frac{d(\sin ^{-1}(f(x))}{dx},"$f(x)=\left(\sin \left(\tan ^{-1} x\right)+\sin \left(\cot ^{-1} x\right)\right)^{2}-1,\ |x|>1$ If $\displaystyle\frac{\mathrm{d} y}{\mathrm{d} x}=\frac{1}{2} \frac{\mathrm{d}}{\mathrm{d} x}\left(\sin ^{-1}(f(x))\right)$ and $y(\sqrt{3})=\frac{\pi}{6}$ , then $y(-\sqrt{3})$ is equal to : Options: $1. \quad-\frac{\pi}{6}\\ 2. \qquad \frac{2 \pi}{3}\\ 3. \qquad \frac{5 \pi}{6}\\ 4. \qquad\frac{\pi}{3}$ Now I am getting two Answers in two methods. Can anyone tell me which method is wrong and why? Method - 1 $f(x) = [\sin(\tan^{-1}x) + \sin(\cot^{-1}x) ]^2 -1 $ . Let $\tan^{-1}x = \theta $ . So $f(x) = [\sin(\theta) + \sin(\frac{\pi}{2} - \theta) ]^2 -1 = \sin 2\theta$ . So $f(x) = \sin 2\theta = \frac{2\tan \theta}{1+ \tan^2 \theta} = \frac{2x}{1+x^2}\tag 1$ . Now $\frac{d}{dx} \sin^{-1} \frac{2x}{1+x^2} = \frac{2(1-x^2)}{\sqrt{(1-x^2)^2}(1+x^2)} = \frac{2(1-x^2)}{(1+x^2)(x^2 -1)}$ [Since $|x| > 1$ ]. Now $\frac{dy}{dx} = \frac{1}{2}\frac{d(\sin^{-1}f(x)}{dx}$ . So $\frac{dy}{dx} = \frac{-1}{1+x^2}$ . So $y= - \tan^{-1} x + C$ . Now as $y(\sqrt 3) = \frac{\pi}{6}$ , $C = \frac{\pi }{2}$ . So $y = -\tan^{-1} x + \frac{\pi}{2}$ . So $ \displaystyle   y(-\sqrt 3) =  \frac{\pi}{3} + \frac{\pi}{2} = \frac{5\pi}{6}$ Method -2 - $\displaystyle\frac{\mathrm{d} y}{\mathrm{d} x}=\frac{1}{2} \frac{\mathrm{d}}{\mathrm{d} x}\left(\sin ^{-1}(f(x))\right)$ . So $y = \frac{\sin^{-1}f(x)}{2 } + C$ . Now $f(x) =  \sin (2\tan^{-1} x)$ . SO $\displaystyle y = \frac{\sin^{-1}(\sin (2\tan^{-1} x))}{2 } + C$ . Now as $y(\sqrt 3) = \frac{\pi}{6}$ , $C = 0$ . So $\displaystyle y = \frac{\sin^{-1}(\sin (2\tan^{-1} x))}{2 } $ . So $ \displaystyle y(-\sqrt 3) = \frac{-\pi}{6}$ I am really confused. Why I am getting two answers? Can anyone please help me out?","If and , then is equal to : Options: Now I am getting two Answers in two methods. Can anyone tell me which method is wrong and why? Method - 1 . Let . So . So . Now [Since ]. Now . So . So . Now as , . So . So Method -2 - . So . Now . SO . Now as , . So . So I am really confused. Why I am getting two answers? Can anyone please help me out?","f(x)=\left(\sin \left(\tan ^{-1} x\right)+\sin \left(\cot ^{-1} x\right)\right)^{2}-1,\ |x|>1 \displaystyle\frac{\mathrm{d} y}{\mathrm{d} x}=\frac{1}{2} \frac{\mathrm{d}}{\mathrm{d} x}\left(\sin ^{-1}(f(x))\right) y(\sqrt{3})=\frac{\pi}{6} y(-\sqrt{3}) 1. \quad-\frac{\pi}{6}\\
2. \qquad \frac{2 \pi}{3}\\
3. \qquad \frac{5 \pi}{6}\\
4. \qquad\frac{\pi}{3} f(x) = [\sin(\tan^{-1}x) + \sin(\cot^{-1}x) ]^2 -1  \tan^{-1}x = \theta  f(x) = [\sin(\theta) + \sin(\frac{\pi}{2} - \theta) ]^2 -1 = \sin 2\theta f(x) = \sin 2\theta = \frac{2\tan \theta}{1+ \tan^2 \theta} = \frac{2x}{1+x^2}\tag 1 \frac{d}{dx} \sin^{-1} \frac{2x}{1+x^2} = \frac{2(1-x^2)}{\sqrt{(1-x^2)^2}(1+x^2)} = \frac{2(1-x^2)}{(1+x^2)(x^2 -1)} |x| > 1 \frac{dy}{dx} = \frac{1}{2}\frac{d(\sin^{-1}f(x)}{dx} \frac{dy}{dx} = \frac{-1}{1+x^2} y= - \tan^{-1} x + C y(\sqrt 3) = \frac{\pi}{6} C = \frac{\pi }{2} y = -\tan^{-1} x + \frac{\pi}{2}  \displaystyle   y(-\sqrt 3) =  \frac{\pi}{3} + \frac{\pi}{2} = \frac{5\pi}{6} \displaystyle\frac{\mathrm{d} y}{\mathrm{d} x}=\frac{1}{2} \frac{\mathrm{d}}{\mathrm{d} x}\left(\sin ^{-1}(f(x))\right) y = \frac{\sin^{-1}f(x)}{2 } + C f(x) =  \sin (2\tan^{-1} x) \displaystyle y = \frac{\sin^{-1}(\sin (2\tan^{-1} x))}{2 } + C y(\sqrt 3) = \frac{\pi}{6} C = 0 \displaystyle y = \frac{\sin^{-1}(\sin (2\tan^{-1} x))}{2 }   \displaystyle y(-\sqrt 3) = \frac{-\pi}{6}","['ordinary-differential-equations', 'derivatives', 'solution-verification']"
60,How to choose the order of a Runge-Kutta method?,How to choose the order of a Runge-Kutta method?,,"I have seen that Runge-Kutta's methods are a family of methods used to approximate the solution of an initial value problem. I have also seen that they are classified depending on their order (with the second-order R-K being the Euler's Modified method, and the fourth-order R-K being the most used among them). So, given a first-order ODE $y'=f(x,y)$ with an initial condition $y(x_0)=y_0$ , what is the criteria to follow to choose the order of the Runge-Kutta method to be used?","I have seen that Runge-Kutta's methods are a family of methods used to approximate the solution of an initial value problem. I have also seen that they are classified depending on their order (with the second-order R-K being the Euler's Modified method, and the fourth-order R-K being the most used among them). So, given a first-order ODE with an initial condition , what is the criteria to follow to choose the order of the Runge-Kutta method to be used?","y'=f(x,y) y(x_0)=y_0","['ordinary-differential-equations', 'numerical-methods', 'runge-kutta-methods']"
61,Derivative of $p\left( x \right) = \frac{1}{{\sqrt {2\pi } }}\int\limits_x^\infty {{{\mathop{\rm e}\nolimits} ^{ - \frac{{{u^2}}}{2}}}du} $?,Derivative of ?,p\left( x \right) = \frac{1}{{\sqrt {2\pi } }}\int\limits_x^\infty {{{\mathop{\rm e}\nolimits} ^{ - \frac{{{u^2}}}{2}}}du} ,"I found a result, I have, $p\left( x \right) = \frac{1}{{\sqrt {2\pi } }}\int\limits_x^\infty  {{{\mathop{\rm e}\nolimits} ^{ - \frac{{{u^2}}}{2}}}du} $ $y =  - p\left( x \right)\log p\left( x \right) - (1 - p\left( x \right))\log (1 - p\left( x \right))$ I found a solution for the derivative of $y$ at $x=0$ is, $\frac{{dy\left( 0 \right)}}{{dx}} = \frac{2}{\pi }$ But I could not able to show that. It comes zero for me every time.","I found a result, I have, I found a solution for the derivative of at is, But I could not able to show that. It comes zero for me every time.",p\left( x \right) = \frac{1}{{\sqrt {2\pi } }}\int\limits_x^\infty  {{{\mathop{\rm e}\nolimits} ^{ - \frac{{{u^2}}}{2}}}du}  y =  - p\left( x \right)\log p\left( x \right) - (1 - p\left( x \right))\log (1 - p\left( x \right)) y x=0 \frac{{dy\left( 0 \right)}}{{dx}} = \frac{2}{\pi },"['calculus', 'ordinary-differential-equations', 'derivatives', 'probability-distributions']"
62,Using differential equations to determine the number of rolls on a roll of toilet paper,Using differential equations to determine the number of rolls on a roll of toilet paper,,"Puzzle: A role of toilet paper has $180$ sheets on it. The outside is covered with exactly two sheets. The inside around the cardboard cylinder is covered by exactly one. Question of the puzzle: how many layers of toilet paper are on the roll of toilet paper? The ""Given"" Solution: One way to solve this is by saying that the average round is covered by $1.5$ sheets, so therefore the answer is $120$ (I have no source for whether this is actually correct) I Tried: I tried to solve it with a differential equation, but ultimately failed:  Let $S$ be the number of toilet paper sheets on the roll, and $n$ the number of rotations. I think the number of sheets per rotation depends linearly on the number of rotations at a given point, because with every rotation the toilet role becomes more thick, so: $$\frac{\mathrm{d}S}{\mathrm{d}n}=kn.$$ This equation is separable, so $$dS=(kn)\,\mathrm{d}n.$$ Integrate to get $$S=\frac{1}{2}kn^2+C.$$ Now we need to find the values of constants $k$ and $C$ : we know that when $S=1$ then $n=1$ and also when $S=180$ then $\frac{dS}{dn}=2$ . But now I am stuck. My Question: What would be the correct way of solving this problem using differential equations ? This question is also linked to this question: Toilet paper puzzle (question 2)","Puzzle: A role of toilet paper has sheets on it. The outside is covered with exactly two sheets. The inside around the cardboard cylinder is covered by exactly one. Question of the puzzle: how many layers of toilet paper are on the roll of toilet paper? The ""Given"" Solution: One way to solve this is by saying that the average round is covered by sheets, so therefore the answer is (I have no source for whether this is actually correct) I Tried: I tried to solve it with a differential equation, but ultimately failed:  Let be the number of toilet paper sheets on the roll, and the number of rotations. I think the number of sheets per rotation depends linearly on the number of rotations at a given point, because with every rotation the toilet role becomes more thick, so: This equation is separable, so Integrate to get Now we need to find the values of constants and : we know that when then and also when then . But now I am stuck. My Question: What would be the correct way of solving this problem using differential equations ? This question is also linked to this question: Toilet paper puzzle (question 2)","180 1.5 120 S n \frac{\mathrm{d}S}{\mathrm{d}n}=kn. dS=(kn)\,\mathrm{d}n. S=\frac{1}{2}kn^2+C. k C S=1 n=1 S=180 \frac{dS}{dn}=2","['calculus', 'ordinary-differential-equations', 'puzzle']"
63,Three ODE with exponentials - proof verification,Three ODE with exponentials - proof verification,,"Suppose $a>0$ . I am given the following three equations: $$ \begin{align*} x_1'(t)&=e^{-ax_1(t)}-e^{-ax_2(t)}\\ x_2'(t)&=e^{-ax_2(t)}-e^{-ax_3(t)}\\ x_3'(t)&=e^{-ax_3(t)}, \end{align*} $$ where all three functions $x_i, i=1,2,3$ , are non-negative. The third equation can be solved explicitly (by separation of variables), $$ x_3(t)=\frac{1}{a}(\ln(a)+\ln(t+C)). $$ In particular, $x_3(t)\to\infty$ as $t\to\infty$ . Now I would like to prove that (1) $x_1(t)\to\infty$ and $x_2(t)\to\infty$ as $t\to\infty$ , (2) there exists some $T>0$ such that $x_1(t)<x_2(t)<x_3(t)$ for all $t>T$ . Here is what I tried. Proof of (1) First show that $x_2(t)\to\infty$ as $t\to\infty$ . Suppose by contradiction, that $x_2$ is bounded, i.e. there exists some $M>0$ such that $x_2\leq M$ . Then, $x_2'(t)\geq e^{-aM}-e^{-ax_3(t)}$ and, for $t>0$ large enough, $x_2'(t)>0$ since $e^{-ax_3(t)}\to 0$ . This implies that $x_2$ converges since $x_2(t)$ is bounded from above and, eventually, monotonically increasing. Consequently, $x_2'(t)\to 0$ , implying $x_2(t)\to x_3(t)$ . This contradicts the boundedness of $x_2$ . Thus, $x_2(t)\to\infty$ for $t\to\infty$ . Next, since it is proven that $x_2(t)\to\infty$ , the same argument holds for $x_1(t)$ which concludes the proof. Proof of (2) First show that there exists some $T>0$ such that $x_2(t)<x_3(t)$ for all $t>T$ .  To this end, assume by contradiction that for all $T>0$ there exists some $t'>T$ such that $x_2(t')\geq x_2(t')$ , i.e. $x_2'(t')\leq 0$ . Due to $x_2(t)\to\infty$ there also exists some $t''>t'$ such that $x_2'(t'')>0$ . Since $x_3$ increases monotonically, this implies that $x_2$ oscillates around $x_3$ . However, this is not possible since the nullcline $x_2=x_3$ can be crossed only in one direction. More precisely, the set $\{(x_2,x_3): x_2<x_3\}=\{(x_2,x_3): x_2'>0\}$ is forward invariant. Hence, there exists some $T>0$ such that $x_2'(t)>0$ for all $t>T$ . Now, since $x_2$ increases monotonically for $t>T$ , repeat the same argument to show that there exists some $T'>T$ such that $x_1(t)<x_2(t)$ for all $t>T'$ . All together, statement (2) holds for $T'$ . I am very curious to know if my proofs are okay.","Suppose . I am given the following three equations: where all three functions , are non-negative. The third equation can be solved explicitly (by separation of variables), In particular, as . Now I would like to prove that (1) and as , (2) there exists some such that for all . Here is what I tried. Proof of (1) First show that as . Suppose by contradiction, that is bounded, i.e. there exists some such that . Then, and, for large enough, since . This implies that converges since is bounded from above and, eventually, monotonically increasing. Consequently, , implying . This contradicts the boundedness of . Thus, for . Next, since it is proven that , the same argument holds for which concludes the proof. Proof of (2) First show that there exists some such that for all .  To this end, assume by contradiction that for all there exists some such that , i.e. . Due to there also exists some such that . Since increases monotonically, this implies that oscillates around . However, this is not possible since the nullcline can be crossed only in one direction. More precisely, the set is forward invariant. Hence, there exists some such that for all . Now, since increases monotonically for , repeat the same argument to show that there exists some such that for all . All together, statement (2) holds for . I am very curious to know if my proofs are okay.","a>0 
\begin{align*}
x_1'(t)&=e^{-ax_1(t)}-e^{-ax_2(t)}\\
x_2'(t)&=e^{-ax_2(t)}-e^{-ax_3(t)}\\
x_3'(t)&=e^{-ax_3(t)},
\end{align*}
 x_i, i=1,2,3 
x_3(t)=\frac{1}{a}(\ln(a)+\ln(t+C)).
 x_3(t)\to\infty t\to\infty x_1(t)\to\infty x_2(t)\to\infty t\to\infty T>0 x_1(t)<x_2(t)<x_3(t) t>T x_2(t)\to\infty t\to\infty x_2 M>0 x_2\leq M x_2'(t)\geq e^{-aM}-e^{-ax_3(t)} t>0 x_2'(t)>0 e^{-ax_3(t)}\to 0 x_2 x_2(t) x_2'(t)\to 0 x_2(t)\to x_3(t) x_2 x_2(t)\to\infty t\to\infty x_2(t)\to\infty x_1(t) T>0 x_2(t)<x_3(t) t>T T>0 t'>T x_2(t')\geq x_2(t') x_2'(t')\leq 0 x_2(t)\to\infty t''>t' x_2'(t'')>0 x_3 x_2 x_3 x_2=x_3 \{(x_2,x_3): x_2<x_3\}=\{(x_2,x_3): x_2'>0\} T>0 x_2'(t)>0 t>T x_2 t>T T'>T x_1(t)<x_2(t) t>T' T'","['real-analysis', 'ordinary-differential-equations', 'solution-verification']"
64,2nd-order ODE Solution for when Discriminant = 0,2nd-order ODE Solution for when Discriminant = 0,,"My textbook says the following: Knowing $y(t)=ce^{rt}$ is a solution for $ay''+by'+cy=0$ , we find for the case of $b^2-4ac=0$ that $r=\frac{-b}{2a}$ . Therefore one particular solution is $y(t) = \exp\left(\frac{-b}{2a}t \right)$ . However, another linearly independent solution exists and that is $y(t)=t\exp\left(\frac{-b}{2a}t \right)$ . The derivation to find the second equation is that we can say $y(t)=ce^{rt}=v(t)\exp\left(\frac{-b}{2a}t \right)$ with variation of parameters, and then plug that $v(t)\exp\left(\frac{-b}{2a}t \right)$ into the equation (1) $$ay''+by'+c=0 \tag{1}$$ we find that $v''(t) = 0$ and therefore v(t) is a linear function chosen to be $v(t)=t$ . Hence the 2nd solution is $y(t)=te^{rt}$ and solution space is $y(t)=c_1e^{rt}+c_2te^{rt}$ . I have plugged $v(t)\exp\left(\frac{-b}{2a}t \right)$ many times into equation (1) yet can never seem to get the conclusion that $v''(t) = 0$ . What does my textbook mean by this? CITATION: page 213, Differential Equations & Linear Algebra, Second Edition, by Farlow, Hall, McDill, West","My textbook says the following: Knowing is a solution for , we find for the case of that . Therefore one particular solution is . However, another linearly independent solution exists and that is . The derivation to find the second equation is that we can say with variation of parameters, and then plug that into the equation (1) we find that and therefore v(t) is a linear function chosen to be . Hence the 2nd solution is and solution space is . I have plugged many times into equation (1) yet can never seem to get the conclusion that . What does my textbook mean by this? CITATION: page 213, Differential Equations & Linear Algebra, Second Edition, by Farlow, Hall, McDill, West",y(t)=ce^{rt} ay''+by'+cy=0 b^2-4ac=0 r=\frac{-b}{2a} y(t) = \exp\left(\frac{-b}{2a}t \right) y(t)=t\exp\left(\frac{-b}{2a}t \right) y(t)=ce^{rt}=v(t)\exp\left(\frac{-b}{2a}t \right) v(t)\exp\left(\frac{-b}{2a}t \right) ay''+by'+c=0 \tag{1} v''(t) = 0 v(t)=t y(t)=te^{rt} y(t)=c_1e^{rt}+c_2te^{rt} v(t)\exp\left(\frac{-b}{2a}t \right) v''(t) = 0,"['linear-algebra', 'ordinary-differential-equations']"
65,What is this Lie group and does it have interesting properties?,What is this Lie group and does it have interesting properties?,,"For a fixed positive diagonal matrix $D$ , the set of all real matrices $A$ satisfying $A^T = -D A D^{-1}$ form a Lie algebra with the matrix commutator as the Lie bracket.  Since $$ \left( e^{A} \right)^T = e^{A^T} = e^{-D A D^{-1}} = D e^{-A} D^{-1}, $$ the Lie group consists of matrices $G=e^{A}$ satisfying $$ G^T = D G^{-1} D^{-1} \implies G = (D^{-1})^T (G^{-1})^T D^T $$ Does this Lie group have a special name? Remarks: 1) The Lie algebra corresponds to ODE's whose trajectories are ellipsoids. Please see the post When do the solutions of a linear ODE system lie on ellipses? . 2) Note that in the special case of $D$ being the identity matrix, this Lie algebra is the skew-symmetric matrices and the corresponding Lie group is $SO(n)$ , the group of rotations.","For a fixed positive diagonal matrix , the set of all real matrices satisfying form a Lie algebra with the matrix commutator as the Lie bracket.  Since the Lie group consists of matrices satisfying Does this Lie group have a special name? Remarks: 1) The Lie algebra corresponds to ODE's whose trajectories are ellipsoids. Please see the post When do the solutions of a linear ODE system lie on ellipses? . 2) Note that in the special case of being the identity matrix, this Lie algebra is the skew-symmetric matrices and the corresponding Lie group is , the group of rotations.","D A A^T = -D A D^{-1} 
\left( e^{A} \right)^T = e^{A^T} = e^{-D A D^{-1}} = D e^{-A} D^{-1},
 G=e^{A} 
G^T = D G^{-1} D^{-1} \implies G = (D^{-1})^T (G^{-1})^T D^T
 D SO(n)","['ordinary-differential-equations', 'lie-groups', 'lie-algebras']"
66,Find $a$ such that family of $x^2+ay^2=r^2$ is orthogonal to $y=5x^2$,Find  such that family of  is orthogonal to,a x^2+ay^2=r^2 y=5x^2,"I got a different solution from my professor on this exercise: Find $a>0$ such that the family of ellipses $x^2+ay^2=r^2$ is orthogonal to $y=5x^2$ , with $r>0$ . Solution: $a=2$ . Let me show you what I've done: The family of orthogonal curves to $f(x,y,r)=x^2+ay^2-r^2$ is $$2x-2ay\cdot\frac{dx}{dy}=0 \implies \frac{1}{x}dx=\frac{1}{a}ydy \implies Kx=e^{1/{a}}\cdot{y}$$ Hence $$y=kxe^{1/a}$$ is the family of orthogonal curves to the ellipse. But that's nowhere close to $y=5x^2$ , and of course if I do $a=2$ my solution is not equal to $y=5x^2$ , given that $K$ and $a$ must be constants and cannot depend on $x$ . Therefore I came up with the solution that $y-5x^2=0$ cannot be orthogonal to $x^2+ay^2-r^2=0$ . What am I missing here? Thanks for your time.","I got a different solution from my professor on this exercise: Find such that the family of ellipses is orthogonal to , with . Solution: . Let me show you what I've done: The family of orthogonal curves to is Hence is the family of orthogonal curves to the ellipse. But that's nowhere close to , and of course if I do my solution is not equal to , given that and must be constants and cannot depend on . Therefore I came up with the solution that cannot be orthogonal to . What am I missing here? Thanks for your time.","a>0 x^2+ay^2=r^2 y=5x^2 r>0 a=2 f(x,y,r)=x^2+ay^2-r^2 2x-2ay\cdot\frac{dx}{dy}=0 \implies \frac{1}{x}dx=\frac{1}{a}ydy \implies Kx=e^{1/{a}}\cdot{y} y=kxe^{1/a} y=5x^2 a=2 y=5x^2 K a x y-5x^2=0 x^2+ay^2-r^2=0","['calculus', 'ordinary-differential-equations', 'derivatives']"
67,Sturm-Liouville Problem: Find Eigenfunctions,Sturm-Liouville Problem: Find Eigenfunctions,,"I am looking for eigenfunctions of the Sturm-Liouville Problem: $y'' + \lambda y = 0$ With conditions: $ y'(0) = 0$ $y'(\pi) = y(\pi)$ I found that assuming $\lambda \le 0$ implies $y = 0$ . Supposing $\lambda > 0$ , we have: $y = A\cos(\sqrt{\lambda}x) + B\sin(\sqrt{\lambda}x)$ $y' = -A\sqrt{\lambda}\sin(\sqrt{\lambda}x) + B\sqrt{\lambda}\cos(\sqrt{\lambda}x)$ The first condition implies: $B\sqrt{\lambda} = 0$ $B = 0$ From the second condition: $A\cos(\sqrt{\lambda}\pi) + B\sin(\sqrt{\lambda}\pi) = -A\sqrt{\lambda}\sin(\sqrt{\lambda}\pi) + B\sqrt{\lambda}\cos(\sqrt{\lambda}\pi)$ $A\cos(\sqrt{\lambda}\pi) = -A\sqrt{\lambda}\sin(\sqrt{\lambda}\pi)$ $\cos(\sqrt{\lambda}\pi) = -\sqrt{\lambda}\sin(\sqrt{\lambda}\pi)$ Now I am looking for values of $\lambda$ which satisfy the above equation (hopefully there are infinite such values). Then the corresponding eigenfunctions will take the form $y = \cos(\sqrt{\lambda}x)$ . The problem is that I don't know how to find $\lambda$ . I appreciate any help!","I am looking for eigenfunctions of the Sturm-Liouville Problem: With conditions: I found that assuming implies . Supposing , we have: The first condition implies: From the second condition: Now I am looking for values of which satisfy the above equation (hopefully there are infinite such values). Then the corresponding eigenfunctions will take the form . The problem is that I don't know how to find . I appreciate any help!",y'' + \lambda y = 0  y'(0) = 0 y'(\pi) = y(\pi) \lambda \le 0 y = 0 \lambda > 0 y = A\cos(\sqrt{\lambda}x) + B\sin(\sqrt{\lambda}x) y' = -A\sqrt{\lambda}\sin(\sqrt{\lambda}x) + B\sqrt{\lambda}\cos(\sqrt{\lambda}x) B\sqrt{\lambda} = 0 B = 0 A\cos(\sqrt{\lambda}\pi) + B\sin(\sqrt{\lambda}\pi) = -A\sqrt{\lambda}\sin(\sqrt{\lambda}\pi) + B\sqrt{\lambda}\cos(\sqrt{\lambda}\pi) A\cos(\sqrt{\lambda}\pi) = -A\sqrt{\lambda}\sin(\sqrt{\lambda}\pi) \cos(\sqrt{\lambda}\pi) = -\sqrt{\lambda}\sin(\sqrt{\lambda}\pi) \lambda y = \cos(\sqrt{\lambda}x) \lambda,['ordinary-differential-equations']
68,Polynomial collocation method,Polynomial collocation method,,"I was reading this article ( https://pdfs.semanticscholar.org/6b11/ff0dcf14991f7be50e354d85c932409a01d9.pdf ) on colloaction method, where author has considered the IVP $$y'(t)=f(t,y(t))~~,~~t \in I=[0,T]~~,~~y(0)=y_0$$ Assume $f:I \times \Omega \subset \mathbb{R} \to \mathbb{R}$ is Lipschitz. Now define a mesh $I_h=\{t_n : 0=t_0 <t_1< \ldots <t_N=T\}$ and $h_n=t_{n+1}-t_n$ and $h=\text{max} \{h_n: 0 \le n \le N-1 \}$ which is the step size. Now the solution will be approximated by an element $u_h$ of the polynomial space $$S^{(0)}_m(I_h)=\{v \in C(I): v|_{[t_n,t_{n+1}]} \in \pi_m\}$$ Where $\pi_m$ denotes the space of all real polynomials of degree less than or equal to $m$ . Then author has written  that $dim\,S^{(0)}_m(I_h)\,=Nm+1$ , which im having a little difficulty to understand.Another doubt is does $u_h$ is getting approximated as a polynomial in every single intervals in the mesh or over the whole range ? Also the author has defined a set of collocation points $X_h=\{t=t_n+c_ih_n:0 \le c_1 \le \ldots <c_m \le 1\}$ and its cardinality as $$\begin{align}|X_h|&=Nm ~\text{if}~0 < c_1 < \ldots <c_m \le 1 ,\\&=N(m-1)+1 ~\text{if}~0 < c_1 < \ldots <c_m = 1 ,m \ge 2 \tag{*}\end{align}$$ I'm also having difficulty to understand $(*)$ . Also in general what is the difference between RK methods and Collocation , what extra advantage collocation provide over RK methods ?","I was reading this article ( https://pdfs.semanticscholar.org/6b11/ff0dcf14991f7be50e354d85c932409a01d9.pdf ) on colloaction method, where author has considered the IVP Assume is Lipschitz. Now define a mesh and and which is the step size. Now the solution will be approximated by an element of the polynomial space Where denotes the space of all real polynomials of degree less than or equal to . Then author has written  that , which im having a little difficulty to understand.Another doubt is does is getting approximated as a polynomial in every single intervals in the mesh or over the whole range ? Also the author has defined a set of collocation points and its cardinality as I'm also having difficulty to understand . Also in general what is the difference between RK methods and Collocation , what extra advantage collocation provide over RK methods ?","y'(t)=f(t,y(t))~~,~~t \in I=[0,T]~~,~~y(0)=y_0 f:I \times \Omega \subset \mathbb{R} \to \mathbb{R} I_h=\{t_n : 0=t_0 <t_1< \ldots <t_N=T\} h_n=t_{n+1}-t_n h=\text{max} \{h_n: 0 \le n \le N-1 \} u_h S^{(0)}_m(I_h)=\{v \in C(I): v|_{[t_n,t_{n+1}]} \in \pi_m\} \pi_m m dim\,S^{(0)}_m(I_h)\,=Nm+1 u_h X_h=\{t=t_n+c_ih_n:0 \le c_1 \le \ldots <c_m \le 1\} \begin{align}|X_h|&=Nm ~\text{if}~0 < c_1 < \ldots <c_m \le 1 ,\\&=N(m-1)+1 ~\text{if}~0 < c_1 < \ldots <c_m = 1 ,m \ge 2 \tag{*}\end{align} (*)","['ordinary-differential-equations', 'numerical-methods', 'control-theory', 'runge-kutta-methods']"
69,How to find this ODE solution $f''(x)+x^2f(x)=0$?,How to find this ODE solution ?,f''(x)+x^2f(x)=0,"My idea: For $f(x)\neq 0$ , we have $$ \dfrac{f''(x)}{f(x)}=-x^2. $$ Note that $$ \dfrac{d}{dx}\left( \dfrac{f'(x)}{f(x)} \right)=\dfrac{f''(x)f(x)-[f'(x)]^2}{[f(x)]^2}=\dfrac{f''(x)}{f(x)} - \left( \dfrac{f'(x)}{f(x)} \right)^2. $$ Now let $u(x)=\dfrac{f'(x)}{f(x)}$ . It follows that, $$\dfrac{du}{dx}+u^2=-x^2.$$ But this way all I can do is getting this Ricatti equation.","My idea: For , we have Note that Now let . It follows that, But this way all I can do is getting this Ricatti equation.",f(x)\neq 0  \dfrac{f''(x)}{f(x)}=-x^2.   \dfrac{d}{dx}\left( \dfrac{f'(x)}{f(x)} \right)=\dfrac{f''(x)f(x)-[f'(x)]^2}{[f(x)]^2}=\dfrac{f''(x)}{f(x)} - \left( \dfrac{f'(x)}{f(x)} \right)^2.  u(x)=\dfrac{f'(x)}{f(x)} \dfrac{du}{dx}+u^2=-x^2.,['ordinary-differential-equations']
70,Discrepancy between explicit ODE solution and phase line analysis,Discrepancy between explicit ODE solution and phase line analysis,,"Suppose we have the separable ODE, $$ \frac{dy}{dt} = e^t\frac{y^2-9}{2y}, \;\; y(0) = -5 $$ The explicit solution to this problem is, $$ y = -\sqrt{9 + 16e^{e^t-1}} $$ I'm confused about the behavior of the solution when $t \rightarrow -\infty$ . According to the explicit solution, $y \rightarrow -\sqrt{9 + 16e^{-1}}$ . However, if we do a phase line analysis, we can show that $y = -3$ is an unstable stationary point. So as we go back in time, shouldn't, $y \rightarrow -3$ ? Why do the answers not match? Does phase line analysis not work for non-autonomous equations like above?","Suppose we have the separable ODE, The explicit solution to this problem is, I'm confused about the behavior of the solution when . According to the explicit solution, . However, if we do a phase line analysis, we can show that is an unstable stationary point. So as we go back in time, shouldn't, ? Why do the answers not match? Does phase line analysis not work for non-autonomous equations like above?"," \frac{dy}{dt} = e^t\frac{y^2-9}{2y}, \;\; y(0) = -5   y = -\sqrt{9 + 16e^{e^t-1}}  t \rightarrow -\infty y \rightarrow -\sqrt{9 + 16e^{-1}} y = -3 y \rightarrow -3","['calculus', 'ordinary-differential-equations', 'dynamical-systems']"
71,Rise function (Gomp),Rise function (Gomp),,"I am a biologist trying to understand the ODE of growth tumour: Let $N(t)$ denote the number of cancerous cells at time $t$ , which are proliferating at a rate form $$\frac{dN}{dt}=\frac{\lambda N}{\alpha}\left[1-\left(\frac{N}{\theta}\right)^{\alpha}\right] ... (1)$$ where $\alpha(\geq 0 )$ and $\theta(\geq 0 )$ are determined from the growth characteristics of the tumour. and then it says that the solution to this equation is: $$ N(t) = N_0 \{ \left[ \frac{N_0}{\theta} \right]^{\alpha} + e^{\lambda t} \left( 1-\left[ \frac{N_0}{\theta}\right]^{\alpha}\right)  \}^{-1/\alpha} ...(2)$$ where $N_0 = N(t=0)$ . I still don`t understand how you get this answer, I tried to do the simple form, i.e, solving the Gompertzian growth rate, $$\frac{dN}{dt}= -\lambda N \ ln \left( \frac{N}{\theta}\right) $$ , I solved this using the method of separable variables and with some help hehe My first question is: How can you generalised Gompertzian growth into (1) ? It is not clear for me how you can get (2)? And then the paper at some point consider now the effect of exposing a tumour cell population to a cycle-nonspecific  drug at concentration $C(t)$ , thus it gets a new generalised equation: $$ \frac{1}{N} \ \frac{dN}{dt} = \begin{cases}  \lambda - \mu C(t), \ N \leq N_c,\\ \lambda + \frac{\lambda_1}{\alpha} \left[1- \left(\frac{N}{N_c}\right)^{\alpha}\right]- \mu C(t) , \ N \geq N_c \end{cases}$$ Here my question is why is the meaning of the cases in the paper, for example, biologically talking, what does it mean considering the case $N_0 \geq N_c \lambda - \mu C_0 \geq 0$ ? This is the paper: https://core.ac.uk/download/pdf/82695656.pdf Really, I would appreciate your hints, help, comments to understand this paper. Thank you for your time and help in advance.","I am a biologist trying to understand the ODE of growth tumour: Let denote the number of cancerous cells at time , which are proliferating at a rate form where and are determined from the growth characteristics of the tumour. and then it says that the solution to this equation is: where . I still don`t understand how you get this answer, I tried to do the simple form, i.e, solving the Gompertzian growth rate, , I solved this using the method of separable variables and with some help hehe My first question is: How can you generalised Gompertzian growth into (1) ? It is not clear for me how you can get (2)? And then the paper at some point consider now the effect of exposing a tumour cell population to a cycle-nonspecific  drug at concentration , thus it gets a new generalised equation: Here my question is why is the meaning of the cases in the paper, for example, biologically talking, what does it mean considering the case ? This is the paper: https://core.ac.uk/download/pdf/82695656.pdf Really, I would appreciate your hints, help, comments to understand this paper. Thank you for your time and help in advance.","N(t) t \frac{dN}{dt}=\frac{\lambda N}{\alpha}\left[1-\left(\frac{N}{\theta}\right)^{\alpha}\right] ... (1) \alpha(\geq 0 ) \theta(\geq 0 )  N(t) = N_0 \{ \left[ \frac{N_0}{\theta} \right]^{\alpha} + e^{\lambda t} \left( 1-\left[ \frac{N_0}{\theta}\right]^{\alpha}\right)  \}^{-1/\alpha} ...(2) N_0 = N(t=0) \frac{dN}{dt}= -\lambda N \ ln \left( \frac{N}{\theta}\right)  C(t)  \frac{1}{N} \ \frac{dN}{dt} = \begin{cases}
 \lambda - \mu C(t), \ N \leq N_c,\\
\lambda + \frac{\lambda_1}{\alpha} \left[1- \left(\frac{N}{N_c}\right)^{\alpha}\right]- \mu C(t) , \ N \geq N_c
\end{cases} N_0 \geq N_c \lambda - \mu C_0 \geq 0","['ordinary-differential-equations', 'biology']"
72,Initial value problem blowing up in finite time,Initial value problem blowing up in finite time,,"Lets consider the IVP $\dot x = x^{3} - e^{t^{2}}x^{2}$ with $(t,x) \in (0,\infty) \times \mathbb{R}$ and $x(0)=\xi$ . I want to prove that when $\xi \in [0,1]$ , then $\lim_{t\rightarrow \infty} x(t)=0$ and the solution is defined for all $t\geq 0$ and when $\xi\geq K$ , with $K$ sufficiently big, $x(t)$ blows up in finite time. My main problem is that this is my first encounter with ODEs and is getting really hard for me to figure out the techniques for problem solving. Any help would be really appreciated.","Lets consider the IVP with and . I want to prove that when , then and the solution is defined for all and when , with sufficiently big, blows up in finite time. My main problem is that this is my first encounter with ODEs and is getting really hard for me to figure out the techniques for problem solving. Any help would be really appreciated.","\dot x = x^{3} - e^{t^{2}}x^{2} (t,x) \in (0,\infty) \times \mathbb{R} x(0)=\xi \xi \in [0,1] \lim_{t\rightarrow \infty} x(t)=0 t\geq 0 \xi\geq K K x(t)","['real-analysis', 'ordinary-differential-equations']"
73,Direction of a Point in a Vector Field,Direction of a Point in a Vector Field,,"Consider the system of first order ODEs $$x'=x(y-1) \ \ y'=y(2-x^2-y). \tag{1}$$ For the equilibrium points $(\pm 1,1)$ , these give stable spirals. I am trying to determine the direction of rotation of this spiral. I considered a point, $$(x,y)=(2,2),$$ and by substituting this into $(1)$ , I found that $$x'>0, \  \ y'<0.$$ By constructing an arrow diagram as shown below, I believed that the point $(x,y)=(2,2)$ rotates anticlockwise (and hence the spiral rotates anticlockwise). But the solution in my books states this is in fact a clockwise rotation.","Consider the system of first order ODEs For the equilibrium points , these give stable spirals. I am trying to determine the direction of rotation of this spiral. I considered a point, and by substituting this into , I found that By constructing an arrow diagram as shown below, I believed that the point rotates anticlockwise (and hence the spiral rotates anticlockwise). But the solution in my books states this is in fact a clockwise rotation.","x'=x(y-1) \ \ y'=y(2-x^2-y). \tag{1} (\pm 1,1) (x,y)=(2,2), (1) x'>0, \  \ y'<0. (x,y)=(2,2)","['ordinary-differential-equations', 'vectors', 'dynamical-systems']"
74,Cannot solve $ y = xy'+x^3(y')^2 $,Cannot solve, y = xy'+x^3(y')^2 ,"I'm trying to solve the following differential equation: $$ y = xy'+x^3(y')^2 $$ I have tried almost every method for solving first order differential equations, and did many substitutions $ y=u(x)p(x) $ , using various functions as $ u(x) $ . However, I still cannot find its general solution. I would appreciate any hint.","I'm trying to solve the following differential equation: I have tried almost every method for solving first order differential equations, and did many substitutions , using various functions as . However, I still cannot find its general solution. I would appreciate any hint.", y = xy'+x^3(y')^2   y=u(x)p(x)   u(x) ,['ordinary-differential-equations']
75,Periodic Orbit on a Nonlinear System,Periodic Orbit on a Nonlinear System,,"Let the following ODE system be \begin{align} x'&=x-y+x^2-2x(x^2+y^2)^\frac{1}{2} \\ y'&=x+y+xy-2y(x^2+y^2)^{\frac{1}{2}} \end{align} We can prove easily that $(0,0)$ is the unique equilibrium point. Using the polar coordinates, we can rewrite the system as follows: \begin{align} \rho'&=\rho+\rho^2(-2+\cos\theta) \\ \theta'&=1 \end{align} Working with the system (for $\theta$ is simple, for $\rho$ we must proceed as a Bernoulli equation), we arrange the following solutions: \begin{align} \rho(t)&=\frac{1}{\frac{\cos(\theta_{0}+t)+\sin(\theta_{0}+t)}{2}+2+Ce^{-t}} \\ \theta&=t+\theta_{0} \end{align} Where we can define C for $t=0$ as $C=\frac{1}{\rho_{0}}-2-\frac{\cos\theta_{0}+\sin\theta_{0}}{2}$ The question is, once we have arrived here and we convert into Cartesian coordinates using $\theta_{0}=\arctan(\frac{y_{0}}{x_{0}})$ ; $\rho_{0}=(x_{0}^2+y_{0}^2)^\frac{1}{2}$ , and $x=\rho\cos\theta$ ; $y=\rho\sin\theta$ , how can I find a periodic solution in that system?","Let the following ODE system be We can prove easily that is the unique equilibrium point. Using the polar coordinates, we can rewrite the system as follows: Working with the system (for is simple, for we must proceed as a Bernoulli equation), we arrange the following solutions: Where we can define C for as The question is, once we have arrived here and we convert into Cartesian coordinates using ; , and ; , how can I find a periodic solution in that system?","\begin{align}
x'&=x-y+x^2-2x(x^2+y^2)^\frac{1}{2}
\\
y'&=x+y+xy-2y(x^2+y^2)^{\frac{1}{2}}
\end{align} (0,0) \begin{align}
\rho'&=\rho+\rho^2(-2+\cos\theta)
\\
\theta'&=1
\end{align} \theta \rho \begin{align}
\rho(t)&=\frac{1}{\frac{\cos(\theta_{0}+t)+\sin(\theta_{0}+t)}{2}+2+Ce^{-t}}
\\
\theta&=t+\theta_{0}
\end{align} t=0 C=\frac{1}{\rho_{0}}-2-\frac{\cos\theta_{0}+\sin\theta_{0}}{2} \theta_{0}=\arctan(\frac{y_{0}}{x_{0}}) \rho_{0}=(x_{0}^2+y_{0}^2)^\frac{1}{2} x=\rho\cos\theta y=\rho\sin\theta",['ordinary-differential-equations']
76,Solving an ODE: closed forms of $x(t)$ and $y(t)$,Solving an ODE: closed forms of  and,x(t) y(t),I am given the following system. \begin{split} \overset{.}{x}&= y\\ \overset{.}{y}&= k(1-y^2)^{3/2}e^{-x} \end{split} writing $dx= \overset{.}{x}dt$ and $dy= \overset{.}{y}dt$ we arrive at $$\frac{ydy}{(1-y^2)^{3/2}}= ke^{-x}dx$$ Integrating both sides yields $$(1-y^2)^{-1/2}= ke^{-x}+c.$$ How do I get close forms of $x(t)$ and $y(t)$ in terme of $t$ ?,I am given the following system. writing and we arrive at Integrating both sides yields How do I get close forms of and in terme of ?,"\begin{split}
\overset{.}{x}&= y\\
\overset{.}{y}&= k(1-y^2)^{3/2}e^{-x}
\end{split} dx= \overset{.}{x}dt dy= \overset{.}{y}dt \frac{ydy}{(1-y^2)^{3/2}}= ke^{-x}dx (1-y^2)^{-1/2}= ke^{-x}+c. x(t) y(t) t","['real-analysis', 'calculus', 'ordinary-differential-equations', 'analysis']"
77,Can solution to differential equation be continuous extended?,Can solution to differential equation be continuous extended?,,"Let $v:\mathbb{R}^n\to\mathbb{R}^n$ be continuous. Let $\gamma:[0,T)\to\mathbb{R}^n$ be the unique solution to $\frac{d\gamma}{dt}=v(\gamma),\gamma(0)=p$ . Suppose $\gamma$ remains bounded. Show that $\lim_{t\to T}\gamma(t)$ exists so that $\gamma$ admits a continuous extension to $[0,T]$ . I don't have much idea. I'm thinking about Picard-Lindelof Theorem. Does the converse work? The solution is unique so $v$ is lipschitz? If so, uniform continuity can somehow get to what we want? And how does $\gamma$ being bounded come into play? The class I'm taking is not mainly about differential equations and I don't have much knowledge in it as well. This is like a question for us to play around ourselves. Any help is appreciated!","Let be continuous. Let be the unique solution to . Suppose remains bounded. Show that exists so that admits a continuous extension to . I don't have much idea. I'm thinking about Picard-Lindelof Theorem. Does the converse work? The solution is unique so is lipschitz? If so, uniform continuity can somehow get to what we want? And how does being bounded come into play? The class I'm taking is not mainly about differential equations and I don't have much knowledge in it as well. This is like a question for us to play around ourselves. Any help is appreciated!","v:\mathbb{R}^n\to\mathbb{R}^n \gamma:[0,T)\to\mathbb{R}^n \frac{d\gamma}{dt}=v(\gamma),\gamma(0)=p \gamma \lim_{t\to T}\gamma(t) \gamma [0,T] v \gamma","['real-analysis', 'ordinary-differential-equations']"
78,Largest circle in basin of attraction of the origin.,Largest circle in basin of attraction of the origin.,,"We're given the following dynamical system: $$ \begin{aligned} \dot x &= -x + y + x (x^2 + y^2)\\ \dot y &= -y -2x + y (x^2 + y^2) \end{aligned} $$ What's the largest constant $r_0$ such that the circle $x^2 + y^2 < r_0^2$ lies in the origin's basin of attraction? So far, with relatively easy algebra, I've got: $$ \begin{aligned} \dot r &= \frac{r}{2}(-2-\sin(2 \phi)+2r^2) \\ \dot \phi &= -(1+\cos^2(\phi)) \end{aligned} $$ Which immediately shows $r_0 \geq \sqrt{\frac12}$ . How to show that there is no better bound?","We're given the following dynamical system: What's the largest constant such that the circle lies in the origin's basin of attraction? So far, with relatively easy algebra, I've got: Which immediately shows . How to show that there is no better bound?", \begin{aligned} \dot x &= -x + y + x (x^2 + y^2)\\ \dot y &= -y -2x + y (x^2 + y^2) \end{aligned}  r_0 x^2 + y^2 < r_0^2  \begin{aligned} \dot r &= \frac{r}{2}(-2-\sin(2 \phi)+2r^2) \\ \dot \phi &= -(1+\cos^2(\phi)) \end{aligned}  r_0 \geq \sqrt{\frac12},"['ordinary-differential-equations', 'dynamical-systems', 'polar-coordinates', 'basins-of-attraction']"
79,Generalizing the solution to an ODE,Generalizing the solution to an ODE,,"Is there a way to solve the following ODE for general integral values of $m$ \begin{align} \frac{\partial A(x)}{ \partial x} = -A(x)^m + \frac1x  \label{rec}\tag{1} \end{align} I have some ways to approach this problem for a special case of $m=2$ . For this case, if we substitute $A(x) = \frac{u^\prime(x)}{u(x)}$ , we would get a differential equation of the form, $$u^{\prime\prime}(x) = \frac{u(x)}x$$ And, it is possible to write a solution for this equation in terms of Bessel functions. But I don't know how to generalize this for higher $m$ . Any help would be appreciated!","Is there a way to solve the following ODE for general integral values of I have some ways to approach this problem for a special case of . For this case, if we substitute , we would get a differential equation of the form, And, it is possible to write a solution for this equation in terms of Bessel functions. But I don't know how to generalize this for higher . Any help would be appreciated!","m \begin{align}
\frac{\partial A(x)}{ \partial x} = -A(x)^m + \frac1x  \label{rec}\tag{1}
\end{align} m=2 A(x) = \frac{u^\prime(x)}{u(x)} u^{\prime\prime}(x) = \frac{u(x)}x m","['ordinary-differential-equations', 'bessel-functions']"
80,Numerical solution of ODE with Delta function,Numerical solution of ODE with Delta function,,"I want to model a dynamical system of the form $\frac{\text{d}x}{\text{d}t} = f(x)+nx\delta(\pi(t-0.2)). $ The problem is that I have a point source which is reoccurring at fixed time steps (say at 0.2,1.2,2.2...). How can I handle this numerically?  I have tried to figure out solutions and it seems that there are two ""easy"" approaches: 1) Consider a grid with a fixed step size and define your delta function as 0 on all grid points except the relevant points. However, the influence of the delta function depends strongly on the step size solving this numerically which is why I think this is not correct. 2) Use a Gaussian to represent the Delta-function. This could also work for adaptive step sizes. However, in this case the results depend on the variance of the Gaussian which should be small. Is this a better approach? If this approach works, I could generate an array consisting of the Gaussians at different time steps which is zero elsewhere and use this with normal ODE-solvers, right? The third approach is a little bit nasty in the sense that it cannot be used with ""normal"" ODE solvers. It would be to evaluate f(x) until we approach the point source and take $y(+\epsilon)=e^ny(-\epsilon)$ ( Numerical way to deal with Dirac delta. ). This does not depend on the variance or step size. Should I go with this one? I could use normal ODE-solvers until the fixed time step and then just take the exponential. And do you have any references on that? Thank you very much for your help!","I want to model a dynamical system of the form The problem is that I have a point source which is reoccurring at fixed time steps (say at 0.2,1.2,2.2...). How can I handle this numerically?  I have tried to figure out solutions and it seems that there are two ""easy"" approaches: 1) Consider a grid with a fixed step size and define your delta function as 0 on all grid points except the relevant points. However, the influence of the delta function depends strongly on the step size solving this numerically which is why I think this is not correct. 2) Use a Gaussian to represent the Delta-function. This could also work for adaptive step sizes. However, in this case the results depend on the variance of the Gaussian which should be small. Is this a better approach? If this approach works, I could generate an array consisting of the Gaussians at different time steps which is zero elsewhere and use this with normal ODE-solvers, right? The third approach is a little bit nasty in the sense that it cannot be used with ""normal"" ODE solvers. It would be to evaluate f(x) until we approach the point source and take ( Numerical way to deal with Dirac delta. ). This does not depend on the variance or step size. Should I go with this one? I could use normal ODE-solvers until the fixed time step and then just take the exponential. And do you have any references on that? Thank you very much for your help!","\frac{\text{d}x}{\text{d}t} = f(x)+nx\delta(\pi(t-0.2)).
 y(+\epsilon)=e^ny(-\epsilon)","['ordinary-differential-equations', 'numerical-methods', 'dirac-delta']"
81,Green function for solving ODE: continuity and discontinuity of derivatives,Green function for solving ODE: continuity and discontinuity of derivatives,,"I am studying Green functions and I want to find the Green function $G(x,x_0)$ of the linear operator \begin{equation} \mathcal{L}=\left(\frac{d^n}{dx^n}+a_1(x)\frac{d^{n-1}}{dx^{n-1}}+\dots+a_n(x)  \right) \end{equation} This means solving the following problem \begin{equation} \frac{d^n G}{dx^n}+a_1(x)\frac{d^{n-1}G}{dx^{n-1}}+\dots+a_n(x)G=\delta(x-x_0) \tag{1} \end{equation} I understand that I have to impose continuity in $x_0$ for all the derivatives, up to $n-2$ , and impose the jump discontinuity in $x_0$ for the $(n-1)$ -th derivative. I don't exactly get why this is the case. I have searched in some lecture notes and found the following clues: First approach They proceed by integrating equation $(1)$ , and they say ""we obtain $G^{(n-1)}=H(x-x_0)$ + some continuous functions. The $(n-1)$ -th derivative is not continuous, but suffers a discontinuous jump there. Integrating again shows that $G^{(n−2)}$ is continuous"". My problem here is the following:  how can I say that the result of integrating equation $(1)$ gets me ""some continuous functions"" if I don't know anything about the continuity of $G(x, x_0)$ and its derivatives? Second approach I found it for a $2^{nd}$ order ODE but I assume it can be generalized. It proceeds by saying ""Suppose first that $G(x,x_0)$ was discontinuous at $x=x_0$ , with the discontinuity modelled by a step function. Then $G' \propto \delta(x-x_0)$ and consequently $G''\propto \delta(x−x_0)$ . However, the form of the ODE shows that $\mathcal{L}G$ involves no generalized functions beyond $\delta(x-x_0)$ , thus $G(x, ξ)$ must be continuous."" He THEN proceeds in integrating the ODE in a small neighborhood of $x_0$ , and  he asserts that ""Also, since $G$ is continuous, $G'$ must be bounded so the term $G'$ also cannot contribute as the integration region shrinks to zero size"". He then concludes that it's only the second derivative that contributes to the magnitude 1 jump. My problems here: 1) we started by modeling the discontinuity of $G$ by a step function, noticed that there was a contradiction and consequently stated that ""so the function must be continuous"". But isn't it a bit restrictive, to just assume a jump discontinuity? 2) Can this be generalized for a generic $n$ -order equation? My questions To resume, I would like to know: 1) What is the correct way to proceed to demonstrate that $G$ and its derivatives up to the $n-2$ -th are continuous in $x_0$ and its $(n-1)$ -th derivative has a jump discontinuity 2) As a side question: if the $(n-1)$ -th derivative of the function is discontinuous in $x_0$ how can $G^{n}$ exist in $x_0$ ? I'm a physicist not a mathematician, so please be kind =)","I am studying Green functions and I want to find the Green function of the linear operator This means solving the following problem I understand that I have to impose continuity in for all the derivatives, up to , and impose the jump discontinuity in for the -th derivative. I don't exactly get why this is the case. I have searched in some lecture notes and found the following clues: First approach They proceed by integrating equation , and they say ""we obtain + some continuous functions. The -th derivative is not continuous, but suffers a discontinuous jump there. Integrating again shows that is continuous"". My problem here is the following:  how can I say that the result of integrating equation gets me ""some continuous functions"" if I don't know anything about the continuity of and its derivatives? Second approach I found it for a order ODE but I assume it can be generalized. It proceeds by saying ""Suppose first that was discontinuous at , with the discontinuity modelled by a step function. Then and consequently . However, the form of the ODE shows that involves no generalized functions beyond , thus must be continuous."" He THEN proceeds in integrating the ODE in a small neighborhood of , and  he asserts that ""Also, since is continuous, must be bounded so the term also cannot contribute as the integration region shrinks to zero size"". He then concludes that it's only the second derivative that contributes to the magnitude 1 jump. My problems here: 1) we started by modeling the discontinuity of by a step function, noticed that there was a contradiction and consequently stated that ""so the function must be continuous"". But isn't it a bit restrictive, to just assume a jump discontinuity? 2) Can this be generalized for a generic -order equation? My questions To resume, I would like to know: 1) What is the correct way to proceed to demonstrate that and its derivatives up to the -th are continuous in and its -th derivative has a jump discontinuity 2) As a side question: if the -th derivative of the function is discontinuous in how can exist in ? I'm a physicist not a mathematician, so please be kind =)","G(x,x_0) \begin{equation}
\mathcal{L}=\left(\frac{d^n}{dx^n}+a_1(x)\frac{d^{n-1}}{dx^{n-1}}+\dots+a_n(x)  \right)
\end{equation} \begin{equation}
\frac{d^n G}{dx^n}+a_1(x)\frac{d^{n-1}G}{dx^{n-1}}+\dots+a_n(x)G=\delta(x-x_0)
\tag{1}
\end{equation} x_0 n-2 x_0 (n-1) (1) G^{(n-1)}=H(x-x_0) (n-1) G^{(n−2)} (1) G(x, x_0) 2^{nd} G(x,x_0) x=x_0 G' \propto \delta(x-x_0) G''\propto \delta(x−x_0) \mathcal{L}G \delta(x-x_0) G(x, ξ) x_0 G G' G' G n G n-2 x_0 (n-1) (n-1) x_0 G^{n} x_0","['ordinary-differential-equations', 'greens-function']"
82,Demonstrate that solution of differential equation is bounded,Demonstrate that solution of differential equation is bounded,,"Let $b(t) \in C^1([0,+\infty))$ . I have to find a formula for the solution of this Cauchy's problem: $$ \left\{\begin{aligned} x''(t)+x(t)&=b(t) \\ x(0)&=x_0\\ x'(0)&=x_1 \end{aligned}\right. $$ I have solved this part of the question and the formula is: $$x(t) = x_0\cos(t)+x_1\sin(t) -\cos(t)\int_0^tb(s)\sin(s)\,ds+\sin(t)\int_0^tb(s)\cos(s)\,ds.$$ Then the problem asks to demonstrate that if $b(t)$ is bounded and monotone also $x(t)$ is bounded. It also asks to find a counterexample if $b(t)$ is bounded but not monotone. I can solve the second part by picking for example $b(t) = \cos(t)$ but I'm not able to demonstrate the first fact. I have tried to estimate $|x(t)|$ but the only inequality I come up with is $|x(t)| \leq |x_0|+|x_1|+2Mt$ where $M:=\max_{[0,+\infty)}|b(t)|.$ Thank you in advance for your help.",Let . I have to find a formula for the solution of this Cauchy's problem: I have solved this part of the question and the formula is: Then the problem asks to demonstrate that if is bounded and monotone also is bounded. It also asks to find a counterexample if is bounded but not monotone. I can solve the second part by picking for example but I'm not able to demonstrate the first fact. I have tried to estimate but the only inequality I come up with is where Thank you in advance for your help.,"b(t) \in C^1([0,+\infty)) 
\left\{\begin{aligned}
x''(t)+x(t)&=b(t) \\
x(0)&=x_0\\
x'(0)&=x_1
\end{aligned}\right.
 x(t) = x_0\cos(t)+x_1\sin(t) -\cos(t)\int_0^tb(s)\sin(s)\,ds+\sin(t)\int_0^tb(s)\cos(s)\,ds. b(t) x(t) b(t) b(t) = \cos(t) |x(t)| |x(t)| \leq |x_0|+|x_1|+2Mt M:=\max_{[0,+\infty)}|b(t)|.","['ordinary-differential-equations', 'estimation', 'cauchy-problem']"
83,How to solve this ode $xy''-(1+x)y'+y=x^2$?,How to solve this ode ?,xy''-(1+x)y'+y=x^2,"Find the general solution of $xy''-(1+x)y'+y=x^2$ knowing that the homogeneous equation has the following solution: $e^{ax}$ , where $a$ is a parameter you have to find. I have found that $a=1$ or $a=1/x$ .","Find the general solution of knowing that the homogeneous equation has the following solution: , where is a parameter you have to find. I have found that or .",xy''-(1+x)y'+y=x^2 e^{ax} a a=1 a=1/x,['ordinary-differential-equations']
84,Conditions when solutions of $\dot{x} = f(x)$ exist for all time,Conditions when solutions of  exist for all time,\dot{x} = f(x),"I am reading the following textbook: Introduction to Applied Nonlinear Dynamical Systems and Chaos by Wiggins  p.92 (top) Consider $$\dot{x} = f(x)$$ where $f(x)$ is $C^r$ , $r\geq 1$ , on some open set $U\in \mathbb{R}^n$ . Suppose that the solutions exist for all time (Leave it as an exercise to make the necessary modifications when solutions exist only on finite time intervals). My problem is what are the conditions to make solutions exist for all time. Consider and example: $$\dot{x} = x^2, \ \ \ \ x\in \mathbb{R}$$ the solution through $x_0$ at $t = 0$ is $$x(t,0,x_0) = \frac{-x_0}{x_0t - 1}$$ this solution (trajectory) does not exist for all time, since it becomes infinite at $t = 1/x_0$ . So what are the conditions to make solution exist for all time","I am reading the following textbook: Introduction to Applied Nonlinear Dynamical Systems and Chaos by Wiggins  p.92 (top) Consider where is , , on some open set . Suppose that the solutions exist for all time (Leave it as an exercise to make the necessary modifications when solutions exist only on finite time intervals). My problem is what are the conditions to make solutions exist for all time. Consider and example: the solution through at is this solution (trajectory) does not exist for all time, since it becomes infinite at . So what are the conditions to make solution exist for all time","\dot{x} = f(x) f(x) C^r r\geq 1 U\in \mathbb{R}^n \dot{x} = x^2, \ \ \ \ x\in \mathbb{R} x_0 t = 0 x(t,0,x_0) = \frac{-x_0}{x_0t - 1} t = 1/x_0","['ordinary-differential-equations', 'dynamical-systems']"
85,Using variable spearation to solve a linear ODE of first order,Using variable spearation to solve a linear ODE of first order,,"Suppose I want to solve the ODE: $$f'(x) + 2f(x) = 3$$ I want to use variable separation, so I get: $$ f'(x) = 3 - 2f(x)$$ Now I want to divide by $3-2f(x)$ but I am unsure what I need to assume on $f$ in order to do that. That is, does that expression have to be non zero for all x, or only for a certain x? After that point I know what to do but I don't understand what I am supposed to do here. thanks","Suppose I want to solve the ODE: I want to use variable separation, so I get: Now I want to divide by but I am unsure what I need to assume on in order to do that. That is, does that expression have to be non zero for all x, or only for a certain x? After that point I know what to do but I don't understand what I am supposed to do here. thanks",f'(x) + 2f(x) = 3  f'(x) = 3 - 2f(x) 3-2f(x) f,['ordinary-differential-equations']
86,Finding $\beta$ where $u_x(0)=\beta$ is a boundary value of a heat equation,Finding  where  is a boundary value of a heat equation,\beta u_x(0)=\beta,"Suppose an ice core are perfectly insulated and heated at one end at a rate $\alpha$ and cooled from the other end at a rate $\beta$ . Consider the following boundary value problem $$u_t-u_{xx}=0, \ u_x(0)=\beta, \ u_x(l)=\alpha,$$ where $u$ is temperature, $t$ is time and $l$ is the length of the core. What should $\beta$ be such that the ice cores temperature remains stable ( $u_t=0$ )? My attempt: If $u_t=0$ , then this implies $u_{xx}=0$ . if we integrate both sides with respect to $x$ , we get $$u_x=A$$ for some $A\in\mathbb{R}$ . So, $$u_x(0)=\beta\implies A=\beta,\\u_x(l)=\alpha\implies A=\alpha.$$ Hence, we conclude that $\beta=\alpha\in\mathbb{R}$ . For part (b), assuming $\alpha=1, l=10$ and the average temperature of the core is $-15$ , what is the solution for $u$ in the stable case. Finally, If the cooling mechanism were to fail ( $\beta=0$ ) how long would it take before the ice core started to melt (i.e. when would $u$ rise above $0$ at any point)?","Suppose an ice core are perfectly insulated and heated at one end at a rate and cooled from the other end at a rate . Consider the following boundary value problem where is temperature, is time and is the length of the core. What should be such that the ice cores temperature remains stable ( )? My attempt: If , then this implies . if we integrate both sides with respect to , we get for some . So, Hence, we conclude that . For part (b), assuming and the average temperature of the core is , what is the solution for in the stable case. Finally, If the cooling mechanism were to fail ( ) how long would it take before the ice core started to melt (i.e. when would rise above at any point)?","\alpha \beta u_t-u_{xx}=0, \ u_x(0)=\beta, \ u_x(l)=\alpha, u t l \beta u_t=0 u_t=0 u_{xx}=0 x u_x=A A\in\mathbb{R} u_x(0)=\beta\implies A=\beta,\\u_x(l)=\alpha\implies A=\alpha. \beta=\alpha\in\mathbb{R} \alpha=1, l=10 -15 u \beta=0 u 0","['ordinary-differential-equations', 'proof-verification']"
87,First order non linear ODE with Bernoulli,First order non linear ODE with Bernoulli,,"I have a problem with this equation: $ y'(x)-xy(x)=-xy^4(x) $ with initial condition $ y(x_{0})=y_{0}$. I'm arrived to prove that $ y_{0}= (Ce^{-\frac{3}{2}x_{0}^{2}}+1)^{-3} $ but now i can't move on. Moreover, WolframAlpha's solution is next to impossible… Thanks for any help!","I have a problem with this equation: $ y'(x)-xy(x)=-xy^4(x) $ with initial condition $ y(x_{0})=y_{0}$. I'm arrived to prove that $ y_{0}= (Ce^{-\frac{3}{2}x_{0}^{2}}+1)^{-3} $ but now i can't move on. Moreover, WolframAlpha's solution is next to impossible… Thanks for any help!",,['ordinary-differential-equations']
88,"The solutions of $(y')^2=P(y)$ with $\deg P \in \{3,4\}$.",The solutions of  with .,"(y')^2=P(y) \deg P \in \{3,4\}","Here is some background on my question. The Weierstraß function (as well as its translates) $\wp(x)$ solves the implicit first-order ODE $$(y')^2=4y^3-g_2y-g_3.$$ Differentiating gives the second-order explicit ODE $$y''=12y^2-g_2, $$ in which $g_3$ makes no appearance. $\wp$ can be used to express the solution of any first-order equation of the form $$(y')^2=a_3 y^3+a_2 y^2+a_1 y+a_0, $$ or equivalently, any second-order equation of the form $$y''=b_2 y^2+b_1 y+b_0. $$ In both cases, all it takes is a simple change of variables to remove one of the powers, and get the leading coefficient right. My question is about differential equations whose RHS is one degree larger, that is $$(y')^2=a_4 y^4+a_3y^3+a_2y^2+a_1 y+a_0 , \tag{1}$$ or $$y''=b_3 y^3+b_2y^2+b_1 y+b_0. \tag{2} $$ In some special cases, such as $$y''=2k^2 y^3-(1+k^2) y, $$ and  $$y''=-2y^3+(2-k^2)y,$$ the Jacobi elliptic functions $\operatorname{sn},\operatorname{dn}$ play a role in the solution. Here is my question: For which values of the coefficients in $(1)$ and $(2)$ can the ODEs be solved explicitly in terms of special functions? What are the solutions in those cases? Thank you!","Here is some background on my question. The Weierstraß function (as well as its translates) $\wp(x)$ solves the implicit first-order ODE $$(y')^2=4y^3-g_2y-g_3.$$ Differentiating gives the second-order explicit ODE $$y''=12y^2-g_2, $$ in which $g_3$ makes no appearance. $\wp$ can be used to express the solution of any first-order equation of the form $$(y')^2=a_3 y^3+a_2 y^2+a_1 y+a_0, $$ or equivalently, any second-order equation of the form $$y''=b_2 y^2+b_1 y+b_0. $$ In both cases, all it takes is a simple change of variables to remove one of the powers, and get the leading coefficient right. My question is about differential equations whose RHS is one degree larger, that is $$(y')^2=a_4 y^4+a_3y^3+a_2y^2+a_1 y+a_0 , \tag{1}$$ or $$y''=b_3 y^3+b_2y^2+b_1 y+b_0. \tag{2} $$ In some special cases, such as $$y''=2k^2 y^3-(1+k^2) y, $$ and  $$y''=-2y^3+(2-k^2)y,$$ the Jacobi elliptic functions $\operatorname{sn},\operatorname{dn}$ play a role in the solution. Here is my question: For which values of the coefficients in $(1)$ and $(2)$ can the ODEs be solved explicitly in terms of special functions? What are the solutions in those cases? Thank you!",,"['ordinary-differential-equations', 'special-functions', 'closed-form', 'elliptic-functions']"
89,Limit of a state-transition matrix,Limit of a state-transition matrix,,"Let $\omega$ be a positive real number and let $A,B\in\mathbb{R}^{n\times n}$ be $n\times n$ real matrices. Consider the following linear time-varying dynamical system $$ \dot{x}(t) = (A + \cos(\omega t)B)x(t),\quad x(0)=x_0\in\mathbb{R}^{n}. $$ Let $\Phi(t,0)$ denote the state-transition matrix of the above system. My question. Is it true that   $$ \lim_{\omega \to \infty} \Phi(t,0) = e^{At} \ \ \ ? $$ Some remarks. If matrices $A$ and $B$ commute, this is true. Indeed, in this case it holds $$ \Phi(t,0) = e^{\int_0^t A + \cos(\omega \tau)B\, \mathrm{d}\tau}. $$ In the non-commuting case, I didn't manage to prove this. The main issue here is that a closed-form expression of $\Phi(t,0)$ does not exist, apparently. The only (perhaps useful) idea that I had so far is to exploit the Peano-Baker expansion of $\Phi(t,0)$. However, even with this tool, I couldn't provide an answer to my question. Thanks for your help!","Let $\omega$ be a positive real number and let $A,B\in\mathbb{R}^{n\times n}$ be $n\times n$ real matrices. Consider the following linear time-varying dynamical system $$ \dot{x}(t) = (A + \cos(\omega t)B)x(t),\quad x(0)=x_0\in\mathbb{R}^{n}. $$ Let $\Phi(t,0)$ denote the state-transition matrix of the above system. My question. Is it true that   $$ \lim_{\omega \to \infty} \Phi(t,0) = e^{At} \ \ \ ? $$ Some remarks. If matrices $A$ and $B$ commute, this is true. Indeed, in this case it holds $$ \Phi(t,0) = e^{\int_0^t A + \cos(\omega \tau)B\, \mathrm{d}\tau}. $$ In the non-commuting case, I didn't manage to prove this. The main issue here is that a closed-form expression of $\Phi(t,0)$ does not exist, apparently. The only (perhaps useful) idea that I had so far is to exploit the Peano-Baker expansion of $\Phi(t,0)$. However, even with this tool, I couldn't provide an answer to my question. Thanks for your help!",,"['ordinary-differential-equations', 'analysis', 'limits', 'dynamical-systems']"
90,Techniques to prove that solutions of ODE are defined for all time.,Techniques to prove that solutions of ODE are defined for all time.,,"What are the main techniques for proving that the solutions of ODE are defined for all times? Obviously I refer to qualitative study of $$y'=f(t,y)$$ with $f:\mathbb{R}\times\mathbb{R}^n\to\mathbb{R}^n $ at least $C^0$. The standard way is to use classical reults (e.g. prove that $f$ is sublinear or globally lipschitz) but these rusults are often not applicable. In this case I try to show that the solutions are bounded (it is known that if the orbits are bounded then the solutions are defined for all time ). To do this I usually try one of the following techniques: I look for a constant of the motion and I study the level sets to understand the shape of the orbits (and see if they are bounded); I find a particular solution that is a closed and curve (e.g. a circle): in fact for the uniqueness the orbits can not intersect and therefore all orbits that start from inside this closed curve will have to remain there and will therefore be limited; I study the function $h(t): = x^2(t) + y^2(t)$: I make the derivative and I try to show that it is limited; I try a change of variables (e.g. polar coordinates) that simplifies the equations. In conclusion: 1) Do you know other general techniques to show that the orbits are bounded? 2) However, boundedness is only a sufficient condition to prove that solutions are defined for all time! In case the solutions are unbounded how can I prove that they are defined for all time? For example consider $$\begin{cases}\dot x=-y^2\\ \dot y = x^2\end{cases}$$ or $$\begin{cases}\dot x=y^2\\ \dot y = x^2\end{cases}$$ The level sets of the constants of motion ($E:=x^3\pm y^3$) of these system are open and unbounded curve. In this case the only techniques that I know is to cumpute $t=\int^{+\infty}_{x_0}...dx$ and understand if this integral converges or diverges (for other details see the end of this page: Qualitative study of $ \dot x = - y^2$, $\dot y= x^2 $ ). 3) Advice on books and readings about this problem are welcome.","What are the main techniques for proving that the solutions of ODE are defined for all times? Obviously I refer to qualitative study of $$y'=f(t,y)$$ with $f:\mathbb{R}\times\mathbb{R}^n\to\mathbb{R}^n $ at least $C^0$. The standard way is to use classical reults (e.g. prove that $f$ is sublinear or globally lipschitz) but these rusults are often not applicable. In this case I try to show that the solutions are bounded (it is known that if the orbits are bounded then the solutions are defined for all time ). To do this I usually try one of the following techniques: I look for a constant of the motion and I study the level sets to understand the shape of the orbits (and see if they are bounded); I find a particular solution that is a closed and curve (e.g. a circle): in fact for the uniqueness the orbits can not intersect and therefore all orbits that start from inside this closed curve will have to remain there and will therefore be limited; I study the function $h(t): = x^2(t) + y^2(t)$: I make the derivative and I try to show that it is limited; I try a change of variables (e.g. polar coordinates) that simplifies the equations. In conclusion: 1) Do you know other general techniques to show that the orbits are bounded? 2) However, boundedness is only a sufficient condition to prove that solutions are defined for all time! In case the solutions are unbounded how can I prove that they are defined for all time? For example consider $$\begin{cases}\dot x=-y^2\\ \dot y = x^2\end{cases}$$ or $$\begin{cases}\dot x=y^2\\ \dot y = x^2\end{cases}$$ The level sets of the constants of motion ($E:=x^3\pm y^3$) of these system are open and unbounded curve. In this case the only techniques that I know is to cumpute $t=\int^{+\infty}_{x_0}...dx$ and understand if this integral converges or diverges (for other details see the end of this page: Qualitative study of $ \dot x = - y^2$, $\dot y= x^2 $ ). 3) Advice on books and readings about this problem are welcome.",,"['ordinary-differential-equations', 'analysis', 'reference-request', 'dynamical-systems', 'alternative-proof']"
91,PDE : $x^2 z_x + y^2 z_y = z(x+y)$,PDE :,x^2 z_x + y^2 z_y = z(x+y),"Solving the PDE $$x^2 z_x + y^2 z_y = z(x+y)$$ I came across an error in my calculations which I cannot find : $$\frac{\mathrm{d}x}{x^2}=\frac{\mathrm{d}y}{y^2}=\frac{\mathrm{d}z}{z(x+y)}$$ The first integral curve is given as : $$\frac{\mathrm{d}x}{x^2}=\frac{\mathrm{d}y}{y^2}\implies z_1 = \frac{1}{x} - \frac{1}{y}$$ Now, for the second integral curve, I use the following differential subtraction trick to get rid of the $(x+y)$ term : $$\frac{\mathrm{d}x-\mathrm{d}y}{x^2-y^2}=\frac{\mathrm{d}z}{z(x+y)}$$ $$\Rightarrow$$ $$\frac{\mathrm{d}(x-y)}{x-y} = \frac{\mathrm{d}z}{z}$$ $$\implies$$ $$z_2 = \frac{z}{x-y}$$ And thus the general solution is : $$z_2=F(z_1)\Rightarrow z(x,y)=(x-y)F\bigg(\frac{1}{x}-\frac{1}{y}\bigg)$$ where $F$ is a $C^1$ function of $x$ and $y$. Wolfram Alpha though, states that the solution is $z(x,y)=xyF(1/x-1/y$), which means that I have found the integral curve $z_2$ wrong. I cannot find any fault in my calculations though. The correct integral curve should be : $$z_2 = \frac{z}{xy}$$ How would one come to this calculation though ? I would really appreciate your help","Solving the PDE $$x^2 z_x + y^2 z_y = z(x+y)$$ I came across an error in my calculations which I cannot find : $$\frac{\mathrm{d}x}{x^2}=\frac{\mathrm{d}y}{y^2}=\frac{\mathrm{d}z}{z(x+y)}$$ The first integral curve is given as : $$\frac{\mathrm{d}x}{x^2}=\frac{\mathrm{d}y}{y^2}\implies z_1 = \frac{1}{x} - \frac{1}{y}$$ Now, for the second integral curve, I use the following differential subtraction trick to get rid of the $(x+y)$ term : $$\frac{\mathrm{d}x-\mathrm{d}y}{x^2-y^2}=\frac{\mathrm{d}z}{z(x+y)}$$ $$\Rightarrow$$ $$\frac{\mathrm{d}(x-y)}{x-y} = \frac{\mathrm{d}z}{z}$$ $$\implies$$ $$z_2 = \frac{z}{x-y}$$ And thus the general solution is : $$z_2=F(z_1)\Rightarrow z(x,y)=(x-y)F\bigg(\frac{1}{x}-\frac{1}{y}\bigg)$$ where $F$ is a $C^1$ function of $x$ and $y$. Wolfram Alpha though, states that the solution is $z(x,y)=xyF(1/x-1/y$), which means that I have found the integral curve $z_2$ wrong. I cannot find any fault in my calculations though. The correct integral curve should be : $$z_2 = \frac{z}{xy}$$ How would one come to this calculation though ? I would really appreciate your help",,"['integration', 'ordinary-differential-equations', 'multivariable-calculus', 'partial-differential-equations']"
92,Monotonic solution of first order ODE,Monotonic solution of first order ODE,,"Let $f:\mathbb{R} \to \mathbb{R} $ be a continuous function. Let $x'(t)=f(x(t))$ be a maximal solution to the first order ODE. Show that the solution is a monotonic function. This is the classical problem when you have the intuition, but you don't know how to write it down. My first idea was to show that if it's not monotonic, then you will have two points, say $a$ and $b$ with $x(a)=x(b)$ but $x'(a)> 0$, $x'(b)<0$.","Let $f:\mathbb{R} \to \mathbb{R} $ be a continuous function. Let $x'(t)=f(x(t))$ be a maximal solution to the first order ODE. Show that the solution is a monotonic function. This is the classical problem when you have the intuition, but you don't know how to write it down. My first idea was to show that if it's not monotonic, then you will have two points, say $a$ and $b$ with $x(a)=x(b)$ but $x'(a)> 0$, $x'(b)<0$.",,"['real-analysis', 'ordinary-differential-equations']"
93,Show that $g'(x)+g(x)-2e^x=0$,Show that,g'(x)+g(x)-2e^x=0,"Given a function $g$ which has derivative $g'$ for all x $\in {R}$ and satisfying $g'(0)=2$ and $g(x+y)=e^yg(x)+e^xg(y)$ for all $x,y\in {R}$ Show that $g'(x)+g(x)-2e^x=0$ $\dfrac{g(x+y)}{e^{x+y}}=\dfrac{g(x)}{e^{x}}+\dfrac{g(y)}{e^{y}}$ Putting $x=0$ , $g'(y)=2e^y+g(x)$ Also putting $y=0$ ,we get, $0=e^xg(0)\implies g(0)=0$ I had a strong hunch that $g(x)=e^x-e^{-x}$ but it does not satisfy $g(x+y)=e^yg(x)+e^xg(y)$ Please help.","Given a function which has derivative for all x and satisfying and for all Show that Putting , Also putting ,we get, I had a strong hunch that but it does not satisfy Please help.","g g' \in {R} g'(0)=2 g(x+y)=e^yg(x)+e^xg(y) x,y\in {R} g'(x)+g(x)-2e^x=0 \dfrac{g(x+y)}{e^{x+y}}=\dfrac{g(x)}{e^{x}}+\dfrac{g(y)}{e^{y}} x=0 g'(y)=2e^y+g(x) y=0 0=e^xg(0)\implies g(0)=0 g(x)=e^x-e^{-x} g(x+y)=e^yg(x)+e^xg(y)",['ordinary-differential-equations']
94,Bendixon-Dulac applied to an extended Lotka Volterra system,Bendixon-Dulac applied to an extended Lotka Volterra system,,"I'm doing the following problem: Consider the following system modelling the interaction between two species: $$\begin{cases}\dot{x}=x(ax^n+by^n+c) \\ \dot{y}=y(dx^n+ey^n+f) \end{cases}$$ with $(a,b,c,d,e,f)\in\mathbb{R^6}$ , $n\in\mathbb{N}$ . Find a relation $\phi(a,b,c,d,e,f,n)$ such that: $i)$ If $\phi(a,b,c,d,e,f,n)\neq 0$ , then the system has no periodic orbits. Indication: Prove that you can reduce the problem to the first quadrant, and then use the following Dulac function: $D(x,y)=x^\alpha y^\beta$ , with properly $\alpha$ and $\beta$ . I've proved that we can reduce the problem to the first quadrant by seeing that $x=0$ and $y=0$ are orbits of the system, so by uniqueness of solutions, we cannot have a periodic orbit crossing them. Furthermore, we can change the variables to reduce it to the first quadrant. Now we have to use the Bendixon-Dulac Theorem, with $B(x,y)$ the function they gave to us. Now the main thing that I don't know how to do is to obtain that relation $\phi(a,b,c,d,e,f,n)$ . I don't know how to get there. I found the divergence of the Bendixon Dulac theorem but there's a lot of terms and it's really confusing. Do we have to equal it to zero to find the terms $\alpha$ and $\beta$ ?","I'm doing the following problem: Consider the following system modelling the interaction between two species: with , . Find a relation such that: If , then the system has no periodic orbits. Indication: Prove that you can reduce the problem to the first quadrant, and then use the following Dulac function: , with properly and . I've proved that we can reduce the problem to the first quadrant by seeing that and are orbits of the system, so by uniqueness of solutions, we cannot have a periodic orbit crossing them. Furthermore, we can change the variables to reduce it to the first quadrant. Now we have to use the Bendixon-Dulac Theorem, with the function they gave to us. Now the main thing that I don't know how to do is to obtain that relation . I don't know how to get there. I found the divergence of the Bendixon Dulac theorem but there's a lot of terms and it's really confusing. Do we have to equal it to zero to find the terms and ?","\begin{cases}\dot{x}=x(ax^n+by^n+c) \\ \dot{y}=y(dx^n+ey^n+f) \end{cases} (a,b,c,d,e,f)\in\mathbb{R^6} n\in\mathbb{N} \phi(a,b,c,d,e,f,n) i) \phi(a,b,c,d,e,f,n)\neq 0 D(x,y)=x^\alpha y^\beta \alpha \beta x=0 y=0 B(x,y) \phi(a,b,c,d,e,f,n) \alpha \beta","['calculus', 'ordinary-differential-equations', 'derivatives', 'algebraic-topology']"
95,Green's functions and Fredholm equations,Green's functions and Fredholm equations,,"If I have an ODE $Lf(x)=g(x)$, then I can write the solution as $f(x)=\int G(x,y)g(y)dy$ where $G(x,y)$ is the Green's function of $L$ (it is the kernel of the inverse of $L$). Here $g(x)$ is given and $f(x)$ is to be found. I also read about the Fredholm equation, which is of the same form, $f(x)=\int K(x,y)g(y)dy$, but where now $f$ is given and $g$ is to be found. My question is about the relation between Green's functions and Fredholm equations. I have read that using the Green's function we are ""reducing"" the ODE to a Fredholm equation. I don't understand this statement. They look more like complementary problems, with different origins and different applications. I would appreciate a broad overview of this. Which problem is more fundamental? What is going on here?","If I have an ODE $Lf(x)=g(x)$, then I can write the solution as $f(x)=\int G(x,y)g(y)dy$ where $G(x,y)$ is the Green's function of $L$ (it is the kernel of the inverse of $L$). Here $g(x)$ is given and $f(x)$ is to be found. I also read about the Fredholm equation, which is of the same form, $f(x)=\int K(x,y)g(y)dy$, but where now $f$ is given and $g$ is to be found. My question is about the relation between Green's functions and Fredholm equations. I have read that using the Green's function we are ""reducing"" the ODE to a Fredholm equation. I don't understand this statement. They look more like complementary problems, with different origins and different applications. I would appreciate a broad overview of this. Which problem is more fundamental? What is going on here?",,"['real-analysis', 'ordinary-differential-equations']"
96,Show that every nontrivial solution to the differential equation has at most one zero,Show that every nontrivial solution to the differential equation has at most one zero,,"Show that every nontrivial solution to the equation $y''-e^{x}y=0$ can have at most one zero on the interval $0<x<+\infty$. My idea is to compare it to the eq $y''-y=0$, then use the Sturm Comparison Theorem, but I'm unsure of how to execute this.","Show that every nontrivial solution to the equation $y''-e^{x}y=0$ can have at most one zero on the interval $0<x<+\infty$. My idea is to compare it to the eq $y''-y=0$, then use the Sturm Comparison Theorem, but I'm unsure of how to execute this.",,['ordinary-differential-equations']
97,ODE- Maximal interval,ODE- Maximal interval,,"I am working on the following problem from Gerald Teschl's book on ODE's and am at a loss of how to proceed. Consider a first-order autonomous equation in $\mathbb{R}^1$ with $f(x)$ Lipschitz. Suposse $f(0)=f(1)= 0$. Show that solutions starting in $[0,1]$ cannot leave this interval. What is the maximal interval of definition $(T_{-},T_{+})$ for solutions starting in $[0,1]$? Does such a solution have a limit as $t\rightarrow T_{+}$ or $t\rightarrow T_{-}$ ? Any help would be appreciated. Thanks!","I am working on the following problem from Gerald Teschl's book on ODE's and am at a loss of how to proceed. Consider a first-order autonomous equation in $\mathbb{R}^1$ with $f(x)$ Lipschitz. Suposse $f(0)=f(1)= 0$. Show that solutions starting in $[0,1]$ cannot leave this interval. What is the maximal interval of definition $(T_{-},T_{+})$ for solutions starting in $[0,1]$? Does such a solution have a limit as $t\rightarrow T_{+}$ or $t\rightarrow T_{-}$ ? Any help would be appreciated. Thanks!",,"['ordinary-differential-equations', 'lipschitz-functions']"
98,Symplectomorphism from one point to another in a connected symplectic manifold,Symplectomorphism from one point to another in a connected symplectic manifold,,"I was taking a look at the following old question and I was wondering if this could be extended to symplectic manifolds. So if we have a symplectic manifold $(M,\omega)$ and two different points $x,y \in M$, does there exist a symplectomorphism $\phi: M \to M$ such that $\phi(x) = y$? I tried the same approach as in that question but I couldn't prove that $\tilde{X}$ was a symplectic vector field.","I was taking a look at the following old question and I was wondering if this could be extended to symplectic manifolds. So if we have a symplectic manifold $(M,\omega)$ and two different points $x,y \in M$, does there exist a symplectomorphism $\phi: M \to M$ such that $\phi(x) = y$? I tried the same approach as in that question but I couldn't prove that $\tilde{X}$ was a symplectic vector field.",,"['ordinary-differential-equations', 'symplectic-geometry']"
99,Higher order Sturm-Liouville form,Higher order Sturm-Liouville form,,"The differential equation book I was reading briefly mentions about the generalization S-L form and it says if we consider the BVP $L(y)=\lambda r(x)y$ where $L(y)=\displaystyle P_n(x)\frac{d^ny}{dx^n}+\dots +P_1(x)\frac{dy}{dx}+P_0(x)y$, then the problem is said to be self-adjoint if $(L(u),v)=(u,L(v))$ and this requires $L$ to be of even order. I'm not sure why the last condition requires $L$ to be of even order, I tried to google and found some documentations saying $L$ is self-adjoint if it's of even order and anti self-adjoint(i.e. $(L(u),v)=-(u,L(v)))$ if it's of odd order but couldn't find any explanation. Can anyone explain this to me?","The differential equation book I was reading briefly mentions about the generalization S-L form and it says if we consider the BVP $L(y)=\lambda r(x)y$ where $L(y)=\displaystyle P_n(x)\frac{d^ny}{dx^n}+\dots +P_1(x)\frac{dy}{dx}+P_0(x)y$, then the problem is said to be self-adjoint if $(L(u),v)=(u,L(v))$ and this requires $L$ to be of even order. I'm not sure why the last condition requires $L$ to be of even order, I tried to google and found some documentations saying $L$ is self-adjoint if it's of even order and anti self-adjoint(i.e. $(L(u),v)=-(u,L(v)))$ if it's of odd order but couldn't find any explanation. Can anyone explain this to me?",,"['ordinary-differential-equations', 'boundary-value-problem', 'sturm-liouville']"
