,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,"A ""What's my vector?"" game","A ""What's my vector?"" game",,"Alice chooses a binary vector $V$ of length $n$ which is unknown to Bob.  In each round Bob can choose any number of indices $i$ and then Alice tells Bob how many of the $V_i$ are set to $1$.   The game terminates when Bob can say with certainty exactly which binary vector Alice has. Alice  would like the game to go on as long as possible and Bob would like to make it as short as possible. What are Alice and Bob's best strategies and how long does the game take? For $n=7$ the following strategy always works. Bob chooses indices 35, 234, 24567, 12356,14567 . This takes $5$ rounds. For $n=10$ the following strategy always works. Bob chooses indices 1-3-4-5-6-7-8, 3-5-9, 1-2-3-8-10,1-2-4-5-9-10,1-2-5-7-8, 1-2-3-4-5-8-10, 2-4-7-8-10. This takes $7$ rounds.","Alice chooses a binary vector $V$ of length $n$ which is unknown to Bob.  In each round Bob can choose any number of indices $i$ and then Alice tells Bob how many of the $V_i$ are set to $1$.   The game terminates when Bob can say with certainty exactly which binary vector Alice has. Alice  would like the game to go on as long as possible and Bob would like to make it as short as possible. What are Alice and Bob's best strategies and how long does the game take? For $n=7$ the following strategy always works. Bob chooses indices 35, 234, 24567, 12356,14567 . This takes $5$ rounds. For $n=10$ the following strategy always works. Bob chooses indices 1-3-4-5-6-7-8, 3-5-9, 1-2-3-8-10,1-2-4-5-9-10,1-2-5-7-8, 1-2-3-4-5-8-10, 2-4-7-8-10. This takes $7$ rounds.",,"['linear-algebra', 'combinatorics', 'puzzle']"
1,Singular-value inequalities,Singular-value inequalities,,"This is my question: Is the following statement true ? Let $H$ be a real or complex Hilbertspace and $R,S:H \to H$ compact operators. For every $n\in\mathbb{N}$ the following inequality holds: $$\sum_{j=1}^n s_j(RS) \leq \sum_{j=1}^n s_j(R)s_j(S)$$ Note : $s_j(R)$ denotes the j-th singular value of the opeartor $R$. The sequence of the singular values falls monotonically to zero. With best regards, mat Edit : I found out, that the statement is true for products instead of sums. By that I mean: Let $H$ be a $\mathbb{K}$-Hilbertspace and $R,S: H \to H$ compact operators. For every $n\in\mathbb{N}$ we have: $$\Pi_{j=1}^n s_j(RS) \leq \Pi_{j=1}^n s_j(R)s_j(S)$$ Is it possible to derive the statement for sums from this?","This is my question: Is the following statement true ? Let $H$ be a real or complex Hilbertspace and $R,S:H \to H$ compact operators. For every $n\in\mathbb{N}$ the following inequality holds: $$\sum_{j=1}^n s_j(RS) \leq \sum_{j=1}^n s_j(R)s_j(S)$$ Note : $s_j(R)$ denotes the j-th singular value of the opeartor $R$. The sequence of the singular values falls monotonically to zero. With best regards, mat Edit : I found out, that the statement is true for products instead of sums. By that I mean: Let $H$ be a $\mathbb{K}$-Hilbertspace and $R,S: H \to H$ compact operators. For every $n\in\mathbb{N}$ we have: $$\Pi_{j=1}^n s_j(RS) \leq \Pi_{j=1}^n s_j(R)s_j(S)$$ Is it possible to derive the statement for sums from this?",,"['linear-algebra', 'functional-analysis', 'inequality', 'operator-theory', 'spectral-theory']"
2,Simultaneously vanishing quadratic forms,Simultaneously vanishing quadratic forms,,"Given a set of Hermitian matrices $\{A_i\}$, is there a simple way to check if there exists a vector $c$ such that for all $i$: $$c^* A_i c = 0?$$ Namely, when can the quadratic forms defined by the $\{A_i\}$ simultaneously vanish?","Given a set of Hermitian matrices $\{A_i\}$, is there a simple way to check if there exists a vector $c$ such that for all $i$: $$c^* A_i c = 0?$$ Namely, when can the quadratic forms defined by the $\{A_i\}$ simultaneously vanish?",,"['linear-algebra', 'matrices', 'systems-of-equations', 'quadratic-forms']"
3,Minimal and characteristic polynomials on tensor product spaces,Minimal and characteristic polynomials on tensor product spaces,,"Given two finite-dimensional vector spaces $V$ and $W$ over a common field $k$ as well as $k$-linear transformations $\varphi \colon V \to V$ and $\psi \colon W \to W$, what can be said in general about the minimal and characteristic polynomials of the tensor product of the linear transformations? That is, can one describe the minimal and characteristic polynomials of $\varphi \otimes \psi \colon V \otimes W \to V \otimes W$ in terms of the minimal and characteristic polynomials of $\varphi$ and $\psi$? Moreover, what do we glean in addition by letting $\psi$ be the identity matrix? (Edit 1: the case where $\psi$ is the identity matrix for both the characteristic and minimal polynomials is explained by @darij grinberg in the comments.) Edit 2: @darig grinbeg's comments outline that the characteristic polynomial of $\varphi \otimes \psi$ is indeed determined by the characteristic polynomials of $\varphi$ and $\psi$, though we don't yet have a `nice' relation. Nothing has been said about the minimal polynomial.","Given two finite-dimensional vector spaces $V$ and $W$ over a common field $k$ as well as $k$-linear transformations $\varphi \colon V \to V$ and $\psi \colon W \to W$, what can be said in general about the minimal and characteristic polynomials of the tensor product of the linear transformations? That is, can one describe the minimal and characteristic polynomials of $\varphi \otimes \psi \colon V \otimes W \to V \otimes W$ in terms of the minimal and characteristic polynomials of $\varphi$ and $\psi$? Moreover, what do we glean in addition by letting $\psi$ be the identity matrix? (Edit 1: the case where $\psi$ is the identity matrix for both the characteristic and minimal polynomials is explained by @darij grinberg in the comments.) Edit 2: @darig grinbeg's comments outline that the characteristic polynomial of $\varphi \otimes \psi$ is indeed determined by the characteristic polynomials of $\varphi$ and $\psi$, though we don't yet have a `nice' relation. Nothing has been said about the minimal polynomial.",,"['linear-algebra', 'abstract-algebra', 'tensor-products']"
4,Abelian subgroups of $GL_n(\mathbb{F}_p)$,Abelian subgroups of,GL_n(\mathbb{F}_p),"Let $p$ be a prime number, and let $k=\mathbb{F}_p$ be the field of $p$ elements.  Let $G=GL_n(k)$.  We know that $$|G|=\prod_{i=0}^{n-1}(p^n-p^i)=p^{\binom{n}{2}}\prod_{i=0}^{n-1}(p^{n-i}-1)$$ so that the Sylow $p$-subgroups of $G$ have order $p^{\binom{n}{2}}$.  One such subgroup is $U$, the upper-triangular unipotent subgroup consisting of all upper-triangular matrices with $1$'s on the diagonal.  Let $A_{ij}=I_n+E_{ij}$ for $j>i$, where $E_{ij}$ is the matrix with a $1$ in the $ij$th entry and $0$'s elsewhere.  If we find $i_1,\ldots,i_r,j_1,\ldots,j_r$ such that the $A_{i_kj_k}$ pairwise commute, then we have: $$(\mathbb{Z}/p\mathbb{Z})^{\oplus r}\cong \langle A_{i_1j_1},\ldots,A_{i_rj_r}\rangle\subset U$$ Two questions: Is every copy of $(\mathbb{Z}/p\mathbb{Z})^{\oplus r}$ inside of $U$ conjugate under $G$ to a subgroup of the form $\langle A_{i_1j_1},\ldots,A_{i_rj_r}\rangle$? Do there exist distinct, conjugate subgroups of the form $\langle A_{i_1j_1},\ldots,A_{i_rj_r}\rangle$ and $\langle A_{i_1'j_1'},\ldots,A_{i_r'j_r'}\rangle$? The largest value of $r$ for which such subgroups exist is $r=\lfloor n^2/4\rfloor$.  In this case, I believe the answers to these questions are yes and no, respectively.  I'd like to know if this also holds for smaller values of $r$.","Let $p$ be a prime number, and let $k=\mathbb{F}_p$ be the field of $p$ elements.  Let $G=GL_n(k)$.  We know that $$|G|=\prod_{i=0}^{n-1}(p^n-p^i)=p^{\binom{n}{2}}\prod_{i=0}^{n-1}(p^{n-i}-1)$$ so that the Sylow $p$-subgroups of $G$ have order $p^{\binom{n}{2}}$.  One such subgroup is $U$, the upper-triangular unipotent subgroup consisting of all upper-triangular matrices with $1$'s on the diagonal.  Let $A_{ij}=I_n+E_{ij}$ for $j>i$, where $E_{ij}$ is the matrix with a $1$ in the $ij$th entry and $0$'s elsewhere.  If we find $i_1,\ldots,i_r,j_1,\ldots,j_r$ such that the $A_{i_kj_k}$ pairwise commute, then we have: $$(\mathbb{Z}/p\mathbb{Z})^{\oplus r}\cong \langle A_{i_1j_1},\ldots,A_{i_rj_r}\rangle\subset U$$ Two questions: Is every copy of $(\mathbb{Z}/p\mathbb{Z})^{\oplus r}$ inside of $U$ conjugate under $G$ to a subgroup of the form $\langle A_{i_1j_1},\ldots,A_{i_rj_r}\rangle$? Do there exist distinct, conjugate subgroups of the form $\langle A_{i_1j_1},\ldots,A_{i_rj_r}\rangle$ and $\langle A_{i_1'j_1'},\ldots,A_{i_r'j_r'}\rangle$? The largest value of $r$ for which such subgroups exist is $r=\lfloor n^2/4\rfloor$.  In this case, I believe the answers to these questions are yes and no, respectively.  I'd like to know if this also holds for smaller values of $r$.",,"['linear-algebra', 'group-theory', 'finite-groups', 'algebraic-groups', 'groups-of-lie-type']"
5,Monotone matrix norms,Monotone matrix norms,,"[Ciarlet 2.2-10] Let $\mathscr{S}_n$ be the set of symmetric matrices and $\mathscr{S}_n^+$ the subset of non-negative definite symmetric matrices. A matrix norm $\|\cdot\|$ to be monotone if $$A\in\mathscr{S}_n^+\; \wedge\; B-A\in\mathscr{S}_n^+\ \Rightarrow\ \|A\|\ \leq\ \|B\|.$$ Show that the norms $\|\cdot\|_2$ and $\|\cdot\|_F$ (Frobenus norm) are monotone. More generally, show that if a matrix norm $\|\cdot\|$ is invariant under unitary transformations, that is, if $\|A\| = \|AU\| = \|UA\|$ for every unitary matrix $U$, then it is monotone. Let $\|\cdot\|$ be a monotone norm and $\mbox{cond}(\cdot)$ the condition number function associated with it. Prove that $$A,B\in\mathscr{S}_n^*\ \Rightarrow\ \mbox{cond}(A+B)\ \leq\ \max\left\{\mbox{cond}(A),\; \mbox{cond}(B)\right\}$$ where $\mathscr{S}_n^*$ denotes the subset of positive definite symmetric matrices. I already have proved (1), and I proved that $\lambda_i(A) \leq \lambda_i(B)$, $\forall\ i=1,2,\ldots,n$ and $\forall A,B-A\in\mathscr{S}_n^+$. But I have had problems in order to prove (2) and (3). For (2), i proved that  \begin{eqnarray*} \|A\| & = & \|U^*AU\|\ =\ \|\mbox{diag}(\lambda_i(A))\|,\\[0.3cm] \|B\| & = & \|V^*BV\|\ =\ \|\mbox{diag}(\lambda_i(B))\|. \end{eqnarray*} but I don't know what I should do next. Please help me and thanks in advance.","[Ciarlet 2.2-10] Let $\mathscr{S}_n$ be the set of symmetric matrices and $\mathscr{S}_n^+$ the subset of non-negative definite symmetric matrices. A matrix norm $\|\cdot\|$ to be monotone if $$A\in\mathscr{S}_n^+\; \wedge\; B-A\in\mathscr{S}_n^+\ \Rightarrow\ \|A\|\ \leq\ \|B\|.$$ Show that the norms $\|\cdot\|_2$ and $\|\cdot\|_F$ (Frobenus norm) are monotone. More generally, show that if a matrix norm $\|\cdot\|$ is invariant under unitary transformations, that is, if $\|A\| = \|AU\| = \|UA\|$ for every unitary matrix $U$, then it is monotone. Let $\|\cdot\|$ be a monotone norm and $\mbox{cond}(\cdot)$ the condition number function associated with it. Prove that $$A,B\in\mathscr{S}_n^*\ \Rightarrow\ \mbox{cond}(A+B)\ \leq\ \max\left\{\mbox{cond}(A),\; \mbox{cond}(B)\right\}$$ where $\mathscr{S}_n^*$ denotes the subset of positive definite symmetric matrices. I already have proved (1), and I proved that $\lambda_i(A) \leq \lambda_i(B)$, $\forall\ i=1,2,\ldots,n$ and $\forall A,B-A\in\mathscr{S}_n^+$. But I have had problems in order to prove (2) and (3). For (2), i proved that  \begin{eqnarray*} \|A\| & = & \|U^*AU\|\ =\ \|\mbox{diag}(\lambda_i(A))\|,\\[0.3cm] \|B\| & = & \|V^*BV\|\ =\ \|\mbox{diag}(\lambda_i(B))\|. \end{eqnarray*} but I don't know what I should do next. Please help me and thanks in advance.",,"['linear-algebra', 'normed-spaces']"
6,Characterization of the subspaces of $\mathbb R^{m\times n}$ induced by rank-1 matrices?,Characterization of the subspaces of  induced by rank-1 matrices?,\mathbb R^{m\times n},"Consider a linear subspace $S$ of the space of $m\times n$ real valued matrices $\mathbb R^{m\times n}$ . When does $S$ admit a basis consisting only of rank-1 matrices? I.e. is there a simple criterion or algorithm to check for the existence of such a basis? Background: I would also be interested in the case when we also allow diagonal, permutation, or generally sparse matrices. This is because the matrix vector product $A\cdot x$ , which usually costs $O(n^2)$ to evaluate, can be computed in $O(n)$ for the aforementioned matrix types. Thus, if we have a subspace $S\le \mathbb R^{m\times n}$ with $\dim(S)\ll m\times n$ we can potentially achieve huge performance gains by representing parametrized  matrices in $S$ using a basis which only consists of these ""cheap"" matrices. Formulation as an optimization problem: Given a finite set of $m\times n$ real-valued matrices $\mathcal A = \{A_1,\ldots, A_d\}\subset \mathbb R^{m\times n}$ , find $$ \min_{\mathcal B = \{B_1, \ldots, B_d\}\subset \mathbb R^{m\times n}}  \sum_{i=1}^d \text{rank}(B_i) \qquad\text{s.t.}\quad\big\langle \mathcal A\big\rangle = \big\langle \mathcal B\big\rangle $$ Is there any hope of solving this problem exactly? If not, what types of optimization algorithms are suitable for this kind of problem, given that both the objective function and the constraint are discontinuous? Example: given $$S= \left\langle \underset{A_1}{\begin{bmatrix}4 & 5 & 6 \\ 7 & 9 & 11 \end{bmatrix}}, \underset{A_2}{\begin{bmatrix}2 & 3 & 4 \\ 5 & 7 & 9 \end{bmatrix}} \right\rangle $$ We can represent $S$ equivalently as $$S= \left\langle  \underset{B_1}{\begin{bmatrix}3 & 4 & 5 \\ 6 & 8 & 10 \end{bmatrix}}, \underset{B_2}{\begin{bmatrix}1 & 1 & 1 \\ 1 & 1 & 1 \end{bmatrix}} \right\rangle = \left\langle \underset{u_1v_1^T}{\begin{bmatrix} 1 \\ 2 \end{bmatrix}\begin{bmatrix}3 & 4 & 5\end{bmatrix}}, \underset{u_2v_2^T}{\begin{bmatrix} 1 \\ 1 \end{bmatrix}\begin{bmatrix} 1 & 1 & 1\end{bmatrix}}\right\rangle  $$ Because $A_1=u_1v_1^T + u_2v_2^T$ and $A_2 = u_1v_1^T - u_2v_2^T$ . Now consider a parametrized matrix $C$ in $S$ . If we express $C$ in the basis $A$ , i.e. $C = \lambda_1 A_1 + \lambda_2 A_2$ , then computing the matrix vector product $C\cdot x$ will cost us at least 16/10 multiplications/additions, whereas if we represent it in the basis $B$ , i.e. $C=\lambda_1 u_1v_1^T + \lambda_2 u_2v_2^T$ , then the matrix-vector product $C\cdot x$ can be done in 12/8 multiplications/additions. More generally, for a space of dimension $d$ the scaling is $O(dmn)$ for a dense basis and $O(d(m+n))$ for a rank-1 basis in terms of both elementary multiplications and additions. (using elementary matrix multiplication)","Consider a linear subspace of the space of real valued matrices . When does admit a basis consisting only of rank-1 matrices? I.e. is there a simple criterion or algorithm to check for the existence of such a basis? Background: I would also be interested in the case when we also allow diagonal, permutation, or generally sparse matrices. This is because the matrix vector product , which usually costs to evaluate, can be computed in for the aforementioned matrix types. Thus, if we have a subspace with we can potentially achieve huge performance gains by representing parametrized  matrices in using a basis which only consists of these ""cheap"" matrices. Formulation as an optimization problem: Given a finite set of real-valued matrices , find Is there any hope of solving this problem exactly? If not, what types of optimization algorithms are suitable for this kind of problem, given that both the objective function and the constraint are discontinuous? Example: given We can represent equivalently as Because and . Now consider a parametrized matrix in . If we express in the basis , i.e. , then computing the matrix vector product will cost us at least 16/10 multiplications/additions, whereas if we represent it in the basis , i.e. , then the matrix-vector product can be done in 12/8 multiplications/additions. More generally, for a space of dimension the scaling is for a dense basis and for a rank-1 basis in terms of both elementary multiplications and additions. (using elementary matrix multiplication)","S m\times n \mathbb R^{m\times n} S A\cdot x O(n^2) O(n) S\le \mathbb R^{m\times n} \dim(S)\ll m\times n S m\times n \mathcal A = \{A_1,\ldots, A_d\}\subset \mathbb R^{m\times n}  \min_{\mathcal B = \{B_1, \ldots, B_d\}\subset \mathbb R^{m\times n}}  \sum_{i=1}^d \text{rank}(B_i) \qquad\text{s.t.}\quad\big\langle \mathcal A\big\rangle = \big\langle \mathcal B\big\rangle  S= \left\langle
\underset{A_1}{\begin{bmatrix}4 & 5 & 6 \\ 7 & 9 & 11 \end{bmatrix}},
\underset{A_2}{\begin{bmatrix}2 & 3 & 4 \\ 5 & 7 & 9 \end{bmatrix}}
\right\rangle
 S S= \left\langle 
\underset{B_1}{\begin{bmatrix}3 & 4 & 5 \\ 6 & 8 & 10 \end{bmatrix}},
\underset{B_2}{\begin{bmatrix}1 & 1 & 1 \\ 1 & 1 & 1 \end{bmatrix}}
\right\rangle
= \left\langle
\underset{u_1v_1^T}{\begin{bmatrix} 1 \\ 2 \end{bmatrix}\begin{bmatrix}3 & 4 & 5\end{bmatrix}},
\underset{u_2v_2^T}{\begin{bmatrix} 1 \\ 1 \end{bmatrix}\begin{bmatrix} 1 & 1 & 1\end{bmatrix}}\right\rangle 
 A_1=u_1v_1^T + u_2v_2^T A_2 = u_1v_1^T - u_2v_2^T C S C A C = \lambda_1 A_1 + \lambda_2 A_2 C\cdot x B C=\lambda_1 u_1v_1^T + \lambda_2 u_2v_2^T C\cdot x d O(dmn) O(d(m+n))","['linear-algebra', 'matrices', 'optimization', 'matrix-rank']"
7,Is this matrix function bounded from above by a norm,Is this matrix function bounded from above by a norm,,"Given two symmetric, positive definite matrices $A$ and $B$ , let $$ d(A, B) = \textrm{tr}(A) + \textrm{tr}(B) - 2 \, \textrm{tr} \, \left((A^{1/2} B A^{1/2})^{1/2}\right). $$ This function coincides with the square of the 2-Wasserstein distance between Gaussians  with equal means and with covariance matrices given by $A$ and $B$ , respectively. Is $d(\cdot, \cdot)$ bounded from above by a matrix norm? That is to say, is there a constant $C$ such that $$ d(A, B) \leq C \|A - B\| \qquad \forall A, B \in \mathbb R^{n\times n} \, s.p.d., $$ where $\|\cdot\|$ is any matrix norm? In dimension one, this is true: $$ d(A, B) = (\sqrt A - \sqrt B)^2 \leq |A-B|, $$ because $|\sqrt{A} - \sqrt{B}| \leq \sqrt{|A - B|}$ by concavity. More generally, if $A$ and $B$ commute, i.e. if there exists $P$ such that $A = P D_A P^T$ and $B = P D_B P^T$ , $$ d(A, B) = \mathrm{tr}(D_A) + \mathrm{tr}(D_B) - 2 \, \mathrm{tr}((D_A D_B)^{1/2}) = \mathrm{tr}(|D_A^{1/2} - D_B^{1/2}|^2) \leq \mathrm{tr} (|D_A - D_B|) \leq n\|A - B\|_2. $$ What about the general case? A friend pointed out to me that the Araki–Lieb–Thirring inequality , with $r=1/2$ and $q=1$ , could be employed to obtain $$ \mathrm{tr}((A^{1/2}BA^{1/2})^{1/2}) \geq \mathrm{tr}(A^{1/4}B^{1/2}A^{1/4}) = \mathrm{tr}(A^{1/2}B^{1/2}), $$ which implies that $$ d(A, B) \leq \mathrm{tr}((A^{1/2} - B^{1/2})^2) = \|A^{1/2} - B^{1/2}\|_F^2. $$","Given two symmetric, positive definite matrices and , let This function coincides with the square of the 2-Wasserstein distance between Gaussians  with equal means and with covariance matrices given by and , respectively. Is bounded from above by a matrix norm? That is to say, is there a constant such that where is any matrix norm? In dimension one, this is true: because by concavity. More generally, if and commute, i.e. if there exists such that and , What about the general case? A friend pointed out to me that the Araki–Lieb–Thirring inequality , with and , could be employed to obtain which implies that","A B 
d(A, B) = \textrm{tr}(A) + \textrm{tr}(B) - 2 \, \textrm{tr} \, \left((A^{1/2} B A^{1/2})^{1/2}\right).
 A B d(\cdot, \cdot) C 
d(A, B) \leq C \|A - B\| \qquad \forall A, B \in \mathbb R^{n\times n} \, s.p.d.,
 \|\cdot\| 
d(A, B) = (\sqrt A - \sqrt B)^2 \leq |A-B|,
 |\sqrt{A} - \sqrt{B}| \leq \sqrt{|A - B|} A B P A = P D_A P^T B = P D_B P^T 
d(A, B) = \mathrm{tr}(D_A) + \mathrm{tr}(D_B) - 2 \, \mathrm{tr}((D_A D_B)^{1/2}) = \mathrm{tr}(|D_A^{1/2} - D_B^{1/2}|^2) \leq \mathrm{tr} (|D_A - D_B|) \leq n\|A - B\|_2.
 r=1/2 q=1 
\mathrm{tr}((A^{1/2}BA^{1/2})^{1/2}) \geq \mathrm{tr}(A^{1/4}B^{1/2}A^{1/4}) = \mathrm{tr}(A^{1/2}B^{1/2}),
 
d(A, B) \leq \mathrm{tr}((A^{1/2} - B^{1/2})^2) = \|A^{1/2} - B^{1/2}\|_F^2.
","['linear-algebra', 'matrices']"
8,Symmetric matrix with given determinant,Symmetric matrix with given determinant,,"The matrix \begin{equation} A :=  \begin{pmatrix} x & 0 & 0 & z \\ 0 & y & 0 & x \\ 0 & 0 & z & y \\ y & z & x & w \end{pmatrix} \end{equation} has determinant \begin{equation} \det A =  -x^2y^2 - x^2z^2 -y^2z^2+ xyzw, \end{equation} which describes the Steiner surface. However, $A$ is not symmetric, so it does not answer whether the Steiner surface is a symmetroid or not. Is it possible to find a symmetric matrix $S$ such that each nonzero entry in $S$ is a linear form in $x, y, z, w$ and $\det S = \lambda \det A$ for some $\lambda \in \mathbb{R} \setminus \{0\}$?","The matrix \begin{equation} A :=  \begin{pmatrix} x & 0 & 0 & z \\ 0 & y & 0 & x \\ 0 & 0 & z & y \\ y & z & x & w \end{pmatrix} \end{equation} has determinant \begin{equation} \det A =  -x^2y^2 - x^2z^2 -y^2z^2+ xyzw, \end{equation} which describes the Steiner surface. However, $A$ is not symmetric, so it does not answer whether the Steiner surface is a symmetroid or not. Is it possible to find a symmetric matrix $S$ such that each nonzero entry in $S$ is a linear form in $x, y, z, w$ and $\det S = \lambda \det A$ for some $\lambda \in \mathbb{R} \setminus \{0\}$?",,"['linear-algebra', 'matrices']"
9,any pattern here ? (revised 2),any pattern here ? (revised 2),,"for any positive number $k$, I have a $(k+1)*(k+1)$ matrix. I wonder if these matrices follow any ""obvious"" pattern. My goal is to guess the elements for matrix with $k=5$ and above (most probably in a recursive way, and using some combinatorial functions) $$k=1, \begin{bmatrix} 1 & 1\\  1 & -1 \end{bmatrix}$$ $$k=2, \begin{bmatrix} 2&	2&	2\\		 2&	0&	-2\\		 2&	-2&	2		 \end{bmatrix}$$ $$k=3, \begin{bmatrix} 3&	3&	3&	3\\	 3&	1&	-1&	-3\\	 3&	-1&	-1&	3\\	 3&	-3&	3&	-3 \end{bmatrix}$$ $$k=4, \begin{bmatrix} 6&	6&	6&	6&	6\\ 6&	3&	0&	-3&	-6\\ 6&	0&	-2&	0&	6\\ 6&	-3&	0&	3&	-6\\ 6&	-6&	6&	-6&	6 \end{bmatrix}$$ Any of the above matrices may be multiplied by an arbitrary constant (for example the top-left element of each matrix) to normalize. any guess or comment is appreciated. EDIT: here is how I derived these: see here for definitions. given the eigen decomposition $AA^{T}=Q \Lambda Q^{T}$, we define $W=A^{T} Q \Lambda^{-1}Q^{T}$, considering the symmetry, if the kmer $u_j$ and gapped-kmer $v_i$ have $m$ mismatches, then $w_{j,i}=w_m$. Hence to find $W$, we only need to find {$w_0,...,w_k$}. I guess that $w_m$ can be written as following ($c_0$ is a constant): $$w_m = c_0\sum_{l=0}^k \alpha_{k,m,l} \binom{L}{l} $$ for $k=1,2,3,4$ and $m=0,1,..,k$ and $l=0,1,..,k$, I have numerically calculated the value for $\alpha_{k,m,l}$, which is given as the above matrices. The question is what $\alpha_{k,m,l}$ is for general k. EDIT: also see here for a solution to eigen decomposition by Siva. EDIT: with a few reasonable assumptions, I have found a common pattern and made an R code that generates these matrices recursively. Following are few more matrices generated by my program (unlike the above four matrices, the following are neither proven nor tested to be the solution for the above $\alpha_{k,m,l}$ equation: $$k=5, \begin{bmatrix}    10&   10&   10&   10&   10&   10\\    10&    6&    2&   -2&   -6&  -10\\    10&    2&   -2&   -2&    2&   10\\    10&   -2&   -2&    2&    2&  -10\\   10&   -6&    2&    2&   -6&   10\\   10&  -10&   10&  -10&   10&  -10 \end{bmatrix}$$ $$k=6, \begin{bmatrix}    15&   15  & 15&   15   &15   &15   &15\\    15 &  10   & 5 &   0 &  -5&  -10 & -15\\    15  &  5  & -1  & -3 &  -1 &   5  & 15\\    15 &   0   &-3   & 0 &   3  &  0 & -15\\    15  & -5  & -1  &  3 &  -1   &-5  & 15\\    15 & -10 &   5  &  0 &  -5  & 10 & -15\\    15  &-15&   15  &-15&   15  &-15&   15 \end{bmatrix}$$ Again the scales are arbitrary. EDIT: We recently found a solution for $w(m)$ as following:  $$w_m=\frac{(-1)^{m}}{2^{L}\binom{L}{k-m}}\frac{L-k}{L-k+m}\sum_{t=0}^{k-m}\binom{L}{t} =\frac{{k-L \choose m}}{2^{L}\binom{L}{k} \binom{k}{m}} \sum_{t=0}^{k-m}\binom{L}{t}$$ For interested readers, this solution is described in Robust k-mer frequency estimation using gapped k-mers . Now, having this result how can one solve for $\alpha_{k,m,l}$ in $w_m = c_0\sum_{l=0}^k \alpha_{k,m,l} \binom{L}{l}$? Note that here $c_0$ may depend on $k$ and $L$ but not $m$ or $l$. My guess is that $c_0^{-1}=2^L\binom{L}{k}$. Also $\alpha_{k,m,l}$ is a function of $k,m$ and $l$ and most likely symmetric in $l$ and $m$ as the above examples suggest.","for any positive number $k$, I have a $(k+1)*(k+1)$ matrix. I wonder if these matrices follow any ""obvious"" pattern. My goal is to guess the elements for matrix with $k=5$ and above (most probably in a recursive way, and using some combinatorial functions) $$k=1, \begin{bmatrix} 1 & 1\\  1 & -1 \end{bmatrix}$$ $$k=2, \begin{bmatrix} 2&	2&	2\\		 2&	0&	-2\\		 2&	-2&	2		 \end{bmatrix}$$ $$k=3, \begin{bmatrix} 3&	3&	3&	3\\	 3&	1&	-1&	-3\\	 3&	-1&	-1&	3\\	 3&	-3&	3&	-3 \end{bmatrix}$$ $$k=4, \begin{bmatrix} 6&	6&	6&	6&	6\\ 6&	3&	0&	-3&	-6\\ 6&	0&	-2&	0&	6\\ 6&	-3&	0&	3&	-6\\ 6&	-6&	6&	-6&	6 \end{bmatrix}$$ Any of the above matrices may be multiplied by an arbitrary constant (for example the top-left element of each matrix) to normalize. any guess or comment is appreciated. EDIT: here is how I derived these: see here for definitions. given the eigen decomposition $AA^{T}=Q \Lambda Q^{T}$, we define $W=A^{T} Q \Lambda^{-1}Q^{T}$, considering the symmetry, if the kmer $u_j$ and gapped-kmer $v_i$ have $m$ mismatches, then $w_{j,i}=w_m$. Hence to find $W$, we only need to find {$w_0,...,w_k$}. I guess that $w_m$ can be written as following ($c_0$ is a constant): $$w_m = c_0\sum_{l=0}^k \alpha_{k,m,l} \binom{L}{l} $$ for $k=1,2,3,4$ and $m=0,1,..,k$ and $l=0,1,..,k$, I have numerically calculated the value for $\alpha_{k,m,l}$, which is given as the above matrices. The question is what $\alpha_{k,m,l}$ is for general k. EDIT: also see here for a solution to eigen decomposition by Siva. EDIT: with a few reasonable assumptions, I have found a common pattern and made an R code that generates these matrices recursively. Following are few more matrices generated by my program (unlike the above four matrices, the following are neither proven nor tested to be the solution for the above $\alpha_{k,m,l}$ equation: $$k=5, \begin{bmatrix}    10&   10&   10&   10&   10&   10\\    10&    6&    2&   -2&   -6&  -10\\    10&    2&   -2&   -2&    2&   10\\    10&   -2&   -2&    2&    2&  -10\\   10&   -6&    2&    2&   -6&   10\\   10&  -10&   10&  -10&   10&  -10 \end{bmatrix}$$ $$k=6, \begin{bmatrix}    15&   15  & 15&   15   &15   &15   &15\\    15 &  10   & 5 &   0 &  -5&  -10 & -15\\    15  &  5  & -1  & -3 &  -1 &   5  & 15\\    15 &   0   &-3   & 0 &   3  &  0 & -15\\    15  & -5  & -1  &  3 &  -1   &-5  & 15\\    15 & -10 &   5  &  0 &  -5  & 10 & -15\\    15  &-15&   15  &-15&   15  &-15&   15 \end{bmatrix}$$ Again the scales are arbitrary. EDIT: We recently found a solution for $w(m)$ as following:  $$w_m=\frac{(-1)^{m}}{2^{L}\binom{L}{k-m}}\frac{L-k}{L-k+m}\sum_{t=0}^{k-m}\binom{L}{t} =\frac{{k-L \choose m}}{2^{L}\binom{L}{k} \binom{k}{m}} \sum_{t=0}^{k-m}\binom{L}{t}$$ For interested readers, this solution is described in Robust k-mer frequency estimation using gapped k-mers . Now, having this result how can one solve for $\alpha_{k,m,l}$ in $w_m = c_0\sum_{l=0}^k \alpha_{k,m,l} \binom{L}{l}$? Note that here $c_0$ may depend on $k$ and $L$ but not $m$ or $l$. My guess is that $c_0^{-1}=2^L\binom{L}{k}$. Also $\alpha_{k,m,l}$ is a function of $k,m$ and $l$ and most likely symmetric in $l$ and $m$ as the above examples suggest.",,"['linear-algebra', 'sequences-and-series', 'combinatorics', 'power-series', 'recreational-mathematics']"
10,Proving the Product of Unitary Matrices is also Unitary,Proving the Product of Unitary Matrices is also Unitary,,"This is my attempt so far: Suppose I have two unitary matrices, A and B, such that $A^*A=I$ and $B^*B=I$. We want to show that $(AB)^*(AB)=I$. So we have that $$(AB)^*(AB)=B^*A^*AB=B^*IB=I.$$ Does this work as a proof? Do I need to show anything else?","This is my attempt so far: Suppose I have two unitary matrices, A and B, such that $A^*A=I$ and $B^*B=I$. We want to show that $(AB)^*(AB)=I$. So we have that $$(AB)^*(AB)=B^*A^*AB=B^*IB=I.$$ Does this work as a proof? Do I need to show anything else?",,['linear-algebra']
11,Simultaneous diagonalization of quadratic forms,Simultaneous diagonalization of quadratic forms,,"I would like to collect references (or direct quotations) about as many ""simultaneous diagonalization"" results in linear algebra as possible. Let $V$ be an $n$ -dimentional ( $n$ finite) vector space over the field $\mathbb{K}$ . Note that if the characteristic of $\mathbb{K}$ is $\neq$ from $2$ then speaking of quadratic forms is essentially the same as speaking of bilinear forms. Theorem 1. Let $\mathcal{S}$ be a set of diagonalizable operators on $V$ . Then $\mathcal{S}$ is simultaneously diagonalizable if and only if it commutes (i.e. $AB=BA\;\;\; \forall A,B\in\mathcal{S}$ ). Theorem 2. Let $\mathbb{K}=\mathbb{C}$ . Let $\langle\;|\;\rangle$ be a non-degenerate symmetric bilinear form and $b$ any symmetric bilinear form on $V$ . Write $b(\cdot,\cdot)=\langle A\cdot|\cdot\rangle$ for a $\langle\;|\;\rangle$ -symmetric operator $A$ . Suppose $A$ is diagonalizable with distinct eigenvalues $\lambda_1,\dots , \lambda_n$ . Then there is a $\langle\;|\;\rangle$ -orthonormal basis $\mathcal{B}$ in which the matrix of $b$ is diagonal with diagonal entries $\lambda_1,\dots , \lambda_n$ . Proof 2. Let $\mathcal{B}=\{ v_1,\dots , v_n \}$ a basis of eigenvectors for $A$ , with distinct eigenvalues $\lambda_1, \dots , \lambda_n$ . Then $\lambda_l\cdot\langle v_l|v_k \rangle=\langle \lambda_l\cdot v_l|v_k \rangle=\langle A(v_l) |v_k \rangle=\langle v_l |A^{T}(v_k) \rangle=\langle v_l| A(v_k)\rangle=\lambda_k\cdot\langle v_l|v_k \rangle$ , hence $(\lambda_l-\lambda_k)\cdot\langle v_l|v_k \rangle=0$ for all $l,k$ . Then eigenvectors of distinct eigenvalues are orthogonal. Furthermore, $\langle v_i|v_i \rangle\neq 0$ for all $i$ or $v_i$ would be orthogonal to $\mathcal{B}$ for some $i$ . Now let $\xi_i$ be a square root of $\lambda_i$ in $\mathbb{C}$ , and set $w_i:=\frac{1}{\xi_i}\cdot v_i$ . Then $\{ w_1,\dots , w_n \}$ is the basis we are seeking. Theorem 3. Let $\mathbb{K}=\mathbb{R}$ . Let $q$ be an indefinite quadratic form and $q'$ any quadratic form on $V$ . Suppose that, for every $v\in V$ , $q(v)=0$ implies $q'(v)=0$ . Then there is $\alpha\in\mathbb{R}$ such that $q'=\alpha \cdot q$ . Furthermore, if $\alpha\neq 0$ , also $q'$ is indefinite and, for all $v \in V$ , $q(v)=0$ if and only if $q'(v)=0$ . Note that in particular, by Sylvester's theorem, $q$ and $q'$ are simultaneously diagonalizable. Proof 3. See Elton, Indefinite quadratic forms and the invariance of the interval in Special Relativity . Theorem 4. Let $\mathbb{K}=\mathbb{R}$ . Let $q$ and $q'$ be two semidefinite quadratic forms on $V$ . Suppose that for all $v\in V$ $q(v)=0$ implies $q'(v)=0$ . Then $q$ and $q'$ are simultaneosly diagonalizable. Proof 4. See Elton, Indefinite quadratic forms and the invariance of the interval in Special Relativity . Theorem 5. Let $\mathbb{K}=\mathbb{R}$ . Let $\langle\;|\;\rangle$ be a non-degenerate symmetric bilinear form and $b$ any symmetric bilinear form on $V$ . Then there is a $\langle\;|\;\rangle$ -orthonormal basis $\mathcal{B}$ of $V$ in which the matrix of $b$ is diagonal. Proof 5. Write $b(\cdot,\cdot)=\langle A\cdot|\cdot\rangle$ for a $\langle\;|\;\rangle$ -symmetric operator $A$ . Then by the Spectral Theorem for real symmetric operators there is a $\langle\;|\;\rangle$ -orthogonal basis $\mathcal{B}=\{ e_1, \dots, e_n \}$ in which the matrix of $A$ is diagonal with diagonal entries $\lambda_1,\dots , \lambda_n$ . Then $b(e_i,e_j)=\langle Ae_i|e_j \rangle=\lambda_i\cdot\langle e_i|e_j \rangle=\lambda_i\cdot\delta_{ij}$ ( $\delta_{ij}$ is Kronecker's symbol), hence $\mathcal{B}$ diagonalizes $b$ . Theorem 6. Let $\mathbb{K}=\mathbb{C}$ . Let $\langle\;|\;\rangle$ be a Hermitian (i.e. a sesquilinear positive definite) form and $b$ any sesquilinear form on $V$ . Then there is a $\langle\;|\;\rangle$ -orthonormal basis $\mathcal{B}$ of $V$ in which the matrix of $b$ is diagonal. Proof 6. As in Theorem 5. but using the Spectral Theorem for Hermitian operators instead. Edit: I have collected two more theorems. Theorem 7. Let $\mathbb{K}$ be of characteristic $\neq 2$ . Let $\langle\; |\; \rangle_1$ , $\langle\; |\; \rangle_2$ be two non degenerate symmetric bilinear forms on $V$ . Let $\Phi_i:V\to V^*, v\mapsto \langle v | \;\rangle_i$ , $i=1,2$ . Then $\langle\; | \;\rangle_1$ and $\langle\; | \;\rangle_2$ are simultaneously diagonalizable if and only if $\Phi_2^{-1}\circ\Phi_1\in \mathrm{Aut}(V)$ is diagonalizable as an operator. Proof 7. See M.J.Wonenburger, Simultaneous Diagonalization of Symmetric Bilinear Forms . Remark: Note that $\Phi_2^{-1}\circ\Phi_1$ is symmetric w.r.t. $\langle\; | \;\rangle_2$ , and $\Phi_1^{-1}\circ\Phi_2$ is symmetric w.r.t. $\langle\; | \;\rangle_1$ . So, in case $\mathbb{K}=\mathbb{R}$ , we recover part of Theorem 5. Theorem 8. Let $\mathbb{K}$ be a real closed field . Let $f_1, f_2$ be two quadratic forms on $V$ such that there is no $v\neq 0$ in $V$ for which $f_1(v)=f_2(v)=0$ . Suppose $n=\dim V>2$ , then $f_1$ and $f_2$ are simultaneously diagonalizable. Proof 8. See M.J.Wonenburger, Simultaneous Diagonalization of Symmetric Bilinear Forms . Q1. I would like to know other theorems like the above ones (not deriving from them in a tautological way). Q2. Is there a more general theorem on the topic that treats all fields at once and reduces to (a subset of) the above theorems by specialization? Q3. What happens in characteristic $2$ ?","I would like to collect references (or direct quotations) about as many ""simultaneous diagonalization"" results in linear algebra as possible. Let be an -dimentional ( finite) vector space over the field . Note that if the characteristic of is from then speaking of quadratic forms is essentially the same as speaking of bilinear forms. Theorem 1. Let be a set of diagonalizable operators on . Then is simultaneously diagonalizable if and only if it commutes (i.e. ). Theorem 2. Let . Let be a non-degenerate symmetric bilinear form and any symmetric bilinear form on . Write for a -symmetric operator . Suppose is diagonalizable with distinct eigenvalues . Then there is a -orthonormal basis in which the matrix of is diagonal with diagonal entries . Proof 2. Let a basis of eigenvectors for , with distinct eigenvalues . Then , hence for all . Then eigenvectors of distinct eigenvalues are orthogonal. Furthermore, for all or would be orthogonal to for some . Now let be a square root of in , and set . Then is the basis we are seeking. Theorem 3. Let . Let be an indefinite quadratic form and any quadratic form on . Suppose that, for every , implies . Then there is such that . Furthermore, if , also is indefinite and, for all , if and only if . Note that in particular, by Sylvester's theorem, and are simultaneously diagonalizable. Proof 3. See Elton, Indefinite quadratic forms and the invariance of the interval in Special Relativity . Theorem 4. Let . Let and be two semidefinite quadratic forms on . Suppose that for all implies . Then and are simultaneosly diagonalizable. Proof 4. See Elton, Indefinite quadratic forms and the invariance of the interval in Special Relativity . Theorem 5. Let . Let be a non-degenerate symmetric bilinear form and any symmetric bilinear form on . Then there is a -orthonormal basis of in which the matrix of is diagonal. Proof 5. Write for a -symmetric operator . Then by the Spectral Theorem for real symmetric operators there is a -orthogonal basis in which the matrix of is diagonal with diagonal entries . Then ( is Kronecker's symbol), hence diagonalizes . Theorem 6. Let . Let be a Hermitian (i.e. a sesquilinear positive definite) form and any sesquilinear form on . Then there is a -orthonormal basis of in which the matrix of is diagonal. Proof 6. As in Theorem 5. but using the Spectral Theorem for Hermitian operators instead. Edit: I have collected two more theorems. Theorem 7. Let be of characteristic . Let , be two non degenerate symmetric bilinear forms on . Let , . Then and are simultaneously diagonalizable if and only if is diagonalizable as an operator. Proof 7. See M.J.Wonenburger, Simultaneous Diagonalization of Symmetric Bilinear Forms . Remark: Note that is symmetric w.r.t. , and is symmetric w.r.t. . So, in case , we recover part of Theorem 5. Theorem 8. Let be a real closed field . Let be two quadratic forms on such that there is no in for which . Suppose , then and are simultaneously diagonalizable. Proof 8. See M.J.Wonenburger, Simultaneous Diagonalization of Symmetric Bilinear Forms . Q1. I would like to know other theorems like the above ones (not deriving from them in a tautological way). Q2. Is there a more general theorem on the topic that treats all fields at once and reduces to (a subset of) the above theorems by specialization? Q3. What happens in characteristic ?","V n n \mathbb{K} \mathbb{K} \neq 2 \mathcal{S} V \mathcal{S} AB=BA\;\;\; \forall A,B\in\mathcal{S} \mathbb{K}=\mathbb{C} \langle\;|\;\rangle b V b(\cdot,\cdot)=\langle A\cdot|\cdot\rangle \langle\;|\;\rangle A A \lambda_1,\dots , \lambda_n \langle\;|\;\rangle \mathcal{B} b \lambda_1,\dots , \lambda_n \mathcal{B}=\{ v_1,\dots , v_n \} A \lambda_1, \dots , \lambda_n \lambda_l\cdot\langle v_l|v_k \rangle=\langle \lambda_l\cdot v_l|v_k \rangle=\langle A(v_l) |v_k \rangle=\langle v_l |A^{T}(v_k) \rangle=\langle v_l| A(v_k)\rangle=\lambda_k\cdot\langle v_l|v_k \rangle (\lambda_l-\lambda_k)\cdot\langle v_l|v_k \rangle=0 l,k \langle v_i|v_i \rangle\neq 0 i v_i \mathcal{B} i \xi_i \lambda_i \mathbb{C} w_i:=\frac{1}{\xi_i}\cdot v_i \{ w_1,\dots , w_n \} \mathbb{K}=\mathbb{R} q q' V v\in V q(v)=0 q'(v)=0 \alpha\in\mathbb{R} q'=\alpha \cdot q \alpha\neq 0 q' v \in V q(v)=0 q'(v)=0 q q' \mathbb{K}=\mathbb{R} q q' V v\in V q(v)=0 q'(v)=0 q q' \mathbb{K}=\mathbb{R} \langle\;|\;\rangle b V \langle\;|\;\rangle \mathcal{B} V b b(\cdot,\cdot)=\langle A\cdot|\cdot\rangle \langle\;|\;\rangle A \langle\;|\;\rangle \mathcal{B}=\{ e_1, \dots, e_n \} A \lambda_1,\dots , \lambda_n b(e_i,e_j)=\langle Ae_i|e_j \rangle=\lambda_i\cdot\langle e_i|e_j \rangle=\lambda_i\cdot\delta_{ij} \delta_{ij} \mathcal{B} b \mathbb{K}=\mathbb{C} \langle\;|\;\rangle b V \langle\;|\;\rangle \mathcal{B} V b \mathbb{K} \neq 2 \langle\; |\; \rangle_1 \langle\; |\; \rangle_2 V \Phi_i:V\to V^*, v\mapsto \langle v | \;\rangle_i i=1,2 \langle\; | \;\rangle_1 \langle\; | \;\rangle_2 \Phi_2^{-1}\circ\Phi_1\in \mathrm{Aut}(V) \Phi_2^{-1}\circ\Phi_1 \langle\; | \;\rangle_2 \Phi_1^{-1}\circ\Phi_2 \langle\; | \;\rangle_1 \mathbb{K}=\mathbb{R} \mathbb{K} f_1, f_2 V v\neq 0 V f_1(v)=f_2(v)=0 n=\dim V>2 f_1 f_2 2","['linear-algebra', 'quadratic-forms']"
12,Pfaffian properties,Pfaffian properties,,"Given a $2n\times 2n$ real skew-symmetric matrix $A$, its Pfaffian $\mathrm{Pf}$ is defined as: $$ \mathrm{Pf}(A) = \frac1{2^n n!}\sum_{\sigma\in S_{2n}}\mathrm{sgn}(\sigma)\prod_{i=1}^n A_{\sigma(2i-1),\sigma(2i)} $$ where $S_{2n}$ is the symmetric group, and $\mathrm{sgn}(\sigma)$ is the signature of the permutation $\sigma$. I'm looking for general properties of Pfaffians, which go beyond the ones stated in its wiki entry. More specifically, I'd like to know general conditions for the equality between the Pfaffian of a convex sum, and the convex sum of Pfaffians. In plain math, let $A$ and $B$ be $2n\times 2n$ real skew-symmetric matrices, and $\lambda \in [0,1]$. What are the conditions for the equality $$\mathrm{Pf}((1-\lambda)A +\lambda \;B) = (1-\lambda)\;\mathrm{Pf}(A) +\lambda\; \mathrm{Pf}(B)$$  to hold? A general reference on Pfaffians would also be very appreciated. Thanks,","Given a $2n\times 2n$ real skew-symmetric matrix $A$, its Pfaffian $\mathrm{Pf}$ is defined as: $$ \mathrm{Pf}(A) = \frac1{2^n n!}\sum_{\sigma\in S_{2n}}\mathrm{sgn}(\sigma)\prod_{i=1}^n A_{\sigma(2i-1),\sigma(2i)} $$ where $S_{2n}$ is the symmetric group, and $\mathrm{sgn}(\sigma)$ is the signature of the permutation $\sigma$. I'm looking for general properties of Pfaffians, which go beyond the ones stated in its wiki entry. More specifically, I'd like to know general conditions for the equality between the Pfaffian of a convex sum, and the convex sum of Pfaffians. In plain math, let $A$ and $B$ be $2n\times 2n$ real skew-symmetric matrices, and $\lambda \in [0,1]$. What are the conditions for the equality $$\mathrm{Pf}((1-\lambda)A +\lambda \;B) = (1-\lambda)\;\mathrm{Pf}(A) +\lambda\; \mathrm{Pf}(B)$$  to hold? A general reference on Pfaffians would also be very appreciated. Thanks,",,"['linear-algebra', 'matrices', 'pfaffian']"
13,What am I doing wrong solving this system of equations?,What am I doing wrong solving this system of equations?,,"$$\begin{cases} 2x_1+5x_2-8x_3=8\\ 4x_1+3x_2-9x_3=9\\ 2x_1+3x_2-5x_3=7\\ x_1+8x_2-7x_3=12 \end{cases}$$ From my elementary row operations, I get that it has no solution. (Row operations are to be read from top to bottom.) $$\left[\begin{array}{ccc|c} 2 & 5 & -8 & 8  \\ 4 & 3 & -9 & 9  \\ 2 & 3 & -5 & 7  \\ 1 & 8 & -7 & 12 \end{array}\right] \overset{\overset{\large{R_1\to R_1-R_3}}{{R_2\to R_2-2R_3}}}{\overset{R_3\to R_3-2R_4}{\large\longrightarrow}} \left[\begin{array}{ccc|c} 0 &  2  & -3 &  1  \\ 0 & -3  &  1 & -5  \\ 0 & -13 &  9 & -17 \\ 1 &  8  & -7 &  12  \end{array}\right] \overset{\overset{\large{R_3\,\leftrightarrow\, R_4}}{R_2\,\leftrightarrow\, R_3}}{\overset{R_1\,\leftrightarrow\,R_2}{\large\longrightarrow}} \left[\begin{array}{ccc|c} 1 &  8  & -7 &  12 \\ 0 &  2  & -3 &  1  \\ 0 & -3  &  1 & -5  \\ 0 & -13 &  9 & -17 \end{array}\right]$$ $$\overset{R_4\to R_4-R_3}{\large\longrightarrow} \left[\begin{array}{ccc|c} 1 &  8  & -7 &  12 \\ 0 &  2  & -3 &  1  \\ 0 & -3  &  1 & -5  \\ 0 &  10 &  8 & -12 \end{array}\right] \overset{\overset{\large{R_3\to R_3+R_2}}{R_4\to R_4-5R_2}}{\large\longrightarrow} \left[\begin{array}{ccc|c} 1 &  8 & -7  &  12 \\ 0 &  2 & -3  &  1  \\ 0 & -1 & -2  & -4  \\ 0 &  0 &  23 & -17 \end{array}\right] \overset{\overset{\large{R_2\to R_2+2R_3}}{R_3\to-R_3}}{\large\longrightarrow}$$ $$\left[\begin{array}{ccc|c} 1 & 8 & -7  &  12 \\ 0 & 0 & -7  & -7  \\ 0 & 1 &  2  &  4  \\ 0 & 0 &  23 & -17 \\ \end{array}\right] \overset{R_2\,\leftrightarrow\,R_3}{\large\longrightarrow} \left[\begin{array}{ccc|c} 1 & 8 & -7  &  12 \\ 0 & 1 &  2  &  4  \\ 0 & 0 & -7  & -7  \\ 0 & 0 &  23 & -17 \\ \end{array}\right]$$ However, the answer in the book $(3, 2, 1)$ fits the system. Was there an arithmetical mistake, or do I misunderstand something fundamentally?","From my elementary row operations, I get that it has no solution. (Row operations are to be read from top to bottom.) However, the answer in the book fits the system. Was there an arithmetical mistake, or do I misunderstand something fundamentally?","\begin{cases}
2x_1+5x_2-8x_3=8\\
4x_1+3x_2-9x_3=9\\
2x_1+3x_2-5x_3=7\\
x_1+8x_2-7x_3=12
\end{cases} \left[\begin{array}{ccc|c}
2 & 5 & -8 & 8  \\
4 & 3 & -9 & 9  \\
2 & 3 & -5 & 7  \\
1 & 8 & -7 & 12
\end{array}\right]
\overset{\overset{\large{R_1\to R_1-R_3}}{{R_2\to R_2-2R_3}}}{\overset{R_3\to R_3-2R_4}{\large\longrightarrow}}
\left[\begin{array}{ccc|c}
0 &  2  & -3 &  1  \\
0 & -3  &  1 & -5  \\
0 & -13 &  9 & -17 \\
1 &  8  & -7 &  12 
\end{array}\right]
\overset{\overset{\large{R_3\,\leftrightarrow\, R_4}}{R_2\,\leftrightarrow\, R_3}}{\overset{R_1\,\leftrightarrow\,R_2}{\large\longrightarrow}}
\left[\begin{array}{ccc|c}
1 &  8  & -7 &  12 \\
0 &  2  & -3 &  1  \\
0 & -3  &  1 & -5  \\
0 & -13 &  9 & -17
\end{array}\right] \overset{R_4\to R_4-R_3}{\large\longrightarrow}
\left[\begin{array}{ccc|c}
1 &  8  & -7 &  12 \\
0 &  2  & -3 &  1  \\
0 & -3  &  1 & -5  \\
0 &  10 &  8 & -12
\end{array}\right]
\overset{\overset{\large{R_3\to R_3+R_2}}{R_4\to R_4-5R_2}}{\large\longrightarrow}
\left[\begin{array}{ccc|c}
1 &  8 & -7  &  12 \\
0 &  2 & -3  &  1  \\
0 & -1 & -2  & -4  \\
0 &  0 &  23 & -17
\end{array}\right]
\overset{\overset{\large{R_2\to R_2+2R_3}}{R_3\to-R_3}}{\large\longrightarrow} \left[\begin{array}{ccc|c}
1 & 8 & -7  &  12 \\
0 & 0 & -7  & -7  \\
0 & 1 &  2  &  4  \\
0 & 0 &  23 & -17 \\
\end{array}\right]
\overset{R_2\,\leftrightarrow\,R_3}{\large\longrightarrow}
\left[\begin{array}{ccc|c}
1 & 8 & -7  &  12 \\
0 & 1 &  2  &  4  \\
0 & 0 & -7  & -7  \\
0 & 0 &  23 & -17 \\
\end{array}\right] (3, 2, 1)","['linear-algebra', 'systems-of-equations']"
14,"Alternative to Axler's ""Linear Algebra Done Right""","Alternative to Axler's ""Linear Algebra Done Right""",,"I have been using Axler's ""Linear Algebra Done Right."" In fact I have recommended it here often. I was wondering if there is a text at that level or higher that uses ""kernel"" rather than ""null space""? And that does not go so far out of it's way to avoid matrices. Thanks.","I have been using Axler's ""Linear Algebra Done Right."" In fact I have recommended it here often. I was wondering if there is a text at that level or higher that uses ""kernel"" rather than ""null space""? And that does not go so far out of it's way to avoid matrices. Thanks.",,['linear-algebra']
15,Why does the subspace need to go through the origin?,Why does the subspace need to go through the origin?,,"I understand that the main difference between a subspace and a hyperplane is that the subspace must go through the origin. Why does need to happen? In other words, why does a subspace always have to go through the origin? What restricts it from doing otherwise?","I understand that the main difference between a subspace and a hyperplane is that the subspace must go through the origin. Why does need to happen? In other words, why does a subspace always have to go through the origin? What restricts it from doing otherwise?",,['linear-algebra']
16,Show that $B$ is invertible if $B=A^2-2A+2I$ and $A^3=2I$,Show that  is invertible if  and,B B=A^2-2A+2I A^3=2I,"If $A$ is $40\times 40$ matrix such that $A^3=2I$ show that $B$ is invertible where $B=A^2-2A+2I$. I tried to evaluate $B(A-I)$ , $B(A+I)$ , $B(A-2I)$ ...  but I couldn't find anything.","If $A$ is $40\times 40$ matrix such that $A^3=2I$ show that $B$ is invertible where $B=A^2-2A+2I$. I tried to evaluate $B(A-I)$ , $B(A+I)$ , $B(A-2I)$ ...  but I couldn't find anything.",,['linear-algebra']
17,Prove that $\det(AB - BA) = \frac{1}{3}\left(\mathrm{Trace}(AB - BA)^3\right)$,Prove that,\det(AB - BA) = \frac{1}{3}\left(\mathrm{Trace}(AB - BA)^3\right),I want a correct and feasible answer to this question.  So does anyone have any creative ideas to prove this equation? $A$ and $B$ are $3\times3$ matrices. $\det(AB - BA) = \dfrac{1}{3}\operatorname{Trace}\left((AB - BA)^3\right)$ Answer: We can write and compute both sides to prove it but this is not a good solution!,I want a correct and feasible answer to this question.  So does anyone have any creative ideas to prove this equation? and are matrices. Answer: We can write and compute both sides to prove it but this is not a good solution!,A B 3\times3 \det(AB - BA) = \dfrac{1}{3}\operatorname{Trace}\left((AB - BA)^3\right),"['linear-algebra', 'matrices', 'determinant']"
18,"Is $1 = [1] = [[1]] = [[[ 1 ]]], \ldots$?",Is ?,"1 = [1] = [[1]] = [[[ 1 ]]], \ldots","Is the following true? Where [a] is a 1x1 matrix containing the object a. $$ \begin{bmatrix} 2 \end{bmatrix} =\\ \begin{bmatrix} \begin{bmatrix} 2 \end{bmatrix} \end{bmatrix} =\\ \begin{bmatrix} \begin{bmatrix} \begin{bmatrix} 2 \end{bmatrix} \end{bmatrix} \end{bmatrix}\\ \vdots $$ I am curious because I am writing a function for adding matrices, but there is no rule forbidding elements of matrices to be matrices; so I want to know if $$ \begin{bmatrix} \begin{bmatrix} \begin{bmatrix} 2 \end{bmatrix} \end{bmatrix} \end{bmatrix}  + \begin{bmatrix} \begin{bmatrix} 2 \end{bmatrix} \end{bmatrix} = 4 $$ after canonicalization or not. I also want to know if this holds for direct sum: $$ \begin{bmatrix} \begin{bmatrix} \begin{bmatrix} 2\\ 4 \end{bmatrix} \end{bmatrix} \end{bmatrix}  + \begin{bmatrix} \begin{bmatrix} 2\\ 4 \end{bmatrix} \end{bmatrix} =  \begin{bmatrix} 4\\ 8 \end{bmatrix} $$ I am aware that this messes up the convenient ""indexing"" property, as A[0] will no longer make sense; but matrices don't require to support this property anyway, we just have it for free for implementing them as arrays, so it's a nonissue. I am curious if there is any particular harm to the algebra if 1 = [1] = [[1]] = [[[1]]] that makes it unusable, or if it just causes minor nuisance? Breakages so far/severity: Indexing of a matrix(minor) [3] at 0 is 3, but 3 at 0 is undefined(since indexing is not defined for scalars), and it is not clear whether [[3; 4]] at 0 is [3; 4] which is what it would be in current algebra, whereas it would be 3 in proposed algebra. This is not very severe since matrices are not really arrays, but bilinear maps, so indexing them is fairly naive to begin with. This property won't work unless the given matrix is canonicalized before being indexed. Multiplication of matrix by scalar(minor/real damage) . There are some concerns over whether or not multiplication by scalar is invalid, but my previous argument is invalid. so I removed it.","Is the following true? Where [a] is a 1x1 matrix containing the object a. $$ \begin{bmatrix} 2 \end{bmatrix} =\\ \begin{bmatrix} \begin{bmatrix} 2 \end{bmatrix} \end{bmatrix} =\\ \begin{bmatrix} \begin{bmatrix} \begin{bmatrix} 2 \end{bmatrix} \end{bmatrix} \end{bmatrix}\\ \vdots $$ I am curious because I am writing a function for adding matrices, but there is no rule forbidding elements of matrices to be matrices; so I want to know if $$ \begin{bmatrix} \begin{bmatrix} \begin{bmatrix} 2 \end{bmatrix} \end{bmatrix} \end{bmatrix}  + \begin{bmatrix} \begin{bmatrix} 2 \end{bmatrix} \end{bmatrix} = 4 $$ after canonicalization or not. I also want to know if this holds for direct sum: $$ \begin{bmatrix} \begin{bmatrix} \begin{bmatrix} 2\\ 4 \end{bmatrix} \end{bmatrix} \end{bmatrix}  + \begin{bmatrix} \begin{bmatrix} 2\\ 4 \end{bmatrix} \end{bmatrix} =  \begin{bmatrix} 4\\ 8 \end{bmatrix} $$ I am aware that this messes up the convenient ""indexing"" property, as A[0] will no longer make sense; but matrices don't require to support this property anyway, we just have it for free for implementing them as arrays, so it's a nonissue. I am curious if there is any particular harm to the algebra if 1 = [1] = [[1]] = [[[1]]] that makes it unusable, or if it just causes minor nuisance? Breakages so far/severity: Indexing of a matrix(minor) [3] at 0 is 3, but 3 at 0 is undefined(since indexing is not defined for scalars), and it is not clear whether [[3; 4]] at 0 is [3; 4] which is what it would be in current algebra, whereas it would be 3 in proposed algebra. This is not very severe since matrices are not really arrays, but bilinear maps, so indexing them is fairly naive to begin with. This property won't work unless the given matrix is canonicalized before being indexed. Multiplication of matrix by scalar(minor/real damage) . There are some concerns over whether or not multiplication by scalar is invalid, but my previous argument is invalid. so I removed it.",,['linear-algebra']
19,Is it acceptable to have a fraction in an eigenvector?,Is it acceptable to have a fraction in an eigenvector?,,"The professor teaching a class I am taking wants me to find the eigenvalues and the eigenvectors for the following matrix below. $$\begin{bmatrix}-5 & 5\\4 & 3\end{bmatrix}$$ I have succeeded in getting the eigenvalues, which are $\lambda= \{ 5,-7 \}$ . When finding the eigenvector for $\lambda= 5$ , I get $\begin{bmatrix}1/2\\1 \end{bmatrix}$ . However, the correct answer is $\begin{bmatrix}1\\2 \end{bmatrix}$ . I have tried doing this question using multiple online matrix calculators. One of which gives me $\begin{bmatrix}1/2\\1 \end{bmatrix}$ , and the other gives me $\begin{bmatrix}1\\2 \end{bmatrix}$ . The online calculator that gave me $\begin{bmatrix}1\\2 \end{bmatrix}$ explains, that y=2, hence $\begin{bmatrix}1/2·2\\1·2 \end{bmatrix} = \begin{bmatrix}1\\2 \end{bmatrix}$ . What I do not understand is, why is y must equal to 2?Is it because there cannot be a fraction in an eigenvector?","The professor teaching a class I am taking wants me to find the eigenvalues and the eigenvectors for the following matrix below. I have succeeded in getting the eigenvalues, which are . When finding the eigenvector for , I get . However, the correct answer is . I have tried doing this question using multiple online matrix calculators. One of which gives me , and the other gives me . The online calculator that gave me explains, that y=2, hence . What I do not understand is, why is y must equal to 2?Is it because there cannot be a fraction in an eigenvector?","\begin{bmatrix}-5 & 5\\4 & 3\end{bmatrix} \lambda= \{ 5,-7 \} \lambda= 5 \begin{bmatrix}1/2\\1 \end{bmatrix} \begin{bmatrix}1\\2 \end{bmatrix} \begin{bmatrix}1/2\\1 \end{bmatrix} \begin{bmatrix}1\\2 \end{bmatrix} \begin{bmatrix}1\\2 \end{bmatrix} \begin{bmatrix}1/2·2\\1·2 \end{bmatrix} = \begin{bmatrix}1\\2 \end{bmatrix}","['linear-algebra', 'matrices', 'eigenvalues-eigenvectors']"
20,What is the physical significance of the determinants of orthogonal matrices having the value of $\pm 1$?,What is the physical significance of the determinants of orthogonal matrices having the value of ?,\pm 1,"I'm new to linear algebra and while studying orthogonal matrices, I found out that their determinant is always $\pm 1$ . Why is that so? What could be the physical significance behind it? I know that linear algebra can be intuitive when visualized, which 3B1B's videos made me realize, hence I would like to know more about this. Thanks in advance!","I'm new to linear algebra and while studying orthogonal matrices, I found out that their determinant is always . Why is that so? What could be the physical significance behind it? I know that linear algebra can be intuitive when visualized, which 3B1B's videos made me realize, hence I would like to know more about this. Thanks in advance!",\pm 1,"['linear-algebra', 'matrices', 'determinant', 'intuition', 'orthogonal-matrices']"
21,Map closed under addition but not multiplication,Map closed under addition but not multiplication,,I have been helping undergrads in an introduction to linear algebra course. When solving some exercise consisting in showing that a map is linear some get lazy after proving that it is closed under addition and do not prove the closure under scalar multiplication. I wanted to confront them with an example of a map closed under addition but not under the multiplication but could not come up with an example. Do you have any?,I have been helping undergrads in an introduction to linear algebra course. When solving some exercise consisting in showing that a map is linear some get lazy after proving that it is closed under addition and do not prove the closure under scalar multiplication. I wanted to confront them with an example of a map closed under addition but not under the multiplication but could not come up with an example. Do you have any?,,"['linear-algebra', 'education']"
22,Are the functions $\sin^n(x)$ linearly independent?,Are the functions  linearly independent?,\sin^n(x),"The following problem is from Golan's linear algebra book. I have posted a proposed solution in the answers. Problem: For $n\in \mathbb{N}$, consider the function $f_n(x)=\sin^n(x)$ as an element of the vector space $\mathbb{R}^\mathbb{R}$ over $\mathbb{R}$. Is the subset $\{f_n:\ n\in\mathbb{N}\}$ linearly independent?","The following problem is from Golan's linear algebra book. I have posted a proposed solution in the answers. Problem: For $n\in \mathbb{N}$, consider the function $f_n(x)=\sin^n(x)$ as an element of the vector space $\mathbb{R}^\mathbb{R}$ over $\mathbb{R}$. Is the subset $\{f_n:\ n\in\mathbb{N}\}$ linearly independent?",,['linear-algebra']
23,Characteristic polynomial of a nilpotent matrix,Characteristic polynomial of a nilpotent matrix,,"Let $A$ be $n\times n$ nilpotent matrix. How to calculate  its characteristic polynomial? I know it should be $X^n$, but I don't know why?","Let $A$ be $n\times n$ nilpotent matrix. How to calculate  its characteristic polynomial? I know it should be $X^n$, but I don't know why?",,['linear-algebra']
24,if matrix such $AA^T=A^2$ then $A$ is symmetric?,if matrix such  then  is symmetric?,AA^T=A^2 A,"let matrix $A_{n\times n}$ is  real matrix,such $AA^T=A^2$, The transpose of matrix $A$ is written $A^T$, show that : the matrix $A$ is Symmetric matrices maybe this problem have more methos,because it is know that if matrix $A$ is symmetric,then we have $AA^T=A^2$,But for my problem,I can't prove it.Thank you","let matrix $A_{n\times n}$ is  real matrix,such $AA^T=A^2$, The transpose of matrix $A$ is written $A^T$, show that : the matrix $A$ is Symmetric matrices maybe this problem have more methos,because it is know that if matrix $A$ is symmetric,then we have $AA^T=A^2$,But for my problem,I can't prove it.Thank you",,['linear-algebra']
25,A matrix satisfying $AB-BA=B$,A matrix satisfying,AB-BA=B,If $A$ and $B$ are two matrices of $\mathcal{M}_n(\mathbb{R}$) such that $$AB-BA=B$$ how can we prove that $B$ isn't invertible? my attempt: I found that $\mathrm{tr}(B)=0$ but I know that this is insufficient. Thanks,If $A$ and $B$ are two matrices of $\mathcal{M}_n(\mathbb{R}$) such that $$AB-BA=B$$ how can we prove that $B$ isn't invertible? my attempt: I found that $\mathrm{tr}(B)=0$ but I know that this is insufficient. Thanks,,['linear-algebra']
26,Is $\mathbb R^2$ a field?,Is  a field?,\mathbb R^2,"I'm new to this very interesting world of mathematics, and I'm trying to learn some linear algebra from Khan academy. In the world of vector spaces and fields, I keep coming across the definition of $\mathbb R^2$ as a vector space ontop of the field $\mathbb R$. This makes me think, Why can't $\mathbb R^2$ be a field of its own?  Would that make $\mathbb R^2$ a field and a vector space? Thanks","I'm new to this very interesting world of mathematics, and I'm trying to learn some linear algebra from Khan academy. In the world of vector spaces and fields, I keep coming across the definition of $\mathbb R^2$ as a vector space ontop of the field $\mathbb R$. This makes me think, Why can't $\mathbb R^2$ be a field of its own?  Would that make $\mathbb R^2$ a field and a vector space? Thanks",,['linear-algebra']
27,Is the converse of Cayley-Hamilton Theorem true?,Is the converse of Cayley-Hamilton Theorem true?,,"The question is motivated from the following problem: Let $I\neq A\neq -I$, where $I$ is the identity matrix and $A$ is a real $2\times 2$ matrix. If $A=A^{-1}$, then the trace of $A$ is $$ (A) 2 \quad(B)1 \quad(C)0 \quad (D)-1 \quad (E)-2$$ Since $A=A^{-1}$, $A^2=I$. If the converse of Cayley-Hamilton Theorem is true, then $\lambda^2=1$ and thus $\lambda=\pm1$. And then $\rm{trace}(A)=1+(-1)=0$. Here are my questions: Is  $C$ the answer to the quoted problem? Is the converse of Cayley-Hamilton Theorem, i.e.,""for the square real matrix $A$, if $p(A)=0$, then $p(\lambda)$ is the characteristic polynomial of the matrix $A$"" true? If it is not, then what's the right method to solve the problem above?","The question is motivated from the following problem: Let $I\neq A\neq -I$, where $I$ is the identity matrix and $A$ is a real $2\times 2$ matrix. If $A=A^{-1}$, then the trace of $A$ is $$ (A) 2 \quad(B)1 \quad(C)0 \quad (D)-1 \quad (E)-2$$ Since $A=A^{-1}$, $A^2=I$. If the converse of Cayley-Hamilton Theorem is true, then $\lambda^2=1$ and thus $\lambda=\pm1$. And then $\rm{trace}(A)=1+(-1)=0$. Here are my questions: Is  $C$ the answer to the quoted problem? Is the converse of Cayley-Hamilton Theorem, i.e.,""for the square real matrix $A$, if $p(A)=0$, then $p(\lambda)$ is the characteristic polynomial of the matrix $A$"" true? If it is not, then what's the right method to solve the problem above?",,['linear-algebra']
28,Is it true that any matrix in $M_2(\mathbb R)$ is the sum of two squares?,Is it true that any matrix in  is the sum of two squares?,M_2(\mathbb R),"I recently show that every polynomial with real coefficient and $P$ is always is a sum of two squares of polynomials, therefore positive. These questions also appear often in arithmetic. What if we change polynomials by matrices? I ask my professor and unfortunately he had no idea if this result is already shown or not. I tried, unsuccessfully, several tracks : First denote that $$\forall M\in M_n(\mathbb R) , M=1/2[(A+A{}^t)+(A-A{}^t)].$$ and perhaps used the following result : Any symmetric positive matrix admits a square root Proof. By spectral theorem, there exist $\Omega \in \mathbb O_n(\mathbb R)$ and $(\lambda_1,...\lambda_n)\in (\mathbb R^{+})^n$ such that $S=\Omega D \Omega^{-1}$ with $D$ the diagonal matrix formed by the positive eigenvalues​​. Let $D'$ the diagonal matrix formed of the roots of the eigenvalues​​. Then , $D'²=S$ and $D'{}^t=D'$ thus she is a symmetric positive matrix. Question : Is it possible to believe that all matrices (say in $M_2(\mathbb R)$ ) are the sum of two squares?","I recently show that every polynomial with real coefficient and is always is a sum of two squares of polynomials, therefore positive. These questions also appear often in arithmetic. What if we change polynomials by matrices? I ask my professor and unfortunately he had no idea if this result is already shown or not. I tried, unsuccessfully, several tracks : First denote that and perhaps used the following result : Any symmetric positive matrix admits a square root Proof. By spectral theorem, there exist and such that with the diagonal matrix formed by the positive eigenvalues​​. Let the diagonal matrix formed of the roots of the eigenvalues​​. Then , and thus she is a symmetric positive matrix. Question : Is it possible to believe that all matrices (say in ) are the sum of two squares?","P \forall M\in M_n(\mathbb R) , M=1/2[(A+A{}^t)+(A-A{}^t)]. \Omega \in \mathbb O_n(\mathbb R) (\lambda_1,...\lambda_n)\in (\mathbb R^{+})^n S=\Omega D \Omega^{-1} D D' D'²=S D'{}^t=D' M_2(\mathbb R)","['linear-algebra', 'matrices']"
29,Canonical Isomorphism Between $\mathbf{V}$ and $(\mathbf{V}^*)^*$,Canonical Isomorphism Between  and,\mathbf{V} (\mathbf{V}^*)^*,"For the finite-dimensional case, we have a canonical isomorphism between $\mathbf{V}$, a vector space with the usual addition and scalar multiplication, and $(\mathbf{V}^*)^*$, the ""dual of the dual of $\mathbf{V}$."" This canonical isomorphism means that the isomorphism is always the same, independent of additional choices. We can define a map $I : \mathbf{V} \to (\mathbf{V}^*)^*$ by $$x \mapsto I(x) \in (\mathbf{V}^*)^* \ \text{ where } \ I(x)(f) = f(x) \ \text{for any } \  f \in \mathbf{V}^*$$ My Question: what can go wrong in the infinite-dimensional case? The notes I am studying remark that if $\mathbf{V}$ is finite-dimensional, then $I$ is an isomorphism, but in the infinite-dimensional case we can go wrong? How?","For the finite-dimensional case, we have a canonical isomorphism between $\mathbf{V}$, a vector space with the usual addition and scalar multiplication, and $(\mathbf{V}^*)^*$, the ""dual of the dual of $\mathbf{V}$."" This canonical isomorphism means that the isomorphism is always the same, independent of additional choices. We can define a map $I : \mathbf{V} \to (\mathbf{V}^*)^*$ by $$x \mapsto I(x) \in (\mathbf{V}^*)^* \ \text{ where } \ I(x)(f) = f(x) \ \text{for any } \  f \in \mathbf{V}^*$$ My Question: what can go wrong in the infinite-dimensional case? The notes I am studying remark that if $\mathbf{V}$ is finite-dimensional, then $I$ is an isomorphism, but in the infinite-dimensional case we can go wrong? How?",,"['linear-algebra', 'vector-spaces']"
30,Advanced Linear Algebra courses in graduate schools,Advanced Linear Algebra courses in graduate schools,,"After studying general a linear algebra course, how would an advanced linear algebra course differ from the general course? And would an advanced linear algebra course be taught in graduate schools?","After studying general a linear algebra course, how would an advanced linear algebra course differ from the general course? And would an advanced linear algebra course be taught in graduate schools?",,"['linear-algebra', 'reference-request', 'soft-question', 'advice']"
31,"If $A$ is an $n \times n$ matrix such that $A^2=0$, is $A+I_{n}$ invertible?","If  is an  matrix such that , is  invertible?",A n \times n A^2=0 A+I_{n},"If $A$ is an $n \times n$ matrix such that $A^2=0$, is $A+I_{n}$ invertible? This question yielded two different proofs from my professors, which managed to get conflicting results (true and false). Could you please weigh in and explain what's happening, and offer a working proof? Proof that it is invertible: Consider matrix $A-I_{n}$. Multiplying $(A+I_{n})$ by $(A-I_{n})$ we get $A^2-AI_{n}+AI_{n}-I^2_{n}$. This simplifies to $A^2-I^2_{n}$ which is equal to $-I_{n}$, since $A^2=0$. So, the professor argued, since we have shown that there exists a $B$ such that $(A+I_{n})$ times $B$ is equal to $I$, $(A+I_{n})$ must be invertible. I am afraid, though, that she forgot about the negative sign that was leftover in front of the $I$ -- from what I understand, $(A+I_{n})$*$(A-I_{n})$=$-I$ does not mean that $(A+I_{n})$ is invertible. Proof that it is not invertible: Assume that $A(x)=0$ has a non-trivial solution. Now, given $(A+I_{n})(x)=\vec{0}$, multiply both sides by $A$. We get $A(A+I_{n})(x)=A(\vec{0})$, which can be written as $(A^2+A)(x)=\vec{0}$, which simplifies to $A(x)=0$, as $A^2=0$. Since we assumed that $A(x)=0$ has a non-trivial solution, we just demonstrated that $(A+I_{n})$ has a non-trivial solution, too. Hence, it is not invertible. I am not sure if I reproduced the second proof in complete accuracy (I think I did), but the idea was to show that if $A(x)=\vec{0}$ has a non-trivial solution, $A(A+I_{n})$ does too, rendering $A(A+I_{n})$ non-invertible. But regardless of the proofs, I can think of examples that show that at least in some cases, the statement is true; consider matrices $\begin{bmatrix} 0 & 0\\  0 & 0 \end{bmatrix}$ and $\begin{bmatrix} 0 & 1\\  0 & 0 \end{bmatrix}$ which, when added $I_{2}$ to, become invertible. Thanks a lot!","If $A$ is an $n \times n$ matrix such that $A^2=0$, is $A+I_{n}$ invertible? This question yielded two different proofs from my professors, which managed to get conflicting results (true and false). Could you please weigh in and explain what's happening, and offer a working proof? Proof that it is invertible: Consider matrix $A-I_{n}$. Multiplying $(A+I_{n})$ by $(A-I_{n})$ we get $A^2-AI_{n}+AI_{n}-I^2_{n}$. This simplifies to $A^2-I^2_{n}$ which is equal to $-I_{n}$, since $A^2=0$. So, the professor argued, since we have shown that there exists a $B$ such that $(A+I_{n})$ times $B$ is equal to $I$, $(A+I_{n})$ must be invertible. I am afraid, though, that she forgot about the negative sign that was leftover in front of the $I$ -- from what I understand, $(A+I_{n})$*$(A-I_{n})$=$-I$ does not mean that $(A+I_{n})$ is invertible. Proof that it is not invertible: Assume that $A(x)=0$ has a non-trivial solution. Now, given $(A+I_{n})(x)=\vec{0}$, multiply both sides by $A$. We get $A(A+I_{n})(x)=A(\vec{0})$, which can be written as $(A^2+A)(x)=\vec{0}$, which simplifies to $A(x)=0$, as $A^2=0$. Since we assumed that $A(x)=0$ has a non-trivial solution, we just demonstrated that $(A+I_{n})$ has a non-trivial solution, too. Hence, it is not invertible. I am not sure if I reproduced the second proof in complete accuracy (I think I did), but the idea was to show that if $A(x)=\vec{0}$ has a non-trivial solution, $A(A+I_{n})$ does too, rendering $A(A+I_{n})$ non-invertible. But regardless of the proofs, I can think of examples that show that at least in some cases, the statement is true; consider matrices $\begin{bmatrix} 0 & 0\\  0 & 0 \end{bmatrix}$ and $\begin{bmatrix} 0 & 1\\  0 & 0 \end{bmatrix}$ which, when added $I_{2}$ to, become invertible. Thanks a lot!",,"['linear-algebra', 'matrices', 'inverse']"
32,What does $\mathbb{R}^n \to \mathbb{R}^m$ mean? And what is $\mathbb{R}^n$?,What does  mean? And what is ?,\mathbb{R}^n \to \mathbb{R}^m \mathbb{R}^n,What the does $\mathbb{R^n}$ mean? For example if something says that it is a transformation $T:\mathbb{R}^2 \rightarrow \mathbb{R}^3$. Does that mean that $\mathbb{R}^2 = 2 \times 2$ matrix? and that $\mathbb{R}^3 = 3 \times 3 $ matrix?,What the does $\mathbb{R^n}$ mean? For example if something says that it is a transformation $T:\mathbb{R}^2 \rightarrow \mathbb{R}^3$. Does that mean that $\mathbb{R}^2 = 2 \times 2$ matrix? and that $\mathbb{R}^3 = 3 \times 3 $ matrix?,,['linear-algebra']
33,Limit of a matrix multiplication,Limit of a matrix multiplication,,How can I calculate this limit: $\displaystyle\lim_{n\to\infty}\begin{bmatrix}0.9 & 0.2\\0.1 & 0.8\end{bmatrix}^n$ What is the tool that i need to aply? eigenvalues and eigenvectors? diagonalization? canonical form? (This came in a contest and was the only problem i cannont have an idea for solve it).,How can I calculate this limit: $\displaystyle\lim_{n\to\infty}\begin{bmatrix}0.9 & 0.2\\0.1 & 0.8\end{bmatrix}^n$ What is the tool that i need to aply? eigenvalues and eigenvectors? diagonalization? canonical form? (This came in a contest and was the only problem i cannont have an idea for solve it).,,['linear-algebra']
34,What is the mathematical intuition behind àl-jàbrà?,What is the mathematical intuition behind àl-jàbrà?,,"The term algebra comes from the arabic term àl-jàbrà that means ""to force"" , ""to restore"" . Over centuries mathematicians, in east and west, celebrate by this term mathematical disciplines. What is the mathematical intuition behind this term ( ""to force"" , ""to restore"" ) and modern derivatives such as in abstract algebra and algebraic geometry? Append found the following information : In Arabic, al- is the definite article ""the."" The first noun in the title is jebr ""reunion of broken parts,"" from the verb jabara ""to reunite, to consolidate."" The second noun is from the verb qabala, with meanings that include ""to place in front of, to balance, to oppose, to set equal."" Together the two nouns describe some of the manipulations so common in algebra: combining like terms, transposing a term to the opposite side of an equation, setting two quantities equal, etc. Because the original Arabic title was so long, and because it was in Arabic, Europeans soon shortened it. The result was algeber or something phonetically similar, which then took on the meanings of both nouns and eventually acquired its modern sense...","The term algebra comes from the arabic term àl-jàbrà that means ""to force"" , ""to restore"" . Over centuries mathematicians, in east and west, celebrate by this term mathematical disciplines. What is the mathematical intuition behind this term ( ""to force"" , ""to restore"" ) and modern derivatives such as in abstract algebra and algebraic geometry? Append found the following information : In Arabic, al- is the definite article ""the."" The first noun in the title is jebr ""reunion of broken parts,"" from the verb jabara ""to reunite, to consolidate."" The second noun is from the verb qabala, with meanings that include ""to place in front of, to balance, to oppose, to set equal."" Together the two nouns describe some of the manipulations so common in algebra: combining like terms, transposing a term to the opposite side of an equation, setting two quantities equal, etc. Because the original Arabic title was so long, and because it was in Arabic, Europeans soon shortened it. The result was algeber or something phonetically similar, which then took on the meanings of both nouns and eventually acquired its modern sense...",,"['linear-algebra', 'abstract-algebra', 'terminology', 'math-history']"
35,"While proving that every vector space has a basis, why are only finite linear combinations used in the proof?","While proving that every vector space has a basis, why are only finite linear combinations used in the proof?",,"Statement : Every vector space has a basis Standard Proof :It is observed that a maximal linearly independent set is a basis.   Let $\mathscr{Y}$ be a chain of linearly independent subsets of a vector space $\mathscr{V}$. The union of such a set can serve as an upper bound for it.To apply Zorn's lemma,we have to check whether the union is linearly independent? Well, if $t_1,\dots,t_n$ belong to the union, then each $t_i$ belongs to some linearly independent set $L_i\in \mathscr{Y}$. Because $\mathscr{Y}$ is a chain, one of these sets $L_i$ contains all the others. If that is $L_j$, then the linear independence of $L_j$ implies that no non-trivial linear combination of $t_1,\dots,t_n$ can be zero, which proves that the union of the sets in $\mathscr{Y}$ is linearly independent. Therefore, by Zorn’s lemma, there is a maximal linearly independent set and hence a basis. My question : Why are we using only finite linear combinations to show that the union is linearly independent. Surely, if the union is infinite,then there do exist many infinite linear combinations of elements of the union, which cannot be proven to be linearly independent by the same reasoning. I suspect, that perhaps we are not concerned with infinite linear combinations due to issues of convergence, but I'm not sure. Clear answers will be appreciated.","Statement : Every vector space has a basis Standard Proof :It is observed that a maximal linearly independent set is a basis.   Let $\mathscr{Y}$ be a chain of linearly independent subsets of a vector space $\mathscr{V}$. The union of such a set can serve as an upper bound for it.To apply Zorn's lemma,we have to check whether the union is linearly independent? Well, if $t_1,\dots,t_n$ belong to the union, then each $t_i$ belongs to some linearly independent set $L_i\in \mathscr{Y}$. Because $\mathscr{Y}$ is a chain, one of these sets $L_i$ contains all the others. If that is $L_j$, then the linear independence of $L_j$ implies that no non-trivial linear combination of $t_1,\dots,t_n$ can be zero, which proves that the union of the sets in $\mathscr{Y}$ is linearly independent. Therefore, by Zorn’s lemma, there is a maximal linearly independent set and hence a basis. My question : Why are we using only finite linear combinations to show that the union is linearly independent. Surely, if the union is infinite,then there do exist many infinite linear combinations of elements of the union, which cannot be proven to be linearly independent by the same reasoning. I suspect, that perhaps we are not concerned with infinite linear combinations due to issues of convergence, but I'm not sure. Clear answers will be appreciated.",,['linear-algebra']
36,Why is some power of a permutation matrix always the identity?,Why is some power of a permutation matrix always the identity?,,"If you take powers of a permutation, why is some  $$ P^k = I $$ Find a 5 by 5 permutation  $$ P $$ so that the smallest power to equal I is $$ P^6 = I $$ (This is a challenge question, Combine a 2 by 2 block with a 3 by 3 block.) I couldn't solve the question anyway, but what does 2  by 2 block mean? Is block another way of saying matrix? Thanks","If you take powers of a permutation, why is some  $$ P^k = I $$ Find a 5 by 5 permutation  $$ P $$ so that the smallest power to equal I is $$ P^6 = I $$ (This is a challenge question, Combine a 2 by 2 block with a 3 by 3 block.) I couldn't solve the question anyway, but what does 2  by 2 block mean? Is block another way of saying matrix? Thanks",,"['linear-algebra', 'abstract-algebra']"
37,Eigenvalues of product matrix,Eigenvalues of product matrix,,"I have two matrices, both positive definite, real symmetric and one is diagonal. What can I say about lower and upper bound of the eigenvalues of the product matrix in terms the of lower and upper bounds on eigenvalues of those two matrices.","I have two matrices, both positive definite, real symmetric and one is diagonal. What can I say about lower and upper bound of the eigenvalues of the product matrix in terms the of lower and upper bounds on eigenvalues of those two matrices.",,[]
38,$Tr(A^2)=Tr(A^3)=Tr(A^4)$ then find $Tr(A)$,then find,Tr(A^2)=Tr(A^3)=Tr(A^4) Tr(A),"Let $A$ be a non singular $n\times n$ matrix  with all eigenvalues real and  $$Tr(A^2)=Tr(A^3)=Tr(A^4).$$Find $Tr(A)$. I considered $2\times 2$ matrix $\begin{bmatrix}a&b\\c&d\end{bmatrix}$ and tried computing traces of $A^2,A^3,A^4$ and ended up with following $Tr(A^2)=Tr(A)^2-2\det(A)$ $Tr(A^3)=Tr(A)^3-3Tr(A)\det(A)$ $Tr(A^4)=Tr(A)^4-4Tr(A)^2\det(A)+2\det(A)$ I have no idea how to proceed from here...","Let $A$ be a non singular $n\times n$ matrix  with all eigenvalues real and  $$Tr(A^2)=Tr(A^3)=Tr(A^4).$$Find $Tr(A)$. I considered $2\times 2$ matrix $\begin{bmatrix}a&b\\c&d\end{bmatrix}$ and tried computing traces of $A^2,A^3,A^4$ and ended up with following $Tr(A^2)=Tr(A)^2-2\det(A)$ $Tr(A^3)=Tr(A)^3-3Tr(A)\det(A)$ $Tr(A^4)=Tr(A)^4-4Tr(A)^2\det(A)+2\det(A)$ I have no idea how to proceed from here...",,"['linear-algebra', 'abstract-algebra']"
39,Ideas of finding counterexamples?,Ideas of finding counterexamples?,,"The questions are from an exercise in Gibert Strang's Linear Algebra . Construct $2$ by $2$ matrices such that the eigenvalues of $AB$ are not the products of the eigenvalues of $A$ and $B$, and the eigenvalues of $A+B$ are not the sums of the individual eigenvalues. It's obvious that either $A$ or $B$ is NOT diagonal matrix. Here is my question: How should I approach the construction? Any heuristics? Added: This seems to be a rather ""stupid"" question. Trial and error with MATLAB may lead to the results. However, I'd like to go a little bit further. ""Trial and error"" can be viewed as one kind of method for counterexample finding. My second question may be more ""stupid"" and very vague: Can any one come up with a ""general idea"" for the construction of counterexamples in mathematics?","The questions are from an exercise in Gibert Strang's Linear Algebra . Construct $2$ by $2$ matrices such that the eigenvalues of $AB$ are not the products of the eigenvalues of $A$ and $B$, and the eigenvalues of $A+B$ are not the sums of the individual eigenvalues. It's obvious that either $A$ or $B$ is NOT diagonal matrix. Here is my question: How should I approach the construction? Any heuristics? Added: This seems to be a rather ""stupid"" question. Trial and error with MATLAB may lead to the results. However, I'd like to go a little bit further. ""Trial and error"" can be viewed as one kind of method for counterexample finding. My second question may be more ""stupid"" and very vague: Can any one come up with a ""general idea"" for the construction of counterexamples in mathematics?",,['linear-algebra']
40,"What does ""extend linearly"" mean in linear algebra?","What does ""extend linearly"" mean in linear algebra?",,"The following proposition is from one of Gowers's articles : Let $X$ be a vector space, and $x\in X$ , $x\neq {\bf 0}$ . Then there exists a linear map $g:X\to {\mathbb R}$ such that $g(x)\neq 0$ . The existence of this map can be proved as follows. Using the axiom of choice, one can show that the vector $x$ can be extended to a basis of $X$ . Let $g(x)=1$ , let $g(y)=0$ and extend linearly . Here are my questions: What does "" extend linearly "" mean in the proof? I don't see any context about what $y$ is in his proof. So what is it here?","The following proposition is from one of Gowers's articles : Let be a vector space, and , . Then there exists a linear map such that . The existence of this map can be proved as follows. Using the axiom of choice, one can show that the vector can be extended to a basis of . Let , let and extend linearly . Here are my questions: What does "" extend linearly "" mean in the proof? I don't see any context about what is in his proof. So what is it here?",X x\in X x\neq {\bf 0} g:X\to {\mathbb R} g(x)\neq 0 x X g(x)=1 g(y)=0 y,[]
41,Proving an inequality with Cauchy-Schwarz,Proving an inequality with Cauchy-Schwarz,,"In the ""User's guide to viscosity solutions"" by Crandall, Ishii and Lions ( link ), they make the following claim (inequality (A.4) p. 58) : Given $x$ , $\xi$ $\in \mathbb{R}^n$ , $A \in \cal{S}(n)$ (space of symmetric $n \times n$ matrices) , for all $\varepsilon >0$ , the Cauchy-Schwarz inequality yields $$\langle Ax,x \rangle \leq \langle (A+\varepsilon A^2) \xi,\xi \rangle+\left(\frac{1}{\varepsilon} + \|A\|\right)|x-\xi|^2,$$ where I guess $\|A\|$ is the spectral radius of $A$ . I have tried without success to prove this inequality and would appreciate some help.","In the ""User's guide to viscosity solutions"" by Crandall, Ishii and Lions ( link ), they make the following claim (inequality (A.4) p. 58) : Given , , (space of symmetric matrices) , for all , the Cauchy-Schwarz inequality yields where I guess is the spectral radius of . I have tried without success to prove this inequality and would appreciate some help.","x \xi \in \mathbb{R}^n A \in \cal{S}(n) n \times n \varepsilon >0 \langle Ax,x \rangle \leq \langle (A+\varepsilon A^2) \xi,\xi \rangle+\left(\frac{1}{\varepsilon} + \|A\|\right)|x-\xi|^2, \|A\| A","['linear-algebra', 'inequality', 'hilbert-spaces', 'quadratic-forms', 'cauchy-schwarz-inequality']"
42,$\mathbb{R}$ and $\mathbb{C}$ as $\mathbb{Q}$ vector spaces,and  as  vector spaces,\mathbb{R} \mathbb{C} \mathbb{Q},"Q: If we consider $\mathbb{R}$ and $\mathbb{C}$ as $\mathbb{Q}$-vector spaces, then how can we show they are isomorphic? I know that if two vector spaces have bases with the same cardinality, then they are isomorphic.  Also, Zorn's lemma tells us that every vector space has a basis. In this case, answering my question amounts to showing that any bases of  $\mathbb{R}$ and $\mathbb{C}$ over $\mathbb{Q}$ have the same cardinality. In other words, I need to show dim$ \mathbb{R} =$ dim $\mathbb{C}$ over $\mathbb{Q}$, that is, they have bases with the same cardinality.  Can anyone help? Thank you!!","Q: If we consider $\mathbb{R}$ and $\mathbb{C}$ as $\mathbb{Q}$-vector spaces, then how can we show they are isomorphic? I know that if two vector spaces have bases with the same cardinality, then they are isomorphic.  Also, Zorn's lemma tells us that every vector space has a basis. In this case, answering my question amounts to showing that any bases of  $\mathbb{R}$ and $\mathbb{C}$ over $\mathbb{Q}$ have the same cardinality. In other words, I need to show dim$ \mathbb{R} =$ dim $\mathbb{C}$ over $\mathbb{Q}$, that is, they have bases with the same cardinality.  Can anyone help? Thank you!!",,"['linear-algebra', 'abstract-algebra', 'vector-spaces']"
43,Is there a geometrical interpretation to the notion of eigenvector and eigenvalues?,Is there a geometrical interpretation to the notion of eigenvector and eigenvalues?,,The wiki article on eigenvectors offers the following geometrical interpretation: Each application of the matrix to an arbitrary vector yields a result which will have rotated towards the eigenvector with the largest eigenvalue. Qn 1: If there is any other geometrical interpretation particularly in the context of a covariance matrix? The wiki also discusses the difference between left and right eigenvectors. Qn 2: Do the above geometrical interpretations hold irrespective of whether they are left or right eigenvectors?,The wiki article on eigenvectors offers the following geometrical interpretation: Each application of the matrix to an arbitrary vector yields a result which will have rotated towards the eigenvector with the largest eigenvalue. Qn 1: If there is any other geometrical interpretation particularly in the context of a covariance matrix? The wiki also discusses the difference between left and right eigenvectors. Qn 2: Do the above geometrical interpretations hold irrespective of whether they are left or right eigenvectors?,,['linear-algebra']
44,Diagonalizability of symmetric bilinear forms over fields of characteristic $2$,Diagonalizability of symmetric bilinear forms over fields of characteristic,2,"Theorem : A symmetric bilinear form $H$ on a finite dimension vector space $V$ over a field $\mathbb{F}$ , where it is not of characteristic two, is   diagonalizable. Proof: By induction on $\text{dim} \; V=n$ . (induction base) $n=0$ . Then it is trivial. (induction hypothesis) Assume the above statement holds for all   bilinear forms on vector spaces of dimension $n-1$ . (inductive step) Suppose the space $\text{dim} \; V=n$ . If the   bilinear form $H=0$ , then it is trivial. Hence, assume that $H\neq0$ ,   then there exists $z \in V$ such that $H(z,z) \neq 0$ . Let $W = \operatorname{span}\{z\}^\perp$ and we have $V = W \oplus \operatorname{span}\{z\}$ . As $\text{dim} \; W=n-1$ , the theorem holds   for this space and there is a basis $\beta = \{v_1, ..., v_{n-1}\}$ where $H$ is diagonal. Then, by extending the basis we have $\gamma = \beta \cup \{z\} \subset V$ . Then we have: $H(v_{i},z) = 0$ for all $i=0, ..., n-1$ . Hence, this implies that there exists a basis $\gamma \subset V$ such that the matrix corresponding to $H$ is diagonal. Why is the field of characteristic two excluded?","Theorem : A symmetric bilinear form on a finite dimension vector space over a field , where it is not of characteristic two, is   diagonalizable. Proof: By induction on . (induction base) . Then it is trivial. (induction hypothesis) Assume the above statement holds for all   bilinear forms on vector spaces of dimension . (inductive step) Suppose the space . If the   bilinear form , then it is trivial. Hence, assume that ,   then there exists such that . Let and we have . As , the theorem holds   for this space and there is a basis where is diagonal. Then, by extending the basis we have . Then we have: for all . Hence, this implies that there exists a basis such that the matrix corresponding to is diagonal. Why is the field of characteristic two excluded?","H V \mathbb{F} \text{dim} \; V=n n=0 n-1 \text{dim} \; V=n H=0 H\neq0 z \in V H(z,z) \neq 0 W =
\operatorname{span}\{z\}^\perp V = W \oplus
\operatorname{span}\{z\} \text{dim} \; W=n-1 \beta = \{v_1, ..., v_{n-1}\} H \gamma = \beta \cup \{z\} \subset V H(v_{i},z) = 0 i=0, ..., n-1 \gamma
\subset V H","['linear-algebra', 'finite-fields', 'quadratic-forms', 'multilinear-algebra', 'bilinear-form']"
45,$\operatorname{tr}(A)=\operatorname{tr}(A^{2})= \ldots = \operatorname{tr}(A^{n})=0$ implies $A$ is nilpotent,implies  is nilpotent,\operatorname{tr}(A)=\operatorname{tr}(A^{2})= \ldots = \operatorname{tr}(A^{n})=0 A,"$\DeclareMathOperator{\tr}{tr}$ Let's consider a $n \times n$ matrix and the sequence of traces $\tr(A)=\tr(A^{2})= \ldots = \tr(A^{n})=0$ . How to prove that $A$ is a nilpotent matrix (a matrix so that $A^{k} \times u = 0$ for all $u \in V$ and for some $k$ )? Maybe it would be reasonable to consider $A$ 's Jordan form (we assume that the action occurs over an algebraically closed field, such as $\mathbb{C}$ ). For example, suppose that we have found it and $A'$ consists of $m$ blocks of sizes $r_{1}, \ldots r_{m}$ ( $r_{1}+r_{2}+\ldots+r_{m}=m, r_{i} \in \mathbb{Z}_{+}$ ) with $\alpha_{1}, \ldots \alpha_{m}$ as corresponding eigenvalues to each of the blocks. Then, according to the problem will get the system $$\begin{align} r_{1}\alpha_{1}+&\ldots+r_{m}\alpha_{m}=0 \\ r_{1}{\alpha_{1}}^{2}+&\ldots+r_{m} {\alpha_{m}} ^{2}=0\\ &\ ~\,\vdots \\ r_{1}{\alpha_{1}}^{n}+&\ldots +r_{m}{\alpha_{m}}^{n}=0  \end{align}$$ But the system's analysis over $\mathbb{C}$ seems to be too much complicated. Actually, the problem must have more or less simple solution. Any piece of advice would be much appreciated.","Let's consider a matrix and the sequence of traces . How to prove that is a nilpotent matrix (a matrix so that for all and for some )? Maybe it would be reasonable to consider 's Jordan form (we assume that the action occurs over an algebraically closed field, such as ). For example, suppose that we have found it and consists of blocks of sizes ( ) with as corresponding eigenvalues to each of the blocks. Then, according to the problem will get the system But the system's analysis over seems to be too much complicated. Actually, the problem must have more or less simple solution. Any piece of advice would be much appreciated.","\DeclareMathOperator{\tr}{tr} n \times n \tr(A)=\tr(A^{2})= \ldots = \tr(A^{n})=0 A A^{k} \times u = 0 u \in V k A \mathbb{C} A' m r_{1}, \ldots r_{m} r_{1}+r_{2}+\ldots+r_{m}=m, r_{i} \in \mathbb{Z}_{+} \alpha_{1}, \ldots \alpha_{m} \begin{align}
r_{1}\alpha_{1}+&\ldots+r_{m}\alpha_{m}=0 \\
r_{1}{\alpha_{1}}^{2}+&\ldots+r_{m} {\alpha_{m}} ^{2}=0\\
&\ ~\,\vdots \\
r_{1}{\alpha_{1}}^{n}+&\ldots +r_{m}{\alpha_{m}}^{n}=0 
\end{align} \mathbb{C}","['linear-algebra', 'matrices', 'polynomials', 'trace']"
46,Show determinant of matrix is non-zero,Show determinant of matrix is non-zero,,"I have $a,b,c\in\mathbb{Q}$ not all zero. ($a^2+b^2+c^2\ne 0$), I want to show that the following determinant is then non-zero. I failed to arrive at an appropriate form of the polynomial. Help please. $$\left|\begin{bmatrix} a & 2c & 2b\\b & a & 2c\\ c & b & a\end{bmatrix}\right| = a^3+2 b^3-6 a b c+4 c^3$$ Second question, what is the easiest way to argue that $\{1,\sqrt[3]{2},(\sqrt[3]{2})^2\}$ is linearly independent in $\mathbb{Q}$? Motivation: Prove that $\mathbb{Q}[\sqrt[3]{2}] = \{a+b\sqrt[3]{2}+c(\sqrt[3]{2})^2\;|\;a,b,c\in\mathbb{Q}\}$ forms a field. Proof: Since $\mathbb{Q}[\sqrt[3]{2}] \subset \mathbb{R}$, we prove $\mathbb{Q}[\sqrt[3]{2}]$ is a subfield of $(\mathbb{R},+,\cdot)$ $\forall (a+b\sqrt[3]{2}+c(\sqrt[3]{2})^2)\in \mathbb{Q}[\sqrt[3]{2}]\backslash\{0\}.$ We want to find $(a+b\sqrt[3]{2}+c(\sqrt[3]{2})^2)$ such that $(a+b\sqrt[3]{2}+c(\sqrt[3]{2})^2)(d+e\sqrt[3]{2}+f(\sqrt[3]{2})^2) =$ $ (ad+2ec+2bf)+(ae+bd+2cf)\sqrt[3]{2}+(af+cd+be)(\sqrt[3]{2})^2 = 1$ Since $\{1,\sqrt[3]{2},(\sqrt[3]{2})^2\}$ is linearly independent (?) over $\mathbb{Q}$, we show there is unique solution to: $$\begin{bmatrix} a & 2c & 2b\\b & a & 2c\\ c & b & a\end{bmatrix} \cdot \left[\begin{array}{l l} d\\e\\f \end{array}\right] = \left[\begin{array}{l l} 1\\0\\0 \end{array}\right] $$ Which is equivalent in showing the determinant is non-zero $$\left|\begin{bmatrix} a & 2c & 2b\\b & a & 2c\\ c & b & a\end{bmatrix}\right| = a^3+2 b^3-6 a b c+4 c^3=(?)$$ By subfield test, 1)2)3)4) is enough to say that $(\mathbb{Q}[\sqrt[3]{2}],+,\cdot)$ is a subfield of $(\mathbb{R},+,\cdot)$ therefore a field. EDIT : If you have shorter way that prove the proposition without touching my 2 questions, that is even better.","I have $a,b,c\in\mathbb{Q}$ not all zero. ($a^2+b^2+c^2\ne 0$), I want to show that the following determinant is then non-zero. I failed to arrive at an appropriate form of the polynomial. Help please. $$\left|\begin{bmatrix} a & 2c & 2b\\b & a & 2c\\ c & b & a\end{bmatrix}\right| = a^3+2 b^3-6 a b c+4 c^3$$ Second question, what is the easiest way to argue that $\{1,\sqrt[3]{2},(\sqrt[3]{2})^2\}$ is linearly independent in $\mathbb{Q}$? Motivation: Prove that $\mathbb{Q}[\sqrt[3]{2}] = \{a+b\sqrt[3]{2}+c(\sqrt[3]{2})^2\;|\;a,b,c\in\mathbb{Q}\}$ forms a field. Proof: Since $\mathbb{Q}[\sqrt[3]{2}] \subset \mathbb{R}$, we prove $\mathbb{Q}[\sqrt[3]{2}]$ is a subfield of $(\mathbb{R},+,\cdot)$ $\forall (a+b\sqrt[3]{2}+c(\sqrt[3]{2})^2)\in \mathbb{Q}[\sqrt[3]{2}]\backslash\{0\}.$ We want to find $(a+b\sqrt[3]{2}+c(\sqrt[3]{2})^2)$ such that $(a+b\sqrt[3]{2}+c(\sqrt[3]{2})^2)(d+e\sqrt[3]{2}+f(\sqrt[3]{2})^2) =$ $ (ad+2ec+2bf)+(ae+bd+2cf)\sqrt[3]{2}+(af+cd+be)(\sqrt[3]{2})^2 = 1$ Since $\{1,\sqrt[3]{2},(\sqrt[3]{2})^2\}$ is linearly independent (?) over $\mathbb{Q}$, we show there is unique solution to: $$\begin{bmatrix} a & 2c & 2b\\b & a & 2c\\ c & b & a\end{bmatrix} \cdot \left[\begin{array}{l l} d\\e\\f \end{array}\right] = \left[\begin{array}{l l} 1\\0\\0 \end{array}\right] $$ Which is equivalent in showing the determinant is non-zero $$\left|\begin{bmatrix} a & 2c & 2b\\b & a & 2c\\ c & b & a\end{bmatrix}\right| = a^3+2 b^3-6 a b c+4 c^3=(?)$$ By subfield test, 1)2)3)4) is enough to say that $(\mathbb{Q}[\sqrt[3]{2}],+,\cdot)$ is a subfield of $(\mathbb{R},+,\cdot)$ therefore a field. EDIT : If you have shorter way that prove the proposition without touching my 2 questions, that is even better.",,"['linear-algebra', 'determinant']"
47,Why does the Cauchy-Schwarz inequality hold in any inner product space?,Why does the Cauchy-Schwarz inequality hold in any inner product space?,,"I am working through linear algebra problems in Apostol's Calculus , and he has numerous problems that seem to imply that Cauchy-Schwarz holds no matter how the inner product is defined. Then, he has problems where the triangle inequality holds despite alternative definitions of vector norm. This got me thinking, since the proof of the triangle inequality in Apostol relies on Cauchy-Schwarz, that the triangle inequality would hold regardless of how the vector norm is defined (if it involves the dot product). I then found this response to a question, which states what I was thinking. Are there any proofs that the Cauchy-Schwarz inequality holds in any inner product space (I looked for some and found none and could not prove it myself)? I've had a semester of algebra (Artin) and some analysis, if there is a proof at such a level of understanding. Intuitive explanations are good, too.","I am working through linear algebra problems in Apostol's Calculus , and he has numerous problems that seem to imply that Cauchy-Schwarz holds no matter how the inner product is defined. Then, he has problems where the triangle inequality holds despite alternative definitions of vector norm. This got me thinking, since the proof of the triangle inequality in Apostol relies on Cauchy-Schwarz, that the triangle inequality would hold regardless of how the vector norm is defined (if it involves the dot product). I then found this response to a question, which states what I was thinking. Are there any proofs that the Cauchy-Schwarz inequality holds in any inner product space (I looked for some and found none and could not prove it myself)? I've had a semester of algebra (Artin) and some analysis, if there is a proof at such a level of understanding. Intuitive explanations are good, too.",,['linear-algebra']
48,Proof Complex positive definite => self-adjoint,Proof Complex positive definite => self-adjoint,,I am looking for a proof of the theorem that says: A is a complex positive definite endomorphism and therefore is A self-adjoint. Does anybody of you know how to do this?,I am looking for a proof of the theorem that says: A is a complex positive definite endomorphism and therefore is A self-adjoint. Does anybody of you know how to do this?,,"['linear-algebra', 'functional-analysis']"
49,How to prove the existence and uniqueness of Cholesky decomposition?,How to prove the existence and uniqueness of Cholesky decomposition?,,"Given a real Hermitian positive-definite matrix $A$ is a decomposition of the form $A=L L^T$ where L is a lower triangular matrix with positive diagonal entries. I read some proofs about the existence of Cholesky decomposition. Most of them start from LDU decomposition. Then the proof shows that $U^T=L$ and $A=LDU=LD^{\frac{1}{2}} D^{\frac{1}{2}}L^T=CC^T$ where $C=L D^\frac{1}{2}$. How can I prove the existence of Cholesky decomposition without any preassumption like LDU decomposition exists? Or how can I prove LDU decomposition exists? I know it may be easy. But I just cannot figure it out. For uniqueness, I think it's not hard to prove.","Given a real Hermitian positive-definite matrix $A$ is a decomposition of the form $A=L L^T$ where L is a lower triangular matrix with positive diagonal entries. I read some proofs about the existence of Cholesky decomposition. Most of them start from LDU decomposition. Then the proof shows that $U^T=L$ and $A=LDU=LD^{\frac{1}{2}} D^{\frac{1}{2}}L^T=CC^T$ where $C=L D^\frac{1}{2}$. How can I prove the existence of Cholesky decomposition without any preassumption like LDU decomposition exists? Or how can I prove LDU decomposition exists? I know it may be easy. But I just cannot figure it out. For uniqueness, I think it's not hard to prove.",,"['linear-algebra', 'matrices', 'matrix-decomposition', 'positive-definite', 'cholesky-decomposition']"
50,Prove The Orthogonal Complement of an Intersection is the Sum of Orthogonal Complements,Prove The Orthogonal Complement of an Intersection is the Sum of Orthogonal Complements,,How does one prove that $(A∩B)^⊥=A^⊥+B^⊥$? Seems a bit harder than proving $(A+B)^⊥=A^⊥∩B^⊥$.,How does one prove that $(A∩B)^⊥=A^⊥+B^⊥$? Seems a bit harder than proving $(A+B)^⊥=A^⊥∩B^⊥$.,,"['linear-algebra', 'vector-spaces', 'orthogonality']"
51,How to explain to a high school student why a linear differential equation is linear?,How to explain to a high school student why a linear differential equation is linear?,,"My mother is teaching a high school course on multivariable calculus, and they were studying linear differential equations of the form $$y' + P(x) y = Q(x),$$ and the question of why this equation is called ""linear"" came up. In terms that these students are familiar with, since they haven't been exposed to linear algebra yet, my thought was to say that the equation for a line, $y = mx+b$, is ""linear"" in $x$ (ignoring the technicality that it's actually an affine equation, not a linear one), because it's in the form ""coefficient times $x$"", and then we allow another term which is just a lonely coefficient. And then we extend this notion to saying that the above differential equation is ""linear"" in $y$ and $y'$, but this time the coefficients are allowed to be functions of $x$. That's probably a good enough hand-wavy explanation to help students remember the definition, at the very least. I couldn't really think of a good reason why it ""should"", a priori, be kosher to allow coefficients to be functions of $x$ here. At that point it seems to me like you just have to get into the linear algebraic definition of linearity, which, being completely foreign to the students... it just seems to be a bit too deep of a rabbit hole for this purpose. So my question is: does anyone have a better way of approaching this? And if you think my hand-wavy explanation above is largely acceptable, is there a way you can explain why multiplying by non-constant functions of $x$ ""should"" still be considered linear in $y$?","My mother is teaching a high school course on multivariable calculus, and they were studying linear differential equations of the form $$y' + P(x) y = Q(x),$$ and the question of why this equation is called ""linear"" came up. In terms that these students are familiar with, since they haven't been exposed to linear algebra yet, my thought was to say that the equation for a line, $y = mx+b$, is ""linear"" in $x$ (ignoring the technicality that it's actually an affine equation, not a linear one), because it's in the form ""coefficient times $x$"", and then we allow another term which is just a lonely coefficient. And then we extend this notion to saying that the above differential equation is ""linear"" in $y$ and $y'$, but this time the coefficients are allowed to be functions of $x$. That's probably a good enough hand-wavy explanation to help students remember the definition, at the very least. I couldn't really think of a good reason why it ""should"", a priori, be kosher to allow coefficients to be functions of $x$ here. At that point it seems to me like you just have to get into the linear algebraic definition of linearity, which, being completely foreign to the students... it just seems to be a bit too deep of a rabbit hole for this purpose. So my question is: does anyone have a better way of approaching this? And if you think my hand-wavy explanation above is largely acceptable, is there a way you can explain why multiplying by non-constant functions of $x$ ""should"" still be considered linear in $y$?",,"['linear-algebra', 'ordinary-differential-equations', 'education']"
52,Minimum of a quadratic form,Minimum of a quadratic form,,"If $\bf{A}$ is a real symmetric matrix, we know that it has orthogonal eigenvectors. Now, say we want to find a unit vector $\bf{n}$ that minimizes the form: $${\bf{n}}^T{\bf{A}}\ {\bf{n}}$$  How can one prove that this vector is given by the eigenvector corresponding to the minimum eigenvalue of $\bf{A}$? I have a proof of my own, but it's rather unelegant - how would you go about proving this?","If $\bf{A}$ is a real symmetric matrix, we know that it has orthogonal eigenvectors. Now, say we want to find a unit vector $\bf{n}$ that minimizes the form: $${\bf{n}}^T{\bf{A}}\ {\bf{n}}$$  How can one prove that this vector is given by the eigenvector corresponding to the minimum eigenvalue of $\bf{A}$? I have a proof of my own, but it's rather unelegant - how would you go about proving this?",,"['linear-algebra', 'matrices', 'optimization', 'eigenvalues-eigenvectors', 'quadratic-forms']"
53,Differences between infinite-dimensional and finite-dimensional vector spaces,Differences between infinite-dimensional and finite-dimensional vector spaces,,"I've just started a course in Representation Theory, and in solving our first homework I've used a couple of theorems about finite-dimensional vector spaces (for an example, rank-nullity theorem). My colleagues pointed out to me that we are working in general vector spaces, so I've patched up places where I've used those theorems with more general arguments. So, why did I make that mistake? Well, in my previous linear algebra courses we mostly worked with finite-dimensional vector spaces, so in my mind I started to consider all vector spaces finite-dimensional. To fix that, and to prevent future mishaps, I would like to see a few differences between finite-dim. and infinite-dim. vector spaces. More 'obvious' fact is in finite-dimensional space, the better.","I've just started a course in Representation Theory, and in solving our first homework I've used a couple of theorems about finite-dimensional vector spaces (for an example, rank-nullity theorem). My colleagues pointed out to me that we are working in general vector spaces, so I've patched up places where I've used those theorems with more general arguments. So, why did I make that mistake? Well, in my previous linear algebra courses we mostly worked with finite-dimensional vector spaces, so in my mind I started to consider all vector spaces finite-dimensional. To fix that, and to prevent future mishaps, I would like to see a few differences between finite-dim. and infinite-dim. vector spaces. More 'obvious' fact is in finite-dimensional space, the better.",,"['linear-algebra', 'abstract-algebra', 'vector-spaces', 'big-list']"
54,Prove Sylvester rank inequality: $\text{rank}(AB)\ge\text{rank}(A)+\text{rank}(B)-n$,Prove Sylvester rank inequality:,\text{rank}(AB)\ge\text{rank}(A)+\text{rank}(B)-n,"If $A$ is a $m \times n$ matrix and $B$ a $n \times k$ matrix, prove that $$\text{rank}(AB)\ge\text{rank}(A)+\text{rank}(B)-n.$$ Also show when equality occurs.","If $A$ is a $m \times n$ matrix and $B$ a $n \times k$ matrix, prove that $$\text{rank}(AB)\ge\text{rank}(A)+\text{rank}(B)-n.$$ Also show when equality occurs.",,"['linear-algebra', 'matrices', 'inequality', 'contest-math', 'matrix-rank']"
55,Characteristic polynomial of a matrix of $1$'s,Characteristic polynomial of a matrix of 's,1,"I am trying to calculate the characteristic polynomial of the $n \times n$ matrix $A = \{ a_{ij} = 1 \}$ . Case $n=2$ : I obtained $p(\lambda)=\lambda^2-2\lambda$ . Case $n=3$ : I obtained $p(\lambda)=-\lambda^3+3\lambda^2$ . Case $n=4$ : I obtained $p(\lambda)=\lambda^4 - 4\lambda^3$ . I guess that for the general case, we have $$p(\lambda)=(-1)^n\lambda^{n}+(-1)^{n-1}n\lambda^{n-1}$$ I tried to use induction, but it didn't work, unless I've done wrong. Can somebody help me? Or give me a hint?","I am trying to calculate the characteristic polynomial of the matrix . Case : I obtained . Case : I obtained . Case : I obtained . I guess that for the general case, we have I tried to use induction, but it didn't work, unless I've done wrong. Can somebody help me? Or give me a hint?",n \times n A = \{ a_{ij} = 1 \} n=2 p(\lambda)=\lambda^2-2\lambda n=3 p(\lambda)=-\lambda^3+3\lambda^2 n=4 p(\lambda)=\lambda^4 - 4\lambda^3 p(\lambda)=(-1)^n\lambda^{n}+(-1)^{n-1}n\lambda^{n-1},"['linear-algebra', 'matrices', 'characteristic-polynomial', 'rank-1-matrices']"
56,"Geometrically explained, why do Linear Transformations Take a circle to an ellipse","Geometrically explained, why do Linear Transformations Take a circle to an ellipse",,"Short Version: How can it be geometrically shown that non-singular 2D linear transformations take circles to ellipses? (Also, its probably important to state I'd prefer an explanation that doesn't use SVD, as I don't really understand it yet...although I see it everywhere) Long Version: Let's use the definition of an ellipse as being a circle stretched in two perpendicular directions. The two directions its stretched in will correspond to the two axes of the ellipse. We begin by defining the circle as the endpoints of all possible 2D unit vectors. The ellipse (or at least I""m TOLD it's an ellipse) is the shape drawn by the endpoints of all these vectors after they have all been transformed in the same way (aka multiplied by the same nonsingular matrix $A$ ) . For linear transformations represented by diagonal matrices, it's easy to see. We're just stretching the circle in the X and Y directions. For linear transformations represented by symmetric matrices...its a little harder, but I can see the transformation because the eigenvectors of the symmetric matrix are perpendicular, and if we change to a basis where those eigenvectors are the basis vectors, the transformation can be represented by a diagonal matrix (as for WHY symmetric matrices can be decomposed this way I don't yet really understand - but for the purpose of this question I'm just accepting that they can; I'm accepting that the eigenvectors of symmetric matrices are perpendicular to one another) . So, just like diagonal matrices, symmetric matrices also correspond to stretching a unit circle in perpendicular directions - but unless the symmetric matrix is diagonal, these are perpendicular directions different from the X and Y directions. Buuut...what about for nonsymmetric matrices? Thanks! EDIT: I've now learned of the polar decomposition of any real matrix, and that provides a beautiful explanation for why any real matrix takes a circle to an ellipse! $A=QS$ , where $Q$ is an orthogonal matrix (rotation) and $S$ is a symmetric matrix (stretching in the direction of the eigenvectors). The symmetric matrix will definitely correspond to making an ellipse (since it scales in orthogonal directions, although perhaps not our regular $x$ and $y$ directions) and all the orthonormal matrix will do is rotate this ellipse. However, all the explanations I've seen so far that PROVE that polar decompositions of real matrices are always possible use an algebraic explanation instead of a geometric one ...so they aren't really what I'm looking for. Thanks again!","Short Version: How can it be geometrically shown that non-singular 2D linear transformations take circles to ellipses? (Also, its probably important to state I'd prefer an explanation that doesn't use SVD, as I don't really understand it yet...although I see it everywhere) Long Version: Let's use the definition of an ellipse as being a circle stretched in two perpendicular directions. The two directions its stretched in will correspond to the two axes of the ellipse. We begin by defining the circle as the endpoints of all possible 2D unit vectors. The ellipse (or at least I""m TOLD it's an ellipse) is the shape drawn by the endpoints of all these vectors after they have all been transformed in the same way (aka multiplied by the same nonsingular matrix ) . For linear transformations represented by diagonal matrices, it's easy to see. We're just stretching the circle in the X and Y directions. For linear transformations represented by symmetric matrices...its a little harder, but I can see the transformation because the eigenvectors of the symmetric matrix are perpendicular, and if we change to a basis where those eigenvectors are the basis vectors, the transformation can be represented by a diagonal matrix (as for WHY symmetric matrices can be decomposed this way I don't yet really understand - but for the purpose of this question I'm just accepting that they can; I'm accepting that the eigenvectors of symmetric matrices are perpendicular to one another) . So, just like diagonal matrices, symmetric matrices also correspond to stretching a unit circle in perpendicular directions - but unless the symmetric matrix is diagonal, these are perpendicular directions different from the X and Y directions. Buuut...what about for nonsymmetric matrices? Thanks! EDIT: I've now learned of the polar decomposition of any real matrix, and that provides a beautiful explanation for why any real matrix takes a circle to an ellipse! , where is an orthogonal matrix (rotation) and is a symmetric matrix (stretching in the direction of the eigenvectors). The symmetric matrix will definitely correspond to making an ellipse (since it scales in orthogonal directions, although perhaps not our regular and directions) and all the orthonormal matrix will do is rotate this ellipse. However, all the explanations I've seen so far that PROVE that polar decompositions of real matrices are always possible use an algebraic explanation instead of a geometric one ...so they aren't really what I'm looking for. Thanks again!",A A=QS Q S x y,"['linear-algebra', 'linear-transformations', 'intuition']"
57,Hadamard's inequality proof,Hadamard's inequality proof,,"I have the following inequality to prove. With $A \in M_n(R)$ show that: $$ (\det(A))^2 \leq \prod_{i=1}^n\left( \sum_{k=1}^n A_{k,i}^2\right) $$ What I already have: I found out that:  $$G(v_1,\ldots,v_m) = \det(A^T A)=\det(A^T)\cdot\det(A)=(\det(A))^2 $$ Also that with  $G(v_1,\ldots,v_m) = (\det(A))^2$ results that: $$\operatorname{Vol}(v_1,\ldots,v_m)= \left|\det(A)\right| =\prod_{i=1}^n |s(v_k,u_i)|$$ I don't even now if this is right.. It would be really helpful if someone could help me with the proof..","I have the following inequality to prove. With $A \in M_n(R)$ show that: $$ (\det(A))^2 \leq \prod_{i=1}^n\left( \sum_{k=1}^n A_{k,i}^2\right) $$ What I already have: I found out that:  $$G(v_1,\ldots,v_m) = \det(A^T A)=\det(A^T)\cdot\det(A)=(\det(A))^2 $$ Also that with  $G(v_1,\ldots,v_m) = (\det(A))^2$ results that: $$\operatorname{Vol}(v_1,\ldots,v_m)= \left|\det(A)\right| =\prod_{i=1}^n |s(v_k,u_i)|$$ I don't even now if this is right.. It would be really helpful if someone could help me with the proof..",,"['linear-algebra', 'inequality']"
58,Prove that the system $A^T A x = A^T b$ always has a solution,Prove that the system  always has a solution,A^T A x = A^T b,Prove that the system $$A^T A x = A^T b$$ always has a solution. The matrices and vectors are all real. The matrix $A$ is $m \times n$. I think it makes sense intuitively but I can't prove it formally.,Prove that the system $$A^T A x = A^T b$$ always has a solution. The matrices and vectors are all real. The matrix $A$ is $m \times n$. I think it makes sense intuitively but I can't prove it formally.,,"['linear-algebra', 'systems-of-equations', 'least-squares']"
59,What is the rank of the matrix consisting of all permutations of one vector? [duplicate],What is the rank of the matrix consisting of all permutations of one vector? [duplicate],,"This question already has answers here : Subspace generated by permutations of a vector in a vector space (2 answers) Closed 7 years ago . Let $a=(a_1,...,a_n)^\top\in\mathbb{R}^n$ be a column vector and let $M_1,...,M_{n!}$ denote all $n\times n$ permutation matrices. When is the rank of the matrix that consists of all possible permutations of $a$: $$ A=[M_1 a \,|\; ... \; |\, M_{n!} a]\in\mathbb{R}^{n\times n!} $$  equal to $n$? Obviously, $rank(A)\le n$ and if all entries of $a$ are identical, then $rank(A)=1$. Moreover, if $A$ has rank $n$, then there exist two entries $i,j$ s.t. $a_i\not=a_j$. Is the converse statement also true?","This question already has answers here : Subspace generated by permutations of a vector in a vector space (2 answers) Closed 7 years ago . Let $a=(a_1,...,a_n)^\top\in\mathbb{R}^n$ be a column vector and let $M_1,...,M_{n!}$ denote all $n\times n$ permutation matrices. When is the rank of the matrix that consists of all possible permutations of $a$: $$ A=[M_1 a \,|\; ... \; |\, M_{n!} a]\in\mathbb{R}^{n\times n!} $$  equal to $n$? Obviously, $rank(A)\le n$ and if all entries of $a$ are identical, then $rank(A)=1$. Moreover, if $A$ has rank $n$, then there exist two entries $i,j$ s.t. $a_i\not=a_j$. Is the converse statement also true?",,"['linear-algebra', 'permutations', 'matrix-rank']"
60,Finding all matrices $B$ such that $AB=BA$ for a fixed matrix $A$,Finding all matrices  such that  for a fixed matrix,B AB=BA A,Let $$ A=\begin{pmatrix} 1 & 0 & 0 \\ 0& 1 & 0 \\ 3 & 1 & 2 \end{pmatrix}  $$ Find all matrices $B$ such that $AB=BA$. Attempt at solution: I can show that $A$ is invertible so its inverse must be one of the elements. But how do I go about showing there are more of them? or not?. I can set set up the unknown matrix to be a matrix with 9 unknowns and then (at least in principle) try to solve or this system. But I think this is not a very productive way to do this. If these were $2 \times 2$ matrces that would be ok. How should I proceed? Any hints? Thanks for your time and help.,Let $$ A=\begin{pmatrix} 1 & 0 & 0 \\ 0& 1 & 0 \\ 3 & 1 & 2 \end{pmatrix}  $$ Find all matrices $B$ such that $AB=BA$. Attempt at solution: I can show that $A$ is invertible so its inverse must be one of the elements. But how do I go about showing there are more of them? or not?. I can set set up the unknown matrix to be a matrix with 9 unknowns and then (at least in principle) try to solve or this system. But I think this is not a very productive way to do this. If these were $2 \times 2$ matrces that would be ok. How should I proceed? Any hints? Thanks for your time and help.,,"['linear-algebra', 'matrices']"
61,Geometric multiplicity of an eigenvalue,Geometric multiplicity of an eigenvalue,,"Geometric multiplicity of an eigenvalue of a matrix is the dimension of the corresponding eigenspace.  The algebraic multiplicity is its multiplicity as a root of the characteristic polynomial. It is known that the geometric multiplicity of an eigenvalue cannot be greater than the algebraic multiplicity .  This fact can be shown easily using the Jordan normal form of a matrix. I was wondering if there is a more elementary way to prove this fact, possibly longer but without using the Jordan normal form?  (This is an exercise in Kreyszig's book on functional analysis , and given the author's style, I suspect that he did not intend the solution to use Jordan form, because otherwise I guess he would have given a hint about that. But I might be wrong.)","Geometric multiplicity of an eigenvalue of a matrix is the dimension of the corresponding eigenspace.  The algebraic multiplicity is its multiplicity as a root of the characteristic polynomial. It is known that the geometric multiplicity of an eigenvalue cannot be greater than the algebraic multiplicity .  This fact can be shown easily using the Jordan normal form of a matrix. I was wondering if there is a more elementary way to prove this fact, possibly longer but without using the Jordan normal form?  (This is an exercise in Kreyszig's book on functional analysis , and given the author's style, I suspect that he did not intend the solution to use Jordan form, because otherwise I guess he would have given a hint about that. But I might be wrong.)",,"['linear-algebra', 'eigenvalues-eigenvectors']"
62,When does $\mbox{tr} \left( A^2 \right) = \mbox{tr} (A)^2$ hold?,When does  hold?,\mbox{tr} \left( A^2 \right) = \mbox{tr} (A)^2,"Suppose $A$ is a square matrix whose eigenvalues are $\lambda_1, \lambda_2, \dots, \lambda_n$ . The equation $$\mbox{tr} \left( A^2 \right) = \mbox{tr} (A)^2$$ implies that $$\sum_{i\not=j}\lambda_i\lambda_j=0$$ What can we learn about the matrix $A$ from the equation above? What kind of matrix has this property?",Suppose is a square matrix whose eigenvalues are . The equation implies that What can we learn about the matrix from the equation above? What kind of matrix has this property?,"A \lambda_1, \lambda_2, \dots, \lambda_n \mbox{tr} \left( A^2 \right) = \mbox{tr} (A)^2 \sum_{i\not=j}\lambda_i\lambda_j=0 A","['linear-algebra', 'matrices', 'quadratics', 'trace']"
63,Intuitive geometric explanation: existence of eigenvalue in odd dimension real vector space.,Intuitive geometric explanation: existence of eigenvalue in odd dimension real vector space.,,"I'm looking for an intuitive geometric explanation for the fact that given an odd dimensional real vector space $W$ and an endomorphism $T:W \rightarrow W$, there exists a real eigenvalue of $T$. I'm not looking for an algebraic explanation which involves the degree of characteristic polynomial.","I'm looking for an intuitive geometric explanation for the fact that given an odd dimensional real vector space $W$ and an endomorphism $T:W \rightarrow W$, there exists a real eigenvalue of $T$. I'm not looking for an algebraic explanation which involves the degree of characteristic polynomial.",,"['linear-algebra', 'geometry', 'eigenvalues-eigenvectors', 'linear-transformations']"
64,Sum of two subspaces is equal to the span of their union,Sum of two subspaces is equal to the span of their union,,"Let $V_{1}$ and $V_{2}$ be two subspaces of $V$. Define the sum of $V_{1}$ and $V_{2}$ to be the subset of $V$ $V_{1}+V_{2}=${$ \overrightarrow v_{1} + \overrightarrow v_{2}: \overrightarrow v_{1} \in V_{1}, \overrightarrow v_{2} \in V_{2}$} Prove that $V_{1}+V_{2}=Span(V_{1} \cup V_{2})$ Approach: I want to prove $V_{1}+V_{2} \subset Span(V_{1} \cup V_{2})$ and $Span(V_{1} \cup V_{2}) \subset V_{1}+V_{2}$. Any tips on how to proceed?","Let $V_{1}$ and $V_{2}$ be two subspaces of $V$. Define the sum of $V_{1}$ and $V_{2}$ to be the subset of $V$ $V_{1}+V_{2}=${$ \overrightarrow v_{1} + \overrightarrow v_{2}: \overrightarrow v_{1} \in V_{1}, \overrightarrow v_{2} \in V_{2}$} Prove that $V_{1}+V_{2}=Span(V_{1} \cup V_{2})$ Approach: I want to prove $V_{1}+V_{2} \subset Span(V_{1} \cup V_{2})$ and $Span(V_{1} \cup V_{2}) \subset V_{1}+V_{2}$. Any tips on how to proceed?",,"['linear-algebra', 'vector-spaces']"
65,Proof relating inverse to determinant,Proof relating inverse to determinant,,"I'm reading a paper regarding the consistency of a statistical estimator, and the author claimed that the following is an identity: $$ \mathbf{x}^\top (\Sigma + \mathbf{x}\mathbf{x}^\top)^{-1}\mathbf{x} = 1- \frac{\det (\Sigma)}{\det (\Sigma+\mathbf{x}\mathbf{x}^\top)}$$ Here $\mathbf{x}$ is a vector and $\Sigma$ is a covariance matrix. Apart from that, I don't think the author has specified any more assumptions/constraints. While superficially it appears that this result might be provable via the Sherman-Morrison formula, I haven't been able to work it out. Any help/pointers would be appreciated ! Thanks.","I'm reading a paper regarding the consistency of a statistical estimator, and the author claimed that the following is an identity: $$ \mathbf{x}^\top (\Sigma + \mathbf{x}\mathbf{x}^\top)^{-1}\mathbf{x} = 1- \frac{\det (\Sigma)}{\det (\Sigma+\mathbf{x}\mathbf{x}^\top)}$$ Here $\mathbf{x}$ is a vector and $\Sigma$ is a covariance matrix. Apart from that, I don't think the author has specified any more assumptions/constraints. While superficially it appears that this result might be provable via the Sherman-Morrison formula, I haven't been able to work it out. Any help/pointers would be appreciated ! Thanks.",,['linear-algebra']
66,The relationship between the eigenvalues of matrices $XY$ and $YX$,The relationship between the eigenvalues of matrices  and,XY YX,"If $X \in \mathbb{C}^{m \times n}$ and $Y \in \mathbb{C}^{n \times m}$ ( $m \geq n$ ), how to prove the following? $$\sigma (XY) = \sigma(YX) \cup \underbrace{\left \{ 0, ..., 0 \right \}}_{m-n}$$ Here, $\sigma$ denotes the set of eigenvalues/spectrum.","If and ( ), how to prove the following? Here, denotes the set of eigenvalues/spectrum.","X \in \mathbb{C}^{m \times n} Y \in \mathbb{C}^{n \times m} m \geq n \sigma (XY) = \sigma(YX) \cup \underbrace{\left \{ 0, ..., 0 \right \}}_{m-n} \sigma","['linear-algebra', 'matrices', 'eigenvalues-eigenvectors']"
67,Is it possible to decompose a matrix as the product of two vectors?,Is it possible to decompose a matrix as the product of two vectors?,,"This question maybe stupid to some of you, but I would like to know whether it is possible to decompose a matrix $M_{m\times n}$ as the product of two vectors, i.e. $$M_{m\times n} = \vec{y}_{m\times 1}\times\vec{x}_{1\times n}+const.$$ Obviously, this should be true for some cases, but I am not sure whether this conclusion is always hold. Meanwhile, I want to know at what condition, we can do this decomposition and how to find the vectors $\vec{y}$ and $\vec{x}$?","This question maybe stupid to some of you, but I would like to know whether it is possible to decompose a matrix $M_{m\times n}$ as the product of two vectors, i.e. $$M_{m\times n} = \vec{y}_{m\times 1}\times\vec{x}_{1\times n}+const.$$ Obviously, this should be true for some cases, but I am not sure whether this conclusion is always hold. Meanwhile, I want to know at what condition, we can do this decomposition and how to find the vectors $\vec{y}$ and $\vec{x}$?",,"['linear-algebra', 'matrices']"
68,Jordan-Chevalley vs Jordan normal decomposition,Jordan-Chevalley vs Jordan normal decomposition,,"I am confused about a proof of the Jordan-Chevalley decomposition I was reading in Peterson's linear algebra book. Let $T : V \to V$ be an $n$-dimensional operator on a complex vector space. The Jordan-Chevalley decomposition tells us that $T = S + N$, where $S$ is diagonalisable, $N$ is nilpotent, and $SN = NS$. If $$m_T(t) = (t - \lambda_1)^{m_1}\cdots(t - \lambda_k)^{m_k}$$ is the minimal polynomial of $T$, where the $\lambda_i$ are distinct, then we have a $T$-invariant decomposition $$V = \ker(T - \lambda_1I)^{m_1} \oplus \cdots \oplus \ker(T - \lambda_kI)^{m_k}.$$ Then a matrix representation for $T$ can be written as a block diagonal matrix where the $i$th diagonal block is a matrix representation for $T$ restricted to $\ker(T - \lambda_i)^{m_i}$ (call this matrix $T_i$). Using the rational canonical form, one can see that $T_i$ is similar to $$\left[\begin{matrix} \lambda & 1 & \cdots & 0 \\ 0 & \lambda \\ \vdots & & \ddots & 1\\ 0 & & & \lambda \end{matrix}\right].$$ This is a Jordan block, however. So I don't understand how the Jordan-Chevalley decomposition is different from the Jordan decomposition. Yet I know they can't be the same, because this proof seems to imply that there is only one block for each distinct eigenvalue, which is not necessarily true. What exactly is going on here?","I am confused about a proof of the Jordan-Chevalley decomposition I was reading in Peterson's linear algebra book. Let $T : V \to V$ be an $n$-dimensional operator on a complex vector space. The Jordan-Chevalley decomposition tells us that $T = S + N$, where $S$ is diagonalisable, $N$ is nilpotent, and $SN = NS$. If $$m_T(t) = (t - \lambda_1)^{m_1}\cdots(t - \lambda_k)^{m_k}$$ is the minimal polynomial of $T$, where the $\lambda_i$ are distinct, then we have a $T$-invariant decomposition $$V = \ker(T - \lambda_1I)^{m_1} \oplus \cdots \oplus \ker(T - \lambda_kI)^{m_k}.$$ Then a matrix representation for $T$ can be written as a block diagonal matrix where the $i$th diagonal block is a matrix representation for $T$ restricted to $\ker(T - \lambda_i)^{m_i}$ (call this matrix $T_i$). Using the rational canonical form, one can see that $T_i$ is similar to $$\left[\begin{matrix} \lambda & 1 & \cdots & 0 \\ 0 & \lambda \\ \vdots & & \ddots & 1\\ 0 & & & \lambda \end{matrix}\right].$$ This is a Jordan block, however. So I don't understand how the Jordan-Chevalley decomposition is different from the Jordan decomposition. Yet I know they can't be the same, because this proof seems to imply that there is only one block for each distinct eigenvalue, which is not necessarily true. What exactly is going on here?",,"['linear-algebra', 'matrices', 'jordan-normal-form']"
69,Why are inner products defined to be linear in the first argument only?,Why are inner products defined to be linear in the first argument only?,,"It seems to me that if the base field is the real numbers, then we have linearity in both arguments i.e. $\langle u + v, w + z\rangle = \langle u,w\rangle + \langle u,z\rangle + \langle v,w\rangle + \langle v,z\rangle$ because we know $\langle x,y\rangle = \langle y,x\rangle$ for any $y,x$ Do we only define inner products to be linear in their first argument in case the base field is the complex numbers? Could we have just defined inner products over the real numbers to say that inner products are linear in both arguments?","It seems to me that if the base field is the real numbers, then we have linearity in both arguments i.e. $\langle u + v, w + z\rangle = \langle u,w\rangle + \langle u,z\rangle + \langle v,w\rangle + \langle v,z\rangle$ because we know $\langle x,y\rangle = \langle y,x\rangle$ for any $y,x$ Do we only define inner products to be linear in their first argument in case the base field is the complex numbers? Could we have just defined inner products over the real numbers to say that inner products are linear in both arguments?",,"['linear-algebra', 'inner-products']"
70,What is (fundamentally) a coordinate system ?,What is (fundamentally) a coordinate system ?,,"Consider the following construction of vectors and points. Let's start with a vector space , or more specifically a coordinate space $F^N$ over a field $F$ and of $N$ dimensions. The elements of this vector space are vectors $\vec{v}$ and we can express their coordinates using a basis $(\vec{e_1}, \vec{e_2}, ... \vec{e_N})$ of the vector space (and any coordinate space comes with a standard basis ). Using this vector space, one can construct the related affine space $A$, whose elements are points. As there is no origin in an affine space, to locate a point one need a coordinate system which can be specified by an origin and a basis. (First question: does this construction make sense ?) My problem, is that I am not sure to understand what is fundamentally a coordinate system... My problem is the following: in 3D cartesian coordinates , it is pretty simple: the vector space is $\mathbb{R}^{3}$, so we can construct the related affine space with points, and by specifying an origin $O$ plus the standard basis, we can identify the position of the point $P$ by the coordinates of the vector going from $O$ to $P$. But with spherical coordinates I am lost and I am not sure to really understand what mathematical objects is/are $(r, \theta, \phi)$ and $(\vec{e_r}, \vec{e_\theta}, \vec{e_\phi})$: in the contrary of cartesian coordinates, the basis $(\vec{e_r}, \vec{e_\theta}, \vec{e_\phi})$ will change from one point to another. Why is that ? Is the vector space underlying $(r, \theta, \phi)$ and linked with $(\vec{e_r}, \vec{e_\theta}, \vec{e_\phi})$ the same as the one we use in the case of cartesian coordinates. I think I am missing a basic thing...","Consider the following construction of vectors and points. Let's start with a vector space , or more specifically a coordinate space $F^N$ over a field $F$ and of $N$ dimensions. The elements of this vector space are vectors $\vec{v}$ and we can express their coordinates using a basis $(\vec{e_1}, \vec{e_2}, ... \vec{e_N})$ of the vector space (and any coordinate space comes with a standard basis ). Using this vector space, one can construct the related affine space $A$, whose elements are points. As there is no origin in an affine space, to locate a point one need a coordinate system which can be specified by an origin and a basis. (First question: does this construction make sense ?) My problem, is that I am not sure to understand what is fundamentally a coordinate system... My problem is the following: in 3D cartesian coordinates , it is pretty simple: the vector space is $\mathbb{R}^{3}$, so we can construct the related affine space with points, and by specifying an origin $O$ plus the standard basis, we can identify the position of the point $P$ by the coordinates of the vector going from $O$ to $P$. But with spherical coordinates I am lost and I am not sure to really understand what mathematical objects is/are $(r, \theta, \phi)$ and $(\vec{e_r}, \vec{e_\theta}, \vec{e_\phi})$: in the contrary of cartesian coordinates, the basis $(\vec{e_r}, \vec{e_\theta}, \vec{e_\phi})$ will change from one point to another. Why is that ? Is the vector space underlying $(r, \theta, \phi)$ and linked with $(\vec{e_r}, \vec{e_\theta}, \vec{e_\phi})$ the same as the one we use in the case of cartesian coordinates. I think I am missing a basic thing...",,"['linear-algebra', 'geometry', 'vector-spaces', '3d', 'coordinate-systems']"
71,Why is the identity the only symmetric $0$-$1$ matrix with all eigenvalues positive?,Why is the identity the only symmetric - matrix with all eigenvalues positive?,0 1,"While thinking about this question I managed to convince myself that the identity is the only symmetric $0$-$1$ matrix with all eigenvalues positive.  However, the argument is rather low-level.  It does not give much insight into why, out of all symmetric matrices, $0$-$1$ matrices, and matrices with all eigenvalues positive, the identity is the only matrix that simultaneously has all three properties. So my question is Can someone give an intuitive explanation for why the identity is the only symmetric $0$-$1$ matrix with all eigenvalues positive? Such an explanation might entail a bigger-picture argument than the one I came up with. For reference, here's my argument.  Let $A$ be a symmetric $0$-$1$ matrix with all eigenvalues positive. Symmetric and all eigenvalues positive implies $A$ is positive definite. $A$ must have all $1$'s on its diagonal.  This is because $a_{jj} = 0 \implies {\bf e}_j^T A {\bf e}_j = 0$, which contradicts positive definite. $A$ must have all $0$'s for its off-diagonal elements.  This is because $A$ is symmetric implies $a_{ij} = a_{ji}$, and $a_{ij} = a_{ji} = 1 \implies ({\bf e}_i - {\bf e}_j)^T A ({\bf e}_i - {\bf e}_j) = 0$, which contradicts positive definite. Thus $A$ is the identity. For clarification : I am looking for an answer along these lines: ""A symmetric matrix implies or is equivalent to $X$ about the underlying linear transformation.  A $0$-$1$ matrix implies or is equivalent to $Y$ about the underlying linear transformation.  All eigenvalues positive implies or is equivalent to $Z$ about the underlying linear transformation.  ($X$, $Y$, and $Z$ are all big-picture properties.)  [Insert argument here.]  Thus the only matrix that simultaneously satisfies $X$, $Y$, and $Z$ is the identity."" Robert Israel's answer, while nice, is not the kind of thing I'm hoping for.  I view it as a more elegant version of my own low-level argument about what form the entries of the matrix have to take, not what kind of linear transformation has these three properties.","While thinking about this question I managed to convince myself that the identity is the only symmetric $0$-$1$ matrix with all eigenvalues positive.  However, the argument is rather low-level.  It does not give much insight into why, out of all symmetric matrices, $0$-$1$ matrices, and matrices with all eigenvalues positive, the identity is the only matrix that simultaneously has all three properties. So my question is Can someone give an intuitive explanation for why the identity is the only symmetric $0$-$1$ matrix with all eigenvalues positive? Such an explanation might entail a bigger-picture argument than the one I came up with. For reference, here's my argument.  Let $A$ be a symmetric $0$-$1$ matrix with all eigenvalues positive. Symmetric and all eigenvalues positive implies $A$ is positive definite. $A$ must have all $1$'s on its diagonal.  This is because $a_{jj} = 0 \implies {\bf e}_j^T A {\bf e}_j = 0$, which contradicts positive definite. $A$ must have all $0$'s for its off-diagonal elements.  This is because $A$ is symmetric implies $a_{ij} = a_{ji}$, and $a_{ij} = a_{ji} = 1 \implies ({\bf e}_i - {\bf e}_j)^T A ({\bf e}_i - {\bf e}_j) = 0$, which contradicts positive definite. Thus $A$ is the identity. For clarification : I am looking for an answer along these lines: ""A symmetric matrix implies or is equivalent to $X$ about the underlying linear transformation.  A $0$-$1$ matrix implies or is equivalent to $Y$ about the underlying linear transformation.  All eigenvalues positive implies or is equivalent to $Z$ about the underlying linear transformation.  ($X$, $Y$, and $Z$ are all big-picture properties.)  [Insert argument here.]  Thus the only matrix that simultaneously satisfies $X$, $Y$, and $Z$ is the identity."" Robert Israel's answer, while nice, is not the kind of thing I'm hoping for.  I view it as a more elegant version of my own low-level argument about what form the entries of the matrix have to take, not what kind of linear transformation has these three properties.",,"['linear-algebra', 'matrices']"
72,eigenvalues of certain block matrices,eigenvalues of certain block matrices,,"This question inquired about the determinant of this matrix: $$ \begin{bmatrix}   -\lambda &1  &0  &1  &0  &1  \\    1& -\lambda &1  &0  &1  &0  \\    0&  1& -\lambda &1  &0  &1  \\    1&  0&  1& -\lambda &1  &0  \\    0&  1&  0&  1& -\lambda &1  \\    1&  0&  1&  0&1 & -\lambda \end{bmatrix} $$ and of other matrices in a sequence to which it belongs.  In a comment I mentioned that if we permute the indices 1, 2, 3, 4, 5, 6 to put the odd ones first and then the even ones, thus 1, 3, 5, 2, 4, 6, then we get this: $$ \begin{bmatrix} -\lambda & 0 & 0 & 1 & 1 & 1 \\ 0 & -\lambda & 0 & 1 & 1 & 1 \\ 0 & 0 & -\lambda & 1 & 1 & 1 \\ 1 & 1 & 1 & -\lambda & 0 & 0 \\ 1 & 1 & 1 & 0 & -\lambda & 0 \\ 1 & 1 & 1 & 0 & 0 & -\lambda \end{bmatrix} $$ So this is of the form $$ \begin{bmatrix} A & B \\ B & A \end{bmatrix} $$ where $A$ and $B$ are symmetric matrices whose characteristic polynomials and eigenvalues are easily found, even if we consider not this one case of $6\times 6$ matrices, but arbitrarily large matrices following the same pattern. Are there simple formulas for determinants, characteristic polynomials, and eigenvalues for matrices of this latter kind? I thought of the Haynesworth inertia additivity formula because I only vaguely remembered what it said.  But apparently it only counts positive, negative, and zero eigenvalues.","This question inquired about the determinant of this matrix: $$ \begin{bmatrix}   -\lambda &1  &0  &1  &0  &1  \\    1& -\lambda &1  &0  &1  &0  \\    0&  1& -\lambda &1  &0  &1  \\    1&  0&  1& -\lambda &1  &0  \\    0&  1&  0&  1& -\lambda &1  \\    1&  0&  1&  0&1 & -\lambda \end{bmatrix} $$ and of other matrices in a sequence to which it belongs.  In a comment I mentioned that if we permute the indices 1, 2, 3, 4, 5, 6 to put the odd ones first and then the even ones, thus 1, 3, 5, 2, 4, 6, then we get this: $$ \begin{bmatrix} -\lambda & 0 & 0 & 1 & 1 & 1 \\ 0 & -\lambda & 0 & 1 & 1 & 1 \\ 0 & 0 & -\lambda & 1 & 1 & 1 \\ 1 & 1 & 1 & -\lambda & 0 & 0 \\ 1 & 1 & 1 & 0 & -\lambda & 0 \\ 1 & 1 & 1 & 0 & 0 & -\lambda \end{bmatrix} $$ So this is of the form $$ \begin{bmatrix} A & B \\ B & A \end{bmatrix} $$ where $A$ and $B$ are symmetric matrices whose characteristic polynomials and eigenvalues are easily found, even if we consider not this one case of $6\times 6$ matrices, but arbitrarily large matrices following the same pattern. Are there simple formulas for determinants, characteristic polynomials, and eigenvalues for matrices of this latter kind? I thought of the Haynesworth inertia additivity formula because I only vaguely remembered what it said.  But apparently it only counts positive, negative, and zero eigenvalues.",,"['linear-algebra', 'matrices', 'eigenvalues-eigenvectors', 'block-matrices']"
73,Distance of a matrix from the orthogonal group,Distance of a matrix from the orthogonal group,,"Let $\lVert A\rVert = \left(\sum_{i,j=1}^n \left\| a_{ij} \right\|^2\right)^{\frac{1}{2}} = \sqrt{\operatorname{tr}(A^\top A)}$ be the Frobenius norm on $n \times n$ matrices. Fix $A \in GL_n(\mathbb{R})$. 1) Is there a formula for the $dist(A,O(n))$? where $dist^2(A,O(n)) =\underset{X \in O(n)}{\text{min}} \|A - X\|^2$ (O(n) is the orthogonal group , i.e matrices satisfying $X^TX=I_d$, The minimum exists since $O(n)$ is compact) Note: In general we don't always have a unique minimizer (i.e there can be more than one orthogonal matrice which is closest to $A$ in $o(n)$), at least if we consider non-invertible matrices $A$. For example, $A=0$ is in the same distance from each element of $o(n)$ since the Frobenius norm of any isometry is $\sqrt n$. Question: Can we prove the minimizer is unique (if $A \in GL_n(\mathbb{R})$)? If so, is the function: $GL_n(\mathbb{R}) \to \mathbb{R}^{n^2} \, ,  \, A \to X(A)$ where $X(A)$ is the minimizer, smooth? Can we provide an explicit formula for it? I have tried using Lagrange's multipliers, but so far with no success. (I couldn't determine if the gradients of all the $n^2$ constraints are always linearly independent. Remark: \begin{align} \|A - X\|^2 &= \mathrm{tr} \left( (A-X)^t (A-X)    \right) \\\             &= \mathrm{tr} \left( (A^t -X^t) (A-X)  \right)  \\\             &= \mathrm{tr} (A^tA -A^tX - X^tA + X^tX)    \\\             &= \mathrm{tr} (A^tA) - \mathrm{tr}(A^tX) - \mathrm{tr}((A^tX)^t) +  \mathrm{tr} (I_d)\\\             &= n + \mathrm{tr} (A^tA) - 2\mathrm{tr}(A^tX) \end{align} so minimizing $\|A - X\|$ is equivalent to maximizing $\mathrm{tr}(A^tX)$. (In particular, the objective function to optimize is linear and not quadratic)","Let $\lVert A\rVert = \left(\sum_{i,j=1}^n \left\| a_{ij} \right\|^2\right)^{\frac{1}{2}} = \sqrt{\operatorname{tr}(A^\top A)}$ be the Frobenius norm on $n \times n$ matrices. Fix $A \in GL_n(\mathbb{R})$. 1) Is there a formula for the $dist(A,O(n))$? where $dist^2(A,O(n)) =\underset{X \in O(n)}{\text{min}} \|A - X\|^2$ (O(n) is the orthogonal group , i.e matrices satisfying $X^TX=I_d$, The minimum exists since $O(n)$ is compact) Note: In general we don't always have a unique minimizer (i.e there can be more than one orthogonal matrice which is closest to $A$ in $o(n)$), at least if we consider non-invertible matrices $A$. For example, $A=0$ is in the same distance from each element of $o(n)$ since the Frobenius norm of any isometry is $\sqrt n$. Question: Can we prove the minimizer is unique (if $A \in GL_n(\mathbb{R})$)? If so, is the function: $GL_n(\mathbb{R}) \to \mathbb{R}^{n^2} \, ,  \, A \to X(A)$ where $X(A)$ is the minimizer, smooth? Can we provide an explicit formula for it? I have tried using Lagrange's multipliers, but so far with no success. (I couldn't determine if the gradients of all the $n^2$ constraints are always linearly independent. Remark: \begin{align} \|A - X\|^2 &= \mathrm{tr} \left( (A-X)^t (A-X)    \right) \\\             &= \mathrm{tr} \left( (A^t -X^t) (A-X)  \right)  \\\             &= \mathrm{tr} (A^tA -A^tX - X^tA + X^tX)    \\\             &= \mathrm{tr} (A^tA) - \mathrm{tr}(A^tX) - \mathrm{tr}((A^tX)^t) +  \mathrm{tr} (I_d)\\\             &= n + \mathrm{tr} (A^tA) - 2\mathrm{tr}(A^tX) \end{align} so minimizing $\|A - X\|$ is equivalent to maximizing $\mathrm{tr}(A^tX)$. (In particular, the objective function to optimize is linear and not quadratic)",,"['linear-algebra', 'matrices', 'metric-spaces', 'inner-products']"
74,How to solve this to find the Null Space,How to solve this to find the Null Space,,"What I did: I put this into reduced row echelon form: $$\begin{bmatrix} 1 & -2 & 2 & 4 \\ 0 & 0 & 1 & 1 \\ 0 & 0 & 0 & 0 \\ 0 & 0 & 0 & 0 \end{bmatrix}$$ It is clear that the $r(M)=2$, because there are two independent rows. Now for the null space, I wrote down the equations from the reduced row echelon form: $$x-2y+2z+4t=0$$ $$z+t=0$$ I can't seem to write $x$ and $y$ separately in terms of $z$ and $t$. Any hints?","What I did: I put this into reduced row echelon form: $$\begin{bmatrix} 1 & -2 & 2 & 4 \\ 0 & 0 & 1 & 1 \\ 0 & 0 & 0 & 0 \\ 0 & 0 & 0 & 0 \end{bmatrix}$$ It is clear that the $r(M)=2$, because there are two independent rows. Now for the null space, I wrote down the equations from the reduced row echelon form: $$x-2y+2z+4t=0$$ $$z+t=0$$ I can't seem to write $x$ and $y$ separately in terms of $z$ and $t$. Any hints?",,"['linear-algebra', 'vector-spaces']"
75,Proof behind $S^n\cong SO(n+1)/SO(n)$,Proof behind,S^n\cong SO(n+1)/SO(n),"I have been trying to understand the fact that $S^n \cong SO(n+1)/SO(n)$. I believe I have the intuition correct at this point; consider the case when $n=2$ as we have $S^2 \cong SO(3)/SO(2)$.: We are trying to find the correspondence between rotations in $\mathbb{R}^3$ and points on $S^2$. At first I incorrectly thought that these spaces were isomorphic, however one then realizes that there are more rotations than points on a sphere, in the following sense: Consider the point $p$ at the ""north pole"" of the sphere $S^2$. We can correspond $p$ with any point on the sphere by rotating the sphere, that is, by applying elements on $SO(3)$ so that $p$ ends up at any place upon it. However, we can first apply any rotation around the $z$-axis (through $p$). So, by ""modding out"" these rotations, which are exactly the elements of $SO(2)$, we have our isomorphism. I am looking for someone to help me formalize this into a proof. Thank you!","I have been trying to understand the fact that $S^n \cong SO(n+1)/SO(n)$. I believe I have the intuition correct at this point; consider the case when $n=2$ as we have $S^2 \cong SO(3)/SO(2)$.: We are trying to find the correspondence between rotations in $\mathbb{R}^3$ and points on $S^2$. At first I incorrectly thought that these spaces were isomorphic, however one then realizes that there are more rotations than points on a sphere, in the following sense: Consider the point $p$ at the ""north pole"" of the sphere $S^2$. We can correspond $p$ with any point on the sphere by rotating the sphere, that is, by applying elements on $SO(3)$ so that $p$ ends up at any place upon it. However, we can first apply any rotation around the $z$-axis (through $p$). So, by ""modding out"" these rotations, which are exactly the elements of $SO(2)$, we have our isomorphism. I am looking for someone to help me formalize this into a proof. Thank you!",,"['linear-algebra', 'abstract-algebra', 'differential-geometry']"
76,Spectrum of a compact operator,Spectrum of a compact operator,,"If the spectrum of a compact operator is finite, I don't understand why $0$ has to be a member. I have proved that for all $\epsilon > 0$, there is only a finite number of eigenvectors which have eigenvalues $x$ satisfying $|x|>\epsilon$. I can see that when the spectrum is countable, the sequence of eigenvalue tends to zero so since the spectrum is closed, $0$ is a member but don't see why when the spectrum is finite.  Also, since every operator on a finite dimensional space is compact, am I right in thinking this results is only valid for infinite dimensions? Thanks","If the spectrum of a compact operator is finite, I don't understand why $0$ has to be a member. I have proved that for all $\epsilon > 0$, there is only a finite number of eigenvectors which have eigenvalues $x$ satisfying $|x|>\epsilon$. I can see that when the spectrum is countable, the sequence of eigenvalue tends to zero so since the spectrum is closed, $0$ is a member but don't see why when the spectrum is finite.  Also, since every operator on a finite dimensional space is compact, am I right in thinking this results is only valid for infinite dimensions? Thanks",,['linear-algebra']
77,Expressing a matrix as an expansion of its eigenvalues,Expressing a matrix as an expansion of its eigenvalues,,"This shouldn't be too difficult but I can't find a satisfactory proof. Show that a real, symmetric matrix $A$ with dimensions $D \times D$ satisfying the eigenvector   equation $Au_{i} = \lambda u_{i}$ can be expressed as an expansion of   its eigenvalues in the following way: $$A = \sum_{i=1}^{D}\lambda_{i}u_{i}u_{i}^{T}$$ and similarly, the   inverse $A^{-1}$ can be expressed as $$A^{-1} = \sum_{i=1}^{D}\frac{1}{\lambda_{i}}u_{i}u_{i}^{T}$$ I suppose this is an alternative form of eigendecomposition. I know this can be proved using $AU = U\Lambda$ where $\Lambda$ is a diagonal matrix and $U$ an orthogonal matrix, but it's a somewhat tedious procedure. An additional question: Do I need to assume a real, symmetric matrix? Thanks a lot","This shouldn't be too difficult but I can't find a satisfactory proof. Show that a real, symmetric matrix with dimensions satisfying the eigenvector   equation can be expressed as an expansion of   its eigenvalues in the following way: and similarly, the   inverse can be expressed as I suppose this is an alternative form of eigendecomposition. I know this can be proved using where is a diagonal matrix and an orthogonal matrix, but it's a somewhat tedious procedure. An additional question: Do I need to assume a real, symmetric matrix? Thanks a lot",A D \times D Au_{i} = \lambda u_{i} A = \sum_{i=1}^{D}\lambda_{i}u_{i}u_{i}^{T} A^{-1} A^{-1} = \sum_{i=1}^{D}\frac{1}{\lambda_{i}}u_{i}u_{i}^{T} AU = U\Lambda \Lambda U,"['linear-algebra', 'matrices', 'eigenvalues-eigenvectors']"
78,What is the relationship between the Hodge dual of p-vectors and the dual space of an ordinary vector space?,What is the relationship between the Hodge dual of p-vectors and the dual space of an ordinary vector space?,,"I understand what the Hodge dual is, but I can't quite wrap my head around the dual space of vector space. They seem very similar, almost the same, but perhaps they are unrelated. For instance, in $\mathbb{R}^3$, the blade $a \wedge b$ gives you a subspace that's like a plane, and the dual is roughly the normal to the plane. Is there a similarly simple example for the dual space of a vector space, or is there a way to describe the vector space dual in terms of the Hodge dual?","I understand what the Hodge dual is, but I can't quite wrap my head around the dual space of vector space. They seem very similar, almost the same, but perhaps they are unrelated. For instance, in $\mathbb{R}^3$, the blade $a \wedge b$ gives you a subspace that's like a plane, and the dual is roughly the normal to the plane. Is there a similarly simple example for the dual space of a vector space, or is there a way to describe the vector space dual in terms of the Hodge dual?",,"['linear-algebra', 'exterior-algebra']"
79,Determinants of matrices defined by the minimum/maximum indices of their entries,Determinants of matrices defined by the minimum/maximum indices of their entries,,"$ \newcommand{\m}[1]{\left( \begin{matrix} #1 \end{matrix} \right)} $ The Problem: Consider the matrices $A_n := \left(a^{(n)}_{i,j}\right)_{1 \le i,j \le n},B_n := \left(b^{(n)}_{i,j}\right)_{1 \le i,j \le n} \in M_{n \times n}(\Bbb R)$ with entries defined as follows: $$a^{(n)}_{i,j} := \min \{i,j\} \qquad b^{(n)}_{i,j} := \max \{i,j\}$$ (Examples of these matrices follow momentarily, to show how the structure of each ""evolves"" and emerges as $n$ increases.) What are $\det(A_n)$ and $\det(B_n)$ in the general $n \times n$ case, for each $n \in \Bbb Z^+$ ? Background/Context: I happened to have a question similar to this on my linear algebra final a few weeks ago. (In full disclosure: final grades have already been determined and all that, so this isn't any longer an ""active"" test question.) However, that problem used a matrix similar to $A_n$ , without specific numbers, and only the $4 \times 4$ case. I then recently found a video by Dr. Peyam which considered the $5 \times 5$ cases for both $A_n$ and $B_n$ , and hinted that these possibly came from a Putnam exam. I'm not sure if it was, though. I looked back through all of the Putnam problems back to $1985$ and didn't see this exact one, though one was similar: $A2$ from $2014$ , where $A_n$ is instead defined by $a^{(n)}_{i,j} = 1/\min\{i,j\}$ . (Maybe I'll look at that some other time.) I imagine the definition for $B_n$ is just considered a natural generalization of this problem. In any event, I couldn't find this problem handled in its generality on MSE, so I figured I'd try to figure it out, and post my own solution. Of course, if you have your own solutions, they're greatly welcomed! Each can provide their own insights. Small Cases for the Matrices: To help show the structure of the matrices, $A_n$ and $B_n$ look like, in the cases for $n=1,2,\cdots,6$ , as follows. Determinants are included, but calculated by Wolfram. \begin{alignat*}{3} &\boxed{n=1} &&\qquad A_1 = \m{ 1  }  &&\qquad \det(A_1) = 1\\ & &&\qquad B_1 = \m{ 1 } &&\qquad \det(B_1) = 1\\ &\boxed{n=2} &&\qquad A_2 = \m{ 1 & 1 \\ 1 & 2  }  &&\qquad \det(A_2) = 1\\ & &&\qquad B_2 = \m{ 1 & 2    \\ 2 & 2   } &&\qquad \det(B_2) = -2\\ &\boxed{n=3} &&\qquad A_3 = \m{ 1 & 1 & 1 \\ 1 & 2 & 2 \\ 1 & 2 & 3 }  &&\qquad \det(A_3) = 1\\ & &&\qquad B_3 = \m{ 1 & 2 & 3   \\ 2 & 2 & 3  \\ 3 & 3 & 3  } &&\qquad \det(B_3) = 3\\ &\boxed{n=4} &&\qquad A_4 = \m{ 1 & 1 & 1 & 1 \\ 1 & 2 & 2 & 2 \\ 1 & 2 & 3 & 3 \\ 1 & 2 & 3 & 4 }  &&\qquad \det(A_4) = 1\\ & &&\qquad B_4 = \m{ 1 & 2 & 3 & 4  \\ 2 & 2 & 3 & 4 \\ 3 & 3 & 3 & 4   \\ 4 & 4 & 4 & 4  } &&\qquad \det(B_4) = -4\\ &\boxed{n=5} &&\qquad A_5 = \m{ 1 & 1 & 1 & 1 & 1 \\ 1 & 2 & 2 & 2 & 2 \\ 1 & 2 & 3 & 3 & 3 \\ 1 & 2 & 3 & 4 & 4 \\ 1 & 2 & 3 & 4 & 5 }  &&\qquad \det(A_5) = 1\\ & &&\qquad B_5 = \m{ 1 & 2 & 3 & 4 & 5  \\ 2 & 2 & 3 & 4 & 5 \\ 3 & 3 & 3 & 4 & 5  \\ 4 & 4 & 4 & 4 & 5 \\ 5 & 5 & 5 & 5 & 5 } &&\qquad \det(B_5) = 5\\ &\boxed{n=6} &&\qquad A_6 = \m{ 1 & 1 & 1 & 1 & 1 & 1\\ 1 & 2 & 2 & 2 & 2 & 2\\ 1 & 2 & 3 & 3 & 3 & 3\\ 1 & 2 & 3 & 4 & 4 & 4\\ 1 & 2 & 3 & 4 & 5 & 5\\ 1 & 2 & 3 & 4 & 5 & 6}  &&\qquad \det(A_6) = 1\\ & &&\qquad B_6 = \m{ 1 & 2 & 3 & 4 & 5 & 6 \\ 2 & 2 & 3 & 4 & 5 & 6\\ 3 & 3 & 3 & 4 & 5 & 6 \\ 4 & 4 & 4 & 4 & 5 & 6 \\ 5 & 5 & 5 & 5 & 5 & 6 \\ 6 & 6 & 6 & 6 & 6 & 6} &&\qquad \det(B_6) = -6 \end{alignat*} One reasonably conjectures that $\det(A_n) = 1$ for all $n$ and $\det(B_n) = (-1)^{n-1} n$ for all $n$ .","The Problem: Consider the matrices with entries defined as follows: (Examples of these matrices follow momentarily, to show how the structure of each ""evolves"" and emerges as increases.) What are and in the general case, for each ? Background/Context: I happened to have a question similar to this on my linear algebra final a few weeks ago. (In full disclosure: final grades have already been determined and all that, so this isn't any longer an ""active"" test question.) However, that problem used a matrix similar to , without specific numbers, and only the case. I then recently found a video by Dr. Peyam which considered the cases for both and , and hinted that these possibly came from a Putnam exam. I'm not sure if it was, though. I looked back through all of the Putnam problems back to and didn't see this exact one, though one was similar: from , where is instead defined by . (Maybe I'll look at that some other time.) I imagine the definition for is just considered a natural generalization of this problem. In any event, I couldn't find this problem handled in its generality on MSE, so I figured I'd try to figure it out, and post my own solution. Of course, if you have your own solutions, they're greatly welcomed! Each can provide their own insights. Small Cases for the Matrices: To help show the structure of the matrices, and look like, in the cases for , as follows. Determinants are included, but calculated by Wolfram. One reasonably conjectures that for all and for all .","
\newcommand{\m}[1]{\left( \begin{matrix} #1 \end{matrix} \right)}
 A_n := \left(a^{(n)}_{i,j}\right)_{1 \le i,j \le n},B_n := \left(b^{(n)}_{i,j}\right)_{1 \le i,j \le n} \in M_{n \times n}(\Bbb R) a^{(n)}_{i,j} := \min \{i,j\} \qquad b^{(n)}_{i,j} := \max \{i,j\} n \det(A_n) \det(B_n) n \times n n \in \Bbb Z^+ A_n 4 \times 4 5 \times 5 A_n B_n 1985 A2 2014 A_n a^{(n)}_{i,j} = 1/\min\{i,j\} B_n A_n B_n n=1,2,\cdots,6 \begin{alignat*}{3}
&\boxed{n=1} &&\qquad A_1 = \m{
1  }  &&\qquad \det(A_1) = 1\\
& &&\qquad B_1 = \m{
1 }
&&\qquad \det(B_1) = 1\\
&\boxed{n=2} &&\qquad A_2 = \m{
1 & 1 \\
1 & 2  } 
&&\qquad \det(A_2) = 1\\
& &&\qquad B_2 = \m{
1 & 2    \\
2 & 2   }
&&\qquad \det(B_2) = -2\\
&\boxed{n=3} &&\qquad A_3 = \m{
1 & 1 & 1 \\
1 & 2 & 2 \\
1 & 2 & 3 } 
&&\qquad \det(A_3) = 1\\
& &&\qquad B_3 = \m{
1 & 2 & 3   \\
2 & 2 & 3  \\
3 & 3 & 3  }
&&\qquad \det(B_3) = 3\\
&\boxed{n=4} &&\qquad A_4 = \m{
1 & 1 & 1 & 1 \\
1 & 2 & 2 & 2 \\
1 & 2 & 3 & 3 \\
1 & 2 & 3 & 4 } 
&&\qquad \det(A_4) = 1\\
& &&\qquad B_4 = \m{
1 & 2 & 3 & 4  \\
2 & 2 & 3 & 4 \\
3 & 3 & 3 & 4   \\
4 & 4 & 4 & 4  }
&&\qquad \det(B_4) = -4\\
&\boxed{n=5} &&\qquad A_5 = \m{
1 & 1 & 1 & 1 & 1 \\
1 & 2 & 2 & 2 & 2 \\
1 & 2 & 3 & 3 & 3 \\
1 & 2 & 3 & 4 & 4 \\
1 & 2 & 3 & 4 & 5 } 
&&\qquad \det(A_5) = 1\\
& &&\qquad B_5 = \m{
1 & 2 & 3 & 4 & 5  \\
2 & 2 & 3 & 4 & 5 \\
3 & 3 & 3 & 4 & 5  \\
4 & 4 & 4 & 4 & 5 \\
5 & 5 & 5 & 5 & 5 }
&&\qquad \det(B_5) = 5\\
&\boxed{n=6} &&\qquad A_6 = \m{
1 & 1 & 1 & 1 & 1 & 1\\
1 & 2 & 2 & 2 & 2 & 2\\
1 & 2 & 3 & 3 & 3 & 3\\
1 & 2 & 3 & 4 & 4 & 4\\
1 & 2 & 3 & 4 & 5 & 5\\
1 & 2 & 3 & 4 & 5 & 6} 
&&\qquad \det(A_6) = 1\\
& &&\qquad B_6 = \m{
1 & 2 & 3 & 4 & 5 & 6 \\
2 & 2 & 3 & 4 & 5 & 6\\
3 & 3 & 3 & 4 & 5 & 6 \\
4 & 4 & 4 & 4 & 5 & 6 \\
5 & 5 & 5 & 5 & 5 & 6 \\
6 & 6 & 6 & 6 & 6 & 6}
&&\qquad \det(B_6) = -6
\end{alignat*} \det(A_n) = 1 n \det(B_n) = (-1)^{n-1} n n","['linear-algebra', 'matrices', 'determinant', 'alternative-proof']"
80,Online visualization tool for planes (spans in linear algebra),Online visualization tool for planes (spans in linear algebra),,"I would like to visualize planes in 3D as I start learning linear algebra, to build a solid foundation. Surprisingly, I have been unable to find an online tool (website/web app) to visualize planes in 3 dimensions. For example, I'd like to be able to enter 3 points and see the plane. Does something like this exist?","I would like to visualize planes in 3D as I start learning linear algebra, to build a solid foundation. Surprisingly, I have been unable to find an online tool (website/web app) to visualize planes in 3 dimensions. For example, I'd like to be able to enter 3 points and see the plane. Does something like this exist?",,"['linear-algebra', 'math-software']"
81,Adjoint Operator and Inverse,Adjoint Operator and Inverse,,"I am solving the following question and I am not really sure about the way I approach Question 1: Assume that $T:U\rightarrow U$ is invertible map. Prove that $(T^*)^{-1}=(T^{-1})^*$ Here is my answer: Notice that $\langle Tv,u\rangle = \langle v, T^*u \rangle$ for all $u,v\in U$ Then  $\langle T^*(T^{-1})^*v,u\rangle = \langle (T^{-1})^* v, Tu \rangle = \langle v,T^{-1}(Tu)\rangle = \langle u,v\rangle$ $T^* (T^{-1})^* = 1 $ so $(T^{-1})^* $ is the inverse of $T^*$. Hence, $(T^{-1})^*=(T^*)^{-1}$. Question 2: Prove that for every operator $T$ the operators $T^*T$ and $T T^*$ are self-adjoint Assumption: I have to prove $T^*T = T$ and $T T^* = T $ Suppose $T\in L (V,W)$. Fix $ w\in W$ . $\langle Tv,u\rangle = \langle v, T^*u \rangle$ for all $u,v\in U$ Then  $\langle TT^*v,u\rangle = \langle ((Tv)T^*,u \rangle = \langle Tv,Tu\rangle $ ?? $\langle T^*Tv,u\rangle = \langle (T^* (Tv),u \rangle = \langle Tv,Tu\rangle$ ??? I am not sure whether I am right or wrong for the above problem so please help me correct the above problem and understand adjoint and self-adjoint.","I am solving the following question and I am not really sure about the way I approach Question 1: Assume that $T:U\rightarrow U$ is invertible map. Prove that $(T^*)^{-1}=(T^{-1})^*$ Here is my answer: Notice that $\langle Tv,u\rangle = \langle v, T^*u \rangle$ for all $u,v\in U$ Then  $\langle T^*(T^{-1})^*v,u\rangle = \langle (T^{-1})^* v, Tu \rangle = \langle v,T^{-1}(Tu)\rangle = \langle u,v\rangle$ $T^* (T^{-1})^* = 1 $ so $(T^{-1})^* $ is the inverse of $T^*$. Hence, $(T^{-1})^*=(T^*)^{-1}$. Question 2: Prove that for every operator $T$ the operators $T^*T$ and $T T^*$ are self-adjoint Assumption: I have to prove $T^*T = T$ and $T T^* = T $ Suppose $T\in L (V,W)$. Fix $ w\in W$ . $\langle Tv,u\rangle = \langle v, T^*u \rangle$ for all $u,v\in U$ Then  $\langle TT^*v,u\rangle = \langle ((Tv)T^*,u \rangle = \langle Tv,Tu\rangle $ ?? $\langle T^*Tv,u\rangle = \langle (T^* (Tv),u \rangle = \langle Tv,Tu\rangle$ ??? I am not sure whether I am right or wrong for the above problem so please help me correct the above problem and understand adjoint and self-adjoint.",,['linear-algebra']
82,Find the span of a set of vectors,Find the span of a set of vectors,,"In general, what is the most straightforward way to find the span of a set of vectors? I'm trying to find the span of these three vectors: $$\{[1, 3, 3], [0, 0, 1], [1, 3, 1]\}$$","In general, what is the most straightforward way to find the span of a set of vectors? I'm trying to find the span of these three vectors: $$\{[1, 3, 3], [0, 0, 1], [1, 3, 1]\}$$",,['linear-algebra']
83,A non-negative matrix has a non-negative inverse. What other properties does it have?,A non-negative matrix has a non-negative inverse. What other properties does it have?,,"This is homework for my mathematical optimization class. Here is the exact question: Element-wise nonnegative matrix and inverse. Suppose a matrix $A \in\Bbb R^{n\times n}$   , and its inverse $B$, have all their elements nonnegative, i.e., $A_{ij}\geq 0$, $B_{ij}\geq 0$, for $i,j = 1,\dots,n$. What can you say must be true of   $A$ and $B$? Please give your answer first, and then the justification.   Your solution (which includes what you can say about $A$ and $B$, as well   as your justification) must be short. I have no idea what they are looking for; so far, I've got just the basic facts stemming from the fact that an inverse exists (it's square, the determinant is non-zero etc.). What can I deduce from the ""non-negative"" property?","This is homework for my mathematical optimization class. Here is the exact question: Element-wise nonnegative matrix and inverse. Suppose a matrix $A \in\Bbb R^{n\times n}$   , and its inverse $B$, have all their elements nonnegative, i.e., $A_{ij}\geq 0$, $B_{ij}\geq 0$, for $i,j = 1,\dots,n$. What can you say must be true of   $A$ and $B$? Please give your answer first, and then the justification.   Your solution (which includes what you can say about $A$ and $B$, as well   as your justification) must be short. I have no idea what they are looking for; so far, I've got just the basic facts stemming from the fact that an inverse exists (it's square, the determinant is non-zero etc.). What can I deduce from the ""non-negative"" property?",,"['linear-algebra', 'matrices']"
84,"Why are the eigenvalues of these ""bitwise XOR matrices"" integers?","Why are the eigenvalues of these ""bitwise XOR matrices"" integers?",,"In the course of playing around with this question , I have hit upon a question of my own. Consider the $n\times n$ symmetric matrix $\mathbf X$ whose entries are given by $x_{j,k}=(j-1)\mathbin{\mathrm{XOR}}(k-1)$, where $\mathrm{XOR}$ is bitwise XOR . (As noted in the other question, the matrix is essentially the Cayley table for nimber addition .) Looking at the eigenvalues of $\mathbf X$ when $n$ is a power of $2$, I have observed a few things with the first few members: All the eigenvalues are integers If $n=2^m$, then there are $m+1$ nonzero eigenvalues. Of the $m+1$ nonzero eigenvalues, one is positive, and the other eigenvalues are negative. The positive eigenvalue of the $2^m \times 2^m$ matrix $\mathbf X$ is $2^{m-1}(2^m-1)$ . The $m$ negative eigenvalues of $\mathbf X$ are the negatives of the powers of $2$ between $2^{m-1}$ and $2^{2m-2}$, inclusive. (For those who want to try it out in Mathematica : Table[Eigenvalues[Array[BitXor, {2^n, 2^n}, {0, 0}], n + 1], {n, 8}] ) Is there a simple explanation for these observations?","In the course of playing around with this question , I have hit upon a question of my own. Consider the $n\times n$ symmetric matrix $\mathbf X$ whose entries are given by $x_{j,k}=(j-1)\mathbin{\mathrm{XOR}}(k-1)$, where $\mathrm{XOR}$ is bitwise XOR . (As noted in the other question, the matrix is essentially the Cayley table for nimber addition .) Looking at the eigenvalues of $\mathbf X$ when $n$ is a power of $2$, I have observed a few things with the first few members: All the eigenvalues are integers If $n=2^m$, then there are $m+1$ nonzero eigenvalues. Of the $m+1$ nonzero eigenvalues, one is positive, and the other eigenvalues are negative. The positive eigenvalue of the $2^m \times 2^m$ matrix $\mathbf X$ is $2^{m-1}(2^m-1)$ . The $m$ negative eigenvalues of $\mathbf X$ are the negatives of the powers of $2$ between $2^{m-1}$ and $2^{2m-2}$, inclusive. (For those who want to try it out in Mathematica : Table[Eigenvalues[Array[BitXor, {2^n, 2^n}, {0, 0}], n + 1], {n, 8}] ) Is there a simple explanation for these observations?",,"['linear-algebra', 'matrices', 'eigenvalues-eigenvectors']"
85,"A ""geometric'' infinite sum of matrices","A ""geometric'' infinite sum of matrices",,"The sum $$ I + A + A^2 + A^3 + \cdots $$ equals $$(I-A)^{-1}$$ under the assumption $\rho(A)<1$, which is necessary to make the sum converge. My question: what does the sum  $$ I + A^T A + (A^2)^T A^2 + (A^3)^T A^3 + \cdots$$ equal under the same assumption? Is there a similarly neat expression? It is true that this last sum converges under the assumption $\rho(A)<1$. The ``obvious'' guess $(I-A^T A)^{-1}$ for the sum is not true because $ (A^2)^T A^2 \neq (A^T A)^2$; in fact, this guess does not even make sense because $\rho(A)<1$ does not rule out $\rho(A^T A)=1$.","The sum $$ I + A + A^2 + A^3 + \cdots $$ equals $$(I-A)^{-1}$$ under the assumption $\rho(A)<1$, which is necessary to make the sum converge. My question: what does the sum  $$ I + A^T A + (A^2)^T A^2 + (A^3)^T A^3 + \cdots$$ equal under the same assumption? Is there a similarly neat expression? It is true that this last sum converges under the assumption $\rho(A)<1$. The ``obvious'' guess $(I-A^T A)^{-1}$ for the sum is not true because $ (A^2)^T A^2 \neq (A^T A)^2$; in fact, this guess does not even make sense because $\rho(A)<1$ does not rule out $\rho(A^T A)=1$.",,['linear-algebra']
86,Demystifying the Determinant,Demystifying the Determinant,,"Preface: I’ve read a few of the posts on here about the determinant but none seem to put to rest the the questions I have, so I’ve decided to ask outright. Question: In general, I would like to just have a deeper understanding of where the determinant comes from. It seems so fundamental to almost any field of mathematics. So here are my questions about it: Who discovered the formula for the determinant? Was it noticed or was it built for a specific purpose? If so what was that motivation? Why/how does it relate to consistent/inconsistent systems? How does it relate to cross product? Why is it that two vectors aligned in a certain way produces a third orthogonal vector and how does the determinant make this happen? Why is the determinant used in finding  eigenvalues and eigenvectors? Any details would be very much appreciated!","Preface: I’ve read a few of the posts on here about the determinant but none seem to put to rest the the questions I have, so I’ve decided to ask outright. Question: In general, I would like to just have a deeper understanding of where the determinant comes from. It seems so fundamental to almost any field of mathematics. So here are my questions about it: Who discovered the formula for the determinant? Was it noticed or was it built for a specific purpose? If so what was that motivation? Why/how does it relate to consistent/inconsistent systems? How does it relate to cross product? Why is it that two vectors aligned in a certain way produces a third orthogonal vector and how does the determinant make this happen? Why is the determinant used in finding  eigenvalues and eigenvectors? Any details would be very much appreciated!",,"['linear-algebra', 'determinant']"
87,Prove that $\det(A)=p_1p_2-ba={bf(a)-af(b)\over b-a}$,Prove that,\det(A)=p_1p_2-ba={bf(a)-af(b)\over b-a},"Let $f(x)=(p_1-x)\cdots (p_n-x)$ $p_1,\ldots, p_n\in \mathbb R$ and let $a,b\in \mathbb R$ such that $a\neq b$ Prove that $\det A={bf(a)-af(b)\over b-a}$ where $A$ is the matrix: $$\begin{pmatrix}p_1 & a & a & \cdots & a \\ b & p_2 & a & \cdots & a \\ \vdots & \vdots & \vdots & \ddots & \vdots \\ b & b & b & \cdots & p_n \end{pmatrix}$$ that is the entries $k_{ij}=a$ if $i<j$, $k_{ij}=p_i$ if $i=j$ and $k_{ij}=b$ if $i>j$ I tried to do it by induction over $n$. The base case for $n=2$ is easy $\det(A)=p_1p_2-ba={bf(a)-af(b)\over b-a}$ The induction step is where I don´t know what to do. I tried to solve the dterminant by brute force(applying my induction hypothesis for n and prove it for n+1) but I don´t know how to reduce it. It gets horrible. I would really appreciate if you can help me with this problem. Any comments, suggestions or hints would be highly appreciated","Let $f(x)=(p_1-x)\cdots (p_n-x)$ $p_1,\ldots, p_n\in \mathbb R$ and let $a,b\in \mathbb R$ such that $a\neq b$ Prove that $\det A={bf(a)-af(b)\over b-a}$ where $A$ is the matrix: $$\begin{pmatrix}p_1 & a & a & \cdots & a \\ b & p_2 & a & \cdots & a \\ \vdots & \vdots & \vdots & \ddots & \vdots \\ b & b & b & \cdots & p_n \end{pmatrix}$$ that is the entries $k_{ij}=a$ if $i<j$, $k_{ij}=p_i$ if $i=j$ and $k_{ij}=b$ if $i>j$ I tried to do it by induction over $n$. The base case for $n=2$ is easy $\det(A)=p_1p_2-ba={bf(a)-af(b)\over b-a}$ The induction step is where I don´t know what to do. I tried to solve the dterminant by brute force(applying my induction hypothesis for n and prove it for n+1) but I don´t know how to reduce it. It gets horrible. I would really appreciate if you can help me with this problem. Any comments, suggestions or hints would be highly appreciated",,"['linear-algebra', 'matrices', 'polynomials', 'induction', 'determinant']"
88,proof on trace trick,proof on trace trick,,"I wanted to find the maximum likelihood estimator for $\mathbf{\Sigma}$ in the multivariate gaussian. I was anticipating the solution would be a bit involved and messy, if not 'brute-forced', but I was surprised to find an elegant and clever shortcut known as the trace trick. For a vector $\mathbf{x}$ and a matrix $\mathbf{A}$, $\mathbf{x}^T\mathbf{Ax} = tr (\mathbf{x}^T\mathbf{Ax}) = tr (\mathbf{xx}^T\mathbf{A}) = tr (\mathbf{Axx}^T)$. I am not sure how this can be true. Any insight/rough sketch of proof would be great.","I wanted to find the maximum likelihood estimator for $\mathbf{\Sigma}$ in the multivariate gaussian. I was anticipating the solution would be a bit involved and messy, if not 'brute-forced', but I was surprised to find an elegant and clever shortcut known as the trace trick. For a vector $\mathbf{x}$ and a matrix $\mathbf{A}$, $\mathbf{x}^T\mathbf{Ax} = tr (\mathbf{x}^T\mathbf{Ax}) = tr (\mathbf{xx}^T\mathbf{A}) = tr (\mathbf{Axx}^T)$. I am not sure how this can be true. Any insight/rough sketch of proof would be great.",,['linear-algebra']
89,Vector spaces - Multiplying by zero scalar yields zero vector,Vector spaces - Multiplying by zero scalar yields zero vector,,"Please rate and comment. I want to improve; constructive criticism is highly appreciated. Please take style into account as well. The following proof is solely based on vector space related axioms. Axiom names are italicised . They are defined in Wikipedia (see vector space article). Vector spaces - Multiplying by zero scalar yields zero vector \begin{array}{lrll} \text{Let} & \dots & \text{be} & \dots \\ \hline & F && \text{a field.} \\ & V && \text{a vector space over $F$.} \\ & 0 && \text{an identity element of addition of $F$.} \\ & \mathbf{0} && \text{an identity element of addition of $V$.} \\ & \mathbf{v} && \text{an arbitrary vector in $V$.} \\ \end{array} $$\text{Then, }0\mathbf{v} = \mathbf{0}.$$ Proof. We will denote by $1$ an identity element of scalar multiplication; we will denote by $(-\mathbf{v})$ an additive inverse of $\mathbf{v}$. \begin{align*} 0\mathbf{v} &= 0\mathbf{v} + \mathbf{0}                     && \text{by }\textit{Identity element of vector addition} \\ &= 0\mathbf{v} + (\mathbf{v} + (-\mathbf{v}))   && \text{by }\textit{Inverse elements of vector addition} \\ &= (0\mathbf{v} + \mathbf{v}) + (-\mathbf{v})   && \text{by }\textit{Associativity of vector addition} \\ &= (0\mathbf{v} + 1\mathbf{v}) + (-\mathbf{v})  && \text{by }\textit{Identity element of scalar multiplication} \\ &= ((0 + 1)\mathbf{v}) + (-\mathbf{v})          && \text{by }\textit{Distributivity of scalar multiplication (field addition)} \\ &= ((1 + 0)\mathbf{v}) + (-\mathbf{v})          && \text{by }\textit{Commutativity of field addition} \\ &= (1\mathbf{v}) + (-\mathbf{v})                && \text{by }\textit{Identity element of field addition} \\ &= \mathbf{v} + (-\mathbf{v})                   && \text{by }\textit{Identity element of scalar multiplication} \\ &= \mathbf{0}                                   && \text{by }\textit{Inverse elements of vector addition} \\ \end{align*} QED","Please rate and comment. I want to improve; constructive criticism is highly appreciated. Please take style into account as well. The following proof is solely based on vector space related axioms. Axiom names are italicised . They are defined in Wikipedia (see vector space article). Vector spaces - Multiplying by zero scalar yields zero vector \begin{array}{lrll} \text{Let} & \dots & \text{be} & \dots \\ \hline & F && \text{a field.} \\ & V && \text{a vector space over $F$.} \\ & 0 && \text{an identity element of addition of $F$.} \\ & \mathbf{0} && \text{an identity element of addition of $V$.} \\ & \mathbf{v} && \text{an arbitrary vector in $V$.} \\ \end{array} $$\text{Then, }0\mathbf{v} = \mathbf{0}.$$ Proof. We will denote by $1$ an identity element of scalar multiplication; we will denote by $(-\mathbf{v})$ an additive inverse of $\mathbf{v}$. \begin{align*} 0\mathbf{v} &= 0\mathbf{v} + \mathbf{0}                     && \text{by }\textit{Identity element of vector addition} \\ &= 0\mathbf{v} + (\mathbf{v} + (-\mathbf{v}))   && \text{by }\textit{Inverse elements of vector addition} \\ &= (0\mathbf{v} + \mathbf{v}) + (-\mathbf{v})   && \text{by }\textit{Associativity of vector addition} \\ &= (0\mathbf{v} + 1\mathbf{v}) + (-\mathbf{v})  && \text{by }\textit{Identity element of scalar multiplication} \\ &= ((0 + 1)\mathbf{v}) + (-\mathbf{v})          && \text{by }\textit{Distributivity of scalar multiplication (field addition)} \\ &= ((1 + 0)\mathbf{v}) + (-\mathbf{v})          && \text{by }\textit{Commutativity of field addition} \\ &= (1\mathbf{v}) + (-\mathbf{v})                && \text{by }\textit{Identity element of field addition} \\ &= \mathbf{v} + (-\mathbf{v})                   && \text{by }\textit{Identity element of scalar multiplication} \\ &= \mathbf{0}                                   && \text{by }\textit{Inverse elements of vector addition} \\ \end{align*} QED",,"['linear-algebra', 'vector-spaces', 'proof-writing', 'proof-verification', 'learning']"
90,How to determine the side on which a point lies?,How to determine the side on which a point lies?,,"Suppose we have a linear equation and a point in the plane, then how can one determine on which side of the line the point lies?","Suppose we have a linear equation and a point in the plane, then how can one determine on which side of the line the point lies?",,"['linear-algebra', 'geometry']"
91,"How to interpret $(V^*)^*$, the dual space of the dual space?","How to interpret , the dual space of the dual space?",(V^*)^*,"Suppose $V$ is a real vector space. Then $V^*$ , its dual space, is the vector space of linear maps $V\to \mathbb R$ How then do I interpret $(V^*)^*$ , the dual space of the dual space?","Suppose is a real vector space. Then , its dual space, is the vector space of linear maps How then do I interpret , the dual space of the dual space?",V V^* V\to \mathbb R (V^*)^*,"['linear-algebra', 'dual-spaces']"
92,Why does a positive definite matrix defines a convex cone?,Why does a positive definite matrix defines a convex cone?,,"I've been working on convex optimization and got stuck. What exactly does a positive definite(p.d) matrix represent geometrically ? what kind of vector space it forms ? If I have a p.d matrix which represent a convex cone (which I can't understand why), how do I prove the convexity for that matrix ? What's the input variable say X should be ? Say if I have a plane, $$W^TX  = B$$ at least I know I should put X into the equation, but for a p.d matrix... it's just a matrix, why does that even represent a function ? I am totally confused. Any hint helps a lot.","I've been working on convex optimization and got stuck. What exactly does a positive definite(p.d) matrix represent geometrically ? what kind of vector space it forms ? If I have a p.d matrix which represent a convex cone (which I can't understand why), how do I prove the convexity for that matrix ? What's the input variable say X should be ? Say if I have a plane, $$W^TX  = B$$ at least I know I should put X into the equation, but for a p.d matrix... it's just a matrix, why does that even represent a function ? I am totally confused. Any hint helps a lot.",,"['linear-algebra', 'convex-optimization']"
93,simple ring which is not semisimple,simple ring which is not semisimple,,"Let $V$ be a vector space of countably infinite dimension over a field $k$ and put $R = \text{End}_k(V)$. Then it is not hard to see that $R$ has a unique proper nonzero two-sided ideal $I$, which consists of those operators with finite rank. So in particular $R/I$ is a simple ring, but I'm pretty sure it's not semisimple. How can I show this? It would be enough to prove that $R/I$ is not artinian.","Let $V$ be a vector space of countably infinite dimension over a field $k$ and put $R = \text{End}_k(V)$. Then it is not hard to see that $R$ has a unique proper nonzero two-sided ideal $I$, which consists of those operators with finite rank. So in particular $R/I$ is a simple ring, but I'm pretty sure it's not semisimple. How can I show this? It would be enough to prove that $R/I$ is not artinian.",,"['linear-algebra', 'noncommutative-algebra']"
94,Lights out game on hexagonal grid,Lights out game on hexagonal grid,,"I greatly enjoyed the Lights Out game described here (I am sorry I had to link to an older page because some wikidiot keeps deleting most of the page). Its mathematical analysis is here (it's just linear algebra) Now I just discovered a hexagonal version: http://cwynn.com/games/lightsoutmobile.htm (and hopefully soon) https://sites.google.com/site/beyondlightsout/ Are there any mathematical results for this version of the game, before I dig in and start the analysis myself. I have the major references including Turning Lights Out with Linear Algebra , by M. Anderson and T. Feil (1998). Math Magazine , vol. 71, no. 4, October 1998, pp. 300-303.  It's here (thanks to J.M.). Update: I've been playing with the iPhone T-Lights hexagonal versions. I can get most of them, except for the ones where the ""template"" is a Y shape. Any ideas?","I greatly enjoyed the Lights Out game described here (I am sorry I had to link to an older page because some wikidiot keeps deleting most of the page). Its mathematical analysis is here (it's just linear algebra) Now I just discovered a hexagonal version: http://cwynn.com/games/lightsoutmobile.htm (and hopefully soon) https://sites.google.com/site/beyondlightsout/ Are there any mathematical results for this version of the game, before I dig in and start the analysis myself. I have the major references including Turning Lights Out with Linear Algebra , by M. Anderson and T. Feil (1998). Math Magazine , vol. 71, no. 4, October 1998, pp. 300-303.  It's here (thanks to J.M.). Update: I've been playing with the iPhone T-Lights hexagonal versions. I can get most of them, except for the ones where the ""template"" is a Y shape. Any ideas?",,"['linear-algebra', 'combinatorics', 'discrete-mathematics', 'recreational-mathematics']"
95,Why any short exact sequence of vector spaces may be seen as a direct sum?,Why any short exact sequence of vector spaces may be seen as a direct sum?,,"This is actually the first time I have worked with short exact sequences and I have no clue why the following assertion is true: Any short exact sequence of vector spaces   $$ 0 \longrightarrow U \overset{A}{\longrightarrow} V \overset{B}{\longrightarrow} W \longrightarrow 0 $$   reduces to a decomposition $V \simeq U \oplus W$. Can anyone give a clear proof? I only know from the definition of short exact sequences that $A$ is injective, $B$ is surjective, and $\text{im}(A) = \ker(B)$","This is actually the first time I have worked with short exact sequences and I have no clue why the following assertion is true: Any short exact sequence of vector spaces   $$ 0 \longrightarrow U \overset{A}{\longrightarrow} V \overset{B}{\longrightarrow} W \longrightarrow 0 $$   reduces to a decomposition $V \simeq U \oplus W$. Can anyone give a clear proof? I only know from the definition of short exact sequences that $A$ is injective, $B$ is surjective, and $\text{im}(A) = \ker(B)$",,"['linear-algebra', 'differential-topology', 'exact-sequence']"
96,Orthogonal basis for infinite-dimensional vector spaces,Orthogonal basis for infinite-dimensional vector spaces,,"It is quite simple to show that all finite-dimensional vector spaces with inner product have an orthogonal basis, with the standard definition of a basis from linear algebra. However, I am in trouble to find any reference about infinite-dimensional case. Do all infinite-dimensional vector space with inner product have an orthogonal basis (again with the standard definition of a basis from linear algebra)?","It is quite simple to show that all finite-dimensional vector spaces with inner product have an orthogonal basis, with the standard definition of a basis from linear algebra. However, I am in trouble to find any reference about infinite-dimensional case. Do all infinite-dimensional vector space with inner product have an orthogonal basis (again with the standard definition of a basis from linear algebra)?",,['linear-algebra']
97,Show that if $A^{n}=I$ then $A$ is diagonalizable.,Show that if  then  is diagonalizable.,A^{n}=I A,"Suppose $A$ is an $m \times m$ matrix which satisfies $A^{n}=1$ for some $n$, then why is $A$ necessarily diagonalizable. Not sure if this is helpful, but here's my thinking so far: We know that $A$ satisfies $p(x)=x^{n}-1=(x-1)(x^{n-1}+\ldots+x+1)$.  If $A=I$ it is clearly diagonalizable so we may assume that $A$ is a root of the other factor. Edit: Actually, I'm a bit confused and not even sure if we can say that much.  Since the ring of $m \times m$ matrices is not an integral domain, we can not conclude that if $A-I \not = 0$ then $(A^{n-1}+\ldots+A+I)=0$, correct?","Suppose $A$ is an $m \times m$ matrix which satisfies $A^{n}=1$ for some $n$, then why is $A$ necessarily diagonalizable. Not sure if this is helpful, but here's my thinking so far: We know that $A$ satisfies $p(x)=x^{n}-1=(x-1)(x^{n-1}+\ldots+x+1)$.  If $A=I$ it is clearly diagonalizable so we may assume that $A$ is a root of the other factor. Edit: Actually, I'm a bit confused and not even sure if we can say that much.  Since the ring of $m \times m$ matrices is not an integral domain, we can not conclude that if $A-I \not = 0$ then $(A^{n-1}+\ldots+A+I)=0$, correct?",,"['linear-algebra', 'diagonalization']"
98,Does anyone know a non-trivial surjective multiplicative homomorphism from the $4\times 4$ matrices to the $2 \times 2 $ matrices?,Does anyone know a non-trivial surjective multiplicative homomorphism from the  matrices to the  matrices?,4\times 4 2 \times 2 ,It's well known the determinant provides a surjective homomorphism between $n \times n$ real matrices and $\mathbb{R}$ with the operation of multiplication. I was curious of inter-matrix multiplicative homorphisms. The trivial idea of viewing a $4\times 4$ matrix as a $2\times 2$ matrix with entries in the $2\times 2$ matrices suggests a determinant formula (of the usual type) but unfortunately that doesn't actually seem to work at all as a homomorphism after investigating it with sympy. Doing some reading I stumbled upon: Quasideterminants for non-commutative rings. But i'm a bit dissatisfied. I would like to think such a homomorphism should exist but I just don't know how to find it.,It's well known the determinant provides a surjective homomorphism between real matrices and with the operation of multiplication. I was curious of inter-matrix multiplicative homorphisms. The trivial idea of viewing a matrix as a matrix with entries in the matrices suggests a determinant formula (of the usual type) but unfortunately that doesn't actually seem to work at all as a homomorphism after investigating it with sympy. Doing some reading I stumbled upon: Quasideterminants for non-commutative rings. But i'm a bit dissatisfied. I would like to think such a homomorphism should exist but I just don't know how to find it.,n \times n \mathbb{R} 4\times 4 2\times 2 2\times 2,"['linear-algebra', 'abstract-algebra', 'matrices', 'determinant']"
99,"The ""range"" and ""image"" of a transformation refer to the same thing, right?","The ""range"" and ""image"" of a transformation refer to the same thing, right?",,"I'm hoping this is true because I've been told that ""the rank of a transformation is the dimension of its image"", and also that ""the rank of a transformation is the dimension of its range"". This doesn't make sense if range and image aren't the same thing, so I'm just wanting to be completely sure. Furthermore, does codomain refer to the image of the transformation or to the vector space to which the images of the transformation are brought?","I'm hoping this is true because I've been told that ""the rank of a transformation is the dimension of its image"", and also that ""the rank of a transformation is the dimension of its range"". This doesn't make sense if range and image aren't the same thing, so I'm just wanting to be completely sure. Furthermore, does codomain refer to the image of the transformation or to the vector space to which the images of the transformation are brought?",,"['linear-algebra', 'linear-transformations', 'matrix-rank']"
