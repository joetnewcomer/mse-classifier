,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,Show that the function f solves the homogenous wave equation,Show that the function f solves the homogenous wave equation,,"This is a cleaned up and refined repost of my previous attempt , after I did some research on the subject. SOLVED The only problem here was that I was making tons of little mistakes, always watch your signs! Task $\text{Let } \; c \in \mathbb{R} \; \text{ be a given parameter, with } \; c > 0$ $\text{ Show that } \; f: (\mathbb{R}^3 \setminus \{ \vec{0} \}) \times \mathbb{R} \to \mathbb{R} \; \text{ with:}$ $$ f(x,y,z,t) :=  \frac {\cos(\|(x,y,z)\|_2 -ct)}{\|(x,y,z)\|_2} $$ $\text{ solves the homogeneous wave equation:}$ $$ \frac {\partial^²f}{\partial x^2}(x,y,z,t) + \frac {\partial^²f}{\partial y^2}(x,y,z,t) + \frac {\partial^²f}{\partial z^2}(x,y,z,t) - \frac {1}{c^2} \frac {\partial^²f}{\partial t^2}(x,y,z,t) = 0 $$ $\text{for all } \; (x,y,z,t) \in (\mathbb{R}^3 \setminus \{ \vec{0} \}) \times \mathbb{R}$ $\text{You may use following rule:}$ $ \text{Let } \; g \in \mathscr{C}^2(\mathbb{R}), r: \mathbb{R}^3  \setminus \{ \vec{0} \} \to \mathbb{R} \text{ with } r(\vec{x}) := \|\vec{x}\|_2 \; \text { and } \; \vec{x} \in \mathbb{R}^3  \setminus \{ \vec{0} \} \; \text{ then: }$ $$ \frac {\partial^2 (g \circ r)}{\partial x_i^2}(\vec{x}) = \frac {g'(r(\vec{x}))}{r(\vec{x})} + \left( g''(r(\vec{x})) - \frac {g'(r(\vec{x}))}{r(\vec{x})} \right) \frac {x_i^2}{r^2(\vec{x})} \quad \text {for $\quad$ i = 1,2,3} $$ My Efforts $$ r = \|(x,y,z)\|_2 = \sqrt {x^2 + y^2 + z^2} \; (= r(\vec{x})) $$ $$ \frac {\partial^2 f}{\partial t^2}(x,y,z,t) = -\frac {c^2\cos(r-ct)}{r} \Rightarrow \frac {1}{c^2} \frac {\partial^²f}{\partial t^2}(x,y,z,t) = -\frac {\cos(r-ct)}{r} $$ $$ \Rightarrow \frac {\partial^²f}{\partial x^2}(x,y,z,t) + \frac {\partial^²f}{\partial y^2}(x,y,z,t) + \frac {\partial^²f}{\partial z^2}(x,y,z,t) = -\frac {\cos(r-ct)}{r} $$ $$ \sum_{i=1}^{3} \frac {\partial^2 f}{\partial x_i^2}(\vec{x},t) = \sum_{i=1}^{3} \frac {\partial^2 (g \circ r)}{\partial x_i^2}(\vec{x}) $$ $$ = 3 * \frac {g'(r(\vec{x}))}{r(\vec{x})} + \left( g''(r(\vec{x})) - \frac {g'(r(\vec{x}))}{r(\vec{x})} \right) \frac {1}{r(\vec{x})} $$ $$ \Rightarrow 3 * \frac {g'(r(\vec{x}))}{r(\vec{x})} + \left( \frac {g''(r(\vec{x}))}{r(\vec{x})} - \frac {g'(r(\vec{x}))}{r^2(\vec{x})} \right) = -\frac {\cos(r-ct)}{r} $$ $$ g'(r(\vec{x})) = -\frac {\sin(r-ct)}{r} - \frac {\cos(r-ct)}{r^2} $$ $$ g''(r(\vec{x})) = \frac {2\sin(r-ct)}{r^2} - \frac {\cos(r-ct)}{r} + \frac {2\cos(r-ct)}{r^3} $$ $$ \Rightarrow \sum_{i=1}^{3} \frac {\partial^2 f}{\partial x_i^2}(\vec{x},t) = -3\frac {r\sin(r-ct)+\cos(r-ct)}{r^3} + \left( \frac {2\sin(r-ct)}{r^2} - \frac {\cos(r-ct)}{r} + \frac {2\cos(r-ct)}{r^3} + \frac {r\sin(r-ct)+\cos(r-ct)}{r^3}\right) * r^2 $$ $$ = -3\frac {r\sin(r-ct)+\cos(r-ct)}{r^3} + \left( \frac {3r\sin(r-ct) - (r^2-3)\cos(r-ct)}{r^5} \right) * r^2 $$ $$ = -\frac {3r\sin(r-ct)+3\cos(r-ct)}{r^3} + \frac {3r\sin(r-ct) + 3\cos(r-ct) - r^2\cos(r-ct)}{r^3} $$ $$ = -\frac {3r\sin(r-ct)+3\cos(r-ct)}{r^3} + \frac {3r\sin(r-ct) + 3\cos(r-ct)}{r^3} - \frac{r^2\cos(r-ct)}{r^3} = -\frac {\cos(r-ct)}{r} $$ $$ \Rightarrow \sum_{i=1}^{3} \frac {\partial^2 f}{\partial x_i^2}(\vec{x},t) + \frac {\cos(r-ct)}{r}  =  -\frac {\cos(r-ct)}{r} + \frac {\cos(r-ct)}{r} = 0 $$ Question Where is my mistake / How to solve this equation?","This is a cleaned up and refined repost of my previous attempt , after I did some research on the subject. SOLVED The only problem here was that I was making tons of little mistakes, always watch your signs! Task $\text{Let } \; c \in \mathbb{R} \; \text{ be a given parameter, with } \; c > 0$ $\text{ Show that } \; f: (\mathbb{R}^3 \setminus \{ \vec{0} \}) \times \mathbb{R} \to \mathbb{R} \; \text{ with:}$ $$ f(x,y,z,t) :=  \frac {\cos(\|(x,y,z)\|_2 -ct)}{\|(x,y,z)\|_2} $$ $\text{ solves the homogeneous wave equation:}$ $$ \frac {\partial^²f}{\partial x^2}(x,y,z,t) + \frac {\partial^²f}{\partial y^2}(x,y,z,t) + \frac {\partial^²f}{\partial z^2}(x,y,z,t) - \frac {1}{c^2} \frac {\partial^²f}{\partial t^2}(x,y,z,t) = 0 $$ $\text{for all } \; (x,y,z,t) \in (\mathbb{R}^3 \setminus \{ \vec{0} \}) \times \mathbb{R}$ $\text{You may use following rule:}$ $ \text{Let } \; g \in \mathscr{C}^2(\mathbb{R}), r: \mathbb{R}^3  \setminus \{ \vec{0} \} \to \mathbb{R} \text{ with } r(\vec{x}) := \|\vec{x}\|_2 \; \text { and } \; \vec{x} \in \mathbb{R}^3  \setminus \{ \vec{0} \} \; \text{ then: }$ $$ \frac {\partial^2 (g \circ r)}{\partial x_i^2}(\vec{x}) = \frac {g'(r(\vec{x}))}{r(\vec{x})} + \left( g''(r(\vec{x})) - \frac {g'(r(\vec{x}))}{r(\vec{x})} \right) \frac {x_i^2}{r^2(\vec{x})} \quad \text {for $\quad$ i = 1,2,3} $$ My Efforts $$ r = \|(x,y,z)\|_2 = \sqrt {x^2 + y^2 + z^2} \; (= r(\vec{x})) $$ $$ \frac {\partial^2 f}{\partial t^2}(x,y,z,t) = -\frac {c^2\cos(r-ct)}{r} \Rightarrow \frac {1}{c^2} \frac {\partial^²f}{\partial t^2}(x,y,z,t) = -\frac {\cos(r-ct)}{r} $$ $$ \Rightarrow \frac {\partial^²f}{\partial x^2}(x,y,z,t) + \frac {\partial^²f}{\partial y^2}(x,y,z,t) + \frac {\partial^²f}{\partial z^2}(x,y,z,t) = -\frac {\cos(r-ct)}{r} $$ $$ \sum_{i=1}^{3} \frac {\partial^2 f}{\partial x_i^2}(\vec{x},t) = \sum_{i=1}^{3} \frac {\partial^2 (g \circ r)}{\partial x_i^2}(\vec{x}) $$ $$ = 3 * \frac {g'(r(\vec{x}))}{r(\vec{x})} + \left( g''(r(\vec{x})) - \frac {g'(r(\vec{x}))}{r(\vec{x})} \right) \frac {1}{r(\vec{x})} $$ $$ \Rightarrow 3 * \frac {g'(r(\vec{x}))}{r(\vec{x})} + \left( \frac {g''(r(\vec{x}))}{r(\vec{x})} - \frac {g'(r(\vec{x}))}{r^2(\vec{x})} \right) = -\frac {\cos(r-ct)}{r} $$ $$ g'(r(\vec{x})) = -\frac {\sin(r-ct)}{r} - \frac {\cos(r-ct)}{r^2} $$ $$ g''(r(\vec{x})) = \frac {2\sin(r-ct)}{r^2} - \frac {\cos(r-ct)}{r} + \frac {2\cos(r-ct)}{r^3} $$ $$ \Rightarrow \sum_{i=1}^{3} \frac {\partial^2 f}{\partial x_i^2}(\vec{x},t) = -3\frac {r\sin(r-ct)+\cos(r-ct)}{r^3} + \left( \frac {2\sin(r-ct)}{r^2} - \frac {\cos(r-ct)}{r} + \frac {2\cos(r-ct)}{r^3} + \frac {r\sin(r-ct)+\cos(r-ct)}{r^3}\right) * r^2 $$ $$ = -3\frac {r\sin(r-ct)+\cos(r-ct)}{r^3} + \left( \frac {3r\sin(r-ct) - (r^2-3)\cos(r-ct)}{r^5} \right) * r^2 $$ $$ = -\frac {3r\sin(r-ct)+3\cos(r-ct)}{r^3} + \frac {3r\sin(r-ct) + 3\cos(r-ct) - r^2\cos(r-ct)}{r^3} $$ $$ = -\frac {3r\sin(r-ct)+3\cos(r-ct)}{r^3} + \frac {3r\sin(r-ct) + 3\cos(r-ct)}{r^3} - \frac{r^2\cos(r-ct)}{r^3} = -\frac {\cos(r-ct)}{r} $$ $$ \Rightarrow \sum_{i=1}^{3} \frac {\partial^2 f}{\partial x_i^2}(\vec{x},t) + \frac {\cos(r-ct)}{r}  =  -\frac {\cos(r-ct)}{r} + \frac {\cos(r-ct)}{r} = 0 $$ Question Where is my mistake / How to solve this equation?",,"['multivariable-calculus', 'trigonometry', 'partial-differential-equations', 'partial-derivative', 'homogeneous-equation']"
1,Tangent space and implicit function theorem,Tangent space and implicit function theorem,,"Let's say we have a $C^1$-function $f:X\to\mathbb{R}^m$ ($X\subset\mathbb{R}^{n+m}$ an open set) and the rank of the matrix $Df(x)$ is $m.$ We'll let $Z=\lbrace x\in X:f(x)=0\rbrace$ and take some $x_0\in Z.$ In addition, we let $T$ be the set of $u\in\mathbb{R}^{n+m}$ such that there is an open $Y\subset\mathbb{R}$ with $0\in Y$ and continuously differentiable $c$ with $c(Y)\subset Z,$ $c(0)=x_0$ and $Dc(0)=u.$ I aim to show that $T$ is an $n$-dimensional subspace of $\mathbb{R}^{n+m},$ and I've got to a point where I'm a bit stuck, so I'm looking for help on where to go next. I wanted to use the implicit function theorem, first taking the $m$ rows that were linearly dependent to the end WLOG, so that they make an $m\times m$ (sub)matrix which must be invertible. Then I could use the implicit function theorem and find some set $X'$ containing $x_0,$ and a set $U\subset\mathbb{R}^n,$ with a $C^1$ $g:V\to\mathbb{R}^m$ so that $Z\cap X'=\lbrace (v,g(v)):v\in V\rbrace.$ The fact that $T$ should be a vector space is more or less straightforward but I can't quite prove that the dimension should be $n,$ but I believe the implicit function theorem can be used for this. I've also shown, since $T$ looks a lot like a tangent space at $x_0$ or something of the sort, that $\langle Df(x_0),u\rangle=0$ for any $u.$ I'm kind of tempted to let $u$ run over unit vectors or something like that, but I'm not entirely sure if it would be useful here, it would tell me more about the orthogonal complement. I get the feeling I'm quite close to completing the proof, but I just need some help knowing which way to go from here (ie. hints greatly appreciated).","Let's say we have a $C^1$-function $f:X\to\mathbb{R}^m$ ($X\subset\mathbb{R}^{n+m}$ an open set) and the rank of the matrix $Df(x)$ is $m.$ We'll let $Z=\lbrace x\in X:f(x)=0\rbrace$ and take some $x_0\in Z.$ In addition, we let $T$ be the set of $u\in\mathbb{R}^{n+m}$ such that there is an open $Y\subset\mathbb{R}$ with $0\in Y$ and continuously differentiable $c$ with $c(Y)\subset Z,$ $c(0)=x_0$ and $Dc(0)=u.$ I aim to show that $T$ is an $n$-dimensional subspace of $\mathbb{R}^{n+m},$ and I've got to a point where I'm a bit stuck, so I'm looking for help on where to go next. I wanted to use the implicit function theorem, first taking the $m$ rows that were linearly dependent to the end WLOG, so that they make an $m\times m$ (sub)matrix which must be invertible. Then I could use the implicit function theorem and find some set $X'$ containing $x_0,$ and a set $U\subset\mathbb{R}^n,$ with a $C^1$ $g:V\to\mathbb{R}^m$ so that $Z\cap X'=\lbrace (v,g(v)):v\in V\rbrace.$ The fact that $T$ should be a vector space is more or less straightforward but I can't quite prove that the dimension should be $n,$ but I believe the implicit function theorem can be used for this. I've also shown, since $T$ looks a lot like a tangent space at $x_0$ or something of the sort, that $\langle Df(x_0),u\rangle=0$ for any $u.$ I'm kind of tempted to let $u$ run over unit vectors or something like that, but I'm not entirely sure if it would be useful here, it would tell me more about the orthogonal complement. I get the feeling I'm quite close to completing the proof, but I just need some help knowing which way to go from here (ie. hints greatly appreciated).",,"['real-analysis', 'multivariable-calculus', 'implicit-function-theorem']"
2,Evaluating the triple integral $\int_{0}^{\sqrt{2}}\left(\int_{0}^{\sqrt{2-x^2}}\left(\int_{x^2+y^2}^{2}xdz\right)dy\right)dx$,Evaluating the triple integral,\int_{0}^{\sqrt{2}}\left(\int_{0}^{\sqrt{2-x^2}}\left(\int_{x^2+y^2}^{2}xdz\right)dy\right)dx,"I am trying to solve the following problem: Evaluate $$\int_{0}^{\sqrt{2}}\left(\int_{0}^{\sqrt{2-x^2}}\left(\int_{x^2+y^2}^{2}xdz\right)dy\right)dx$$ Sketch the region of integration and evaluate the integral by expressing the order of integration as dxdydz. Here's what I have done(or thought of) so far: Basically, we have to integrate the function $f(x,y,z)=x$ in the region between the paraboloid $z=x^2+y^2$and the plane $z=2$ over a quarter of a plane disc of radius $\sqrt2$ in the first quadrant. That should allow me to complete the sketch of the region of integration. Now, on to the integration: The above integral can also be written as: $$\int_{?}^{?}\left(\int_{0}^{\sqrt{2}}\left(\int_{0}^{\sqrt{2-y^2}}xdx\right)dy\right)dz$$ and here's where my problem occurs. What should come in place of those question marks in the third integral Evaluating my first two integrals left me with $\int\frac{2\sqrt{2}}{3}dz$. If I put $x^2+y^2$ and $2$ as the limits I would get my answer in terms of $x\;and\;y$ which is clearly unfeasible. So please tell me...what can I do next or what have I done wrong uptil now.","I am trying to solve the following problem: Evaluate $$\int_{0}^{\sqrt{2}}\left(\int_{0}^{\sqrt{2-x^2}}\left(\int_{x^2+y^2}^{2}xdz\right)dy\right)dx$$ Sketch the region of integration and evaluate the integral by expressing the order of integration as dxdydz. Here's what I have done(or thought of) so far: Basically, we have to integrate the function $f(x,y,z)=x$ in the region between the paraboloid $z=x^2+y^2$and the plane $z=2$ over a quarter of a plane disc of radius $\sqrt2$ in the first quadrant. That should allow me to complete the sketch of the region of integration. Now, on to the integration: The above integral can also be written as: $$\int_{?}^{?}\left(\int_{0}^{\sqrt{2}}\left(\int_{0}^{\sqrt{2-y^2}}xdx\right)dy\right)dz$$ and here's where my problem occurs. What should come in place of those question marks in the third integral Evaluating my first two integrals left me with $\int\frac{2\sqrt{2}}{3}dz$. If I put $x^2+y^2$ and $2$ as the limits I would get my answer in terms of $x\;and\;y$ which is clearly unfeasible. So please tell me...what can I do next or what have I done wrong uptil now.",,"['integration', 'multivariable-calculus', 'definite-integrals']"
3,Not exactly Partial Derivative,Not exactly Partial Derivative,,"I've been just introduced to concept of Partial Derivative, My question is for some continuous and differentiable $g(x)$ we have $$g'(x)=\lim\limits_{\delta x \to 0}  \left( \frac{g(x+\delta x) - g(x)}{\delta x} \right)$$ similarly for , for some continuous and differentiable $f(x,y,z)$ we have $$\frac {\partial f}{\partial y}=\lim\limits_{\delta y \to 0}  \left( \frac{f(x,y+\delta y,z) - f(x,y,z)}{\delta y} \right)$$ That means I'm looking at change in $f(x,y,z)$ w.r.t $y$, keeping all other variables constant But What if I am not interested in observing change in $f(x,y,z)$ w.r.t. one variable keeping all other constant. Actually, I want to observe change in $f(x,y,z)$ w.r.t.  both $x$ & $y$ at once keeping $z$ constant. will I get what I'm looking for ( i.e. change in $f(x,y,z)$ w.r.t. change in $x$ & $y$ ) by evaluating following two expressions? $$\frac{\partial f^2 }{\partial y \partial x} \tag{1}$$ $$\frac{\partial f^2 }{\partial x \partial y } \tag{2}$$ In that case both $1$ & $2$ should be equal. And If both $$\frac{\partial f }{\partial y}     \& \frac{\partial f }{\partial x } $$ are continuous and differentiable then $$\frac{\partial f^2 }{\partial x \partial y } = \frac{\partial f^2 }{\partial y \partial x } =\lim\limits_{ \delta y \to 0 , \delta x \to 0}  \left( \frac{f(x+\delta x ,y+ \delta y,z) - f(x,y,z)}{\delta x  + \delta y} \right)$$ Is above stated expression mathematically correct ( I don't think it could be, cause I myself proposed it ), If so, How can we prove all the equalities mentioned in it If, not what is correct expression in form of limits","I've been just introduced to concept of Partial Derivative, My question is for some continuous and differentiable $g(x)$ we have $$g'(x)=\lim\limits_{\delta x \to 0}  \left( \frac{g(x+\delta x) - g(x)}{\delta x} \right)$$ similarly for , for some continuous and differentiable $f(x,y,z)$ we have $$\frac {\partial f}{\partial y}=\lim\limits_{\delta y \to 0}  \left( \frac{f(x,y+\delta y,z) - f(x,y,z)}{\delta y} \right)$$ That means I'm looking at change in $f(x,y,z)$ w.r.t $y$, keeping all other variables constant But What if I am not interested in observing change in $f(x,y,z)$ w.r.t. one variable keeping all other constant. Actually, I want to observe change in $f(x,y,z)$ w.r.t.  both $x$ & $y$ at once keeping $z$ constant. will I get what I'm looking for ( i.e. change in $f(x,y,z)$ w.r.t. change in $x$ & $y$ ) by evaluating following two expressions? $$\frac{\partial f^2 }{\partial y \partial x} \tag{1}$$ $$\frac{\partial f^2 }{\partial x \partial y } \tag{2}$$ In that case both $1$ & $2$ should be equal. And If both $$\frac{\partial f }{\partial y}     \& \frac{\partial f }{\partial x } $$ are continuous and differentiable then $$\frac{\partial f^2 }{\partial x \partial y } = \frac{\partial f^2 }{\partial y \partial x } =\lim\limits_{ \delta y \to 0 , \delta x \to 0}  \left( \frac{f(x+\delta x ,y+ \delta y,z) - f(x,y,z)}{\delta x  + \delta y} \right)$$ Is above stated expression mathematically correct ( I don't think it could be, cause I myself proposed it ), If so, How can we prove all the equalities mentioned in it If, not what is correct expression in form of limits",,['calculus']
4,Find Point of intersection of the tangent plane to surface,Find Point of intersection of the tangent plane to surface,,"Find the point of intersection of the tangent plane to the surface $z+1=xe^y\cos(z)$ at the point $(1,0,0)$ and the line $L$ given by: $x=2t, y=t+1, z=1-3t$.","Find the point of intersection of the tangent plane to the surface $z+1=xe^y\cos(z)$ at the point $(1,0,0)$ and the line $L$ given by: $x=2t, y=t+1, z=1-3t$.",,['multivariable-calculus']
5,Show that $f(x)=\log(e^{x_1}+...+e^{x_n})$ is convex,Show that  is convex,f(x)=\log(e^{x_1}+...+e^{x_n}),"Show that the following function is convex. $$ f(x) = \log \left( e^{x_1} + \cdots + e^{x_n} \right) $$ I have no idea how to go about it.  I've been told to use Cauchy-Schwarz in order to show that the Hessian is non-negative definite, but I'm not sure how to do that. The Hessian is: For $i \neq j$ : $$\frac{\partial^2 f}{\partial x_i \partial x_j} = \frac{-e^{x_i+x_j}}{\left(\sum e^{x_i}\right)^2}$$ For $ i = j$ : $$\frac{\partial^2 f}{\partial x_i^2} = \frac{e^{x_i}\sum {e^{x_i}}-e^{2x_j}}{\left(\sum e^{x_i}\right)^2}$$ After this, writing out an expression for $v^THv$ gives: $$\sum \frac{\partial^2 f}{\partial x_i^2}v_i^2 + 2\sum\frac{\partial^2 f}{\partial x_ix_j}v_iv_j$$ It's clear the first term is non-negative. I'm not sure what to do about the second term though...","Show that the following function is convex. I have no idea how to go about it.  I've been told to use Cauchy-Schwarz in order to show that the Hessian is non-negative definite, but I'm not sure how to do that. The Hessian is: For : For : After this, writing out an expression for gives: It's clear the first term is non-negative. I'm not sure what to do about the second term though...", f(x) = \log \left( e^{x_1} + \cdots + e^{x_n} \right)  i \neq j \frac{\partial^2 f}{\partial x_i \partial x_j} = \frac{-e^{x_i+x_j}}{\left(\sum e^{x_i}\right)^2}  i = j \frac{\partial^2 f}{\partial x_i^2} = \frac{e^{x_i}\sum {e^{x_i}}-e^{2x_j}}{\left(\sum e^{x_i}\right)^2} v^THv \sum \frac{\partial^2 f}{\partial x_i^2}v_i^2 + 2\sum\frac{\partial^2 f}{\partial x_ix_j}v_iv_j,"['calculus', 'multivariable-calculus', 'convex-analysis', 'hessian-matrix']"
6,Second derivative expression,Second derivative expression,,"I have $f:\mathbb R^n\to \mathbb R$ and $\gamma:\mathbb R \to \mathbb R^n$, which are both $\mathrm C^2$. Considering $g=f\circ \gamma$, how could I express $g''$, second derivative of $g$ in terms of partial derivatives of $f$ and $\gamma$?. The first I know is that  $$ \mathrm Dg(a) = \mathrm Df(\gamma(a)) \circ D\gamma(a), $$ so I consider $Dg:x\mapsto Dg(x)$, but this gets complicated. Thanks in advance. Edit: I know what is the Hessian matrix, I would like to apply it here. Edit II: Well, it is clear that $$ g'(a) h = \langle \nabla  g(a), h\rangle = \langle \nabla f(\gamma(a)), \langle \nabla \gamma(a),h\rangle\rangle $$ so, what now?","I have $f:\mathbb R^n\to \mathbb R$ and $\gamma:\mathbb R \to \mathbb R^n$, which are both $\mathrm C^2$. Considering $g=f\circ \gamma$, how could I express $g''$, second derivative of $g$ in terms of partial derivatives of $f$ and $\gamma$?. The first I know is that  $$ \mathrm Dg(a) = \mathrm Df(\gamma(a)) \circ D\gamma(a), $$ so I consider $Dg:x\mapsto Dg(x)$, but this gets complicated. Thanks in advance. Edit: I know what is the Hessian matrix, I would like to apply it here. Edit II: Well, it is clear that $$ g'(a) h = \langle \nabla  g(a), h\rangle = \langle \nabla f(\gamma(a)), \langle \nabla \gamma(a),h\rangle\rangle $$ so, what now?",,"['multivariable-calculus', 'derivatives']"
7,$\Delta \vec{v}=0$ implies $\nabla\cdot \vec{v}=\nabla\times \vec{v}=0$?,implies ?,\Delta \vec{v}=0 \nabla\cdot \vec{v}=\nabla\times \vec{v}=0,"\begin{align} \Delta\overrightarrow{v}&=\nabla(\nabla\cdot\overrightarrow{v})-\nabla\times(\nabla\times\overrightarrow{v})\\ \nabla\cdot(\overrightarrow{v}\times\overrightarrow{w})&=\overrightarrow{w}\cdot\nabla\times\overrightarrow{v}-\overrightarrow{v}\cdot\nabla\times\overrightarrow{w}\\ \nabla\cdot(f\overrightarrow{v})&=(\nabla f)\cdot\overrightarrow{v}+f\nabla\cdot\overrightarrow{v} \end{align}   Suppose $\overrightarrow{v}$ satisfies $\Delta\overrightarrow{v}=0$   and that $\overrightarrow{v}$ vanishes outside some bounded region $V \subset \Bbb R^3$.   Show that $\nabla\times\overrightarrow{v}=0$ and $\nabla\cdot\overrightarrow{v}=0$. (Hint: Integrate $\overrightarrow{v}\Delta \overrightarrow{v}$ over a suitable region. You will need to make use of the divergence theorem.) I started by integrating the hint, then using the first identity for $\Delta\overrightarrow{v}$, then separated the integral. I tried to get it into the form for Gauss theorem, and know that the dot product is commutative so I tried that, but neither of the integrals that I separated the original one from can be solved with Gauss theorem. I feel that I am very far off the solution, any hints as to the next step would be greatly appreciated. The original document is linked here in case that would be easier to read. Thanks in advance for the help. I am new to math.stackexchange, please let me know if I make any mistakes with inputting my question.","\begin{align} \Delta\overrightarrow{v}&=\nabla(\nabla\cdot\overrightarrow{v})-\nabla\times(\nabla\times\overrightarrow{v})\\ \nabla\cdot(\overrightarrow{v}\times\overrightarrow{w})&=\overrightarrow{w}\cdot\nabla\times\overrightarrow{v}-\overrightarrow{v}\cdot\nabla\times\overrightarrow{w}\\ \nabla\cdot(f\overrightarrow{v})&=(\nabla f)\cdot\overrightarrow{v}+f\nabla\cdot\overrightarrow{v} \end{align}   Suppose $\overrightarrow{v}$ satisfies $\Delta\overrightarrow{v}=0$   and that $\overrightarrow{v}$ vanishes outside some bounded region $V \subset \Bbb R^3$.   Show that $\nabla\times\overrightarrow{v}=0$ and $\nabla\cdot\overrightarrow{v}=0$. (Hint: Integrate $\overrightarrow{v}\Delta \overrightarrow{v}$ over a suitable region. You will need to make use of the divergence theorem.) I started by integrating the hint, then using the first identity for $\Delta\overrightarrow{v}$, then separated the integral. I tried to get it into the form for Gauss theorem, and know that the dot product is commutative so I tried that, but neither of the integrals that I separated the original one from can be solved with Gauss theorem. I feel that I am very far off the solution, any hints as to the next step would be greatly appreciated. The original document is linked here in case that would be easier to read. Thanks in advance for the help. I am new to math.stackexchange, please let me know if I make any mistakes with inputting my question.",,"['calculus', 'multivariable-calculus', 'differential-geometry', 'vector-analysis']"
8,Frenet-Serret formula proof,Frenet-Serret formula proof,,"Prove that $$\textbf{r}''' = [s'''-\kappa^2(s')^3]\textbf{ T } + [3\kappa s's''+\kappa'(s')^2]\textbf{ N }+\kappa \hspace{1mm}\tau (s')^3\textbf{B}.$$ What is $\tau$, I can't figure that part out. All ideas are welcome.","Prove that $$\textbf{r}''' = [s'''-\kappa^2(s')^3]\textbf{ T } + [3\kappa s's''+\kappa'(s')^2]\textbf{ N }+\kappa \hspace{1mm}\tau (s')^3\textbf{B}.$$ What is $\tau$, I can't figure that part out. All ideas are welcome.",,"['calculus', 'multivariable-calculus', 'differential-geometry']"
9,L'hopital's Rule in higher dimensions.,L'hopital's Rule in higher dimensions.,,"I was working on getting intuition behind limits in multivariate calculus and I ran into this article . I am mostly concerned with the case where we have functions of two or three variables. Unfortunately I do not have the necessary background to understand the proof provided but I think that if the partial derivative in a given direction is not zero in the neighborhood of the limit point (for which the numerator and denominator are zero) then we have: $$\lim_{(x,y)\rightarrow (a,b)} \frac{f(x,y)}{g(x,y)}=\lim_{(x,y) \rightarrow (a,b)} \frac{D_vf(x,y)}{D_vg(x,y)}$$ So when seeking to resolve a question about limits (when the numerator and denominator are both zero at the point), I should quickly check the partial derivative of the numerator and denominator in convenient directions and ensure that they do not both vanish in the neighborhood. Is this a correct interpretation?","I was working on getting intuition behind limits in multivariate calculus and I ran into this article . I am mostly concerned with the case where we have functions of two or three variables. Unfortunately I do not have the necessary background to understand the proof provided but I think that if the partial derivative in a given direction is not zero in the neighborhood of the limit point (for which the numerator and denominator are zero) then we have: $$\lim_{(x,y)\rightarrow (a,b)} \frac{f(x,y)}{g(x,y)}=\lim_{(x,y) \rightarrow (a,b)} \frac{D_vf(x,y)}{D_vg(x,y)}$$ So when seeking to resolve a question about limits (when the numerator and denominator are both zero at the point), I should quickly check the partial derivative of the numerator and denominator in convenient directions and ensure that they do not both vanish in the neighborhood. Is this a correct interpretation?",,['multivariable-calculus']
10,"Show $\iint xye^{-xy}\,dx\,dy$ is convergent or divergent",Show  is convergent or divergent,"\iint xye^{-xy}\,dx\,dy","Determine convergence/divergence of $$\iint xye^{-xy}\,dx\,dy$$ for $x,y \geqslant 0$ i.e. in the first quadrant. I have managed to show that $xye^{-xy} \to 0$ in the first quadrant but other than that not gotten very much far unfortunately. One thought I had was to use polar coordinates and for some radius $r_0$ approximate $xye^{-xy} \thicksim e^{-xy} $. I would then want to investigate $$ \iint e^{-xy} \,dx\,dy$$ in the first quadrant from radius $r_0$ to infinity (the angle lies between zero and $\pi/2$). However that integral was no more easier than the previous one. Thoughts?","Determine convergence/divergence of $$\iint xye^{-xy}\,dx\,dy$$ for $x,y \geqslant 0$ i.e. in the first quadrant. I have managed to show that $xye^{-xy} \to 0$ in the first quadrant but other than that not gotten very much far unfortunately. One thought I had was to use polar coordinates and for some radius $r_0$ approximate $xye^{-xy} \thicksim e^{-xy} $. I would then want to investigate $$ \iint e^{-xy} \,dx\,dy$$ in the first quadrant from radius $r_0$ to infinity (the angle lies between zero and $\pi/2$). However that integral was no more easier than the previous one. Thoughts?",,"['multivariable-calculus', 'improper-integrals']"
11,Does every Lipschitz curve have finite length?,Does every Lipschitz curve have finite length?,,"So I thought I had proven that every Lipschitz curve had finite length, but then I read what I think is a counterexample: Let $\gamma :[0,1] \rightarrow \mathbb{R}^2$ the parametrization of a path such that $\gamma(t)=(t,t \sin(\frac{1}{t}))$ while $t>0$ and $\gamma(0)=(0,0)$. This curve is continuous and $\mathcal{C}^1$ everywhere in [0,1] , so it is Lipschitz, but it's length is infinite, isn't it? Am i missing something?","So I thought I had proven that every Lipschitz curve had finite length, but then I read what I think is a counterexample: Let $\gamma :[0,1] \rightarrow \mathbb{R}^2$ the parametrization of a path such that $\gamma(t)=(t,t \sin(\frac{1}{t}))$ while $t>0$ and $\gamma(0)=(0,0)$. This curve is continuous and $\mathcal{C}^1$ everywhere in [0,1] , so it is Lipschitz, but it's length is infinite, isn't it? Am i missing something?",,"['real-analysis', 'multivariable-calculus']"
12,Volume between cylinder and plane,Volume between cylinder and plane,,"Problem: Find the volume bounded by $z = y^2, x =0, y =0, z =9-x$. My working: $z$ goes from $y^2$ to $9-x$ so these are the limits of integration. Work out the points of intersection of $9-x$ and $y^2$. When $y=0$, $9-x=0$ and $x=9$. So $x$ goes from 0 to 9. When $x=0$, $y^2 = 9$ so $y=3$ (take the positive one). So $y$ goes from 0 to 9. Then evaluate  \begin{align} \int_{x=0}^{x=9} \int_{y=0}^{y=9} \int_{z=y^2}^{z=9-x} dz dy dz &= \int_{x=0}^{x=9} \int_{y=0}^{y=9} y^2 - 9 + x dy dx  \\ &= \int_{x=0}^{x=9} 18+3x dx \\ &= \frac{567}{2} \end{align} My textbook says the answer is $\frac{324}{5}$. What have I done wrong?","Problem: Find the volume bounded by $z = y^2, x =0, y =0, z =9-x$. My working: $z$ goes from $y^2$ to $9-x$ so these are the limits of integration. Work out the points of intersection of $9-x$ and $y^2$. When $y=0$, $9-x=0$ and $x=9$. So $x$ goes from 0 to 9. When $x=0$, $y^2 = 9$ so $y=3$ (take the positive one). So $y$ goes from 0 to 9. Then evaluate  \begin{align} \int_{x=0}^{x=9} \int_{y=0}^{y=9} \int_{z=y^2}^{z=9-x} dz dy dz &= \int_{x=0}^{x=9} \int_{y=0}^{y=9} y^2 - 9 + x dy dx  \\ &= \int_{x=0}^{x=9} 18+3x dx \\ &= \frac{567}{2} \end{align} My textbook says the answer is $\frac{324}{5}$. What have I done wrong?",,"['integration', 'multivariable-calculus']"
13,Can the divergence theorem be restricted to flat surfaces?,Can the divergence theorem be restricted to flat surfaces?,,"I am trying to prove this: $S$ is a bounded surface in $\mathbb{R}^2$ and $a$ a given scalar field $u,v$ are such that $\nabla^2 u=0$ on $S$ and $u=v=a$ on $\partial S$ . Then: $$\int_S |\nabla  v|^2\;\mathrm{d}S\geqslant \int_S  |\nabla  u|^2\;\mathrm{d}S$$ I would know how to do this is if $S$ were a volume, because I can use the divergence theorem. So I am wondering: is it true that for a bounded surface in $\mathbb{R}^2,$ $$\int_{\partial S}\mathbf{F}\cdot\mathrm{d}\mathbf{r}=\int_S \text{div}\,\mathbf{F}\;\mathrm{d}S?$$","I am trying to prove this: is a bounded surface in and a given scalar field are such that on and on . Then: I would know how to do this is if were a volume, because I can use the divergence theorem. So I am wondering: is it true that for a bounded surface in","S \mathbb{R}^2 a u,v \nabla^2 u=0 S u=v=a \partial S \int_S |\nabla  v|^2\;\mathrm{d}S\geqslant \int_S  |\nabla  u|^2\;\mathrm{d}S S \mathbb{R}^2, \int_{\partial S}\mathbf{F}\cdot\mathrm{d}\mathbf{r}=\int_S \text{div}\,\mathbf{F}\;\mathrm{d}S?","['integration', 'multivariable-calculus']"
14,Finding the gradient of $\sin^2(2x)\ln(5y+z^2)$,Finding the gradient of,\sin^2(2x)\ln(5y+z^2),"In a Calculus 2-exam, the candidates were presented with the following problem: a) Let $f(x,y,z) = \sin^2(2x)\ln(5y+z^2)$. Compute the gradient. b) Find the directional derivative in the direction u = $(1,2,3)$ at $(x,y,z) = (\pi/4, 1, 0)$ What follows is what I would have replied on an exam. I ask you to find eventual errors, comment on clarity, and if possible, score the answer on a scale from $0$ to $6$: a) The gradient $\nabla f$ is defined as $\nabla f = f_1 \hat{i} + f_2\hat{j} + f_3 \hat{k}$. First, we find all the partial derivatives: $f_1 = 2\sin(2x)2\cos(2x)\ln(5y+z^2)\\ f_2=\sin^2(2x)\cdot\frac{5}{5y+z^2}\\ f_3 = \sin^2(2x)\cdot\frac{2z}{5y+z^2}$ The gradient is now given by $\nabla f = f_1\hat{i} + f_2\hat{j} + f_3\hat{k}$, as stated above. b) We first normalize the vector u . We call this vector v . Then v = $(1/\sqrt{14},2/\sqrt{14},3/\sqrt{14})$. Then we may compute our directional derivative as $\nabla f(\pi/4, 1, 0) \bullet$ v = $2/\sqrt{14}$.","In a Calculus 2-exam, the candidates were presented with the following problem: a) Let $f(x,y,z) = \sin^2(2x)\ln(5y+z^2)$. Compute the gradient. b) Find the directional derivative in the direction u = $(1,2,3)$ at $(x,y,z) = (\pi/4, 1, 0)$ What follows is what I would have replied on an exam. I ask you to find eventual errors, comment on clarity, and if possible, score the answer on a scale from $0$ to $6$: a) The gradient $\nabla f$ is defined as $\nabla f = f_1 \hat{i} + f_2\hat{j} + f_3 \hat{k}$. First, we find all the partial derivatives: $f_1 = 2\sin(2x)2\cos(2x)\ln(5y+z^2)\\ f_2=\sin^2(2x)\cdot\frac{5}{5y+z^2}\\ f_3 = \sin^2(2x)\cdot\frac{2z}{5y+z^2}$ The gradient is now given by $\nabla f = f_1\hat{i} + f_2\hat{j} + f_3\hat{k}$, as stated above. b) We first normalize the vector u . We call this vector v . Then v = $(1/\sqrt{14},2/\sqrt{14},3/\sqrt{14})$. Then we may compute our directional derivative as $\nabla f(\pi/4, 1, 0) \bullet$ v = $2/\sqrt{14}$.",,"['multivariable-calculus', 'partial-derivative', 'solution-verification']"
15,The shortest path connecting three points,The shortest path connecting three points,,"I have 3 points X,Y,Z, lets call them buildings. I need to find the shortest amount of path that connects the 3 buildings, these buildings can be in any sort of shape and any distance from each other, lets call the distance between each building xy, xz, yz. I know that the paths need to converge at a point, but I am unsure how to get there given the information I have. If they formed an equilateral triangle it would look like (or at least I think): X     |               / \   Y   Z But they can be in any shape and that's only one of them, I need help finding the equation(s) that will give the shortest path connecting all 3 of the buildings. I was thinking of using the Pythagorean Theorem but am not 100% sure, I was also thinking about using Lagrange Multipliers but with the information given am not sure how to implement them. I'm just looking for a push in the right direction, I don't need the full solution (it would help but not needed.) If you need any more information about the problem I can try my best but this is about all I have.","I have 3 points X,Y,Z, lets call them buildings. I need to find the shortest amount of path that connects the 3 buildings, these buildings can be in any sort of shape and any distance from each other, lets call the distance between each building xy, xz, yz. I know that the paths need to converge at a point, but I am unsure how to get there given the information I have. If they formed an equilateral triangle it would look like (or at least I think): X     |               / \   Y   Z But they can be in any shape and that's only one of them, I need help finding the equation(s) that will give the shortest path connecting all 3 of the buildings. I was thinking of using the Pythagorean Theorem but am not 100% sure, I was also thinking about using Lagrange Multipliers but with the information given am not sure how to implement them. I'm just looking for a push in the right direction, I don't need the full solution (it would help but not needed.) If you need any more information about the problem I can try my best but this is about all I have.",,"['calculus', 'geometry', 'multivariable-calculus', 'graph-theory', 'lagrange-multiplier']"
16,gradient of norm square of a random vector,gradient of norm square of a random vector,,"Let $g(w)= \|Y_n - f(w,X_n) \|^2$ where $f:\Bbb R^d \times \Bbb R^m \to \Bbb R^k : w \in \Bbb R^d$. What is the gradient of $g$ ? $X_n$ and $Y_n$ are random vectors. Basically, I want to find gradient of a function like $\phi(x) = ||g(x)||^2$ where $g$ is a vector-valued function. $$\phi(x) = \|g(x)\|^2$$ Now,  $$\phi(x+h) = \|g(x+h)\|^2 = \|g(x) + h^TQ\|^2 = \|g(x)\|^2 + \|h^TQ\|^2 + 2\langle g(x),h^TQ\rangle.$$ How to find gradient from this ? Here $Q$ is a matrix where each column is the corresponding gradient for that co-ordinate. I don't think with this method I can get it independent of $h$","Let $g(w)= \|Y_n - f(w,X_n) \|^2$ where $f:\Bbb R^d \times \Bbb R^m \to \Bbb R^k : w \in \Bbb R^d$. What is the gradient of $g$ ? $X_n$ and $Y_n$ are random vectors. Basically, I want to find gradient of a function like $\phi(x) = ||g(x)||^2$ where $g$ is a vector-valued function. $$\phi(x) = \|g(x)\|^2$$ Now,  $$\phi(x+h) = \|g(x+h)\|^2 = \|g(x) + h^TQ\|^2 = \|g(x)\|^2 + \|h^TQ\|^2 + 2\langle g(x),h^TQ\rangle.$$ How to find gradient from this ? Here $Q$ is a matrix where each column is the corresponding gradient for that co-ordinate. I don't think with this method I can get it independent of $h$",,"['multivariable-calculus', 'taylor-expansion']"
17,A singular $n-$cube and a circumference defined the border than 2-cube,A singular cube and a circumference defined the border than 2-cube,n-,"This is an exercise from "" Calculus on Manifolds "" by Michel Spivack (first edition, p.100): If $c$ is a singular $1$-cube in $\mathbb{R}^2-\{0\}$, with $c(0)=c(1)$, show that there is an integer $n$ such that $c-c_{i,n}=\partial c^2$ for some $2-$chain $c^2$. Definitions: A singular $n-$cube in $A\subset \mathbb{R}^n$ is a continous fuction $\alpha:[0,1]^k\to A$. A singular $n-$chain is a sum $\sum^k_{j=1} r_j\alpha_j$, where $\alpha_j$ is a singular $n-$cube and $r_j\in \mathbb{R}$. So, in this case, $c$ is a curve in $\mathbb{R}^2$ and $c_{1,n}$ is a circumference with radio $1$ and a total of $n$ laps around the origin. I can consider a line that intersects the origin, and if $\{U_{+}, U_{-}\}$ is an open cover of $\mathbb{R^2}-\{0\}$, then $\{c^{-1}(U_{+}), c^{-1}(U_{-})\}$ is an open cover of $[0,1]$. I have problems to properly write the proof. Is this a correct idea? How can I end the proof? Thanks.","This is an exercise from "" Calculus on Manifolds "" by Michel Spivack (first edition, p.100): If $c$ is a singular $1$-cube in $\mathbb{R}^2-\{0\}$, with $c(0)=c(1)$, show that there is an integer $n$ such that $c-c_{i,n}=\partial c^2$ for some $2-$chain $c^2$. Definitions: A singular $n-$cube in $A\subset \mathbb{R}^n$ is a continous fuction $\alpha:[0,1]^k\to A$. A singular $n-$chain is a sum $\sum^k_{j=1} r_j\alpha_j$, where $\alpha_j$ is a singular $n-$cube and $r_j\in \mathbb{R}$. So, in this case, $c$ is a curve in $\mathbb{R}^2$ and $c_{1,n}$ is a circumference with radio $1$ and a total of $n$ laps around the origin. I can consider a line that intersects the origin, and if $\{U_{+}, U_{-}\}$ is an open cover of $\mathbb{R^2}-\{0\}$, then $\{c^{-1}(U_{+}), c^{-1}(U_{-})\}$ is an open cover of $[0,1]$. I have problems to properly write the proof. Is this a correct idea? How can I end the proof? Thanks.",,"['multivariable-calculus', 'differential-geometry', 'manifolds', 'manifolds-with-boundary']"
18,Gauss's theorem in 2d: how can it be expressed in differential forms?,Gauss's theorem in 2d: how can it be expressed in differential forms?,,"How do we express the 2d version of Gauss's theorem in the language of differential forms? In 3d, I know it is $$d \left(Fdydz + Gdzdx + Hdxdy\right) = F_x + G_y + H_z dxdydz$$ so by Stokes' theorem, we have that $$\iint_{\partial R} Fdydz + Gdzdx + Hdxdy = \int_R F_x + G_y + H_z dxdydz,$$ and the left side can be identified with the integral of $(F,G,H)\cdot \vec{n}dS$. But how does this work in two dimensions? I.e., how do we show $$\int_{\partial R} (F,G)\cdot \vec{n}dr = \iint_R F_x + G_ydxdy?$$","How do we express the 2d version of Gauss's theorem in the language of differential forms? In 3d, I know it is $$d \left(Fdydz + Gdzdx + Hdxdy\right) = F_x + G_y + H_z dxdydz$$ so by Stokes' theorem, we have that $$\iint_{\partial R} Fdydz + Gdzdx + Hdxdy = \int_R F_x + G_y + H_z dxdydz,$$ and the left side can be identified with the integral of $(F,G,H)\cdot \vec{n}dS$. But how does this work in two dimensions? I.e., how do we show $$\int_{\partial R} (F,G)\cdot \vec{n}dr = \iint_R F_x + G_ydxdy?$$",,"['multivariable-calculus', 'differential-geometry', 'vector-analysis']"
19,Triple integral in spherical coordinates,Triple integral in spherical coordinates,,"I'm trying to evaluate the triple integral $\int\int\int_B\frac{dV}{\sqrt{x^2+y^2+z^2+3}}$, where $B$ is the ball of radius $2$ centered at the origin.  Both the integrand and the nature of $B$ suggest a change to spherical coordinates.  As we know, the volume elements for Cartesian and spherical coordinates are related by $dx dy dz=\rho^2\sin(\varphi)d\rho d\varphi d\theta$, so the original triple integral is equal to the following iterated integral in spherical coordinates: $\int^{2\pi}_{0}\int^{\pi}_{0}\int^{2}_{0}\frac{\rho^2\sin(\varphi)}{\sqrt{\rho^2+3}}d\rho d\varphi d\theta$ The $\rho$-integral here isn't very pleasant to compute, though I suppose it's possible by means of a trigonometric substitution. My questions are: Was it the right decision to switch to spherical coordinates in the first place (as opposed to Cartesian or cylindrical coordinates)?  And if spherical coordinates are the best choice of coordinates, is there an easier way to do the $\rho$-integral than via the trig substitution $\rho=\sqrt{3}\tan(\alpha)$ for $-\pi/2<\alpha<\pi/2$ (assuming I've set up the iterated integral correctly)?","I'm trying to evaluate the triple integral $\int\int\int_B\frac{dV}{\sqrt{x^2+y^2+z^2+3}}$, where $B$ is the ball of radius $2$ centered at the origin.  Both the integrand and the nature of $B$ suggest a change to spherical coordinates.  As we know, the volume elements for Cartesian and spherical coordinates are related by $dx dy dz=\rho^2\sin(\varphi)d\rho d\varphi d\theta$, so the original triple integral is equal to the following iterated integral in spherical coordinates: $\int^{2\pi}_{0}\int^{\pi}_{0}\int^{2}_{0}\frac{\rho^2\sin(\varphi)}{\sqrt{\rho^2+3}}d\rho d\varphi d\theta$ The $\rho$-integral here isn't very pleasant to compute, though I suppose it's possible by means of a trigonometric substitution. My questions are: Was it the right decision to switch to spherical coordinates in the first place (as opposed to Cartesian or cylindrical coordinates)?  And if spherical coordinates are the best choice of coordinates, is there an easier way to do the $\rho$-integral than via the trig substitution $\rho=\sqrt{3}\tan(\alpha)$ for $-\pi/2<\alpha<\pi/2$ (assuming I've set up the iterated integral correctly)?",,"['calculus', 'integration', 'multivariable-calculus', 'definite-integrals']"
20,Double integral of a piecewise function over a rectangle?,Double integral of a piecewise function over a rectangle?,,"Let $f$ be defined on the rectangle $R=[1,2] \times [2,4]$ as follows: $$ f(x,y) = \begin{cases} (x+y)^{-2}, & \text{if }x\leq y \leq 2x; > \\\\ 0, & \text{otherwise. } \end{cases} $$ Compute the value of the double integral $\int\int_Rf$. I think since $f=0$ in the second case, I should just ignore it. However, what confuses me is the condition in the first case. The interval of $y$ is not $[x,2x]$ for all values of $x$. Since $2\leq y \leq 4$, this condition implies that $x=2$ for this interval. But does that mean I ignore $x\in [1,2)$? I'm just not sure how to begin this problem.","Let $f$ be defined on the rectangle $R=[1,2] \times [2,4]$ as follows: $$ f(x,y) = \begin{cases} (x+y)^{-2}, & \text{if }x\leq y \leq 2x; > \\\\ 0, & \text{otherwise. } \end{cases} $$ Compute the value of the double integral $\int\int_Rf$. I think since $f=0$ in the second case, I should just ignore it. However, what confuses me is the condition in the first case. The interval of $y$ is not $[x,2x]$ for all values of $x$. Since $2\leq y \leq 4$, this condition implies that $x=2$ for this interval. But does that mean I ignore $x\in [1,2)$? I'm just not sure how to begin this problem.",,['multivariable-calculus']
21,Systematic method to change the order of integration in multiple integrals,Systematic method to change the order of integration in multiple integrals,,"In many examples of computation of multiple integrals, it is necessary to change the order of integration to achieve the computation. For example, $I=\int_0^1\int_y^1 \cos(x^2)\ dx\ dy$ can be computed using Fubini's theorem as $I=\int_0^1\int_0^x\cos(x^2)\ dy\ dx=\int_0^1 x\cos(x^2)\ dx=\left[\frac{cos(x^2)}{2}\right]_0^1=\frac{\cos(1)}{2}$. It is generally easy to change the order of integration in double integral, but way more difficult in some cases of triple (or more) integrals. For example, $\begin{array}{rcl}\int_0^1\int_0^{1-x^2}\int_0^{1-x}f(x,y,z)\ dy\ dz\ dx & = &  \int_0^1\int_0^{1-\sqrt{1-z}}\int_0^{\sqrt{1-z}}f(x,y,z)\ dx\ dy\ dz  \\ & + &  \int_0^1\int_{1-\sqrt{1-z}}^{1}\int_0^{1-y}f(x,y,z)\ dx\ dy\ dz \end{array}$ Is there a general process to obtain this result? Please note that I know how to tackle this example, I am looking for a generic method/algorithm (hopefully simple enough to explain it to high school students.) Thank you.","In many examples of computation of multiple integrals, it is necessary to change the order of integration to achieve the computation. For example, $I=\int_0^1\int_y^1 \cos(x^2)\ dx\ dy$ can be computed using Fubini's theorem as $I=\int_0^1\int_0^x\cos(x^2)\ dy\ dx=\int_0^1 x\cos(x^2)\ dx=\left[\frac{cos(x^2)}{2}\right]_0^1=\frac{\cos(1)}{2}$. It is generally easy to change the order of integration in double integral, but way more difficult in some cases of triple (or more) integrals. For example, $\begin{array}{rcl}\int_0^1\int_0^{1-x^2}\int_0^{1-x}f(x,y,z)\ dy\ dz\ dx & = &  \int_0^1\int_0^{1-\sqrt{1-z}}\int_0^{\sqrt{1-z}}f(x,y,z)\ dx\ dy\ dz  \\ & + &  \int_0^1\int_{1-\sqrt{1-z}}^{1}\int_0^{1-y}f(x,y,z)\ dx\ dy\ dz \end{array}$ Is there a general process to obtain this result? Please note that I know how to tackle this example, I am looking for a generic method/algorithm (hopefully simple enough to explain it to high school students.) Thank you.",,"['integration', 'multivariable-calculus', 'definite-integrals']"
22,Polar coordinates: $ \iint_D (\sqrt{a^2 - x^2 -y^2} - \sqrt{x^2 + y^2})\:\mathrm{d}x\:\mathrm{d}y$,Polar coordinates:, \iint_D (\sqrt{a^2 - x^2 -y^2} - \sqrt{x^2 + y^2})\:\mathrm{d}x\:\mathrm{d}y,"I need to calculate the following integral $$\iint_D \left(\sqrt{a^2 - x^2 -y^2} - \sqrt{x^2 + y^2}\right)\:\mathrm{d}x\:\mathrm{d}y$$ where $D$ is the disk $x^2 + y^2 \leq a^2$ Using the transformation to polar coordinates $x=r\cos \theta$ and $y = r \sin \theta$, we have the new integration region $$ 0 < r < a$$ $$ 0 < \theta < 2 \pi$$ and the integral will be $$ \int_0^{2\pi} \int_0^a \left(\sqrt{a^2 - r^2} - r\right)r \:\mathrm{d}r\:\mathrm{d}\theta$$ But I am getting $0$ as answer, which is not the given answer. What am I doing wrong? Or maybe is the answer wrong? Thanks in advance!","I need to calculate the following integral $$\iint_D \left(\sqrt{a^2 - x^2 -y^2} - \sqrt{x^2 + y^2}\right)\:\mathrm{d}x\:\mathrm{d}y$$ where $D$ is the disk $x^2 + y^2 \leq a^2$ Using the transformation to polar coordinates $x=r\cos \theta$ and $y = r \sin \theta$, we have the new integration region $$ 0 < r < a$$ $$ 0 < \theta < 2 \pi$$ and the integral will be $$ \int_0^{2\pi} \int_0^a \left(\sqrt{a^2 - r^2} - r\right)r \:\mathrm{d}r\:\mathrm{d}\theta$$ But I am getting $0$ as answer, which is not the given answer. What am I doing wrong? Or maybe is the answer wrong? Thanks in advance!",,"['calculus', 'multivariable-calculus', 'polar-coordinates']"
23,With Stokes's Theorem - Calculate $\iint_S \operatorname{curl} \mathbf{F} \cdot\; d\mathbf{S}$ for $\mathbf{F} = yz^2\mathbf{i}$,With Stokes's Theorem - Calculate  for,\iint_S \operatorname{curl} \mathbf{F} \cdot\; d\mathbf{S} \mathbf{F} = yz^2\mathbf{i},"Consider the bounded surface S that is the union of $x^2 + y^2 = 4$ for $−2 \le z \le 2$   and $(4 − z)^2 = x^2 + y^2 $ for $2 \le z \le 4.$ Sketch the surface.   Use suitable parametrisations for the two parts of S to verify Stokes’s Theorem for    for $\mathbf{F} = (yz^2,0,0)$. Herein, I ask only about computing line integrals instead of $ \iint_S (\nabla × F )· d\mathbf{S}$, by virtue of Stokes's Theorem. Denote the $2 \le z \le 4$ cone P, and the $-2 \le z \le 2$ cylinder C. A boundary curve for $P$ is $x^2 + y^2 \le 4$ on the $z = -2$ plane. By virtue of the quotes on orientation here , we need the normal vector to be downward, in the direction of $(0, 0, -1).$ So the circle must be oriented clockwise when viewed from above $z = -2$ (as in my picture here ). Thus, parameterise with $\mathbf{r}(t) = ( 2\sin t, 2\cos t, -2 )$ for all $0 \le t \le 2\pi$. Then $\oint_{C_1} \mathbf{F} \cdot  d\mathbf{r} = \int^{2\pi}_{0} (8 \cos t, 0, 0,) \cdot ( 2\cos t, ♦ ,♦ ) \, dt = 16\pi$ ♦ denote objects that don't need to be computed because they're dot-producted with 0. $1.$ My answer differs by a negative sign from that without Stokes's Theorem . What's wrong?","Consider the bounded surface S that is the union of $x^2 + y^2 = 4$ for $−2 \le z \le 2$   and $(4 − z)^2 = x^2 + y^2 $ for $2 \le z \le 4.$ Sketch the surface.   Use suitable parametrisations for the two parts of S to verify Stokes’s Theorem for    for $\mathbf{F} = (yz^2,0,0)$. Herein, I ask only about computing line integrals instead of $ \iint_S (\nabla × F )· d\mathbf{S}$, by virtue of Stokes's Theorem. Denote the $2 \le z \le 4$ cone P, and the $-2 \le z \le 2$ cylinder C. A boundary curve for $P$ is $x^2 + y^2 \le 4$ on the $z = -2$ plane. By virtue of the quotes on orientation here , we need the normal vector to be downward, in the direction of $(0, 0, -1).$ So the circle must be oriented clockwise when viewed from above $z = -2$ (as in my picture here ). Thus, parameterise with $\mathbf{r}(t) = ( 2\sin t, 2\cos t, -2 )$ for all $0 \le t \le 2\pi$. Then $\oint_{C_1} \mathbf{F} \cdot  d\mathbf{r} = \int^{2\pi}_{0} (8 \cos t, 0, 0,) \cdot ( 2\cos t, ♦ ,♦ ) \, dt = 16\pi$ ♦ denote objects that don't need to be computed because they're dot-producted with 0. $1.$ My answer differs by a negative sign from that without Stokes's Theorem . What's wrong?",,['multivariable-calculus']
24,Applying the Implicit Function Theorem - how to evaluate the partial derivatives that arise?,Applying the Implicit Function Theorem - how to evaluate the partial derivatives that arise?,,"The problem: We have a function $f: \mathbb{R}^3 \rightarrow \mathbb{R}^2$ such that $f(x,y,z) = (x-xy, x+2y+z^2)$. For a point $(a,b,c)$ such that $f(a,b,c) = 0$, find a condition on $(a,b,c)$ such that there exists an open interval V containing $a$ and an open set W containing $(b,c)$ such that $g: V \rightarrow W$ exists and is differentiable, and $f(x,g(x)) = 0$ for all $x \in V$. My work: We need to use the implicit function theorem, so create the matrix of partial derivatives. The required condition is given when the determinant is non-zero. Work out the entries individually: For notational convenience, let $f^1 = x-xy, f^2 = x+2y+z^2$ $$a_{11} = \frac {\partial f^1}{\partial y}(a) = -x|_a = -a\\ a_{12} = \frac {\partial f^1}{\partial z}(a) = 0\\ a_{21} = \frac {\partial f^2}{\partial y}(a) = 2\\ a_{11} = \frac {\partial f^2}{\partial z}(a) = 2z|_a = ?$$ My question is how to evaluate this last equation. Do I just substitute $z=a$? It seems like I shouldn't. I did that for the first equation since $a$ is the first entry of the ordered triple $(a,b,c)$ in $\mathbb{R}^3$, but the value corresponding to $z$ would be $c$, so do I sub that in instead?","The problem: We have a function $f: \mathbb{R}^3 \rightarrow \mathbb{R}^2$ such that $f(x,y,z) = (x-xy, x+2y+z^2)$. For a point $(a,b,c)$ such that $f(a,b,c) = 0$, find a condition on $(a,b,c)$ such that there exists an open interval V containing $a$ and an open set W containing $(b,c)$ such that $g: V \rightarrow W$ exists and is differentiable, and $f(x,g(x)) = 0$ for all $x \in V$. My work: We need to use the implicit function theorem, so create the matrix of partial derivatives. The required condition is given when the determinant is non-zero. Work out the entries individually: For notational convenience, let $f^1 = x-xy, f^2 = x+2y+z^2$ $$a_{11} = \frac {\partial f^1}{\partial y}(a) = -x|_a = -a\\ a_{12} = \frac {\partial f^1}{\partial z}(a) = 0\\ a_{21} = \frac {\partial f^2}{\partial y}(a) = 2\\ a_{11} = \frac {\partial f^2}{\partial z}(a) = 2z|_a = ?$$ My question is how to evaluate this last equation. Do I just substitute $z=a$? It seems like I shouldn't. I did that for the first equation since $a$ is the first entry of the ordered triple $(a,b,c)$ in $\mathbb{R}^3$, but the value corresponding to $z$ would be $c$, so do I sub that in instead?",,"['multivariable-calculus', 'implicit-function-theorem']"
25,Integrating over a y-simple region $D$?,Integrating over a y-simple region ?,D,"Let $D=\{(x,y)\space|\space1\leq x^2+y^2 \leq 2 \text{ and }y\geq0\}$. Evaluate $\int\int_D(1+xy) dA$. So I stated that $D$ is a y-simple region because for all $(x,y)\in D$, $\sqrt{1-x^2} \leq y \leq \sqrt{2-x^2}$. My book states that for a y-simple region $D$, where $\phi_1(x)\leq y \leq \phi_2(x)$, the integral $\int\int_Df(x,y) dA=\int_a^b\int_{\phi_1(x)}^{\phi_2(x)}f(x,y)dydx$. I tried doing this with $$\int_{-\sqrt{2}}^{\sqrt{2}}\int_{\sqrt{1-x^2}}^{\sqrt{2-x^2}}(1+xy)dydx$$ but I am not getting the correct result. Could someone explain what I am doing wrong?","Let $D=\{(x,y)\space|\space1\leq x^2+y^2 \leq 2 \text{ and }y\geq0\}$. Evaluate $\int\int_D(1+xy) dA$. So I stated that $D$ is a y-simple region because for all $(x,y)\in D$, $\sqrt{1-x^2} \leq y \leq \sqrt{2-x^2}$. My book states that for a y-simple region $D$, where $\phi_1(x)\leq y \leq \phi_2(x)$, the integral $\int\int_Df(x,y) dA=\int_a^b\int_{\phi_1(x)}^{\phi_2(x)}f(x,y)dydx$. I tried doing this with $$\int_{-\sqrt{2}}^{\sqrt{2}}\int_{\sqrt{1-x^2}}^{\sqrt{2-x^2}}(1+xy)dydx$$ but I am not getting the correct result. Could someone explain what I am doing wrong?",,['multivariable-calculus']
26,Cauchy's Theorem and Cauchy's formula,Cauchy's Theorem and Cauchy's formula,,"I came across the following problem in our last midterm exam. I am completely stuck as to how to begin the solution: If $|f(z)|\leq$ max $|f(z+re^{it})|$ ($0\leq t\leq 2\pi$), then $|f|$ has no strict local maximum within its domain of analyticity. Currently I am familiar with Cauchy's Theorem and formula. I am not sure why even the first inequality is true? Can anyone lead to some hints/solutions?","I came across the following problem in our last midterm exam. I am completely stuck as to how to begin the solution: If $|f(z)|\leq$ max $|f(z+re^{it})|$ ($0\leq t\leq 2\pi$), then $|f|$ has no strict local maximum within its domain of analyticity. Currently I am familiar with Cauchy's Theorem and formula. I am not sure why even the first inequality is true? Can anyone lead to some hints/solutions?",,"['complex-analysis', 'multivariable-calculus', 'complex-numbers', 'cauchy-sequences']"
27,an iterated integral question,an iterated integral question,,This iterated integral is proving harder than I thought. Evaluate by reversing the order of integration:  $$ \int_{0}^{1}\left(\int_{y=x}^{\sqrt{x}}\frac{\sin y}{y}dy\right)dx $$,This iterated integral is proving harder than I thought. Evaluate by reversing the order of integration:  $$ \int_{0}^{1}\left(\int_{y=x}^{\sqrt{x}}\frac{\sin y}{y}dy\right)dx $$,,"['multivariable-calculus', 'vector-analysis']"
28,Triple integral in cylindrical coordinates question,Triple integral in cylindrical coordinates question,,Can someone explain the answer given split the integral into a cylinder and volume below a sphere? Thanks.,Can someone explain the answer given split the integral into a cylinder and volume below a sphere? Thanks.,,"['integration', 'multivariable-calculus']"
29,How do I find the Jacobi matrix?,How do I find the Jacobi matrix?,,"I've never done questions like these, so I would very much like some help. We are given a function $f: \mathbb R^n \to \mathbb R$ given by $f(x)=\langle x,\xi\rangle^2$ where $\langle\,,\rangle$ is the standard inner product of $\mathbb R^n$ and $\xi \in \mathbb R^n$. Find $D_f(a)$, meaning, the differential of $f$ in point $a$, or in other words, the jacobi matrix multiplied by vector $a$. Thanks, I would very much like an explanation on how to approach this","I've never done questions like these, so I would very much like some help. We are given a function $f: \mathbb R^n \to \mathbb R$ given by $f(x)=\langle x,\xi\rangle^2$ where $\langle\,,\rangle$ is the standard inner product of $\mathbb R^n$ and $\xi \in \mathbb R^n$. Find $D_f(a)$, meaning, the differential of $f$ in point $a$, or in other words, the jacobi matrix multiplied by vector $a$. Thanks, I would very much like an explanation on how to approach this",,"['calculus', 'multivariable-calculus', 'inner-products']"
30,Can any function be parametrised? [closed],Can any function be parametrised? [closed],,"Closed . This question needs details or clarity . It is not currently accepting answers. Want to improve this question? Add details and clarify the problem by editing this post . Closed 10 years ago . Improve this question I'm going over surface integrals for my Calc 2 exam in May and the questions rely heavily on parametrization. Can it be proven that any function $\mathbb{R^n}\rightarrow \mathbb{R^m}$ (more specifically focusing on the case $m=1$) can be written as a parametrization and changed into a function $\mathbb{R}\rightarrow\mathbb{R}$? Also any tips or hints on trying to find the parametrization would really help, on the more difficult questions I sometimes struggle to find a suitable one. Thanks","Closed . This question needs details or clarity . It is not currently accepting answers. Want to improve this question? Add details and clarify the problem by editing this post . Closed 10 years ago . Improve this question I'm going over surface integrals for my Calc 2 exam in May and the questions rely heavily on parametrization. Can it be proven that any function $\mathbb{R^n}\rightarrow \mathbb{R^m}$ (more specifically focusing on the case $m=1$) can be written as a parametrization and changed into a function $\mathbb{R}\rightarrow\mathbb{R}$? Also any tips or hints on trying to find the parametrization would really help, on the more difficult questions I sometimes struggle to find a suitable one. Thanks",,['multivariable-calculus']
31,"Looking for a specific function from $\mathbb R^2 \rightarrow \mathbb R$ ,something with directional derivatives","Looking for a specific function from  ,something with directional derivatives",\mathbb R^2 \rightarrow \mathbb R,"this function has to be continuous at the origin, have finite directional derivatives there,but they are not bounded. (meaning for some vectors v with |v|=1 the directional derivatives at 0 can be as large as we want) I first thought about $(x^2+y^2)^{1/3}$ but here the directional derivatives are infinite. Any ideas would be welcomed, thanks~","this function has to be continuous at the origin, have finite directional derivatives there,but they are not bounded. (meaning for some vectors v with |v|=1 the directional derivatives at 0 can be as large as we want) I first thought about $(x^2+y^2)^{1/3}$ but here the directional derivatives are infinite. Any ideas would be welcomed, thanks~",,['multivariable-calculus']
32,Differentiable function in n dimensions,Differentiable function in n dimensions,,If a function $f:\mathbb R^{n} \rightarrow \mathbb R^m $  is differentiable at a point $a$ can we say that there is a neighbourhood of $a$ such that $f$ is locally Lipschitz? (i.e.  there is some constant $M < \infty$ such that for all $x$ in that neighbourhood $||f(x)-f(a)|| < M ||x-a||$). I can easily prove this is true when the derivative is bounded but then if it is not bounded could I prove there is some small neighbourhood where the derivative is bounded?,If a function $f:\mathbb R^{n} \rightarrow \mathbb R^m $  is differentiable at a point $a$ can we say that there is a neighbourhood of $a$ such that $f$ is locally Lipschitz? (i.e.  there is some constant $M < \infty$ such that for all $x$ in that neighbourhood $||f(x)-f(a)|| < M ||x-a||$). I can easily prove this is true when the derivative is bounded but then if it is not bounded could I prove there is some small neighbourhood where the derivative is bounded?,,"['real-analysis', 'multivariable-calculus', 'derivatives', 'bounded-variation']"
33,Gradient of a function involving integrals,Gradient of a function involving integrals,,"Statement of the problem Let $F(x,y)=\iint_{R_{xy}} e^{(u-1)}e^{(v^2-v)}dudv$ where $R_{xy}$ is the region $[0,x]\times[0,y]$. Calculate $\nabla F(1,1)$ The attempt at a solution I know that $\nabla F(1,1)=(\dfrac{\partial F(1,1)}{\partial x},\dfrac{\partial F(1,1)}{\partial y})$. I am going to calculate the first partial derivative since the other one can be calculated in a similar way. $\dfrac{\partial F (x,y)}{\partial x}=\dfrac{\partial}{\partial x} \int_0^y\int_0^x e^{(u-1)}e^{(v^2-v)}dudv$. Now, I don't know if the following step is legitimate: $\dfrac{\partial}{\partial x} \int_0^y\int_0^x e^{(u-1)}e^{(v^2-v)}dudv=\int_0^y [\dfrac{\partial}{\partial x} \int_0^x e^{(u-1)}e^{(v^2-v)}du]dv$ If that last step was correct, I would like to know how to justify it. Now, by the fundamental theorem of calculus I know that $\dfrac{\partial}{\partial x} \int_0^x e^{(u-1)}e^{(v^2-v)}du=e^{(x-1)}e^{(v^2-v)}$ Using this I get $\dfrac{\partial F (x,y)}{\partial x}=\int_0^y e^{(x-1)}e^{(v^2-v)}dv=e^{(x-1)}\int_0^y e^{(v^2-v)}dv$ Here I got stuck, I've tried to calculate this integral but I couldn't. Have I done something wrong up to now and maybe that's why I am having trouble with this integral? I would appreciate some help.","Statement of the problem Let $F(x,y)=\iint_{R_{xy}} e^{(u-1)}e^{(v^2-v)}dudv$ where $R_{xy}$ is the region $[0,x]\times[0,y]$. Calculate $\nabla F(1,1)$ The attempt at a solution I know that $\nabla F(1,1)=(\dfrac{\partial F(1,1)}{\partial x},\dfrac{\partial F(1,1)}{\partial y})$. I am going to calculate the first partial derivative since the other one can be calculated in a similar way. $\dfrac{\partial F (x,y)}{\partial x}=\dfrac{\partial}{\partial x} \int_0^y\int_0^x e^{(u-1)}e^{(v^2-v)}dudv$. Now, I don't know if the following step is legitimate: $\dfrac{\partial}{\partial x} \int_0^y\int_0^x e^{(u-1)}e^{(v^2-v)}dudv=\int_0^y [\dfrac{\partial}{\partial x} \int_0^x e^{(u-1)}e^{(v^2-v)}du]dv$ If that last step was correct, I would like to know how to justify it. Now, by the fundamental theorem of calculus I know that $\dfrac{\partial}{\partial x} \int_0^x e^{(u-1)}e^{(v^2-v)}du=e^{(x-1)}e^{(v^2-v)}$ Using this I get $\dfrac{\partial F (x,y)}{\partial x}=\int_0^y e^{(x-1)}e^{(v^2-v)}dv=e^{(x-1)}\int_0^y e^{(v^2-v)}dv$ Here I got stuck, I've tried to calculate this integral but I couldn't. Have I done something wrong up to now and maybe that's why I am having trouble with this integral? I would appreciate some help.",,"['integration', 'multivariable-calculus']"
34,evaluating integral over surface of the sphere,evaluating integral over surface of the sphere,,"This is what I've tried. Am I approaching this the right way? Also, where do I go next?","This is what I've tried. Am I approaching this the right way? Also, where do I go next?",,"['calculus', 'integration', 'multivariable-calculus']"
35,Please check whether my solution is correct? Directional derivative?,Please check whether my solution is correct? Directional derivative?,,"Question says.... Let $f:\mathbb R^2\rightarrow \mathbb R$ be differentiable. At $(1,2)$ $f$ has directional derivative $2$ in the direction towards $(2,2)$ and $-2$ in the direction towards $(1,-1)$. find grad $f$ at $(1,2)$. directional derivative towards $(4,6)$. Solution: direction vector from $(1,2)$ to $(2,2)$ is $(1,0)$.           direction vector from $(1,2)$ to $(1,-1)$ is $(0,-3)$. direction vector from $(1,2)$ to $(4,6)$ is $(3,4)$.          $ u=(3/5 , 4/5)$. D.D $f(1,2)=\mathrm{grad}\, f(1,2)u=(1/5)[\mathrm{grad}\, f(1,2)(3,4)]                        =(1/5)[3 \mathrm{grad}\, f(1,2)(1,0) -4/3 \mathrm{grad}\, f(1,2)(0,-3)]=(1/5)[3*2-4/3*-2]=52/5$. Is it correct.","Question says.... Let $f:\mathbb R^2\rightarrow \mathbb R$ be differentiable. At $(1,2)$ $f$ has directional derivative $2$ in the direction towards $(2,2)$ and $-2$ in the direction towards $(1,-1)$. find grad $f$ at $(1,2)$. directional derivative towards $(4,6)$. Solution: direction vector from $(1,2)$ to $(2,2)$ is $(1,0)$.           direction vector from $(1,2)$ to $(1,-1)$ is $(0,-3)$. direction vector from $(1,2)$ to $(4,6)$ is $(3,4)$.          $ u=(3/5 , 4/5)$. D.D $f(1,2)=\mathrm{grad}\, f(1,2)u=(1/5)[\mathrm{grad}\, f(1,2)(3,4)]                        =(1/5)[3 \mathrm{grad}\, f(1,2)(1,0) -4/3 \mathrm{grad}\, f(1,2)(0,-3)]=(1/5)[3*2-4/3*-2]=52/5$. Is it correct.",,"['calculus', 'multivariable-calculus', 'derivatives', 'partial-derivative']"
36,Finding transformed region by change of variables,Finding transformed region by change of variables,,"I have the equation of a curve $x^{2/3} + y^{2/3} = a^{2/3}$ and I'm using the change of variables $x = u\cos^3v $, $y = u\sin^3v$ . I have calculated the Jacobian $\frac{\partial(x,y)}{\partial(u,v)}$, but how do I use all this stuff to find the area of the region bounded by the curve and the positive x- and y-axes? I'm having particular difficulty figuring out the new limits for the double integral wrt u and v. Thanks","I have the equation of a curve $x^{2/3} + y^{2/3} = a^{2/3}$ and I'm using the change of variables $x = u\cos^3v $, $y = u\sin^3v$ . I have calculated the Jacobian $\frac{\partial(x,y)}{\partial(u,v)}$, but how do I use all this stuff to find the area of the region bounded by the curve and the positive x- and y-axes? I'm having particular difficulty figuring out the new limits for the double integral wrt u and v. Thanks",,"['integration', 'multivariable-calculus', 'partial-derivative']"
37,"Studying the differential form $w(x,y)=xy^ndx+x^mydy$",Studying the differential form,"w(x,y)=xy^ndx+x^mydy","Let $w$ be a differential form defined by $w(x,y)=xy^ndx+x^mydy$ where $m$ and $n$ are non negative integers. 1) For which values of  $m$ and $n$ the differential form $w$ is closed? 2) For these values, is $w$ exact ? if yes then determine all of its primitives? My try: 1) $w$ is defined for all couples $(x,y)$ such that $x>0$ and $y>0$. Moreover if $w$ is closed then necessarely $\dfrac{\partial (xy^n)}{\partial y}=  \dfrac{\partial (x^my)}{\partial x}$ which means $nxy^{n-1}=mx^{m-1}y$ for all $x>0$ and $y>0$. In particular this identity must hold for $x=y=1$, in which case we must have $n=m$. Now this criterion gives that $nxy^{n-1}=nx^{n-1}y$ for all $x>0$ and $y>0$. If $n=m=0$, $w(x,y)=xdx+ydy$ is clearly closed. If $n=m\not = 0$, we have that $xy^{n-1}=x^{n-1}y$ for all $x>0$ and $y>0$, hence $ x^{n-2}=y^{n-2}$ for all $x>0$ and $y>0$, which can not be true. Hence the only value is $n=0$. 2)Since $w$ is defined on the subset $U$ of the plane consisting of couples $(x,y)$ such that $x>0$ and $y>0$ it is clear that $U$ is convex hence star convex and by Poincaré theorem, for $n=0$, $w$ is exact. to find the primitives $f$ of $w$ we solve the equation  $\dfrac{\partial f}{\partial x}=xy^0=x$ which gives that $f(x,y)=x^2/2+c_1(y)$ and the equation $\dfrac{\partial f}{\partial y}=x^0y=y$ which gives that $f(x,y)=y^2/2+c_2(x)$ Hence $f(x,y)=x^2/2+y^2/2+constant$. Is my try correct? thank you for your help!","Let $w$ be a differential form defined by $w(x,y)=xy^ndx+x^mydy$ where $m$ and $n$ are non negative integers. 1) For which values of  $m$ and $n$ the differential form $w$ is closed? 2) For these values, is $w$ exact ? if yes then determine all of its primitives? My try: 1) $w$ is defined for all couples $(x,y)$ such that $x>0$ and $y>0$. Moreover if $w$ is closed then necessarely $\dfrac{\partial (xy^n)}{\partial y}=  \dfrac{\partial (x^my)}{\partial x}$ which means $nxy^{n-1}=mx^{m-1}y$ for all $x>0$ and $y>0$. In particular this identity must hold for $x=y=1$, in which case we must have $n=m$. Now this criterion gives that $nxy^{n-1}=nx^{n-1}y$ for all $x>0$ and $y>0$. If $n=m=0$, $w(x,y)=xdx+ydy$ is clearly closed. If $n=m\not = 0$, we have that $xy^{n-1}=x^{n-1}y$ for all $x>0$ and $y>0$, hence $ x^{n-2}=y^{n-2}$ for all $x>0$ and $y>0$, which can not be true. Hence the only value is $n=0$. 2)Since $w$ is defined on the subset $U$ of the plane consisting of couples $(x,y)$ such that $x>0$ and $y>0$ it is clear that $U$ is convex hence star convex and by Poincaré theorem, for $n=0$, $w$ is exact. to find the primitives $f$ of $w$ we solve the equation  $\dfrac{\partial f}{\partial x}=xy^0=x$ which gives that $f(x,y)=x^2/2+c_1(y)$ and the equation $\dfrac{\partial f}{\partial y}=x^0y=y$ which gives that $f(x,y)=y^2/2+c_2(x)$ Hence $f(x,y)=x^2/2+y^2/2+constant$. Is my try correct? thank you for your help!",,['multivariable-calculus']
38,Derivative of $(Ax - b)^T(Ax-b)$,Derivative of,(Ax - b)^T(Ax-b),"I am trying to take the derivative of $(Ax - b)^T(Ax-b)$ and setting it to zero without expanding the multiplication, by only using matrix calculus. I knew the partial derivative of $x^Tx$ according to $x$ is $2x$, derivative of $Ax - b$ is $A^T$ and by utilizing the chain rule, $(f o g)' = (f'(g))(g')$ I obtained, $$ 2(Ax-b)A^T = 0 $$ or $$ (2Ax-2b)A^T = 0 $$ However the dimensions are not lining up here, if I had $$ 2A^T(Ax-b) = 0 $$ it would be fine. How can I get this result? Is there a rule I skipped that allows the statement above to be the derivative? Note: From this link I found out $$ \frac{\partial}{\partial t} f(g(t)) = \nabla f(g(t))^T \frac{\partial g}{\partial t} $$ If I had $g(x) = Ax-b$ and $f(z) = z^Tz$ then I could have $$ \frac{\partial}{\partial x} f(g(x)) =  2z^TA = 2(Ax-b)^TA = 0 $$ Take transpose of both sides $$ 2A^T(Ax-b) = 0 $$ So I guess my original chain rule was wrong. Does this look correct? If yes, does anyone know where I can find the derivation of the correct Chain Rule?","I am trying to take the derivative of $(Ax - b)^T(Ax-b)$ and setting it to zero without expanding the multiplication, by only using matrix calculus. I knew the partial derivative of $x^Tx$ according to $x$ is $2x$, derivative of $Ax - b$ is $A^T$ and by utilizing the chain rule, $(f o g)' = (f'(g))(g')$ I obtained, $$ 2(Ax-b)A^T = 0 $$ or $$ (2Ax-2b)A^T = 0 $$ However the dimensions are not lining up here, if I had $$ 2A^T(Ax-b) = 0 $$ it would be fine. How can I get this result? Is there a rule I skipped that allows the statement above to be the derivative? Note: From this link I found out $$ \frac{\partial}{\partial t} f(g(t)) = \nabla f(g(t))^T \frac{\partial g}{\partial t} $$ If I had $g(x) = Ax-b$ and $f(z) = z^Tz$ then I could have $$ \frac{\partial}{\partial x} f(g(x)) =  2z^TA = 2(Ax-b)^TA = 0 $$ Take transpose of both sides $$ 2A^T(Ax-b) = 0 $$ So I guess my original chain rule was wrong. Does this look correct? If yes, does anyone know where I can find the derivation of the correct Chain Rule?",,"['linear-algebra', 'multivariable-calculus', 'matrix-calculus']"
39,Implicit Function Theorem [Understanding theorem in book],Implicit Function Theorem [Understanding theorem in book],,"""Let $\mathbb{F}$ be a real-valued continuously differentiable function defeined in a neighborhood of $(X_0, Y_0) \in \mathbb{R}^2$. Suppose that $\mathbb{F}$ satisfies the two conditions: $\mathbb{F}(X_0,Y_0) = Z_0$ and $\frac{d \mathbb{F}}{d Y}(X_0,Y_0) \neq 0$. Then there exist open intervals $U$ and $V$, with $X_0 \in U$ and $Y_0 \in V$, and a unique function $F : U \to V$ satisfying $\mathbb{F}[X, F(x)] = Z_0$ for all $X \in U$, and this function $F$ is continuously differentiable with: $\frac{dY}{dX}(Y_0) = F'(Y_0) = -\left [ \frac{\frac{d\mathbb{F}}{d X}(X_0, Y_0)}{\frac{d\mathbb{F}}{dY}(X_0, Y_0)} \right]$  "". Is this the implicit function theorem? I'm having trouble understanding what this is. Does the unique function $F : U \to V$ mean the $g(x)$ that is usually referred to when talking about the IFT? I don't understand the very last part of the theorem.","""Let $\mathbb{F}$ be a real-valued continuously differentiable function defeined in a neighborhood of $(X_0, Y_0) \in \mathbb{R}^2$. Suppose that $\mathbb{F}$ satisfies the two conditions: $\mathbb{F}(X_0,Y_0) = Z_0$ and $\frac{d \mathbb{F}}{d Y}(X_0,Y_0) \neq 0$. Then there exist open intervals $U$ and $V$, with $X_0 \in U$ and $Y_0 \in V$, and a unique function $F : U \to V$ satisfying $\mathbb{F}[X, F(x)] = Z_0$ for all $X \in U$, and this function $F$ is continuously differentiable with: $\frac{dY}{dX}(Y_0) = F'(Y_0) = -\left [ \frac{\frac{d\mathbb{F}}{d X}(X_0, Y_0)}{\frac{d\mathbb{F}}{dY}(X_0, Y_0)} \right]$  "". Is this the implicit function theorem? I'm having trouble understanding what this is. Does the unique function $F : U \to V$ mean the $g(x)$ that is usually referred to when talking about the IFT? I don't understand the very last part of the theorem.",,"['calculus', 'multivariable-calculus']"
40,Surface area of intersection of two cylinders,Surface area of intersection of two cylinders,,"Let $$R=\{(x,y,z):y^2+z^2\leq 1\,\, \text{and}\,\, x^2+z^2\leq 1\}.$$ Compute the volume of $R$. Compute the area of its boundary $\partial R$. I'm fine with #1. For #2, I have a solution here , which I'm not sure is correct (but I trust the error is with me). I'm wondering where is the error in my method. Consider a cylinder moving left to right along the $y$-axis. Cut it in half along the $yz$-plane, and parameterize it by $y,z$. Let $$D = \{(y,z) \mid -1\leq z\leq 1\}.$$ This infinitely long strip is our domain of parameterization, and the (back half of the) cylinder is parameterized  $$C(y,z)= (\sqrt{1-z^2}, y,z).$$ Now to find the surface area across a bounded region $\Delta \subset D$, we would integrate $$\iint_\Delta |n(y,z)|dydz.$$ I get $|n(y,z)|=\frac{1}{\sqrt{1-z^2}}.$ Now to find the surface area (of the back half of intersection between the two cylinders) we can just integrate $$\iint_\Delta \frac{1}{\sqrt{1-z^2}}dydz,$$ where $\Delta$ is the unit disk in the $yz$-plane. I calculate this to be $4$. But my solution is off by a factor of 2 (according to the answer above, the total integral should be $16$, meaning the integral of the back half should be $8$). What have I failed to consider? (Or perhaps the solution I linked to is off by a factor of 2?)","Let $$R=\{(x,y,z):y^2+z^2\leq 1\,\, \text{and}\,\, x^2+z^2\leq 1\}.$$ Compute the volume of $R$. Compute the area of its boundary $\partial R$. I'm fine with #1. For #2, I have a solution here , which I'm not sure is correct (but I trust the error is with me). I'm wondering where is the error in my method. Consider a cylinder moving left to right along the $y$-axis. Cut it in half along the $yz$-plane, and parameterize it by $y,z$. Let $$D = \{(y,z) \mid -1\leq z\leq 1\}.$$ This infinitely long strip is our domain of parameterization, and the (back half of the) cylinder is parameterized  $$C(y,z)= (\sqrt{1-z^2}, y,z).$$ Now to find the surface area across a bounded region $\Delta \subset D$, we would integrate $$\iint_\Delta |n(y,z)|dydz.$$ I get $|n(y,z)|=\frac{1}{\sqrt{1-z^2}}.$ Now to find the surface area (of the back half of intersection between the two cylinders) we can just integrate $$\iint_\Delta \frac{1}{\sqrt{1-z^2}}dydz,$$ where $\Delta$ is the unit disk in the $yz$-plane. I calculate this to be $4$. But my solution is off by a factor of 2 (according to the answer above, the total integral should be $16$, meaning the integral of the back half should be $8$). What have I failed to consider? (Or perhaps the solution I linked to is off by a factor of 2?)",,"['integration', 'geometry', 'multivariable-calculus', 'surfaces']"
41,Writing triple integrals in spherical coordinates over nonspherical/nonconical regions,Writing triple integrals in spherical coordinates over nonspherical/nonconical regions,,"Defining upper and lower limits of integration for $\rho$ , $\theta$ , and $\phi$ is relatively easy when writing a triple integral in spherical coordinates if the region of integration is defined by spheres and/or cones centered at the origin. However, this becomes more involved when the region is bounded by any other type of surface, such as a cylinder or a paraboloid. The following is an example I came across recently, and I can't figure out how to represent the upper and lower bounds for $\rho$ : Here's what I've come up with so far: $\iiint\limits_E\mathrm{d}V=\int_{\arcsin(\frac{1}{\sqrt{6}})}^{\pi}\int_0^{2\pi}\int_0^\sqrt{6} \rho^2 \sin(\phi)\,\mathrm{d}\rho\,\mathrm{d}\theta\,\mathrm{d}\phi + \int_{\pi}^{\arcsin(\frac{1}{\sqrt{6}})}\int_0^{2\pi}\int_0^{\textbf{???}} \rho^2 \sin(\phi)\,\mathrm{d}\rho\,\mathrm{d}\theta\,\mathrm{d}\phi$ How does one going about finding the equation for $\rho$ on the segment of the region of integration where it meets the sides of the cylinder?","Defining upper and lower limits of integration for , , and is relatively easy when writing a triple integral in spherical coordinates if the region of integration is defined by spheres and/or cones centered at the origin. However, this becomes more involved when the region is bounded by any other type of surface, such as a cylinder or a paraboloid. The following is an example I came across recently, and I can't figure out how to represent the upper and lower bounds for : Here's what I've come up with so far: How does one going about finding the equation for on the segment of the region of integration where it meets the sides of the cylinder?","\rho \theta \phi \rho \iiint\limits_E\mathrm{d}V=\int_{\arcsin(\frac{1}{\sqrt{6}})}^{\pi}\int_0^{2\pi}\int_0^\sqrt{6} \rho^2 \sin(\phi)\,\mathrm{d}\rho\,\mathrm{d}\theta\,\mathrm{d}\phi + \int_{\pi}^{\arcsin(\frac{1}{\sqrt{6}})}\int_0^{2\pi}\int_0^{\textbf{???}} \rho^2 \sin(\phi)\,\mathrm{d}\rho\,\mathrm{d}\theta\,\mathrm{d}\phi \rho","['multivariable-calculus', 'definite-integrals', 'spherical-coordinates']"
42,Multivariable Calculus: Volume,Multivariable Calculus: Volume,,"Trying to figure out the following problem: Evaluate the integral $\int\int\int_EzdV$, where E lies above the paraboloid $z = x^2+y^2$ and below the plane $z=6y$. Round the result to the nearest hundredth. Thus far, I have the following: $\int\int\int_{x^2+y^2}^{6y}zdzdA$ I was thinking of putting the problem in terms of cylindrical coordinates, such that I have: $\int\int\int_{r^2}^{6r\sin{\theta}}zrdzdrd\theta$ Just not sure about the limits for r and $\theta$. When I graph the function, I more a less get an idea. $\theta$ would be from 0 to $\pi$ but I am not sure about r. I would assume you equal $x^2+y^2=6y$ and solve for something. But I am not sure. That being said and I put it in cylindrical coordinates. I get $r^2=6r\sin{\theta}$. Solving for $r$ I get, $r$ = $6\sin{\theta}$. Not sure if this is going about it the right way though. Thanks in advanced.","Trying to figure out the following problem: Evaluate the integral $\int\int\int_EzdV$, where E lies above the paraboloid $z = x^2+y^2$ and below the plane $z=6y$. Round the result to the nearest hundredth. Thus far, I have the following: $\int\int\int_{x^2+y^2}^{6y}zdzdA$ I was thinking of putting the problem in terms of cylindrical coordinates, such that I have: $\int\int\int_{r^2}^{6r\sin{\theta}}zrdzdrd\theta$ Just not sure about the limits for r and $\theta$. When I graph the function, I more a less get an idea. $\theta$ would be from 0 to $\pi$ but I am not sure about r. I would assume you equal $x^2+y^2=6y$ and solve for something. But I am not sure. That being said and I put it in cylindrical coordinates. I get $r^2=6r\sin{\theta}$. Solving for $r$ I get, $r$ = $6\sin{\theta}$. Not sure if this is going about it the right way though. Thanks in advanced.",,"['integration', 'multivariable-calculus', 'conic-sections']"
43,Finding $g_i:\mathbb{R}^n\to\mathbb R$ s.t $f(x)=\sum\limits_{i=1}^nx_i\cdot g_i(x)$,Finding  s.t,g_i:\mathbb{R}^n\to\mathbb R f(x)=\sum\limits_{i=1}^nx_i\cdot g_i(x),"Let $f:\mathbb R^n\to\mathbb R$ differntiable and $f(0)=0$. Prove exist $g_i$ s.t for $x=(x_1,\dots,x_n):f(x)=\sum\limits_{i=1}^nx_i\cdot g_i(x)$. hint:$f(x)=\int\limits_0^1f\prime(tx)dt$. I dont have any idea how to solve it. maybe we can build $g_i=f(x_i\cdot x)$ but im not sure it may help since I cannot take one time $\int f(x_1\cdot x)dx_1$ and in another $\int f(x_n\cdot x)dx_n$). how can I build the sequence of functions and furthermore, what is the intuition for this type of questions involves building functions $\varphi:\mathbb R^n\to\mathbb R^m$ based on given condition with sums or diffrentiablity or partial derivatives?","Let $f:\mathbb R^n\to\mathbb R$ differntiable and $f(0)=0$. Prove exist $g_i$ s.t for $x=(x_1,\dots,x_n):f(x)=\sum\limits_{i=1}^nx_i\cdot g_i(x)$. hint:$f(x)=\int\limits_0^1f\prime(tx)dt$. I dont have any idea how to solve it. maybe we can build $g_i=f(x_i\cdot x)$ but im not sure it may help since I cannot take one time $\int f(x_1\cdot x)dx_1$ and in another $\int f(x_n\cdot x)dx_n$). how can I build the sequence of functions and furthermore, what is the intuition for this type of questions involves building functions $\varphi:\mathbb R^n\to\mathbb R^m$ based on given condition with sums or diffrentiablity or partial derivatives?",,"['calculus', 'real-analysis']"
44,Dirac Delta identity,Dirac Delta identity,,"Reading through the proof on the Helmholtz decomposition of a vector field, I came across the following identity: $$\delta(x-x')=-1/4\pi*\nabla^2*(|x-x'|)^{-1}$$ Does anyone have any insight on how to prove/derive this identity? Thanks in advance. Here's the article for reference: http://faculty.uml.edu/cbaird/95.657(2013)/Helmholtz_Decomposition.pdf","Reading through the proof on the Helmholtz decomposition of a vector field, I came across the following identity: $$\delta(x-x')=-1/4\pi*\nabla^2*(|x-x'|)^{-1}$$ Does anyone have any insight on how to prove/derive this identity? Thanks in advance. Here's the article for reference: http://faculty.uml.edu/cbaird/95.657(2013)/Helmholtz_Decomposition.pdf",,['multivariable-calculus']
45,Show that the function $f = \frac{xy}{x^2 + y^2}$ is continuous along every horizontal and every vertical line,Show that the function  is continuous along every horizontal and every vertical line,f = \frac{xy}{x^2 + y^2},"Consider the function $ f:\mathbb{R^2} \rightarrow \mathbb{R}$ given by $ f(x,y) = \left\{   \begin{array}{l l}     \frac{xy}{x^2 + y^2} & \quad \text{if (x,y) $\neq$ (0,0)}\\     0 & \quad \text{if (x,y) = (0,0)}   \end{array} \right.$ Show that f is continuous along every horizontal and every vertical line (i.e. for every $x_0, y_0 \epsilon \mathbb{R}$, the functions $g,h:\mathbb{R}\rightarrow \mathbb{R}$ given by $g(t) = f(x_0,t)$ and $h(t)=f(t,y_0)$ are continuous). I know that this function is not continuous at $(0,0)$, but the horizontal/vertical line issue is proving difficult to work with. Any help/hints appreciated!","Consider the function $ f:\mathbb{R^2} \rightarrow \mathbb{R}$ given by $ f(x,y) = \left\{   \begin{array}{l l}     \frac{xy}{x^2 + y^2} & \quad \text{if (x,y) $\neq$ (0,0)}\\     0 & \quad \text{if (x,y) = (0,0)}   \end{array} \right.$ Show that f is continuous along every horizontal and every vertical line (i.e. for every $x_0, y_0 \epsilon \mathbb{R}$, the functions $g,h:\mathbb{R}\rightarrow \mathbb{R}$ given by $g(t) = f(x_0,t)$ and $h(t)=f(t,y_0)$ are continuous). I know that this function is not continuous at $(0,0)$, but the horizontal/vertical line issue is proving difficult to work with. Any help/hints appreciated!",,"['real-analysis', 'multivariable-calculus', 'continuity']"
46,Line integrals of given curves,Line integrals of given curves,,"This question has an integral $$\int(x^4+4xy^3)dx+(6x^2y^2-5y^4)dy$$to be evaluated on the parametric curve $$C:(-(t+2)\cos(\pi t^2), t-1)$$I took the partial derivatives of the terms in the bracket and subtracted them to get $0$. However, this is not the right answer. I don't know any other method to solve such integrals.","This question has an integral $$\int(x^4+4xy^3)dx+(6x^2y^2-5y^4)dy$$to be evaluated on the parametric curve $$C:(-(t+2)\cos(\pi t^2), t-1)$$I took the partial derivatives of the terms in the bracket and subtracted them to get $0$. However, this is not the right answer. I don't know any other method to solve such integrals.",,['multivariable-calculus']
47,Partial derivative paradox,Partial derivative paradox,,"Okay, perhaps not a paradox, but somewhat of a lack of understanding on my part. Let $z$ equal some function of $x$ and $y$, i.e. $z = f(x, y)$ and take partial derivatives $\frac{\partial z}{\partial x} = f_x$ and $\frac{\partial z}{\partial y} = f_y$ all and good. But now say I do partial differentiation with respect to z. $\frac{\partial z}{\partial z} = f_z$ which equals $0$ because $f(x, y)$ is a function of $x$ and $y$, but not $z$, so all $x$ and $y$ are held constant and the derivative of a constant is zero. But that's not the right answer is it? $\frac{\partial z}{\partial z}$ should be equal to 1. Where did my logic go wrong?","Okay, perhaps not a paradox, but somewhat of a lack of understanding on my part. Let $z$ equal some function of $x$ and $y$, i.e. $z = f(x, y)$ and take partial derivatives $\frac{\partial z}{\partial x} = f_x$ and $\frac{\partial z}{\partial y} = f_y$ all and good. But now say I do partial differentiation with respect to z. $\frac{\partial z}{\partial z} = f_z$ which equals $0$ because $f(x, y)$ is a function of $x$ and $y$, but not $z$, so all $x$ and $y$ are held constant and the derivative of a constant is zero. But that's not the right answer is it? $\frac{\partial z}{\partial z}$ should be equal to 1. Where did my logic go wrong?",,['multivariable-calculus']
48,"Integral of $f(x,y,z)=x+2y+z$ over a tetrahedron.",Integral of  over a tetrahedron.,"f(x,y,z)=x+2y+z","Let $S$ be the tetrahedron in $\mathbb{R}^{3}$ having vertices $(0,0,0),(1,2,3),  (0,1,2)$ and  $(-1,1,1)$. Evaluate $\int_{S} f$, where $f(x,y,z)=x+2y-z$.","Let $S$ be the tetrahedron in $\mathbb{R}^{3}$ having vertices $(0,0,0),(1,2,3),  (0,1,2)$ and  $(-1,1,1)$. Evaluate $\int_{S} f$, where $f(x,y,z)=x+2y-z$.",,[]
49,Equivalence of integral and differential forms of transport equation,Equivalence of integral and differential forms of transport equation,,"I would like to prove the equivalence of the differential and integral forms of Reynold's transport equation. This problem is stated in terms of fluid mechanics. Problem Statement Let $V$ be a closed volume in $\mathbb{R}^3$,  $A$ be the surface of $V$,  and $\mathbf{n}$ the normal to A ($\mathbf{n}$ is thus a function of position). Let $F(\mathbf{x}, t)$ be a scalar function of three-dimensional position and time. Also let $\mathbf{u} \in \mathbb{R}^3$ be the field velocity in $\mathbb{R}^3$. By letting $V \to 0$, derive this equality: $$ \frac{DF}{Dt}= \frac{\partial F}{\partial t} + \frac{\partial F}{\partial x_i} u_i $$ from this one: $$ \frac{D}{Dt} \int_V F \ dV= \int_V \frac{\partial F}{\partial t}\ dV + \int_A  (\mathbf{u} \cdot \mathbf{n})\ F \ dA, $$ where $D/Dt$ denotes a full derivative and double indices denote summation. Attempt at a Solution We first convert the integral over $A$ to one over $V$ by way of the divergence theorem: $$ \int_A  (\mathbf{u} \cdot \mathbf{n})\ F \ dA = \int_A (F \mathbf{u}) \cdot \mathbf{n} \ dA  = \int_V \nabla \cdot (F\mathbf{u}) \ dV. $$ Expand the divergence:  $$ \int_V \nabla \cdot (F\mathbf{u}) \ dV =  \int_V \nabla F \cdot \mathbf{u} \ dV + \int_V (\nabla \cdot \mathbf{u})F \ dV. $$ Call this result (1). Although I do not understand why it is true, the text frequently uses equalities of the form $$ \lim_{V \to 0} \ \int_V F(\mathbf{x}, t) \ dV = F(\mathbf{x}, t) $$ and so I intend to do so as well here (aside: I guess it's implicit that $V$ on the LHS shrinks to the point $\mathbf{x}$ on the RHS?). Substituting result (1) into the integral statement and (admittedly blindly) applying the preceding rule almost gives me what I want: $$ \frac{DF}{Dt} = \frac{\partial F}{\partial t} + \frac{\partial F}{\partial x_i} u_i +  \lim_{V \to 0} \int_V (\nabla \cdot \mathbf{u})F \ dV. $$ The remaining limit is the result of the expansion of the gradient in result (1). Reiteration of the Question Is the limit in last equation equal to 0, and if so, why, or have I made a mistake?","I would like to prove the equivalence of the differential and integral forms of Reynold's transport equation. This problem is stated in terms of fluid mechanics. Problem Statement Let $V$ be a closed volume in $\mathbb{R}^3$,  $A$ be the surface of $V$,  and $\mathbf{n}$ the normal to A ($\mathbf{n}$ is thus a function of position). Let $F(\mathbf{x}, t)$ be a scalar function of three-dimensional position and time. Also let $\mathbf{u} \in \mathbb{R}^3$ be the field velocity in $\mathbb{R}^3$. By letting $V \to 0$, derive this equality: $$ \frac{DF}{Dt}= \frac{\partial F}{\partial t} + \frac{\partial F}{\partial x_i} u_i $$ from this one: $$ \frac{D}{Dt} \int_V F \ dV= \int_V \frac{\partial F}{\partial t}\ dV + \int_A  (\mathbf{u} \cdot \mathbf{n})\ F \ dA, $$ where $D/Dt$ denotes a full derivative and double indices denote summation. Attempt at a Solution We first convert the integral over $A$ to one over $V$ by way of the divergence theorem: $$ \int_A  (\mathbf{u} \cdot \mathbf{n})\ F \ dA = \int_A (F \mathbf{u}) \cdot \mathbf{n} \ dA  = \int_V \nabla \cdot (F\mathbf{u}) \ dV. $$ Expand the divergence:  $$ \int_V \nabla \cdot (F\mathbf{u}) \ dV =  \int_V \nabla F \cdot \mathbf{u} \ dV + \int_V (\nabla \cdot \mathbf{u})F \ dV. $$ Call this result (1). Although I do not understand why it is true, the text frequently uses equalities of the form $$ \lim_{V \to 0} \ \int_V F(\mathbf{x}, t) \ dV = F(\mathbf{x}, t) $$ and so I intend to do so as well here (aside: I guess it's implicit that $V$ on the LHS shrinks to the point $\mathbf{x}$ on the RHS?). Substituting result (1) into the integral statement and (admittedly blindly) applying the preceding rule almost gives me what I want: $$ \frac{DF}{Dt} = \frac{\partial F}{\partial t} + \frac{\partial F}{\partial x_i} u_i +  \lim_{V \to 0} \int_V (\nabla \cdot \mathbf{u})F \ dV. $$ The remaining limit is the result of the expansion of the gradient in result (1). Reiteration of the Question Is the limit in last equation equal to 0, and if so, why, or have I made a mistake?",,"['multivariable-calculus', 'fluid-dynamics']"
50,The multivariable chain rule and functions that depend on themselves,The multivariable chain rule and functions that depend on themselves,,"I am attempting to show that if $z = z(x,y)$, then $$\frac{\partial z}{\partial x} = -\frac{\partial z}{\partial y}\frac{\partial y}{\partial x}.\qquad\qquad(1)$$  This negative sign does not strike me as intuitive, given the close resemblance this formula has to the single-variable chain rule. From the multivariable chain rule, $$\frac{\partial z(x,y(x,z))}{\partial x} = \frac{\partial z(x,y)}{\partial x} + \frac{\partial z(x,y)}{\partial y}\frac{\partial y(x,z)}{\partial x}\qquad\qquad(2)$$ which is equivalent to $(1)$ if $$\frac{\partial z(x,y(x,z))}{\partial x} = 0,\qquad\qquad(3)$$ but how can $(3)$ be sensibly justified?  How can one show that $z(x,y(x,z))$ only depends on $z$, and is not such reasoning rather circular?","I am attempting to show that if $z = z(x,y)$, then $$\frac{\partial z}{\partial x} = -\frac{\partial z}{\partial y}\frac{\partial y}{\partial x}.\qquad\qquad(1)$$  This negative sign does not strike me as intuitive, given the close resemblance this formula has to the single-variable chain rule. From the multivariable chain rule, $$\frac{\partial z(x,y(x,z))}{\partial x} = \frac{\partial z(x,y)}{\partial x} + \frac{\partial z(x,y)}{\partial y}\frac{\partial y(x,z)}{\partial x}\qquad\qquad(2)$$ which is equivalent to $(1)$ if $$\frac{\partial z(x,y(x,z))}{\partial x} = 0,\qquad\qquad(3)$$ but how can $(3)$ be sensibly justified?  How can one show that $z(x,y(x,z))$ only depends on $z$, and is not such reasoning rather circular?",,"['multivariable-calculus', 'partial-derivative']"
51,Are there cases where the difference between a partial and total derivative is purely reliant on syntax?,Are there cases where the difference between a partial and total derivative is purely reliant on syntax?,,"It's always seemed to me that the difference between a partial and total derivative is artificial. I'm not sure how I'm wrong but surely, I must be. An example of my confusion is the function f(x,t)=x^2+t where x(t)=t. If I take the partial derivative of f w.r.t t, $\frac{\partial f}{\partial t}$=1. Now suppose I take the partial again, but this time I substitute first. f(x(t),t)=t^2+t. This time $\frac{\partial f}{\partial t}$=2t+1. This confuses me because, surely the result of a partial derivative relies only on the function f, and can't be dependent on my choice of how to write it. Unless the partial derivative is really such a whimsical, subjective operator.","It's always seemed to me that the difference between a partial and total derivative is artificial. I'm not sure how I'm wrong but surely, I must be. An example of my confusion is the function f(x,t)=x^2+t where x(t)=t. If I take the partial derivative of f w.r.t t, $\frac{\partial f}{\partial t}$=1. Now suppose I take the partial again, but this time I substitute first. f(x(t),t)=t^2+t. This time $\frac{\partial f}{\partial t}$=2t+1. This confuses me because, surely the result of a partial derivative relies only on the function f, and can't be dependent on my choice of how to write it. Unless the partial derivative is really such a whimsical, subjective operator.",,"['multivariable-calculus', 'partial-derivative']"
52,Proof of the potential function representation of Complex lamellar vector field,Proof of the potential function representation of Complex lamellar vector field,,"Given a continuously differentiable vector field $\bf a$, demonstrate the equivalence (iff) between the requirement that it satisfies ${\bf a}\cdot(\nabla \times {\bf a})=0$ and that it has the representation ${\bf a}=\lambda \nabla \phi$, where $\lambda, \phi$ are scalar functions. Stated in another way, why is ${\bf a}\cdot(\nabla \times {\bf a})=0$ a necessary and sufficent condition for vector fields to have normal congruences. Many reference cites Lord Kelvin as the source of this theorem, but his proof, though classical, is too magnetism-oriented to readily understand. Thanks!","Given a continuously differentiable vector field $\bf a$, demonstrate the equivalence (iff) between the requirement that it satisfies ${\bf a}\cdot(\nabla \times {\bf a})=0$ and that it has the representation ${\bf a}=\lambda \nabla \phi$, where $\lambda, \phi$ are scalar functions. Stated in another way, why is ${\bf a}\cdot(\nabla \times {\bf a})=0$ a necessary and sufficent condition for vector fields to have normal congruences. Many reference cites Lord Kelvin as the source of this theorem, but his proof, though classical, is too magnetism-oriented to readily understand. Thanks!",,['multivariable-calculus']
53,Loss of direction in Gauß' theorem?,Loss of direction in Gauß' theorem?,,"I was wondering about the following: If I have a function $\phi:\mathbb{R}^3\rightarrow \mathbb{R}$ and I want to calculate the mean value of $E=-\nabla \phi$ over a sphere, then $E$ of course if a vector, but the mean value: $ E=-\frac{1}{V_\mathrm{sphere}}\int_V \nabla \phi=-\frac{1}{V_\mathrm{sphere}} \int_{S(\text{sphere})} \phi$ is no longer a vector. so how do I manage it to get also directional information about the mean value of $E$? What I calculated was: $\frac{1}{V_\mathrm{sphere}} \int_{S(\text{sphere})} \phi=\frac{1}{V_\mathrm{sphere}} \int_0^{2\pi} \int_0^{\pi} \phi r^2 \sin(\theta) d\theta d\phi$ This is the way it was done here: Article on electrodynamics on page 3 Please note that all definitions I gave to you, also apply to the notation in this article","I was wondering about the following: If I have a function $\phi:\mathbb{R}^3\rightarrow \mathbb{R}$ and I want to calculate the mean value of $E=-\nabla \phi$ over a sphere, then $E$ of course if a vector, but the mean value: $ E=-\frac{1}{V_\mathrm{sphere}}\int_V \nabla \phi=-\frac{1}{V_\mathrm{sphere}} \int_{S(\text{sphere})} \phi$ is no longer a vector. so how do I manage it to get also directional information about the mean value of $E$? What I calculated was: $\frac{1}{V_\mathrm{sphere}} \int_{S(\text{sphere})} \phi=\frac{1}{V_\mathrm{sphere}} \int_0^{2\pi} \int_0^{\pi} \phi r^2 \sin(\theta) d\theta d\phi$ This is the way it was done here: Article on electrodynamics on page 3 Please note that all definitions I gave to you, also apply to the notation in this article",,"['calculus', 'real-analysis']"
54,"Use implicit function theorem and show $u(x,y)^2+v(x,y)^2=\frac{16}{x+y}$",Use implicit function theorem and show,"u(x,y)^2+v(x,y)^2=\frac{16}{x+y}","I am attempting exercise 11.6.4 on p. 368 of Wade's Introduction to Analysis . It asks: ""Find conditions on a point $(x_0, y_0, u_0, v_0)$ such that there exist functions $u(x,y)$ and v(x,y) that are C1 at $(x_0,y_0)$ and satisfy the simultaneous equations $$F(x,y,u,v)=xu^2 + yv^2 + xy =9 \\ G(x,y,u,v)=xv^2 + yu^2 - xy =7.$$Prove that the solutions satisfy $u^2 + v^2 = \frac{16}{x+y}.$"" The first part is straightforward Implicit function theorem, I get that $\frac{\partial(F,G)}{\partial(u,v)}=4uv(x+y)(x-y)$, so the conditions are $u,v \neq 0$ and $|x|\neq |y|$. I am finding the relation on $x,y, u(x,y),v(x,y)$ more difficult to prove. If I differentiate implicitly, I get for instance $$F_x=u^2 + 2ux\cdot u_x+2vy\cdot v_x + y = 0\\G_x = v^2 + 2xv\cdot v_x + 2uy \cdot u_x -y =0$$so by Cramer's rule $$u_x = \frac{\left|\begin{matrix}-y-u^2 & 2vy \\ y-v^2 & 2xv\end{matrix}\right|}{\left|\begin{matrix}2ux & 2vy \\ 2uy & 2xv\end{matrix}\right|},$$ but I don't see how that is helping us. Hunting around, I find that $(x,y,u,v)=(0,1,\sqrt{7},3)$ is a point where $F,G$ are satisfied and $\frac{\partial(F,G)}{\partial(u,v)}\neq 0$, and I note the relation holds there, although that doesn't help me to show it in general. Any ideas? I apologize for the duplication with this recent question , which has not been answered (it was asked by someone else). I have tried to ask the question in a more clear, thorough way with my post.","I am attempting exercise 11.6.4 on p. 368 of Wade's Introduction to Analysis . It asks: ""Find conditions on a point $(x_0, y_0, u_0, v_0)$ such that there exist functions $u(x,y)$ and v(x,y) that are C1 at $(x_0,y_0)$ and satisfy the simultaneous equations $$F(x,y,u,v)=xu^2 + yv^2 + xy =9 \\ G(x,y,u,v)=xv^2 + yu^2 - xy =7.$$Prove that the solutions satisfy $u^2 + v^2 = \frac{16}{x+y}.$"" The first part is straightforward Implicit function theorem, I get that $\frac{\partial(F,G)}{\partial(u,v)}=4uv(x+y)(x-y)$, so the conditions are $u,v \neq 0$ and $|x|\neq |y|$. I am finding the relation on $x,y, u(x,y),v(x,y)$ more difficult to prove. If I differentiate implicitly, I get for instance $$F_x=u^2 + 2ux\cdot u_x+2vy\cdot v_x + y = 0\\G_x = v^2 + 2xv\cdot v_x + 2uy \cdot u_x -y =0$$so by Cramer's rule $$u_x = \frac{\left|\begin{matrix}-y-u^2 & 2vy \\ y-v^2 & 2xv\end{matrix}\right|}{\left|\begin{matrix}2ux & 2vy \\ 2uy & 2xv\end{matrix}\right|},$$ but I don't see how that is helping us. Hunting around, I find that $(x,y,u,v)=(0,1,\sqrt{7},3)$ is a point where $F,G$ are satisfied and $\frac{\partial(F,G)}{\partial(u,v)}\neq 0$, and I note the relation holds there, although that doesn't help me to show it in general. Any ideas? I apologize for the duplication with this recent question , which has not been answered (it was asked by someone else). I have tried to ask the question in a more clear, thorough way with my post.",,"['multivariable-calculus', 'implicit-differentiation']"
55,3D Equations of Planes,3D Equations of Planes,,"So the above question is from a Calc III class I'm taking but I'm not sure I understand the solution... $n_1$ represents the vector normal to the given plane. Therefore taking the cross product of $n_1$ and the vector $P_1P_2$ would output a vector that is parallel to the given plane, not orthogonal as desired. Unless I'm mistaken... Anyone care to elaborate? Are both solutions correct? Here was my attempted solution: Let $r$ be the vector $<3,-1,-1>$ and let $M_1$ and $M_2$ be arbitrary points satisfying the equation of the plane given. I chose $M_1 = (3,10,0)$ and $M_2 = (2,6,0)$ therefore let $m$ be the corresponding vector $m = <-1,-4,0>$ then $$r \times m = <-4,1,-13>$$ which gives a final equation of $$ -4(x+2) + y-1 -13(z-4)$$","So the above question is from a Calc III class I'm taking but I'm not sure I understand the solution... $n_1$ represents the vector normal to the given plane. Therefore taking the cross product of $n_1$ and the vector $P_1P_2$ would output a vector that is parallel to the given plane, not orthogonal as desired. Unless I'm mistaken... Anyone care to elaborate? Are both solutions correct? Here was my attempted solution: Let $r$ be the vector $<3,-1,-1>$ and let $M_1$ and $M_2$ be arbitrary points satisfying the equation of the plane given. I chose $M_1 = (3,10,0)$ and $M_2 = (2,6,0)$ therefore let $m$ be the corresponding vector $m = <-1,-4,0>$ then $$r \times m = <-4,1,-13>$$ which gives a final equation of $$ -4(x+2) + y-1 -13(z-4)$$",,['multivariable-calculus']
56,Solutions of Elliptic PDEs: scaling and ellipticity,Solutions of Elliptic PDEs: scaling and ellipticity,,"Suppose we have a weak solution of the uniformly elliptic equation $$\sum_{i,j=1}^n D_i(a_{i,j}(x)D_j u(x)) = 0$$ in all of $\mathbb{R}^n$ Where $a_{i,j}$ are bounded and measurable and $D_i$ represents the partial derivative with respect to $x_i$. Consider the scaling $y=\frac{x}{r}$ for some $r>0$ Can I say that for all $r>0$,  $u(rx)$  is a (weak) solution of another elliptic equation with the same ellipticity constant, in the unit ball $B(0,1)$  ?? Thank you.","Suppose we have a weak solution of the uniformly elliptic equation $$\sum_{i,j=1}^n D_i(a_{i,j}(x)D_j u(x)) = 0$$ in all of $\mathbb{R}^n$ Where $a_{i,j}$ are bounded and measurable and $D_i$ represents the partial derivative with respect to $x_i$. Consider the scaling $y=\frac{x}{r}$ for some $r>0$ Can I say that for all $r>0$,  $u(rx)$  is a (weak) solution of another elliptic equation with the same ellipticity constant, in the unit ball $B(0,1)$  ?? Thank you.",,"['real-analysis', 'multivariable-calculus', 'partial-differential-equations']"
57,What does it mean by piecewise smooth boundary?,What does it mean by piecewise smooth boundary?,,I will be highly obliged if anyone can give me any reference where i can get the definition of domain (in $\mathbb{R^n}$) with piecewise smooth boundary. My question is when a domain in $\mathbb{R^n}$ is said to have a piecewise smooth boundary? I tried a lot to find in Google but i didn't get.Please help me out.,I will be highly obliged if anyone can give me any reference where i can get the definition of domain (in $\mathbb{R^n}$) with piecewise smooth boundary. My question is when a domain in $\mathbb{R^n}$ is said to have a piecewise smooth boundary? I tried a lot to find in Google but i didn't get.Please help me out.,,"['reference-request', 'multivariable-calculus']"
58,Laplacian on Sphere of Function Only Depending on Angle Between Points,Laplacian on Sphere of Function Only Depending on Angle Between Points,,"Consider a function $f:S^2 \to \mathbb{R}$ , with $S^2$ the unit $2$-sphere in $\mathbb{R}^3$. Let's say that $f$ depends only on the polar angle $\theta$ from the north pole (e.g., $f(r,\theta,\phi) = e^{\cos(\theta)} $ which is also just $e^z$ in Cartesian cordinates). Now, we define the Laplacian on the sphere by $$\Delta f = \frac{1}{\sin(\theta)} \frac{\partial}{\partial \theta} \sin(\theta) \frac{\partial f}{\partial \theta} + \frac{1}{\sin^2(\theta)} \frac{\partial^2 f}{\partial \phi^2}$$ where $\phi$ is the azimuthal angle in the $xy$ plane. Now, another way of looking at $f$ is saying that its value at $p \in S^2$ depends on the norm distance between the point $p_0 = (0,0,1)$ and $p$, since $||p-p_0|| = \sqrt{||p||^2 -2(p,p_0) + ||p_0||^2} = \sqrt{2-2 \cos(\theta)}$, where we just used the statement $a \cdot b = |a| |b| \cos(\theta)$. Hence, we can view $f$ as a function depending on the distance from $p$ to $p_0$. From here on, consider a chosen function $f$  whose form we know, and whose value at a point $p$  depends only on the cosine of the angle between a fixed chosen point $p_0$ (the north pole) and $p$. With this, we can then compute $\Delta f$, and we note that all of the $\phi$ terms are automatically zero, so $\Delta f$ can be computed completely in terms of the $\theta$ derivatives. Recall that we saw that $f$ depends on ly on $\theta$, or equivalently, $f$ depends only on $||p-p_0||$, that is $f(p) = f(||p-p_0||)$. This gives rise to many different ""rotated"" versions of $f$ if I move $p_0$ from the north pole to somewhere else. I want to investigate the following question: How can we take the Laplacian of $f$ if we vary $p_0$? Does it have a simple form and how does it related to the Laplacian of $f(||p-p_0||)$ when $p_0$ is the north pole? So, let's consider this ""rotated"" version of $f$. That is, consider a point $p_0$ besides $(0,0,1)$ such as maybe $(1,0,0)$. Now, instead of depending on the angle between $p$ and the north pole, it depends on the angle $\theta'$ between $p$ and this shifted point $p_0$. In some sense, we are essentially rotating the sphere and assigning this new $p_0$ to be the north pole. Here is what I want to do: I want to compuate $\Delta f$ for this new shifted $f$ where $f$ depends on this new $\theta'$. I have been told that I can just do the following: since $f(\theta')$ depends only on the polar distance, the Laplacian of $f$ is the same as taking the orignal $f$ with $p_0$ being the north pole, taking the Laplacian of that function (which only depends on $\theta$), which yields some function $g(\theta) = \Delta f$. Now, if I use a different point $p_0$ than the north pole, the claim is that $\Delta f(||p-p_0||) = g(\theta')$. That is, its the same function, just with a swap of the variable from $\theta$ to $\theta'$. Therefore, to compute $\Delta f(||p-p_0||)$ for any different $p_0$, it suffices to compute the simple case of $\Delta f(\theta)$ by $$ g(\theta) = \Delta f(\theta) = \frac{1}{\sin(\theta)} \frac{\partial }{\partial \theta} \sin(\theta) \frac{\partial f}{\partial \theta}$$ and then swapping that $\theta$ with the different $\theta'$ depending on the new $p_0$. That seems reasonable to me, but I'm just worried that it looks like on a rotated version of the function, the azimuthal derivatives will impact it. Certainly if I view things in terms of $\theta$ and $\phi$, there will be $\phi$ components. The assertion I suppose is that in this rotated setting, $\Delta f(\theta')$ still depends only on $\theta'$ and has the same form as $\Delta f(\theta)$ as if it were centered at the north pole. Is this true? I apologize if this is confusing or trivial!","Consider a function $f:S^2 \to \mathbb{R}$ , with $S^2$ the unit $2$-sphere in $\mathbb{R}^3$. Let's say that $f$ depends only on the polar angle $\theta$ from the north pole (e.g., $f(r,\theta,\phi) = e^{\cos(\theta)} $ which is also just $e^z$ in Cartesian cordinates). Now, we define the Laplacian on the sphere by $$\Delta f = \frac{1}{\sin(\theta)} \frac{\partial}{\partial \theta} \sin(\theta) \frac{\partial f}{\partial \theta} + \frac{1}{\sin^2(\theta)} \frac{\partial^2 f}{\partial \phi^2}$$ where $\phi$ is the azimuthal angle in the $xy$ plane. Now, another way of looking at $f$ is saying that its value at $p \in S^2$ depends on the norm distance between the point $p_0 = (0,0,1)$ and $p$, since $||p-p_0|| = \sqrt{||p||^2 -2(p,p_0) + ||p_0||^2} = \sqrt{2-2 \cos(\theta)}$, where we just used the statement $a \cdot b = |a| |b| \cos(\theta)$. Hence, we can view $f$ as a function depending on the distance from $p$ to $p_0$. From here on, consider a chosen function $f$  whose form we know, and whose value at a point $p$  depends only on the cosine of the angle between a fixed chosen point $p_0$ (the north pole) and $p$. With this, we can then compute $\Delta f$, and we note that all of the $\phi$ terms are automatically zero, so $\Delta f$ can be computed completely in terms of the $\theta$ derivatives. Recall that we saw that $f$ depends on ly on $\theta$, or equivalently, $f$ depends only on $||p-p_0||$, that is $f(p) = f(||p-p_0||)$. This gives rise to many different ""rotated"" versions of $f$ if I move $p_0$ from the north pole to somewhere else. I want to investigate the following question: How can we take the Laplacian of $f$ if we vary $p_0$? Does it have a simple form and how does it related to the Laplacian of $f(||p-p_0||)$ when $p_0$ is the north pole? So, let's consider this ""rotated"" version of $f$. That is, consider a point $p_0$ besides $(0,0,1)$ such as maybe $(1,0,0)$. Now, instead of depending on the angle between $p$ and the north pole, it depends on the angle $\theta'$ between $p$ and this shifted point $p_0$. In some sense, we are essentially rotating the sphere and assigning this new $p_0$ to be the north pole. Here is what I want to do: I want to compuate $\Delta f$ for this new shifted $f$ where $f$ depends on this new $\theta'$. I have been told that I can just do the following: since $f(\theta')$ depends only on the polar distance, the Laplacian of $f$ is the same as taking the orignal $f$ with $p_0$ being the north pole, taking the Laplacian of that function (which only depends on $\theta$), which yields some function $g(\theta) = \Delta f$. Now, if I use a different point $p_0$ than the north pole, the claim is that $\Delta f(||p-p_0||) = g(\theta')$. That is, its the same function, just with a swap of the variable from $\theta$ to $\theta'$. Therefore, to compute $\Delta f(||p-p_0||)$ for any different $p_0$, it suffices to compute the simple case of $\Delta f(\theta)$ by $$ g(\theta) = \Delta f(\theta) = \frac{1}{\sin(\theta)} \frac{\partial }{\partial \theta} \sin(\theta) \frac{\partial f}{\partial \theta}$$ and then swapping that $\theta$ with the different $\theta'$ depending on the new $p_0$. That seems reasonable to me, but I'm just worried that it looks like on a rotated version of the function, the azimuthal derivatives will impact it. Certainly if I view things in terms of $\theta$ and $\phi$, there will be $\phi$ components. The assertion I suppose is that in this rotated setting, $\Delta f(\theta')$ still depends only on $\theta'$ and has the same form as $\Delta f(\theta)$ as if it were centered at the north pole. Is this true? I apologize if this is confusing or trivial!",,"['multivariable-calculus', 'spherical-geometry', 'spherical-coordinates']"
59,"Is $f(x,y) = x^2y + x y^2$ (quasi-) concave or convex?",Is  (quasi-) concave or convex?,"f(x,y) = x^2y + x y^2","I should analyze whether the function $$f(x,y) = x^2y + x y^2 \text{ where }  x,y > 0$$ is (quasi-) concave or convex. Thus, as usual, I set up the Hessian as $$ D^2f(x,y) = \left( \begin{array}{cc} 2y & 2x + 2y \\ 2x + 2y & 2x \\  \end{array} \right)  $$ which, given the constraints $x,y > 0$, is indefinite. So I need the bordered Hessian: $$ H= \left( \begin{array}{ccc} 0 & 2xy + y^2 & x^2 + 2xy \\ 2xy + y^2 & 2y & 2x + 2y \\ x^2 + 2xy & 2x+2y & 2x \end{array} \right) $$ and check its determinant to find out about its properties. I end up with $$ \det|H| = 2x(2x^3 + x^3 y - 2x^2 y + 4x^2 y^2 + 4 x y^3 + 2 y^4)$$ which, considering $x,y > 0$, looks quite positive to me, so $f$ would be quasiconcave. But how can I prove that $\det|H| > 0$? Or is there any easier approach? Thanks!","I should analyze whether the function $$f(x,y) = x^2y + x y^2 \text{ where }  x,y > 0$$ is (quasi-) concave or convex. Thus, as usual, I set up the Hessian as $$ D^2f(x,y) = \left( \begin{array}{cc} 2y & 2x + 2y \\ 2x + 2y & 2x \\  \end{array} \right)  $$ which, given the constraints $x,y > 0$, is indefinite. So I need the bordered Hessian: $$ H= \left( \begin{array}{ccc} 0 & 2xy + y^2 & x^2 + 2xy \\ 2xy + y^2 & 2y & 2x + 2y \\ x^2 + 2xy & 2x+2y & 2x \end{array} \right) $$ and check its determinant to find out about its properties. I end up with $$ \det|H| = 2x(2x^3 + x^3 y - 2x^2 y + 4x^2 y^2 + 4 x y^3 + 2 y^4)$$ which, considering $x,y > 0$, looks quite positive to me, so $f$ would be quasiconcave. But how can I prove that $\det|H| > 0$? Or is there any easier approach? Thanks!",,"['multivariable-calculus', 'convex-analysis']"
60,Gradient of a scalar function acting on a vector function,Gradient of a scalar function acting on a vector function,,"If I have a vector function that is constructed from a scalar function acting on a vector function, what is it's gradient? $$\psi(x)=\phi(f(x))$$ where $$x\in\mathbb{R}^n, f\in\mathbb{R}^n\rightarrow\mathbb{R}^1, \phi\in\mathbb{R}^1\rightarrow\mathbb{R}^1$$ Is the following correct? $$\nabla\psi(x)=\nabla(\phi(f(x)))=\frac{d\phi}{df}\cdot{\nabla}f(x)$$ where ${\nabla}f(x)=\left[\frac{df}{dx_1},\frac{df}{dx_2},...,\frac{df}{dx_n}\right]^T$","If I have a vector function that is constructed from a scalar function acting on a vector function, what is it's gradient? $$\psi(x)=\phi(f(x))$$ where $$x\in\mathbb{R}^n, f\in\mathbb{R}^n\rightarrow\mathbb{R}^1, \phi\in\mathbb{R}^1\rightarrow\mathbb{R}^1$$ Is the following correct? $$\nabla\psi(x)=\nabla(\phi(f(x)))=\frac{d\phi}{df}\cdot{\nabla}f(x)$$ where ${\nabla}f(x)=\left[\frac{df}{dx_1},\frac{df}{dx_2},...,\frac{df}{dx_n}\right]^T$",,['multivariable-calculus']
61,L'Hospital Rule in Multivariable Calculus,L'Hospital Rule in Multivariable Calculus,,"I was just wondering. Given functions $f(x,y), g(x,y) $, and the corresponding limit $\lim_{(x,y) \to (0,0) } \frac{f(x,y)}{g(x,y)} $ , where $\lim_{(x,y) \to (0,0) } f = \lim_{(x,y) \to (0,0) }g = 0  $. Assume that when moving to polar coordinates, we get that: $\lim_{ r \to 0^+ } f(rcos\theta, rsin\theta)  = \lim_{ r \to 0^+ } g(rcos\theta, rsin\theta)=0   $.  Can someone please tell me whether it is possible or not to differentiate $f,g$ with respect to $r$ and use l'Hospital rule? I think that this is not legal, since $\theta$ can also be a function of $r$ , but I can't find any good example for the following claim: There exist functions $f,g$ such that $ \lim_{ r \to 0^+ } f(rcos\theta, rsin\theta)  = \lim_{ r \to 0^+ } g(rcos\theta, rsin\theta)=0 $  but l'Hospital rule doesn't apply ... Hope I made myself clear enough . Thanks a lot","I was just wondering. Given functions $f(x,y), g(x,y) $, and the corresponding limit $\lim_{(x,y) \to (0,0) } \frac{f(x,y)}{g(x,y)} $ , where $\lim_{(x,y) \to (0,0) } f = \lim_{(x,y) \to (0,0) }g = 0  $. Assume that when moving to polar coordinates, we get that: $\lim_{ r \to 0^+ } f(rcos\theta, rsin\theta)  = \lim_{ r \to 0^+ } g(rcos\theta, rsin\theta)=0   $.  Can someone please tell me whether it is possible or not to differentiate $f,g$ with respect to $r$ and use l'Hospital rule? I think that this is not legal, since $\theta$ can also be a function of $r$ , but I can't find any good example for the following claim: There exist functions $f,g$ such that $ \lim_{ r \to 0^+ } f(rcos\theta, rsin\theta)  = \lim_{ r \to 0^+ } g(rcos\theta, rsin\theta)=0 $  but l'Hospital rule doesn't apply ... Hope I made myself clear enough . Thanks a lot",,['multivariable-calculus']
62,integrating a vector over a sphere,integrating a vector over a sphere,,"I have the following triple integral in spherical coordinates $(r,\theta,\phi)$: $$\int_0^{2\pi}\int_0^\pi\int_0^RCr^3\hat\theta\cdot r^2dr\sin{\theta}d\theta d\phi$$ How do I handle the $\hat\theta$? If I ignore it, I get $\frac{2}{3}\pi CR^6$. So is my answer the vector $\frac{2}{3}\pi CR^6\hat\theta$? Do I need to integrate the unit vector $\hat\theta$? If so, how?","I have the following triple integral in spherical coordinates $(r,\theta,\phi)$: $$\int_0^{2\pi}\int_0^\pi\int_0^RCr^3\hat\theta\cdot r^2dr\sin{\theta}d\theta d\phi$$ How do I handle the $\hat\theta$? If I ignore it, I get $\frac{2}{3}\pi CR^6$. So is my answer the vector $\frac{2}{3}\pi CR^6\hat\theta$? Do I need to integrate the unit vector $\hat\theta$? If so, how?",,"['multivariable-calculus', 'definite-integrals', 'spherical-coordinates']"
63,Linearization of an implicitly defined function.,Linearization of an implicitly defined function.,,"Problem: Given the equation: $xz^{2}+y^{2}z^{5}=19$ Also given: (3,4,1) is a solution to the equation.  This point is not the only solution. 1) Find dz/dx and dz/dy (through implicit differentiation) evaluated at (3,4). 2) Find the linearization L(x,y) of z(x,y) at (3,4) 3)  Use L to approximate z(3.01, 4.02).  Plug this (approximate) result into the given equation to see if it satisfies it. My attempted solution: 1) dz/dx = $(-z^{2})/(2xz+5y^{2}z^{4})$ dz/dx evaluated at (3,4) = $(-z^{2})/(6z+80z^{4})$ dz/dy = $(-2yz^{5})/((2xz+5y^{2}z^{4})$ dz/dy evaluated at (3,4) = $(-8z^{5})/(6z+80z^{4})$ 2) Formula for linearization: L(x) = f(a) + Df(a) (x-a) f(a) = $3z^{2} + 16z^{5}=19$ $(z-1)(16z^{4} + 16z^{3} + 16z^{2} +19{z} +19)=0$ z=1 is the only real solution I'm not entirely sure I am solving f(a) correctly.  Up until now, I have only dealt with explicitly defined functions. Df(a) = Dz(a)[dz/dx evaluated at a  dz/dy evaluated at a] Dz= \begin{bmatrix} (-z^{2})/(6z)+80z^{4}),(-8z^{5})/((6z)+80z^{4})&\end{bmatrix} (x-a) =  \begin{bmatrix} x-3,y-4\\ \end{bmatrix} (Transposed so that it is a column vector). L(x) = $1 + (x-3)*((-z^{2})/(6z+80z^{4})+(y-4)*((-8z^{5})/(6z+80z^{4}))$ 3) $1+((-0.01z^{2})/(6z+80z^{4})) + ((-0.16z^{5}/(6z+80z^{4}))$ Is this the approximation for z(3,4)?  I'm not sure how to proceed from here.  What do I plug into the original equation to see that it approximately satisfies the equation? Thanks!  Sorry about the formatting. I tried to tidy up the formatting a bit.  I'm not too familiar with latex though.","Problem: Given the equation: $xz^{2}+y^{2}z^{5}=19$ Also given: (3,4,1) is a solution to the equation.  This point is not the only solution. 1) Find dz/dx and dz/dy (through implicit differentiation) evaluated at (3,4). 2) Find the linearization L(x,y) of z(x,y) at (3,4) 3)  Use L to approximate z(3.01, 4.02).  Plug this (approximate) result into the given equation to see if it satisfies it. My attempted solution: 1) dz/dx = $(-z^{2})/(2xz+5y^{2}z^{4})$ dz/dx evaluated at (3,4) = $(-z^{2})/(6z+80z^{4})$ dz/dy = $(-2yz^{5})/((2xz+5y^{2}z^{4})$ dz/dy evaluated at (3,4) = $(-8z^{5})/(6z+80z^{4})$ 2) Formula for linearization: L(x) = f(a) + Df(a) (x-a) f(a) = $3z^{2} + 16z^{5}=19$ $(z-1)(16z^{4} + 16z^{3} + 16z^{2} +19{z} +19)=0$ z=1 is the only real solution I'm not entirely sure I am solving f(a) correctly.  Up until now, I have only dealt with explicitly defined functions. Df(a) = Dz(a)[dz/dx evaluated at a  dz/dy evaluated at a] Dz= \begin{bmatrix} (-z^{2})/(6z)+80z^{4}),(-8z^{5})/((6z)+80z^{4})&\end{bmatrix} (x-a) =  \begin{bmatrix} x-3,y-4\\ \end{bmatrix} (Transposed so that it is a column vector). L(x) = $1 + (x-3)*((-z^{2})/(6z+80z^{4})+(y-4)*((-8z^{5})/(6z+80z^{4}))$ 3) $1+((-0.01z^{2})/(6z+80z^{4})) + ((-0.16z^{5}/(6z+80z^{4}))$ Is this the approximation for z(3,4)?  I'm not sure how to proceed from here.  What do I plug into the original equation to see that it approximately satisfies the equation? Thanks!  Sorry about the formatting. I tried to tidy up the formatting a bit.  I'm not too familiar with latex though.",,"['multivariable-calculus', 'approximation', 'implicit-differentiation']"
64,Jacobian of $f(|x|)x$,Jacobian of,f(|x|)x,"Suppose one has a $C^1$ function $f:\mathbb{R} \rightarrow [0,\infty)$. A throwaway line in a paper I'm reading claims  \begin{equation} \det \left[ \nabla \left( f(|\vec{x}|)\vec{x} \right) \right] = f^{n-1}(|\vec{x}|) \left[f(|\vec{x}|) + f'(|\vec{x}|)|\vec{x}| \right] \end{equation} for $\vec{x} \in \mathbb{R}^n$. Any ideas on how to prove this?","Suppose one has a $C^1$ function $f:\mathbb{R} \rightarrow [0,\infty)$. A throwaway line in a paper I'm reading claims  \begin{equation} \det \left[ \nabla \left( f(|\vec{x}|)\vec{x} \right) \right] = f^{n-1}(|\vec{x}|) \left[f(|\vec{x}|) + f'(|\vec{x}|)|\vec{x}| \right] \end{equation} for $\vec{x} \in \mathbb{R}^n$. Any ideas on how to prove this?",,"['linear-algebra', 'multivariable-calculus']"
65,Assignment of Subscripts in Einstein Summation Notation,Assignment of Subscripts in Einstein Summation Notation,,"I'm trying to understand the following conversion from vector form into Einstein summation notation, found on P2 of http://www.stanford.edu/~vkl/research/notes/index_not.pdf which states: Show $\mathbf{v} \cdot \nabla\mathbf{v} = \nabla\left(\frac{|\mathbf{v}|^2}{2}\right)+(\color{brown}{\nabla \times \mathbf{v})} \times \mathbf{v}$ Proof: $v_a \partial_a v_b = \partial_b\left(\frac{v_av_a}{2} \right) + \epsilon_{bac}\color{brown}{(\epsilon_{adf}\partial_dv_f)}v_c \tag{*}$ (Rest of proof omitted here) $\Large{\text{Question #1.}}$ How did they get the LHS of (*)? I don't think my course covers the gradient of a vector, so I don't know how to convert it into Einstein notation. Or is it supposed to be $\nabla \cdot \mathbf{v}$ ? $\Large{\text{Question #2.}} $ The solution seems to be working with the $b$ th component of $ \mathbf{v} \cdot \nabla\mathbf{v}$ , but it doesn't say so. Because $\mathbf{u} \times \mathbf{v} = \epsilon_{acb}u_av_j\color{red}{\mathbf{\hat{e_b}}}$ , is the solution missing $\color{red}{\mathbf{\hat{e_b}}}$ on the RHS: $$\partial_b\left(\frac{v_av_a}{2} \right) + \underbrace{\epsilon_{acb}}_{\Large{= \epsilon_{bac}}}\color{brown}{(\epsilon_{adf}\partial_dv_f)}v_c\color{red}{\mathbf{\hat{e_b}} }?$$ Here are two supplementary questions in response to tom's answer: $\Large{\text{Question #1.1.}} $ How did you realise that $\mathbf{v} \cdot \nabla\mathbf{v}$ should've been written as $(\mathbf{v} \cdot \nabla)\mathbf{v}$ ? $\Large{\text{Question #2.1.}}$ I don't understand your answer. $((\color{brown}{\nabla \times \mathbf{v})} \times \mathbf{v})$ is a vector so why does the given solution not convert this to $\epsilon_{acb}\color{brown}{(\epsilon_{adf}\partial_dv_f)}v_c\color{red}{\mathbf{\hat{e_b}} }$ ? Here are two supplementary questions in response to Muphrid's answer: $\Large{\text{Question #1.2.}} $ What would be the ""order of operations"" which you mentioned in your answer? $\Large{\text{Question #2.2.}}$ Sorry, I don't understand what you mean by ""...it would otherwise appear as the same basis vector in each term."" Could you please clarify? Here are two supplementary questions in response to Muphrid's 2nd answer: $\Large{\text{Question #2.3.}}$ Could you please explain how a free index ( $\color{red}{b}$ here) means that we are looking at the component of this free index ( $\color{red}{b}$ th component here)?","I'm trying to understand the following conversion from vector form into Einstein summation notation, found on P2 of http://www.stanford.edu/~vkl/research/notes/index_not.pdf which states: Show Proof: (Rest of proof omitted here) How did they get the LHS of (*)? I don't think my course covers the gradient of a vector, so I don't know how to convert it into Einstein notation. Or is it supposed to be ? The solution seems to be working with the th component of , but it doesn't say so. Because , is the solution missing on the RHS: Here are two supplementary questions in response to tom's answer: How did you realise that should've been written as ? I don't understand your answer. is a vector so why does the given solution not convert this to ? Here are two supplementary questions in response to Muphrid's answer: What would be the ""order of operations"" which you mentioned in your answer? Sorry, I don't understand what you mean by ""...it would otherwise appear as the same basis vector in each term."" Could you please clarify? Here are two supplementary questions in response to Muphrid's 2nd answer: Could you please explain how a free index ( here) means that we are looking at the component of this free index ( th component here)?",\mathbf{v} \cdot \nabla\mathbf{v} = \nabla\left(\frac{|\mathbf{v}|^2}{2}\right)+(\color{brown}{\nabla \times \mathbf{v})} \times \mathbf{v} v_a \partial_a v_b = \partial_b\left(\frac{v_av_a}{2} \right) + \epsilon_{bac}\color{brown}{(\epsilon_{adf}\partial_dv_f)}v_c \tag{*} \Large{\text{Question #1.}} \nabla \cdot \mathbf{v} \Large{\text{Question #2.}}  b  \mathbf{v} \cdot \nabla\mathbf{v} \mathbf{u} \times \mathbf{v} = \epsilon_{acb}u_av_j\color{red}{\mathbf{\hat{e_b}}} \color{red}{\mathbf{\hat{e_b}}} \partial_b\left(\frac{v_av_a}{2} \right) + \underbrace{\epsilon_{acb}}_{\Large{= \epsilon_{bac}}}\color{brown}{(\epsilon_{adf}\partial_dv_f)}v_c\color{red}{\mathbf{\hat{e_b}} }? \Large{\text{Question #1.1.}}  \mathbf{v} \cdot \nabla\mathbf{v} (\mathbf{v} \cdot \nabla)\mathbf{v} \Large{\text{Question #2.1.}} ((\color{brown}{\nabla \times \mathbf{v})} \times \mathbf{v}) \epsilon_{acb}\color{brown}{(\epsilon_{adf}\partial_dv_f)}v_c\color{red}{\mathbf{\hat{e_b}} } \Large{\text{Question #1.2.}}  \Large{\text{Question #2.2.}} \Large{\text{Question #2.3.}} \color{red}{b} \color{red}{b},[]
66,How to get a unit vector from another unit vector and angle between them?,How to get a unit vector from another unit vector and angle between them?,,How to get a unit vector from another unit vector and angle between them? Is it possible? I need something like  this:,How to get a unit vector from another unit vector and angle between them? Is it possible? I need something like  this:,,"['multivariable-calculus', 'vectors']"
67,Matrix Derivative of $ABC$ with respect to $B$,Matrix Derivative of  with respect to,ABC B,"I have looked throughout the matrix cookbook and other sources, but am a bit confused by this problem. If I have a function $F = ABC$, what is the partial derivative of $F$ with respect to $B$? When it comes to matrix functions I could not find a nice answer to this problem. I did find something akin to $dF = CAdX$, but that doesn't seem to make sense as the dimensions of $C$ and $A$ would not match up for multiplication. A could be a $3\times3$, $B$ a $3\times 3$, and $ C$ a $3\times 4$. I'd appreciate help on this. Thanks. Edit- Well I had changed the problem to be more in line with what I was originally working with below in the comments. Here is the problem: $f = tr((ABC)(ABC)^T)$ and I want the partial derivative $\frac{\partial}{\partial B} tr((ABC)(ABC)^T)$. I ended up using a modified version of copper.hat's answer. I combined $A$ and $B$ to get something like this $2(ABC)C^T$ for the gradient. Thank you both for the help. Both answers below are technically correct, I just accepted the shorter and more convenient form.","I have looked throughout the matrix cookbook and other sources, but am a bit confused by this problem. If I have a function $F = ABC$, what is the partial derivative of $F$ with respect to $B$? When it comes to matrix functions I could not find a nice answer to this problem. I did find something akin to $dF = CAdX$, but that doesn't seem to make sense as the dimensions of $C$ and $A$ would not match up for multiplication. A could be a $3\times3$, $B$ a $3\times 3$, and $ C$ a $3\times 4$. I'd appreciate help on this. Thanks. Edit- Well I had changed the problem to be more in line with what I was originally working with below in the comments. Here is the problem: $f = tr((ABC)(ABC)^T)$ and I want the partial derivative $\frac{\partial}{\partial B} tr((ABC)(ABC)^T)$. I ended up using a modified version of copper.hat's answer. I combined $A$ and $B$ to get something like this $2(ABC)C^T$ for the gradient. Thank you both for the help. Both answers below are technically correct, I just accepted the shorter and more convenient form.",,"['matrices', 'functions', 'multivariable-calculus', 'derivatives']"
68,In Search of a More Elegant Solution,In Search of a More Elegant Solution,,"I was asked to determine the maximum and minimum value of $$f(x,y,z)=(3x+4y+5z^{2})e^{-x^{2}-y^{2}-z^{2}}$$ on $\mathbb{R}^{3}$. Now, I employed the usually strategy; in other words calculating the partial derivatives, setting each to zero, and the solve for $x,y,z$ before comparing the values of the stationary points. I obtained $$M=5e^{-3/4}$$ as the maximum value and $$m=(-5e^{-1/2})/{\sqrt{2}}$$ as the minimum value, both of which turned out to be correct. However, as I decided to solve for $x,y,z$ by the method of substitution, the calculations became somewhat hostile. I'm sure there must be a simpler way to arrive at the solutions, and I would be thrilled if someone here would be so generous as to share such a solution.","I was asked to determine the maximum and minimum value of $$f(x,y,z)=(3x+4y+5z^{2})e^{-x^{2}-y^{2}-z^{2}}$$ on $\mathbb{R}^{3}$. Now, I employed the usually strategy; in other words calculating the partial derivatives, setting each to zero, and the solve for $x,y,z$ before comparing the values of the stationary points. I obtained $$M=5e^{-3/4}$$ as the maximum value and $$m=(-5e^{-1/2})/{\sqrt{2}}$$ as the minimum value, both of which turned out to be correct. However, as I decided to solve for $x,y,z$ by the method of substitution, the calculations became somewhat hostile. I'm sure there must be a simpler way to arrive at the solutions, and I would be thrilled if someone here would be so generous as to share such a solution.",,"['multivariable-calculus', 'optimization']"
69,Is Poisson's/Laplace's equation rotationally invariant?,Is Poisson's/Laplace's equation rotationally invariant?,,"Is Poisson's equation rotationally invariant? Is Laplace's equation rotationally invariant? If so, how can I see this? in reply to comments, my problem is that: In my notes, when calculating the Green's function, the first step taken is to state that the problem is rotationally symmetric, so the Green's function $G(x⃗ ;x_ 0)=G(|x −\vec x_0|)$. Why is this valid?","Is Poisson's equation rotationally invariant? Is Laplace's equation rotationally invariant? If so, how can I see this? in reply to comments, my problem is that: In my notes, when calculating the Green's function, the first step taken is to state that the problem is rotationally symmetric, so the Green's function $G(x⃗ ;x_ 0)=G(|x −\vec x_0|)$. Why is this valid?",,"['multivariable-calculus', 'partial-differential-equations', 'harmonic-functions', 'poissons-equation']"
70,Prove equality for multivariate function,Prove equality for multivariate function,,"I am given that $z=f(x^2+y^2)$ and need to show that $y\frac{\partial z}{\partial x}-x\frac{\partial z}{\partial y}=0$. Listed below is my attempt. Is this correct? I feel that it seems too simplistic. $\large{ \frac{\partial z}{\partial x} = f'(x^2+y^2) \dot{} 2x\\ \frac{\partial z}{\partial y} = f'(x^2+y^2) \dot{} 2y }$ Hence, $$ \begin{align*} y\frac{\partial z}{\partial x}-x\frac{\partial z}{\partial y} &= f'(x^2+y^2) \dot{} 2x \times y - f'(x^2+y^2) \dot{} 2y \times x \\&=2xy \times f'(x^2+y^2) -2xy \times f'(x^2+y^2) \\&=0 \end{align*} $$","I am given that $z=f(x^2+y^2)$ and need to show that $y\frac{\partial z}{\partial x}-x\frac{\partial z}{\partial y}=0$. Listed below is my attempt. Is this correct? I feel that it seems too simplistic. $\large{ \frac{\partial z}{\partial x} = f'(x^2+y^2) \dot{} 2x\\ \frac{\partial z}{\partial y} = f'(x^2+y^2) \dot{} 2y }$ Hence, $$ \begin{align*} y\frac{\partial z}{\partial x}-x\frac{\partial z}{\partial y} &= f'(x^2+y^2) \dot{} 2x \times y - f'(x^2+y^2) \dot{} 2y \times x \\&=2xy \times f'(x^2+y^2) -2xy \times f'(x^2+y^2) \\&=0 \end{align*} $$",,['multivariable-calculus']
71,"Help me derive Ampere's law from Biot-Savart, magnetostatic case ... how do I use $\nabla\cdot\vec{J} = 0$?","Help me derive Ampere's law from Biot-Savart, magnetostatic case ... how do I use ?",\nabla\cdot\vec{J} = 0,"I was trying to derive $\nabla \times \vec{B}\left(\vec{r}\right) = \mu_0 \vec{J}\left(\vec{r}\right)$ from: $$ \vec{B} \left( \vec{r} \right) =     \frac{\mu_0}{4\pi}       \iiint_{V_s}            \frac{\vec{J}\left(\vec{r}_s\right)}{\left|\vec{r}-\vec{r}_s\right|^3}            \times \left(\vec{r}-\vec{r}_s\right)        \space dV\left(\vec{r}_s\right) $$ So I first substitute $\nabla\frac{1}{|\vec{r}-\vec{a}|} = -\frac{\vec{r}-\vec{a}}{|\vec{r}-\vec{a}|^3}$ and get: $$ \vec{B} \left( \vec{r} \right) =     -\frac{\mu_0}{4\pi}       \iiint_{V_s}           \vec{J}\left(\vec{r}_s\right)           \times            \nabla_{\vec{r}}            \frac{1}{\left|\vec{r}-\vec{r}_s\right|}        \space dV\left(\vec{r}_s\right) $$ Which, because $\vec{J}\left(\vec{r}_s\right)$ doesn't depend on $\vec{r}$, is equal to: $$ \vec{B} \left( \vec{r} \right) =     \frac{\mu_0}{4\pi}       \iiint_{V_s}           \nabla_{\vec{r}}           \times             \frac{\vec{J}\left(\vec{r}_s\right)}{\left|\vec{r}-\vec{r}_s\right|}        \space dV\left(\vec{r}_s\right) $$ Now if I take the curl of both side, I get: $$ \nabla_{\vec{r}} \times \vec{B} \left( \vec{r} \right) =     \frac{\mu_0}{4\pi}       \iiint_{V_s}           \nabla_{\vec{r}} \times           \left(           \nabla_{\vec{r}}           \times            \frac{\vec{J}\left(\vec{r}_s\right)}{\left|\vec{r}-\vec{r}_s\right|}           \right)       \space dV\left(\vec{r}_s\right) $$ Which is: $$    \frac{\mu_0}{4\pi}       \iiint_{V_s}           \left(                     \nabla_{\vec{r}}\left(\nabla_{\vec{r}} \cdot \frac{\vec{J}\left(\vec{r}_s\right)}{\left|\vec{r}-\vec{r}_s\right|} \right)            -  {\nabla^2}_{\vec{r}} \frac{\vec{J}\left(\vec{r}_s\right)}{\left|\vec{r}-\vec{r}_s\right|}           \right)       \space dV\left(\vec{r}_s\right) $$ Which, because $\vec{J}\left(\vec{r}_s\right)$ doesn't depend on $\vec{r}$, is: $$    \frac{\mu_0}{4\pi}       \iiint_{V_s}           \left(           \nabla_{\vec{r}}             \left(               \vec{J}\left(\vec{r}_s\right) \cdot \nabla_{\vec{r}} \frac {1} {\left|\vec{r}-\vec{r}_s\right|}             \right)           + \vec{J}\left(\vec{r}_s\right) \space 4\pi \delta^3 \left(\vec{r}-\vec{r}_s\right)           \right)       \space dV\left(\vec{r}_s\right) $$ Which is: $$    \left[    \frac{\mu_0}{4\pi}       \iiint_{V_s}           \nabla_{\vec{r}}             \left(               \vec{J}\left(\vec{r}_s\right) \cdot \nabla_{\vec{r}} \frac {1} {\left|\vec{r}-\vec{r}_s\right|}             \right)       \space dV\left(\vec{r}_s\right)     \right]    \Large{ + \space\mu_0 \vec{J}\left(\vec{r}\right)} $$ Which is: $$     \frac{\mu_0}{4\pi}     \left[        \nabla_{\vec{r}}       \iiint_{V_s}             \left(               \vec{J}\left(\vec{r}_s\right) \cdot \nabla_{\vec{r}} \frac {1} {\left|\vec{r}-\vec{r}_s\right|}             \right)       \space dV\left(\vec{r}_s\right)       \right]       \Large{+ \space\mu_0 \vec{J}\left(\vec{r}\right)} $$ Now my question is, given $\nabla \cdot \vec{J}\left(\vec{r}_s\right) = 0$, how can I show that $ \nabla \iiint_{V_s}             \left(               \vec{J}\left(\vec{r}_s\right) \cdot \nabla \frac {1} {\left|\vec{r}-\vec{r}_s\right|}             \right)       \space dV\left(\vec{r}_s\right) $ reduces to zero? Which expression reduces to $\nabla \cdot \vec{J}\left(\vec{r}_s\right)$, and how? What is the applicable identity? Attempt So to attack the integral $ \iiint_{V_s} \left( \vec{J}\left(\vec{r}_s\right) \cdot \nabla \frac {1} {\left|\vec{r}-\vec{r}_s\right|} \right) \space dV\left(\vec{r}_s\right) $, I first switch the variable over which the gradient is taken from $\vec{r}$ to $\vec{r}_s$: $$ \nabla_{\vec{r}_s}\frac {1} {\left|\vec{r}-\vec{r}_s\right|} = -\nabla_{\vec{r}}\frac {1} {\left|\vec{r}-\vec{r}_s\right|} $$ My integral, therefore, becomes: $$  -\iiint_{V_s} \left( \vec{J}\left(\vec{r}_s\right) \cdot \nabla_{\vec{r}_s} \frac {1} {\left|\vec{r}-\vec{r}_s\right|} \right) \space dV\left(\vec{r}_s\right)  $$ Which is, doing a vector integration by parts: $$  \iiint_{V_s} \frac {\nabla_{\vec{r}_s} \cdot \vec{J}\left(\vec{r}_s\right)} {\left|\vec{r}-\vec{r}_s\right|}  dV\left(\vec{r}_s\right) -  \oint_{\partial V_s}  \frac {\vec{J}\left(\vec{r}_s\right)} {\left|\vec{r}-\vec{r}_s\right|}   \cdot d\vec{S}\left(\vec{r}_s\right) $$ The first term is zero because we know that $\nabla\cdot\vec{J}=0$. I am guessing that the second term also zero because, by definition, there is no current density outside of $V_s$. (This is correct, as confirmed by the answer below)","I was trying to derive $\nabla \times \vec{B}\left(\vec{r}\right) = \mu_0 \vec{J}\left(\vec{r}\right)$ from: $$ \vec{B} \left( \vec{r} \right) =     \frac{\mu_0}{4\pi}       \iiint_{V_s}            \frac{\vec{J}\left(\vec{r}_s\right)}{\left|\vec{r}-\vec{r}_s\right|^3}            \times \left(\vec{r}-\vec{r}_s\right)        \space dV\left(\vec{r}_s\right) $$ So I first substitute $\nabla\frac{1}{|\vec{r}-\vec{a}|} = -\frac{\vec{r}-\vec{a}}{|\vec{r}-\vec{a}|^3}$ and get: $$ \vec{B} \left( \vec{r} \right) =     -\frac{\mu_0}{4\pi}       \iiint_{V_s}           \vec{J}\left(\vec{r}_s\right)           \times            \nabla_{\vec{r}}            \frac{1}{\left|\vec{r}-\vec{r}_s\right|}        \space dV\left(\vec{r}_s\right) $$ Which, because $\vec{J}\left(\vec{r}_s\right)$ doesn't depend on $\vec{r}$, is equal to: $$ \vec{B} \left( \vec{r} \right) =     \frac{\mu_0}{4\pi}       \iiint_{V_s}           \nabla_{\vec{r}}           \times             \frac{\vec{J}\left(\vec{r}_s\right)}{\left|\vec{r}-\vec{r}_s\right|}        \space dV\left(\vec{r}_s\right) $$ Now if I take the curl of both side, I get: $$ \nabla_{\vec{r}} \times \vec{B} \left( \vec{r} \right) =     \frac{\mu_0}{4\pi}       \iiint_{V_s}           \nabla_{\vec{r}} \times           \left(           \nabla_{\vec{r}}           \times            \frac{\vec{J}\left(\vec{r}_s\right)}{\left|\vec{r}-\vec{r}_s\right|}           \right)       \space dV\left(\vec{r}_s\right) $$ Which is: $$    \frac{\mu_0}{4\pi}       \iiint_{V_s}           \left(                     \nabla_{\vec{r}}\left(\nabla_{\vec{r}} \cdot \frac{\vec{J}\left(\vec{r}_s\right)}{\left|\vec{r}-\vec{r}_s\right|} \right)            -  {\nabla^2}_{\vec{r}} \frac{\vec{J}\left(\vec{r}_s\right)}{\left|\vec{r}-\vec{r}_s\right|}           \right)       \space dV\left(\vec{r}_s\right) $$ Which, because $\vec{J}\left(\vec{r}_s\right)$ doesn't depend on $\vec{r}$, is: $$    \frac{\mu_0}{4\pi}       \iiint_{V_s}           \left(           \nabla_{\vec{r}}             \left(               \vec{J}\left(\vec{r}_s\right) \cdot \nabla_{\vec{r}} \frac {1} {\left|\vec{r}-\vec{r}_s\right|}             \right)           + \vec{J}\left(\vec{r}_s\right) \space 4\pi \delta^3 \left(\vec{r}-\vec{r}_s\right)           \right)       \space dV\left(\vec{r}_s\right) $$ Which is: $$    \left[    \frac{\mu_0}{4\pi}       \iiint_{V_s}           \nabla_{\vec{r}}             \left(               \vec{J}\left(\vec{r}_s\right) \cdot \nabla_{\vec{r}} \frac {1} {\left|\vec{r}-\vec{r}_s\right|}             \right)       \space dV\left(\vec{r}_s\right)     \right]    \Large{ + \space\mu_0 \vec{J}\left(\vec{r}\right)} $$ Which is: $$     \frac{\mu_0}{4\pi}     \left[        \nabla_{\vec{r}}       \iiint_{V_s}             \left(               \vec{J}\left(\vec{r}_s\right) \cdot \nabla_{\vec{r}} \frac {1} {\left|\vec{r}-\vec{r}_s\right|}             \right)       \space dV\left(\vec{r}_s\right)       \right]       \Large{+ \space\mu_0 \vec{J}\left(\vec{r}\right)} $$ Now my question is, given $\nabla \cdot \vec{J}\left(\vec{r}_s\right) = 0$, how can I show that $ \nabla \iiint_{V_s}             \left(               \vec{J}\left(\vec{r}_s\right) \cdot \nabla \frac {1} {\left|\vec{r}-\vec{r}_s\right|}             \right)       \space dV\left(\vec{r}_s\right) $ reduces to zero? Which expression reduces to $\nabla \cdot \vec{J}\left(\vec{r}_s\right)$, and how? What is the applicable identity? Attempt So to attack the integral $ \iiint_{V_s} \left( \vec{J}\left(\vec{r}_s\right) \cdot \nabla \frac {1} {\left|\vec{r}-\vec{r}_s\right|} \right) \space dV\left(\vec{r}_s\right) $, I first switch the variable over which the gradient is taken from $\vec{r}$ to $\vec{r}_s$: $$ \nabla_{\vec{r}_s}\frac {1} {\left|\vec{r}-\vec{r}_s\right|} = -\nabla_{\vec{r}}\frac {1} {\left|\vec{r}-\vec{r}_s\right|} $$ My integral, therefore, becomes: $$  -\iiint_{V_s} \left( \vec{J}\left(\vec{r}_s\right) \cdot \nabla_{\vec{r}_s} \frac {1} {\left|\vec{r}-\vec{r}_s\right|} \right) \space dV\left(\vec{r}_s\right)  $$ Which is, doing a vector integration by parts: $$  \iiint_{V_s} \frac {\nabla_{\vec{r}_s} \cdot \vec{J}\left(\vec{r}_s\right)} {\left|\vec{r}-\vec{r}_s\right|}  dV\left(\vec{r}_s\right) -  \oint_{\partial V_s}  \frac {\vec{J}\left(\vec{r}_s\right)} {\left|\vec{r}-\vec{r}_s\right|}   \cdot d\vec{S}\left(\vec{r}_s\right) $$ The first term is zero because we know that $\nabla\cdot\vec{J}=0$. I am guessing that the second term also zero because, by definition, there is no current density outside of $V_s$. (This is correct, as confirmed by the answer below)",,"['multivariable-calculus', 'vector-analysis']"
72,Jacobian of a Composition involving a Linear Transformation,Jacobian of a Composition involving a Linear Transformation,,"Let $f:{\mathbb{R}^n} \to \mathbb{R}$. For each $z \in {\mathbb{R}^n}$ define $\tilde f\left( z \right) = f\left( x \right)$, where $x = Az + s$, for some $A \in {\mathbb{R}^{n \times n}}$, $s \in {\mathbb{R}^n}$. I want to find $\nabla \tilde f\left( z \right)$ and ${\nabla ^2}\tilde f\left( z \right)$ in terms of $\nabla f\left( x \right)$ and ${\nabla ^2}f\left( x \right)$, respectively. Please note the following: In class we defined the gradient of a function $f:{\mathbb{R}^n} \to \mathbb{R}$ w.r.t. $x=(x_{1},...,x_{n})$ as follows: $$\nabla f\left( x \right) = {\left[ {\frac{{\partial f}}{{\partial {x_1}}}\left( x \right),...,\frac{{\partial f}}{{\partial {x_n}}}\left( x \right)} \right]^T};$$ ${\nabla ^2}f\left( x \right)$ is the Hessian of $f$; I'll denote the Jacobian of $f$ by ${J_f}$. Here's what I've done: $x = \varphi \left( z \right) = Az + s$, where $A = {\left( {{a_{ij}}} \right)_{\scriptstyle1 \le i \le n\atop\scriptstyle1 \le j \le n}} $ and $s = {\left( {{s_i}} \right)_{1 \le i \le n}} $. $$\nabla \tilde f\left( z \right) = {J_{\tilde f}}{\left( z \right)^T} = {J_{f \circ \varphi }}{\left( z \right)^T} = {\left( {{J_f}\left( {\varphi \left( z \right)} \right){J_\varphi }\left( z \right)} \right)^T} = {J_\varphi }{\left( z \right)^T}{J_f}{\left( x \right)^T} = {J_\varphi }{\left( z \right)^T}\nabla f\left( x \right)$$ Now, as the i th component of $\varphi$ is $${\varphi _i \left( z \right)} = \sum\limits_{j = 1}^n {{a_{ij}}z}  + {s_i}$$ then, $\frac{{\partial {\varphi _i}}}{{\partial {z_i}}}\left( z \right) = {a_{ij}}$, for all $j = 1,...,n$ and $z \in {\mathbb{R}^n}$. Consequently, ${J_\varphi }{\left( z \right)}=A$, so I can conclude that: $$\nabla \tilde f\left( z \right) = A^T \nabla f\left( x \right).$$ The problem arises when I try to find ${\nabla ^2}\tilde f\left( z \right)$. I know that: $$\nabla \tilde f = {A^T} \circ \nabla f \circ \varphi$$ so $${\nabla ^2}\tilde f\left( z \right) = {J_{{A^T} \circ \nabla f \circ \varphi }}\left( z \right).$$ However, I don't know how to take $A^T$ out of the Jacobian. I suspect that: $${J_{{A^T} \circ \nabla f \circ \varphi }}\left( z \right) = {A^T} {J_{\nabla f \circ \varphi }}\left( z \right) = {A^T} {J_{\nabla f}}\left( {\varphi \left( z \right)} \right) {J_\varphi }\left( z \right) = {A^T} {J_{\nabla f}}\left( {\varphi \left( z \right)} \right) A = {A^T} {\nabla ^2}f\left( x \right) A$$ but I don't know to justify the first step in the last string of equalities. Is my suspicion correct? Why? I'd greatly appreciate any help. Thanks in advance.","Let $f:{\mathbb{R}^n} \to \mathbb{R}$. For each $z \in {\mathbb{R}^n}$ define $\tilde f\left( z \right) = f\left( x \right)$, where $x = Az + s$, for some $A \in {\mathbb{R}^{n \times n}}$, $s \in {\mathbb{R}^n}$. I want to find $\nabla \tilde f\left( z \right)$ and ${\nabla ^2}\tilde f\left( z \right)$ in terms of $\nabla f\left( x \right)$ and ${\nabla ^2}f\left( x \right)$, respectively. Please note the following: In class we defined the gradient of a function $f:{\mathbb{R}^n} \to \mathbb{R}$ w.r.t. $x=(x_{1},...,x_{n})$ as follows: $$\nabla f\left( x \right) = {\left[ {\frac{{\partial f}}{{\partial {x_1}}}\left( x \right),...,\frac{{\partial f}}{{\partial {x_n}}}\left( x \right)} \right]^T};$$ ${\nabla ^2}f\left( x \right)$ is the Hessian of $f$; I'll denote the Jacobian of $f$ by ${J_f}$. Here's what I've done: $x = \varphi \left( z \right) = Az + s$, where $A = {\left( {{a_{ij}}} \right)_{\scriptstyle1 \le i \le n\atop\scriptstyle1 \le j \le n}} $ and $s = {\left( {{s_i}} \right)_{1 \le i \le n}} $. $$\nabla \tilde f\left( z \right) = {J_{\tilde f}}{\left( z \right)^T} = {J_{f \circ \varphi }}{\left( z \right)^T} = {\left( {{J_f}\left( {\varphi \left( z \right)} \right){J_\varphi }\left( z \right)} \right)^T} = {J_\varphi }{\left( z \right)^T}{J_f}{\left( x \right)^T} = {J_\varphi }{\left( z \right)^T}\nabla f\left( x \right)$$ Now, as the i th component of $\varphi$ is $${\varphi _i \left( z \right)} = \sum\limits_{j = 1}^n {{a_{ij}}z}  + {s_i}$$ then, $\frac{{\partial {\varphi _i}}}{{\partial {z_i}}}\left( z \right) = {a_{ij}}$, for all $j = 1,...,n$ and $z \in {\mathbb{R}^n}$. Consequently, ${J_\varphi }{\left( z \right)}=A$, so I can conclude that: $$\nabla \tilde f\left( z \right) = A^T \nabla f\left( x \right).$$ The problem arises when I try to find ${\nabla ^2}\tilde f\left( z \right)$. I know that: $$\nabla \tilde f = {A^T} \circ \nabla f \circ \varphi$$ so $${\nabla ^2}\tilde f\left( z \right) = {J_{{A^T} \circ \nabla f \circ \varphi }}\left( z \right).$$ However, I don't know how to take $A^T$ out of the Jacobian. I suspect that: $${J_{{A^T} \circ \nabla f \circ \varphi }}\left( z \right) = {A^T} {J_{\nabla f \circ \varphi }}\left( z \right) = {A^T} {J_{\nabla f}}\left( {\varphi \left( z \right)} \right) {J_\varphi }\left( z \right) = {A^T} {J_{\nabla f}}\left( {\varphi \left( z \right)} \right) A = {A^T} {\nabla ^2}f\left( x \right) A$$ but I don't know to justify the first step in the last string of equalities. Is my suspicion correct? Why? I'd greatly appreciate any help. Thanks in advance.",,"['real-analysis', 'multivariable-calculus']"
73,Transformation of domain in Evans,Transformation of domain in Evans,,"From Evans, Partial Differential Equations, Page 53. Let $\Phi(x,s)=\frac{1}{{4\pi t}^{n/2}}e^{-\frac{|x|^{2}}{4t}}$. Evans used $E(x,t,r)$ to denote the region $$(y,s)\in \mathbb{R}^{n+1}|s\le t, \Phi(x-y,t-s)\ge \frac{1}{r^{n}}$$ In particular he used $E(r)$ to denote the region $E(0,0,r):\Phi(-y,-s)\ge \frac{1}{r^{n}}$. My question is how the following two equalities (here $u$ is an $C^{2}$ function in $y$ and $s$): $$\int\int_{E(1)}\sum^{n}_{i=1}u_{y_{i}}y_{i}\frac{|y|}{s^{2}}+2ru_{s}\frac{|y|^{2}}{s}dyds=\frac{1}{r^{n+1}}\int\int_{E(r)}\sum^{n}_{i=1}u_{y_{i}}y_{i}\frac{|y|}{s^{2}}+2u_{s}\frac{|y|^{2}}{s}dyds$$ and $$\frac{1}{r^{n}}\int\int_{E(r)}u(y,s)\frac{|y|^{2}}{s^{2}}dyds=\int\int_{E(1)}u(ry,r^{2}s)\frac{|y|^{2}}{s^{2}}dyds$$ The transformation of domain is the obvious issue at here. For any function $u(y,s)$, to transform $\int\int_{E(r)}u(y,s)$ to $\int\int_{E(1)}u(y',s')$ one need to make certain dilation and change of variables. The reason is when we change $\Phi(x,t)\rightarrow \Phi(rx,r^{2}t)$, there is an extra $\frac{1}{r^{n}}$ term such that $\frac{1}{r^{n}}\Phi(x,t)=\Phi(rx,r^{2}t)$. Therefore $\Phi(-y,-s)\ge 1$ implies $\Phi(-ry,-r^{2}s)\ge \frac{1}{r^{n}}$. So $(y,s)\rightarrow (ry,r^{2}s)$ change $E(1)$ to $E(r)$. But in the second inequality the situation is reversed. I do not really know how to reach from here to the above equalities, so I decided to ask.","From Evans, Partial Differential Equations, Page 53. Let $\Phi(x,s)=\frac{1}{{4\pi t}^{n/2}}e^{-\frac{|x|^{2}}{4t}}$. Evans used $E(x,t,r)$ to denote the region $$(y,s)\in \mathbb{R}^{n+1}|s\le t, \Phi(x-y,t-s)\ge \frac{1}{r^{n}}$$ In particular he used $E(r)$ to denote the region $E(0,0,r):\Phi(-y,-s)\ge \frac{1}{r^{n}}$. My question is how the following two equalities (here $u$ is an $C^{2}$ function in $y$ and $s$): $$\int\int_{E(1)}\sum^{n}_{i=1}u_{y_{i}}y_{i}\frac{|y|}{s^{2}}+2ru_{s}\frac{|y|^{2}}{s}dyds=\frac{1}{r^{n+1}}\int\int_{E(r)}\sum^{n}_{i=1}u_{y_{i}}y_{i}\frac{|y|}{s^{2}}+2u_{s}\frac{|y|^{2}}{s}dyds$$ and $$\frac{1}{r^{n}}\int\int_{E(r)}u(y,s)\frac{|y|^{2}}{s^{2}}dyds=\int\int_{E(1)}u(ry,r^{2}s)\frac{|y|^{2}}{s^{2}}dyds$$ The transformation of domain is the obvious issue at here. For any function $u(y,s)$, to transform $\int\int_{E(r)}u(y,s)$ to $\int\int_{E(1)}u(y',s')$ one need to make certain dilation and change of variables. The reason is when we change $\Phi(x,t)\rightarrow \Phi(rx,r^{2}t)$, there is an extra $\frac{1}{r^{n}}$ term such that $\frac{1}{r^{n}}\Phi(x,t)=\Phi(rx,r^{2}t)$. Therefore $\Phi(-y,-s)\ge 1$ implies $\Phi(-ry,-r^{2}s)\ge \frac{1}{r^{n}}$. So $(y,s)\rightarrow (ry,r^{2}s)$ change $E(1)$ to $E(r)$. But in the second inequality the situation is reversed. I do not really know how to reach from here to the above equalities, so I decided to ask.",,"['multivariable-calculus', 'partial-differential-equations', 'partial-derivative']"
74,Multivariable calculus question,Multivariable calculus question,,"Let $r(t)$ and $s(t)$ ($t\in\mathbb{R}$) be two differentiable vector functions describing the motions of two particles $R$ and $S$ respectively travelling in the same direction along the same curve. We further assume that $r(0) = s(0)$. Why is the following statement false? If $r(t)$ is smooth (i.e. $r'(t)\neq <0,0,0>$ for all $t\in\mathbb{R}$), then $s(t)$ is smooth.","Let $r(t)$ and $s(t)$ ($t\in\mathbb{R}$) be two differentiable vector functions describing the motions of two particles $R$ and $S$ respectively travelling in the same direction along the same curve. We further assume that $r(0) = s(0)$. Why is the following statement false? If $r(t)$ is smooth (i.e. $r'(t)\neq <0,0,0>$ for all $t\in\mathbb{R}$), then $s(t)$ is smooth.",,['multivariable-calculus']
75,Discrete approximations of $\nabla^2{\bf v}$,Discrete approximations of,\nabla^2{\bf v},"I am writing a Navier Stokes solver. The vector field is represented as a grid with integer coordinates I am looking at other people's computer code. I don't entirely understand the vector calculus, but if I am interpreting it correctly, I see the $\nabla^2{\bf v}$ term approximated (in the three-dimensional case) as $\left( \begin{array}{c} \sum_{i=\pm 1}v_x(x+i,y,z)+v_x(x,y+i)+v_x(x,y,z+i) \\ \sum_{i=\pm 1}v_y(x+i,y,z)+v_y(x,y+i)+v_y(x,y,z+i) \\ \sum_{i=\pm 1}v_z(x+i,y,z)+v_z(x,y+i)+v_z(x,y,z+i) \\  \end{array} \right) - 6{\bf v}(x,y,z)$ This term needs to be scaled by the resolution of the grid but it is essentially the sum of the gradients coming into the grid cell at each edge. First question, have I interpreted this correctly? Second question: what is a better approximation of this term? The above approximation derives from the simple practical fact that each cell in the grid normally has direct access only to its immediate lateral neighbours. This simplifies the code. However, constraints on the grid resolution and time step, combined with sometimes-large viscosity and diffusion parameters, means the simulation sometimes behaves badly. I would like to try a better approximation of the diffuse term, considering diagonal neighbours and neighbours further than one grid step away. (Existing numerical tricks that I have seen seem to interact badly with my boundary conditions, so I would like to build a solution from the ground up.) What should I use?","I am writing a Navier Stokes solver. The vector field is represented as a grid with integer coordinates I am looking at other people's computer code. I don't entirely understand the vector calculus, but if I am interpreting it correctly, I see the $\nabla^2{\bf v}$ term approximated (in the three-dimensional case) as $\left( \begin{array}{c} \sum_{i=\pm 1}v_x(x+i,y,z)+v_x(x,y+i)+v_x(x,y,z+i) \\ \sum_{i=\pm 1}v_y(x+i,y,z)+v_y(x,y+i)+v_y(x,y,z+i) \\ \sum_{i=\pm 1}v_z(x+i,y,z)+v_z(x,y+i)+v_z(x,y,z+i) \\  \end{array} \right) - 6{\bf v}(x,y,z)$ This term needs to be scaled by the resolution of the grid but it is essentially the sum of the gradients coming into the grid cell at each edge. First question, have I interpreted this correctly? Second question: what is a better approximation of this term? The above approximation derives from the simple practical fact that each cell in the grid normally has direct access only to its immediate lateral neighbours. This simplifies the code. However, constraints on the grid resolution and time step, combined with sometimes-large viscosity and diffusion parameters, means the simulation sometimes behaves badly. I would like to try a better approximation of the diffuse term, considering diagonal neighbours and neighbours further than one grid step away. (Existing numerical tricks that I have seen seem to interact badly with my boundary conditions, so I would like to build a solution from the ground up.) What should I use?",,"['multivariable-calculus', 'numerical-methods', 'approximation']"
76,Green's representation as a classical solution of Poisson's equation,Green's representation as a classical solution of Poisson's equation,,"If there exists Green's function $ G(x,y) $ and if we have $ u \in C^2(\Omega) $ such that $$ u(x) = \int_\Omega G(x,y)f(y)dy -  \int_{\partial\Omega}\frac{\partial G}{\partial\nu}(x,y)g(y)dS(y) $$ then under what conditions of $\Omega, f$ and $g$ will $u$ be a solution of the problem \begin{equation*}  \begin{cases}    -\Delta u = f & in\ \Omega  \\    u = g & in\ \partial\Omega   \end{cases} \end{equation*} under usual conditions of $\Omega$ and for $ g \in C^1(\partial\Omega) $ it can be checked that $ u = g $ in $ \partial\Omega$ but how to calculate Laplacian of $u$ ?","If there exists Green's function $ G(x,y) $ and if we have $ u \in C^2(\Omega) $ such that $$ u(x) = \int_\Omega G(x,y)f(y)dy -  \int_{\partial\Omega}\frac{\partial G}{\partial\nu}(x,y)g(y)dS(y) $$ then under what conditions of $\Omega, f$ and $g$ will $u$ be a solution of the problem \begin{equation*}  \begin{cases}    -\Delta u = f & in\ \Omega  \\    u = g & in\ \partial\Omega   \end{cases} \end{equation*} under usual conditions of $\Omega$ and for $ g \in C^1(\partial\Omega) $ it can be checked that $ u = g $ in $ \partial\Omega$ but how to calculate Laplacian of $u$ ?",,"['multivariable-calculus', 'partial-differential-equations']"
77,Unusual function format and its partial derivatives.,Unusual function format and its partial derivatives.,,"I came across a function of this format: $z = f(u,v)$ where $u = x^2y^2$ and $v = 5x + 1$ Because this function is not in the same format of the ones I've seen before (explicit or implicit), I don't know how to find (or even to do a single solving step!) its partial derivatives. Basically, I need to show that: $\frac{\partial^2z}{\partial x\partial y} = 4xy\frac{\partial f}{\partial u} +4x^3y^3\frac{\partial^2f}{\partial u^2} +10x^2y\frac{\partial^2f}{\partial v \partial u}$ Anyone could tell me how I should approach this type of problem? Thanks!","I came across a function of this format: $z = f(u,v)$ where $u = x^2y^2$ and $v = 5x + 1$ Because this function is not in the same format of the ones I've seen before (explicit or implicit), I don't know how to find (or even to do a single solving step!) its partial derivatives. Basically, I need to show that: $\frac{\partial^2z}{\partial x\partial y} = 4xy\frac{\partial f}{\partial u} +4x^3y^3\frac{\partial^2f}{\partial u^2} +10x^2y\frac{\partial^2f}{\partial v \partial u}$ Anyone could tell me how I should approach this type of problem? Thanks!",,"['multivariable-calculus', 'coordinate-systems']"
78,An n dimensional even function,An n dimensional even function,,"I am now investigate some behavior of n-dimensional even functions on $\Bbb R^n$. For 1-dimensional even functions, because $f(x) = f(-x)$ for all $x \in \Bbb R$, so we only have to investigate for $x \geq 0$. For 2-dimensional even functions, because $f(x_1, x_2) = f(-x_1, -x_2)$ for all $x = (x_1, x_2) \in \Bbb R^2$, we only have to investigate for the region $R = \{ (x_1, x_2) \in \Bbb R^2 \; | \;x_2 \geq -x_1 \}. $ Then how can I generalize this for $n$ dimensional  even functions? (I mean how can I genralize the region $R$ above) Please help!","I am now investigate some behavior of n-dimensional even functions on $\Bbb R^n$. For 1-dimensional even functions, because $f(x) = f(-x)$ for all $x \in \Bbb R$, so we only have to investigate for $x \geq 0$. For 2-dimensional even functions, because $f(x_1, x_2) = f(-x_1, -x_2)$ for all $x = (x_1, x_2) \in \Bbb R^2$, we only have to investigate for the region $R = \{ (x_1, x_2) \in \Bbb R^2 \; | \;x_2 \geq -x_1 \}. $ Then how can I generalize this for $n$ dimensional  even functions? (I mean how can I genralize the region $R$ above) Please help!",,"['real-analysis', 'functions', 'multivariable-calculus']"
79,Use Stokes' Theorem to evaluate integral,Use Stokes' Theorem to evaluate integral,,"Use the stroke theorem to evaluate $$ \int_C{ \vec{F} \cdot \vec{dr}} $$ where C is oriented counterclockwise as viewed from above. $$ \vec{F} =  \langle x+y^2, y+z^2, z+x^2 \rangle $$ C is the triangle with vertices (3, 0, 0) , (0, 3, 0) , and (0, 0, 3) . Approach so far I evaluated the curl of F,  $$ curl \vec{F} = \langle -2z, -2x, -2y \rangle$$ Then I want to dot this with dS, but I'm not sure what dS is? What is dS, or for that matter . What is S here. Is it the triangular region (looked down toward XY plane) that would be bounded by line y= 3 -x and y = 0 ? If so, how do I describe S in order for it to be dotted?","Use the stroke theorem to evaluate $$ \int_C{ \vec{F} \cdot \vec{dr}} $$ where C is oriented counterclockwise as viewed from above. $$ \vec{F} =  \langle x+y^2, y+z^2, z+x^2 \rangle $$ C is the triangle with vertices (3, 0, 0) , (0, 3, 0) , and (0, 0, 3) . Approach so far I evaluated the curl of F,  $$ curl \vec{F} = \langle -2z, -2x, -2y \rangle$$ Then I want to dot this with dS, but I'm not sure what dS is? What is dS, or for that matter . What is S here. Is it the triangular region (looked down toward XY plane) that would be bounded by line y= 3 -x and y = 0 ? If so, how do I describe S in order for it to be dotted?",,"['multivariable-calculus', 'vector-spaces']"
80,Pf. of weak lower semicontinuity for convex Lagrangians,Pf. of weak lower semicontinuity for convex Lagrangians,,"This question is about the proof of Theorem 1 in Chapter 8 of Evans' PDE book (p. 468 in the 2nd edition). Let $u,u_k\in\mathrm{W}^{1,q}(U)$ for all $k\in\mathbb{N}$, $U\subset\mathbb{R}^n$ be open, bounded, $L$ smooth, $1<q<\infty$ and  $$G_\epsilon=E_\epsilon\cap F_\epsilon$$ where $E_\epsilon$ measurable s.t. $|U-E_\epsilon|\le \epsilon$ and $$F_\epsilon=\left\{x\in U\,|\,|u(x)|+|Du(x)|\le\frac{1}{\epsilon}\right\}.$$ If $$u_k\rightarrow u\;\;\text{uniformly in}\;\;E_\epsilon$$ why does the limit $$ \lim_{k\rightarrow\infty}\int_{G_\epsilon} L(Du,u_k,x) dx = \int_{G_\epsilon}L(Du,u,x)dx, $$ hold? I suppose this is the reason why $F_\epsilon$ is defined as it is, but I don't see the exact connection. I would appreciate if somebody could look it up and help me out :-).","This question is about the proof of Theorem 1 in Chapter 8 of Evans' PDE book (p. 468 in the 2nd edition). Let $u,u_k\in\mathrm{W}^{1,q}(U)$ for all $k\in\mathbb{N}$, $U\subset\mathbb{R}^n$ be open, bounded, $L$ smooth, $1<q<\infty$ and  $$G_\epsilon=E_\epsilon\cap F_\epsilon$$ where $E_\epsilon$ measurable s.t. $|U-E_\epsilon|\le \epsilon$ and $$F_\epsilon=\left\{x\in U\,|\,|u(x)|+|Du(x)|\le\frac{1}{\epsilon}\right\}.$$ If $$u_k\rightarrow u\;\;\text{uniformly in}\;\;E_\epsilon$$ why does the limit $$ \lim_{k\rightarrow\infty}\int_{G_\epsilon} L(Du,u_k,x) dx = \int_{G_\epsilon}L(Du,u,x)dx, $$ hold? I suppose this is the reason why $F_\epsilon$ is defined as it is, but I don't see the exact connection. I would appreciate if somebody could look it up and help me out :-).",,"['multivariable-calculus', 'partial-differential-equations', 'calculus-of-variations']"
81,Flux of a vector field,Flux of a vector field,,"I've been trying to solve a flux integral with Gauss' theorem so a little input would be appreciated. Problem statement: Find the flux of ${\bf{F}}(x,y,z) = (x,y,z^2)$ upwards through the surface ${\bf r}(u,v) = (u \cos v, u \sin v, u), \hspace{1em} (0 \leq u \leq 2; 0 \leq v \leq \pi)$ OK. I notice that $z = u$ so $0 \leq z \leq 2$. Furthermore I notice that $x^2 + y^2 = z^2$ so $x^2 + y^2 \leq 4$. It makes sense to use cylindrical coordinates so $(0 \leq r \leq 2)$ and $(0 \leq \theta \leq 2 \pi)$. Finally $div {\bf F} = 2(z+1)$.With this in mind I set up my integral \begin{align*} 2\int ^{2 \pi} _0 \int ^2 _0 \int _0 ^2 (z+1)rdrdzd\theta &= \int ^{2 \pi} _0 \int ^2 _0[(z+1)r^2]_0 ^2 dzd\theta \\ &= 4\int ^{2 \pi} _0 \int ^2 _0 z + 1 dzd\theta\\ &= 4\int ^{2 \pi} _0  [1/2 z^2 + z]_0 ^2 d\theta \\ &= 16 \int _0 ^{2 \pi}d\theta \\ &= 32 \pi \end{align*} And I'm not sure how to continue from this point so if anyone can offer help it would be appreciated. Thanks!","I've been trying to solve a flux integral with Gauss' theorem so a little input would be appreciated. Problem statement: Find the flux of ${\bf{F}}(x,y,z) = (x,y,z^2)$ upwards through the surface ${\bf r}(u,v) = (u \cos v, u \sin v, u), \hspace{1em} (0 \leq u \leq 2; 0 \leq v \leq \pi)$ OK. I notice that $z = u$ so $0 \leq z \leq 2$. Furthermore I notice that $x^2 + y^2 = z^2$ so $x^2 + y^2 \leq 4$. It makes sense to use cylindrical coordinates so $(0 \leq r \leq 2)$ and $(0 \leq \theta \leq 2 \pi)$. Finally $div {\bf F} = 2(z+1)$.With this in mind I set up my integral \begin{align*} 2\int ^{2 \pi} _0 \int ^2 _0 \int _0 ^2 (z+1)rdrdzd\theta &= \int ^{2 \pi} _0 \int ^2 _0[(z+1)r^2]_0 ^2 dzd\theta \\ &= 4\int ^{2 \pi} _0 \int ^2 _0 z + 1 dzd\theta\\ &= 4\int ^{2 \pi} _0  [1/2 z^2 + z]_0 ^2 d\theta \\ &= 16 \int _0 ^{2 \pi}d\theta \\ &= 32 \pi \end{align*} And I'm not sure how to continue from this point so if anyone can offer help it would be appreciated. Thanks!",,['multivariable-calculus']
82,Surface integral (stokes?),Surface integral (stokes?),,"I want to solve the following problem, I want to find $$ \iint_S x \, \mathrm{d}S $$ where S is the part of the parabolic cylinder that lies inside of  the cylinder $z = x^2/2$, and in the first octant of the cylinder $x^2 + y^2 = 1$ I was obviously thinking about switching to cylindrical coordinates, but I have problems setting up the problem and finding the limits. Could I get some tips / help ? =)","I want to solve the following problem, I want to find $$ \iint_S x \, \mathrm{d}S $$ where S is the part of the parabolic cylinder that lies inside of  the cylinder $z = x^2/2$, and in the first octant of the cylinder $x^2 + y^2 = 1$ I was obviously thinking about switching to cylindrical coordinates, but I have problems setting up the problem and finding the limits. Could I get some tips / help ? =)",,"['integration', 'multivariable-calculus']"
83,Need help computing the partial derivatives of a vector function.,Need help computing the partial derivatives of a vector function.,,"I need help computing the partial derivative shown below.  I've never taken a course in vector analysis so I'm not if my previous attempts at solving the problem were even on the right track.  If someone could show me step by step what to do, that would be a great help to me. I need to compute $$ \frac{\partial{f}}{\partial{X_{0}}}$$ where $$ f(X_{0},X_{1},X_{2}) =\frac{W_{u}}{\mid\mid W_{u} \mid\mid} $$ and $$ W_{u} = a(X_{1} - X_{0}) + c(X_{2} - X_{0} ) $$ All of the variables above denoted by capital letters are vectors in $\mathbb{R}^{3}$.  I've been stuck on a couple of different aspects of this.  Firstly, is there a ""quotient rule"" for taking this type of derivative?  And does it work the same way as it does in single variable calculus?  I assumed that this was the case and went along with the problem however when I did this I eded up with an equation that didn't really make sense to me (matrices, vectors, and scalars added together).  If someone could please break down the process for solving this I would much appreciate it. Thanks!","I need help computing the partial derivative shown below.  I've never taken a course in vector analysis so I'm not if my previous attempts at solving the problem were even on the right track.  If someone could show me step by step what to do, that would be a great help to me. I need to compute $$ \frac{\partial{f}}{\partial{X_{0}}}$$ where $$ f(X_{0},X_{1},X_{2}) =\frac{W_{u}}{\mid\mid W_{u} \mid\mid} $$ and $$ W_{u} = a(X_{1} - X_{0}) + c(X_{2} - X_{0} ) $$ All of the variables above denoted by capital letters are vectors in $\mathbb{R}^{3}$.  I've been stuck on a couple of different aspects of this.  Firstly, is there a ""quotient rule"" for taking this type of derivative?  And does it work the same way as it does in single variable calculus?  I assumed that this was the case and went along with the problem however when I did this I eded up with an equation that didn't really make sense to me (matrices, vectors, and scalars added together).  If someone could please break down the process for solving this I would much appreciate it. Thanks!",,"['calculus', 'multivariable-calculus', 'derivatives', 'vector-analysis']"
84,"Resolve this double integral $\iint_D x(y+x^2)e^{y^2-x^4}dxdy$ over $D=\{ x \geq 0, x^2\leq y \leq x^2+1,2-x^2\leq y \leq 3-x^2 \}$",Resolve this double integral  over,"\iint_D x(y+x^2)e^{y^2-x^4}dxdy D=\{ x \geq 0, x^2\leq y \leq x^2+1,2-x^2\leq y \leq 3-x^2 \}","We can rewrite  $$ I=\iint_D x(y+x^2)e^{(y-x^2)(y+x^2)} dx dy $$ $$ D= \{ (x,y) \in \mathbb{R}^2:x \geq 0 \land 0 \leq y-x^2 \leq 1 \land 2 \leq y+x^2 \leq 3 \}. $$ With this new notation we can let  $$ \Phi(x,y) =  \begin{pmatrix}   y+x^2 \\   y-x^2 \end{pmatrix} = \begin{pmatrix}   u \\   v \end{pmatrix} $$ and $$ J_{\Phi}(x,y) =    \begin{vmatrix}     2x & 1 \\    -2x & 1   \end{vmatrix} = 2x +2x = 4x. $$ Hence $$\begin{align*}  I &= \int_2^3 \left( \int_0^1 x u e^{uv} \frac{1}{ |4x| } dv \right) du \\     &= \frac{1}{4} \int_2^3 \left( u  \int_0^1 e^{uv} dv \right) du \\   &= \frac{1}{4} \int_2^3 \left( u \int_0^1 e^{uv}  dv \right) du \\   &= \frac{1}{4} \int_2^3 \left( u \frac{1}{u} e^{uv} \Big\vert_{v=0}^{v=1}  \right) du \\   &= \frac{1}{4} \int_2^3 \left( e^{uv} \Big\vert_{v=0}^{v=1}  \right) du \\   &= \frac{1}{4} \int_2^3 \left( e^{u}-1  \right) du \\   &= \frac{1}{4} \left( e^{u}-u  \right) \Big\vert_{u=2}^{u=3}  \\   &= \frac{1}{4} \left( e^{3}-3 -e^2+2 \right) \\   &= \frac{1}{4} \left( e^{3}-e^2-1 \right) \\ \end{align*}$$ Is this correct? I have some problem to affirm this because if i don't consider the condition $x \geq 0 $ in the domain $D$ the integral must be zero because the integrand function is odd respect the $x-$variable and the domain $D$ is symmetric respect the $y-$axis","We can rewrite  $$ I=\iint_D x(y+x^2)e^{(y-x^2)(y+x^2)} dx dy $$ $$ D= \{ (x,y) \in \mathbb{R}^2:x \geq 0 \land 0 \leq y-x^2 \leq 1 \land 2 \leq y+x^2 \leq 3 \}. $$ With this new notation we can let  $$ \Phi(x,y) =  \begin{pmatrix}   y+x^2 \\   y-x^2 \end{pmatrix} = \begin{pmatrix}   u \\   v \end{pmatrix} $$ and $$ J_{\Phi}(x,y) =    \begin{vmatrix}     2x & 1 \\    -2x & 1   \end{vmatrix} = 2x +2x = 4x. $$ Hence $$\begin{align*}  I &= \int_2^3 \left( \int_0^1 x u e^{uv} \frac{1}{ |4x| } dv \right) du \\     &= \frac{1}{4} \int_2^3 \left( u  \int_0^1 e^{uv} dv \right) du \\   &= \frac{1}{4} \int_2^3 \left( u \int_0^1 e^{uv}  dv \right) du \\   &= \frac{1}{4} \int_2^3 \left( u \frac{1}{u} e^{uv} \Big\vert_{v=0}^{v=1}  \right) du \\   &= \frac{1}{4} \int_2^3 \left( e^{uv} \Big\vert_{v=0}^{v=1}  \right) du \\   &= \frac{1}{4} \int_2^3 \left( e^{u}-1  \right) du \\   &= \frac{1}{4} \left( e^{u}-u  \right) \Big\vert_{u=2}^{u=3}  \\   &= \frac{1}{4} \left( e^{3}-3 -e^2+2 \right) \\   &= \frac{1}{4} \left( e^{3}-e^2-1 \right) \\ \end{align*}$$ Is this correct? I have some problem to affirm this because if i don't consider the condition $x \geq 0 $ in the domain $D$ the integral must be zero because the integrand function is odd respect the $x-$variable and the domain $D$ is symmetric respect the $y-$axis",,"['integration', 'multivariable-calculus']"
85,Existence of total derivative of a function,Existence of total derivative of a function,,"Given the function  $ f(x,y) = \sqrt[3]{y}\cdot \arctan(x)$ discuss the existence and continuity of it's partial derivatives and existence of it's total derivative. Since the partial derivative $ \frac{\partial f}{\partial y} = \frac{\arctan(x)}{3\sqrt[3]{y^2}}$ has discontinuity at $y=0$, I tried to compute the partial  derivative at $(x,0)$ using the limit, which gives: $$ \lim_{t\to0} \frac{f(x,t) - f(x,0)}{t} = \lim_{t\to 0} \frac{\sqrt[3]{t}}{t}\arctan(x) = +\infty.$$ Does this mean that the partial derivative doesn't exist at $(x,0)$? So there's no total derivative at $(x,0)$ and it can be said that the function is not differentiable at $\mathbb{R}^2$ ?","Given the function  $ f(x,y) = \sqrt[3]{y}\cdot \arctan(x)$ discuss the existence and continuity of it's partial derivatives and existence of it's total derivative. Since the partial derivative $ \frac{\partial f}{\partial y} = \frac{\arctan(x)}{3\sqrt[3]{y^2}}$ has discontinuity at $y=0$, I tried to compute the partial  derivative at $(x,0)$ using the limit, which gives: $$ \lim_{t\to0} \frac{f(x,t) - f(x,0)}{t} = \lim_{t\to 0} \frac{\sqrt[3]{t}}{t}\arctan(x) = +\infty.$$ Does this mean that the partial derivative doesn't exist at $(x,0)$? So there's no total derivative at $(x,0)$ and it can be said that the function is not differentiable at $\mathbb{R}^2$ ?",,"['multivariable-calculus', 'derivatives']"
86,"Why $(\frac{\partial x}{\partial z})_w$ can be expressed as-$\frac{\frac{\partial (F,G)}{\partial (z,y)}}{\frac{\partial (F,G)}{\partial (x,y)}}$",Why  can be expressed as-,"(\frac{\partial x}{\partial z})_w \frac{\frac{\partial (F,G)}{\partial (z,y)}}{\frac{\partial (F,G)}{\partial (x,y)}}","Consider $F(x,y,z,w)=0$ and $G(x,y,z,w)=0$ . Why $(\frac{\partial x}{\partial z})_w$= -$\frac{\frac{\partial (F,G)}{\partial (z,y)}}{\frac{\partial (F,G)}{\partial (x,y)}}$.Noted that the  $(\frac{\partial x}{\partial z})_w$= -$\frac{\frac{\partial (F,G)}{\partial (z,y)}}{\frac{\partial (F,G)}{\partial (x,y)}}$=-$\frac{F_3G_2-F_2G_3}{F_1G_2-F_2G_1}$","Consider $F(x,y,z,w)=0$ and $G(x,y,z,w)=0$ . Why $(\frac{\partial x}{\partial z})_w$= -$\frac{\frac{\partial (F,G)}{\partial (z,y)}}{\frac{\partial (F,G)}{\partial (x,y)}}$.Noted that the  $(\frac{\partial x}{\partial z})_w$= -$\frac{\frac{\partial (F,G)}{\partial (z,y)}}{\frac{\partial (F,G)}{\partial (x,y)}}$=-$\frac{F_3G_2-F_2G_3}{F_1G_2-F_2G_1}$",,['multivariable-calculus']
87,Distance saved by shortening the road,Distance saved by shortening the road,,A long distance drivers technique for saving time is to drive the width of the road you are on.  To do this you drive so that you are placing you car on the inside of all turns as much as possible.  I want to know how much distance is saved. From the point of view of a math problem it could be presented as two parralel  curves in 3space forming a ribbon and you want to know the difference in length of the edges compared to a radius minimizing path.  The problem can have a simplified presentation of two curves in a plane. I am not sure of the best equation form for the edges to make it possible to calculate the radius min line between.,A long distance drivers technique for saving time is to drive the width of the road you are on.  To do this you drive so that you are placing you car on the inside of all turns as much as possible.  I want to know how much distance is saved. From the point of view of a math problem it could be presented as two parralel  curves in 3space forming a ribbon and you want to know the difference in length of the edges compared to a radius minimizing path.  The problem can have a simplified presentation of two curves in a plane. I am not sure of the best equation form for the edges to make it possible to calculate the radius min line between.,,"['calculus', 'multivariable-calculus', 'optimization']"
88,What's $\partial_x^{\alpha}$ when the coordinate system changes?,What's  when the coordinate system changes?,\partial_x^{\alpha},"In ${\mathbb R}^n$, let $F$ be a smooth one-to-one mapping of $\Omega$ onto some open set $\Omega'$, where $\Omega\subset{\mathbb R}^n$ is open. Set $y=F(x)$. Assume that the Jacobian matrix $J_x=[(\partial y_i/\partial x_j)(x)]$ is nonsingular for $x\in\Omega$. We have $$\frac{\partial}{\partial x_j}=\sum\frac{\partial y_i}{\partial x_j}\frac{\partial}{\partial y_i}.$$ Here are my questions: Is there a neat way to calculate $$\frac{\partial^2}{\partial x_j\partial x_k}?$$ After several steps trial, I am completely confused.  More generally, what is $\partial_x^{\alpha}$ in terms of the $y$ coordinate system? Here $$\partial_x^{\alpha}:=\frac{\partial^{|\alpha|}}{\partial x_1^{\alpha_1}\cdots\partial x_n^{\alpha_n}}.$$","In ${\mathbb R}^n$, let $F$ be a smooth one-to-one mapping of $\Omega$ onto some open set $\Omega'$, where $\Omega\subset{\mathbb R}^n$ is open. Set $y=F(x)$. Assume that the Jacobian matrix $J_x=[(\partial y_i/\partial x_j)(x)]$ is nonsingular for $x\in\Omega$. We have $$\frac{\partial}{\partial x_j}=\sum\frac{\partial y_i}{\partial x_j}\frac{\partial}{\partial y_i}.$$ Here are my questions: Is there a neat way to calculate $$\frac{\partial^2}{\partial x_j\partial x_k}?$$ After several steps trial, I am completely confused.  More generally, what is $\partial_x^{\alpha}$ in terms of the $y$ coordinate system? Here $$\partial_x^{\alpha}:=\frac{\partial^{|\alpha|}}{\partial x_1^{\alpha_1}\cdots\partial x_n^{\alpha_n}}.$$",,[]
89,Using minors to determine type of extrema,Using minors to determine type of extrema,,"I've found this page for calculating the minors (Hauptminoren) of the Hessian matrix to determine which of the critical points of the matrix correspond to a maximum, a minimum or a saddle point. They say the matrix is positive definite if all $|q_A| > 0$ and negative if $|q_A|(-1)^k>0$, and a saddle point if otherwise. But what happens if one of $|q_A|$ is zero? No conclusion possible?","I've found this page for calculating the minors (Hauptminoren) of the Hessian matrix to determine which of the critical points of the matrix correspond to a maximum, a minimum or a saddle point. They say the matrix is positive definite if all $|q_A| > 0$ and negative if $|q_A|(-1)^k>0$, and a saddle point if otherwise. But what happens if one of $|q_A|$ is zero? No conclusion possible?",,"['calculus', 'multivariable-calculus']"
90,Expressing the Gradient in terms of an Arbitrary Scalar Product,Expressing the Gradient in terms of an Arbitrary Scalar Product,,"As part of determining the expression for the gradient in terms of an arbitrary inner product, I arrived at the following problem: Given: $y = (y^1, \dots, y^n) \in \mathbb{R}^n$ is a selected point $h = (h^1, \dots, h^n) \in \mathbb{R}^n$ is an arbitrary point. $f$ is a real-valued function defined on $\mathbb{R}^n$ $[g_{ij}]$ is a positive definite, symmetric  $n \times n$ matrix I'm trying to show that $$ \sum\limits_{j=1}^n \partial_{j}f(x_0)h^j = \sum\limits_{j,k=1}^n g_{jk}y^j h^k	  $$ implies $$ \partial_{j} f(x_0) = \sum\limits_{k=1}^n g_{jk}y^k $$ The only way I can reasonably see how to arrive at the conclusion is to reason as follows: Since the antecedent holds for arbitrary points in $\mathbb{R}^n$ it must hold for the particular $n$ points associated with the standard basis vectors $e_1, \dots, e_n$ So, for example, if we take the first point $h = (1, 0, \dots, 0)$ it follows that $\partial_{1}f(x_0) = \sum\limits_{k=1}^n g_{1k}y^k$ and so on and hence for the $j^{th}$ point we have the conclusion above. So, my questions: Is this line of reasoning correct? Is there a better or more direct way to demonstrate the conclusion? EDIT I'm updating this post to provide additional contextual information. Given that $[g_{ij}]$ is a positive-definite and symmetric $n \times n$ matrix, it can be shown that the function $$ (. | . )^g:\mathbb{R}^n \rightarrow \mathbb{R} $$ given by $$ (x | y)^g = \sum\limits_{j,k=1}^n g_{jk}y^j x^k $$ is a scalar product on $\mathbb{R}^n$.  Now, let $f$ be a function on $\mathbb{R}^n$ that is differentiable at $x_0$. By the Riesz represenation theorem  (for finite-dimensional Hilbert spaces) since $df(x_0)$ is a continuous linear form there exists a unique vector $y$ such that $df(x_0)h  = (y | h)^g \; \forall h\in \mathbb{R^n}$ This unique vector $y$ is defined to be the gradient of $f$ at $x_0$ with respect to the scalar product $(x | y)^g$ and is denoted by $y = \nabla^g f(x_0)$ I am working through the details of the proof that $$ \nabla^g f(x_0) = (g^{1k}\partial_{k}f(x_0), \dots, g^{nk}\partial_{k}f(x_0))  $$ where the repeated upper/lower indices indication summation from $1 \dots n$ and $g^{ij}$ represents the $i-j$ entry of the inverse of the matrix $[g_{ij}]$","As part of determining the expression for the gradient in terms of an arbitrary inner product, I arrived at the following problem: Given: $y = (y^1, \dots, y^n) \in \mathbb{R}^n$ is a selected point $h = (h^1, \dots, h^n) \in \mathbb{R}^n$ is an arbitrary point. $f$ is a real-valued function defined on $\mathbb{R}^n$ $[g_{ij}]$ is a positive definite, symmetric  $n \times n$ matrix I'm trying to show that $$ \sum\limits_{j=1}^n \partial_{j}f(x_0)h^j = \sum\limits_{j,k=1}^n g_{jk}y^j h^k	  $$ implies $$ \partial_{j} f(x_0) = \sum\limits_{k=1}^n g_{jk}y^k $$ The only way I can reasonably see how to arrive at the conclusion is to reason as follows: Since the antecedent holds for arbitrary points in $\mathbb{R}^n$ it must hold for the particular $n$ points associated with the standard basis vectors $e_1, \dots, e_n$ So, for example, if we take the first point $h = (1, 0, \dots, 0)$ it follows that $\partial_{1}f(x_0) = \sum\limits_{k=1}^n g_{1k}y^k$ and so on and hence for the $j^{th}$ point we have the conclusion above. So, my questions: Is this line of reasoning correct? Is there a better or more direct way to demonstrate the conclusion? EDIT I'm updating this post to provide additional contextual information. Given that $[g_{ij}]$ is a positive-definite and symmetric $n \times n$ matrix, it can be shown that the function $$ (. | . )^g:\mathbb{R}^n \rightarrow \mathbb{R} $$ given by $$ (x | y)^g = \sum\limits_{j,k=1}^n g_{jk}y^j x^k $$ is a scalar product on $\mathbb{R}^n$.  Now, let $f$ be a function on $\mathbb{R}^n$ that is differentiable at $x_0$. By the Riesz represenation theorem  (for finite-dimensional Hilbert spaces) since $df(x_0)$ is a continuous linear form there exists a unique vector $y$ such that $df(x_0)h  = (y | h)^g \; \forall h\in \mathbb{R^n}$ This unique vector $y$ is defined to be the gradient of $f$ at $x_0$ with respect to the scalar product $(x | y)^g$ and is denoted by $y = \nabla^g f(x_0)$ I am working through the details of the proof that $$ \nabla^g f(x_0) = (g^{1k}\partial_{k}f(x_0), \dots, g^{nk}\partial_{k}f(x_0))  $$ where the repeated upper/lower indices indication summation from $1 \dots n$ and $g^{ij}$ represents the $i-j$ entry of the inverse of the matrix $[g_{ij}]$",,"['linear-algebra', 'multivariable-calculus']"
91,How do I find Orthogonal Projection given two Vectors?,How do I find Orthogonal Projection given two Vectors?,,"Given two vectors, a and b, how do I find the Orthogonal Projection? I've already found the Scalar and Vector Projections. \begin{align*} \text{Scalar}&:\quad  \frac{-90 + -25 + 24}{\sqrt{9^2+5^2+8^2}};\\ \text{Vector}&:\quad \left(\left(\frac{-91}{\sqrt{170}}\right)\left(\frac{-9}{\sqrt{170}}\right), \left(\frac{-91}{\sqrt{170}}\right)\left(\frac{-5}{\sqrt{170}}\right), \left(\frac{-91}{\sqrt{170}}\right)\left(\frac{8}{\sqrt{170}}\right)\right). \end{align*} Here's the original question: Let a = (-4, -8, -4) and b = (-3, -2, 0) be vectors. Find the scalar, vector, and orthogonal projections of b onto a. Thank you for your time and help.","Given two vectors, a and b, how do I find the Orthogonal Projection? I've already found the Scalar and Vector Projections. \begin{align*} \text{Scalar}&:\quad  \frac{-90 + -25 + 24}{\sqrt{9^2+5^2+8^2}};\\ \text{Vector}&:\quad \left(\left(\frac{-91}{\sqrt{170}}\right)\left(\frac{-9}{\sqrt{170}}\right), \left(\frac{-91}{\sqrt{170}}\right)\left(\frac{-5}{\sqrt{170}}\right), \left(\frac{-91}{\sqrt{170}}\right)\left(\frac{8}{\sqrt{170}}\right)\right). \end{align*} Here's the original question: Let a = (-4, -8, -4) and b = (-3, -2, 0) be vectors. Find the scalar, vector, and orthogonal projections of b onto a. Thank you for your time and help.",,['multivariable-calculus']
92,What's wrong with my calculation for checking the divergence law?,What's wrong with my calculation for checking the divergence law?,,"Here is a problem in Griffiths Introduction to Electrodynamics as follows. Check the divergence theorem for the function $\mathbf{v} = r^2\mathbf{\hat{r}}$, using as your volume, the sphere of radius R, centered at the origin? Here is my calculation for the surface intergral part, please help me finding the error in it. $$\mathbf{v} = r^2(\sin\theta \cos\phi \ \mathbf{\hat{x}} + \sin\theta \sin\phi \ \mathbf{\hat{y}} + \cos \theta \ \mathbf{\hat{z}})$$ \begin{align} \oint_s \mathbf{v} \cdot d\mathbf{a} &= 2 \iint r^2 \cos\theta\ dxdy  \\ &= 2 \iint r^2 \cdot \frac{z}{r} dxdy \\ &= 2R \iint \sqrt{R^2 - x^2 - y^2} dxdy \\ &= 2R \int_{r=0}^{R}\sqrt{R^2 - r^2} rdr \int_{\theta=0}^{2\pi} d\theta \\ &= 4\pi R\int_{\theta=0}^{\frac{\pi}{2}}\sqrt{R^2 - R^2{\sin^2\theta}} R\sin\theta\ R\cos\theta\ d\theta  \\ &= \frac{4\pi R^4}{3} \end{align} But the right answer should be $4\pi R^4$ considering the left part of the divergence theorem $\int_V \nabla\cdot\mathbf{v}\ d\tau$. I have check my answer serveral times but could not find error, could you help me? thanks.","Here is a problem in Griffiths Introduction to Electrodynamics as follows. Check the divergence theorem for the function $\mathbf{v} = r^2\mathbf{\hat{r}}$, using as your volume, the sphere of radius R, centered at the origin? Here is my calculation for the surface intergral part, please help me finding the error in it. $$\mathbf{v} = r^2(\sin\theta \cos\phi \ \mathbf{\hat{x}} + \sin\theta \sin\phi \ \mathbf{\hat{y}} + \cos \theta \ \mathbf{\hat{z}})$$ \begin{align} \oint_s \mathbf{v} \cdot d\mathbf{a} &= 2 \iint r^2 \cos\theta\ dxdy  \\ &= 2 \iint r^2 \cdot \frac{z}{r} dxdy \\ &= 2R \iint \sqrt{R^2 - x^2 - y^2} dxdy \\ &= 2R \int_{r=0}^{R}\sqrt{R^2 - r^2} rdr \int_{\theta=0}^{2\pi} d\theta \\ &= 4\pi R\int_{\theta=0}^{\frac{\pi}{2}}\sqrt{R^2 - R^2{\sin^2\theta}} R\sin\theta\ R\cos\theta\ d\theta  \\ &= \frac{4\pi R^4}{3} \end{align} But the right answer should be $4\pi R^4$ considering the left part of the divergence theorem $\int_V \nabla\cdot\mathbf{v}\ d\tau$. I have check my answer serveral times but could not find error, could you help me? thanks.",,['multivariable-calculus']
93,"Flux of $\mathbf{F}=(z+\sin(y), xyz, 1)$ across the boundary of $V=\{(x,y,z)\in\mathbb{R}^3: 0\leq z\leq x^2+y^2\leq 2\}$",Flux of  across the boundary of,"\mathbf{F}=(z+\sin(y), xyz, 1) V=\{(x,y,z)\in\mathbb{R}^3: 0\leq z\leq x^2+y^2\leq 2\}","I have calculated the outward flux of $\mathbf{F}=(z+\sin(y), xyz, 1)$ across $V=\{(x,y,z)\in\mathbb{R}^3: 0\leq z\leq x^2+y^2\leq 2\}$ directly and using the divergence theorem and the answers I have got seem to coincide but I am still unsure if I have done the calculations with the right orientation: I should calculate the outward flux but it seems to me that $\frac{\partial\mathbf{g}}{\partial r}\times\frac{\partial\mathbf{g}}{\partial\theta}$ is pointing inside $V$ which is a contradiction; so, I would be grateful if someone would check out my work and explain to me how to verify that I have chosen the right orientation. Thanks. What I have done: Given that $\partial V=\partial V_{top}\cup\partial V_{lat}+\partial V_{bottom}$ we parametrize the top surface boundary of $V$ by $\mathbf{g}:(0,\sqrt{2})\times (0,2\pi)\to\mathbb{R}^3,\ \mathbf{g}\begin{pmatrix}r\\ \theta\end{pmatrix}=\begin{bmatrix}r\cos(\theta)\\ r\sin(\theta)\\ r^2\end{bmatrix}$ so $\frac{\partial\mathbf{g}}{\partial r}=(\cos(\theta),\sin(\theta), 2r)$ , $\frac{\partial\mathbf{g}}{\partial\theta}=(-r\sin(\theta),r\cos(\theta), 0)$ , $\frac{\partial\mathbf{g}}{\partial r}\times\frac{\partial\mathbf{g}}{\partial\theta}=(-2r^2\cos(\theta), -2r^2\sin(\theta), r)$ , the lateral surface boundary by $\mathbf{h}:[0,2\pi]\times [0,2]\to\mathbb{R}^3, \mathbf{h}\begin{pmatrix}\theta\\ z\end{pmatrix}=\begin{bmatrix}\cos(\theta)\\\sin(\theta)\\ z\end{bmatrix}$ , $\frac{\partial\mathbf{h}}{\partial\theta}=(-\sin(\theta),\cos(\theta),0), \frac{\partial\mathbf{h}}{\partial z}=(0,0,1), \frac{\partial\mathbf{h}}{\partial\theta}\times \frac{\partial\mathbf{h}}{\partial z}=(\cos(\theta),\sin(\theta),0)$ , the bottom surface by $\mathbf{f}:(0,2\pi)\times(0,\sqrt{2})\to\mathbb{R}^3,\ \mathbf{f}\begin{pmatrix}\theta\\ r\end{pmatrix}=\begin{bmatrix}r\cos(\theta)\\ r\sin(\theta)\\ 0\end{bmatrix}$ so $\frac{\partial\mathbf{f}}{\partial r}=(\cos(\theta),\sin(\theta), 0)$ , $\frac{\partial\mathbf{f}}{\partial\theta}=(-r\sin(\theta),r\cos(\theta), 0)$ , $\frac{\partial\mathbf{f}}{\partial\theta}\times\frac{\partial\mathbf{f}}{\partial r}=(0, 0, -r)$ we get \begin{align} \text{flux}_{\partial V}(\mathbf{F})&=\text{flux}_{\partial V_{top}}(\mathbf{F})+\text{flux}_{\partial V_{lat}}(\mathbf{F})+\text{flux}_{\partial V_{bottom}}(\mathbf{F})\\ &=\int_{\partial V_{top}}\mathbf{F}\left(\mathbf{g}(r,\theta)\right)\cdot\left(\frac{\partial\mathbf{g}}{\partial r}\times\frac{\partial\mathbf{g}}{\partial\theta}\right)drd\theta+\int_{\partial V_{lat}}\mathbf{F}(\mathbf{h}(\theta, z))\cdot\left(\frac{\partial\mathbf{h}}{\partial\theta}\times\frac{\partial\mathbf{h}}{\partial z}\right)d\theta dz\\ &+\int_{\partial V_{bottom}}\mathbf{F}(\mathbf{f}(\theta, r))\cdot\left(\frac{\partial\mathbf{f}}{\partial\theta}\times\frac{\partial\mathbf{f}}{\partial r}\right)d\theta dr=\\&=\int_{r=0}^{r=\sqrt{2}}\left(\int_{\theta=0}^{\theta=2\pi}\left( (r^2+\sin(r\sin(\theta)))(-2r^2\cos(\theta))+r^4\cos(\theta)\sin(\theta)(-2r^2\sin(\theta))+1\cdot r \right)dr\right)d\theta\\& +\int_{z=0}^{z=2}\left(\int_{\theta=0}^{\theta=2\pi}(z+\sin(\sin(\theta))\cos(\theta)+(\cos(\theta)\sin(\theta)z)\sin(\theta)+1\cdot 0)d\theta\right)dz\\& +\int_{r=0}^{r=\sqrt{2}}\left(\int_{\theta=0}^{\theta=2\pi}[(0+\sin(r\sin(\theta)))0+0\cdot 0+1\cdot (-r)]d\theta\right)dr\\&=\int_{r=0}^{r=\sqrt{2}} \left(\int_{\theta=0}^{\theta=2\pi}(-2r^4\cos(\theta)-2r^2\cos(\theta)\sin(2\sin(\theta))-2r^6\cos(\theta)\sin^2(\theta)+r)dr\right)d\theta+0-2\pi\\ &=\int_{r=0}^{r=\sqrt{2}}(2\pi r)dr-2\pi=2\pi-2\pi=0. \end{align} Applying the divergence theorem we get: $$\text{flux}_{\partial V}(\mathbf{F})= \iiint_V \text{div}(\mathbf{F})\,dxdydz=\int_{\theta=0}^{\theta=2\pi}\int_{r=0}^{r=\sqrt{2}}\int_{z=0}^{z=2}r\cos(\theta)r^2\cdot rdr d\theta=0.$$","I have calculated the outward flux of across directly and using the divergence theorem and the answers I have got seem to coincide but I am still unsure if I have done the calculations with the right orientation: I should calculate the outward flux but it seems to me that is pointing inside which is a contradiction; so, I would be grateful if someone would check out my work and explain to me how to verify that I have chosen the right orientation. Thanks. What I have done: Given that we parametrize the top surface boundary of by so , , , the lateral surface boundary by , , the bottom surface by so , , we get Applying the divergence theorem we get:","\mathbf{F}=(z+\sin(y), xyz, 1) V=\{(x,y,z)\in\mathbb{R}^3: 0\leq z\leq x^2+y^2\leq 2\} \frac{\partial\mathbf{g}}{\partial r}\times\frac{\partial\mathbf{g}}{\partial\theta} V \partial V=\partial V_{top}\cup\partial V_{lat}+\partial V_{bottom} V \mathbf{g}:(0,\sqrt{2})\times (0,2\pi)\to\mathbb{R}^3,\ \mathbf{g}\begin{pmatrix}r\\ \theta\end{pmatrix}=\begin{bmatrix}r\cos(\theta)\\ r\sin(\theta)\\ r^2\end{bmatrix} \frac{\partial\mathbf{g}}{\partial r}=(\cos(\theta),\sin(\theta), 2r) \frac{\partial\mathbf{g}}{\partial\theta}=(-r\sin(\theta),r\cos(\theta), 0) \frac{\partial\mathbf{g}}{\partial r}\times\frac{\partial\mathbf{g}}{\partial\theta}=(-2r^2\cos(\theta), -2r^2\sin(\theta), r) \mathbf{h}:[0,2\pi]\times [0,2]\to\mathbb{R}^3, \mathbf{h}\begin{pmatrix}\theta\\ z\end{pmatrix}=\begin{bmatrix}\cos(\theta)\\\sin(\theta)\\ z\end{bmatrix} \frac{\partial\mathbf{h}}{\partial\theta}=(-\sin(\theta),\cos(\theta),0), \frac{\partial\mathbf{h}}{\partial z}=(0,0,1), \frac{\partial\mathbf{h}}{\partial\theta}\times \frac{\partial\mathbf{h}}{\partial z}=(\cos(\theta),\sin(\theta),0) \mathbf{f}:(0,2\pi)\times(0,\sqrt{2})\to\mathbb{R}^3,\ \mathbf{f}\begin{pmatrix}\theta\\ r\end{pmatrix}=\begin{bmatrix}r\cos(\theta)\\ r\sin(\theta)\\ 0\end{bmatrix} \frac{\partial\mathbf{f}}{\partial r}=(\cos(\theta),\sin(\theta), 0) \frac{\partial\mathbf{f}}{\partial\theta}=(-r\sin(\theta),r\cos(\theta), 0) \frac{\partial\mathbf{f}}{\partial\theta}\times\frac{\partial\mathbf{f}}{\partial r}=(0, 0, -r) \begin{align}
\text{flux}_{\partial V}(\mathbf{F})&=\text{flux}_{\partial V_{top}}(\mathbf{F})+\text{flux}_{\partial V_{lat}}(\mathbf{F})+\text{flux}_{\partial V_{bottom}}(\mathbf{F})\\ &=\int_{\partial V_{top}}\mathbf{F}\left(\mathbf{g}(r,\theta)\right)\cdot\left(\frac{\partial\mathbf{g}}{\partial r}\times\frac{\partial\mathbf{g}}{\partial\theta}\right)drd\theta+\int_{\partial V_{lat}}\mathbf{F}(\mathbf{h}(\theta, z))\cdot\left(\frac{\partial\mathbf{h}}{\partial\theta}\times\frac{\partial\mathbf{h}}{\partial z}\right)d\theta dz\\ &+\int_{\partial V_{bottom}}\mathbf{F}(\mathbf{f}(\theta, r))\cdot\left(\frac{\partial\mathbf{f}}{\partial\theta}\times\frac{\partial\mathbf{f}}{\partial r}\right)d\theta dr=\\&=\int_{r=0}^{r=\sqrt{2}}\left(\int_{\theta=0}^{\theta=2\pi}\left( (r^2+\sin(r\sin(\theta)))(-2r^2\cos(\theta))+r^4\cos(\theta)\sin(\theta)(-2r^2\sin(\theta))+1\cdot r \right)dr\right)d\theta\\& +\int_{z=0}^{z=2}\left(\int_{\theta=0}^{\theta=2\pi}(z+\sin(\sin(\theta))\cos(\theta)+(\cos(\theta)\sin(\theta)z)\sin(\theta)+1\cdot 0)d\theta\right)dz\\& +\int_{r=0}^{r=\sqrt{2}}\left(\int_{\theta=0}^{\theta=2\pi}[(0+\sin(r\sin(\theta)))0+0\cdot 0+1\cdot (-r)]d\theta\right)dr\\&=\int_{r=0}^{r=\sqrt{2}} \left(\int_{\theta=0}^{\theta=2\pi}(-2r^4\cos(\theta)-2r^2\cos(\theta)\sin(2\sin(\theta))-2r^6\cos(\theta)\sin^2(\theta)+r)dr\right)d\theta+0-2\pi\\ &=\int_{r=0}^{r=\sqrt{2}}(2\pi r)dr-2\pi=2\pi-2\pi=0.
\end{align} \text{flux}_{\partial V}(\mathbf{F})=
\iiint_V \text{div}(\mathbf{F})\,dxdydz=\int_{\theta=0}^{\theta=2\pi}\int_{r=0}^{r=\sqrt{2}}\int_{z=0}^{z=2}r\cos(\theta)r^2\cdot rdr d\theta=0.","['multivariable-calculus', 'solution-verification', 'divergence-theorem']"
94,Are all continuous functions that send conics to conics in $\mathbb{R}^2$ a generalized linear fractional transform?,Are all continuous functions that send conics to conics in  a generalized linear fractional transform?,\mathbb{R}^2,"Motivation: Consider an arbitrary conic section in $\mathbb{R}^2$ given by $$ Ax^2 + Bxy + Cy^2 + Dx + Ey + F = 0 $$ Now consider the map $$\phi: \mathbb{R}^2 \rightarrow \mathbb{R}^2, \phi_{\begin{pmatrix} a & b & c \\ d & e & f \\ g & h & i \end{pmatrix} }(x,y) = \left( \frac{ax + by + c}{dx+ey+f}, \frac{gx+hy+i}{dx+ey+f} \right)  $$ This map sends a conic section to a conic section. This is easy enough to verify with algebra by observing that if for some (x,y): $$ Ax^2 + Bxy + Cy^2 + Dx + Ey + F = 0 $$ Then there exists an $x'$ and $y'$ s.t. (if $\phi$ is invertible) $$ A \phi(x')^2 + B\phi(x')\phi(y') + C\phi(y')^2 + D\phi(x') + E\phi(y') + F  = 0$$ And therefore $$ A  \left( \frac{ax' + by' + c}{dx'+ey'+f} \right)^2 + B  \left( \frac{ax' + by' + c}{dx'+ey'+f} \right) \left(  \frac{gx'+hy'+i}{dx'+ey'+f} \right) + C \left(  \frac{gx'+hy'+i}{dx'+ey'+f} \right)^2 + D \left( \frac{ax' + by' + c}{dx'+ey'+f} \right) + E  \left(  \frac{gx'+hy'+i}{dx'+ey'+f} \right)  + F  =  0 $$ But by multiplying out all the denominators and grouping like powers of $(x')^n(y')^m$ we can clearly see that $(x',y')$ itself lies on some conic section. So then (it feels clear to me that) we can conclude $\phi$ sends a conic section to a section. The question: Now is this set of $\phi$ the ONLY continuous functions $\mathbb{R}^2 \rightarrow \mathbb{R}^2$ that send conics to conics? Or do there exist some more exotic functions with this property? It does seem this family of functions is well known as the set of homographies which I found out about via this question .","Motivation: Consider an arbitrary conic section in given by Now consider the map This map sends a conic section to a conic section. This is easy enough to verify with algebra by observing that if for some (x,y): Then there exists an and s.t. (if is invertible) And therefore But by multiplying out all the denominators and grouping like powers of we can clearly see that itself lies on some conic section. So then (it feels clear to me that) we can conclude sends a conic section to a section. The question: Now is this set of the ONLY continuous functions that send conics to conics? Or do there exist some more exotic functions with this property? It does seem this family of functions is well known as the set of homographies which I found out about via this question .","\mathbb{R}^2  Ax^2 + Bxy + Cy^2 + Dx + Ey + F = 0  \phi: \mathbb{R}^2 \rightarrow \mathbb{R}^2, \phi_{\begin{pmatrix} a & b & c \\ d & e & f \\ g & h & i \end{pmatrix} }(x,y) = \left( \frac{ax + by + c}{dx+ey+f}, \frac{gx+hy+i}{dx+ey+f} \right)    Ax^2 + Bxy + Cy^2 + Dx + Ey + F = 0  x' y' \phi  A \phi(x')^2 + B\phi(x')\phi(y') + C\phi(y')^2 + D\phi(x') + E\phi(y') + F  = 0  A  \left( \frac{ax' + by' + c}{dx'+ey'+f} \right)^2 + B  \left( \frac{ax' + by' + c}{dx'+ey'+f} \right) \left(  \frac{gx'+hy'+i}{dx'+ey'+f} \right) + C \left(  \frac{gx'+hy'+i}{dx'+ey'+f} \right)^2 + D \left( \frac{ax' + by' + c}{dx'+ey'+f} \right) + E  \left(  \frac{gx'+hy'+i}{dx'+ey'+f} \right)  + F  =  0  (x')^n(y')^m (x',y') \phi \phi \mathbb{R}^2 \rightarrow \mathbb{R}^2","['real-analysis', 'complex-analysis', 'multivariable-calculus', 'algebraic-geometry', 'projective-geometry']"
95,Calculation of the volume of a solid of rotation,Calculation of the volume of a solid of rotation,,"Calculate the centroid of the homogeneous solid generated by the rotation around the y-axis of the domain in the xy-plane defined by: $$D=\left \{(x,y)\in \Bbb R^2 : x \in [1,2],0\leq y \leq \frac{1}{x}\right\}$$ I'm calculating only the ordinate of this centroid because all other coordinates will be zero. $$y_G = \frac{\iiint_V y \ dx \ dy \ dz}{(V)}$$ I'm applying the Guldinus theorem related to solids of revolution. $$V=2\pi \iint_D x \ dx \ dy \Rightarrow 2\pi \int^2_1 x \ dx \ \int^{\frac{1}{x}}_0 dy \Rightarrow 2\pi \int^2_1 dx \Rightarrow 2\pi$$ I'm doing the numerator in the same way. $$y_G = \frac{2\pi \iint_D xy \ dx \ dy}{2\pi}$$ I'm applying the Tonelli-Fubini reduction formula $$\int^2_1 x \ dx \ \int^{\frac{1}{x}}_0 y \ dy \Rightarrow \frac{1}{2} \int^2_1 x \ [y^2]^{\frac{1}{x}}_0 \ dx \Rightarrow \frac{1}{2} \int^2_1 \frac{dx}{x} = \frac{1}{2} \log|x| \Bigg|^2_1 \Rightarrow \frac{1}{2}\log2$$ My questions are: Was the exercise performed correctly? Are there other ways to do this exercise?",Calculate the centroid of the homogeneous solid generated by the rotation around the y-axis of the domain in the xy-plane defined by: I'm calculating only the ordinate of this centroid because all other coordinates will be zero. I'm applying the Guldinus theorem related to solids of revolution. I'm doing the numerator in the same way. I'm applying the Tonelli-Fubini reduction formula My questions are: Was the exercise performed correctly? Are there other ways to do this exercise?,"D=\left \{(x,y)\in \Bbb R^2 : x \in [1,2],0\leq y \leq \frac{1}{x}\right\} y_G = \frac{\iiint_V y \ dx \ dy \ dz}{(V)} V=2\pi \iint_D x \ dx \ dy \Rightarrow 2\pi \int^2_1 x \ dx \ \int^{\frac{1}{x}}_0 dy \Rightarrow 2\pi \int^2_1 dx \Rightarrow 2\pi y_G = \frac{2\pi \iint_D xy \ dx \ dy}{2\pi} \int^2_1 x \ dx \ \int^{\frac{1}{x}}_0 y \ dy \Rightarrow \frac{1}{2} \int^2_1 x \ [y^2]^{\frac{1}{x}}_0 \ dx \Rightarrow \frac{1}{2} \int^2_1 \frac{dx}{x} = \frac{1}{2} \log|x| \Bigg|^2_1 \Rightarrow \frac{1}{2}\log2","['real-analysis', 'integration', 'multivariable-calculus']"
96,Maximize $f(\mathbf n)=\dfrac{N!}{\prod_{j=1}^M n_j!}$ subject to $\sum_{j=1}^M n_j=N$ and $\sum_{j=1}^M e_jn_j=E$,Maximize  subject to  and,f(\mathbf n)=\dfrac{N!}{\prod_{j=1}^M n_j!} \sum_{j=1}^M n_j=N \sum_{j=1}^M e_jn_j=E,"The following exercise is a recap on probability and maths for statistical mechanics: Maximize $$f(n_1,n_2,\, ...,\, n_M)=\dfrac{N!}{\prod_{j=1}^M n_j!}$$ subject to $$\sum_{j=1}^M n_j=N\ \text{ and }\ \sum_{j=1}^M e_jn_j=E,$$ with $e_j$ and $E$ constants. Physical background: This problem can be understood as a problem of $M$ states and $N$ independent particles. The aim is to determine what is the distribution of particles among the different states that maximizes the number of microstates compatible with fixed values of $N$ and $E$ . Attempt: I should impose $$\frac{\partial}{\partial n_i}(f(\mathbf n)-\lambda A(\mathbf n)-\beta B(\mathbf n))=0,\text{ for } i=\{1,\,...,M\},$$ where $A(\mathbf n)\equiv\sum_{j=1}^M n_j-N$ and $B(\mathbf n)\equiv \sum_{j=1}^M e_jn_j-E$ , and so, $$\frac{\partial}{\partial n_i}\left(\dfrac{N!}{\prod_{j=1}^M n_j!}-\sum_{j=1}^M(\lambda+\beta e_j)n_j -\lambda N-\beta E\right)=0$$ $$\dfrac{N!n_i!}{\prod_{j=1}^M n_j!}\dfrac{\partial}{\partial n_i}\left(\frac{1}{n_i!}\right)-(\lambda+\beta e_i)=0,$$ but now I'm stuck here as I don't really know how to calculate the derivative of $1/n_i!$ or even how to proceed once I do it because the derivative should include digamma or gamma functions...","The following exercise is a recap on probability and maths for statistical mechanics: Maximize subject to with and constants. Physical background: This problem can be understood as a problem of states and independent particles. The aim is to determine what is the distribution of particles among the different states that maximizes the number of microstates compatible with fixed values of and . Attempt: I should impose where and , and so, but now I'm stuck here as I don't really know how to calculate the derivative of or even how to proceed once I do it because the derivative should include digamma or gamma functions...","f(n_1,n_2,\, ...,\, n_M)=\dfrac{N!}{\prod_{j=1}^M n_j!} \sum_{j=1}^M n_j=N\ \text{ and }\ \sum_{j=1}^M e_jn_j=E, e_j E M N N E \frac{\partial}{\partial n_i}(f(\mathbf n)-\lambda A(\mathbf n)-\beta B(\mathbf n))=0,\text{ for } i=\{1,\,...,M\}, A(\mathbf n)\equiv\sum_{j=1}^M n_j-N B(\mathbf n)\equiv \sum_{j=1}^M e_jn_j-E \frac{\partial}{\partial n_i}\left(\dfrac{N!}{\prod_{j=1}^M n_j!}-\sum_{j=1}^M(\lambda+\beta e_j)n_j -\lambda N-\beta E\right)=0 \dfrac{N!n_i!}{\prod_{j=1}^M n_j!}\dfrac{\partial}{\partial n_i}\left(\frac{1}{n_i!}\right)-(\lambda+\beta e_i)=0, 1/n_i!","['combinatorics', 'multivariable-calculus', 'lagrange-multiplier', 'statistical-mechanics']"
97,Example implementing the Chain Rule in a textbook by Charles Chapman Pugh,Example implementing the Chain Rule in a textbook by Charles Chapman Pugh,,"I am asking for help interpreting an example in a textbook. The author gives two functions from different dimensions of Euclidean space, and he precisely describes the image of arbitrary elements under these functions, but he does not state the arbitrary elements in the domains. Here is the example from Chapter 5. Maybe someone has a different edition of the textbook. Let $f: \mathbb{R}^{2} \to \mathbb{R}^{3}$ and $g: \mathbb{R}^{3} \to \mathbb{R}$ be defined by $f = (x,y,z)$ and $g = w$ where $$w = w(x,y,z) = xy + yz + xz$$ and $$x = x(s,t) = st, \quad y = y(s,t) = s\cos{t} \quad z = z(s,t) = s\sin{t}.$$ a.) Find the matrices that represent the linear transformations $(\mathrm{D}f)_{p}$ and $(\mathrm{D}g)_{q}$ where $p = (s_{\circ}, t_{\circ}) = (0,1)$ and $q = f(p)$ . b.) Use the Chain Rule to calculate the $1 \times 2$ matrix $[\partial{w}/\partial{s}, \partial{w}/\partial{t}]$ that represents $(\mathrm{D}(g\circ{f}))_{p}$ . c.) Substitute the functions $x = x(s,t)$ , $y = y(s,t)$ , and $z = z(s,t)$ directly into $w = w(x,y,z)$ in order to get $[\partial{w}/\partial{s}, \partial{w}/\partial{t}]$ , verifying the answer in Part b.).","I am asking for help interpreting an example in a textbook. The author gives two functions from different dimensions of Euclidean space, and he precisely describes the image of arbitrary elements under these functions, but he does not state the arbitrary elements in the domains. Here is the example from Chapter 5. Maybe someone has a different edition of the textbook. Let and be defined by and where and a.) Find the matrices that represent the linear transformations and where and . b.) Use the Chain Rule to calculate the matrix that represents . c.) Substitute the functions , , and directly into in order to get , verifying the answer in Part b.).","f: \mathbb{R}^{2} \to \mathbb{R}^{3} g: \mathbb{R}^{3} \to \mathbb{R} f = (x,y,z) g = w w = w(x,y,z) = xy + yz + xz x = x(s,t) = st, \quad y = y(s,t) = s\cos{t} \quad z = z(s,t) = s\sin{t}. (\mathrm{D}f)_{p} (\mathrm{D}g)_{q} p = (s_{\circ}, t_{\circ}) = (0,1) q = f(p) 1 \times 2 [\partial{w}/\partial{s}, \partial{w}/\partial{t}] (\mathrm{D}(g\circ{f}))_{p} x = x(s,t) y = y(s,t) z = z(s,t) w = w(x,y,z) [\partial{w}/\partial{s}, \partial{w}/\partial{t}]","['real-analysis', 'multivariable-calculus', 'chain-rule', 'pushforward']"
98,Must the orientation of a parameterized curve $\gamma:I\subseteq\mathbb{R}\to \mathbb{R}^n$ be in the direction of the increasing parameter?,Must the orientation of a parameterized curve  be in the direction of the increasing parameter?,\gamma:I\subseteq\mathbb{R}\to \mathbb{R}^n,"In the context of evaluating the line integral of a vector field $\vec{F}:\vec{x}\in\mathbb{R}^n\mapsto \vec{F}(\vec{x})\in\mathbb{R}^n$ along a regular parameterized curve $\gamma:t\in{I}\subseteq\mathbb{R}\mapsto \vec{x}(t)\in\mathbb{R}^n$ , we define $$\int_{\gamma}\vec{F}\cdot d\vec{x} = \int_{I}\vec{F} \left(\vec{x}(t)\right)\cdot \frac{d\vec{x}}{dt}(t)dt$$ I have learned that this definition ensures its invariance under an orientation-preserving diffeomorphism $\tau:t\in I \mapsto \tau(t)\in I'$ since the chain rule provides that $$\frac{d\vec{x}}{d\tau} (t(\tau)) = \frac{d\vec{x}}{dt} (t(\tau)) \frac{dt}{d\tau} (\tau)$$ and the change in measure introduces a factor $\lvert{\frac{dt}{d\tau}(\tau)}\rvert$ such that $$\int_{\gamma} \vec{F} \cdot d\vec{x} = \int_{I'}\vec{F}(\vec{x}(t(\tau))) \cdot \left[\frac{\frac{d\vec{x}}{d\tau}(t(\tau))} {\frac{dt}{d\tau}(\tau)}\right] \lvert \frac{dt}{d\tau}(\tau)\rvert d\tau = \int_{I'}\vec{F} \left(\vec{x}(t(\tau))\right)\cdot \frac{d\vec{x}}{d\tau}(t(\tau))d\tau$$ This seems to crucially rely on $\frac{dt}{d\tau}(\tau)$ being positive on $I'$ or equivalently $\frac{d\tau}{dt}(t)$ being positive on $I$ , which I took to be the definition of $\tau(t)$ being an orientation-preserving reparameterization of the regular curve $\gamma$ . Since $\frac{d\vec{x}}{d\tau} (t(\tau)) = \frac{d\vec{x}}{dt} (t(\tau)) \frac{dt}{d\tau} (\tau)$ , I have noted that this is equivalent to the tangent vectors being parallel at any given point along the curve. My confusion then comes from what is meant by orientation. It seems that the orientation of $\gamma$ could equally be defined as being along the direction that the parameter decreases (antiparallel to the tangent vector) instead of the direction that the parameter increases (parallel to the tangent vector). Both are compatible with the notion of orientation-preserving reparameterizations. Is orientation independent of parameterization? For simple curves, does the notation $\int_{\gamma}$ only refer to integrating over the image $\{\gamma(t)\vert t\in I\}$ or the family of parameterizations of the image $\{\gamma(t)\vert t\in I\}$ with a particular orientation? If so, it is more correct to write $$\int_{\gamma}\vec{F}\cdot d\vec{x} = \pm \int_{I}\vec{F} \left(\vec{x}(t)\right)\cdot \frac{d\vec{x}}{dt}(t)dt$$ where $\pm$ is chosen such that the orientation of $\gamma$ is in the direction of increasing $\pm t$ ? For example, suppose I wanted to integrate some vector field $\vec{F}:\vec{x}\in\mathbb{R}^3\mapsto \vec{F}(\vec{x})\in\mathbb{R}^3$ along the straight line segment connecting $(1,1,1)$ to the origin, starting from $(1,1,1)$ and ending at $(0,0,0)$ . If we define $\gamma:t\in[0,1]\mapsto (t,t,t)\in \mathbb{R}^3$ , would this line integral still be denoted $\int_{\gamma}\vec{F}(\vec{x}) \cdot d\vec{x}$ or would it be necessary to write $-\int_{\gamma}\vec{F}(\vec{x}) \cdot d\vec{x}$ or even $\int_{-\gamma}\vec{F}(\vec{x}) \cdot d\vec{x}$ , where $-\gamma:t\in[0,1]\mapsto (1-t,1-t,1-t)\in \mathbb{R}^3$ ?","In the context of evaluating the line integral of a vector field along a regular parameterized curve , we define I have learned that this definition ensures its invariance under an orientation-preserving diffeomorphism since the chain rule provides that and the change in measure introduces a factor such that This seems to crucially rely on being positive on or equivalently being positive on , which I took to be the definition of being an orientation-preserving reparameterization of the regular curve . Since , I have noted that this is equivalent to the tangent vectors being parallel at any given point along the curve. My confusion then comes from what is meant by orientation. It seems that the orientation of could equally be defined as being along the direction that the parameter decreases (antiparallel to the tangent vector) instead of the direction that the parameter increases (parallel to the tangent vector). Both are compatible with the notion of orientation-preserving reparameterizations. Is orientation independent of parameterization? For simple curves, does the notation only refer to integrating over the image or the family of parameterizations of the image with a particular orientation? If so, it is more correct to write where is chosen such that the orientation of is in the direction of increasing ? For example, suppose I wanted to integrate some vector field along the straight line segment connecting to the origin, starting from and ending at . If we define , would this line integral still be denoted or would it be necessary to write or even , where ?","\vec{F}:\vec{x}\in\mathbb{R}^n\mapsto \vec{F}(\vec{x})\in\mathbb{R}^n \gamma:t\in{I}\subseteq\mathbb{R}\mapsto \vec{x}(t)\in\mathbb{R}^n \int_{\gamma}\vec{F}\cdot d\vec{x} = \int_{I}\vec{F} \left(\vec{x}(t)\right)\cdot \frac{d\vec{x}}{dt}(t)dt \tau:t\in I \mapsto \tau(t)\in I' \frac{d\vec{x}}{d\tau} (t(\tau)) = \frac{d\vec{x}}{dt} (t(\tau)) \frac{dt}{d\tau} (\tau) \lvert{\frac{dt}{d\tau}(\tau)}\rvert \int_{\gamma} \vec{F} \cdot d\vec{x} = \int_{I'}\vec{F}(\vec{x}(t(\tau))) \cdot \left[\frac{\frac{d\vec{x}}{d\tau}(t(\tau))} {\frac{dt}{d\tau}(\tau)}\right] \lvert \frac{dt}{d\tau}(\tau)\rvert d\tau = \int_{I'}\vec{F} \left(\vec{x}(t(\tau))\right)\cdot \frac{d\vec{x}}{d\tau}(t(\tau))d\tau \frac{dt}{d\tau}(\tau) I' \frac{d\tau}{dt}(t) I \tau(t) \gamma \frac{d\vec{x}}{d\tau} (t(\tau)) = \frac{d\vec{x}}{dt} (t(\tau)) \frac{dt}{d\tau} (\tau) \gamma \int_{\gamma} \{\gamma(t)\vert t\in I\} \{\gamma(t)\vert t\in I\} \int_{\gamma}\vec{F}\cdot d\vec{x} = \pm \int_{I}\vec{F} \left(\vec{x}(t)\right)\cdot \frac{d\vec{x}}{dt}(t)dt \pm \gamma \pm t \vec{F}:\vec{x}\in\mathbb{R}^3\mapsto \vec{F}(\vec{x})\in\mathbb{R}^3 (1,1,1) (1,1,1) (0,0,0) \gamma:t\in[0,1]\mapsto (t,t,t)\in \mathbb{R}^3 \int_{\gamma}\vec{F}(\vec{x}) \cdot d\vec{x} -\int_{\gamma}\vec{F}(\vec{x}) \cdot d\vec{x} \int_{-\gamma}\vec{F}(\vec{x}) \cdot d\vec{x} -\gamma:t\in[0,1]\mapsto (1-t,1-t,1-t)\in \mathbb{R}^3","['multivariable-calculus', 'curves', 'line-integrals']"
99,Question about dimensionality of radians and their function when double integrating in polar coordinates,Question about dimensionality of radians and their function when double integrating in polar coordinates,,"When integrating differential forms, we know classically that $$ dx  \, dy = -dy \, dx$$ Which geometrically comes from the orientation of an area formed by the two forms, such that the counter-clockwise motion from $dx$ to $dy$ defines a positive area, and so forth. Radians are typically given as being a dimensionless and unitless quantity, but observe the following integral expressed in polar coordinates: $$ \int_\limits{\theta} \int_\limits{r} dr \, d\theta$$ (i) Does it hold still that $dr \, d\theta = - d\theta \, dr$ ? Furthermore, while in the cartesian coordinates $dx , dy$ we can see that walking in some direction has a vector-like quality to it, which extends to tangent bundles and makes integrals obvious. (ii) If radians are truly dimensionless then how do you extend the wedge product to $d \theta$ and $dr$ ? Even neglecting integration and differentiation, are you not allowed to put a vector field on a surface given in polar coordinates? If either are true, it seems there must be a dimensional and oriented quantity associated with a displacement in the $\theta$ -direction. Neglecting all formalism, I can intuitively walk counter-clockwise around a circle (positive radian displacement) and then walk clockwise instead (negative radian displacement). What then would give radians any less legitimacy as a dimensional unit than meters or joules? Or perhaps the 360 degrees of the circle, where there's a similarity between the conversion of radians to degrees as there is for the conversion of Celsius to Fahrenheit. Thanks in advance for the help.","When integrating differential forms, we know classically that Which geometrically comes from the orientation of an area formed by the two forms, such that the counter-clockwise motion from to defines a positive area, and so forth. Radians are typically given as being a dimensionless and unitless quantity, but observe the following integral expressed in polar coordinates: (i) Does it hold still that ? Furthermore, while in the cartesian coordinates we can see that walking in some direction has a vector-like quality to it, which extends to tangent bundles and makes integrals obvious. (ii) If radians are truly dimensionless then how do you extend the wedge product to and ? Even neglecting integration and differentiation, are you not allowed to put a vector field on a surface given in polar coordinates? If either are true, it seems there must be a dimensional and oriented quantity associated with a displacement in the -direction. Neglecting all formalism, I can intuitively walk counter-clockwise around a circle (positive radian displacement) and then walk clockwise instead (negative radian displacement). What then would give radians any less legitimacy as a dimensional unit than meters or joules? Or perhaps the 360 degrees of the circle, where there's a similarity between the conversion of radians to degrees as there is for the conversion of Celsius to Fahrenheit. Thanks in advance for the help."," dx  \, dy = -dy \, dx dx dy  \int_\limits{\theta} \int_\limits{r} dr \, d\theta dr \, d\theta = - d\theta \, dr dx , dy d \theta dr \theta","['calculus', 'multivariable-calculus', 'differential-geometry', 'differential-forms']"
