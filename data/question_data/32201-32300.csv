,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,A stick of fixed length is broken into 3 pieces. Construct triangle,A stick of fixed length is broken into 3 pieces. Construct triangle,,"A stick of length 1 is broken into 3 pieces in the following way: We chose random interior point on the stick and break it into two pieces After that we choose the longest of the two pieces, choose random point on it and break it again, getting 3 pieces total. The task is to find probability that it would be possible to construct a triangle using that pieces. I've came up with the following idea: Let $X_1, X_2$ be random variable, uniformly distributed across all points on the stick ( $X_1, X_2\in [0,1]$ ). It is clear, that Probability space may be illustrated as follows (marked with yellow): And basing on triangle inequality we may highlight areas on the graph, that would suit us in terms of constructing triangle: So, we get that $|\Omega|=S_{yellow section}=\cfrac{3}{4}$ And probability we are searching for: $Pr(A)=\cfrac{S_{blue section}}{S_{yellow section}}=\cfrac{1}{3}$ . So answer here is $\cfrac{1}{3}$ , however my mate got very different result, including $log$ . Can you help me with this task/find mistake in my solution? Any response is welcome and would be appreciated a lot.","A stick of length 1 is broken into 3 pieces in the following way: We chose random interior point on the stick and break it into two pieces After that we choose the longest of the two pieces, choose random point on it and break it again, getting 3 pieces total. The task is to find probability that it would be possible to construct a triangle using that pieces. I've came up with the following idea: Let be random variable, uniformly distributed across all points on the stick ( ). It is clear, that Probability space may be illustrated as follows (marked with yellow): And basing on triangle inequality we may highlight areas on the graph, that would suit us in terms of constructing triangle: So, we get that And probability we are searching for: . So answer here is , however my mate got very different result, including . Can you help me with this task/find mistake in my solution? Any response is welcome and would be appreciated a lot.","X_1, X_2 X_1, X_2\in [0,1] |\Omega|=S_{yellow section}=\cfrac{3}{4} Pr(A)=\cfrac{S_{blue section}}{S_{yellow section}}=\cfrac{1}{3} \cfrac{1}{3} log","['probability', 'problem-solving', 'geometric-probability']"
1,Intuitive explanation for $\lim\limits_{n\to\infty}\left(\frac{\left(n!\right)^{2}}{\left(n-x\right)!\left(n+x\right)!}\right)^{n}=e^{-x^2}$,Intuitive explanation for,\lim\limits_{n\to\infty}\left(\frac{\left(n!\right)^{2}}{\left(n-x\right)!\left(n+x\right)!}\right)^{n}=e^{-x^2},"In this post I noticed (at first numerically) that: $$\lim\limits_{n\to\infty}\left(\frac{\left(n!\right)^{2}}{\left(n-x\right)!\left(n+x\right)!}\right)^{n}=e^{-x^2}$$ This can be proved by looking at the Taylor expansion $$n\ln\left(\frac{\left(n!\right)^{2}}{\left(n-x\right)!\left(n+x\right)!}\right)=-2n\sum_{k=1}^{\infty}\frac{\psi^{(2k-1)}\left(n+1\right)}{\left(2k\right)!}x^{2k}$$ and the asymptotic expansion $$\psi^{(m)}(n+1)=\left(-1\right)^{\left(m+1\right)}\sum_{k=0}^{\infty}\frac{\left(k+m-1\right)!}{k!}\frac{B_k}{n^{k+m}}$$ where we have chosen $B_1=-\frac12$ . However, this limit seems so beautiful and interesting that it produces the Gaussian function. It makes me wonder if there is a more intuitive way to understand this limit, possibly in a context of probabilities.","In this post I noticed (at first numerically) that: This can be proved by looking at the Taylor expansion and the asymptotic expansion where we have chosen . However, this limit seems so beautiful and interesting that it produces the Gaussian function. It makes me wonder if there is a more intuitive way to understand this limit, possibly in a context of probabilities.",\lim\limits_{n\to\infty}\left(\frac{\left(n!\right)^{2}}{\left(n-x\right)!\left(n+x\right)!}\right)^{n}=e^{-x^2} n\ln\left(\frac{\left(n!\right)^{2}}{\left(n-x\right)!\left(n+x\right)!}\right)=-2n\sum_{k=1}^{\infty}\frac{\psi^{(2k-1)}\left(n+1\right)}{\left(2k\right)!}x^{2k} \psi^{(m)}(n+1)=\left(-1\right)^{\left(m+1\right)}\sum_{k=0}^{\infty}\frac{\left(k+m-1\right)!}{k!}\frac{B_k}{n^{k+m}} B_1=-\frac12,"['probability', 'limits', 'gaussian']"
2,Calculate $\mathbb E(Y^2\mid X)$,Calculate,\mathbb E(Y^2\mid X),"Let $X,Y$ be random variables with the total normal distribution such that $$\mathbb EX=\mathbb EY=0, \operatorname{Var} X=1, \operatorname{Var} Y=5, \operatorname{Cov}(X,Y)=-2$$ Calculate $\mathbb E(Y^2\mid X)$ From this task I can also calculate $\mathbb E(XY)=\operatorname{Cov}(X,Y)+\mathbb EX \cdot \mathbb EY=-2$ $\mathbb EX^2 =\operatorname{Var}X+(\mathbb EX)^2 =1$ $\mathbb EY^2=5$ However, I know that $$\mathbb E(Y^2\mid X)=\int_{\Omega} Y^2 d \mathbb P_X$$ so this information is unhelpful and I don't know how to calculate $\mathbb E(Y^2\mid X)$ .","Let be random variables with the total normal distribution such that Calculate From this task I can also calculate However, I know that so this information is unhelpful and I don't know how to calculate .","X,Y \mathbb EX=\mathbb EY=0, \operatorname{Var} X=1, \operatorname{Var} Y=5, \operatorname{Cov}(X,Y)=-2 \mathbb E(Y^2\mid X) \mathbb E(XY)=\operatorname{Cov}(X,Y)+\mathbb EX \cdot \mathbb EY=-2 \mathbb EX^2 =\operatorname{Var}X+(\mathbb EX)^2 =1 \mathbb EY^2=5 \mathbb E(Y^2\mid X)=\int_{\Omega} Y^2 d \mathbb P_X \mathbb E(Y^2\mid X)","['probability', 'normal-distribution', 'conditional-expectation']"
3,"Given a deck of cards, what are the odds of consecutively drawing higher cards for a given number of draws?","Given a deck of cards, what are the odds of consecutively drawing higher cards for a given number of draws?",,"Lets say you're trying to draw n cards, each higher than the last. How likely are you to succeed in consecutively drawing higher for a given starting number & number of iterations? Somebody said to add my thoughts to the question! We've had a lot (my brother and i are trying to solve it!) so wait just a few minutes while I write it all out! So first we tried by thinking about some easy cases. Lets say you only wanted 3 consecutive higher draws. If you drew a king or a queen you're done right off the bat, the odds of that are 8/52. If you drew a Jack, you have to get the sequence of JQK to succeed. That's $4/52* 4/51*4/50$ . From this we can't yet establish too much, but we found a general way to answer this problem for three draws. If you start with a Jack, there is one way, a 10 in three ways, a 9 in six ways, an 8 in 10 ways... each drop in card number adds one more positive outcome than the last drop did (K-Q added zero, Q-J added one, J-10 added two, 10-9 added three etc).  So the equation is $1(4/52 * 4/51 * 4/50) + 3(4/52 * 4/51 * 4/50) + 6(4/52 * 4/51 * 4/50)... + 66(4/52 * 4/51 * 4/50)$ . (<- if we aren't mistaken) Around here is where we have gotten stuck as brute force gets more difficult and we don't yet have an overarching formula to apply. Any ideas of where to go from here to apply this to higher numbers of draws? BIG EDIT (unless we're super wrong... not 100% sure of proper math notation) Okay... so our thought process from where we left off is as follows: If we're drawing four cards instead of three, the sum is the same except the 66 term is lobbed off (because the 1,3,6,10... series starts a number lower [in the case of 4 cards, you can't draw a jack first] and ends at the same place, thereby eliminating one term) and the inside term gains 4/49 (signifying the probability of another drawn card). The equation would be: $1(4/52 * 4/51 * 4/50 * 4/49) + 3(4/52 * 4/51 * 4/50 * 4/49) + 6(4/52 * 4/51 * 4/50 * 4/49)... + 55(4/52 * 4/51 * 4/50 * 4/49)$ . Notice the similarities between the equation for drawing three cards and drawing four. From this, we derived the (hopefully correct) formula for any number of draws. let x = number of cards you want to draw.  let y = the value of the first card drawn. Note: in the following, $n_{-1} = 0$ $$\sum_{n=0}^{14-x}((n + n_{n-1} +1)*(\prod_{n=0}^{x}(4/(52-n))))$$ Posting for now, any questions will be addressed after we eat dinner.","Lets say you're trying to draw n cards, each higher than the last. How likely are you to succeed in consecutively drawing higher for a given starting number & number of iterations? Somebody said to add my thoughts to the question! We've had a lot (my brother and i are trying to solve it!) so wait just a few minutes while I write it all out! So first we tried by thinking about some easy cases. Lets say you only wanted 3 consecutive higher draws. If you drew a king or a queen you're done right off the bat, the odds of that are 8/52. If you drew a Jack, you have to get the sequence of JQK to succeed. That's . From this we can't yet establish too much, but we found a general way to answer this problem for three draws. If you start with a Jack, there is one way, a 10 in three ways, a 9 in six ways, an 8 in 10 ways... each drop in card number adds one more positive outcome than the last drop did (K-Q added zero, Q-J added one, J-10 added two, 10-9 added three etc).  So the equation is . (<- if we aren't mistaken) Around here is where we have gotten stuck as brute force gets more difficult and we don't yet have an overarching formula to apply. Any ideas of where to go from here to apply this to higher numbers of draws? BIG EDIT (unless we're super wrong... not 100% sure of proper math notation) Okay... so our thought process from where we left off is as follows: If we're drawing four cards instead of three, the sum is the same except the 66 term is lobbed off (because the 1,3,6,10... series starts a number lower [in the case of 4 cards, you can't draw a jack first] and ends at the same place, thereby eliminating one term) and the inside term gains 4/49 (signifying the probability of another drawn card). The equation would be: . Notice the similarities between the equation for drawing three cards and drawing four. From this, we derived the (hopefully correct) formula for any number of draws. let x = number of cards you want to draw.  let y = the value of the first card drawn. Note: in the following, Posting for now, any questions will be addressed after we eat dinner.",4/52* 4/51*4/50 1(4/52 * 4/51 * 4/50) + 3(4/52 * 4/51 * 4/50) + 6(4/52 * 4/51 * 4/50)... + 66(4/52 * 4/51 * 4/50) 1(4/52 * 4/51 * 4/50 * 4/49) + 3(4/52 * 4/51 * 4/50 * 4/49) + 6(4/52 * 4/51 * 4/50 * 4/49)... + 55(4/52 * 4/51 * 4/50 * 4/49) n_{-1} = 0 \sum_{n=0}^{14-x}((n + n_{n-1} +1)*(\prod_{n=0}^{x}(4/(52-n)))),['probability']
4,Conditional probability; two queens attack each other,Conditional probability; two queens attack each other,,"Two queens are randomly placed on a chessboard. What is the probability that they attack each other? A: two queens randomly placed on a chessboard (condition) B: they attack each other I have 2016 ways to place two queens on a chessboard or $\binom{64}{2}$ . If I fix one queen on a chessboard I am left with 63 places to put second queen. After placing first one, no matter where I place it, I have 21 places to put second queen so that it attacks the first queen(7 for diagonal, vertical and horizontal places). Intuitively, the solution would be $\frac{\binom{21}{1}}{\binom{63}{1}}$ or $\frac{21}{63}\approx0.33$ . In my textbook the solution is $\frac{241}{672}\approx0.35$ . Since this is a question from conditional probability I know I have to use this formula P(B\A)= $\frac{P(AB)}{P(A)}$ .I know P(A)=2016, but I get confused when finding intersection AB because it is very similar to B\A, for me.","Two queens are randomly placed on a chessboard. What is the probability that they attack each other? A: two queens randomly placed on a chessboard (condition) B: they attack each other I have 2016 ways to place two queens on a chessboard or . If I fix one queen on a chessboard I am left with 63 places to put second queen. After placing first one, no matter where I place it, I have 21 places to put second queen so that it attacks the first queen(7 for diagonal, vertical and horizontal places). Intuitively, the solution would be or . In my textbook the solution is . Since this is a question from conditional probability I know I have to use this formula P(B\A)= .I know P(A)=2016, but I get confused when finding intersection AB because it is very similar to B\A, for me.",\binom{64}{2} \frac{\binom{21}{1}}{\binom{63}{1}} \frac{21}{63}\approx0.33 \frac{241}{672}\approx0.35 \frac{P(AB)}{P(A)},"['probability', 'conditional-probability', 'chessboard']"
5,Expected Value of a Coin Flipping Game with Variable Coins,Expected Value of a Coin Flipping Game with Variable Coins,,"Main Question: My game works as follows: You start with 1 coin and flip it. When you move to the next round, you add another coin and repeat. You move on to the next round if the majority of the coins flipped in the current round come up heads. Otherwise, you lose the game. I've been trying to calculate the expected value of this game — the average round you get to before you lose. I've calculated that, for a given round R: $P(\text{win round R}) = \frac{1}{2^R}\sum^{R}_{k=floor(R/2)+1}{R \choose k}$ and with a simulation in Java, the expected value came out to be about $1.7229533856734633$ , but I've no clue of a closed form for this value. How would I find this expected value, analytically? Thank you! Simulation Code, if there's discrepancy between the analytic expected value and the simulated one: public static void main(String[] args) {     int total = 0;     double sim = Math.pow(2.0, 30.0);     for (int i = 0; i < sim; i++) {         total += game();     }     System.out.println((double) total / sim); }  public static int flip(int coins) {     int heads = 0;     for (int i = 0; i < coins; i++)      {         if (Math.random() >= 0.5)              heads++;     }     return heads; }  public static int game() {     int coins = 1;     while (flip(coins) > coins/2) {         coins++;     }      return coins; }","Main Question: My game works as follows: You start with 1 coin and flip it. When you move to the next round, you add another coin and repeat. You move on to the next round if the majority of the coins flipped in the current round come up heads. Otherwise, you lose the game. I've been trying to calculate the expected value of this game — the average round you get to before you lose. I've calculated that, for a given round R: and with a simulation in Java, the expected value came out to be about , but I've no clue of a closed form for this value. How would I find this expected value, analytically? Thank you! Simulation Code, if there's discrepancy between the analytic expected value and the simulated one: public static void main(String[] args) {     int total = 0;     double sim = Math.pow(2.0, 30.0);     for (int i = 0; i < sim; i++) {         total += game();     }     System.out.println((double) total / sim); }  public static int flip(int coins) {     int heads = 0;     for (int i = 0; i < coins; i++)      {         if (Math.random() >= 0.5)              heads++;     }     return heads; }  public static int game() {     int coins = 1;     while (flip(coins) > coins/2) {         coins++;     }      return coins; }",P(\text{win round R}) = \frac{1}{2^R}\sum^{R}_{k=floor(R/2)+1}{R \choose k} 1.7229533856734633,"['probability', 'expected-value']"
6,Sigma-Algebra generated by a random vector,Sigma-Algebra generated by a random vector,,"let X,Y be real-valued random variables on some probability space. Then (X,Y) is random variable with values in $(\mathbb{R}^2, \mathcal{B}(\mathbb{R}^2))$ (Borel- $\sigma$ -Algebra). Do we have $ \mathbb{F}^{(X,Y)}= \mathbb{F}^{X} \vee \mathbb{F}^{Y}$ , where $\mathbb{F}^X$ generates the sigma-algebra generated by X and so on?","let X,Y be real-valued random variables on some probability space. Then (X,Y) is random variable with values in (Borel- -Algebra). Do we have , where generates the sigma-algebra generated by X and so on?","(\mathbb{R}^2, \mathcal{B}(\mathbb{R}^2)) \sigma  \mathbb{F}^{(X,Y)}= \mathbb{F}^{X} \vee \mathbb{F}^{Y} \mathbb{F}^X","['probability', 'probability-theory']"
7,Number of trials rolling six 6-sided dice to get 6 unique values,Number of trials rolling six 6-sided dice to get 6 unique values,,"I have six 6-sided dice, and I want to roll them until I have all six distinct values (assume them to be naturals 1-6), in no particular order. The naive strategy would be to roll all 6 dice until I have all 6 values. The expected number of trials until success is $\frac{6^6}{6!} = 64.8$ . A smarter strategy would be to roll a die until its value is distinct from the values of other dice already rolled. Eg. the first die gets rolled once, its value is 3, and then I roll the second one until its value is something else than 3, say 4. Then I roll the 3rd one until its value is neither 3 nor 4, etc. This way, the expected number of trials is $\sum_{i=1}^{6}\frac{6}{i} = 14.7$ . An even smarter strategy would be to first roll all 6 dice, then pick the duplicates ‒ all except one ‒ and roll these again. Repeat untill I have all 6 values. For example: first roll: 1 1 2 4 4 4 (the order doesn't matter) pick 1 4 4, leave 1 2 4 on the table roll the 3 dice (which had values 1 4 4) again repeat this until there are values 1-6 on the table Question: what is the expected number of trials with this strategy?","I have six 6-sided dice, and I want to roll them until I have all six distinct values (assume them to be naturals 1-6), in no particular order. The naive strategy would be to roll all 6 dice until I have all 6 values. The expected number of trials until success is . A smarter strategy would be to roll a die until its value is distinct from the values of other dice already rolled. Eg. the first die gets rolled once, its value is 3, and then I roll the second one until its value is something else than 3, say 4. Then I roll the 3rd one until its value is neither 3 nor 4, etc. This way, the expected number of trials is . An even smarter strategy would be to first roll all 6 dice, then pick the duplicates ‒ all except one ‒ and roll these again. Repeat untill I have all 6 values. For example: first roll: 1 1 2 4 4 4 (the order doesn't matter) pick 1 4 4, leave 1 2 4 on the table roll the 3 dice (which had values 1 4 4) again repeat this until there are values 1-6 on the table Question: what is the expected number of trials with this strategy?",\frac{6^6}{6!} = 64.8 \sum_{i=1}^{6}\frac{6}{i} = 14.7,"['probability', 'expected-value', 'dice']"
8,Show that $P(\lim\limits_{n\to \infty} X_{n}=0 \operatorname{or} 1)=1$ and if $X_{0}=\theta$ then $P(\lim\limits_{n\to \infty} X_{n}=1)=\theta$,Show that  and if  then,P(\lim\limits_{n\to \infty} X_{n}=0 \operatorname{or} 1)=1 X_{0}=\theta P(\lim\limits_{n\to \infty} X_{n}=1)=\theta,"Let $X_{n} \in [0,1]$ be adapted to $\mathcal{F}_{n}$ and $\alpha, \beta > 0$ where $\alpha + \beta =1$ . Further: $P(X_{n+1}=\alpha + \beta X_{n} \vert \mathcal{F}_{n})=X_{n}$ or $P(X_{n+1}= \beta X_{n} \vert \mathcal{F}_{n})=1-X_{n}$ Show that $P(\lim\limits_{n\to \infty} X_{n}=0 \operatorname{or} 1)=1$ and if $X_{0}=\theta$ then $P(\lim\limits_{n\to \infty} X_{n}=1)=\theta$ I honestly do not know where to begin: $E[1_{\{X_{n+1}=\alpha + \beta X_{n}\}} \vert \mathcal{F}_{n}]=X_{n}$ $E[1_{\{X_{n+1}=\beta X_{n}\}} \vert \mathcal{F}_{n}]=1-X_{n}$ Maybe the tower property could help: $E[E[1_{\{X_{n+1}=\alpha + \beta X_{n}\}}\vert \mathcal{F}_{n+1}] \vert \mathcal{F}_{n}]=X_{n}$ But it leads to nothing.",Let be adapted to and where . Further: or Show that and if then I honestly do not know where to begin: Maybe the tower property could help: But it leads to nothing.,"X_{n} \in [0,1] \mathcal{F}_{n} \alpha, \beta > 0 \alpha + \beta =1 P(X_{n+1}=\alpha + \beta X_{n} \vert \mathcal{F}_{n})=X_{n} P(X_{n+1}= \beta X_{n} \vert \mathcal{F}_{n})=1-X_{n} P(\lim\limits_{n\to \infty} X_{n}=0 \operatorname{or} 1)=1 X_{0}=\theta P(\lim\limits_{n\to \infty} X_{n}=1)=\theta E[1_{\{X_{n+1}=\alpha + \beta X_{n}\}} \vert \mathcal{F}_{n}]=X_{n} E[1_{\{X_{n+1}=\beta X_{n}\}} \vert \mathcal{F}_{n}]=1-X_{n} E[E[1_{\{X_{n+1}=\alpha + \beta X_{n}\}}\vert \mathcal{F}_{n+1}] \vert \mathcal{F}_{n}]=X_{n}","['real-analysis', 'probability', 'probability-theory', 'measure-theory', 'martingales']"
9,What is the expectation of the order of automorphism group in Erdos-Renyi random graphs?,What is the expectation of the order of automorphism group in Erdos-Renyi random graphs?,,"Suppose $\Gamma(V, E) \sim G(n, p)$ is an Erdos-Reyi random graph with $n$ vertices and edge probability $p$ . What is the expected size of the automorphism group of $\Gamma$ ? What have I tried: Suppose, $I_{Aut(\Gamma)}$ is the indicator function of $Aut(\Gamma)$ in $Sym(V)$ . Then $|Aut(\Gamma)| = \sum_{\sigma \in Sym(V)} I_{Aut(\Gamma)}(\sigma)$ . Thus $$E(|Aut(\Gamma)|) = E(\sum_{\sigma \in Sym(V)} I_{Aut(\Gamma)}(\sigma)) = \sum_{\sigma \in Sym(V)} E(I_{Aut(\Gamma)}(\sigma)) = \sum_{\sigma \in Sym(V)} P(\sigma \in Aut(\Gamma))$$ However, I do not know how to proceed further. Here is also the analysis of the situation for small $n$ : If $n = 1$ , then $E(|Aut(\Gamma)|) = 1$ because $|Aut(\Gamma)| = 1$ almost surely. If $n = 2$ , then $E(|Aut(\Gamma)|) = 2$ because $|Aut(\Gamma)| = 2$ almost surely. If $n = 3$ , then $|Aut(\Gamma)| = 6$ with probability $p^3 + (1 - p)^3$ and $|Aut(\Gamma)| = 2$ with probability with probability $1 - (p^3 + (1 - p)^3)$ . Thus $E(|Aut(\Gamma)|) = 2 + 4(p^3 + (1 - p)^3)$ . Also note, that because the automorphism group of a graph is always isomorphic to the automorphism group of its complement graph, that value is invariant under the map $p \mapsto (1-p)$","Suppose is an Erdos-Reyi random graph with vertices and edge probability . What is the expected size of the automorphism group of ? What have I tried: Suppose, is the indicator function of in . Then . Thus However, I do not know how to proceed further. Here is also the analysis of the situation for small : If , then because almost surely. If , then because almost surely. If , then with probability and with probability with probability . Thus . Also note, that because the automorphism group of a graph is always isomorphic to the automorphism group of its complement graph, that value is invariant under the map","\Gamma(V, E) \sim G(n, p) n p \Gamma I_{Aut(\Gamma)} Aut(\Gamma) Sym(V) |Aut(\Gamma)| = \sum_{\sigma \in Sym(V)} I_{Aut(\Gamma)}(\sigma) E(|Aut(\Gamma)|) = E(\sum_{\sigma \in Sym(V)} I_{Aut(\Gamma)}(\sigma)) = \sum_{\sigma \in Sym(V)} E(I_{Aut(\Gamma)}(\sigma)) = \sum_{\sigma \in Sym(V)} P(\sigma \in Aut(\Gamma)) n n = 1 E(|Aut(\Gamma)|) = 1 |Aut(\Gamma)| = 1 n = 2 E(|Aut(\Gamma)|) = 2 |Aut(\Gamma)| = 2 n = 3 |Aut(\Gamma)| = 6 p^3 + (1 - p)^3 |Aut(\Gamma)| = 2 1 - (p^3 + (1 - p)^3) E(|Aut(\Gamma)|) = 2 + 4(p^3 + (1 - p)^3) p \mapsto (1-p)","['probability', 'group-theory', 'graph-theory', 'random-graphs', 'automorphism-group']"
10,Probability of winning games at tournament,Probability of winning games at tournament,,"I know it's a simple problem but apparently I am doing something wrong:   The probability of winning every single game at a tournament is 0.4. There is only win and lose - no draw. Find the probability of winning exactly 2 games by playing at most 6 games. Since winning and losing are mutually exclusive, the probability of losing a game is 0.6. The required probability is: Probability of playing 2 games and winning both, + Probability of playing 3 games and winning 2, + Probability of playing 4 games and winning 2, + Probability of playing 5 games and winning 2, + Probability of playing 6 games and winning 2. First one is $(0.4)^2$ Second is ${3\choose 2}(0.4)^2(0.6)^1$ then ${4\choose 2}(0.4)^2(0.6)^2$ ${5\choose 2}(0.4)^2(0.6)^3$ ${6\choose 2}(0.4)^2(0.6)^4$ I am getting a total of 1.45024 and obviously it is wrong. Correct answer is 0.76688 but I don't know what I am doing wrong! Many thanks!","I know it's a simple problem but apparently I am doing something wrong:   The probability of winning every single game at a tournament is 0.4. There is only win and lose - no draw. Find the probability of winning exactly 2 games by playing at most 6 games. Since winning and losing are mutually exclusive, the probability of losing a game is 0.6. The required probability is: Probability of playing 2 games and winning both, + Probability of playing 3 games and winning 2, + Probability of playing 4 games and winning 2, + Probability of playing 5 games and winning 2, + Probability of playing 6 games and winning 2. First one is Second is then I am getting a total of 1.45024 and obviously it is wrong. Correct answer is 0.76688 but I don't know what I am doing wrong! Many thanks!",(0.4)^2 {3\choose 2}(0.4)^2(0.6)^1 {4\choose 2}(0.4)^2(0.6)^2 {5\choose 2}(0.4)^2(0.6)^3 {6\choose 2}(0.4)^2(0.6)^4,['probability']
11,Bounding tails of sum of not identical random variables,Bounding tails of sum of not identical random variables,,"Let $\{X_i\}_{i=1}^n$ be a set of $n$ statistically independent random variables such that $\Pr(X_i=c_1)=\alpha/i$ , and $\Pr(X_i=c_2/i)=1-\alpha/i$ , and the constants $c_1<0,c_2>0$ , and $\alpha>0$ , are such that the expectation of each random variable $\mathbb{E}(X_i) \leq -\beta/i$ , for some $\beta>0$ . I want to upper bound the following probability $$ \Pr\left(\sum_{i=1}^nX_i>C\right), $$ for some value of $C>0$ . I am also interested in upper bounding $$ \Pr\left(\max_{1\leq j\leq n}\sum_{i=1}^jX_i>C\right). $$ Note that $\sum_{i=1}^n\mathbb{E}(X_i) = -\beta\sum_{i=1}^n1/i$ , and so it diverges to $-\infty$ logarithmically in $n$ . Since the random variables are highly non-identical I am not sure how to derive meaningful upper bound on these probabilities. I'm interested in any non-trivial (i.e., strictly less than $1$ ). I tried to use Hoeffding's inequality but it seems that it is not the ""right tool"" to use in this scenario.","Let be a set of statistically independent random variables such that , and , and the constants , and , are such that the expectation of each random variable , for some . I want to upper bound the following probability for some value of . I am also interested in upper bounding Note that , and so it diverges to logarithmically in . Since the random variables are highly non-identical I am not sure how to derive meaningful upper bound on these probabilities. I'm interested in any non-trivial (i.e., strictly less than ). I tried to use Hoeffding's inequality but it seems that it is not the ""right tool"" to use in this scenario.","\{X_i\}_{i=1}^n n \Pr(X_i=c_1)=\alpha/i \Pr(X_i=c_2/i)=1-\alpha/i c_1<0,c_2>0 \alpha>0 \mathbb{E}(X_i) \leq -\beta/i \beta>0 
\Pr\left(\sum_{i=1}^nX_i>C\right),
 C>0 
\Pr\left(\max_{1\leq j\leq n}\sum_{i=1}^jX_i>C\right).
 \sum_{i=1}^n\mathbb{E}(X_i) = -\beta\sum_{i=1}^n1/i -\infty n 1","['probability', 'probability-theory', 'concentration-of-measure']"
12,What's the probability that the teacher teaches her class?,What's the probability that the teacher teaches her class?,,"Hi, thanks for reading! I really need help with this question. I'll post all my progress below - I tried really hard being as thorough as possible, but if I don't meet the guidelines for how a homework question should be asked, please tell me and I'll edit my question! Progress so far: Here is what I'm thinking. Let $P(A)$ be the probability that the professor teaches the class. Let $P(B)$ be the probability that the weather is bad Let $P(S)$ be the probability that an individual student shows up, for any student. Let $P(G)=P(B^C)=(1-P(B))$ be the probability that the weather is good. The weather being good is the complement of the weather being bad. Let $p_b$ be the probability that the student shows up given that the weather is bad. Let $p_g$ be the probability that the student shows up given that the weather is bad. The probability that the weather was bad and a student shows up would be $(p_{b})P(B)$ The probability that the weather was bad and a student shows up would be $(p_g)(1-p(B))$ Let $n$ be the number of students in the class. Let $k$ be the minimum number of students for the teacher to teach. For a student, the probability that they show up on any given day is equal to the probability that they show up and the weather was bad or they show up and weather was good. Since the weather being good and the weather being bad are disjoint events, that means the probability that a student shows up on any given day is the sum of the two probabilities. $P(S)=P(S \cap B)+P(S \cap B^C) = p_{b}P(B) + p_{g}(1-P(B))$ Let's say we want to calculate the total probability that $j$ students show up. Then, we would need to calculate the number of ways that $j$ students CAN show up, which would be $n\choose{j}$ , and multiply it by the probability of one of the specific outcomes where $j$ out of the $n$ students showed up, which would be: $(p_{b}P(B) + p_{g}(1-P(B)))^j *(1-p_{b}P(B) - p_{g}(1-P(B)))^{n-j}$ So, the total probability that $j$ of the $n$ students show up is: $${n \choose j} (p_{b}P(B) + p_{g}(1-P(B)))^j *(1-p_{b}P(B) - p_{g}(1-P(B)))^{n-j}$$ Okay. Almost done. The professor will teach if at least $k$ of the $n$ students show up. That means she'll teach if $k$ of them show up, or $k+1$ of them show up...etc...up to if all $n$ of them show up. Each of the events: $1$ student shows up, $2$ students show up, $3$ students show up...etc...are disjoint. So, the total probability one or the other or the other or the other or....etc....of them happening is the sum of their individual probabilities. Therefore, the probability of the teacher teaching would be given by the probability that $k$ students show up + the probability that $k+1$ students show up plus the probability that $k+2$ students show up plus.....plus the probability that all $n$ students show up. $$P(A) = \sum_{j=k}^{n} {n \choose j} (p_{b}P(B) + p_{g}(1-P(B)))^j *(1-p_{b}P(B) - p_{g}(1-P(B)))^{n-j}$$ WHEW. That was a lot of writing! If you've followed me so far, thank you so much. However, that answer is wrong! Here's the correct answer: Now, the correct answer makes sense to me. However, so does mine...I can't see where I went wrong. I thought perhaps we were both saying the same thing, but writing it differently. But then I tested it out in Wolfram Alpha, and alas, the two equations give different answers. $n=10, \: k=3, \: p_b=0.4, \: p_g = 0.7, \: P(B)=0.8, \: (1-P(B))=0.2$","Hi, thanks for reading! I really need help with this question. I'll post all my progress below - I tried really hard being as thorough as possible, but if I don't meet the guidelines for how a homework question should be asked, please tell me and I'll edit my question! Progress so far: Here is what I'm thinking. Let be the probability that the professor teaches the class. Let be the probability that the weather is bad Let be the probability that an individual student shows up, for any student. Let be the probability that the weather is good. The weather being good is the complement of the weather being bad. Let be the probability that the student shows up given that the weather is bad. Let be the probability that the student shows up given that the weather is bad. The probability that the weather was bad and a student shows up would be The probability that the weather was bad and a student shows up would be Let be the number of students in the class. Let be the minimum number of students for the teacher to teach. For a student, the probability that they show up on any given day is equal to the probability that they show up and the weather was bad or they show up and weather was good. Since the weather being good and the weather being bad are disjoint events, that means the probability that a student shows up on any given day is the sum of the two probabilities. Let's say we want to calculate the total probability that students show up. Then, we would need to calculate the number of ways that students CAN show up, which would be , and multiply it by the probability of one of the specific outcomes where out of the students showed up, which would be: So, the total probability that of the students show up is: Okay. Almost done. The professor will teach if at least of the students show up. That means she'll teach if of them show up, or of them show up...etc...up to if all of them show up. Each of the events: student shows up, students show up, students show up...etc...are disjoint. So, the total probability one or the other or the other or the other or....etc....of them happening is the sum of their individual probabilities. Therefore, the probability of the teacher teaching would be given by the probability that students show up + the probability that students show up plus the probability that students show up plus.....plus the probability that all students show up. WHEW. That was a lot of writing! If you've followed me so far, thank you so much. However, that answer is wrong! Here's the correct answer: Now, the correct answer makes sense to me. However, so does mine...I can't see where I went wrong. I thought perhaps we were both saying the same thing, but writing it differently. But then I tested it out in Wolfram Alpha, and alas, the two equations give different answers.","P(A) P(B) P(S) P(G)=P(B^C)=(1-P(B)) p_b p_g (p_{b})P(B) (p_g)(1-p(B)) n k P(S)=P(S \cap B)+P(S \cap B^C) = p_{b}P(B) + p_{g}(1-P(B)) j j n\choose{j} j n (p_{b}P(B) + p_{g}(1-P(B)))^j *(1-p_{b}P(B) - p_{g}(1-P(B)))^{n-j} j n {n \choose j} (p_{b}P(B) + p_{g}(1-P(B)))^j *(1-p_{b}P(B) - p_{g}(1-P(B)))^{n-j} k n k k+1 n 1 2 3 k k+1 k+2 n P(A) = \sum_{j=k}^{n} {n \choose j} (p_{b}P(B) + p_{g}(1-P(B)))^j *(1-p_{b}P(B) - p_{g}(1-P(B)))^{n-j} n=10, \: k=3, \: p_b=0.4, \: p_g = 0.7, \: P(B)=0.8, \: (1-P(B))=0.2","['probability', 'combinatorics']"
13,Probability is unique?,Probability is unique?,,"In many problems of probability the requirement is : Find the probability of (something). But a probability is a positive numeric function s.t: Axiom 1. $P(E)=1$ ,where $E$ is sample space. Axiom 2. Let $A_1,A_2,\ldots$ be a countable (possibly countably infinite) sequence of pairwise disjoint events. then: $$ P\left(\bigcup_n A_n\right)= \sum_n P(A_n)$$ But there are many functions, satisfying these axioms, so what does it mean to find the probability of some event? Or is the probability somehow unique?","In many problems of probability the requirement is : Find the probability of (something). But a probability is a positive numeric function s.t: Axiom 1. ,where is sample space. Axiom 2. Let be a countable (possibly countably infinite) sequence of pairwise disjoint events. then: But there are many functions, satisfying these axioms, so what does it mean to find the probability of some event? Or is the probability somehow unique?","P(E)=1 E A_1,A_2,\ldots  P\left(\bigcup_n A_n\right)= \sum_n P(A_n)",['probability']
14,Compute the moment generating function of $Y = X_1X_2 + X_1X_3 + X_2X_3$,Compute the moment generating function of,Y = X_1X_2 + X_1X_3 + X_2X_3,"Suppose $X_1, X_2,$ and $X_3$ are independent and $N(0, 1)$ -distributed. Compute the moment generating function of $Y = X_1X_2 + X_1X_3 + X_2X_3$ . I know that any $X_iX_j$ with $i \not =j $ is a joint normal with variables $(x_i,x_j)$ I also know the formula of the moment generating function of a normal distribution. Furthermore, I know that if $Y_1,…, Y_n$ are independent $N(0,1)$ , that is, $Y = (Y_1,…,Y_n )´$ are $N(0,I)$ by definition, the moment generating  function of Y is given by $$e^{\frac{1}{2}\mathbf t' \mathbf t}$$ I thought about using the pdf's and the definition of a moment generating function but it proved to be a really tedious process jacked of multiple integrations. Does anyone know how to easily solve this problem with some relatively simple lines? (Especially using the multivariate normal properties and matrices)","Suppose and are independent and -distributed. Compute the moment generating function of . I know that any with is a joint normal with variables I also know the formula of the moment generating function of a normal distribution. Furthermore, I know that if are independent , that is, are by definition, the moment generating  function of Y is given by I thought about using the pdf's and the definition of a moment generating function but it proved to be a really tedious process jacked of multiple integrations. Does anyone know how to easily solve this problem with some relatively simple lines? (Especially using the multivariate normal properties and matrices)","X_1, X_2, X_3 N(0, 1) Y = X_1X_2 + X_1X_3 + X_2X_3 X_iX_j i \not =j  (x_i,x_j) Y_1,…, Y_n N(0,1) Y = (Y_1,…,Y_n )´ N(0,I) e^{\frac{1}{2}\mathbf t' \mathbf t}","['probability', 'matrices', 'probability-distributions', 'normal-distribution', 'moment-generating-functions']"
15,Probability of covering all vertices of a square,Probability of covering all vertices of a square,,"Let $~A_1 = (0,0), ~~A_2 = (1,0),~~ A_3 = (1,1)~$ and $~A_4 = (0,1)~$ be the four vertices of a square. A particle starts from the point $~A_1~$ at time $~0~$ and moves either to $~A_2~$ or to $~A_4~$ with equal probability. Similarly, in each of the subsequent steps, it randomly chooses one of its adjacent vertices and moves there. Let $~T~$ be the minimum number of steps required to cover all four vertices.  The probability $~P(T = 4)~$ is $(A) ~~~~0$ $(B) ~~~~\frac{1}{16}$ $(C) ~~~~\frac{1}{8}$ $(D) ~~~~\frac{1}{4}$ I am getting it as $~\frac{3}{4}~$ but answer is $~\frac{1}{8}~$ please help!","Let and be the four vertices of a square. A particle starts from the point at time and moves either to or to with equal probability. Similarly, in each of the subsequent steps, it randomly chooses one of its adjacent vertices and moves there. Let be the minimum number of steps required to cover all four vertices.  The probability is I am getting it as but answer is please help!","~A_1 = (0,0), ~~A_2 = (1,0),~~ A_3 = (1,1)~ ~A_4 = (0,1)~ ~A_1~ ~0~ ~A_2~ ~A_4~ ~T~ ~P(T = 4)~ (A) ~~~~0 (B) ~~~~\frac{1}{16} (C) ~~~~\frac{1}{8} (D) ~~~~\frac{1}{4} ~\frac{3}{4}~ ~\frac{1}{8}~",['probability']
16,"A probability inequality: probability that the normalized sum of i.i.d. random variables is bounded below, is bounded below","A probability inequality: probability that the normalized sum of i.i.d. random variables is bounded below, is bounded below",,"I have been told that the following fact is true. Let $X_1,X_2,X_3,\dots$ be i.i.d. random variables. Then there exists $\epsilon$ such that for all $n$ , $$\mathbb{P}\left(\frac{|X_1+\dots+X_n|}{\sqrt{n}}\geq \epsilon\right)\geq \delta.$$ Observe that $\epsilon$ does not depend on $n$ . Both $\epsilon$ and $\delta$ are $>0$ . However, I am struggling to prove this, or find any reference. The kicker is that the $X_i$ 's need not have finite mean or variance. In fact, I am interested in applying this ""fact"" to a situation where the $X_i$ 's must have infinite variance (but possibly zero mean), so elementary things like Markov or Chebyshev's inequality won't help. I am unsure on how to proceed. Any hint would be greatly appreciated! Update on progress: For the $X_i$ that I am interested in, I have deduced the condition $$X_1+X_2+\dots+X_{2^k} \sim2^{k/4}X_i.$$ I also have proved the inequality $$\mathbb{P}(|S_n|>t) \geq \frac{1}{2}\mathbb{P}(\max_j|X_j|>t)\geq\frac{1}{2}(1-e^{-n(1-F(t)+F(-t))}),$$ where $F$ is the c.d.f. of $X_i$ . By $S_n$ , I mean the $S_n=X_1+\dots+X_n$ . Thus it seems like the issue boils down to analyzing the distribution of $X_i$ .","I have been told that the following fact is true. Let be i.i.d. random variables. Then there exists such that for all , Observe that does not depend on . Both and are . However, I am struggling to prove this, or find any reference. The kicker is that the 's need not have finite mean or variance. In fact, I am interested in applying this ""fact"" to a situation where the 's must have infinite variance (but possibly zero mean), so elementary things like Markov or Chebyshev's inequality won't help. I am unsure on how to proceed. Any hint would be greatly appreciated! Update on progress: For the that I am interested in, I have deduced the condition I also have proved the inequality where is the c.d.f. of . By , I mean the . Thus it seems like the issue boils down to analyzing the distribution of .","X_1,X_2,X_3,\dots \epsilon n \mathbb{P}\left(\frac{|X_1+\dots+X_n|}{\sqrt{n}}\geq \epsilon\right)\geq \delta. \epsilon n \epsilon \delta >0 X_i X_i X_i X_1+X_2+\dots+X_{2^k} \sim2^{k/4}X_i. \mathbb{P}(|S_n|>t) \geq \frac{1}{2}\mathbb{P}(\max_j|X_j|>t)\geq\frac{1}{2}(1-e^{-n(1-F(t)+F(-t))}), F X_i S_n S_n=X_1+\dots+X_n X_i","['real-analysis', 'probability', 'probability-theory', 'statistics', 'probability-limit-theorems']"
17,When does order matter in probability,When does order matter in probability,,"In most cases, I can intuitively understand when selection order is important in probabilistic inference. However, I've come across a few cases recently where I've come unstuck. Here's an example, Alice has 2 kids and one of them is a girl. What is the probability that the other child is also a girl? You can assume that there is an equal number of males and females in the world. The answer I am given is as follows, The outcomes for two kids can be {BB, BG, GB, GG} Since it is mentioned that one of them is a girl, we can remove the BB option from the sample space. Therefore the sample space has 3 options while only one fits the second condition. Therefore the probability the second child will be a girl too is 1/3. So if the answer is correct the order of selection matters, because both BG and GB appear as possible outcomes above. On the other hand, the way I tried to answer the question was to note that we are already given that Alice has 2 kids and one of them is a girl. My logic goes that we then have only two possibilities to choose from for the 2nd child G or B, which would lead me to answer 1/2. If the question would have been, e.g. Alice wants to have two kids what is the probability that she will have a G and B. Then in my mind order would matter {BB, BG, GB, GG} but not in the above. Can someone explain to me if/why I am wrong? and in any case, I'm interested to hear your general tips for establishing whether order matters in these types of questions.","In most cases, I can intuitively understand when selection order is important in probabilistic inference. However, I've come across a few cases recently where I've come unstuck. Here's an example, Alice has 2 kids and one of them is a girl. What is the probability that the other child is also a girl? You can assume that there is an equal number of males and females in the world. The answer I am given is as follows, The outcomes for two kids can be {BB, BG, GB, GG} Since it is mentioned that one of them is a girl, we can remove the BB option from the sample space. Therefore the sample space has 3 options while only one fits the second condition. Therefore the probability the second child will be a girl too is 1/3. So if the answer is correct the order of selection matters, because both BG and GB appear as possible outcomes above. On the other hand, the way I tried to answer the question was to note that we are already given that Alice has 2 kids and one of them is a girl. My logic goes that we then have only two possibilities to choose from for the 2nd child G or B, which would lead me to answer 1/2. If the question would have been, e.g. Alice wants to have two kids what is the probability that she will have a G and B. Then in my mind order would matter {BB, BG, GB, GG} but not in the above. Can someone explain to me if/why I am wrong? and in any case, I'm interested to hear your general tips for establishing whether order matters in these types of questions.",,"['probability', 'combinatorics', 'combinations']"
18,"Probability that Brownian motion is negative in $[1, 2]$, given endpoints are positive","Probability that Brownian motion is negative in , given endpoints are positive","[1, 2]","Let $B_t$ be a Brownian motion. Compute $P(\inf_{t \in [1,2]} B_t < 0 \mid B_1 > 0, B_2 > 0)$ . This is a practice interview question I found here . My attempts are below, and I would appreciate any hints. There is a prior question [that may or may not be related to the above question] that asks one to compute $P(B_1 > 0, B_2 > 0)$ . I did this as follows: $P(B_1 > 0, B_2 > 0) = P(B_1 > 0, B_2 - B_1 > -B_1) = P(Z_1 > 0, Z_2 > -Z_1) = \frac{3}{8}$ by applying a symmetry argument to the $(Z_1, Z_2) \sim N(0, I_2)$ distribution. So I could solve the above question by computing $P(\inf_{t \in [0,1]} B_t < 0, B_1 > 0, B_2 > 0)$ . But I don't think this is helpful. I know of the reflection principle, which implies $P(\inf \{t > 0 : B_t = a\} < 1) = 2 P(B_1 \le a) = 2 \Phi(a)$ for $a < 0$ . This could be applied to the original problem via $P(\inf_{t \in [1, 2]} B_t < 0 \mid B_1 = b > 0) = 2 \Phi(-b)$ , but I'm not sure this is the way to go.","Let be a Brownian motion. Compute . This is a practice interview question I found here . My attempts are below, and I would appreciate any hints. There is a prior question [that may or may not be related to the above question] that asks one to compute . I did this as follows: by applying a symmetry argument to the distribution. So I could solve the above question by computing . But I don't think this is helpful. I know of the reflection principle, which implies for . This could be applied to the original problem via , but I'm not sure this is the way to go.","B_t P(\inf_{t \in [1,2]} B_t < 0 \mid B_1 > 0, B_2 > 0) P(B_1 > 0, B_2 > 0) P(B_1 > 0, B_2 > 0) = P(B_1 > 0, B_2 - B_1 > -B_1) = P(Z_1 > 0, Z_2 > -Z_1) = \frac{3}{8} (Z_1, Z_2) \sim N(0, I_2) P(\inf_{t \in [0,1]} B_t < 0, B_1 > 0, B_2 > 0) P(\inf \{t > 0 : B_t = a\} < 1) = 2 P(B_1 \le a) = 2 \Phi(a) a < 0 P(\inf_{t \in [1, 2]} B_t < 0 \mid B_1 = b > 0) = 2 \Phi(-b)","['probability', 'probability-theory', 'stochastic-processes', 'brownian-motion']"
19,Integration of $\int \arcsin(a \sin{x}) dx $,Integration of,\int \arcsin(a \sin{x}) dx ,"I'm an engineering student and I'm working on a probability problem. I'm trying to find out the probability that two random diagonals of two circumferences intersect. I'm considering two circumferences with the same ray r and with their center points at distance c. In according to the value of a=c/r the problem assumes different formulations anyway in any case emerge a strange integral I'm not able to solve. The integral is the following: $$\int \arcsin(a \sin{x}) dx $$ the value of the limits of integration is in according to the value of the parameter a ( for example one integral is integrated between $0$ and $\arccos (a/2)$ ). The software 'Mathematica' doesn't give me any results for this integral so I have tried to work on the integral expanding it with the definition of $\arcsin$, using by parts method and various substitutions with the hope to find out some known form that can be expressed in terms of some special function, but my efforts has been vain up to now. I have found some papers dealing with integrals involving $\ln{sin}$ that maybe have something to do with my problem.Probably I'm facing some hard mathematics that I'm not able to deal with or there is not a solution as the one I'm looking for. I'm waiting for some advice. Thank you.","I'm an engineering student and I'm working on a probability problem. I'm trying to find out the probability that two random diagonals of two circumferences intersect. I'm considering two circumferences with the same ray r and with their center points at distance c. In according to the value of a=c/r the problem assumes different formulations anyway in any case emerge a strange integral I'm not able to solve. The integral is the following: $$\int \arcsin(a \sin{x}) dx $$ the value of the limits of integration is in according to the value of the parameter a ( for example one integral is integrated between $0$ and $\arccos (a/2)$ ). The software 'Mathematica' doesn't give me any results for this integral so I have tried to work on the integral expanding it with the definition of $\arcsin$, using by parts method and various substitutions with the hope to find out some known form that can be expressed in terms of some special function, but my efforts has been vain up to now. I have found some papers dealing with integrals involving $\ln{sin}$ that maybe have something to do with my problem.Probably I'm facing some hard mathematics that I'm not able to deal with or there is not a solution as the one I'm looking for. I'm waiting for some advice. Thank you.",,"['probability', 'integration', 'special-functions']"
20,Linear Combination independent Random Variables,Linear Combination independent Random Variables,,"Let $Y$ and $Z$ two independent real valued random variables with values in $\mathbb{R}$ and let $f_Y(x)$ and $f_Z(x)$ their distributions. Let $a, b \in \mathbb{R}$.  How can the distibution $f_{aY + bZ}(x)$ of the linear combination $aY + bZ$ be simplified? My ideas: If $a, b = 1$ I know that we have the folding formula $f_{Y + Z}(x) = f_Y * f_Z(x)$. Futhermore, what about $f_{aY}(x)$?  Does $f_{aY}(x)= f_Y(x/a)$ or $f_{aY}(x)= f_Y(ax)$ hold? Can I combine this results in some way?","Let $Y$ and $Z$ two independent real valued random variables with values in $\mathbb{R}$ and let $f_Y(x)$ and $f_Z(x)$ their distributions. Let $a, b \in \mathbb{R}$.  How can the distibution $f_{aY + bZ}(x)$ of the linear combination $aY + bZ$ be simplified? My ideas: If $a, b = 1$ I know that we have the folding formula $f_{Y + Z}(x) = f_Y * f_Z(x)$. Futhermore, what about $f_{aY}(x)$?  Does $f_{aY}(x)= f_Y(x/a)$ or $f_{aY}(x)= f_Y(ax)$ hold? Can I combine this results in some way?",,"['probability', 'probability-theory', 'stochastic-processes']"
21,"Having event with probability $p$, how can I get an event with probability $\sqrt{p}$?","Having event with probability , how can I get an event with probability ?",p \sqrt{p},"I'd like a design a circuit that, given a random bit with probability $p$ to be zero, it outputs a random bit with probability $\sqrt{p}$ to be zero. Actually, I am rather looking for $\sqrt{p(1-p)}$, in case it makes things easier. Let's assume that we have an unlimited but finite stream of independent random bits and all the common logic gates (AND, OR, NOT, XOR...). If an exact solution wouldn't be possible in a finite setting. I would also be happy with an approximation, preferably one that can be arbitrarily expanded to reduce the error. I have already looked into a Taylor expansion around $p=1/2$, and it is a good option. The second degree expansion of $\sqrt{p(1-p)}$ is $\frac{1}{2} - (p - \frac{1}{2})^2 + O((p - \frac{1}{2}))^4$ which goes to $\frac{3}{4} + p(1-p)$ which is nice and easy to implement. I am here to see if there is a better solution. This question is highly related to the following question, but the answer is not general enough to satisfy my needs. Understanding what $\sqrt{p}$ means for an event of probability $p$","I'd like a design a circuit that, given a random bit with probability $p$ to be zero, it outputs a random bit with probability $\sqrt{p}$ to be zero. Actually, I am rather looking for $\sqrt{p(1-p)}$, in case it makes things easier. Let's assume that we have an unlimited but finite stream of independent random bits and all the common logic gates (AND, OR, NOT, XOR...). If an exact solution wouldn't be possible in a finite setting. I would also be happy with an approximation, preferably one that can be arbitrarily expanded to reduce the error. I have already looked into a Taylor expansion around $p=1/2$, and it is a good option. The second degree expansion of $\sqrt{p(1-p)}$ is $\frac{1}{2} - (p - \frac{1}{2})^2 + O((p - \frac{1}{2}))^4$ which goes to $\frac{3}{4} + p(1-p)$ which is nice and easy to implement. I am here to see if there is a better solution. This question is highly related to the following question, but the answer is not general enough to satisfy my needs. Understanding what $\sqrt{p}$ means for an event of probability $p$",,['probability']
22,Find the marginal distributions (PDFs) of a multivariate normal distribution,Find the marginal distributions (PDFs) of a multivariate normal distribution,,"Let $\mathbf{x}\in\Bbb{R}^n$ be a multi-variate normal vector with mean $\bar{\mathbf{x}}\in\Bbb{R}^n$ and covariance matrix $\Sigma\in\Bbb{S}_{++}^n$, where $\Bbb{S}_{++}^n$ denotes the space of all $n\times n$ symetric posotive definite matrices with real entries. Also, let $f$ denote the probability density function of $\mathbf{x}$, i.e.,  $$ f(\mathbf{x}) = \frac{1}{(2\pi)^{\frac{n}{2}}\vert\Sigma\vert^{\frac{1}{2}}} \exp \left( -\frac{1}{2} (\mathbf{x}-\bar{\mathbf{x}})^\top \Sigma^{-1} (\mathbf{x}-\bar{\mathbf{x}}) \right). $$ In general we suppose that $\Sigma$ is a non-diagonal matrix. Thus, the elements of $\mathbf{x}=(x_1,\ldots,x_j,\ldots,x_n)^\top$, i.e., the variables $x_j$, $j=1,\ldots,n$ are dependent normal variables. I am interested in the computation of the marginal densities of $x_j$, $j=1,\ldots,n$. In the corresponding Wikipedia's article , it states that: To obtain the marginal distribution over a subset of multivariate normal random variables, one only needs to drop the irrelevant variables (the variables that one wants to marginalize out) from the mean vector and the covariance matrix. It also gives the following example: Example: Let $\mathbf{x}=(x_1, x_2, x_3)^\top$ be multivariate normal random variables with mean vector $\bar{\mathbf{x}}=(\bar{x}_1,\bar{x}_2, \bar{x}_3)^\top$ and covariance matrix $\Sigma$ (standard   parametrization for multivariate normal distributions). Then the joint   distribution of $\mathbf{x}^\prime=(x_1, x_3)^\top$ is multivariate   normal with mean vector $\bar{\mathbf{x}}^\prime=(\bar{x}_1, \bar{x}_3)^\top$ and covariance matrix  $$ \Sigma^\prime = \begin{bmatrix} \Sigma_{11} & \Sigma_{13} \\  \Sigma_{31} & \Sigma_{33} \end{bmatrix}. $$ Does this mean that, if we want to obtain the marginal probability density function of each of $x_j$, for all $j=1,\ldots,n$, we just use the diagonal entries of $\Sigma$ as the variances of the respective random variables? What bugs me is that this would be the same as if $\Sigma$ was diagonal (and thus $x_j$'s independent). Does the solution lie in the parentheses of the above example (i.e., standard parametrization for multivariate normal distributions )?","Let $\mathbf{x}\in\Bbb{R}^n$ be a multi-variate normal vector with mean $\bar{\mathbf{x}}\in\Bbb{R}^n$ and covariance matrix $\Sigma\in\Bbb{S}_{++}^n$, where $\Bbb{S}_{++}^n$ denotes the space of all $n\times n$ symetric posotive definite matrices with real entries. Also, let $f$ denote the probability density function of $\mathbf{x}$, i.e.,  $$ f(\mathbf{x}) = \frac{1}{(2\pi)^{\frac{n}{2}}\vert\Sigma\vert^{\frac{1}{2}}} \exp \left( -\frac{1}{2} (\mathbf{x}-\bar{\mathbf{x}})^\top \Sigma^{-1} (\mathbf{x}-\bar{\mathbf{x}}) \right). $$ In general we suppose that $\Sigma$ is a non-diagonal matrix. Thus, the elements of $\mathbf{x}=(x_1,\ldots,x_j,\ldots,x_n)^\top$, i.e., the variables $x_j$, $j=1,\ldots,n$ are dependent normal variables. I am interested in the computation of the marginal densities of $x_j$, $j=1,\ldots,n$. In the corresponding Wikipedia's article , it states that: To obtain the marginal distribution over a subset of multivariate normal random variables, one only needs to drop the irrelevant variables (the variables that one wants to marginalize out) from the mean vector and the covariance matrix. It also gives the following example: Example: Let $\mathbf{x}=(x_1, x_2, x_3)^\top$ be multivariate normal random variables with mean vector $\bar{\mathbf{x}}=(\bar{x}_1,\bar{x}_2, \bar{x}_3)^\top$ and covariance matrix $\Sigma$ (standard   parametrization for multivariate normal distributions). Then the joint   distribution of $\mathbf{x}^\prime=(x_1, x_3)^\top$ is multivariate   normal with mean vector $\bar{\mathbf{x}}^\prime=(\bar{x}_1, \bar{x}_3)^\top$ and covariance matrix  $$ \Sigma^\prime = \begin{bmatrix} \Sigma_{11} & \Sigma_{13} \\  \Sigma_{31} & \Sigma_{33} \end{bmatrix}. $$ Does this mean that, if we want to obtain the marginal probability density function of each of $x_j$, for all $j=1,\ldots,n$, we just use the diagonal entries of $\Sigma$ as the variances of the respective random variables? What bugs me is that this would be the same as if $\Sigma$ was diagonal (and thus $x_j$'s independent). Does the solution lie in the parentheses of the above example (i.e., standard parametrization for multivariate normal distributions )?",,"['probability', 'normal-distribution', 'density-function']"
23,Generating random numbers with a skewed distribution with many possible values,Generating random numbers with a skewed distribution with many possible values,,"Possible duplicate, but not sure: Generating random numbers with skewed distribution So, I would like to generate integers bewteen $x$ and $y$ with a skewed distribution of $n$. What is the best way to do this? I believe it is different to the link above, because it only chooses between two numbers, $1$ and $46$, but I want a larger range (i.e. between 1-100), but I don't know how.","Possible duplicate, but not sure: Generating random numbers with skewed distribution So, I would like to generate integers bewteen $x$ and $y$ with a skewed distribution of $n$. What is the best way to do this? I believe it is different to the link above, because it only chooses between two numbers, $1$ and $46$, but I want a larger range (i.e. between 1-100), but I don't know how.",,"['probability', 'probability-theory', 'statistics', 'probability-distributions', 'random']"
24,Why square the term $X - \mu$ in the definiton of the variance?,Why square the term  in the definiton of the variance?,X - \mu,"Why is is the variance $\operatorname{Var}(X)$  of a random variable $X$ definied to be $\operatorname{E}[(X-\mu)^2]$? My professor said that you want the variance to be positive, but why not go for  $\operatorname{E}[|X-\mu|]$ then? (Just starting on this subject so maybe a stupid question).","Why is is the variance $\operatorname{Var}(X)$  of a random variable $X$ definied to be $\operatorname{E}[(X-\mu)^2]$? My professor said that you want the variance to be positive, but why not go for  $\operatorname{E}[|X-\mu|]$ then? (Just starting on this subject so maybe a stupid question).",,"['probability', 'statistics', 'definition', 'variance']"
25,Is the definition of conditional probability misleading,Is the definition of conditional probability misleading,,"I am learning probability and statistics from the book Mathematical Statistics and Data Analysis, 3rd Edition by Rice. However just couple pages reading I think his definition on conditional probability is incomplete and misleading. Notice that function P only takes subset of Ω, nothing else. Now here is his definition on conditional probability: The problem is that the P in left is not the same P as in right. Firstly lets ask what is A|B. If A|B is a subset of Ω, then what is the exact components of that? Its awkward to assign anythings into it because really you can't if you use the old P. P on Ω is fundamentally updated by knowing that B is true. So in old P P(B)=something, in new P P(B)=1, and all elements associated with B got updated value. In new P we can say that A|B = A ∩ B or A|B = A. But all those are not clearly stated in his definition (in his following explanation he said the sample space becomes B rather than Ω, which is even more misleading because Ω doesn't need to change). Is my understanding correct? Thanks for any help. A following question can be found in here: Fallacy on using interpretation instead of definition in computing conditional probability? (using multiplication law circularly?) (also I think its best not to define conditional probability this way. This should be a result instead of a definition.)","I am learning probability and statistics from the book Mathematical Statistics and Data Analysis, 3rd Edition by Rice. However just couple pages reading I think his definition on conditional probability is incomplete and misleading. Notice that function P only takes subset of Ω, nothing else. Now here is his definition on conditional probability: The problem is that the P in left is not the same P as in right. Firstly lets ask what is A|B. If A|B is a subset of Ω, then what is the exact components of that? Its awkward to assign anythings into it because really you can't if you use the old P. P on Ω is fundamentally updated by knowing that B is true. So in old P P(B)=something, in new P P(B)=1, and all elements associated with B got updated value. In new P we can say that A|B = A ∩ B or A|B = A. But all those are not clearly stated in his definition (in his following explanation he said the sample space becomes B rather than Ω, which is even more misleading because Ω doesn't need to change). Is my understanding correct? Thanks for any help. A following question can be found in here: Fallacy on using interpretation instead of definition in computing conditional probability? (using multiplication law circularly?) (also I think its best not to define conditional probability this way. This should be a result instead of a definition.)",,"['probability', 'statistics', 'definition']"
26,Reference request for complete probability text without measure theory,Reference request for complete probability text without measure theory,,"I'm looking for a complete probability reference text, covering the majority of standard probability and stochastic process topics that can be covered without the use of measure theory. I've already had a basic course in probability and in stochastic processes, so I'm looking for more of a desk reference type of book. The three that have been recommended to me are Probability, Statistics, and Random Processes by Papoulis Probability for Statistics and Machine Learning by DasGupta Probability By Feller I'm not too interested in Feller, since it seems that vol 2 has a fair bit of measure theory and vol 1 is only discrete. Any other recommendations?","I'm looking for a complete probability reference text, covering the majority of standard probability and stochastic process topics that can be covered without the use of measure theory. I've already had a basic course in probability and in stochastic processes, so I'm looking for more of a desk reference type of book. The three that have been recommended to me are Probability, Statistics, and Random Processes by Papoulis Probability for Statistics and Machine Learning by DasGupta Probability By Feller I'm not too interested in Feller, since it seems that vol 2 has a fair bit of measure theory and vol 1 is only discrete. Any other recommendations?",,"['probability', 'statistics', 'reference-request', 'stochastic-processes', 'book-recommendation']"
27,"Given a probability matrix, find probability that person at zero index can go to other rows","Given a probability matrix, find probability that person at zero index can go to other rows",,"I was trying to solve a problem in which I was given a matrix containing probabilities of person s[0] to go s[n] as s[0][n]. Input: 0   1   0   0   0   1     4   0   0   3   2   0     0   0   0   0   0   0     0   0   0   0   0   0     0   0   0   0   0   0     0   0   0   0   0   0 Tracing the probabilities: [0,1,0,0,0,1]: #s0 the initial state, goes to s1 and s5 with equal probability [4,0,0,3,2,0]: #s1 can become s0,s3 or s4, but with different probabilities [0,0,0,0,0,0]: #s2 is terminal, and unreachable [0,0,0,0,0,0]: #s3 is terminal [0,0,0,0,0,0]: #s4 is terminal [0,0,0,0,0,0]: #s5 is terminal So we can consider different paths to terminal states such as: s0->s1->s3 s0->s1->s0->s1->s0->s1->s4 s0->s1->s0->s5 Tracing probabilities of each, we find that: s2 has probability 0 s3 has probability 3/14 s4 has probability 1/7 s5 has probability 9/14 Output: [s2.numerator,s3.numerator,s4.numerator,s5.denominator] which is: [0,3,2,9,14] I'm trying to apply Regular Markov chain rule but couldn't succeed. Can anyone give me insights about how to solve this problem. Or at least any method or directions which I can use to understand this problem. Any help would be really appreciated","I was trying to solve a problem in which I was given a matrix containing probabilities of person s[0] to go s[n] as s[0][n]. Input: 0   1   0   0   0   1     4   0   0   3   2   0     0   0   0   0   0   0     0   0   0   0   0   0     0   0   0   0   0   0     0   0   0   0   0   0 Tracing the probabilities: [0,1,0,0,0,1]: #s0 the initial state, goes to s1 and s5 with equal probability [4,0,0,3,2,0]: #s1 can become s0,s3 or s4, but with different probabilities [0,0,0,0,0,0]: #s2 is terminal, and unreachable [0,0,0,0,0,0]: #s3 is terminal [0,0,0,0,0,0]: #s4 is terminal [0,0,0,0,0,0]: #s5 is terminal So we can consider different paths to terminal states such as: s0->s1->s3 s0->s1->s0->s1->s0->s1->s4 s0->s1->s0->s5 Tracing probabilities of each, we find that: s2 has probability 0 s3 has probability 3/14 s4 has probability 1/7 s5 has probability 9/14 Output: [s2.numerator,s3.numerator,s4.numerator,s5.denominator] which is: [0,3,2,9,14] I'm trying to apply Regular Markov chain rule but couldn't succeed. Can anyone give me insights about how to solve this problem. Or at least any method or directions which I can use to understand this problem. Any help would be really appreciated",,"['probability', 'markov-chains']"
28,Probability that the sum of $k$ dice is $n$,Probability that the sum of  dice is,k n,"The probability of two dice summing to k is simple enough, make a diagram of the throws, $$ \begin{array}{c}  &|&1&2&3&4&5&6\\\hline 1&|&2&3&4&5&6&7\\ 2&|&3&4&5&6&7&8\\ 3&|&4&5&6&7&8&9\\ 4&|&5&6&7&8&9&\color{green}{10}\\ 5&|&6&7&8&9&\color{green}{10}&11\\ 6&|&7&8&9&\color{green}{10}&11&12\\ \end{array} $$ and the probability of a sum is just the length of the corresponding diagonal divided by $36$. For instance, $\mathrm{P}(\mathrm{sum} = 10) = \frac{3}{36} = \frac{1}{12}$. However, it's not as easy to draw a diagram for three dice or more, so how would a general formula look?","The probability of two dice summing to k is simple enough, make a diagram of the throws, $$ \begin{array}{c}  &|&1&2&3&4&5&6\\\hline 1&|&2&3&4&5&6&7\\ 2&|&3&4&5&6&7&8\\ 3&|&4&5&6&7&8&9\\ 4&|&5&6&7&8&9&\color{green}{10}\\ 5&|&6&7&8&9&\color{green}{10}&11\\ 6&|&7&8&9&\color{green}{10}&11&12\\ \end{array} $$ and the probability of a sum is just the length of the corresponding diagonal divided by $36$. For instance, $\mathrm{P}(\mathrm{sum} = 10) = \frac{3}{36} = \frac{1}{12}$. However, it's not as easy to draw a diagram for three dice or more, so how would a general formula look?",,"['probability', 'dice']"
29,Show that $X = Y$ almost surely,Show that  almost surely,X = Y,"I am trying to solve this problem: Let $X,Y$ be random variables on $(\Omega, \mathcal{F}, P)$ such that $E(Y|\mathcal{G}) = X$ and $E(Y^2|\mathcal{G}) = X^2$, where $\mathcal{G} \subset \mathcal{F}$ is a sigma-algebra. Prove that $X = Y$ almost surely. What I have until now: From the given property, it's easy to see that $\int_{G}XdP = \int_{G} YdP$, for any $G \in \mathcal{G}$. Also, $X$ is $\mathcal{G}$-measurable. I know that if the integrals of two random variables, on the same probability space , over any element of the sigma-algebra in discussion agree, then they are equal almost surely. So, if I could prove that $Y$ is also $\mathcal{G}$-measurable I could apply this result. Or, in te other hand, if I could find a similar equality as above for any element $F \in \mathcal{F}$, then I think the problem is solved too. But I can't do it. Also, I can't see how the information the $X^2$ can be helpful here. Any ideas? Thanks in advance for the support.","I am trying to solve this problem: Let $X,Y$ be random variables on $(\Omega, \mathcal{F}, P)$ such that $E(Y|\mathcal{G}) = X$ and $E(Y^2|\mathcal{G}) = X^2$, where $\mathcal{G} \subset \mathcal{F}$ is a sigma-algebra. Prove that $X = Y$ almost surely. What I have until now: From the given property, it's easy to see that $\int_{G}XdP = \int_{G} YdP$, for any $G \in \mathcal{G}$. Also, $X$ is $\mathcal{G}$-measurable. I know that if the integrals of two random variables, on the same probability space , over any element of the sigma-algebra in discussion agree, then they are equal almost surely. So, if I could prove that $Y$ is also $\mathcal{G}$-measurable I could apply this result. Or, in te other hand, if I could find a similar equality as above for any element $F \in \mathcal{F}$, then I think the problem is solved too. But I can't do it. Also, I can't see how the information the $X^2$ can be helpful here. Any ideas? Thanks in advance for the support.",,"['probability', 'measure-theory', 'lebesgue-integral', 'expectation', 'conditional-expectation']"
30,Expected Entropy Based on Dirichlet Distribution,Expected Entropy Based on Dirichlet Distribution,,"The Dirichlet Distribution basically defines the probability that a sample came from a particular multinomial distribution if we assume that the prior probability of all multinomial distributions having generated the sample are equal. Each multinomial distribution has a corresponding categorical distribution, and the entropy of that categorical distribution is given by $$-\sum_x^{states}\Pr(x)\ln(\Pr(x))$$ Given a point $p=(p_1,p_2,p_3...p_n)$ randomly chosen according to a Dirichlet Distribution with parameters $k_1...k_n$, such that $\sum_ip_i=1$, the entropy of the corresponding categorical distribution is: $$H(p)=-\sum_i^n p_i \ln(p_i)$$ What the expected value of $\text H(p)$? In the special case where the Dirichlet Distribution is just defined by $k_1$ and $k_2$ and $p$ is 2-dimensional, the expected entropy $\text{H}(p)$ is given by the formula  $$\frac{(k_1+k_2) H_{(k_1+k_2-1)}-k_1 H_{k_1}-k_2 H_{(k_2-1)}}{k_1+k_2}$$ Where $H_n$ is the $n$th harmonic number, however I haven't been able to calculate the answer for greater numbers of dimensions.","The Dirichlet Distribution basically defines the probability that a sample came from a particular multinomial distribution if we assume that the prior probability of all multinomial distributions having generated the sample are equal. Each multinomial distribution has a corresponding categorical distribution, and the entropy of that categorical distribution is given by $$-\sum_x^{states}\Pr(x)\ln(\Pr(x))$$ Given a point $p=(p_1,p_2,p_3...p_n)$ randomly chosen according to a Dirichlet Distribution with parameters $k_1...k_n$, such that $\sum_ip_i=1$, the entropy of the corresponding categorical distribution is: $$H(p)=-\sum_i^n p_i \ln(p_i)$$ What the expected value of $\text H(p)$? In the special case where the Dirichlet Distribution is just defined by $k_1$ and $k_2$ and $p$ is 2-dimensional, the expected entropy $\text{H}(p)$ is given by the formula  $$\frac{(k_1+k_2) H_{(k_1+k_2-1)}-k_1 H_{k_1}-k_2 H_{(k_2-1)}}{k_1+k_2}$$ Where $H_n$ is the $n$th harmonic number, however I haven't been able to calculate the answer for greater numbers of dimensions.",,"['probability', 'information-theory', 'entropy']"
31,Computing Characteristic Functional of Brownian Motion,Computing Characteristic Functional of Brownian Motion,,"I am trying to solve the following: Define the characteristic functional of a random process $X_t, T=\mathbb{R}$ by $$L(\varphi)=E\left[\exp \left(i\int_{T}\varphi(t)X_{t}\, dt \right) \right],$$ where $\varphi(t)$ is continuous differentiable with compact support.  Determine the characteristic functional for Brownian motion. I want to compute $L(\varphi)=E[\exp(i\int_{T}\varphi(t)W_{t} \, dt)]$, where $W_t$ is a Brownian motion with continuous trajectories and independent gaussian distributed increments.  How can I apply these facts to compute $L(\varphi)$?  Am I supposed to estimate $\int_{T}\varphi(t)W_{t} \, dt$ using a Riemann sum?","I am trying to solve the following: Define the characteristic functional of a random process $X_t, T=\mathbb{R}$ by $$L(\varphi)=E\left[\exp \left(i\int_{T}\varphi(t)X_{t}\, dt \right) \right],$$ where $\varphi(t)$ is continuous differentiable with compact support.  Determine the characteristic functional for Brownian motion. I want to compute $L(\varphi)=E[\exp(i\int_{T}\varphi(t)W_{t} \, dt)]$, where $W_t$ is a Brownian motion with continuous trajectories and independent gaussian distributed increments.  How can I apply these facts to compute $L(\varphi)$?  Am I supposed to estimate $\int_{T}\varphi(t)W_{t} \, dt$ using a Riemann sum?",,"['probability', 'probability-theory', 'brownian-motion', 'characteristic-functions']"
32,Splitting the birthday paradox into months,Splitting the birthday paradox into months,,"One of the biggest studies in my universities' statistics class is the Birthday Paradox. It main goal is to determine that, given n students in a class, what's the probability that at least two of these students share a birthday. Now, suppose there are k students in a class and every student is equally likely to be born in any of the twelve months of the year. One of the questions we were asked is: What is the minimum value for k such that the probability there are two (or more) students born in the same month is at least 1/2? I modified the original equation to try and account for the fact that this takes count of months and not the entire year. P(A) = Event that two people have the same birthday in the month. Thus, $$P(A') = \frac{k! \cdot365 {365 \choose 30}}{365^k}$$ Where the ${365 \choose 30}$ is taking into account a specific given month. (This is where I believe I'm making an error) Then, the answer can be achieved by $$P(A) = 1-P(A')$$ Finally, I can use some bounding and solve for $$P\left(A \geq \frac{1}{2}\right) $$ However, I'm missing lots of calculations in between the lines which I am not sure how to achieve. How can I go about solving this problem appropriately?","One of the biggest studies in my universities' statistics class is the Birthday Paradox. It main goal is to determine that, given n students in a class, what's the probability that at least two of these students share a birthday. Now, suppose there are k students in a class and every student is equally likely to be born in any of the twelve months of the year. One of the questions we were asked is: What is the minimum value for k such that the probability there are two (or more) students born in the same month is at least 1/2? I modified the original equation to try and account for the fact that this takes count of months and not the entire year. P(A) = Event that two people have the same birthday in the month. Thus, $$P(A') = \frac{k! \cdot365 {365 \choose 30}}{365^k}$$ Where the ${365 \choose 30}$ is taking into account a specific given month. (This is where I believe I'm making an error) Then, the answer can be achieved by $$P(A) = 1-P(A')$$ Finally, I can use some bounding and solve for $$P\left(A \geq \frac{1}{2}\right) $$ However, I'm missing lots of calculations in between the lines which I am not sure how to achieve. How can I go about solving this problem appropriately?",,"['probability', 'statistics', 'paradoxes']"
33,What is the probability that a randomly chosen four-digit integer has distinct digits?,What is the probability that a randomly chosen four-digit integer has distinct digits?,,What is the probability that a randomly chosen four-digit integer has distinct digits? first digit can't be 0 so it'd be between 9 numbers. second digit can be 0 so it'd be between 9 numbers. third would cant be either of the last two so between 8 numbers. fourth would be between 7. 9*9*8*7/9000 = 0.504,What is the probability that a randomly chosen four-digit integer has distinct digits? first digit can't be 0 so it'd be between 9 numbers. second digit can be 0 so it'd be between 9 numbers. third would cant be either of the last two so between 8 numbers. fourth would be between 7. 9*9*8*7/9000 = 0.504,,"['probability', 'discrete-mathematics']"
34,"Suppose $X_1, \ldots, X_n$ are iid standard Cauchy random variables, does $\frac{1}{n}\sum_{i=1}^{n}X_i$ converge in probability or almost surely?","Suppose  are iid standard Cauchy random variables, does  converge in probability or almost surely?","X_1, \ldots, X_n \frac{1}{n}\sum_{i=1}^{n}X_i","Suppose $X_1, \ldots, X_n$ are iid standard Cauchy random variables, does $\frac{1}{n}\sum_{i=1}^{n}X_i$ converge in probability or almost surely? I know that by stable law, $\frac{1}{n}\sum_{i=1}^{n}X_i \sim Cauchy(0,1)$, but how can I find if it converges in probability or not by the definition of convergence in probability?","Suppose $X_1, \ldots, X_n$ are iid standard Cauchy random variables, does $\frac{1}{n}\sum_{i=1}^{n}X_i$ converge in probability or almost surely? I know that by stable law, $\frac{1}{n}\sum_{i=1}^{n}X_i \sim Cauchy(0,1)$, but how can I find if it converges in probability or not by the definition of convergence in probability?",,"['probability', 'probability-theory']"
35,Polar transformation of a probability distribution function,Polar transformation of a probability distribution function,,"I am working through Dirk P Kroese ""Monte Carlo Methods"" notes with one section based on Random Variable Generation from uniform random numbers using polar transformations (section 2.1.2.6). The polar method is based on the polar coordinate transformation $ X=R \cos \Theta$, $Y=R\sin \Theta$, where $\Theta \sim \text{U}(0,2\pi)$ and $R\sim f_R$ are independent. Using standard transformation rules it follows that the joint pdf of $X$ and $Y$ satisfies: $$f_{X,Y}(x,y)=\cfrac{f_R(r)}{2\pi r}$$ with $r=\sqrt{x^2+y^2}$. I don't fully understand how expression  $f_{X,Y}(x,y)$ is obtained from ""standard transformation rules"", please help or hint.","I am working through Dirk P Kroese ""Monte Carlo Methods"" notes with one section based on Random Variable Generation from uniform random numbers using polar transformations (section 2.1.2.6). The polar method is based on the polar coordinate transformation $ X=R \cos \Theta$, $Y=R\sin \Theta$, where $\Theta \sim \text{U}(0,2\pi)$ and $R\sim f_R$ are independent. Using standard transformation rules it follows that the joint pdf of $X$ and $Y$ satisfies: $$f_{X,Y}(x,y)=\cfrac{f_R(r)}{2\pi r}$$ with $r=\sqrt{x^2+y^2}$. I don't fully understand how expression  $f_{X,Y}(x,y)$ is obtained from ""standard transformation rules"", please help or hint.",,"['probability', 'probability-theory', 'transformation', 'polar-coordinates']"
36,Understanding why conditional probability is defined the way it is.,Understanding why conditional probability is defined the way it is.,,Just moments ago I stumbled upon the definition of conditional probability and I'm experiencing some difficulties in acquiring an intuitive understanding of why it's defined the way it is. $$P(E|F)= \dfrac{P(E \cap F)}{P(F)}$$ I'm more familiar with taking into consideration the amount of outcomes of interest and dividing that amount by the amount of outcomes in the sample space. I don't know how to interpret the logic behind conditional probability. I understand that $P(F)$ is the probability that the event which is known to have occured occurs. So I suppose that it can be calulated the way I am familiar with. Presumably the same goes for $P(E \cap F)$ by using the Inclusion–exclusion principle. But why one is divided by the other I cannot fathom. Why is it defined this way? How does one make sense of it.,Just moments ago I stumbled upon the definition of conditional probability and I'm experiencing some difficulties in acquiring an intuitive understanding of why it's defined the way it is. I'm more familiar with taking into consideration the amount of outcomes of interest and dividing that amount by the amount of outcomes in the sample space. I don't know how to interpret the logic behind conditional probability. I understand that is the probability that the event which is known to have occured occurs. So I suppose that it can be calulated the way I am familiar with. Presumably the same goes for by using the Inclusion–exclusion principle. But why one is divided by the other I cannot fathom. Why is it defined this way? How does one make sense of it.,P(E|F)= \dfrac{P(E \cap F)}{P(F)} P(F) P(E \cap F),"['probability', 'discrete-mathematics']"
37,Probability space in the Law of Large Numbers,Probability space in the Law of Large Numbers,,"Suppose I have a probability space $(\Omega,\mathcal{F},P)$ and a random variable $X:\Omega\rightarrow\mathbb{R}$. Consider a random sample $\{X_1,\ldots,X_n\}$ of independent random variables with the same distribution as $X$. I was told that  $X_i:\Omega^n\rightarrow\mathbb{R}$ and $X_i(\omega)=X(\omega_i)$. Then, in the law of large numbers, $$(X_1+\cdots+X_n)/n\rightarrow E[X],$$ does the domain of each term of the sequence changes? In the definition of almost sure convergence the domain is fixed.","Suppose I have a probability space $(\Omega,\mathcal{F},P)$ and a random variable $X:\Omega\rightarrow\mathbb{R}$. Consider a random sample $\{X_1,\ldots,X_n\}$ of independent random variables with the same distribution as $X$. I was told that  $X_i:\Omega^n\rightarrow\mathbb{R}$ and $X_i(\omega)=X(\omega_i)$. Then, in the law of large numbers, $$(X_1+\cdots+X_n)/n\rightarrow E[X],$$ does the domain of each term of the sequence changes? In the definition of almost sure convergence the domain is fixed.",,"['probability', 'probability-theory']"
38,"Let X, Y be positive random variables on a sample space Ω. Assume that X(ω) ≥ Y (ω) for all ω ∈ Ω. Prove that EX ≥ EY .","Let X, Y be positive random variables on a sample space Ω. Assume that X(ω) ≥ Y (ω) for all ω ∈ Ω. Prove that EX ≥ EY .",,"The problem is: Let $X$, $Y$ be positive random variables on a sample space $\Omega$. Assume that $X(\omega)\geq Y (\omega)$ for all $\omega \in \Omega$.  Prove that $\operatorname{E} X \geq \operatorname{E}Y$ . I have a little bit confused about how to use ω here. I did not treat random variable as function before. Thank you","The problem is: Let $X$, $Y$ be positive random variables on a sample space $\Omega$. Assume that $X(\omega)\geq Y (\omega)$ for all $\omega \in \Omega$.  Prove that $\operatorname{E} X \geq \operatorname{E}Y$ . I have a little bit confused about how to use ω here. I did not treat random variable as function before. Thank you",,"['probability', 'probability-theory', 'expectation']"
39,Why are events $A$ and $B$ independent whilst events $A$ and $C$ are not?,Why are events  and  independent whilst events  and  are not?,A B A C,Two ordinary fair dice (one red and one blue) are thrown. Event A: The red die will show a 5 or a 6. Event B: The sum of the two dice will be 7. Event C: The sum of the two dice will be 8. Using the test for independence: $$P(A\cap B) = P(A).P(B) $$ It can be seen that $A$ and $B$ are independent events whilst events $A$ and $C$ are not independent. I can't understand why this is the case. I understand the mathematics but I can't understand the logic behind it. I drew out the sample space but I am none the wiser. Is there some intrinsic difference between events $B$ and $C$ that results in one being independent and the other not?,Two ordinary fair dice (one red and one blue) are thrown. Event A: The red die will show a 5 or a 6. Event B: The sum of the two dice will be 7. Event C: The sum of the two dice will be 8. Using the test for independence: $$P(A\cap B) = P(A).P(B) $$ It can be seen that $A$ and $B$ are independent events whilst events $A$ and $C$ are not independent. I can't understand why this is the case. I understand the mathematics but I can't understand the logic behind it. I drew out the sample space but I am none the wiser. Is there some intrinsic difference between events $B$ and $C$ that results in one being independent and the other not?,,['probability']
40,"Change of measure to make things ""easier""?","Change of measure to make things ""easier""?",,"I am familiar with the Radon-Nikodym Theorem and an R/N Derivative, but while reading a set of lecture notes on: Stochastic Calculus, Filtering, and Stochastic Control in section 1.6: ""Induced measures, independence, and absolute continuity"", (bottom of page 42, emphasis mine): Absolutely continuous measures and the Radon-Nikodym theorem Let $(\Omega, \mathcal{F}, \mathbb{P})$ be a given probability space.   It is often interesting to try to find other measures on $\mathcal{F}$   with different properties. We may have gone through some trouble to   construct a measure $\mathbb{P}$, but once we have such a measure, we   can generate a large family of related measures using a rather simple   technique. This idea will come in very handy in many situations; calculations which are difficult under one measure can often become   very simple if we change to a suitably modiﬁed measure (for example,   if $\{X_n\}$ is a collection of random variables with some complicated   dependencies under $\mathbb{P}$, it may be advantageous to compute   using a modiﬁed measure $\mathbb{Q}$ under which the $X_n$ are   independent. Later on, the change of measure concept will form the basis form one of the most basic tools in our stochastic toolbox, the Girsanov theorem. I'm having trouble conceptualizing the idea that ""calculations which are difficult under one measure can often become very simple if we change to a suitably modiﬁed measure"". Can someone further explain, perhaps by a couple examples, how this is the case? How can $X_n$ have complicated dependencies under $\mathbb{P}$, but simple (independent) dependencies under a $\mathbb{Q}$?","I am familiar with the Radon-Nikodym Theorem and an R/N Derivative, but while reading a set of lecture notes on: Stochastic Calculus, Filtering, and Stochastic Control in section 1.6: ""Induced measures, independence, and absolute continuity"", (bottom of page 42, emphasis mine): Absolutely continuous measures and the Radon-Nikodym theorem Let $(\Omega, \mathcal{F}, \mathbb{P})$ be a given probability space.   It is often interesting to try to find other measures on $\mathcal{F}$   with different properties. We may have gone through some trouble to   construct a measure $\mathbb{P}$, but once we have such a measure, we   can generate a large family of related measures using a rather simple   technique. This idea will come in very handy in many situations; calculations which are difficult under one measure can often become   very simple if we change to a suitably modiﬁed measure (for example,   if $\{X_n\}$ is a collection of random variables with some complicated   dependencies under $\mathbb{P}$, it may be advantageous to compute   using a modiﬁed measure $\mathbb{Q}$ under which the $X_n$ are   independent. Later on, the change of measure concept will form the basis form one of the most basic tools in our stochastic toolbox, the Girsanov theorem. I'm having trouble conceptualizing the idea that ""calculations which are difficult under one measure can often become very simple if we change to a suitably modiﬁed measure"". Can someone further explain, perhaps by a couple examples, how this is the case? How can $X_n$ have complicated dependencies under $\mathbb{P}$, but simple (independent) dependencies under a $\mathbb{Q}$?",,"['probability', 'probability-theory', 'measure-theory', 'examples-counterexamples']"
41,Moment generating function and probability,Moment generating function and probability,,"Problem:Let $X$ and $Y$ be identically distributed independent random variables such that the moment generating function of $X +Y$ is $$M(t) = 0.09 e^{−2t} + 0.24 e^{−t} + 0.34 + 0.24 e^t + 0.09 e^{2t}$$  for $−\infty < t < \infty$. Calculate $P[X \le 0]$. Answer: Because $X$ and $Y$ are independent and identically distributed, the moment generating function of $X+ Y$ equals $K^2(t)$, where $K(t)$ is the moment generating function common to $X$ and $Y$. Thus, $$K(t) = 0.30e^{-t} + 0.40 + 0.30e^t$$ This is the moment generating function of a discrete random variable that assumes the values $-1$, $0$, and $1$ with respective probabilities $0.30$, $0.40$, and $0.30$. The value we seek is thus $0.70$. My question is how to factor the moment generating function of $X+Y$ to $0.30e^-t + 0.40 + 0.30e^t$, is there a general formula to use? I'm also sorry for not using MathJax I'm really having trouble with it.","Problem:Let $X$ and $Y$ be identically distributed independent random variables such that the moment generating function of $X +Y$ is $$M(t) = 0.09 e^{−2t} + 0.24 e^{−t} + 0.34 + 0.24 e^t + 0.09 e^{2t}$$  for $−\infty < t < \infty$. Calculate $P[X \le 0]$. Answer: Because $X$ and $Y$ are independent and identically distributed, the moment generating function of $X+ Y$ equals $K^2(t)$, where $K(t)$ is the moment generating function common to $X$ and $Y$. Thus, $$K(t) = 0.30e^{-t} + 0.40 + 0.30e^t$$ This is the moment generating function of a discrete random variable that assumes the values $-1$, $0$, and $1$ with respective probabilities $0.30$, $0.40$, and $0.30$. The value we seek is thus $0.70$. My question is how to factor the moment generating function of $X+Y$ to $0.30e^-t + 0.40 + 0.30e^t$, is there a general formula to use? I'm also sorry for not using MathJax I'm really having trouble with it.",,"['probability', 'moment-generating-functions']"
42,Moment generating function of $X+Y$ using convolution of $X$ and $Y$,Moment generating function of  using convolution of  and,X+Y X Y,"Given that the pdf of $X+Y$ is the convolution of pdfs $X$ and $Y$ , show that $M_{X+Y}$ is $M_XM_Y$ where $M$ is the moment generating function. $X$ and $Y$ are independent and continuous. I am confused how to proceed from here (see picture). Thank you. My approach:","Given that the pdf of is the convolution of pdfs and , show that is where is the moment generating function. and are independent and continuous. I am confused how to proceed from here (see picture). Thank you. My approach:",X+Y X Y M_{X+Y} M_XM_Y M X Y,"['probability', 'probability-theory', 'probability-distributions', 'probability-limit-theorems']"
43,Alternate proof of the dominated convergence theorem by applying Fatou's lemma to $2g - |f_n - f|$?,Alternate proof of the dominated convergence theorem by applying Fatou's lemma to ?,2g - |f_n - f|,"Here is a proof of the dominated convergence theorem. Theorem. Suppose that $f_n$ are measurable real-valued functions and $f_n(x) \to f(x)$ for each $x$ . Suppose there exists a nonnegative integrable function $g$ such that $|f_n(x)| \le g(x)$ for all $x$ . Then $$\lim_{n \to \infty} \int f_n\,d\mu = \int f\,d\mu.$$ Proof. Since $f_n + g \ge 0$ , by Fatou's lemma, $$\int f + \int g = \int (f + g) \le \liminf_{n \to \infty} \int (f_n + g) = \liminf_{n \to \infty} \int f_n + \int g.$$ Since $g$ is integrable, $$\int f \le \liminf_{n \to \infty} \int f_n.\tag*{$(*)$}$$ Similarly, $g - f_n \ge 0$ , so $$\int g - \int f = \int (g - f) \le \liminf_{n \to \infty} \int (g - f_n) = \int g + \liminf_{n \to \infty} \int (-f_n),$$ and hence $$-\int f \le \liminf_{n \to \infty} \int (-f_n) = -\limsup_{n \to \infty} \int f_n.$$ Therefore $$\int f \ge \limsup_{n \to \infty} \int f_n,$$ which with $(*)$ proves the theorem. $$\tag*{$\square$}$$ My question is as follows. Can we get another proof of the dominated convergence theorem by applying Fatou's lemma to $2g - |f_n - f|$ ?","Here is a proof of the dominated convergence theorem. Theorem. Suppose that are measurable real-valued functions and for each . Suppose there exists a nonnegative integrable function such that for all . Then Proof. Since , by Fatou's lemma, Since is integrable, Similarly, , so and hence Therefore which with proves the theorem. My question is as follows. Can we get another proof of the dominated convergence theorem by applying Fatou's lemma to ?","f_n f_n(x) \to f(x) x g |f_n(x)| \le g(x) x \lim_{n \to \infty} \int f_n\,d\mu = \int f\,d\mu. f_n + g \ge 0 \int f + \int g = \int (f + g) \le \liminf_{n \to \infty} \int (f_n + g) = \liminf_{n \to \infty} \int f_n + \int g. g \int f \le \liminf_{n \to \infty} \int f_n.\tag*{(*)} g - f_n \ge 0 \int g - \int f = \int (g - f) \le \liminf_{n \to \infty} \int (g - f_n) = \int g + \liminf_{n \to \infty} \int (-f_n), -\int f \le \liminf_{n \to \infty} \int (-f_n) = -\limsup_{n \to \infty} \int f_n. \int f \ge \limsup_{n \to \infty} \int f_n, (*) \tag*{\square} 2g - |f_n - f|","['real-analysis', 'probability']"
44,What's the probability of getting $5$ different numbers but not any $6$ when throwing $5$ dice?,What's the probability of getting  different numbers but not any  when throwing  dice?,5 6 5,"I have $5$ dice, I throw them at once. What is the probability of getting $5$ unique numbers, i.e., $1\ \  \&\ \  2\ \  \&\ \  3\ \  \&\ \  4\ \  \&\ \  5$ in any order BUT NOT any $6$? Of course they can be in any order as long as all $5$ dice are all unique numbers but not any $6$. I presume number of possibilities is divided by $6$ to the power of $5$ ($7776$)? But I do not know the number of possible permutations for $5$ unique numbers, excluding a $6$. Please help. The question has an actual usefulness for me. What is the probability of getting $5$ different numbers but not any $6$ when throwing $5$ dice at once?","I have $5$ dice, I throw them at once. What is the probability of getting $5$ unique numbers, i.e., $1\ \  \&\ \  2\ \  \&\ \  3\ \  \&\ \  4\ \  \&\ \  5$ in any order BUT NOT any $6$? Of course they can be in any order as long as all $5$ dice are all unique numbers but not any $6$. I presume number of possibilities is divided by $6$ to the power of $5$ ($7776$)? But I do not know the number of possible permutations for $5$ unique numbers, excluding a $6$. Please help. The question has an actual usefulness for me. What is the probability of getting $5$ different numbers but not any $6$ when throwing $5$ dice at once?",,"['probability', 'permutations', 'combinations', 'dice']"
45,"Asymptotic Moments of the Binomial Distribution, $E(X/(np))^k = 1 + O(k^2/n)$?","Asymptotic Moments of the Binomial Distribution, ?",E(X/(np))^k = 1 + O(k^2/n),"Let $X \sim \text{Binomial}(n, p)$ be the sum of $n$ Bernoulli($p$) random variables. What is the value of $E(X/(np))^k$, where $k$ is a large integer, as $n$ grows large? From calculations the first values are E1        = 1 E(X/np)   = 1 E(X/np)^2 = 1 + r/n E(X/np)^3 = 1 + 3r/n + ((-1+p)(-1+2p))/(np)^2 E(X/np)^4 = 1 + 6r/n + O[1/n]^2 E(X/np)^5 = 1 + 10r/n + O[1/n]^2 E(X/np)^6 = 1 + 15r/n + O[1/n]^2 E(X/np)^7 = 1 + 21r/n + O[1/n]^2 ... where $r = (1-p)/p$ So my guess would be that one could obtain a result like $E(X/(np))^k = 1 + O(k^2/n)$, but I'm not sure how I'd proceed. For sums of $\{+1, -1\}$ random variables, I'm aware that we can bound the moments by replacing each one with a normal random variable with the same variance. However converting this result to the non central case doesn't seem obvious?","Let $X \sim \text{Binomial}(n, p)$ be the sum of $n$ Bernoulli($p$) random variables. What is the value of $E(X/(np))^k$, where $k$ is a large integer, as $n$ grows large? From calculations the first values are E1        = 1 E(X/np)   = 1 E(X/np)^2 = 1 + r/n E(X/np)^3 = 1 + 3r/n + ((-1+p)(-1+2p))/(np)^2 E(X/np)^4 = 1 + 6r/n + O[1/n]^2 E(X/np)^5 = 1 + 10r/n + O[1/n]^2 E(X/np)^6 = 1 + 15r/n + O[1/n]^2 E(X/np)^7 = 1 + 21r/n + O[1/n]^2 ... where $r = (1-p)/p$ So my guess would be that one could obtain a result like $E(X/(np))^k = 1 + O(k^2/n)$, but I'm not sure how I'd proceed. For sums of $\{+1, -1\}$ random variables, I'm aware that we can bound the moments by replacing each one with a normal random variable with the same variance. However converting this result to the non central case doesn't seem obvious?",,"['probability', 'probability-distributions', 'normal-distribution', 'binomial-distribution', 'moment-generating-functions']"
46,What is the probability that the center of a odd sided regular polygon lies inside a triangle formed by the vertices of the polygon?,What is the probability that the center of a odd sided regular polygon lies inside a triangle formed by the vertices of the polygon?,,"Three distinct vertices are choose at random from the vertices of a given regular polygon of $2n+1$ sides. Each of those three vertices will determine a triangle. What is the probability that the center of the polygon lies inside the triangle determined by three vertices of the polygon? Note: All vertices of a regular polygon lie on a common circle (the circumscribed circle), the center of this circle is the center of the polygon. P.S: why is this question asked specially for a odd sided regular polygon, will the answer differ for a even sided regular polygon? I have just found a difficult proof of a Much more general question (Probability of a fixed point in a convex region being inside a triangle formed by any three point form the convex region). I am not interested in such generality, can one give a much simpler proof this special case (the convex region being the odd-sided polygon and the fixed point the center of the polygon).","Three distinct vertices are choose at random from the vertices of a given regular polygon of $2n+1$ sides. Each of those three vertices will determine a triangle. What is the probability that the center of the polygon lies inside the triangle determined by three vertices of the polygon? Note: All vertices of a regular polygon lie on a common circle (the circumscribed circle), the center of this circle is the center of the polygon. P.S: why is this question asked specially for a odd sided regular polygon, will the answer differ for a even sided regular polygon? I have just found a difficult proof of a Much more general question (Probability of a fixed point in a convex region being inside a triangle formed by any three point form the convex region). I am not interested in such generality, can one give a much simpler proof this special case (the convex region being the odd-sided polygon and the fixed point the center of the polygon).",,"['probability', 'geometry', 'polygons', 'geometric-probability']"
47,How to calculate inverse cumulative distribution using a table?,How to calculate inverse cumulative distribution using a table?,,"I need help with this: $$P(X\geq a)=1-F_X(a)=1-\Phi\left(\frac{a-70}{8}\right)=0.25$$ When $X\sim N(70,64)$. I know that it should be: $(a-70)/8 = 0.6745$ How do I get $0.6745$ From $Z$ table? I know that $0.75$ is between $0.67$ and $0.68$ in that table. Do I need to do an average $(0.67+0.68)/2$? the final answer is $a = 75.396$","I need help with this: $$P(X\geq a)=1-F_X(a)=1-\Phi\left(\frac{a-70}{8}\right)=0.25$$ When $X\sim N(70,64)$. I know that it should be: $(a-70)/8 = 0.6745$ How do I get $0.6745$ From $Z$ table? I know that $0.75$ is between $0.67$ and $0.68$ in that table. Do I need to do an average $(0.67+0.68)/2$? the final answer is $a = 75.396$",,"['probability', 'normal-distribution']"
48,Number of vertices of a random convex polygon,Number of vertices of a random convex polygon,,"Take $n>2$ random points, chosen independently with uniform probability on $[0,1]\times[0,1]$. What is the probability $P(n,k)$ that the convex hull of these points is a polygon with exactly $2<k\leq n$ vertices? It seems that $P(3,3)=1$, since after choosing two points, the probability that the third point will be on the line between them is $0$. For $P(4,4)$, assume 3 points have already been chosen, then their convex hull is a triangle which is the intersection of 3 half spaces. The fourth point will create a quadrilateral if and only if it lies in the intersection of exactly two of the above half spaces. But how can we calculate this area? And what if $n>4$? Then things get really tricky!","Take $n>2$ random points, chosen independently with uniform probability on $[0,1]\times[0,1]$. What is the probability $P(n,k)$ that the convex hull of these points is a polygon with exactly $2<k\leq n$ vertices? It seems that $P(3,3)=1$, since after choosing two points, the probability that the third point will be on the line between them is $0$. For $P(4,4)$, assume 3 points have already been chosen, then their convex hull is a triangle which is the intersection of 3 half spaces. The fourth point will create a quadrilateral if and only if it lies in the intersection of exactly two of the above half spaces. But how can we calculate this area? And what if $n>4$? Then things get really tricky!",,"['probability', 'combinatorics', 'puzzle', 'geometric-probability', 'plane-geometry']"
49,Finding the maximum likelihood estimators for this shifted exponential PDF?,Finding the maximum likelihood estimators for this shifted exponential PDF?,,"Consider a random sample $X_1, X_2, \dots, X_n$ from the shifted exponential PDF $$f(x; \lambda, \theta) = \begin{cases}\lambda e^{-\lambda(x-\theta)} ;& x \geq \theta\\                     \theta ; &\text{Otherwise}\end{cases}$$ Taking $\theta = 0$ gives the pdf of the exponential distribution considered previously (with positive density to the right of zero). a. Obtain the maximum likelihood estimators of $\theta$ and $\lambda$ . I followed the basic rules for the MLE and came up with: $$\lambda = \frac{n}{\sum_{i=1}^n(x_i - \theta)}$$ Should I take $\theta$ out and write it as $-n\theta$ and find $\theta$ in terms of $\lambda$ ?",Consider a random sample from the shifted exponential PDF Taking gives the pdf of the exponential distribution considered previously (with positive density to the right of zero). a. Obtain the maximum likelihood estimators of and . I followed the basic rules for the MLE and came up with: Should I take out and write it as and find in terms of ?,"X_1, X_2, \dots, X_n f(x; \lambda, \theta) = \begin{cases}\lambda e^{-\lambda(x-\theta)} ;& x \geq \theta\\
                    \theta ; &\text{Otherwise}\end{cases} \theta = 0 \theta \lambda \lambda = \frac{n}{\sum_{i=1}^n(x_i - \theta)} \theta -n\theta \theta \lambda","['probability', 'statistics']"
50,Sum of probabilities versus probability of sum,Sum of probabilities versus probability of sum,,"Consider two real-valued random variables $X,Y$ defined on the probability space $(\Omega, \mathcal{F}, P)$. let $\epsilon>0$. Could you help me to show that $$ P(|X|+|Y|\geq 2\epsilon^2)\leq P(|X|\geq \epsilon^2)+P(|Y|\geq \epsilon^2) $$ I don't see why. I think this expression is used for example in van der Vaart ""Asymptotic Statistic"" p.95 in the proof of Theorem 7.2","Consider two real-valued random variables $X,Y$ defined on the probability space $(\Omega, \mathcal{F}, P)$. let $\epsilon>0$. Could you help me to show that $$ P(|X|+|Y|\geq 2\epsilon^2)\leq P(|X|\geq \epsilon^2)+P(|Y|\geq \epsilon^2) $$ I don't see why. I think this expression is used for example in van der Vaart ""Asymptotic Statistic"" p.95 in the proof of Theorem 7.2",,"['probability', 'probability-theory']"
51,Calculating the expectation of $X=$ the number of failures until the $r^{th}$ success,Calculating the expectation of  the number of failures until the  success,X= r^{th},"I need to calculate the expectation of $X=$ the number of failures until the $r$-th success in an infinite series of Bernoulli experiments with $p$ the probability of success. ($q=1-p$ the probability of failure) My solution: I figured $$P(X=x)={x+r \choose x}q^xp^r$$ (is this correct?) and $x\geq 0$ (In other words, $X\sim Bin(x+r,q)$. So by definition, $\Bbb EX=\sum_{x=0}^\infty x{x+r \choose x}q^xp^r$. Trying to simplify this, I got to \begin{align*} \frac{qp^r}{r!}\sum_{x=0}^\infty (x+r)(x+r-1) \ldots (x+1)xq^{x-1} & =\frac{qp^r}{r!}\left(\sum_{x=0}^\infty q^{x+r}\right)^{(r+1)}\\ & =\frac{qp^r}{r!}\left(q^r\sum_{x=0}^\infty q^{x}\right)^{(r+1)}\\ & =\frac{qp^r}{r!}(\frac{q^r}{1-q})^{(r+1)} \end{align*} $(r+1)$ denotes taking the $(r+1)^{th}$ derivative in respect to $q$. Now what? How can I simplify that further? Is there a simpler way?","I need to calculate the expectation of $X=$ the number of failures until the $r$-th success in an infinite series of Bernoulli experiments with $p$ the probability of success. ($q=1-p$ the probability of failure) My solution: I figured $$P(X=x)={x+r \choose x}q^xp^r$$ (is this correct?) and $x\geq 0$ (In other words, $X\sim Bin(x+r,q)$. So by definition, $\Bbb EX=\sum_{x=0}^\infty x{x+r \choose x}q^xp^r$. Trying to simplify this, I got to \begin{align*} \frac{qp^r}{r!}\sum_{x=0}^\infty (x+r)(x+r-1) \ldots (x+1)xq^{x-1} & =\frac{qp^r}{r!}\left(\sum_{x=0}^\infty q^{x+r}\right)^{(r+1)}\\ & =\frac{qp^r}{r!}\left(q^r\sum_{x=0}^\infty q^{x}\right)^{(r+1)}\\ & =\frac{qp^r}{r!}(\frac{q^r}{1-q})^{(r+1)} \end{align*} $(r+1)$ denotes taking the $(r+1)^{th}$ derivative in respect to $q$. Now what? How can I simplify that further? Is there a simpler way?",,"['probability', 'probability-theory', 'expectation']"
52,What are the mean and variance of the log of a random variable?,What are the mean and variance of the log of a random variable?,,"Here's the problem. We have a random variable X that follows a Poisson law. If we take the log of this variable, what are the first two moments (mean and variance) of the law it follows? This looks like a simple question, but I can't find anything about it. Any idea? EDIT: In order to prevent the X=0 case, we bias the Poisson law. Our random variable becomes log(X+epsilon) with X~Poisson(λ).","Here's the problem. We have a random variable X that follows a Poisson law. If we take the log of this variable, what are the first two moments (mean and variance) of the law it follows? This looks like a simple question, but I can't find anything about it. Any idea? EDIT: In order to prevent the X=0 case, we bias the Poisson law. Our random variable becomes log(X+epsilon) with X~Poisson(λ).",,"['probability', 'probability-distributions', 'random-variables', 'poisson-distribution']"
53,A and B play a series of games. Find the probability that a total of 4 games are played.,A and B play a series of games. Find the probability that a total of 4 games are played.,,"A and B play a series of games. Each game is independently won by A with probability $p$ and by B with probability $1 􀀀- p$. They stop when the total number of wins of one of the players is two greater than that of the other player. The player with the greater number of total wins is declared the winner of the series. (a) Find the probability that a total of 4 games are played. (b) Find the probability that A is the winner of the series. I though I could do this using conditional probability and independent trials/bernoulli trials but I am really confused. let's say I want to find the probability that A wins three games and B wins 1 game and add that to the probability of B wins 3 games and A wins 1 game, but I am not sure of how to do that because B has to win at least 1 game before A wins 2. If I set $n$ = number of games played would the sample space equal $p^n * (1-p)^n$? How do I solve this? is there an easier way?","A and B play a series of games. Each game is independently won by A with probability $p$ and by B with probability $1 􀀀- p$. They stop when the total number of wins of one of the players is two greater than that of the other player. The player with the greater number of total wins is declared the winner of the series. (a) Find the probability that a total of 4 games are played. (b) Find the probability that A is the winner of the series. I though I could do this using conditional probability and independent trials/bernoulli trials but I am really confused. let's say I want to find the probability that A wins three games and B wins 1 game and add that to the probability of B wins 3 games and A wins 1 game, but I am not sure of how to do that because B has to win at least 1 game before A wins 2. If I set $n$ = number of games played would the sample space equal $p^n * (1-p)^n$? How do I solve this? is there an easier way?",,['probability']
54,Probabilistic approach to prove a graph theory theorem,Probabilistic approach to prove a graph theory theorem,,"A theorem in graph theory is as the following, Let $G=(V,E)$ be a finite graph where $V$ is the set of vertexes and $E$ is the set of edges. Then there exists a vertex subset $W$, i.e. $W \subset V$, such that the number of edges connecting $W$ and $W^C$ is at least $\frac{|E|}{2}$, where $W^C = V\backslash W$ and $|E|$ is the total number of edges in the graph $G$. The question is to prove this theorem by a probabilistic approach. My idea is as follows : Let $|V|=n,|W|=m$, then the maximum number of possible edges between $W,W^C$ is $m\times(n-m)$. The maximum number of possible edges in graph $G$ is $C_n^2 = \frac{n!}{2!\times(n-2)!}$. We can treat the edges as if they were randomly scattered in the $C_n^2$ positions, and with probability $p=\frac{m\times(n-m)}{C_n^2}$ one edge would connect $W,W^C$. Then it is like a Bernoulli trial of $|E|$ times with success probability $p=\frac{m\times(n-m)}{C_n^2}$, and the probability there are at least $\frac{|E|}{2}$ edges connecting $W,W^C$ is $\sum\limits_{k = \left\lceil {\frac{1}{2}|E|} \right\rceil }^{|E|} {C_{|E|}^k{p^k}{{(1 - p)}^{|E| - k}}} $. But this probability is for a particular $W$. We need to show with probability there exists one or more such $W$ satisfying the conditions of the above-mentioned theorem. I got stuck here. Anyone can help with this proof? Thank you!","A theorem in graph theory is as the following, Let $G=(V,E)$ be a finite graph where $V$ is the set of vertexes and $E$ is the set of edges. Then there exists a vertex subset $W$, i.e. $W \subset V$, such that the number of edges connecting $W$ and $W^C$ is at least $\frac{|E|}{2}$, where $W^C = V\backslash W$ and $|E|$ is the total number of edges in the graph $G$. The question is to prove this theorem by a probabilistic approach. My idea is as follows : Let $|V|=n,|W|=m$, then the maximum number of possible edges between $W,W^C$ is $m\times(n-m)$. The maximum number of possible edges in graph $G$ is $C_n^2 = \frac{n!}{2!\times(n-2)!}$. We can treat the edges as if they were randomly scattered in the $C_n^2$ positions, and with probability $p=\frac{m\times(n-m)}{C_n^2}$ one edge would connect $W,W^C$. Then it is like a Bernoulli trial of $|E|$ times with success probability $p=\frac{m\times(n-m)}{C_n^2}$, and the probability there are at least $\frac{|E|}{2}$ edges connecting $W,W^C$ is $\sum\limits_{k = \left\lceil {\frac{1}{2}|E|} \right\rceil }^{|E|} {C_{|E|}^k{p^k}{{(1 - p)}^{|E| - k}}} $. But this probability is for a particular $W$. We need to show with probability there exists one or more such $W$ satisfying the conditions of the above-mentioned theorem. I got stuck here. Anyone can help with this proof? Thank you!",,"['probability', 'graph-theory']"
55,Second derivative of $\int_\mathbb{R}\cos(tx)dp(x)$,Second derivative of,\int_\mathbb{R}\cos(tx)dp(x),"Let $p$ be a probability on $\mathbb{R}$ and  $$f(t):=\int_\mathbb{R}\cos(tx)dp(x).$$ I want to show that if $f''(0)$ exists then $$f''(0)=\lim_{t\to 0}2\frac{f(t)-1}{t^2} \: \:(\star).$$ By differentiation under the integral sign, I obtain $$f''(t)=\int_\mathbb{R}-x^2\cos(tx)dp(x).$$ Now, it seems that I need a sort of ""integration by parts for probabilities"", but I don't know if something similar exists. So what's the fastest way to deduce $(\star)$?","Let $p$ be a probability on $\mathbb{R}$ and  $$f(t):=\int_\mathbb{R}\cos(tx)dp(x).$$ I want to show that if $f''(0)$ exists then $$f''(0)=\lim_{t\to 0}2\frac{f(t)-1}{t^2} \: \:(\star).$$ By differentiation under the integral sign, I obtain $$f''(t)=\int_\mathbb{R}-x^2\cos(tx)dp(x).$$ Now, it seems that I need a sort of ""integration by parts for probabilities"", but I don't know if something similar exists. So what's the fastest way to deduce $(\star)$?",,"['probability', 'integration', 'measure-theory', 'derivatives']"
56,Two cards are drawn without replacement from an ordinary deck of 52 cards. What is the probability that the cards are of the same suit?,Two cards are drawn without replacement from an ordinary deck of 52 cards. What is the probability that the cards are of the same suit?,,"What is the probability that the cards are of the same suit? This specific type of question deals with the use of combinations and permutations. I know that because the cards aren't being replaced after being taken out, order does matter and therefore permutations are used. So I started off with this: (13)P(2) since at first there are 13 cards of the same suit, then 12 after one of the suit is picked out and not replaced. The total came out to be 156 ways. For the sample space I did (52)P(2) since at first there are 52 cards to choose from, and then there are 51 cards to choose from after the first card is picked and not replaced. The total came out to be 2652 ways. So then to find the probability of the question: (13)P(2)/(52)P(2)= 156/2652= 1/17 What is the probability that the cards are both face cards? I started off with this: There are 12 face cards in total, and because cards aren't being replaced, order does matter. So since there are 12 face cards as choices in the first try, then 11 in the second I did (12)P(2) as the numerator. The total came out to be 132. For the denominator, which is the sample space, I did (52)P(2), since you are choosing 2 out of 52 cards and got 2652 ways. So now I do : (12)P(2)/(52)P(2)= 132/2652= 11/221 What is the probability that the cards are a diamond and a spade? I was a little confused on this one, but I tried. Even though I said order matters and to use permutations, I felt like for the numerator I was supposed to combinations since choosing a diamond and spade didn't matter. Since there are 13 spades and 13 diamonds, for the numerator I did (13)C(1)*(13)C(1) which equals to 169 ways of choosing a diamond and a spade. For the sample space I stayed with (52)P(2) since order still matter on this one which equals to 2652 ways. So the final step is : [(13)C(1)*(13)C(1)]/(52)P(2)= 169/2652= 13/204 I would like to know if I did these questions right, because i'm a little confused between using combinations and permutations.","What is the probability that the cards are of the same suit? This specific type of question deals with the use of combinations and permutations. I know that because the cards aren't being replaced after being taken out, order does matter and therefore permutations are used. So I started off with this: (13)P(2) since at first there are 13 cards of the same suit, then 12 after one of the suit is picked out and not replaced. The total came out to be 156 ways. For the sample space I did (52)P(2) since at first there are 52 cards to choose from, and then there are 51 cards to choose from after the first card is picked and not replaced. The total came out to be 2652 ways. So then to find the probability of the question: (13)P(2)/(52)P(2)= 156/2652= 1/17 What is the probability that the cards are both face cards? I started off with this: There are 12 face cards in total, and because cards aren't being replaced, order does matter. So since there are 12 face cards as choices in the first try, then 11 in the second I did (12)P(2) as the numerator. The total came out to be 132. For the denominator, which is the sample space, I did (52)P(2), since you are choosing 2 out of 52 cards and got 2652 ways. So now I do : (12)P(2)/(52)P(2)= 132/2652= 11/221 What is the probability that the cards are a diamond and a spade? I was a little confused on this one, but I tried. Even though I said order matters and to use permutations, I felt like for the numerator I was supposed to combinations since choosing a diamond and spade didn't matter. Since there are 13 spades and 13 diamonds, for the numerator I did (13)C(1)*(13)C(1) which equals to 169 ways of choosing a diamond and a spade. For the sample space I stayed with (52)P(2) since order still matter on this one which equals to 2652 ways. So the final step is : [(13)C(1)*(13)C(1)]/(52)P(2)= 169/2652= 13/204 I would like to know if I did these questions right, because i'm a little confused between using combinations and permutations.",,"['probability', 'permutations', 'combinations']"
57,Expected number of digits of the smallest prime factor of $1270000^{16384}+1$,Expected number of digits of the smallest prime factor of,1270000^{16384}+1,"The number $N\ :=\ 1270000^{16384}+1$ with $100,005$ digits is given. Given, that $N$ is composite and does not have a prime factor below $2\times 10^{13}$, what is the expected number of digits of the smallest prime factor ? On factordb, I checked, that N is composite and I searched a prime factor upto about $2\times 10^{13}$ without finding one. I wonder, how likely it is, that the search is still unsucessful, if I continue to, lets say, $10^{18}$. If the number had no special form, the probablity that there is no prime factor below $10^{26}$, would be about $\frac{1}{2}$, but the situation should be different for $N$ because every prime factor $p$ must be of the form $2^{15}k+1$ If someone wonders, how I arrived to this number. I sieved out the factors of $k\times 1270000^{16384}+1$ and the program skipped $k=1$, I thought the reason would be that $N$ has a small prime factor, which is not the case. The $1270000$ comes from rounding up the smallest number $n$, such that $n^{16384}$ has more than $100,000$ digits.","The number $N\ :=\ 1270000^{16384}+1$ with $100,005$ digits is given. Given, that $N$ is composite and does not have a prime factor below $2\times 10^{13}$, what is the expected number of digits of the smallest prime factor ? On factordb, I checked, that N is composite and I searched a prime factor upto about $2\times 10^{13}$ without finding one. I wonder, how likely it is, that the search is still unsucessful, if I continue to, lets say, $10^{18}$. If the number had no special form, the probablity that there is no prime factor below $10^{26}$, would be about $\frac{1}{2}$, but the situation should be different for $N$ because every prime factor $p$ must be of the form $2^{15}k+1$ If someone wonders, how I arrived to this number. I sieved out the factors of $k\times 1270000^{16384}+1$ and the program skipped $k=1$, I thought the reason would be that $N$ has a small prime factor, which is not the case. The $1270000$ comes from rounding up the smallest number $n$, such that $n^{16384}$ has more than $100,000$ digits.",,"['probability', 'number-theory', 'divisibility', 'prime-factorization']"
58,existence of probability density function,existence of probability density function,,"Let $X$ be a random variable on a probability space $(\Omega,\Sigma,\mathbb{P})$. Let $F_X$ denote the probability distribution function of $X$ given by: $$F_X(y) = P(X\leq y) \text{ for } y \in \mathbb{R}$$ Then, I came across the following: Almost everywhere differentiability of $F_X$ does not imply that the probability density function of $X$, denoted by $f_X$, exists. For example, if $X$ is a random variable such that $P(X = \frac{1}{2}) = 1$, then $F_X$ is differentiable almost everywhere but $f_X$ does not exist. What is the reason that $f_X$ does not exist for $X$ such that $P(X = \frac{1}{2}) = 1$?","Let $X$ be a random variable on a probability space $(\Omega,\Sigma,\mathbb{P})$. Let $F_X$ denote the probability distribution function of $X$ given by: $$F_X(y) = P(X\leq y) \text{ for } y \in \mathbb{R}$$ Then, I came across the following: Almost everywhere differentiability of $F_X$ does not imply that the probability density function of $X$, denoted by $f_X$, exists. For example, if $X$ is a random variable such that $P(X = \frac{1}{2}) = 1$, then $F_X$ is differentiable almost everywhere but $f_X$ does not exist. What is the reason that $f_X$ does not exist for $X$ such that $P(X = \frac{1}{2}) = 1$?",,"['probability', 'probability-theory']"
59,Probability of an infinite subsequence in a randomly generated sequence of order type $\omega_1$,Probability of an infinite subsequence in a randomly generated sequence of order type,\omega_1,"Given for example $\omega_1$ coin tosses (i.e. a mapping from the elements of $\omega_1$ to $\{H,T\}$ with independent probabilities half), what is the probability that there is an infinite subsequence subinterval [ corrected following comments ] consisting only of heads? Is this question even well defined? Does it depend on the set theory axiomatisation? For a countable sequence of tosses (i.e. $<\omega_1$) the answer is presumably zero, as an infinite subinterval of heads is as likely as any of the uncountably many other possible infinite subintervals.","Given for example $\omega_1$ coin tosses (i.e. a mapping from the elements of $\omega_1$ to $\{H,T\}$ with independent probabilities half), what is the probability that there is an infinite subsequence subinterval [ corrected following comments ] consisting only of heads? Is this question even well defined? Does it depend on the set theory axiomatisation? For a countable sequence of tosses (i.e. $<\omega_1$) the answer is presumably zero, as an infinite subinterval of heads is as likely as any of the uncountably many other possible infinite subintervals.",,"['probability', 'sequences-and-series', 'elementary-set-theory']"
60,Mean Square Error of Monte Carlo,Mean Square Error of Monte Carlo,,"Trying to develop the expression for the Mean Square Error (MSE) of Monte Carlo, I found myself a bit lost when going through a simple proof in the literature. I am working in the context of mathematical finance, where the aim is to find an approximation to the ""true value"" of the function $V$. Let's define $V$ as $V = \mathbb E[f]$, whose discrete approximation is $\hat{V} = \mathbb E[\hat{f}]$ and its Monte Carlo estimate is formulated as $$ \hat{Y} = \frac{1}{N}\sum_{n=1}^{N}\hat{f}^{\,(n)} $$ The expression for the MSE in the aforementioned proof is obtained by doing the following: \begin{eqnarray*} \mathbb E\left[\left(\hat{Y} - V \right)^2 \right]&=&\mathbb E\left[\left(\hat{Y} - \mathbb E\left[\,\hat{f}\,\right] + \mathbb E\left[\,\hat{f}\,\right] - \mathbb E\left[\,f\,\right] \right)^2 \right] = \\ &=& \mathbb E[(\hat{Y} - \mathbb E\left[\,\hat{f}\,\right])^2] + \left(\mathbb E[\hat{f}] - \mathbb E[f] \right)^2 + \mathbb E\left[2\left(\hat{Y} - \mathbb E [\,\hat{f}]\right)\left(\mathbb E\left[\,\hat{f}\,\right] - \mathbb E\left[\,f\,\right]\right) \right] = \\ &=& \frac{1}{N}\mathrm{Var}(\,\hat{f}\,) + \left(\mathbb E[\hat{f}] - \mathbb E[f] \right)^2 \end{eqnarray*} It is the last line that flusters me. Particularly, my concerns are: I am assuming that the third element of the second line on the right side tends to $0$ since $\mathbb E [\hat{f}] \rightarrow \mathbb E [f]$ and that's why it disappears in the next line, but is this the real reason why this element dissappears? I would think that the first element of the second line, $\mathbb E[(\hat{Y} - \mathbb E\left[\,\hat{f}\,\right])^2]$ is exactly the variance of $\hat{Y}$, so why do we obtain in the last line the variance term multiplied by the factor $\frac{1}{N}$? If we expanded the expression for $\mathrm{Var}(\hat{Y})$, we should obtain the common factor $\frac{1}{N^2}$ by the variance property, and not just $N^{-1}$. Maybe it is just that I got unnecessarily confused with simple math, so any kind of clarification would be welcome.","Trying to develop the expression for the Mean Square Error (MSE) of Monte Carlo, I found myself a bit lost when going through a simple proof in the literature. I am working in the context of mathematical finance, where the aim is to find an approximation to the ""true value"" of the function $V$. Let's define $V$ as $V = \mathbb E[f]$, whose discrete approximation is $\hat{V} = \mathbb E[\hat{f}]$ and its Monte Carlo estimate is formulated as $$ \hat{Y} = \frac{1}{N}\sum_{n=1}^{N}\hat{f}^{\,(n)} $$ The expression for the MSE in the aforementioned proof is obtained by doing the following: \begin{eqnarray*} \mathbb E\left[\left(\hat{Y} - V \right)^2 \right]&=&\mathbb E\left[\left(\hat{Y} - \mathbb E\left[\,\hat{f}\,\right] + \mathbb E\left[\,\hat{f}\,\right] - \mathbb E\left[\,f\,\right] \right)^2 \right] = \\ &=& \mathbb E[(\hat{Y} - \mathbb E\left[\,\hat{f}\,\right])^2] + \left(\mathbb E[\hat{f}] - \mathbb E[f] \right)^2 + \mathbb E\left[2\left(\hat{Y} - \mathbb E [\,\hat{f}]\right)\left(\mathbb E\left[\,\hat{f}\,\right] - \mathbb E\left[\,f\,\right]\right) \right] = \\ &=& \frac{1}{N}\mathrm{Var}(\,\hat{f}\,) + \left(\mathbb E[\hat{f}] - \mathbb E[f] \right)^2 \end{eqnarray*} It is the last line that flusters me. Particularly, my concerns are: I am assuming that the third element of the second line on the right side tends to $0$ since $\mathbb E [\hat{f}] \rightarrow \mathbb E [f]$ and that's why it disappears in the next line, but is this the real reason why this element dissappears? I would think that the first element of the second line, $\mathbb E[(\hat{Y} - \mathbb E\left[\,\hat{f}\,\right])^2]$ is exactly the variance of $\hat{Y}$, so why do we obtain in the last line the variance term multiplied by the factor $\frac{1}{N}$? If we expanded the expression for $\mathrm{Var}(\hat{Y})$, we should obtain the common factor $\frac{1}{N^2}$ by the variance property, and not just $N^{-1}$. Maybe it is just that I got unnecessarily confused with simple math, so any kind of clarification would be welcome.",,"['probability', 'probability-theory', 'stochastic-processes', 'stochastic-calculus', 'stochastic-analysis']"
61,Coin toss game - Probability of winning,Coin toss game - Probability of winning,,"Question: Two players A and B, alternatively toss a fair coin (A tosses the coin first, then B, than A again, etc.). The sequence of heads and tails is recorded and if there is head followed by a tail (HT subsequence), the game ends and the person who tosses the tail wins. What is the probability that A wins the game? Solution: I was told the following solution. If $P(A),P(B)$ are the probabilities that A, B win, then we can write: $$P(A) = P(A\mid H)P(H)+P(A\mid T)P(T)$$ where $P(H)=P(T)=1/2$ are the probabilites that A gets H, T in the first toss respectively. Then $$P(A\mid T)=P(B)=1-P(A)\tag{1}$$ and $$P(A\mid H)=1/2\cdot 0+1/2\cdot (1-P(A\mid H))\tag{2}$$ which gives $P(A)=4/9$. I am trying to understand equation (1),(2). I understand that if A gets T then the game essentially starts over with B being the first player. But since we want to know the probability of A winning, then shouldn't we have $P(A\mid T)=1-P(B)$? As for (2) I have not been able to find a starting point. Would somebody be able to explain their derivation?","Question: Two players A and B, alternatively toss a fair coin (A tosses the coin first, then B, than A again, etc.). The sequence of heads and tails is recorded and if there is head followed by a tail (HT subsequence), the game ends and the person who tosses the tail wins. What is the probability that A wins the game? Solution: I was told the following solution. If $P(A),P(B)$ are the probabilities that A, B win, then we can write: $$P(A) = P(A\mid H)P(H)+P(A\mid T)P(T)$$ where $P(H)=P(T)=1/2$ are the probabilites that A gets H, T in the first toss respectively. Then $$P(A\mid T)=P(B)=1-P(A)\tag{1}$$ and $$P(A\mid H)=1/2\cdot 0+1/2\cdot (1-P(A\mid H))\tag{2}$$ which gives $P(A)=4/9$. I am trying to understand equation (1),(2). I understand that if A gets T then the game essentially starts over with B being the first player. But since we want to know the probability of A winning, then shouldn't we have $P(A\mid T)=1-P(B)$? As for (2) I have not been able to find a starting point. Would somebody be able to explain their derivation?",,"['probability', 'probability-theory']"
62,Real Applications of Markov's Inequality,Real Applications of Markov's Inequality,,"When is Markov's Inequality useful?  It seems to me that it's a very rough upper bound.  For example, if we roll a die and want to know the probability of the result being a 5 or greater we have that there is at most a 3.5/5 chance.  That's huge relative to the actual chance.  Am I misunderstanding?","When is Markov's Inequality useful?  It seems to me that it's a very rough upper bound.  For example, if we roll a die and want to know the probability of the result being a 5 or greater we have that there is at most a 3.5/5 chance.  That's huge relative to the actual chance.  Am I misunderstanding?",,"['probability', 'inequality']"
63,Central Limit Theorem proof.,Central Limit Theorem proof.,,"I am trying to understand the proof of the Central Limit Theorem in my book. However, I don't really understand  what is going on. I know the proof is assuming that the moment generating functions of each $W_{i}$ exists. Then we will show that eventually the limit of these generating functions approach $e^{t^2/2}$. Can someone please explain what is happening? This is almost in the middle of the proof. Given the following. $M(0) = 1, M^{(1)}(0) = 0, M^{(2)}(0) = 1 $ They apply Taylor's theorem, then to write $M(t)$ and get $M(t) = 1 + M^{(1)}(0)t +  \frac{t^2}{2} M^{2}(r) = 1 + \frac{t^2}{2} M^{2}(r) $. However, I don't understand why the proof stops after the second derivative in the Taylor expansion from the above expression. In addition, I the book does not explain what it's doing in the following algebraic expression . $\lim_{n\rightarrow \infty}$$[M(\frac{t}{\sqrt n})]^n$ = $\lim_{n\rightarrow \infty}$ [$1 + \frac{t^2}{2n} M^{2}(s)]^n$ = exp $\lim_{n\rightarrow \infty} n$ $\ln[$1$ + \frac{t^2}{2n} M^{2}(s)]^n$ = exp  $\lim_{n\rightarrow \infty} \frac{t^{2}}{2}M^{2}(s)$$\frac{ln[ 1 + \frac{t^2}{2n} M^{2}(s)] - ln(1)}{\frac{t^2}{2n} M^{2}(s)}$. and $|s| < \frac{|t|}{\sqrt n}$. I think they are taking natural log on both sides, and applying the the quotient rule. Can someone please help me understand the above. I would really appreciate it, since I will be able to understand the proof. Thank you very much.","I am trying to understand the proof of the Central Limit Theorem in my book. However, I don't really understand  what is going on. I know the proof is assuming that the moment generating functions of each $W_{i}$ exists. Then we will show that eventually the limit of these generating functions approach $e^{t^2/2}$. Can someone please explain what is happening? This is almost in the middle of the proof. Given the following. $M(0) = 1, M^{(1)}(0) = 0, M^{(2)}(0) = 1 $ They apply Taylor's theorem, then to write $M(t)$ and get $M(t) = 1 + M^{(1)}(0)t +  \frac{t^2}{2} M^{2}(r) = 1 + \frac{t^2}{2} M^{2}(r) $. However, I don't understand why the proof stops after the second derivative in the Taylor expansion from the above expression. In addition, I the book does not explain what it's doing in the following algebraic expression . $\lim_{n\rightarrow \infty}$$[M(\frac{t}{\sqrt n})]^n$ = $\lim_{n\rightarrow \infty}$ [$1 + \frac{t^2}{2n} M^{2}(s)]^n$ = exp $\lim_{n\rightarrow \infty} n$ $\ln[$1$ + \frac{t^2}{2n} M^{2}(s)]^n$ = exp  $\lim_{n\rightarrow \infty} \frac{t^{2}}{2}M^{2}(s)$$\frac{ln[ 1 + \frac{t^2}{2n} M^{2}(s)] - ln(1)}{\frac{t^2}{2n} M^{2}(s)}$. and $|s| < \frac{|t|}{\sqrt n}$. I think they are taking natural log on both sides, and applying the the quotient rule. Can someone please help me understand the above. I would really appreciate it, since I will be able to understand the proof. Thank you very much.",,"['probability', 'statistics', 'central-limit-theorem']"
64,A simple example of an incomplete probability space?,A simple example of an incomplete probability space?,,A probability space is complete if every subset of a set of measure zero is measurable. The probability space is incomplete if otherwise. But is there a simple example of an incomplete probability space?,A probability space is complete if every subset of a set of measure zero is measurable. The probability space is incomplete if otherwise. But is there a simple example of an incomplete probability space?,,"['probability', 'measure-theory']"
65,Probability of royal flush dealt to table of n players,Probability of royal flush dealt to table of n players,,"I'm interested in calculating the probability of a royal flush being dealt to ANY of the n players seated at a Texas Hold'em poker game (2 hole cards, 5 community cards). The probability of YOU being dealt a royal flush is readily available on the internet but I'm having trouble finding this particular probability, especially for variable number of players. There are $133,784,560$ 7-hand combinations, $4,324$ of which result in a royal flush so the probability of YOU being dealt a royal flush is $\approx 0.00003232$ It seems that simply $n*0.00003232$ will give a good approximation to the probability I'm after but something seems off about this in that 1 player receiving a royal flush would negate anyone else from receiving a royal flush (except in the case of a royal being on the board). In an attempt to come up with an exact solution I've done the following: Number of ways to deal n-handed NLHE hand (sample space): $$52!/(52-2n-5)! = 52!/(47-2n)!$$ Number of ways royal can be dealt using BOTH hole cards: $$4*n*5*4*{5\choose 3}*3!*(47!/(47-2n)!)$$ (suit,player,1st hole card,2nd hole card,community placement,community order,distribute remaining cards) Number of ways royal can be dealt using ONLY 1 hole card: $$4*n*{2\choose 1}*5*{5\choose 4}*4!*(47!/(47-2n)!)$$ (suit,player,which hole card,hole card,community placement,community order,distribute remaining cards) And finally, number of ways royal can be dealt using 0 hole cards (royal on board) $$4*5!*(47!/(47-2n)!)$$ (suit,community cards,remaining cards) Thus giving the probability of a royal being dealt to ANY player in an n-handed NLHE game being: $$((4*n*5*4*{5\choose 3}*3!*(47!/(47-2n)!)+4*n*{2\choose 1}*5*{5\choose 4}*4!*(47!/(47-2n)!)+4*5!*(47!/(47-2n)!))/(52!/(47-2n)!))$$ For n=1 this gives the $\approx 0.00003232$ as we should expect and for n=9 this yields $181/6497400\approx0.000278573$ which is, as I'd expect, quite close to $9*0.00003232\approx0.00029088$. So, I have 3 real questions: 1.) Are my intuitions about n*[7 card probability] being ONLY an approximation correct? 2.) Is my ""exact"" equation correct? Besides my n=1,n=9 substitutions are there are external sources I can check this against? It's been awhile since I've worked with probability and I'm not too confident in my solution. 3.) Is there a more succinct way of expressing the exact probability given the 7-card probability. Perhaps not simply n*[7 card probability] but a variation of this?","I'm interested in calculating the probability of a royal flush being dealt to ANY of the n players seated at a Texas Hold'em poker game (2 hole cards, 5 community cards). The probability of YOU being dealt a royal flush is readily available on the internet but I'm having trouble finding this particular probability, especially for variable number of players. There are $133,784,560$ 7-hand combinations, $4,324$ of which result in a royal flush so the probability of YOU being dealt a royal flush is $\approx 0.00003232$ It seems that simply $n*0.00003232$ will give a good approximation to the probability I'm after but something seems off about this in that 1 player receiving a royal flush would negate anyone else from receiving a royal flush (except in the case of a royal being on the board). In an attempt to come up with an exact solution I've done the following: Number of ways to deal n-handed NLHE hand (sample space): $$52!/(52-2n-5)! = 52!/(47-2n)!$$ Number of ways royal can be dealt using BOTH hole cards: $$4*n*5*4*{5\choose 3}*3!*(47!/(47-2n)!)$$ (suit,player,1st hole card,2nd hole card,community placement,community order,distribute remaining cards) Number of ways royal can be dealt using ONLY 1 hole card: $$4*n*{2\choose 1}*5*{5\choose 4}*4!*(47!/(47-2n)!)$$ (suit,player,which hole card,hole card,community placement,community order,distribute remaining cards) And finally, number of ways royal can be dealt using 0 hole cards (royal on board) $$4*5!*(47!/(47-2n)!)$$ (suit,community cards,remaining cards) Thus giving the probability of a royal being dealt to ANY player in an n-handed NLHE game being: $$((4*n*5*4*{5\choose 3}*3!*(47!/(47-2n)!)+4*n*{2\choose 1}*5*{5\choose 4}*4!*(47!/(47-2n)!)+4*5!*(47!/(47-2n)!))/(52!/(47-2n)!))$$ For n=1 this gives the $\approx 0.00003232$ as we should expect and for n=9 this yields $181/6497400\approx0.000278573$ which is, as I'd expect, quite close to $9*0.00003232\approx0.00029088$. So, I have 3 real questions: 1.) Are my intuitions about n*[7 card probability] being ONLY an approximation correct? 2.) Is my ""exact"" equation correct? Besides my n=1,n=9 substitutions are there are external sources I can check this against? It's been awhile since I've worked with probability and I'm not too confident in my solution. 3.) Is there a more succinct way of expressing the exact probability given the 7-card probability. Perhaps not simply n*[7 card probability] but a variation of this?",,"['probability', 'poker']"
66,"Probability that $\max(X,Y)> a \min(X,Y)$ where $X$ and $Y$ are independent and uniformly distributed on $[0,1]$",Probability that  where  and  are independent and uniformly distributed on,"\max(X,Y)> a \min(X,Y) X Y [0,1]","Two independent random variable $X$ and $Y$ having probability density functions uniform in the interval [0,1]. When $a \geqslant 1$, what is the probability that $\max(X,Y)> a \min(X,Y)$,  in terms of $a$? I am not able to get the concept and principle behind this question.","Two independent random variable $X$ and $Y$ having probability density functions uniform in the interval [0,1]. When $a \geqslant 1$, what is the probability that $\max(X,Y)> a \min(X,Y)$,  in terms of $a$? I am not able to get the concept and principle behind this question.",,"['probability', 'probability-distributions']"
67,Find the probability that $B$ obtains a new kidney.,Find the probability that  obtains a new kidney.,B,"Look at this problem: Two individuals, $A$ and $B$, both require kidney transplants. If she   does not receive a new kidney, then $A$ will die after an exponential   time with rate $\mu_A$, and $B$ after an exponential time with rate   $\mu_B$. New kidneys arrive in accordance with a Poisson process   having rate $\lambda$. It has been decided that the first kidney will   go to $A$ (or to $B$ if $B$ is alive and $A$ is not at that time) and   the next one to $B$ (if still living). What is the probability that $B$ obtains a new kidney? The solution that the author has offered for this problem is $$\frac{\lambda}{\lambda+\mu_B}\frac{\lambda+\mu_A}{\lambda+\mu_A+\mu_B}$$ but I have another answer to this problem as below: $$\begin{align*}Pr(B\text{ obtains kidney})&=Pr(B\text{ obtains kidney}\mid\space A\space is \space alive)\cdot Pr(A\space is \space alive)\\[0.2cm]&+Pr(B\text{ obtains kidney}\mid\space A\space is\space not \space alive)\cdot Pr(A\space is\space not \space alive)\\[0.2cm]&=\frac{\lambda}{\lambda+\mu_B}\frac{\mu_A}{\lambda+\mu_A}+\frac{\lambda}{\lambda+\mu_A}Pr(B\text{ obtains kidney}\mid \space A\space is \space alive)\end{align*}$$ Now as when $A$ is alive while the first kidney arrives, $B$ must wait for the second kidney and the distribution of the second kidney arrival time is a Gamma distribution with parameters $n=2,\lambda$ and hence  $$Pr(B\text{ obtains kidney}\mid \space A\space is \space alive)=Pr(X_1+X_2<X_B)$$ in which $X_1+X_2$ is the time of arrival of the second kidney, and $X_B$ is the death time of $B$, so we have  $$Pr(B\text{ obtains kidney}\mid \space A\space is \space alive)=Pr(X_1+X_2<X_B)=\left(\frac{\lambda}{\lambda+\mu_B}\right)^2$$ So we have  $$\begin{align*}Pr(B\text{ obtains kidney})&=\frac{\lambda}{\lambda+\mu_B}\frac{\mu_A}{\lambda+\mu_A}+\frac{\lambda}{\lambda+\mu_A}Pr(B\text{ obtains kidney}\mid \space A\space is \space alive)\\&=\frac{\lambda}{\lambda+\mu_B}\frac{\mu_A}{\lambda+\mu_A}+\frac{\lambda}{\lambda+\mu_A}\left(\frac{\lambda}{\lambda+\mu_B}\right)^2\end{align*}$$ and obviously is different with the answer of the book. Can any one help me to find my mistake?","Look at this problem: Two individuals, $A$ and $B$, both require kidney transplants. If she   does not receive a new kidney, then $A$ will die after an exponential   time with rate $\mu_A$, and $B$ after an exponential time with rate   $\mu_B$. New kidneys arrive in accordance with a Poisson process   having rate $\lambda$. It has been decided that the first kidney will   go to $A$ (or to $B$ if $B$ is alive and $A$ is not at that time) and   the next one to $B$ (if still living). What is the probability that $B$ obtains a new kidney? The solution that the author has offered for this problem is $$\frac{\lambda}{\lambda+\mu_B}\frac{\lambda+\mu_A}{\lambda+\mu_A+\mu_B}$$ but I have another answer to this problem as below: $$\begin{align*}Pr(B\text{ obtains kidney})&=Pr(B\text{ obtains kidney}\mid\space A\space is \space alive)\cdot Pr(A\space is \space alive)\\[0.2cm]&+Pr(B\text{ obtains kidney}\mid\space A\space is\space not \space alive)\cdot Pr(A\space is\space not \space alive)\\[0.2cm]&=\frac{\lambda}{\lambda+\mu_B}\frac{\mu_A}{\lambda+\mu_A}+\frac{\lambda}{\lambda+\mu_A}Pr(B\text{ obtains kidney}\mid \space A\space is \space alive)\end{align*}$$ Now as when $A$ is alive while the first kidney arrives, $B$ must wait for the second kidney and the distribution of the second kidney arrival time is a Gamma distribution with parameters $n=2,\lambda$ and hence  $$Pr(B\text{ obtains kidney}\mid \space A\space is \space alive)=Pr(X_1+X_2<X_B)$$ in which $X_1+X_2$ is the time of arrival of the second kidney, and $X_B$ is the death time of $B$, so we have  $$Pr(B\text{ obtains kidney}\mid \space A\space is \space alive)=Pr(X_1+X_2<X_B)=\left(\frac{\lambda}{\lambda+\mu_B}\right)^2$$ So we have  $$\begin{align*}Pr(B\text{ obtains kidney})&=\frac{\lambda}{\lambda+\mu_B}\frac{\mu_A}{\lambda+\mu_A}+\frac{\lambda}{\lambda+\mu_A}Pr(B\text{ obtains kidney}\mid \space A\space is \space alive)\\&=\frac{\lambda}{\lambda+\mu_B}\frac{\mu_A}{\lambda+\mu_A}+\frac{\lambda}{\lambda+\mu_A}\left(\frac{\lambda}{\lambda+\mu_B}\right)^2\end{align*}$$ and obviously is different with the answer of the book. Can any one help me to find my mistake?",,"['probability', 'probability-theory', 'stochastic-processes', 'poisson-distribution']"
68,Showing countable additivitiy of Lebesgue measure,Showing countable additivitiy of Lebesgue measure,,"The following is taken from the classic Probability and Measure by Patrick Billingsley, Theorem 2.2 (page 26 in the 3rd edition). I have a question on his proof, but I give the necessary defintions to make my question self-contained. Denote by $\mathcal B_0$ the field of all finite unions of half-open sub-intervals $(a,b]$ of $(0,1]$, i.e. it is a system of sets containing all half-open sub-intervals of $(0,1]$ and closed under finite intersection and complementation (and therefore also finite unions). It is easy to see that $\mathcal B_0$ is exactly the set $$  \mathcal B_0 = \left\{ \bigcup_{i=1}^n (a_i, b_i] : (a_i, b_i] \subseteq (0,1] ~ \mbox{disjoint} \right\} $$ of all subsets of $(0,1]$ that could be written as finite unions of disjoint subintervals of $(0,1]$. The following property is needed in the following: If $I = \bigcup_k I_k$ and the $I_k$ are disjoint, then $|I| = \sum_k |I_k|$. (*) Then define a mapping $\lambda : \mathcal B_0 \to [0,1]$ by $$  \lambda(A) = \sum_{i=1}^n (b_i - a_i). $$ Then $\lambda : \mathcal B_0 \to [0,1]$ is countable additive , i.e. we have for $A \in \mathcal B_0$ and $A_1, A_2, \ldots \in \mathcal B_0$ disjoint and $A = \bigcup_{k=1}^{\infty} A_k$ $$  \lambda(A) = \lambda(\bigcup_{k=1}^{\infty} A_k) = \sum_{k=1}^{\infty}\lambda(A_k). $$ Proof: Suppose that $A = \bigcup_{k=1}^{\infty} A_k$, where $A$ and the $A_k$ are $\mathcal B_0$-sets and the $A_k$ are disjoint. Then $A = \bigcup_{i=1}^n I_i$ and $A_k = \bigcup_{j=1}^{m_k} J_{kj}$ are disjoint unions of subintervals of $(0,1]$, and the above, and (*) give \begin{align*}  \lambda(A) & = \sum_{i=1}^{n} |I_i| = \sum_{i=1}^n \sum_{k=1}^{\infty} \sum_{j=1}^{m_k} |I_i \cap J_{kj}| \\  & = \sum_{k=1}^{\infty} \sum_{j=1}^{m_k} |J_{kj}| = \sum_{k=1}^{\infty} \lambda(A_k).  \qquad \square \end{align*} To quote the book: [...] proving countable additivity on $\mathcal J$ [i.e. the set of all subintervals of $(0,1]$] requires the deeper property of compactnness. Where exactly is compactness used in the above proof? I do not see where it is applied.","The following is taken from the classic Probability and Measure by Patrick Billingsley, Theorem 2.2 (page 26 in the 3rd edition). I have a question on his proof, but I give the necessary defintions to make my question self-contained. Denote by $\mathcal B_0$ the field of all finite unions of half-open sub-intervals $(a,b]$ of $(0,1]$, i.e. it is a system of sets containing all half-open sub-intervals of $(0,1]$ and closed under finite intersection and complementation (and therefore also finite unions). It is easy to see that $\mathcal B_0$ is exactly the set $$  \mathcal B_0 = \left\{ \bigcup_{i=1}^n (a_i, b_i] : (a_i, b_i] \subseteq (0,1] ~ \mbox{disjoint} \right\} $$ of all subsets of $(0,1]$ that could be written as finite unions of disjoint subintervals of $(0,1]$. The following property is needed in the following: If $I = \bigcup_k I_k$ and the $I_k$ are disjoint, then $|I| = \sum_k |I_k|$. (*) Then define a mapping $\lambda : \mathcal B_0 \to [0,1]$ by $$  \lambda(A) = \sum_{i=1}^n (b_i - a_i). $$ Then $\lambda : \mathcal B_0 \to [0,1]$ is countable additive , i.e. we have for $A \in \mathcal B_0$ and $A_1, A_2, \ldots \in \mathcal B_0$ disjoint and $A = \bigcup_{k=1}^{\infty} A_k$ $$  \lambda(A) = \lambda(\bigcup_{k=1}^{\infty} A_k) = \sum_{k=1}^{\infty}\lambda(A_k). $$ Proof: Suppose that $A = \bigcup_{k=1}^{\infty} A_k$, where $A$ and the $A_k$ are $\mathcal B_0$-sets and the $A_k$ are disjoint. Then $A = \bigcup_{i=1}^n I_i$ and $A_k = \bigcup_{j=1}^{m_k} J_{kj}$ are disjoint unions of subintervals of $(0,1]$, and the above, and (*) give \begin{align*}  \lambda(A) & = \sum_{i=1}^{n} |I_i| = \sum_{i=1}^n \sum_{k=1}^{\infty} \sum_{j=1}^{m_k} |I_i \cap J_{kj}| \\  & = \sum_{k=1}^{\infty} \sum_{j=1}^{m_k} |J_{kj}| = \sum_{k=1}^{\infty} \lambda(A_k).  \qquad \square \end{align*} To quote the book: [...] proving countable additivity on $\mathcal J$ [i.e. the set of all subintervals of $(0,1]$] requires the deeper property of compactnness. Where exactly is compactness used in the above proof? I do not see where it is applied.",,"['probability', 'analysis', 'measure-theory', 'probability-theory', 'lebesgue-measure']"
69,"$E(X|Y)=0$ and $E(X|Z)=0$ imply $E(X|Y,Z)=0$?",and  imply ?,"E(X|Y)=0 E(X|Z)=0 E(X|Y,Z)=0","Is it true that $$ E(X|Y)=0\quad\text{and}\quad E(X|Z)=0 $$ imply $E(X|Y,Z)=0$? From iterated expectations, I understand that $E(X|Y,Z)=0$ implies both $E(X|Y)=0$ and $E(X|Z)=0$ but I don't know how to think about it in the reverse direction.","Is it true that $$ E(X|Y)=0\quad\text{and}\quad E(X|Z)=0 $$ imply $E(X|Y,Z)=0$? From iterated expectations, I understand that $E(X|Y,Z)=0$ implies both $E(X|Y)=0$ and $E(X|Z)=0$ but I don't know how to think about it in the reverse direction.",,"['probability', 'probability-theory']"
70,CDF of probability distribution with replacement [duplicate],CDF of probability distribution with replacement [duplicate],,"This question already has answers here : What is the probability of rolling $n$ dice until each side appears at least once? (2 answers) Closed 8 years ago . I want to get every color of gumball in a gumball machine (where there are 16 types of gumballs, each with a 1/16 chance of obtaining a particular color [assume there are an infinite amount of gumballs]). I'm interested in knowing what the PMF (as a function of # of attempts) of getting every single gumball. ie. what is the probability of getting all 16 colors of gumballs in n trials? I'd rather not have to brute-force the answer and am hoping for a somewhat elegant solution (maybe we can use the negative binomial distribution to find this?).","This question already has answers here : What is the probability of rolling $n$ dice until each side appears at least once? (2 answers) Closed 8 years ago . I want to get every color of gumball in a gumball machine (where there are 16 types of gumballs, each with a 1/16 chance of obtaining a particular color [assume there are an infinite amount of gumballs]). I'm interested in knowing what the PMF (as a function of # of attempts) of getting every single gumball. ie. what is the probability of getting all 16 colors of gumballs in n trials? I'd rather not have to brute-force the answer and am hoping for a somewhat elegant solution (maybe we can use the negative binomial distribution to find this?).",,"['probability', 'probability-distributions', 'coupon-collector']"
71,Why do we use only 1/2 for continuity correction in case of approximating binomial random varable to a standard normal random variable?,Why do we use only 1/2 for continuity correction in case of approximating binomial random varable to a standard normal random variable?,,"I have read about continuity correction in case of approximating a binomial random variable to a standard normal variable. But in all the examples , they only use 1/2 as a continuity correction factor.Let's take an example: Let X be the number of times that a fair coin, flipped 40 times,lands heads. Find the probability that X=20.  X is a binomial random variable with mean 20 and variance 10. The actual answer is 12.54% Use the normal approximation for this question. \begin{align}  Ans: P(X=20) =                  P(19.5 <= X <= 20.5)                                               = P( \frac{19.5-20}{\sqrt(10)} < \frac{X-20}{\sqrt(10)} < \frac{20.5-20}{\sqrt(10)} )                          = P(-0.16 < \frac{X-20}{\sqrt(10)}< 0.16)              = fi(-0.16) - fi(0.16)                = 12.72 \% \end{align} ; where fi is the distribution function of a normal random variable This is pretty close to the actual answer.. Now if I could have written \begin{align}    P(X=20) =                  P(19.99 <= X <= 20.01)                                               = P( \frac{19.99-20}{\sqrt(10)} < \frac{X-20}{\sqrt(10)} < \frac{20.01-20}{\sqrt(10)} )                          = P(-0.031 < \frac{X-20}{\sqrt(10)}< 0.031)              = fi(0.031) - fi(-0.031)                = 2.4 \% \end{align}  I have brought the range very close to 20 instead of just using (19.5,20.5) . Why is the approximation in second case worst than that in first case ??","I have read about continuity correction in case of approximating a binomial random variable to a standard normal variable. But in all the examples , they only use 1/2 as a continuity correction factor.Let's take an example: Let X be the number of times that a fair coin, flipped 40 times,lands heads. Find the probability that X=20.  X is a binomial random variable with mean 20 and variance 10. The actual answer is 12.54% Use the normal approximation for this question. \begin{align}  Ans: P(X=20) =                  P(19.5 <= X <= 20.5)                                               = P( \frac{19.5-20}{\sqrt(10)} < \frac{X-20}{\sqrt(10)} < \frac{20.5-20}{\sqrt(10)} )                          = P(-0.16 < \frac{X-20}{\sqrt(10)}< 0.16)              = fi(-0.16) - fi(0.16)                = 12.72 \% \end{align} ; where fi is the distribution function of a normal random variable This is pretty close to the actual answer.. Now if I could have written \begin{align}    P(X=20) =                  P(19.99 <= X <= 20.01)                                               = P( \frac{19.99-20}{\sqrt(10)} < \frac{X-20}{\sqrt(10)} < \frac{20.01-20}{\sqrt(10)} )                          = P(-0.031 < \frac{X-20}{\sqrt(10)}< 0.031)              = fi(0.031) - fi(-0.031)                = 2.4 \% \end{align}  I have brought the range very close to 20 instead of just using (19.5,20.5) . Why is the approximation in second case worst than that in first case ??",,"['probability', 'probability-theory', 'probability-distributions', 'normal-distribution']"
72,Uniform distribution over disk,Uniform distribution over disk,,"Given two independent random variables $A$ uniform on $[0,1]$ and $B$ uniform on $\left[0,2\pi\right]$ . Obtain the joint pdf, transform to the disk, if necessary modify to obtain the uniform pdf over the disk. The joint pdf factors since $A$ and $B$ are independent. Hence, $f_{A,B}(a,b) = \frac{1}{2\pi}$ , right? Transformation to the disk is done by changing to polar coordinates, i.e. set $X = A \cos B$ and $Y = A \sin B$ , then $A = (X^2+Y^2)^{\frac{1}{2}}$ and $B = \tan^{-1} \frac{Y}{X}$ . Changing the coordinates then yields $f_{X,Y}(x,y) = \frac{1}{2\pi}(x^2+y^2)^{-\frac{1}{2}}$ , right? I do not know about the third part though. Any hints?","Given two independent random variables uniform on and uniform on . Obtain the joint pdf, transform to the disk, if necessary modify to obtain the uniform pdf over the disk. The joint pdf factors since and are independent. Hence, , right? Transformation to the disk is done by changing to polar coordinates, i.e. set and , then and . Changing the coordinates then yields , right? I do not know about the third part though. Any hints?","A [0,1] B \left[0,2\pi\right] A B f_{A,B}(a,b) = \frac{1}{2\pi} X = A \cos B Y = A \sin B A = (X^2+Y^2)^{\frac{1}{2}} B = \tan^{-1} \frac{Y}{X} f_{X,Y}(x,y) = \frac{1}{2\pi}(x^2+y^2)^{-\frac{1}{2}}","['probability', 'probability-distributions', 'uniform-distribution']"
73,Soft question: what are some elementary motivations of using functional analysis to study probability theory?,Soft question: what are some elementary motivations of using functional analysis to study probability theory?,,"Recently I've become curious about the links between functional analysis and probability theory.  What are some simple reasons why a functional analytic approach is preferable to a measure-theoretic approach?  (For example, why would it be interesting to do probability theory via the expectation operator $\mathbb{E}$ instead of the measure $\mathbb{P}$ on a probability space?) What are some interesting, yet elementary applications, examples and theorems taking this direction?","Recently I've become curious about the links between functional analysis and probability theory.  What are some simple reasons why a functional analytic approach is preferable to a measure-theoretic approach?  (For example, why would it be interesting to do probability theory via the expectation operator $\mathbb{E}$ instead of the measure $\mathbb{P}$ on a probability space?) What are some interesting, yet elementary applications, examples and theorems taking this direction?",,"['probability', 'analysis', 'functional-analysis', 'probability-theory', 'soft-question']"
74,independent increments property implies Markov property,independent increments property implies Markov property,,"Let $\{X_t\}_{t\in\mathbb R^+}$ be a stochastic process with values in $\mathbb R$. Suppose that $\{X_t\}$ has independent increments, namely for every $t_1<t_2<\ldots<t_k$ the random variables $X_{t_2}-X_{t_1}$, $X_{t_3}-X_{t_2}$, $\ldots,X_{t_k}-X_{t_{k-1}}$ are independent. I have to prove the Markov property, that is $$P(X_t\in B\,|\, \mathcal F_s)=P(X_t\in B\,|\, X_s)$$ for $s<t$, $B$ measurable and $\mathcal F_s=\sigma(X_s:s\le t)$. Can you help me in order to formalize the details of this proof? Thanks in advance.","Let $\{X_t\}_{t\in\mathbb R^+}$ be a stochastic process with values in $\mathbb R$. Suppose that $\{X_t\}$ has independent increments, namely for every $t_1<t_2<\ldots<t_k$ the random variables $X_{t_2}-X_{t_1}$, $X_{t_3}-X_{t_2}$, $\ldots,X_{t_k}-X_{t_{k-1}}$ are independent. I have to prove the Markov property, that is $$P(X_t\in B\,|\, \mathcal F_s)=P(X_t\in B\,|\, X_s)$$ for $s<t$, $B$ measurable and $\mathcal F_s=\sigma(X_s:s\le t)$. Can you help me in order to formalize the details of this proof? Thanks in advance.",,"['probability', 'stochastic-processes', 'conditional-probability']"
75,Guess the smallest number,Guess the smallest number,,"Three people play a game where each of them writes down a positive integer at the same time. The one who writes a unique and smallest number wins one dollar from every other person. This means if two people happen to write down the same number, then the third one gets one dollar from each of them regardless. Now what number would you put if you were to play this game?","Three people play a game where each of them writes down a positive integer at the same time. The one who writes a unique and smallest number wins one dollar from every other person. This means if two people happen to write down the same number, then the third one gets one dollar from each of them regardless. Now what number would you put if you were to play this game?",,"['probability', 'game-theory']"
76,Confidence interval for estimating probability of a biased coin,Confidence interval for estimating probability of a biased coin,,"Suppose we have a coin with an unknown probability $p$ of coming up heads and that of $1-p$ of coming up tails. Now, we repeatedly flip the coin $n$ times and record the results, heads turn up $X$ time, then we could estimate $p$ with $\hat p = X/n$ . The problem is how close $\hat p$ is to $p$ . For example, if we already know $0.4≤p≤0.6$ , to obtain an estimate of $\hat p$ that is within $5\%$ of the real $p$ , i.e. $$0.95p \le \hat p \le 1.05 p$$ how large does $n$ need to be if we want to ensure the probability the above confidence interval is at least 0.95? i.e. $$\mathbb{P}[0.95p \le \hat p \le 1.05p] \ge 0.95$$","Suppose we have a coin with an unknown probability of coming up heads and that of of coming up tails. Now, we repeatedly flip the coin times and record the results, heads turn up time, then we could estimate with . The problem is how close is to . For example, if we already know , to obtain an estimate of that is within of the real , i.e. how large does need to be if we want to ensure the probability the above confidence interval is at least 0.95? i.e.",p 1-p n X p \hat p = X/n \hat p p 0.4≤p≤0.6 \hat p 5\% p 0.95p \le \hat p \le 1.05 p n \mathbb{P}[0.95p \le \hat p \le 1.05p] \ge 0.95,"['probability', 'statistics', 'probability-theory']"
77,asymptotic normality and central limit theorem,asymptotic normality and central limit theorem,,Here's the question Can somebody explain the difference between asymptotic normality and central limit theorem?  They seem very similar to me.,Here's the question Can somebody explain the difference between asymptotic normality and central limit theorem?  They seem very similar to me.,,"['probability', 'statistics', 'statistical-inference']"
78,"$\exists c>0$, $\forall A\subseteq \Bbb R_{\ne 0}$ s.t. $|A|=n$, $\exists B\subseteq A$ s.t. $|B|>cn$ & $b_1+2b_2=2b_3+2b_3$ has no solutions in $B$.",",  s.t. ,  s.t.  &  has no solutions in .",\exists c>0 \forall A\subseteq \Bbb R_{\ne 0} |A|=n \exists B\subseteq A |B|>cn b_1+2b_2=2b_3+2b_3 B,"EXERCISE 2.7.2 fron Alon and Spencer's The Probabilistic Method . Prove that there is a positive constant $c$ so that every set $A$ of $n$ nonzero reals contains a subset $B\subseteq A$ of size $|B| > cn$ so that there are no $b_{1},b_{2},b_{3},b_{4}\in B$ satisfying $$b_{1}+2b_{2}=2b_{3}+2b_{4}\,.$$ My idea is to regard the elements of $A$ in modulo $m$ for some $m>0$ , then I must find numbers which they are not true in above relation in modulo $m$ .  This is just the Idea but my problem is to make rigid and exact.","EXERCISE 2.7.2 fron Alon and Spencer's The Probabilistic Method . Prove that there is a positive constant so that every set of nonzero reals contains a subset of size so that there are no satisfying My idea is to regard the elements of in modulo for some , then I must find numbers which they are not true in above relation in modulo .  This is just the Idea but my problem is to make rigid and exact.","c A n B\subseteq A |B| > cn b_{1},b_{2},b_{3},b_{4}\in B b_{1}+2b_{2}=2b_{3}+2b_{4}\,. A m m>0 m","['probability', 'combinatorics', 'number-theory', 'probabilistic-method']"
79,Uncorrelatedness and conditional expectation,Uncorrelatedness and conditional expectation,,"Two random variables $X$ and $Y$ . How are the following two statements related: $E(XY) = E(X)E(Y)$ , ( $X$ and $Y$ are called uncorrelated) $E(X\mid Y)= E(X)$ a.s., (what is this case called?) Does one imply the other, and/or are there counterexample to such implications, or are there some condition that can make one imply the other? Thanks! From a deleted reply, there is an interesting statement $E(X\mid Y) E(Y) = E(XY)$ a.s. I don't quite remember it correctly. Can anyone who can see it (with 10k reputation) verify that? I wonder when it is true? Any implication with the previous two statements?","Two random variables and . How are the following two statements related: , ( and are called uncorrelated) a.s., (what is this case called?) Does one imply the other, and/or are there counterexample to such implications, or are there some condition that can make one imply the other? Thanks! From a deleted reply, there is an interesting statement a.s. I don't quite remember it correctly. Can anyone who can see it (with 10k reputation) verify that? I wonder when it is true? Any implication with the previous two statements?",X Y E(XY) = E(X)E(Y) X Y E(X\mid Y)= E(X) E(X\mid Y) E(Y) = E(XY),"['probability', 'conditional-expectation']"
80,Convergence in distribution ( Two equivalent definitions),Convergence in distribution ( Two equivalent definitions),,"I read that for convergence in distribution it is equivalent to have that either the characteristic functions of the random variables convergence pointwise or we have that $F_{X_n} \rightarrow F_{X}$ pointwise, where $F$(the distribution function) is continuous. I could not find a proof of this, so I was wondering how hard it is to show? Does anybody here have a (Internet)-reference or could sketch the idea?","I read that for convergence in distribution it is equivalent to have that either the characteristic functions of the random variables convergence pointwise or we have that $F_{X_n} \rightarrow F_{X}$ pointwise, where $F$(the distribution function) is continuous. I could not find a proof of this, so I was wondering how hard it is to show? Does anybody here have a (Internet)-reference or could sketch the idea?",,"['real-analysis', 'probability']"
81,"What is the probability that when you place 8 towers on a chess-board, none of them can beat the other.","What is the probability that when you place 8 towers on a chess-board, none of them can beat the other.",,"What is the probability that when you place 8 towers on a chess-board, none of them can beat the other. Attempt: ${64 \choose 8}^{-1} \approx1$ in $4\ 400\ 000\ 000$ Correct answer: ${64 \choose 8}^{-1} \cdot 8! \approx 1$ in $9\ 000\ 000$. I disagree with the $8!$. If there's combinations (binomial coefficient) in the denominator, why would there be permutations i.e. the order counts, in the numerator?","What is the probability that when you place 8 towers on a chess-board, none of them can beat the other. Attempt: ${64 \choose 8}^{-1} \approx1$ in $4\ 400\ 000\ 000$ Correct answer: ${64 \choose 8}^{-1} \cdot 8! \approx 1$ in $9\ 000\ 000$. I disagree with the $8!$. If there's combinations (binomial coefficient) in the denominator, why would there be permutations i.e. the order counts, in the numerator?",,"['probability', 'combinatorics']"
82,How does generative model work? Especially in the Natural Language Processing?,How does generative model work? Especially in the Natural Language Processing?,,"AFAIK, the generative language model consists of a probability distribution for some vocabulary. I am wondering how to use this probability distribution to generate a stream of words, i.e. language? If I always pick the word with biggest probability, it will always be the same word because the distribution is fixed. I am not sure if I understand it correctly. Could anyone provide a concrete operational example? Maybe with a toy distribution. Or maybe I can put the question this way: How to generate a sequence of outcomes given the distribution of a   discrete random variable?","AFAIK, the generative language model consists of a probability distribution for some vocabulary. I am wondering how to use this probability distribution to generate a stream of words, i.e. language? If I always pick the word with biggest probability, it will always be the same word because the distribution is fixed. I am not sure if I understand it correctly. Could anyone provide a concrete operational example? Maybe with a toy distribution. Or maybe I can put the question this way: How to generate a sequence of outcomes given the distribution of a   discrete random variable?",,"['probability', 'statistics', 'probability-theory']"
83,How to buy a car optimally in this case?,How to buy a car optimally in this case?,,"X has a car. Its value is unknown yet, but between 0 and 1000, uniformly distributed You offer a price to buy the car. If price < value, you can’t buy. If price >= value, you can buy, give the price of money to X. (for example, if value is 200, and you offer 300, you can buy, but you must give 300 to X) If your buy is successful, Y will pay you the money of 1.5 * value to buy this car himself. (for example, if value is 200, you offer 300, you can buy, give 300 to X, and then Y pays you 1.5*200 = 300, and he takes the car) I think first of all, I need to define the optimal case, which is at the end I earn more money than the price I offered. Assume value is 200. If I offer too low price, nothing will happen, not make much sense If I offer 300, buy the car and then sell the car to Y for 300, in the end, I still have 300, unchanged and makes not much sense. If I offer 250, then I will earn 50 in the end, this makes sense. If I offer 400, then I actually loose 100, this is even worse. What should I do? Update Here is my thinking. Let m be the price I am going to offer and v be the value of the car. What I want is after all things (including I don't buy the car successfully), the money in my hand is still at least m . There are three cases: 1. I can't buy the car, i.e., v > m Because 0 <= v <= 1000 , the probability of v > m is (1000-m)/1000 . 2. I buy the car and I get less than m in the end, i.e., m > 1.5v The probability is m/1500 3. I buy the car and I get more than m in the end, i.e., 1.5v >= m >= v The probability is 1 - (1000-m)/1000 - m/1500 = m/3000 . I wish to have case 1 and case 3, so (1000-m)/1000 + m/3000 > m/1500 . solve this I get 0 <= m <= 791 . As long as I offer a price between 0 and 791, I have bigger possibility to get extra money. (image if I play this for 10000 times)","X has a car. Its value is unknown yet, but between 0 and 1000, uniformly distributed You offer a price to buy the car. If price < value, you can’t buy. If price >= value, you can buy, give the price of money to X. (for example, if value is 200, and you offer 300, you can buy, but you must give 300 to X) If your buy is successful, Y will pay you the money of 1.5 * value to buy this car himself. (for example, if value is 200, you offer 300, you can buy, give 300 to X, and then Y pays you 1.5*200 = 300, and he takes the car) I think first of all, I need to define the optimal case, which is at the end I earn more money than the price I offered. Assume value is 200. If I offer too low price, nothing will happen, not make much sense If I offer 300, buy the car and then sell the car to Y for 300, in the end, I still have 300, unchanged and makes not much sense. If I offer 250, then I will earn 50 in the end, this makes sense. If I offer 400, then I actually loose 100, this is even worse. What should I do? Update Here is my thinking. Let m be the price I am going to offer and v be the value of the car. What I want is after all things (including I don't buy the car successfully), the money in my hand is still at least m . There are three cases: 1. I can't buy the car, i.e., v > m Because 0 <= v <= 1000 , the probability of v > m is (1000-m)/1000 . 2. I buy the car and I get less than m in the end, i.e., m > 1.5v The probability is m/1500 3. I buy the car and I get more than m in the end, i.e., 1.5v >= m >= v The probability is 1 - (1000-m)/1000 - m/1500 = m/3000 . I wish to have case 1 and case 3, so (1000-m)/1000 + m/3000 > m/1500 . solve this I get 0 <= m <= 791 . As long as I offer a price between 0 and 791, I have bigger possibility to get extra money. (image if I play this for 10000 times)",,['probability']
84,"Roll a die, pick that many balls from an urn","Roll a die, pick that many balls from an urn",,"An urn has $5$ white and $10$ black balls. A die is rolled, and that many balls is drawn from the >urn. What is the probability that all the balls drawn are white? My thinking is that each die roll $(d)$ has probability $\frac{1}{6}$. The probability to get only white balls from your draw is $$\frac{C(5, d)}{C(15, d)}$$. Then you add up all the probabilities. I come up with $\frac{5}{66}$, but have no way of knowing if I'm right. Is this the way to go about it? I tried multiplying the probabilities tied to each die roll and came up with an extremely small chance, so that seems wrong to me.","An urn has $5$ white and $10$ black balls. A die is rolled, and that many balls is drawn from the >urn. What is the probability that all the balls drawn are white? My thinking is that each die roll $(d)$ has probability $\frac{1}{6}$. The probability to get only white balls from your draw is $$\frac{C(5, d)}{C(15, d)}$$. Then you add up all the probabilities. I come up with $\frac{5}{66}$, but have no way of knowing if I'm right. Is this the way to go about it? I tried multiplying the probabilities tied to each die roll and came up with an extremely small chance, so that seems wrong to me.",,"['probability', 'dice']"
85,"If the integers m and n are chosen at random between 1 and 100, then the probability that a number of the form $7^{m}+7^{n}$ is divisible by 5 is","If the integers m and n are chosen at random between 1 and 100, then the probability that a number of the form  is divisible by 5 is",7^{m}+7^{n},"If the integers m and n are chosen at random between 1 and 100, then the probability that a number of the form $7^{m}+7^{n}$ is divisible by 5 is $A. 1/4$ $B. 1/7$ $C. 1/8$ $D. 1/49$ I did this: Let $m>n$ (Clearly m and n can't be equal because $5$ can't divide $2*7^{m}$). Now $7^{m}+7^{n}=7^{n}(7^{m-n}+1)$. If $5$ has to divide this, it implies that 5 has to divide $(7^{m-n}+1)$ (because it cannot divide a power of $7$). Since powers of $7$ have a cyclic order of $7,9,3,1$; $7^{m-n}$ has to therefore end in a $9$ and therefore $m-n$ can be $2,6,10,...,98$. Hence the only set of values of $n$ are $(1,2,3,...,97),(1,2,3,...,93),(1,2,3,...,89),...,(1)$. Also fixing $n$ would fix $m$. Therefore the number of favorable cases is $97+93+89+...+2=1224$. Which means that the required probability should be $1224/100C2$ which turns out to be $68/275$, which is not matching with any of the options..  Where did I go wrong ?? Please help !!","If the integers m and n are chosen at random between 1 and 100, then the probability that a number of the form $7^{m}+7^{n}$ is divisible by 5 is $A. 1/4$ $B. 1/7$ $C. 1/8$ $D. 1/49$ I did this: Let $m>n$ (Clearly m and n can't be equal because $5$ can't divide $2*7^{m}$). Now $7^{m}+7^{n}=7^{n}(7^{m-n}+1)$. If $5$ has to divide this, it implies that 5 has to divide $(7^{m-n}+1)$ (because it cannot divide a power of $7$). Since powers of $7$ have a cyclic order of $7,9,3,1$; $7^{m-n}$ has to therefore end in a $9$ and therefore $m-n$ can be $2,6,10,...,98$. Hence the only set of values of $n$ are $(1,2,3,...,97),(1,2,3,...,93),(1,2,3,...,89),...,(1)$. Also fixing $n$ would fix $m$. Therefore the number of favorable cases is $97+93+89+...+2=1224$. Which means that the required probability should be $1224/100C2$ which turns out to be $68/275$, which is not matching with any of the options..  Where did I go wrong ?? Please help !!",,['probability']
86,"Limiting case of Binomial(n,p)/n?","Limiting case of Binomial(n,p)/n?",,"Let the random variable $X$ have distribution $X \sim \text{Binomial}(n,p)$.  Let $Y = X/n$.  What is the limiting distribution of $Y$, as $n \to \infty$?  Does it have a simple distribution? Of course, when $n$ is large, $X$ has approximately the distribution $\text{Poisson}(np)$.  Thus, we could ask the question in the following alternative way: suppose $X^* \sim \text{Poisson}(np)$, and define $Y^* = X^*/n$; what is the limiting distribution of $Y^*$, as $n \to \infty$? I have not been able to find an existing result on this, though it sounds like the sort of thing that someone must have studied long ago.  When I search for limiting distribution and Poisson or limiting distribution and Binomial, I find many references to the fact that $\text{Binomial}(n,p) \to \text{Poisson}(np)$ as $n \to \infty$, which I already knew, so I'm not sure where to look to figure this out.","Let the random variable $X$ have distribution $X \sim \text{Binomial}(n,p)$.  Let $Y = X/n$.  What is the limiting distribution of $Y$, as $n \to \infty$?  Does it have a simple distribution? Of course, when $n$ is large, $X$ has approximately the distribution $\text{Poisson}(np)$.  Thus, we could ask the question in the following alternative way: suppose $X^* \sim \text{Poisson}(np)$, and define $Y^* = X^*/n$; what is the limiting distribution of $Y^*$, as $n \to \infty$? I have not been able to find an existing result on this, though it sounds like the sort of thing that someone must have studied long ago.  When I search for limiting distribution and Poisson or limiting distribution and Binomial, I find many references to the fact that $\text{Binomial}(n,p) \to \text{Poisson}(np)$ as $n \to \infty$, which I already knew, so I'm not sure where to look to figure this out.",,"['probability', 'limits', 'probability-distributions', 'convergence-divergence']"
87,how to prove $(X_{n})_{n\in \mathbb N}$ and $(Y_{n})_{n\in \mathbb N}$ are supermartingale.and $(Y_{n})_{n\in \mathbb N}$ is convergence to -7,how to prove  and  are supermartingale.and  is convergence to -7,(X_{n})_{n\in \mathbb N} (Y_{n})_{n\in \mathbb N} (Y_{n})_{n\in \mathbb N},"Let $p \in [0  , \frac{1}{2}] $ and $\eta_{i}$ be i.i.d random variables and $P(\eta_{i}=1)=p$ and $P(\eta_{i}=-1)=1-p$ and $\mathcal F_{n}=\sigma(\eta_{1},\cdots,\eta_{n})$ and $X_{n}=\sum_{i=1}^{n}\eta_{i}$ and $Y_{n}=X_{T(-7)  \wedge  n}  $ .       show that 1) $(X_{n})_{n\in \mathbb N}$ and  $(Y_{n})_{n\in \mathbb N}$ are supermartingale or matigale or submartingale. 2) $(Y_{n})_{n\in \mathbb N}$ is almost surely convergence to $-7$. thanks for any help.","Let $p \in [0  , \frac{1}{2}] $ and $\eta_{i}$ be i.i.d random variables and $P(\eta_{i}=1)=p$ and $P(\eta_{i}=-1)=1-p$ and $\mathcal F_{n}=\sigma(\eta_{1},\cdots,\eta_{n})$ and $X_{n}=\sum_{i=1}^{n}\eta_{i}$ and $Y_{n}=X_{T(-7)  \wedge  n}  $ .       show that 1) $(X_{n})_{n\in \mathbb N}$ and  $(Y_{n})_{n\in \mathbb N}$ are supermartingale or matigale or submartingale. 2) $(Y_{n})_{n\in \mathbb N}$ is almost surely convergence to $-7$. thanks for any help.",,"['probability', 'probability-theory', 'martingales', 'stopping-times']"
88,Independence of random variables measure theory,Independence of random variables measure theory,,"I wish to show that for two random variables $X$ and $Y$ , the condition $P(X\leq x, Y\leq y ) = P(X\leq x)P(Y\leq y)$ implies that X and Y are independent. I am approaching this problem from a measure theoretic perspective. So in particular I can write that $P(X\leq x) = \mu_X ((-\infty, x])$ and $P(Y\leq y) = \mu_Y ((-\infty, y])$ . Also the independence condition here is that $\sigma(X)$ and $\sigma(Y)$ are independent, where these are the sigma algebras generated by the random variables.","I wish to show that for two random variables and , the condition implies that X and Y are independent. I am approaching this problem from a measure theoretic perspective. So in particular I can write that and . Also the independence condition here is that and are independent, where these are the sigma algebras generated by the random variables.","X Y P(X\leq x, Y\leq y ) = P(X\leq x)P(Y\leq y) P(X\leq x) = \mu_X ((-\infty, x]) P(Y\leq y) = \mu_Y ((-\infty, y]) \sigma(X) \sigma(Y)","['probability', 'measure-theory']"
89,Solving a recurrence for a probability?,Solving a recurrence for a probability?,,"I came across the following recurrence relation when exploring properties of a certain type of randomized perfect binary tree: $$ T(0) = \frac{1}{2} $$ $$ T(k + 1) = 1 - T(k)^2 $$ (Specifically, this is the probability that a random, complete binary tree of height $k$ with values at the leaves and where each internal node is a NAND gate will evaluate to 1.) I was able to get values for this recurrence by writing a quick Python script and found that it quickly starts to oscillate between values very close to 0 and 1 once $k$ gets large.  However, I don't have a closed-form expression for the recurrence. I have never encountered a recurrence like this one before (most recurrences I know of come from the analysis of recursive algorithms or data structures, which rarely have squared terms arise).  Is there a standard technique for solving recurrences of this sort?  If so, how can I use them to solve this recurrence? Thanks!","I came across the following recurrence relation when exploring properties of a certain type of randomized perfect binary tree: $$ T(0) = \frac{1}{2} $$ $$ T(k + 1) = 1 - T(k)^2 $$ (Specifically, this is the probability that a random, complete binary tree of height $k$ with values at the leaves and where each internal node is a NAND gate will evaluate to 1.) I was able to get values for this recurrence by writing a quick Python script and found that it quickly starts to oscillate between values very close to 0 and 1 once $k$ gets large.  However, I don't have a closed-form expression for the recurrence. I have never encountered a recurrence like this one before (most recurrences I know of come from the analysis of recursive algorithms or data structures, which rarely have squared terms arise).  Is there a standard technique for solving recurrences of this sort?  If so, how can I use them to solve this recurrence? Thanks!",,"['probability', 'recurrence-relations']"
90,"Show that $E(|X-a|) = \int_{-\infty}^a P(X < x) \, dx + \int_a^\infty P(X > x) \, dx$",Show that,"E(|X-a|) = \int_{-\infty}^a P(X < x) \, dx + \int_a^\infty P(X > x) \, dx","How to show: $E(|X-a|) = \int_{-\infty}^a P(X < x) \, dx + \int_a^\infty P(X > x) \, dx$. I have: $$E(|X-a|) = \int |X-a| \, dP = \int_{\mathbb{R}} |x-a| P_X(dx) = \int_{(-\infty,a)} a-x P_X(dx) + \int_{(a,\infty)} x-a P_X(dx)$$ But I do not see where to go. And how do I get to the Riemann-Integral? It is just $E(|X|)<\infty$ known, but nothing about an density function (in this case I would know how to get to the Riemann integral). $$E(|X-a|) = \int_{(0,\infty)} P(|X-a| > t) \lambda(dt) = \int_{(0,\infty)} P(\max(X-a,0)+\max(a-X,0) > t) \lambda(dt)$$ This gives, since the events are disjoint, and with Rieman-integrability: $$E(|X-a|) = \int_0^{\infty} P(\max(X-a,0) > t) + P(\max(a-X,0) > t) dx$$ How to go on? Integral is linear, but I do not know how separating them would help.. Finally it is first proven that: $E(|X|) = \int_{-\infty}^0 P(X < x) \, dx + \int_0^\infty P(X > x) \, dx$. Now I want to conclude for $|X-a|$. $E(|X-a|) = \int_{-\infty}^0 P(X-a < x) \, dx + \int_0^\infty P(X-a > x) \, dx$ $= \int_{-\infty}^0 P(X < x+a) \, dx + \int_0^\infty P(X > x+a) \, dx$ $= \int_{-\infty}^a P(X < x) \, dx + \int_a^\infty P(X > x) \, dx$","How to show: $E(|X-a|) = \int_{-\infty}^a P(X < x) \, dx + \int_a^\infty P(X > x) \, dx$. I have: $$E(|X-a|) = \int |X-a| \, dP = \int_{\mathbb{R}} |x-a| P_X(dx) = \int_{(-\infty,a)} a-x P_X(dx) + \int_{(a,\infty)} x-a P_X(dx)$$ But I do not see where to go. And how do I get to the Riemann-Integral? It is just $E(|X|)<\infty$ known, but nothing about an density function (in this case I would know how to get to the Riemann integral). $$E(|X-a|) = \int_{(0,\infty)} P(|X-a| > t) \lambda(dt) = \int_{(0,\infty)} P(\max(X-a,0)+\max(a-X,0) > t) \lambda(dt)$$ This gives, since the events are disjoint, and with Rieman-integrability: $$E(|X-a|) = \int_0^{\infty} P(\max(X-a,0) > t) + P(\max(a-X,0) > t) dx$$ How to go on? Integral is linear, but I do not know how separating them would help.. Finally it is first proven that: $E(|X|) = \int_{-\infty}^0 P(X < x) \, dx + \int_0^\infty P(X > x) \, dx$. Now I want to conclude for $|X-a|$. $E(|X-a|) = \int_{-\infty}^0 P(X-a < x) \, dx + \int_0^\infty P(X-a > x) \, dx$ $= \int_{-\infty}^0 P(X < x+a) \, dx + \int_0^\infty P(X > x+a) \, dx$ $= \int_{-\infty}^a P(X < x) \, dx + \int_a^\infty P(X > x) \, dx$",,['probability']
91,Functions whose derivatives can be written as a function of themself?,Functions whose derivatives can be written as a function of themself?,,"What kinds of function $f: \mathbb{R} \to \mathbb{R}$ can be written as some function of itself? I.e. $f'(x) = g(f(x))$ for some function $g$? If $f$ is given, can $g$ be solved in terms of the symbol $f$ (not in terms of specific $f$), if $g$ exists? My question is related to part 3 of my another question , which asks about when the variance can be represented as a function of mean, both as functions of a distribution parameter, and in particular, when the variance is the derivative of the mean. Thanks!","What kinds of function $f: \mathbb{R} \to \mathbb{R}$ can be written as some function of itself? I.e. $f'(x) = g(f(x))$ for some function $g$? If $f$ is given, can $g$ be solved in terms of the symbol $f$ (not in terms of specific $f$), if $g$ exists? My question is related to part 3 of my another question , which asks about when the variance can be represented as a function of mean, both as functions of a distribution parameter, and in particular, when the variance is the derivative of the mean. Thanks!",,"['probability', 'statistics', 'derivatives']"
92,Statistics and confidence - intervals,Statistics and confidence - intervals,,"An account on server A is more expensive than an account on server B. However, server A is faster. To see whether it's optimal to go with the faster but more expensive server, a manager needs to know how much faster it is . A certain computer algorithm is executed 20 times on server A and 30 times on server B with the following results, Server A  Server B  Sample means      6.7 min   7.5 min   Sample std. dev.  0.6 min   1.2 min A 95% confidence interval for the difference $\mu_{1} - \mu_{2}$ between the mean execution times on server A and server B is [-1.4,-0.2] . Is there a significant  difference between the two servers? (a) Use the confidence interval above to conduct a two-sided test at the 1% level of significance. (b) Compute a p-value of the two-sided test in (a). (c) Is server A really faster? How strong is the evidence? Formulate the suitable hypothesis and alternative and compute the corresponding p-value.","An account on server A is more expensive than an account on server B. However, server A is faster. To see whether it's optimal to go with the faster but more expensive server, a manager needs to know how much faster it is . A certain computer algorithm is executed 20 times on server A and 30 times on server B with the following results, Server A  Server B  Sample means      6.7 min   7.5 min   Sample std. dev.  0.6 min   1.2 min A 95% confidence interval for the difference $\mu_{1} - \mu_{2}$ between the mean execution times on server A and server B is [-1.4,-0.2] . Is there a significant  difference between the two servers? (a) Use the confidence interval above to conduct a two-sided test at the 1% level of significance. (b) Compute a p-value of the two-sided test in (a). (c) Is server A really faster? How strong is the evidence? Formulate the suitable hypothesis and alternative and compute the corresponding p-value.",,"['probability', 'statistics']"
93,Proof that $P(A \mid C) = 1$ implies $P(A \mid B∩ C) = 1$,Proof that  implies,P(A \mid C) = 1 P(A \mid B∩ C) = 1,"This isn't homework. I'm reading Probabilistic Graphical Models by Koller et al.$\space$, and an easy problem in chapter $3$ made me think of a more general problem (which I'm now stuck on). I have everything in place except for this: Let $P(B \cap C) \neq 0$. Then $P(A \space|\space B ∩ C) = 1$ if $P(A \mid C) = 1$. It's intuitively obvious, but I haven't been able to formally prove it. I get $$P(A \mid B∩C) = \dfrac{P(A∩B∩C)}{P(B∩C)} = \dfrac{P(C) P(A\mid C) P(B\mid A∩C)}{P(B∩C)} = \dfrac{P(B\mid A∩C)}{P(B|C)}$$ I don't see why that ratio on the right hand side is $1$. Is it? What am I not seeing?","This isn't homework. I'm reading Probabilistic Graphical Models by Koller et al.$\space$, and an easy problem in chapter $3$ made me think of a more general problem (which I'm now stuck on). I have everything in place except for this: Let $P(B \cap C) \neq 0$. Then $P(A \space|\space B ∩ C) = 1$ if $P(A \mid C) = 1$. It's intuitively obvious, but I haven't been able to formally prove it. I get $$P(A \mid B∩C) = \dfrac{P(A∩B∩C)}{P(B∩C)} = \dfrac{P(C) P(A\mid C) P(B\mid A∩C)}{P(B∩C)} = \dfrac{P(B\mid A∩C)}{P(B|C)}$$ I don't see why that ratio on the right hand side is $1$. Is it? What am I not seeing?",,"['probability', 'conditional-probability']"
94,Difference between upper and lower tails of multiplicative Chernoff bounds,Difference between upper and lower tails of multiplicative Chernoff bounds,,"I'm struggling with the intuition behind why Chernoff bounds differ in the upper and lower tails. That is, for the lower tail we have: $$     Pr(X \le (1 - \delta)\mu)\ \ \le\ \ e^{-\frac{\mu\delta^2}{2}} $$ Whereas for the upper tail: $$     Pr(X \ge (1 + \delta)\mu)\ \ \le\ \ e^{-\frac{\mu\delta^2}{3}} $$ Both bounds are found in roughly the same manner, so at a high level, why would they differ in value when we don't know that the distribution is necessarily asymmetric?","I'm struggling with the intuition behind why Chernoff bounds differ in the upper and lower tails. That is, for the lower tail we have: Whereas for the upper tail: Both bounds are found in roughly the same manner, so at a high level, why would they differ in value when we don't know that the distribution is necessarily asymmetric?","
    Pr(X \le (1 - \delta)\mu)\ \ \le\ \ e^{-\frac{\mu\delta^2}{2}}
 
    Pr(X \ge (1 + \delta)\mu)\ \ \le\ \ e^{-\frac{\mu\delta^2}{3}}
","['probability', 'distribution-tails']"
95,Expected value of the minimum (discrete case),Expected value of the minimum (discrete case),,"Maybe related to this question In the comments of this question they say that it gets easier if the variables are identically and independently distributed. But i don't see how because in my case the variable is discrete Here is my problem : I toss 4 dice and keep the 3 best results. What is the expected value of the result ? I think tossing 4 dice and keep the 3 best is like tossing 4 dice and removing the minimum. Let X be the result of a standard die. Let Y be tossing 4 dice and keeping the 3 best Is that correct : $E(Y) = 4*E(X) - E(min)$ ? So how calculate E(min) ? I know if the variable was uniform on [0,1] I could have started with $F_Y = 1 - ( 1-F_X )^p$ where p is the number of dice I toss, but here the variable is discrete so i don't know where to start. Generalization : How to calculate the expected value of k realizations of a discrete random variable in [0-n]? It's been a while since i studied probability, so my basic calculation may be wrong. Also,  English is not my mother tongue, so please forgive my mistakes. edit : spelling mistakes","Maybe related to this question In the comments of this question they say that it gets easier if the variables are identically and independently distributed. But i don't see how because in my case the variable is discrete Here is my problem : I toss 4 dice and keep the 3 best results. What is the expected value of the result ? I think tossing 4 dice and keep the 3 best is like tossing 4 dice and removing the minimum. Let X be the result of a standard die. Let Y be tossing 4 dice and keeping the 3 best Is that correct : $E(Y) = 4*E(X) - E(min)$ ? So how calculate E(min) ? I know if the variable was uniform on [0,1] I could have started with $F_Y = 1 - ( 1-F_X )^p$ where p is the number of dice I toss, but here the variable is discrete so i don't know where to start. Generalization : How to calculate the expected value of k realizations of a discrete random variable in [0-n]? It's been a while since i studied probability, so my basic calculation may be wrong. Also,  English is not my mother tongue, so please forgive my mistakes. edit : spelling mistakes",,"['probability', 'probability-theory']"
96,Probability in 2-D Poisson process,Probability in 2-D Poisson process,,"I saw this problem in Durrett. Consider two independent Poisson processes $N_1(t)$ and $N_2(t)$ with rates   1 and 2. What is the probability that the two-dimensional process $(N_1(t),N_2(t))$   ever visits the point $(i, j)$? I wrote this as $P(N_1(t)=i,N_2(t)=j)=P(N_1(t)=i)P(N_2(t)=j)=\frac{e^{-\lambda_1t}(\lambda_1t)^i}{i!}\frac{e^{-\lambda_2t}(\lambda_2t)^j}{j!}$ Is this all that is there to the problem or is there something I am missing?","I saw this problem in Durrett. Consider two independent Poisson processes $N_1(t)$ and $N_2(t)$ with rates   1 and 2. What is the probability that the two-dimensional process $(N_1(t),N_2(t))$   ever visits the point $(i, j)$? I wrote this as $P(N_1(t)=i,N_2(t)=j)=P(N_1(t)=i)P(N_2(t)=j)=\frac{e^{-\lambda_1t}(\lambda_1t)^i}{i!}\frac{e^{-\lambda_2t}(\lambda_2t)^j}{j!}$ Is this all that is there to the problem or is there something I am missing?",,"['probability', 'stochastic-processes']"
97,Conditional Probability with balls in an urn,Conditional Probability with balls in an urn,,"Two balls, each equally likely to be colored either red or blue, are put in an urn. At each stage one of the balls is randomly chosen, its color is noted, and it is then returned to the urn. If the first two balls chosen are colored red, what is the probability that   (a) both balls in the urn are colored red; (b)  the next ball chosen will be red? I'm wondering if my method for part (a) is correct: Let $P(R)$ be the probability of picking a red ball: $P(R)=\frac{1}{2}$ Let $P(B)$ be the probability of picking a blue ball: $P(B)=\frac{1}{2}$ Let $P(C)$ be the probability of the condition, i.e., picking two red balls consecutively: $P(C)=P(C|H_1)P(H_1)+P(C|H_2)P(H_2)+P(C|H_3)P(H_3) $ where $P(H_1)$ is the probability both balls in the urn are red: $P(H_1)=(P(R))^2=(\frac{1}{2})^2=\frac{1}4$ $P(H_2)$ is the probability that both balls in the urn are blue: $P(H_2)=(P(B))^2=(\frac{1}{2})^2=\frac{1}4$ $P(H_3)$ is the probability that one ball is red and one is blue inside the urn: $P(H_3)=1-(P(H_1)+P(H_2))=1-\frac{1}2=\frac{1}2$ since the sum of the mutually exclusive hypotheses or events must sum to 1. Thus $P(C)=1\times\frac{1}4+\frac{1}4\times0+\frac{1}2\times\frac{1}4=\frac{3}8$ and $P(H_1|C)= \frac{P(C \bigcap H_1)}{P(C)}=\frac{P(C|H_1)P(H_1)}{P(C)}=\frac{1\times\frac{1}4}{\frac{3}8}=\frac{2}3$ For part (b), I know that $P(R|C)$, the probability of picking a red ball given that the first two balls picked were red is to be found $P(R|C)= \frac{P(C \bigcap R)}{P(C)}=\frac{P(C|R)P(R)}{P(C)}$ How can I find $P(C|R)$?","Two balls, each equally likely to be colored either red or blue, are put in an urn. At each stage one of the balls is randomly chosen, its color is noted, and it is then returned to the urn. If the first two balls chosen are colored red, what is the probability that   (a) both balls in the urn are colored red; (b)  the next ball chosen will be red? I'm wondering if my method for part (a) is correct: Let $P(R)$ be the probability of picking a red ball: $P(R)=\frac{1}{2}$ Let $P(B)$ be the probability of picking a blue ball: $P(B)=\frac{1}{2}$ Let $P(C)$ be the probability of the condition, i.e., picking two red balls consecutively: $P(C)=P(C|H_1)P(H_1)+P(C|H_2)P(H_2)+P(C|H_3)P(H_3) $ where $P(H_1)$ is the probability both balls in the urn are red: $P(H_1)=(P(R))^2=(\frac{1}{2})^2=\frac{1}4$ $P(H_2)$ is the probability that both balls in the urn are blue: $P(H_2)=(P(B))^2=(\frac{1}{2})^2=\frac{1}4$ $P(H_3)$ is the probability that one ball is red and one is blue inside the urn: $P(H_3)=1-(P(H_1)+P(H_2))=1-\frac{1}2=\frac{1}2$ since the sum of the mutually exclusive hypotheses or events must sum to 1. Thus $P(C)=1\times\frac{1}4+\frac{1}4\times0+\frac{1}2\times\frac{1}4=\frac{3}8$ and $P(H_1|C)= \frac{P(C \bigcap H_1)}{P(C)}=\frac{P(C|H_1)P(H_1)}{P(C)}=\frac{1\times\frac{1}4}{\frac{3}8}=\frac{2}3$ For part (b), I know that $P(R|C)$, the probability of picking a red ball given that the first two balls picked were red is to be found $P(R|C)= \frac{P(C \bigcap R)}{P(C)}=\frac{P(C|R)P(R)}{P(C)}$ How can I find $P(C|R)$?",,"['probability', 'conditional-probability']"
98,Calculate $\mathbb{E}[F(Y)]$,Calculate,\mathbb{E}[F(Y)],"I try to resolve this problem, but I have some difficulties to get a clear result. The problem : Let X be a normal random variable with mean 0 and variance 1 (ie. $X\sim \mathcal{N}(0,1)$). Let Y be a normal random variable with mean $m$ and variance $\sigma^{2}$ (ie. $Y\sim \mathcal{N}(m,\sigma^{2})$). X and Y are independent random variables. What I want is to compute $I=\mathbb{E}[\Phi(Y)]$ where $\Phi$ is the the cumulative distribution function (CDF) of $X$. * What I done is wrong * Sorry for my english :)","I try to resolve this problem, but I have some difficulties to get a clear result. The problem : Let X be a normal random variable with mean 0 and variance 1 (ie. $X\sim \mathcal{N}(0,1)$). Let Y be a normal random variable with mean $m$ and variance $\sigma^{2}$ (ie. $Y\sim \mathcal{N}(m,\sigma^{2})$). X and Y are independent random variables. What I want is to compute $I=\mathbb{E}[\Phi(Y)]$ where $\Phi$ is the the cumulative distribution function (CDF) of $X$. * What I done is wrong * Sorry for my english :)",,['probability']
99,Average number of flips for someone to win?,Average number of flips for someone to win?,,"You and I each have 14 dollars. I flip a fair coin repeatedly; if it comes up heads I pay you a dollar, but if it lands tails you pay me a dollar. On average, how many times will I flip the coin before one of us runs out of money? I know that the chance of each of us winning is equal (1/2), but I have no idea what the average number of flips should be. I think it's 28 but it's just a guess.","You and I each have 14 dollars. I flip a fair coin repeatedly; if it comes up heads I pay you a dollar, but if it lands tails you pay me a dollar. On average, how many times will I flip the coin before one of us runs out of money? I know that the chance of each of us winning is equal (1/2), but I have no idea what the average number of flips should be. I think it's 28 but it's just a guess.",,['probability']
