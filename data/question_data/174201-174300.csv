,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,Anti-curl operator,Anti-curl operator,,"It is known that if a vector field $\vec{B}$ is divergence-free, and defined on $\mathbb R^3$ then it can be shown as $\vec{B} = \nabla\times\vec{A}$ for some vector field $A$ . Is there a way to find $A$ that would satisfy this equation? (I know there are many possibilities for $A$ ) Note: I want to find it without using the explicit formula for $B_x(x,y,z), B_y(x,y,z), B_z(x,y,z)$ , but maybe with a formula involving surface/curve integrals. For example, I've found that in the 2D case (if $B_z=0$ and $\vec{B}=\vec{B}(x,y)$ ) then $A$ can be shown as: $$\vec{A}(x,y)=\hat{z}\int_{\vec{R_0}}^{\vec{r}} (\hat{z}\times\vec{B})\cdot\vec{dl}$$ I am looking for something similar in the general case.","It is known that if a vector field is divergence-free, and defined on then it can be shown as for some vector field . Is there a way to find that would satisfy this equation? (I know there are many possibilities for ) Note: I want to find it without using the explicit formula for , but maybe with a formula involving surface/curve integrals. For example, I've found that in the 2D case (if and ) then can be shown as: I am looking for something similar in the general case.","\vec{B} \mathbb R^3 \vec{B} = \nabla\times\vec{A} A A A B_x(x,y,z), B_y(x,y,z), B_z(x,y,z) B_z=0 \vec{B}=\vec{B}(x,y) A \vec{A}(x,y)=\hat{z}\int_{\vec{R_0}}^{\vec{r}} (\hat{z}\times\vec{B})\cdot\vec{dl}","['multivariable-calculus', 'vector-analysis', 'vector-fields', 'curl']"
1,"Is writing the divergence as a ""dot product"" a deception?","Is writing the divergence as a ""dot product"" a deception?",,"Suppose we have the following vector field in $\mathbb{R}^3$ : $$\vec{F}(x,y,z) = F_x \hat{x}+F_y \hat{y}+F_z \hat{z}$$ where $\hat{x}$ , $\hat{y}$ , and $\hat{z}$ are unit vectors in each of the directions on a Cartesian coordinate system, and $F_x$ , $F_y$ , and $F_z$ are some scalar-valued functions of $x$ , $y$ , and $z$ . Equivalently, in spherical coordinates, $$\vec{F}(r,\theta,\phi) = F_r \hat{r}+F_\theta \hat{\theta}+F_\phi \hat{\phi}$$ where $\hat{x}$ , $\hat{y}$ , and $\hat{z}$ have been transformed into $\hat{r}$ , $\hat{\theta}$ , and $\hat{\phi}$ using the relations here , and $F_r$ , $F_\theta$ , and $F_\phi$ are some combinations of $F_x$ , $F_y$ , and $F_z$ that result from this transformation. My question is about deriving the formula for the divergence of this vector field in spherical coordinates. The divergence is commonly written as $$Div(\vec{F})=\vec{\nabla} \cdot \vec{F}$$ One approach I can take to derive the formula is shown here , where I rewrite $$Div(\vec{F})=\vec{\nabla} \cdot \vec{F} = \frac{\partial F_x}{\partial x} + \frac{\partial F_y}{\partial y} + \frac{\partial F_z}{\partial z}$$ and convert $\frac{\partial}{\partial x} = \frac{\partial r}{\partial x} \frac{\partial}{\partial r} + \frac{\partial \theta}{\partial x} \frac{\partial}{\partial \theta} + \frac{\partial \phi}{\partial x} \frac{\partial}{\partial \phi}$ , and $F_x(x,y,z)=F_x(r,\theta,\phi)$ , and so on. Then, I eventually get the correct formula: $$Div(\vec{F}) = \frac{1}{r^2} \frac{\partial (r^2 F_r)}{\partial r} + \frac{1}{r \sin(\theta)} \frac{\partial (F_\theta \sin(\theta))}{\partial \theta} + \frac{1}{r \sin(\theta)} \frac{\partial F_\phi}{\partial \phi}$$ However, I would like to know why the following does not work . I can rewrite the gradient operator as: $$\vec{\nabla} =\frac{\partial}{\partial x} \hat{x} + \frac{\partial}{\partial y} \hat{y} + \frac{\partial}{\partial z} \hat{z} = \frac{\partial}{\partial r} \hat{r} + \frac{1}{r} \frac{\partial}{\partial \theta} \hat{\theta} + \frac{1}{r \sin{\theta}}\frac{\partial}{\partial \phi} \hat{\phi}$$ Then: $$Div(\vec{F}) = \vec{\nabla} \cdot \vec{F} = (\frac{\partial}{\partial r} \hat{r} + \frac{1}{r} \frac{\partial}{\partial \theta} \hat{\theta} + \frac{1}{r \sin{\theta}}\frac{\partial}{\partial \phi} \hat{\phi}) \cdot (F_r \hat{r}+F_\theta \hat{\theta}+F_\phi \hat{\phi})$$ Finally, using the relations between the unit vectors for an orthogonal coordinate system (e.g. $\hat{r} \cdot \hat{\theta} = 0$ , $\hat{\phi} \cdot \hat{\phi} = 1$ , etc.), I end up with $$Div(\vec{F}) = \frac{\partial F_r}{\partial r} + \frac{1}{r} \frac{\partial F_\theta}{\partial \theta} + \frac{1}{r \sin(\theta)} \frac{\partial F_\phi}{\partial \phi}$$ which is not the correct formula. I would like to know where my fault is, and whether this ""dot product notation"" for divergence is only applicable when evaluated in the Cartesian system. And, if so, why?","Suppose we have the following vector field in : where , , and are unit vectors in each of the directions on a Cartesian coordinate system, and , , and are some scalar-valued functions of , , and . Equivalently, in spherical coordinates, where , , and have been transformed into , , and using the relations here , and , , and are some combinations of , , and that result from this transformation. My question is about deriving the formula for the divergence of this vector field in spherical coordinates. The divergence is commonly written as One approach I can take to derive the formula is shown here , where I rewrite and convert , and , and so on. Then, I eventually get the correct formula: However, I would like to know why the following does not work . I can rewrite the gradient operator as: Then: Finally, using the relations between the unit vectors for an orthogonal coordinate system (e.g. , , etc.), I end up with which is not the correct formula. I would like to know where my fault is, and whether this ""dot product notation"" for divergence is only applicable when evaluated in the Cartesian system. And, if so, why?","\mathbb{R}^3 \vec{F}(x,y,z) = F_x \hat{x}+F_y \hat{y}+F_z \hat{z} \hat{x} \hat{y} \hat{z} F_x F_y F_z x y z \vec{F}(r,\theta,\phi) = F_r \hat{r}+F_\theta \hat{\theta}+F_\phi \hat{\phi} \hat{x} \hat{y} \hat{z} \hat{r} \hat{\theta} \hat{\phi} F_r F_\theta F_\phi F_x F_y F_z Div(\vec{F})=\vec{\nabla} \cdot \vec{F} Div(\vec{F})=\vec{\nabla} \cdot \vec{F} = \frac{\partial F_x}{\partial x} + \frac{\partial F_y}{\partial y} + \frac{\partial F_z}{\partial z} \frac{\partial}{\partial x} = \frac{\partial r}{\partial x} \frac{\partial}{\partial r} + \frac{\partial \theta}{\partial x} \frac{\partial}{\partial \theta} + \frac{\partial \phi}{\partial x} \frac{\partial}{\partial \phi} F_x(x,y,z)=F_x(r,\theta,\phi) Div(\vec{F}) = \frac{1}{r^2} \frac{\partial (r^2 F_r)}{\partial r} + \frac{1}{r \sin(\theta)} \frac{\partial (F_\theta \sin(\theta))}{\partial \theta} + \frac{1}{r \sin(\theta)} \frac{\partial F_\phi}{\partial \phi} \vec{\nabla} =\frac{\partial}{\partial x} \hat{x} + \frac{\partial}{\partial y} \hat{y} + \frac{\partial}{\partial z} \hat{z} = \frac{\partial}{\partial r} \hat{r} + \frac{1}{r} \frac{\partial}{\partial \theta} \hat{\theta} + \frac{1}{r \sin{\theta}}\frac{\partial}{\partial \phi} \hat{\phi} Div(\vec{F}) = \vec{\nabla} \cdot \vec{F} = (\frac{\partial}{\partial r} \hat{r} + \frac{1}{r} \frac{\partial}{\partial \theta} \hat{\theta} + \frac{1}{r \sin{\theta}}\frac{\partial}{\partial \phi} \hat{\phi}) \cdot (F_r \hat{r}+F_\theta \hat{\theta}+F_\phi \hat{\phi}) \hat{r} \cdot \hat{\theta} = 0 \hat{\phi} \cdot \hat{\phi} = 1 Div(\vec{F}) = \frac{\partial F_r}{\partial r} + \frac{1}{r} \frac{\partial F_\theta}{\partial \theta} + \frac{1}{r \sin(\theta)} \frac{\partial F_\phi}{\partial \phi}","['multivariable-calculus', 'vector-analysis', 'coordinate-systems', 'spherical-coordinates', 'divergence-operator']"
2,What is the surface area of the 3-dimensional elliptope?,What is the surface area of the 3-dimensional elliptope?,,"The $n$ -elliptope is defined as the set of $n$ -by- $n$ correlation matrices; that is, the set of $n$ -by- $n$ symmetric positive-definite matrices with ones on the diagonal. Such matrices are parametrized by their $n(n-1)/2$ upper off-diagonal elements. In the case of $n=3$ , this yields the 3-elliptope $$\Gamma = \{(x,y,z)\in[-1,1]^3 : x^2+y^2+z^2\leq 1+2xyz\}.$$ The volume of $\Gamma$ was considered in an earlier question ( What is the volume of the $3$ -dimensional elliptope? ) and shown to be $\pi^2/2$ . However, what I'm interested in presently is the subset of singular 3-by-3 correlation matrices. This corresponds precisely to the boundary of the above set: $$\partial \Gamma = \{(x,y,z)\in [-1,1]^3: x^2+y^2+z^2=1+2xyz\}.$$ With that in mind, what I want to know is the the surface area of $\partial \Gamma.$ Formally, this is not so hard: The surface can be expressed as the union of the surfaces $z=f_{\pm}(x,y)=x y\pm \sqrt{1-x^2}\sqrt{1-y^2}$ , and the bottom surface is the mirror of the top. Hence their areas are the same, so the total area is given the double integral $$S=2\int_{-1}^1\int_{-1}^1 \sqrt{1+\left(\frac{\partial f_+}{\partial x}\right)^2+\left(\frac{\partial f_+}{\partial y}\right)^2}\,dx\,dy,$$ where $$\frac{\partial f_+}{\partial x} = y-x\sqrt{\frac{1-y^2}{1-x^2}},\quad \frac{\partial f_+}{\partial y}=x-y\sqrt{\frac{1-x^2}{1-y^2}}.$$ If one substitutes $x=\cos\alpha,y=\cos\beta$ over the ranges $0\leq \alpha,\beta\leq \pi$ , then the result may be placed in the form $$S = 2\int_{0}^\pi\int_0^\pi \sqrt{\sin^2(\alpha)  \sin^2(\alpha -\beta )+\sin^2(\beta ) \sin^2(\alpha -\beta )+\sin^2(\alpha)  \sin^2(\beta) }\;d\alpha \,d\beta.$$ Alas, while this integral is intriguing it has defied my attempts at analytical solution (as well as those of Mathematica). Numerically, however, the integral seems to be exactly $5\pi$ . Can anyone show that this result is correct?","The -elliptope is defined as the set of -by- correlation matrices; that is, the set of -by- symmetric positive-definite matrices with ones on the diagonal. Such matrices are parametrized by their upper off-diagonal elements. In the case of , this yields the 3-elliptope The volume of was considered in an earlier question ( What is the volume of the -dimensional elliptope? ) and shown to be . However, what I'm interested in presently is the subset of singular 3-by-3 correlation matrices. This corresponds precisely to the boundary of the above set: With that in mind, what I want to know is the the surface area of Formally, this is not so hard: The surface can be expressed as the union of the surfaces , and the bottom surface is the mirror of the top. Hence their areas are the same, so the total area is given the double integral where If one substitutes over the ranges , then the result may be placed in the form Alas, while this integral is intriguing it has defied my attempts at analytical solution (as well as those of Mathematica). Numerically, however, the integral seems to be exactly . Can anyone show that this result is correct?","n n n n n n(n-1)/2 n=3 \Gamma = \{(x,y,z)\in[-1,1]^3 : x^2+y^2+z^2\leq 1+2xyz\}. \Gamma 3 \pi^2/2 \partial \Gamma = \{(x,y,z)\in [-1,1]^3: x^2+y^2+z^2=1+2xyz\}. \partial \Gamma. z=f_{\pm}(x,y)=x y\pm \sqrt{1-x^2}\sqrt{1-y^2} S=2\int_{-1}^1\int_{-1}^1 \sqrt{1+\left(\frac{\partial f_+}{\partial x}\right)^2+\left(\frac{\partial f_+}{\partial y}\right)^2}\,dx\,dy, \frac{\partial f_+}{\partial x} = y-x\sqrt{\frac{1-y^2}{1-x^2}},\quad \frac{\partial f_+}{\partial y}=x-y\sqrt{\frac{1-x^2}{1-y^2}}. x=\cos\alpha,y=\cos\beta 0\leq \alpha,\beta\leq \pi S = 2\int_{0}^\pi\int_0^\pi \sqrt{\sin^2(\alpha)  \sin^2(\alpha -\beta )+\sin^2(\beta ) \sin^2(\alpha -\beta )+\sin^2(\alpha)  \sin^2(\beta) }\;d\alpha \,d\beta. 5\pi","['multivariable-calculus', 'definite-integrals', 'analytic-geometry', 'closed-form', 'spectrahedra']"
3,Gradient and Jacobian row and column conventions,Gradient and Jacobian row and column conventions,,"Say $f$ is a scalar valued function from $\mathbb{R}^n \to \mathbb{R}$. When I learnt about the gradient $\nabla f(\mathbf{x})$ I always thought of it as a column vector in the same space as $\mathbf{x}$. That way, the dot product $\nabla f \cdot \mathbf{v}$ gives the directional derivative in direction $\mathbf{v}$. All the definitions I can find of the Jacobian of $\mathbf{y} = \psi(\mathbf{x})$ however define it as: \begin{bmatrix}  \frac{\partial y_1}{\partial x_1} & \frac{\partial y_1}{\partial x_2} & \cdots & \frac{\partial y_1}{\partial x_n}\\ \frac{\partial y_2}{\partial x_1} & \frac{\partial y_2}{\partial x_2} & \cdots & \frac{\partial y_2}{\partial x_n}\\ \vdots&\vdots \\ \frac{\partial y_m}{\partial x_1} & \frac{\partial y_m}{\partial x_2} & \cdots & \frac{\partial y_m}{\partial x_n}\\ \end{bmatrix} But this would make $\nabla f$ a row vector, which then means the directional derivative is no longer $\nabla f \cdot \mathbf{v}$. Which way is correct? What are the consequences if I accidently write the Jacobian the opposite way? I have found some similar questions here but none that answer my question directly. I'm still learning this stuff so please explain in simple terms :)","Say $f$ is a scalar valued function from $\mathbb{R}^n \to \mathbb{R}$. When I learnt about the gradient $\nabla f(\mathbf{x})$ I always thought of it as a column vector in the same space as $\mathbf{x}$. That way, the dot product $\nabla f \cdot \mathbf{v}$ gives the directional derivative in direction $\mathbf{v}$. All the definitions I can find of the Jacobian of $\mathbf{y} = \psi(\mathbf{x})$ however define it as: \begin{bmatrix}  \frac{\partial y_1}{\partial x_1} & \frac{\partial y_1}{\partial x_2} & \cdots & \frac{\partial y_1}{\partial x_n}\\ \frac{\partial y_2}{\partial x_1} & \frac{\partial y_2}{\partial x_2} & \cdots & \frac{\partial y_2}{\partial x_n}\\ \vdots&\vdots \\ \frac{\partial y_m}{\partial x_1} & \frac{\partial y_m}{\partial x_2} & \cdots & \frac{\partial y_m}{\partial x_n}\\ \end{bmatrix} But this would make $\nabla f$ a row vector, which then means the directional derivative is no longer $\nabla f \cdot \mathbf{v}$. Which way is correct? What are the consequences if I accidently write the Jacobian the opposite way? I have found some similar questions here but none that answer my question directly. I'm still learning this stuff so please explain in simple terms :)",,['multivariable-calculus']
4,How do you pronounce (partial) derivatives?,How do you pronounce (partial) derivatives?,,"I am not an English speaker that is why I asked this question. In addition, I think english.stackexchange.com is not the proper place to ask this because (I am so sorry) I don't think most of them know mathematics deeply. How do you pronounce the following derivatives in English? $\frac{\textrm{d}y}{\textrm{d}x}$ $\frac{\textrm{d}^2y}{\textrm{d}x^2}$ $\frac{\partial y}{\partial x}$ $\frac{\partial^2 y}{\partial x^2}$ $\frac{\partial^3 y}{\partial x^2\partial z}$ Feel free to edit the tag if it has been chosen incorrectly. For example, is it correct if I pronounce $\frac{\textrm{d}y}{\textrm{d}x}$ as ""dee wai over dee eks""?","I am not an English speaker that is why I asked this question. In addition, I think english.stackexchange.com is not the proper place to ask this because (I am so sorry) I don't think most of them know mathematics deeply. How do you pronounce the following derivatives in English? Feel free to edit the tag if it has been chosen incorrectly. For example, is it correct if I pronounce as ""dee wai over dee eks""?",\frac{\textrm{d}y}{\textrm{d}x} \frac{\textrm{d}^2y}{\textrm{d}x^2} \frac{\partial y}{\partial x} \frac{\partial^2 y}{\partial x^2} \frac{\partial^3 y}{\partial x^2\partial z} \frac{\textrm{d}y}{\textrm{d}x},"['multivariable-calculus', 'notation', 'derivatives', 'pronunciation']"
5,Properties and notation of third-order (and higher) partial-derivatives,Properties and notation of third-order (and higher) partial-derivatives,,"This question has been bothering me for quite a while and I still haven't found a satisfying answer anywhere on the internet or in any of my books (which may not be that advanced, mind you...). Since I couldn't find a similar question here on MSE, it probably is rather obvious if you have a sufficiently  advanced level, which I do not (I'm still undergraduate and my knowledge of tensors is really small!). Out of simplicity, I'll take a specific example for my question. Here it is: Let $f : \mathbb{R}^2 \rightarrow \mathbb{R}$ ; $(x,y) \rightarrow f(x,y)$ be a continous function with two variables $x$ and $y$. The ""equivalent"" of the first derivative known from single-variable functions would be the gradient: $\nabla f = \left(\begin{array}{cccc} f_x \\ f_y \end{array}\right)$ The second derivative is the Hessian-Matrix: $H_f(x,y) = \left(\begin{array}{cccc} f_{xx} & f_{xy} \\ f_{yx} & f_{yy} \end{array}\right)$ So what does the third-order derivative of this function (or in general...) look like? I've been able to work out that it is a tensor with order $3$. How does one notate such a geometric object? Concerning the properties: the gradient is a vector field depicting the ""slope"" of the function in any point using a vector (i.e.: an orientation and a magnitude). The Hessian-Matrix shows us the rate of change of the gradient (correct use of words?). So by this analogy, the third derivative measures the rate of change of the second derivative. But how? Feel free to change the tags and edit my answer, if you think it necessary. I'm looking forward to any answers! Thanks in advance, SDV","This question has been bothering me for quite a while and I still haven't found a satisfying answer anywhere on the internet or in any of my books (which may not be that advanced, mind you...). Since I couldn't find a similar question here on MSE, it probably is rather obvious if you have a sufficiently  advanced level, which I do not (I'm still undergraduate and my knowledge of tensors is really small!). Out of simplicity, I'll take a specific example for my question. Here it is: Let $f : \mathbb{R}^2 \rightarrow \mathbb{R}$ ; $(x,y) \rightarrow f(x,y)$ be a continous function with two variables $x$ and $y$. The ""equivalent"" of the first derivative known from single-variable functions would be the gradient: $\nabla f = \left(\begin{array}{cccc} f_x \\ f_y \end{array}\right)$ The second derivative is the Hessian-Matrix: $H_f(x,y) = \left(\begin{array}{cccc} f_{xx} & f_{xy} \\ f_{yx} & f_{yy} \end{array}\right)$ So what does the third-order derivative of this function (or in general...) look like? I've been able to work out that it is a tensor with order $3$. How does one notate such a geometric object? Concerning the properties: the gradient is a vector field depicting the ""slope"" of the function in any point using a vector (i.e.: an orientation and a magnitude). The Hessian-Matrix shows us the rate of change of the gradient (correct use of words?). So by this analogy, the third derivative measures the rate of change of the second derivative. But how? Feel free to change the tags and edit my answer, if you think it necessary. I'm looking forward to any answers! Thanks in advance, SDV",,"['multivariable-calculus', 'derivatives', 'soft-question', 'tensors']"
6,Prove that $\sum\limits_{cyc}\frac{a}{\sqrt{a+3b}}\geq\sqrt{a+b+c+d}$,Prove that,\sum\limits_{cyc}\frac{a}{\sqrt{a+3b}}\geq\sqrt{a+b+c+d},"Let $a$, $b$, $c$ and $d$ be positive numbers. Prove that: $$\frac{a}{\sqrt{a+3b}}+\frac{b}{\sqrt{b+3c}}+\frac{c}{\sqrt{c+3d}}+\frac{d}{\sqrt{d+3a}}\geq\sqrt{a+b+c+d}$$ I tried Holder, AM-GM and more, but without success.","Let $a$, $b$, $c$ and $d$ be positive numbers. Prove that: $$\frac{a}{\sqrt{a+3b}}+\frac{b}{\sqrt{b+3c}}+\frac{c}{\sqrt{c+3d}}+\frac{d}{\sqrt{d+3a}}\geq\sqrt{a+b+c+d}$$ I tried Holder, AM-GM and more, but without success.",,"['multivariable-calculus', 'inequality', 'contest-math', 'radicals']"
7,"Can ""being differentiable"" imply ""having continuous partial derivatives""?","Can ""being differentiable"" imply ""having continuous partial derivatives""?",,"Consider the following theorem: Let $E$ be a subset of ${\bf R}^n$, $f:E\to {\bf R}^m$ be a function, $F$ be a subset of $E$, and $x_0$ be an interior point of $F$. If all the partial derivatives $\frac{\partial f}{\partial x_j}$ exist on $F$ and are continuous at $x_0$, then $f$ is differentiable at $x_0$. And I consider the converse of the above theorem: Let $E$ be a subset of ${\bf R}^n$, $f:E\to {\bf R}^m$ be a function, $F$ be a subset of $E$, and $x_0$ be an interior point of $F$. If $f$ is differentiable at $x_0$, then all the partial derivatives $\frac{\partial f}{\partial x_j}$ exist on some neighbourhood of $x_0$ and are continuous at $x_0$. It is trivial to show that the converse is NOT true when $m=1$. It seems no hope that it will be true when $m\geq 2$. Here is my question : Is the converse   true when $m\geq 2$? If is not true, how to construct the counterexample? Edit : The title is corrected. Edit : Since another question is not relevant to the first one here, I think, I put it into another post .","Consider the following theorem: Let $E$ be a subset of ${\bf R}^n$, $f:E\to {\bf R}^m$ be a function, $F$ be a subset of $E$, and $x_0$ be an interior point of $F$. If all the partial derivatives $\frac{\partial f}{\partial x_j}$ exist on $F$ and are continuous at $x_0$, then $f$ is differentiable at $x_0$. And I consider the converse of the above theorem: Let $E$ be a subset of ${\bf R}^n$, $f:E\to {\bf R}^m$ be a function, $F$ be a subset of $E$, and $x_0$ be an interior point of $F$. If $f$ is differentiable at $x_0$, then all the partial derivatives $\frac{\partial f}{\partial x_j}$ exist on some neighbourhood of $x_0$ and are continuous at $x_0$. It is trivial to show that the converse is NOT true when $m=1$. It seems no hope that it will be true when $m\geq 2$. Here is my question : Is the converse   true when $m\geq 2$? If is not true, how to construct the counterexample? Edit : The title is corrected. Edit : Since another question is not relevant to the first one here, I think, I put it into another post .",,['multivariable-calculus']
8,Proof that continuous partial derivatives implies differentiability,Proof that continuous partial derivatives implies differentiability,,"This is the statement of Theorem 2.8 from Spivak's Calculus on Manifolds. I'd like feedback on if this looks fine as far as a generalization to his proof goes: Theorem: If $f: \mathbb{R}^n \rightarrow \mathbb{R}^m$, then $Df(a)$ exists if all $D_jf^i(x)$ exist in an open set containing $a$ and if each function $D_jf^i$ is continuous at $a$. Proof: Let $f: \mathbb{R}^n \rightarrow \mathbb{R}^m$ and suppose that all $D_jf^i(x)$ exist in an open set containing $a=(a^1,...,a^n)$ and that each function $D_jf^i$ is continuous at $a$. Then, for each $j$ such that $1 \leq j \leq n$, by the mean value theorem, we can find $b^j$ satisfying $a^j<b^j<a^j+h^j$, so that, $$\lim_{h \to 0} \frac{|f(a+h)-f(a)-(\sum_{j=1}^n D_jf^1(a)(h^j),...,\sum_{j=1}^n D_jf^m(a)(h^j))|}{|h|}=$$ $$\lim_{h \to 0} \frac{|f(a^1+h^1,a^2...,a^n)-f(a)+...+f(a+h)-f(a^1+h^1,...,a^{n-1}+h^{n-1},a^n)-(...)|}{|h|}=$$ $$\lim_{h \to 0} \frac{|D_1f(b^1,a^2...,a^n)(h^1)+...+D_nf(a^1+h^1,...,a^{n-1}+h^{n-1},b^n)(h^n)-(...)|}{|h|}=$$ $$\lim_{h \to 0} \frac{| (\sum_{j=1}^n [D_jf^1(c_j)-D_jf^1(a)](h^j),...,\sum_{j=1}^n [D_jf^m(c_j)-D_jf^m(a)](h^j))|}{|h|} \leq$$ $$\lim_{h \to 0} |(\sum_{j=1}^n |D_jf^1(c_j)-D_jf^1(a)|\frac{|h^j|}{|h|},...,\sum_{j=1}^n |D_jf^m(c_j)-D_jf^m(a)|\frac{|h^j|}{|h|})| \leq$$ $$\lim_{h \to 0} |(\sum_{j=1}^n |D_jf^1(c_j)-D_jf^1(a)|(1),...,\sum_{j=1}^n |D_jf^m(c_j)-D_jf^m(a)|(1))|=0,$$ where $h=(h^1,...,h^m)$, each $c_j$ is defined suitably in terms of $a^j$'s, $b^j$'s and $h^j$'s, and the last equality holds by the continuity hypothesis. Therefore $Df(a)$ exists. Thanks in advance.","This is the statement of Theorem 2.8 from Spivak's Calculus on Manifolds. I'd like feedback on if this looks fine as far as a generalization to his proof goes: Theorem: If $f: \mathbb{R}^n \rightarrow \mathbb{R}^m$, then $Df(a)$ exists if all $D_jf^i(x)$ exist in an open set containing $a$ and if each function $D_jf^i$ is continuous at $a$. Proof: Let $f: \mathbb{R}^n \rightarrow \mathbb{R}^m$ and suppose that all $D_jf^i(x)$ exist in an open set containing $a=(a^1,...,a^n)$ and that each function $D_jf^i$ is continuous at $a$. Then, for each $j$ such that $1 \leq j \leq n$, by the mean value theorem, we can find $b^j$ satisfying $a^j<b^j<a^j+h^j$, so that, $$\lim_{h \to 0} \frac{|f(a+h)-f(a)-(\sum_{j=1}^n D_jf^1(a)(h^j),...,\sum_{j=1}^n D_jf^m(a)(h^j))|}{|h|}=$$ $$\lim_{h \to 0} \frac{|f(a^1+h^1,a^2...,a^n)-f(a)+...+f(a+h)-f(a^1+h^1,...,a^{n-1}+h^{n-1},a^n)-(...)|}{|h|}=$$ $$\lim_{h \to 0} \frac{|D_1f(b^1,a^2...,a^n)(h^1)+...+D_nf(a^1+h^1,...,a^{n-1}+h^{n-1},b^n)(h^n)-(...)|}{|h|}=$$ $$\lim_{h \to 0} \frac{| (\sum_{j=1}^n [D_jf^1(c_j)-D_jf^1(a)](h^j),...,\sum_{j=1}^n [D_jf^m(c_j)-D_jf^m(a)](h^j))|}{|h|} \leq$$ $$\lim_{h \to 0} |(\sum_{j=1}^n |D_jf^1(c_j)-D_jf^1(a)|\frac{|h^j|}{|h|},...,\sum_{j=1}^n |D_jf^m(c_j)-D_jf^m(a)|\frac{|h^j|}{|h|})| \leq$$ $$\lim_{h \to 0} |(\sum_{j=1}^n |D_jf^1(c_j)-D_jf^1(a)|(1),...,\sum_{j=1}^n |D_jf^m(c_j)-D_jf^m(a)|(1))|=0,$$ where $h=(h^1,...,h^m)$, each $c_j$ is defined suitably in terms of $a^j$'s, $b^j$'s and $h^j$'s, and the last equality holds by the continuity hypothesis. Therefore $Df(a)$ exists. Thanks in advance.",,['multivariable-calculus']
9,Relation between the Hessian matrix and curvature,Relation between the Hessian matrix and curvature,,"According to Hessian matrix ,  It describes the local curvature of a function. AFAIK, for one-variable function $f(x)$ , its local curvature is $$\kappa = \frac{|f''|}{(1 + f'^2)^{3/2}},$$ and its Hessian matrix is $$\mathcal{Hess}(f) = [f''],$$ right? And here is my problem, I think the local curvature is not just described by its Hessian matrix, because $f'$ also has its role in it, doesn't it? And furthermore, for 2-variable function $f(x,y)$ , its Hessian matrix is $$\mathcal{Hess}(f) = \left[ \begin{array}{cc} f_{xx}'' & f_{xy}'' \\ f_{xy}'' & f_{yy}'' \end{array} \right].$$ How does it relate to the local curvature of $f(x,y)$ ?","According to Hessian matrix ,  It describes the local curvature of a function. AFAIK, for one-variable function , its local curvature is and its Hessian matrix is right? And here is my problem, I think the local curvature is not just described by its Hessian matrix, because also has its role in it, doesn't it? And furthermore, for 2-variable function , its Hessian matrix is How does it relate to the local curvature of ?","f(x) \kappa = \frac{|f''|}{(1 + f'^2)^{3/2}}, \mathcal{Hess}(f) = [f''], f' f(x,y) \mathcal{Hess}(f) = \left[ \begin{array}{cc}
f_{xx}'' & f_{xy}'' \\
f_{xy}'' & f_{yy}''
\end{array} \right]. f(x,y)","['multivariable-calculus', 'curvature', 'hessian-matrix', 'scalar-fields']"
10,What is the difference between $d$ and $\partial$?,What is the difference between  and ?,d \partial,"After seeing the following equation in a lecture about tensor analysis , I became confused. $$ \frac{d\phi}{ds}=\frac{\partial \phi}{\partial x^m}\frac{dx^m}{ds} $$ What exactly is the difference between $d$ and $\partial$?","After seeing the following equation in a lecture about tensor analysis , I became confused. $$ \frac{d\phi}{ds}=\frac{\partial \phi}{\partial x^m}\frac{dx^m}{ds} $$ What exactly is the difference between $d$ and $\partial$?",,"['multivariable-calculus', 'notation', 'derivatives']"
11,Is it possible for the Lagrange multiplier to be equal to zero?,Is it possible for the Lagrange multiplier to be equal to zero?,,"I would like to find the extrema of the function $f(x,y)=x^2+4xy+4y^2$ subject to $x^2+2y^2=4$ using Lagrange Multipliers. Is it possible to get for the Lagrange multipliers the value zero? I don't think so because the gradient vectors for $f$ and $g(x,y)=x^2+2y^2$ have to be proportional. So this problem has two solutions. But their image by the function $f$ is the same. So, how can I know if they correspond to a maximum or to a minimum?","I would like to find the extrema of the function $f(x,y)=x^2+4xy+4y^2$ subject to $x^2+2y^2=4$ using Lagrange Multipliers. Is it possible to get for the Lagrange multipliers the value zero? I don't think so because the gradient vectors for $f$ and $g(x,y)=x^2+2y^2$ have to be proportional. So this problem has two solutions. But their image by the function $f$ is the same. So, how can I know if they correspond to a maximum or to a minimum?",,"['multivariable-calculus', 'functions', 'optimization', 'lagrange-multiplier']"
12,Show that the form $w$ is closed but not exact,Show that the form  is closed but not exact,w,"Let $$w~=~\dfrac{-y}{x^2+y^2}dx+\dfrac{x}{x^2+y^2}dy, \qquad (x,y)~\in\mathbb{R}^2\backslash \{(0,0)\}.$$ Showing that $w$ is closed is easy. Just calculate $dw$ and you'll get 0. But how do I show that $w$ is not exact? In other words, I need to prove that there is no form $\lambda$ such that $w=d \lambda$ Should I assume that $w=d \lambda$ and try to get to a contradiction?","Let $$w~=~\dfrac{-y}{x^2+y^2}dx+\dfrac{x}{x^2+y^2}dy, \qquad (x,y)~\in\mathbb{R}^2\backslash \{(0,0)\}.$$ Showing that $w$ is closed is easy. Just calculate $dw$ and you'll get 0. But how do I show that $w$ is not exact? In other words, I need to prove that there is no form $\lambda$ such that $w=d \lambda$ Should I assume that $w=d \lambda$ and try to get to a contradiction?",,"['multivariable-calculus', 'differential-forms']"
13,How to derive the divergence theorem from the General Stokes theorem?,How to derive the divergence theorem from the General Stokes theorem?,,"So, given the generalized Stokes theorem: $$\int_{\partial M} \omega = \int_M d\omega$$ where $M$ is an $n$ -dimensional surface and $\omega$ is a $p$ -form on $M$ ( $p$ < $n$ ). How can I derive the Divergence Theorem? $$\iint_S {\bf F} \cdot d{\bf S} = \iiint_R \text{div}\;{\bf F}\; dV$$ I also have another related question. I'm learning that there are several theorems, like the divergence theorem, that are special cases of the generalized Stokes Theorem. For example, apparently, the Kelvin-Stokes Theorem is a special case of the General Stokes Theorem where $n=2$ . So my 2nd question is, what if $n=1$ in the general stokes theorem? What does that imply or lead to? Thank You.","So, given the generalized Stokes theorem: where is an -dimensional surface and is a -form on ( < ). How can I derive the Divergence Theorem? I also have another related question. I'm learning that there are several theorems, like the divergence theorem, that are special cases of the generalized Stokes Theorem. For example, apparently, the Kelvin-Stokes Theorem is a special case of the General Stokes Theorem where . So my 2nd question is, what if in the general stokes theorem? What does that imply or lead to? Thank You.",\int_{\partial M} \omega = \int_M d\omega M n \omega p M p n \iint_S {\bf F} \cdot d{\bf S} = \iiint_R \text{div}\;{\bf F}\; dV n=2 n=1,"['multivariable-calculus', 'differential-geometry', 'stokes-theorem']"
14,How (and why) would I reparameterize a curve in terms of arclength?,How (and why) would I reparameterize a curve in terms of arclength?,,"Let's say I have a helix: $\mathbf{r}(t) = \cos t \mathbf{i} + \sin t \mathbf{j} + t\mathbf{k}$ If I'm asked to ""reparameterize"" this function with respect to arclength starting at $(1,0,0)$ for increasing value of $t$... conceptually I don't know what that means. Mechanically, I know that I need to find the norm of $\mathbf{r'}(t)$ and the integral of that as $t$ goes from some $t_1$ to some $t_2$ gives me the arclength between $t_1$ and $t_2$. Beyond that, I'm pretty lost. My calculus books is explaining that you would want to do this so your function wouldn't be tied to any particular coordinate system. Does that mean that this is just another way to represent the equation of a curve?","Let's say I have a helix: $\mathbf{r}(t) = \cos t \mathbf{i} + \sin t \mathbf{j} + t\mathbf{k}$ If I'm asked to ""reparameterize"" this function with respect to arclength starting at $(1,0,0)$ for increasing value of $t$... conceptually I don't know what that means. Mechanically, I know that I need to find the norm of $\mathbf{r'}(t)$ and the integral of that as $t$ goes from some $t_1$ to some $t_2$ gives me the arclength between $t_1$ and $t_2$. Beyond that, I'm pretty lost. My calculus books is explaining that you would want to do this so your function wouldn't be tied to any particular coordinate system. Does that mean that this is just another way to represent the equation of a curve?",,['multivariable-calculus']
15,"The ""second derivative test"" for $f(x,y)$","The ""second derivative test"" for","f(x,y)","I'm currently taking multivariable calculus, and I'm familiar with the second partial derivative test. That is, the formula $D(a, b) = f_{xx}(a,b)f_{yy}(a, b) - (f_{xy}(a, b))^2$ to determine the behavior of $f(x,y)$ at the point $(a, b, f(a,b))$. However, my professor simply ""spat"" this formula at us and provided almost no explanation of its derivation/where it comes from. After researching a bit on my own, I now know that it's the determinant of the Hessian matrix for $f(x,y)$, and I see how the formula is easily derived from that matrix. Wikipedia just says: The following test can be applied at a non-degenerate critical point $x$. If the Hessian is positive definite at $x$, then $f$ attains a local minimum at $x$. If the Hessian is negative definite at $x$, then $f$ attains a local maximum at $x$. If the Hessian has both positive and negative eigenvalues then $x$ is a saddle point for $f$ (this is true even if $x$ is degenerate). Otherwise the test is inconclusive."" I understand that, but I still don't understand why the determinant of this matrix happens to model the behavior of $f$ in this way. Why is it? And if the test happens to fail, what steps should then be taken to determine the nature of $f(x,y)$ at $(a, b, f(a,b))$?","I'm currently taking multivariable calculus, and I'm familiar with the second partial derivative test. That is, the formula $D(a, b) = f_{xx}(a,b)f_{yy}(a, b) - (f_{xy}(a, b))^2$ to determine the behavior of $f(x,y)$ at the point $(a, b, f(a,b))$. However, my professor simply ""spat"" this formula at us and provided almost no explanation of its derivation/where it comes from. After researching a bit on my own, I now know that it's the determinant of the Hessian matrix for $f(x,y)$, and I see how the formula is easily derived from that matrix. Wikipedia just says: The following test can be applied at a non-degenerate critical point $x$. If the Hessian is positive definite at $x$, then $f$ attains a local minimum at $x$. If the Hessian is negative definite at $x$, then $f$ attains a local maximum at $x$. If the Hessian has both positive and negative eigenvalues then $x$ is a saddle point for $f$ (this is true even if $x$ is degenerate). Otherwise the test is inconclusive."" I understand that, but I still don't understand why the determinant of this matrix happens to model the behavior of $f$ in this way. Why is it? And if the test happens to fail, what steps should then be taken to determine the nature of $f(x,y)$ at $(a, b, f(a,b))$?",,"['multivariable-calculus', 'determinant', 'partial-derivative']"
16,Gradient of a Vector Valued function,Gradient of a Vector Valued function,,"I read somewhere, gradient vector is defined only for scalar valued functions and not for vector valued functions. When gradient of a vector is definitely defined(correct, right?), why is gradient vector of a vector valued function not defined?  Is my understanding incorrect? Is there not a contradiction? I would appreciate clear clarification. Thank You.","I read somewhere, gradient vector is defined only for scalar valued functions and not for vector valued functions. When gradient of a vector is definitely defined(correct, right?), why is gradient vector of a vector valued function not defined?  Is my understanding incorrect? Is there not a contradiction? I would appreciate clear clarification. Thank You.",,"['multivariable-calculus', 'derivatives']"
17,Property of Dirac delta function in $\mathbb{R}^n$,Property of Dirac delta function in,\mathbb{R}^n,"How does one prove the following identity? $$\int _Vf(\pmb{r})\delta (g(\pmb{r})) \, d\pmb{r}=\int_S \frac{f(\pmb{r})}{\left|\operatorname{grad} g(\pmb{r})\right|} \, d\sigma$$ where $S$ is the surface inside $V$ where $g(\pmb{r})=0$ and it is assumed that $\operatorname{grad} g(\pmb{r})\neq 0$ . Thanks. Edit: I have proved a one-dimensional version of this formula: $$\delta (g(x))=\sum _a \frac{\delta (x-a)}{\left|g'(a)\right|}$$ where $a$ goes through the zeroes of $g(x)$ and it is assumed that at those points $g'(a)\neq 0$ . the integral can be divided into into a sum of integrals over small intervals containing the zeros of $g(x)$ . In these intervals $g(x)$ can be approximated by $g(a)+(x-a)g'(a)=(x-a)g'(a)$ since $g(a)=0$ . Thus $$\int_{-\infty}^\infty f(x)\delta (g(x)) \, dx=\sum _a \int _{a-\varepsilon}^{a+\varepsilon }f(x)\delta ((x-a)g'(a)) \, dx$$ Using the property $\delta (kx)=\frac{\delta (x)}{|k|}$ , it follows that $$\int_{-\infty}^\infty f(x)\delta (g(x)) \, dx=\sum _a \frac{f(a)}{\left|g'(a)\right|}$$ This is the same result we would have obtained if we had written $\sum _a \frac{\delta (x-a)}{\left|g'(a)\right|}$ instead of $\delta (g(x))$ as a factor of the integrand.","How does one prove the following identity? where is the surface inside where and it is assumed that . Thanks. Edit: I have proved a one-dimensional version of this formula: where goes through the zeroes of and it is assumed that at those points . the integral can be divided into into a sum of integrals over small intervals containing the zeros of . In these intervals can be approximated by since . Thus Using the property , it follows that This is the same result we would have obtained if we had written instead of as a factor of the integrand.","\int _Vf(\pmb{r})\delta (g(\pmb{r})) \, d\pmb{r}=\int_S \frac{f(\pmb{r})}{\left|\operatorname{grad} g(\pmb{r})\right|} \, d\sigma S V g(\pmb{r})=0 \operatorname{grad} g(\pmb{r})\neq 0 \delta (g(x))=\sum _a \frac{\delta (x-a)}{\left|g'(a)\right|} a g(x) g'(a)\neq 0 g(x) g(x) g(a)+(x-a)g'(a)=(x-a)g'(a) g(a)=0 \int_{-\infty}^\infty f(x)\delta (g(x)) \, dx=\sum _a \int _{a-\varepsilon}^{a+\varepsilon }f(x)\delta ((x-a)g'(a)) \, dx \delta (kx)=\frac{\delta (x)}{|k|} \int_{-\infty}^\infty f(x)\delta (g(x)) \, dx=\sum _a \frac{f(a)}{\left|g'(a)\right|} \sum _a \frac{\delta (x-a)}{\left|g'(a)\right|} \delta (g(x))","['multivariable-calculus', 'physics', 'distribution-theory']"
18,Intuition for why $f_{xy} = f_{yx}$,Intuition for why,f_{xy} = f_{yx},"If we have a function $f(x,y)$ , why is it that $f_{xy} = f_{yx}$ ? I'm looking for an intuitive, qualitative reason rather than a rigorous proof. $f_{yx}$ represents the rate of change of the gradient parallel to the $x$ axis, as you move along the $y$ axis. Similarly, $f_{xy}$ represents the rate of change of the gradient parallel to the $y$ axis, as you move along the $x$ axis. At least, this is how I understand it. However, I can't see any reason why the two should be the same.","If we have a function , why is it that ? I'm looking for an intuitive, qualitative reason rather than a rigorous proof. represents the rate of change of the gradient parallel to the axis, as you move along the axis. Similarly, represents the rate of change of the gradient parallel to the axis, as you move along the axis. At least, this is how I understand it. However, I can't see any reason why the two should be the same.","f(x,y) f_{xy} = f_{yx} f_{yx} x y f_{xy} y x","['multivariable-calculus', 'intuition']"
19,Are independent variables really independent?,Are independent variables really independent?,,"I am studying partial differential equations, in which the class notes includes the following statement: $$x^2 + y^2 + (z-c)^2 = a^2$$ Here z is the dependent variable; x,y are independent variables; and a,c are arbitary constants. Mathematically, all the variables depend on every other variable, so there shouldn't be any difference between x,y or z. Is this a subjective distinction? Aren't variables objectively all dependent on each other?","I am studying partial differential equations, in which the class notes includes the following statement: $$x^2 + y^2 + (z-c)^2 = a^2$$ Here z is the dependent variable; x,y are independent variables; and a,c are arbitary constants. Mathematically, all the variables depend on every other variable, so there shouldn't be any difference between x,y or z. Is this a subjective distinction? Aren't variables objectively all dependent on each other?",,"['multivariable-calculus', 'partial-derivative']"
20,Intuition on the curl formula,Intuition on the curl formula,,"I'm trying to understand the formula for the curl:  $$\operatorname{Curl}(F) = \nabla \times F $$ Why the vector product? What means the vector product of a vector and a operator? What is the meaning of each of the resulting terms $  \frac { \partial F_y }{ \partial x } -\frac { \partial F_x }{ \partial y } $  ? I think that I understand the concept of the curl, I recommend this if anyone is interested. The problem is in the formula.","I'm trying to understand the formula for the curl:  $$\operatorname{Curl}(F) = \nabla \times F $$ Why the vector product? What means the vector product of a vector and a operator? What is the meaning of each of the resulting terms $  \frac { \partial F_y }{ \partial x } -\frac { \partial F_x }{ \partial y } $  ? I think that I understand the concept of the curl, I recommend this if anyone is interested. The problem is in the formula.",,"['multivariable-calculus', 'intuition']"
21,Intuition behind Lagrange multiplier,Intuition behind Lagrange multiplier,,"I noticed that all attempts of showcasing the intuition behind Lagrange's multipliers basically resort to the following example (taken from Wikipedia): The reason why such examples make sense is that the level curves of the f function are either only decreasing (d1 < d2 < d3) or only increasing (d1 > d2 > d3) concentrically, so it's obvious that the most centric level curve touching the constraint curve is the minimum/maximum that we are looking for. But in real life examples, I imagine we can have a function f whose level curves might look like this (they don't decrease/increase in an orderly fashion): In the above example that I thought of, the maximization of f (subject to the g constraint) would not be the blue point (where the constraint curve is tangent to a level curve of f), but the two green points. I haven't seen this kind of level arrangement in any tutorial/lecture/course and it just seems to me that every demonstration conveniently picks the most favorable scenario for presenting the intuition. I must be wrong somewhere but I can't figure out where.","I noticed that all attempts of showcasing the intuition behind Lagrange's multipliers basically resort to the following example (taken from Wikipedia): The reason why such examples make sense is that the level curves of the f function are either only decreasing (d1 < d2 < d3) or only increasing (d1 > d2 > d3) concentrically, so it's obvious that the most centric level curve touching the constraint curve is the minimum/maximum that we are looking for. But in real life examples, I imagine we can have a function f whose level curves might look like this (they don't decrease/increase in an orderly fashion): In the above example that I thought of, the maximization of f (subject to the g constraint) would not be the blue point (where the constraint curve is tangent to a level curve of f), but the two green points. I haven't seen this kind of level arrangement in any tutorial/lecture/course and it just seems to me that every demonstration conveniently picks the most favorable scenario for presenting the intuition. I must be wrong somewhere but I can't figure out where.",,"['multivariable-calculus', 'optimization', 'lagrange-multiplier']"
22,Unique critical point does not imply global maximum/global minimum,Unique critical point does not imply global maximum/global minimum,,"I have actually two questions here, but both are very much related so I decided to put them both in this question. From Wikipedia I found the following example of a function that has a single critical point which is a local minimum, yet is not a global minimum: $f(x,y) = x^2 + y^2(1-x)^3$ Indeed the only critical point of $f$ is $(0,0)$ which is a local minimum by the second derivative test, but $(0,0)$ is not a global minimum because $f(4,1) = -11$. Why does this happen? I am having some trouble understanding intuitively why this is true. I know that $f$ has a minimum when restricted to a compact set. Then $(0,0)$ is a global minimum in any closed ball around $(0,0)$, and thus $(0,0)$ is a global maximum?? This is not true because $f$ is a counterexample, but I can't quite understand why. In some cases we do have a global minimum at the unique critical point. For example, let $f(x,y) = (-x+y)^2 + (x-1)^2 + (x+y-1)^2$ Then $f$ has a unique critical point at $(\frac{2}{3}, \frac{1}{6})$. The critical point turns out to be a global minimum (according to the graph and wolframalpha). How would I prove that the point is a global minimum? And how does one in general decide that a function has a global minimum/maximum on a critical point when the function is defined on all of $\mathbb{R}^2$, particularly when there is only one critical point?","I have actually two questions here, but both are very much related so I decided to put them both in this question. From Wikipedia I found the following example of a function that has a single critical point which is a local minimum, yet is not a global minimum: $f(x,y) = x^2 + y^2(1-x)^3$ Indeed the only critical point of $f$ is $(0,0)$ which is a local minimum by the second derivative test, but $(0,0)$ is not a global minimum because $f(4,1) = -11$. Why does this happen? I am having some trouble understanding intuitively why this is true. I know that $f$ has a minimum when restricted to a compact set. Then $(0,0)$ is a global minimum in any closed ball around $(0,0)$, and thus $(0,0)$ is a global maximum?? This is not true because $f$ is a counterexample, but I can't quite understand why. In some cases we do have a global minimum at the unique critical point. For example, let $f(x,y) = (-x+y)^2 + (x-1)^2 + (x+y-1)^2$ Then $f$ has a unique critical point at $(\frac{2}{3}, \frac{1}{6})$. The critical point turns out to be a global minimum (according to the graph and wolframalpha). How would I prove that the point is a global minimum? And how does one in general decide that a function has a global minimum/maximum on a critical point when the function is defined on all of $\mathbb{R}^2$, particularly when there is only one critical point?",,['multivariable-calculus']
23,"How prove there exists a point $(x_{0},y_{0})$, such $\Delta f|_{(x_{0},y_{0})}\ge 0$","How prove there exists a point , such","(x_{0},y_{0}) \Delta f|_{(x_{0},y_{0})}\ge 0","Question: Assume that the function $f(x,y)$ is twice continuously differentiable on $\mathbb R^2$, and $$f\big|_{\partial\Sigma}=0,\quad \text{where}\,\,\,\partial\Sigma=\{(x,y)\in\mathbb R^2:x^2+y^2=1\}, $$ and $$ \lim_{x\to\infty}f(x,0)=1. $$ Show that there exists a point $(x_{0},y_{0})$, such $\Delta f(x_{0},y_{0})\ge 0$, where $\Delta$ is Laplace operator. See http://en.wikipedia.org/wiki/Laplace_operator This problem  is from (problem 8): http://www.doc88.com/p-98539219502.html My try: since  $$ \Delta f=\dfrac{\partial^2f}{\partial x^2}+\dfrac{\partial^2f}{\partial y^2}, $$ so assume that $$\Delta f=\dfrac{\partial^2f}{\partial x^2}+\dfrac{\partial^2f}{\partial y^2}<0,\,\forall(x,y)\in \mathbb R^2,$$ then I can't , This problem is hard? Thank you","Question: Assume that the function $f(x,y)$ is twice continuously differentiable on $\mathbb R^2$, and $$f\big|_{\partial\Sigma}=0,\quad \text{where}\,\,\,\partial\Sigma=\{(x,y)\in\mathbb R^2:x^2+y^2=1\}, $$ and $$ \lim_{x\to\infty}f(x,0)=1. $$ Show that there exists a point $(x_{0},y_{0})$, such $\Delta f(x_{0},y_{0})\ge 0$, where $\Delta$ is Laplace operator. See http://en.wikipedia.org/wiki/Laplace_operator This problem  is from (problem 8): http://www.doc88.com/p-98539219502.html My try: since  $$ \Delta f=\dfrac{\partial^2f}{\partial x^2}+\dfrac{\partial^2f}{\partial y^2}, $$ so assume that $$\Delta f=\dfrac{\partial^2f}{\partial x^2}+\dfrac{\partial^2f}{\partial y^2}<0,\,\forall(x,y)\in \mathbb R^2,$$ then I can't , This problem is hard? Thank you",,['multivariable-calculus']
24,"Multivariable Calculus books similar to ""Advanced Calculus of Several Variables"" by C.H. Edwards","Multivariable Calculus books similar to ""Advanced Calculus of Several Variables"" by C.H. Edwards",,"I am currently trying to teach myself multivariable calculus using C.H. Edwards' ""Advanced Calculus of Several Variables"", but the text unfortunately doesn't have very many problems with solutions. I've attempted a number of the problems, but I'm not sure if my solutions are correct. Could someone suggest any other books that I could use that have lots of problems that are of the same caliber as those in this book? Thanks! :-)","I am currently trying to teach myself multivariable calculus using C.H. Edwards' ""Advanced Calculus of Several Variables"", but the text unfortunately doesn't have very many problems with solutions. I've attempted a number of the problems, but I'm not sure if my solutions are correct. Could someone suggest any other books that I could use that have lots of problems that are of the same caliber as those in this book? Thanks! :-)",,"['reference-request', 'multivariable-calculus', 'self-learning']"
25,Calculate the area on a sphere of the intersection of two spherical caps,Calculate the area on a sphere of the intersection of two spherical caps,,"Given a sphere of radius $r$ with two spherical caps on it defined by the radii ($a_1$ and $a_2$) of the bases of the spherical caps, given a separation of the two spherical caps by angle $\theta$, how do you calculate the surface area of that intersection? To clarify, the area is that of the curved surface on the sphere defined by the intersection. At the extreme where both $a_1,a_2 = r$, we would be describing a spherical lune. Alternatively define the spherical caps by the angles  $\Phi_1 = \arcsin(a_1/r)$ and $\Phi_2 = \arcsin(a_2/r)$.","Given a sphere of radius $r$ with two spherical caps on it defined by the radii ($a_1$ and $a_2$) of the bases of the spherical caps, given a separation of the two spherical caps by angle $\theta$, how do you calculate the surface area of that intersection? To clarify, the area is that of the curved surface on the sphere defined by the intersection. At the extreme where both $a_1,a_2 = r$, we would be describing a spherical lune. Alternatively define the spherical caps by the angles  $\Phi_1 = \arcsin(a_1/r)$ and $\Phi_2 = \arcsin(a_2/r)$.",,['multivariable-calculus']
26,"What are examples of parallelizable manifolds, and why does parallelizable correspond to $TM$ being trivial?","What are examples of parallelizable manifolds, and why does parallelizable correspond to  being trivial?",TM,"I know a differentiable manifold $M$ of dimension $n$ is parallelizable if there exist (smooth of course) vector fields $\{X_i\}_{j=1}^n$ which are linearly independent in $T_pM$ at each point $p \in M$. What are few examples of parallelizable manifolds of dimension at least ? What is an example which is compact? What are some nonparalleizable ones? How do I show that a manifold $M$ as above is parallelizable if and only if the tangent bundle $TM$ is trivial? I am a total beginner with these smooth manifold things, so any help or references to a book that can help answer my quetsions would be greatly appreciated! Some thoughts so far. I give some examples in $\mathbb{R}^3$. A cylinder is certainly paralleizable by taking vector fields $X_1$ constant, parallel to the direction of the axis and $X_2$ constant, pointing around the circumference of the cylinder. Another example is a torus; take vector fields $X_1$ constant, pointing along the circumference of the ""big circle"" and $X_2$ constant, looping in and out of the ""donut hole."" The torus is compact, and the cylinder is also compact if it is finite. As for nonparallelizable manifolds $S^2$ is an example; any attempt to parallelize $S^2$ leads to points on the sphere at which the vector field cannot be nonzero, which isn't good if we want to get the tangent spce at those points by combining tangent vectors from the vector fields. This is true by the Hairy Ball Theorem.","I know a differentiable manifold $M$ of dimension $n$ is parallelizable if there exist (smooth of course) vector fields $\{X_i\}_{j=1}^n$ which are linearly independent in $T_pM$ at each point $p \in M$. What are few examples of parallelizable manifolds of dimension at least ? What is an example which is compact? What are some nonparalleizable ones? How do I show that a manifold $M$ as above is parallelizable if and only if the tangent bundle $TM$ is trivial? I am a total beginner with these smooth manifold things, so any help or references to a book that can help answer my quetsions would be greatly appreciated! Some thoughts so far. I give some examples in $\mathbb{R}^3$. A cylinder is certainly paralleizable by taking vector fields $X_1$ constant, parallel to the direction of the axis and $X_2$ constant, pointing around the circumference of the cylinder. Another example is a torus; take vector fields $X_1$ constant, pointing along the circumference of the ""big circle"" and $X_2$ constant, looping in and out of the ""donut hole."" The torus is compact, and the cylinder is also compact if it is finite. As for nonparallelizable manifolds $S^2$ is an example; any attempt to parallelize $S^2$ leads to points on the sphere at which the vector field cannot be nonzero, which isn't good if we want to get the tangent spce at those points by combining tangent vectors from the vector fields. This is true by the Hairy Ball Theorem.",,"['multivariable-calculus', 'differential-geometry']"
27,Curl in cylindrical coordinates,Curl in cylindrical coordinates,,"I'm trying to figure out how to calculate curl ( $\nabla \times \vec{V}^{\,}$ ) when the velocity vector is represented in cylindrical coordinates. The way I thought I would do it is by calculating this determinant: $$\left|\begin{matrix}   e_r & e_{\theta} & e_{z} \\   \frac{\partial }{\partial r} & \frac{1}{r} \frac{\partial }{\partial \theta} & \frac{\partial }{\partial z} \\   v_r & v_\theta & v_z  \end{matrix}\right|$$ Which gives: $$\left\lbrack \frac{1}{r} \frac{\partial v_z}{\partial \theta} - \frac{\partial v_\theta}{\partial z}, \frac{\partial v_r}{\partial z} - \frac{\partial v_z}{\partial r}, \frac{\partial v_\theta}{\partial r} - \frac{1}{r} \frac{\partial v_r}{\partial \theta}\right\rbrack$$ But I think the correct curl is: $$\left\lbrack \frac{1}{r} \frac{\partial v_z}{\partial \theta} - \frac{\partial v_\theta}{\partial z}, \frac{\partial v_r}{\partial z} - \frac{\partial v_z}{\partial r}, \frac{1}{r} \frac{\partial rv_\theta}{\partial r} - \frac{1}{r} \frac{\partial v_r}{\partial \theta}\right\rbrack$$ Can anyone explain why this is? It seems sort of like the way to calculate it is with: $$\left|\begin{matrix}   e_r & e_{\theta} & e_{z} \\   \frac{1}{r}\frac{\partial }{\partial r} & \frac{1}{r} \frac{\partial }{\partial \theta} & \frac{\partial }{\partial z} \\   v_r & rv_\theta & v_z  \end{matrix} \right|$$ Is that correct?",I'm trying to figure out how to calculate curl ( ) when the velocity vector is represented in cylindrical coordinates. The way I thought I would do it is by calculating this determinant: Which gives: But I think the correct curl is: Can anyone explain why this is? It seems sort of like the way to calculate it is with: Is that correct?,"\nabla \times \vec{V}^{\,} \left|\begin{matrix}
  e_r & e_{\theta} & e_{z} \\
  \frac{\partial }{\partial r} & \frac{1}{r} \frac{\partial }{\partial \theta} & \frac{\partial }{\partial z} \\
  v_r & v_\theta & v_z
 \end{matrix}\right| \left\lbrack \frac{1}{r} \frac{\partial v_z}{\partial \theta} - \frac{\partial v_\theta}{\partial z}, \frac{\partial v_r}{\partial z} - \frac{\partial v_z}{\partial r}, \frac{\partial v_\theta}{\partial r} - \frac{1}{r} \frac{\partial v_r}{\partial \theta}\right\rbrack \left\lbrack \frac{1}{r} \frac{\partial v_z}{\partial \theta} - \frac{\partial v_\theta}{\partial z}, \frac{\partial v_r}{\partial z} - \frac{\partial v_z}{\partial r}, \frac{1}{r} \frac{\partial rv_\theta}{\partial r} - \frac{1}{r} \frac{\partial v_r}{\partial \theta}\right\rbrack \left|\begin{matrix}
  e_r & e_{\theta} & e_{z} \\
  \frac{1}{r}\frac{\partial }{\partial r} & \frac{1}{r} \frac{\partial }{\partial \theta} & \frac{\partial }{\partial z} \\
  v_r & rv_\theta & v_z
 \end{matrix}
\right|","['multivariable-calculus', 'change-of-variable', 'cylindrical-coordinates', 'curl', 'curvilinear-coordinates']"
28,What is the remainder for a taylor series of two variables.,What is the remainder for a taylor series of two variables.,,"I know that for a function of one variable $f(x)$, the Taylor expansion is $$f(x)=f(x_0)+f'(x_0)(x-x_0)+...+\frac{f^{(n)}(x_0)}{n!}(x-x_0)^n+R_n(x,x_0) $$ where  \begin{align*} R_n(x,x_0)&=\int^x_{x_0}\frac{(x-t)^n}{n!} f^{(n+1)}(t)dt \\ &=f^{(n+1)}(\xi)\int^x_{x_0} \frac{(x-t)^n}{n!}dt \\ &=f^{(n+1)}(\xi) \frac{(x-x_0)^{n+1}}{(n+1)!}. \end{align*} My question is for $f(x,y)$ with a Taylor expansion $$f(x)=f(x_0,y_0)+f_x(x_0,y_0)(x-x_0)+f_y(x_0,y_0)(y-y_0)+...+R_n(x,x_0,y,y_0) $$  What is $R_n(x,x_0,y,y_0)$?","I know that for a function of one variable $f(x)$, the Taylor expansion is $$f(x)=f(x_0)+f'(x_0)(x-x_0)+...+\frac{f^{(n)}(x_0)}{n!}(x-x_0)^n+R_n(x,x_0) $$ where  \begin{align*} R_n(x,x_0)&=\int^x_{x_0}\frac{(x-t)^n}{n!} f^{(n+1)}(t)dt \\ &=f^{(n+1)}(\xi)\int^x_{x_0} \frac{(x-t)^n}{n!}dt \\ &=f^{(n+1)}(\xi) \frac{(x-x_0)^{n+1}}{(n+1)!}. \end{align*} My question is for $f(x,y)$ with a Taylor expansion $$f(x)=f(x_0,y_0)+f_x(x_0,y_0)(x-x_0)+f_y(x_0,y_0)(y-y_0)+...+R_n(x,x_0,y,y_0) $$  What is $R_n(x,x_0,y,y_0)$?",,"['multivariable-calculus', 'taylor-expansion']"
29,Visualizing Commutator of Two Vector Fields,Visualizing Commutator of Two Vector Fields,,"I'm reading a book on calculus, the part about vector fields on  manifolds. It's a nice book, but with a severe drawback --- it has no pictures. I like how vectors are treated algebraically, as derivatives over a local ring (ring of germs). But I still want to use ""geometrical"" view on vector fields. The problem is I can't imagine vector field ""multiplication"" as a composition of derivatives. And thus I can't picture commutator of two vector fields. Has anybody here got pictures too help me?","I'm reading a book on calculus, the part about vector fields on  manifolds. It's a nice book, but with a severe drawback --- it has no pictures. I like how vectors are treated algebraically, as derivatives over a local ring (ring of germs). But I still want to use ""geometrical"" view on vector fields. The problem is I can't imagine vector field ""multiplication"" as a composition of derivatives. And thus I can't picture commutator of two vector fields. Has anybody here got pictures too help me?",,"['differential-geometry', 'multivariable-calculus']"
30,Understanding the concept behind the Lagrangian multiplier,Understanding the concept behind the Lagrangian multiplier,,"I've been trying to understand the principles behind the Lagrangian multipliers and I think I've got a rough understanding of it. Would appreciate it if you guys could help me answer a few questions! I've pretty much self-studied this from here and here . As far as I understand it, the Lagrangian multiplier essentially works by ensuring the gradient of the function is equal to the gradient of the restraint. Assuming that $g(x,y) = c$ is the restraint, it also ensures the point satisfies the restraint. However, I don't understand the reasoning behind some of the statements in the ideashop link: 1) ""The most important thing to know about gradients is that they always point in the direction of a function's steepest slope at a given point."" . Apparently a gradient is a collection of partial first derivatives but I don't understand how one gets from a collection of partial first derivatives to 'Always pointing in the direction of the steepest slope'. Wouldn't it be changing direction as one went around a 3D surface? 2) Why is the gradient always perpendicular to the level curve? 3) How does one get from $$\nabla L=\begin{bmatrix}\frac{\partial f}{\partial x_1}-\lambda\frac{\partial g}{\partial x_1}\\\frac{\partial f}{\partial x_2}-\lambda\frac{\partial g}{\partial x_2}\\g(x_1,x_2)-c\end{bmatrix}=0$$ to $$L(x_1,x_2,\lambda)=f(x_1,x_2)-\lambda(g(x_1,x_2)-c)$$ (Both sourced from ideashop) ? Thank you for your help in advance!","I've been trying to understand the principles behind the Lagrangian multipliers and I think I've got a rough understanding of it. Would appreciate it if you guys could help me answer a few questions! I've pretty much self-studied this from here and here . As far as I understand it, the Lagrangian multiplier essentially works by ensuring the gradient of the function is equal to the gradient of the restraint. Assuming that $g(x,y) = c$ is the restraint, it also ensures the point satisfies the restraint. However, I don't understand the reasoning behind some of the statements in the ideashop link: 1) ""The most important thing to know about gradients is that they always point in the direction of a function's steepest slope at a given point."" . Apparently a gradient is a collection of partial first derivatives but I don't understand how one gets from a collection of partial first derivatives to 'Always pointing in the direction of the steepest slope'. Wouldn't it be changing direction as one went around a 3D surface? 2) Why is the gradient always perpendicular to the level curve? 3) How does one get from $$\nabla L=\begin{bmatrix}\frac{\partial f}{\partial x_1}-\lambda\frac{\partial g}{\partial x_1}\\\frac{\partial f}{\partial x_2}-\lambda\frac{\partial g}{\partial x_2}\\g(x_1,x_2)-c\end{bmatrix}=0$$ to $$L(x_1,x_2,\lambda)=f(x_1,x_2)-\lambda(g(x_1,x_2)-c)$$ (Both sourced from ideashop) ? Thank you for your help in advance!",,"['multivariable-calculus', 'optimization']"
31,How to explain this quirk of the chain rule?,How to explain this quirk of the chain rule?,,"Assume I have a function $f = f(y, \phi(y,x))$ and I want to calculate $\frac{\partial f}{\partial y}$, I use the chain rule to get \begin{equation} \frac{\partial f}{\partial y} = \frac{\partial f}{\partial y} + \frac{\partial f}{\partial \phi}\frac{\partial \phi}{\partial y}  \end{equation} but obviously the $\frac{\partial f}{\partial y}$ represent different things on each side of equality. How do I explain this? I'm guessing it is a notational issue. Edit: Just to give some context why this troubles me. Here $x_i$ refers to the ith component of the vector $\mathbf{x}$ in euclidean space. In an acoustic textbook the Lighthill stress tensor $T_{ij}$ is involved in the following identity: \begin{equation} \frac{\partial}{\partial x_i} \frac{T_{ij}(\mathbf{y},t-|\mathbf{x}-\mathbf{y}|/c)}{|\mathbf{x}-\mathbf{y}|} = \frac{\frac{\partial T_{ij}}{\partial y_i}}{|\mathbf{x}-\mathbf{y}|} - \frac{\partial}{\partial y_i} \frac{T_{ij}(\mathbf{y},t-|\mathbf{x}-\mathbf{y}|/c)}{|\mathbf{x}-\mathbf{y}|}  \end{equation} This can only be resolved if the numerator in the term $\frac{\frac{\partial T_{ij}}{\partial y_i}}{|\mathbf{x}-\mathbf{y}|}$ is given a different interpretation...Just try showing this: Let $t-|\mathbf{x}-\mathbf{y}|/c = \phi(t,\mathbf{x}, \mathbf{y})$ \begin{array}{lcl}  \frac{\partial}{\partial x_i} \frac{T_{ij}(\mathbf{y},\phi)}{|\mathbf{x}-\mathbf{y}|} & = & \frac{1}{|\mathbf{x}-\mathbf{y}|} \frac{\partial}{\partial x_i}T_{ij}(\mathbf{y},\phi) + T_{ij}(\mathbf{y},\phi) \frac{\partial}{\partial x_i} \frac{1}{|\mathbf{x}-\mathbf{y}|}  \\   & = &  \frac{1}{|\mathbf{x}-\mathbf{y}|} (\frac{\partial T_{ij}}{\partial \phi}\frac{\partial \phi}{\partial x_i}) + T_{ij}(\mathbf{y},\phi) \frac{\partial}{\partial x_i} \frac{1}{|\mathbf{x}-\mathbf{y}|}\\  & = &  -\frac{1}{|\mathbf{x}-\mathbf{y}|} (\frac{\partial T_{ij}}{\partial \phi}\frac{\partial \phi}{\partial y_i}) + T_{ij}(\mathbf{y},\phi) \frac{\partial}{\partial x_i} \frac{1}{|\mathbf{x}-\mathbf{y}|}  \end{array} \begin{array}{lcl}  \frac{\partial}{\partial y_i} \frac{T_{ij}(\mathbf{y},\phi)}{|\mathbf{x}-\mathbf{y}|} & = & \frac{1}{|\mathbf{x}-\mathbf{y}|} \frac{\partial}{\partial y_i}T_{ij}(\mathbf{y},\phi) + T_{ij}(\mathbf{y},\phi) \frac{\partial}{\partial y_i} \frac{1}{|\mathbf{x}-\mathbf{y}|}  \\   & = &  \frac{1}{|\mathbf{x}-\mathbf{y}|} ( \frac{\partial}{\partial y_i}T_{ij} +\frac{\partial T_{ij}}{\partial \phi}\frac{\partial \phi}{\partial y_i}) - T_{ij}(\mathbf{y},\phi) \frac{\partial}{\partial x_i} \frac{1}{|\mathbf{x}-\mathbf{y}|}  \end{array} Adding up the last line from each expression gives the result.","Assume I have a function $f = f(y, \phi(y,x))$ and I want to calculate $\frac{\partial f}{\partial y}$, I use the chain rule to get \begin{equation} \frac{\partial f}{\partial y} = \frac{\partial f}{\partial y} + \frac{\partial f}{\partial \phi}\frac{\partial \phi}{\partial y}  \end{equation} but obviously the $\frac{\partial f}{\partial y}$ represent different things on each side of equality. How do I explain this? I'm guessing it is a notational issue. Edit: Just to give some context why this troubles me. Here $x_i$ refers to the ith component of the vector $\mathbf{x}$ in euclidean space. In an acoustic textbook the Lighthill stress tensor $T_{ij}$ is involved in the following identity: \begin{equation} \frac{\partial}{\partial x_i} \frac{T_{ij}(\mathbf{y},t-|\mathbf{x}-\mathbf{y}|/c)}{|\mathbf{x}-\mathbf{y}|} = \frac{\frac{\partial T_{ij}}{\partial y_i}}{|\mathbf{x}-\mathbf{y}|} - \frac{\partial}{\partial y_i} \frac{T_{ij}(\mathbf{y},t-|\mathbf{x}-\mathbf{y}|/c)}{|\mathbf{x}-\mathbf{y}|}  \end{equation} This can only be resolved if the numerator in the term $\frac{\frac{\partial T_{ij}}{\partial y_i}}{|\mathbf{x}-\mathbf{y}|}$ is given a different interpretation...Just try showing this: Let $t-|\mathbf{x}-\mathbf{y}|/c = \phi(t,\mathbf{x}, \mathbf{y})$ \begin{array}{lcl}  \frac{\partial}{\partial x_i} \frac{T_{ij}(\mathbf{y},\phi)}{|\mathbf{x}-\mathbf{y}|} & = & \frac{1}{|\mathbf{x}-\mathbf{y}|} \frac{\partial}{\partial x_i}T_{ij}(\mathbf{y},\phi) + T_{ij}(\mathbf{y},\phi) \frac{\partial}{\partial x_i} \frac{1}{|\mathbf{x}-\mathbf{y}|}  \\   & = &  \frac{1}{|\mathbf{x}-\mathbf{y}|} (\frac{\partial T_{ij}}{\partial \phi}\frac{\partial \phi}{\partial x_i}) + T_{ij}(\mathbf{y},\phi) \frac{\partial}{\partial x_i} \frac{1}{|\mathbf{x}-\mathbf{y}|}\\  & = &  -\frac{1}{|\mathbf{x}-\mathbf{y}|} (\frac{\partial T_{ij}}{\partial \phi}\frac{\partial \phi}{\partial y_i}) + T_{ij}(\mathbf{y},\phi) \frac{\partial}{\partial x_i} \frac{1}{|\mathbf{x}-\mathbf{y}|}  \end{array} \begin{array}{lcl}  \frac{\partial}{\partial y_i} \frac{T_{ij}(\mathbf{y},\phi)}{|\mathbf{x}-\mathbf{y}|} & = & \frac{1}{|\mathbf{x}-\mathbf{y}|} \frac{\partial}{\partial y_i}T_{ij}(\mathbf{y},\phi) + T_{ij}(\mathbf{y},\phi) \frac{\partial}{\partial y_i} \frac{1}{|\mathbf{x}-\mathbf{y}|}  \\   & = &  \frac{1}{|\mathbf{x}-\mathbf{y}|} ( \frac{\partial}{\partial y_i}T_{ij} +\frac{\partial T_{ij}}{\partial \phi}\frac{\partial \phi}{\partial y_i}) - T_{ij}(\mathbf{y},\phi) \frac{\partial}{\partial x_i} \frac{1}{|\mathbf{x}-\mathbf{y}|}  \end{array} Adding up the last line from each expression gives the result.",,"['multivariable-calculus', 'notation']"
32,Why is differential calculus built on open sets?,Why is differential calculus built on open sets?,,"For example in W. Rudin: Principles of Mathematical Analysis in every theorem or definition regarding the derivatives of a function from $\mathbb{R}^n \to \mathbb{R}^m$ there it always says at the beginning: ""Let $E$ be an open set in $\mathbb{R}^n$, $f$ maps $E$ to $\mathbb{R}^m$ ..."". Why is the differential calculus built on the open subsets of $\mathbb{R}^n$? What is the precise (maybe topological) explanation?","For example in W. Rudin: Principles of Mathematical Analysis in every theorem or definition regarding the derivatives of a function from $\mathbb{R}^n \to \mathbb{R}^m$ there it always says at the beginning: ""Let $E$ be an open set in $\mathbb{R}^n$, $f$ maps $E$ to $\mathbb{R}^m$ ..."". Why is the differential calculus built on the open subsets of $\mathbb{R}^n$? What is the precise (maybe topological) explanation?",,"['multivariable-calculus', 'derivatives']"
33,Matrix Calculus in Least-Square method,Matrix Calculus in Least-Square method,,"In the proof of matrix solution of Least Square Method, I see some matrix calculus, which I have no clue. Can anyone explain to me or recommend me a good link to study this sort of matrix calculus? In Least-Square method, we want to find such a vector $x$ such that $||Ax-b||$ is minimized. Assume $r=Ax-b$ $\Rightarrow\|r\|^2=x^TA^TAx-2b^TAx+b^Tb$ $\Rightarrow \nabla_x \|r\|^2=2A^TAx-2A^Tb$ In the end we set the gradient to zero and find the minimized solution. I understand the whole idea, but I just don't know how exactly we did matrix calculus here, or say I don't know how to do the matrix calculus here. For example, can anyone tell me how we got those transpose in $\|r\|^2$(By what rule?) and how we got the gradient?(how do we take the gradient exactly in matrix format)? I'll really appreciate if you can help me out. Thanks!","In the proof of matrix solution of Least Square Method, I see some matrix calculus, which I have no clue. Can anyone explain to me or recommend me a good link to study this sort of matrix calculus? In Least-Square method, we want to find such a vector $x$ such that $||Ax-b||$ is minimized. Assume $r=Ax-b$ $\Rightarrow\|r\|^2=x^TA^TAx-2b^TAx+b^Tb$ $\Rightarrow \nabla_x \|r\|^2=2A^TAx-2A^Tb$ In the end we set the gradient to zero and find the minimized solution. I understand the whole idea, but I just don't know how exactly we did matrix calculus here, or say I don't know how to do the matrix calculus here. For example, can anyone tell me how we got those transpose in $\|r\|^2$(By what rule?) and how we got the gradient?(how do we take the gradient exactly in matrix format)? I'll really appreciate if you can help me out. Thanks!",,"['multivariable-calculus', 'derivatives', 'matrix-calculus', 'least-squares']"
34,Vector sum in spherical coordinates,Vector sum in spherical coordinates,,"I can't seem to come up with a simple formula to head-tail adding two vectors in spherical coordinates. So I'd like to know: Can anybody point out a way to do it in spherical coordinates (without converting back and forth from cartesian coordinates)? For the sake of execution speed in a computer program, is it faster to do it straight in spherical coordinates or converting back and forth from cartesian coordinates? update : to clarify, I'm not talking about the trivial case in which the tails of the two vectors lay in the same point","I can't seem to come up with a simple formula to head-tail adding two vectors in spherical coordinates. So I'd like to know: Can anybody point out a way to do it in spherical coordinates (without converting back and forth from cartesian coordinates)? For the sake of execution speed in a computer program, is it faster to do it straight in spherical coordinates or converting back and forth from cartesian coordinates? update : to clarify, I'm not talking about the trivial case in which the tails of the two vectors lay in the same point",,"['multivariable-calculus', 'spherical-coordinates']"
35,Entropy of the multivariate Gaussian,Entropy of the multivariate Gaussian,,"Show that the entropy of the multivariate Gaussian $N(x|\mu,\Sigma)$ is given by   \begin{align} H[x] = \frac12\ln|\Sigma| + \frac{D}{2}(1 + \ln(2\pi)) \end{align}   where $D$ is the dimensionality of $x$. My solution. Entropy for normal distribution: \begin{align} H[x] = -\int_{-\infty}^{+\infty}N(x|\mu,\Sigma)\ln(N(x|\mu,\Sigma)) dx = &&\text{by definition of entropy}\\ = -E[\ln(N(x|\mu,\Sigma))] =\\ = -E[\ln((2\pi)^{-\frac{D}{2}} |\Sigma|^{-\frac12} e^{-\frac12(x - \mu)^T\Sigma^{-1}(x - \mu)})] = &&\text{definition of multivariable gaussian}\\ = \frac{D}{2}\ln(2\pi) + \frac12\ln |\Sigma| + \frac12E[(x - \mu)^T\Sigma^{-1}(x - \mu)] &&\text{the log of a product is the sum of the logs}. \end{align} Consider the third term: \begin{align} \frac12E[(x - \mu)^T\Sigma^{-1}(x - \mu)] = \\ = \frac12E[x^T\Sigma^{-1}x - x^T\Sigma^{-1}\mu - \mu^T\Sigma^{-1}x + \mu^T\Sigma^{-1}\mu] = \\ = \frac12E[x^T\Sigma^{-1}x] - \frac12E[2\mu^T\Sigma^{-1}x] + \frac12E[\mu^T\Sigma^{-1}\mu] = \\ = \frac12E[x^T\Sigma^{-1}x] - \mu^T\Sigma^{-1}E[x] + \frac12\mu^T\Sigma^{-1}\mu = \\ = \frac12E[x^T\Sigma^{-1}x] - \mu^T\Sigma^{-1}\mu + \frac12\mu^T\Sigma^{-1}\mu = &&\text{Since $E[x] = \mu$}\\ = \frac12E[x^T\Sigma^{-1}x] - \frac12\mu^T\Sigma^{-1}\mu \end{align} How can I simplify the term: $E[x^T\Sigma^{-1}x]$ ?","Show that the entropy of the multivariate Gaussian $N(x|\mu,\Sigma)$ is given by   \begin{align} H[x] = \frac12\ln|\Sigma| + \frac{D}{2}(1 + \ln(2\pi)) \end{align}   where $D$ is the dimensionality of $x$. My solution. Entropy for normal distribution: \begin{align} H[x] = -\int_{-\infty}^{+\infty}N(x|\mu,\Sigma)\ln(N(x|\mu,\Sigma)) dx = &&\text{by definition of entropy}\\ = -E[\ln(N(x|\mu,\Sigma))] =\\ = -E[\ln((2\pi)^{-\frac{D}{2}} |\Sigma|^{-\frac12} e^{-\frac12(x - \mu)^T\Sigma^{-1}(x - \mu)})] = &&\text{definition of multivariable gaussian}\\ = \frac{D}{2}\ln(2\pi) + \frac12\ln |\Sigma| + \frac12E[(x - \mu)^T\Sigma^{-1}(x - \mu)] &&\text{the log of a product is the sum of the logs}. \end{align} Consider the third term: \begin{align} \frac12E[(x - \mu)^T\Sigma^{-1}(x - \mu)] = \\ = \frac12E[x^T\Sigma^{-1}x - x^T\Sigma^{-1}\mu - \mu^T\Sigma^{-1}x + \mu^T\Sigma^{-1}\mu] = \\ = \frac12E[x^T\Sigma^{-1}x] - \frac12E[2\mu^T\Sigma^{-1}x] + \frac12E[\mu^T\Sigma^{-1}\mu] = \\ = \frac12E[x^T\Sigma^{-1}x] - \mu^T\Sigma^{-1}E[x] + \frac12\mu^T\Sigma^{-1}\mu = \\ = \frac12E[x^T\Sigma^{-1}x] - \mu^T\Sigma^{-1}\mu + \frac12\mu^T\Sigma^{-1}\mu = &&\text{Since $E[x] = \mu$}\\ = \frac12E[x^T\Sigma^{-1}x] - \frac12\mu^T\Sigma^{-1}\mu \end{align} How can I simplify the term: $E[x^T\Sigma^{-1}x]$ ?",,"['multivariable-calculus', 'entropy', 'gaussian-integral']"
36,Geometric interpretation of duality and Slater's condition,Geometric interpretation of duality and Slater's condition,,"I am trying to study about optimization problems, Lagrange duality and related topics. I came across some presentation on the net, which claims to show the geometric interpretation of the duality and Slater's condition for a simple problem with only a single constraint : $$\begin{equation*} \begin{aligned} & \underset{x}{\text{minimize}} & & f_0(x) \\ & \text{subject to} & & f_1(x) \leq 0. \end{aligned} \end{equation*}$$ Here is the following slide: Now, I understand how $p^*$ is depicted: Primal problem has a constraint $f_1(x) \leq 0$ and we only consider negative $u$ values therefore. The point $(u,t)$ with the minimum $t$ value is picked where $u \leq 0$. But I completely don't understand how I should interpret the dual function $g(\lambda)$ to begin with. $g(\lambda)$ is depicted as a line (hyperplane). But according to the definition of $g(\lambda)$ it must be a scalar value. The dual problem is $g(\lambda) = \inf_x(f_0(x) + \lambda f_1(x))$ where $\lambda \geq 0$. So, for a given $\lambda \geq 0$ we must go and seek a $(u,t)$ point in $G$ which minimizes $g(\lambda)$. How is this connected with a hyperplane to begin with? We are in $(u,t)$ space, which has no $\lambda$ parametrization in it. I direly need some clarifications here.","I am trying to study about optimization problems, Lagrange duality and related topics. I came across some presentation on the net, which claims to show the geometric interpretation of the duality and Slater's condition for a simple problem with only a single constraint : $$\begin{equation*} \begin{aligned} & \underset{x}{\text{minimize}} & & f_0(x) \\ & \text{subject to} & & f_1(x) \leq 0. \end{aligned} \end{equation*}$$ Here is the following slide: Now, I understand how $p^*$ is depicted: Primal problem has a constraint $f_1(x) \leq 0$ and we only consider negative $u$ values therefore. The point $(u,t)$ with the minimum $t$ value is picked where $u \leq 0$. But I completely don't understand how I should interpret the dual function $g(\lambda)$ to begin with. $g(\lambda)$ is depicted as a line (hyperplane). But according to the definition of $g(\lambda)$ it must be a scalar value. The dual problem is $g(\lambda) = \inf_x(f_0(x) + \lambda f_1(x))$ where $\lambda \geq 0$. So, for a given $\lambda \geq 0$ we must go and seek a $(u,t)$ point in $G$ which minimizes $g(\lambda)$. How is this connected with a hyperplane to begin with? We are in $(u,t)$ space, which has no $\lambda$ parametrization in it. I direly need some clarifications here.",,"['multivariable-calculus', 'optimization', 'convex-optimization', 'lagrange-multiplier']"
37,Any diffeomorphism can be locally factorised with several primitive diffeomorphisms.,Any diffeomorphism can be locally factorised with several primitive diffeomorphisms.,,"I ask in this question how to solve an exercise of the text Analysis on Manifolds by James Munkres: the exercise consist to prove a theorem about diffeomorphism with more restrictive conditions. Since the solution I gave is similar to the proof of the theorem that Munkres gave for sake of completenes I put it here but if you want you can directely read my solution and when I refer to Munkres's proof you can see the step I use because I indicate it explicitely In text Analysis on Manifolds by James Munkres there is the following definition. So Munkres states that any diffeomorphism can be locally factorised with several primitive diffeomorphims so that he claims that the following theorem holds. Well Munkres prove it in four sep. So in the first step he proves that any linear transfomation is a diffeomorphism that could be factorised locally with severl primitive diffeomorphisms as you can check (if you like) to follow. Then in the second step he proves that any traslation is a diffeomorphism that could be factorised locally with two primitive diffeomorphisms as you can check (if you like) to follow. Then in the third step he proves that the theorem holds in the case where $g(0)=0$ and $Dg(0)=I_n$ as you can check (if you like) to follow. Finally in the four step he proves the general case using the preceding results. However then as exercise Munkres ask to show that the preceding theorem holds with more restrictive definition of diffeomorphism that is he ask to prove the following statement. So unfortunately I don't be able to use the Munkres's hit (are you able to do it?) but it seems I found the following alternative solution. First of all we observe that any linear transmormation could be factorised with serveral primitive diffeomorphisms: indeed as you can check the elementary row operation of type $2$ and $3$ (for datail see the step $1$ of the preceding theorem) modify only the $i$ -th row of a matirx and the elements of the $i$ -th row of a matrix are the $i$ -th components of the image of the vector of a base. Then if $t$ is a traslation of length and direction $\pmb c:=(c_1,...,c_n)$ then for any $i=1,...,n$ we can define the primitive diffeomorphism $$ t_i(x):=(x_1,..,x_i+c_i,...,x_n) $$ and observe that $$ t(x)=x+c_1\cdot e_1+c_2\cdot e_2+...+c_n\cdot e_n= \\ =\Biggl(\Big(...\big((x+c_1\cdot e_1)+c_2\cdot e_2\big)+...\Big)+c_n\cdot e_n\Biggl)=\Biggl(\Big(...\big(t_1(x)+c_2\cdot e_2\big)+...\Big)+c_n\cdot e_n\Biggl)=\Biggl(\Big(...\big(t_2(t_1(x))\big)+...\Big)+c_n\cdot e_n\Biggl)= \\ =...=t_n\Big(...\big(t_2(t_1(x)\big)...\Big)=(t_n\,\circ...\circ\,t_1)(x) $$ so that any traslation could be factorised with serveral primitive diffeomorphisms. Now as Munkres did we consider the special case where $a=0$ , $g(0)=0$ and $Dg(0)=I_n$ . So first we define the function $h_1A\rightarrow\Bbb R^n$ through the condition $$ h_1(x):=\big(g_1(x),x_2,...,x_n\big) $$ and we observe that if $g_1(0)=0$ and $Dg_1=e_1$ then $h_1(0)=0$ and $Dh_1(0)=I_n$ and so it follows form the inverse function theorem that $h_1$ is a diffeomorphism ( clearly it is primitive! ) of a open neighborhood $V_0$ of $0$ with an open set $V_1$ that contains $h_1(0)=0$ . So now we define the function $h_2:V_1\rightarrow\Bbb R^n$ through the condition $$ h_2(y):=\Big(y_1,\big(g_2\,\circ\,h_1^{-1}\big)(y),y_2,...,y_n\Big) $$ for any $y\in V_1$ and we observe that if $g_2(0)=0$ and $Dg_2(0)=e_2$ then $g\big(h^{-1}_1(0)\big)=0$ and $Dg_2\big(h^{-1}_1(0)\big)=Dg_2\big(h^{-1}_1(0)\big)\cdot Dh^{-1}_1(0)=Dg_2(0)\cdot\big(Dh_1(0)\big)^{-1}=e_2\cdot I_n=e_2$ and so $h_2(0)=0$ and $D h_2(0)=I_n$ thus it follows form the inverse function theorem that $h_2$ is a diffeomorphism ( clearly it is primitive! ) of a open neighborhood $U_1$ of $0$ with an open set $U_2$ that contains $h_2(0)=0$ . So the sets $$ W_0:=h^{-1}_1[U_1\cap V_1], W_1:=(U_1\cap V_1)\,\,\,\text{and}\,\,\,W_2:=h_2[U_1\cap V_1] $$ are open neighborhoods of $0$ and the function $(h_2|_{W_1}\,\circ\,h_1|_{W_0})$ is a diffeomorphism from $W_0$ to $W_2$ such that $$ (h_2|_{W_1}\,\circ\,h_1|_{W_0})(x):=h_2(h_1(x))=h_2\big((g_1(x),x_2,...,x_n)\big)=\Big(g_1(x),\big(g_2\,\circ\,h^{-1}_1\big)\big((g_1(x),x_2,...,x_n)\big),x_3,...,x_n\Big)=\Big(g_1(x),\big(g_2\,\circ\,h^{-1}_1\big)\big(h_1(x))\big),x_3,...,x_n\Big)=\Big(g_1(x),g_2\big(h^{-1}_1(h_1(x))\big),x_3,...,x_n\Big)=\big(g_1(x),g_2(x),x_3,...,x_n\big) $$ for any $x\in W_0$ . So for $i<n$ we assume that $V_0,V_2,...,V_i$ is a collection of open neighborhoods of $0$ for which exist a sequence of primitive diffeomorphisms $h_1,...,h_i$ there defined such that $$ h_1(0)=h_2(0)=...=h_i(0)=0\,\,\,\text{and}\,\,\,Dh_1(0)=Dh_2(0)=...=Dh_i(0)=I_n $$ and moreover we suppose that $$ \big(h_i\,\circ...\circ\,h_2\,\circ\,h_1\big)(x)=\big(g_1(x),g_2(x),...,g_i(x),x_{i+1},...,x_n\big) $$ for any $x\in V_0$ . So we define the function $h_{i+1}:V_i\rightarrow\Bbb R^n$ through the condition $$ h_{i+1}(y):=\Big(y_1,...,y_i,\big(g_{i+1}\,\circ\,h_1^{-1}\,\circ\,h_2^{-1}\,\circ...\circ\,h_i^{-1}\big)(y),y_{i+2},...,y_n\Big) $$ for any $y\in V_i$ and so we observe that if by assumption $g_{i+1}(0)=0$ , $Dg_{i+1}(0)=e_{i+1}$ and if by the inductive hypothesis $h_1^{-1}(0)=h_2^{-1}(0)=...=h_i^{-1}(0)=0$ and $\big(Dh_1(0)\big)^{-1}=\big(Dh_2(0)\big)^{-1}=...=\big(Dh_i(0)\big)^{-1}=I_n$ then $$ (g_{i+1}\,\circ\,h_1^{-1}\,\circ\,h_2^{-1}\,\circ...\circ\,h_i^{-1}\big)(0)=0 $$ and $$ D\big(g_{i+1}\,\circ\,h_1^{-1}\,\circ\,h_2^{-1}\,\circ...\circ\,h_i^{-1}\big)(0)=Dg_{i+1}(0)\cdot Dh_1^{-1}(0)\cdot Dh_2^{-1}(0)\cdot...\cdot Dh_i^{-1}(0)=e_{i+1}\cdot I_n\cdot I_n\cdot...\cdot I_n=e_{i+1} $$ so that $h_{i+1}(0)=0$ and $Dh_{i+1}(0)=I_n$ and so it follows form the inverse function theorem that $h_{i+1}$ is a diffeomorphism ( clearly it is primitive! ) of a open neighborhood $ U_i$ of $0$ with an open set $U_{i+1}$ that contains $h_{i+1}(0)=0$ . So the sets $$ W_0:=h^{-1}_1\Big[h^{-1}_2\big[...[h^{-1}_i[U_i\cap V_i]...\big]\Big],\,W_1:=h^{-1}_2\big[...[h^{-1}_i[U_i\cap V_i]...\big],\,..., \\ W_{i-1}:=h^{-1}_i[U_i\cap V_i],\,W_i:=U_i\cap V_i\,\,\,\text{and}\,\,\,W_{i+1}:=h_{i+1}[U_i\cap V_i] $$ are open neighborhoods of $0$ and the function $(h_{i+1}|_{W_i}\,\circ\,h_i|_{W_{i-1}}\,\circ\,...\circ\,h_2|_{W_1}\,\circ\,h_1|_{W_0})$ is a diffeomorphism of $W_0$ in $W_{i+1}$ such that $$ \big(h_{i+1}|_{W_i}\,\circ\,h_i|_{W_{i-1}}\,\circ\,...\circ\,h_2|_{W_1}\,\circ\,h_1|_{W_0}\big)(x)=\big(h_{i+1}\,\circ\,h_i\,\circ\,...\circ\,h_2\,\circ\,h_1\big)(x)=\Big(h_{i+1}\,\circ\big(h_i\,\circ\,...\circ\,h_2\,\circ\,h_1\big)\Big)(x)=h_{i+1}\Big(\big(h_i\,\circ\,...\circ\,h_2\,\circ\,h_1\big)(x)\Big)=h_{i+1}\Big(\big(g_1(x),g_2(x),...,g_i(x),x_{i+1},...,x_n\big)\Big)= \\ =\Biggl(g_1(x),g_2(x),...,g_i(x),\Big(g_{i+1}\,\circ\,h_1^{-1}\,\circ\,h_2^{-1}\,\circ...\circ\,h_i^{-1}\Big)\Big(\big(g_1(x),g_2(x),...,g_i(x),x_{i+1},...,x_n\big)\Big),x_{i+2},...,x_n\Biggl)=\Biggl(g_1(x),g_2(x),...,g_i(x),\Big(g_{i+1}\,\circ\,h_1^{-1}\,\circ\,h_2^{-1}\,\circ...\circ\,h_i^{-1}\Big)\Big(\big(h_i\,\circ...\circ\,h_2\,\circ\,h_1\big)(x)\Big),x_{i+2},...,x_n\Biggl)= \\ =\Biggl(g_1(x),g_2(x),...,g_i(x),\Big(g_{i+1}\,\circ\big(h_1^{-1}\,\circ\,h_2^{-1}\,\circ...\circ\,h_i^{-1}\big)\Big)\Big(\big(h_i\,\circ...\circ\,h_2\,\circ\,h_1\big)(x)\Big),x_{i+2},...,x_n\Biggl)= \\ =\Biggl(g_1(x),g_2(x),...,g_i(x),g_{i+1}\biggl(\Big(h_1^{-1}\,\circ\,h_2^{-1}\,\circ...\circ\,h_i^{-1}\Big)\Big(\big(h_i\,\circ...\circ\,h_2\,\circ\,h_1\big)(x)\Big)\biggl),x_{i+2},...,x_n\Biggl)= \\ =\Biggl(g_1(x),g_2(x),...,g_i(x),g_{i+1}\biggl(\Big(\big(h_1^{-1}\,\circ\,h_2^{-1}\,\circ...\circ\,h_i^{-1}\big)\circ\big(h_i\,\circ...\circ\,h_2\,\circ\,h_1\big)\Big)(x)\biggl),x_{i+2},...,x_n\Biggl)= \\ =\Big(g_1(x),g_2(x),...,g_i(x),g_{i+1}(x),x_{i+2},...,x_n\Big) $$ for any $x\in W_0$ . So finally by finite induction theorem we conclude that there exist a sequence $V_0,V_1,...,V_{n-1}$ of length $n$ for which exist a sequence $h_1,...,h_n$ of primitive diffeomorphism there defined,combinable and such that $g|_{V_0}\equiv h_n\,\circ...\circ\,h_2\,\circ\,h_1$ . Finally with the same arguments that Munkres did in the step $4$ we conclude that the theorem holds. So I ask if the solution I gave to exercise is correct and if not I ask to solve it. So could someone help me, please?","I ask in this question how to solve an exercise of the text Analysis on Manifolds by James Munkres: the exercise consist to prove a theorem about diffeomorphism with more restrictive conditions. Since the solution I gave is similar to the proof of the theorem that Munkres gave for sake of completenes I put it here but if you want you can directely read my solution and when I refer to Munkres's proof you can see the step I use because I indicate it explicitely In text Analysis on Manifolds by James Munkres there is the following definition. So Munkres states that any diffeomorphism can be locally factorised with several primitive diffeomorphims so that he claims that the following theorem holds. Well Munkres prove it in four sep. So in the first step he proves that any linear transfomation is a diffeomorphism that could be factorised locally with severl primitive diffeomorphisms as you can check (if you like) to follow. Then in the second step he proves that any traslation is a diffeomorphism that could be factorised locally with two primitive diffeomorphisms as you can check (if you like) to follow. Then in the third step he proves that the theorem holds in the case where and as you can check (if you like) to follow. Finally in the four step he proves the general case using the preceding results. However then as exercise Munkres ask to show that the preceding theorem holds with more restrictive definition of diffeomorphism that is he ask to prove the following statement. So unfortunately I don't be able to use the Munkres's hit (are you able to do it?) but it seems I found the following alternative solution. First of all we observe that any linear transmormation could be factorised with serveral primitive diffeomorphisms: indeed as you can check the elementary row operation of type and (for datail see the step of the preceding theorem) modify only the -th row of a matirx and the elements of the -th row of a matrix are the -th components of the image of the vector of a base. Then if is a traslation of length and direction then for any we can define the primitive diffeomorphism and observe that so that any traslation could be factorised with serveral primitive diffeomorphisms. Now as Munkres did we consider the special case where , and . So first we define the function through the condition and we observe that if and then and and so it follows form the inverse function theorem that is a diffeomorphism ( clearly it is primitive! ) of a open neighborhood of with an open set that contains . So now we define the function through the condition for any and we observe that if and then and and so and thus it follows form the inverse function theorem that is a diffeomorphism ( clearly it is primitive! ) of a open neighborhood of with an open set that contains . So the sets are open neighborhoods of and the function is a diffeomorphism from to such that for any . So for we assume that is a collection of open neighborhoods of for which exist a sequence of primitive diffeomorphisms there defined such that and moreover we suppose that for any . So we define the function through the condition for any and so we observe that if by assumption , and if by the inductive hypothesis and then and so that and and so it follows form the inverse function theorem that is a diffeomorphism ( clearly it is primitive! ) of a open neighborhood of with an open set that contains . So the sets are open neighborhoods of and the function is a diffeomorphism of in such that for any . So finally by finite induction theorem we conclude that there exist a sequence of length for which exist a sequence of primitive diffeomorphism there defined,combinable and such that . Finally with the same arguments that Munkres did in the step we conclude that the theorem holds. So I ask if the solution I gave to exercise is correct and if not I ask to solve it. So could someone help me, please?","g(0)=0 Dg(0)=I_n 2 3 1 i i i t \pmb c:=(c_1,...,c_n) i=1,...,n 
t_i(x):=(x_1,..,x_i+c_i,...,x_n)
 
t(x)=x+c_1\cdot e_1+c_2\cdot e_2+...+c_n\cdot e_n=
\\
=\Biggl(\Big(...\big((x+c_1\cdot e_1)+c_2\cdot e_2\big)+...\Big)+c_n\cdot e_n\Biggl)=\Biggl(\Big(...\big(t_1(x)+c_2\cdot e_2\big)+...\Big)+c_n\cdot e_n\Biggl)=\Biggl(\Big(...\big(t_2(t_1(x))\big)+...\Big)+c_n\cdot e_n\Biggl)=
\\
=...=t_n\Big(...\big(t_2(t_1(x)\big)...\Big)=(t_n\,\circ...\circ\,t_1)(x)
 a=0 g(0)=0 Dg(0)=I_n h_1A\rightarrow\Bbb R^n 
h_1(x):=\big(g_1(x),x_2,...,x_n\big)
 g_1(0)=0 Dg_1=e_1 h_1(0)=0 Dh_1(0)=I_n h_1 V_0 0 V_1 h_1(0)=0 h_2:V_1\rightarrow\Bbb R^n 
h_2(y):=\Big(y_1,\big(g_2\,\circ\,h_1^{-1}\big)(y),y_2,...,y_n\Big)
 y\in V_1 g_2(0)=0 Dg_2(0)=e_2 g\big(h^{-1}_1(0)\big)=0 Dg_2\big(h^{-1}_1(0)\big)=Dg_2\big(h^{-1}_1(0)\big)\cdot Dh^{-1}_1(0)=Dg_2(0)\cdot\big(Dh_1(0)\big)^{-1}=e_2\cdot I_n=e_2 h_2(0)=0 D h_2(0)=I_n h_2 U_1 0 U_2 h_2(0)=0 
W_0:=h^{-1}_1[U_1\cap V_1], W_1:=(U_1\cap V_1)\,\,\,\text{and}\,\,\,W_2:=h_2[U_1\cap V_1]
 0 (h_2|_{W_1}\,\circ\,h_1|_{W_0}) W_0 W_2 
(h_2|_{W_1}\,\circ\,h_1|_{W_0})(x):=h_2(h_1(x))=h_2\big((g_1(x),x_2,...,x_n)\big)=\Big(g_1(x),\big(g_2\,\circ\,h^{-1}_1\big)\big((g_1(x),x_2,...,x_n)\big),x_3,...,x_n\Big)=\Big(g_1(x),\big(g_2\,\circ\,h^{-1}_1\big)\big(h_1(x))\big),x_3,...,x_n\Big)=\Big(g_1(x),g_2\big(h^{-1}_1(h_1(x))\big),x_3,...,x_n\Big)=\big(g_1(x),g_2(x),x_3,...,x_n\big)
 x\in W_0 i<n V_0,V_2,...,V_i 0 h_1,...,h_i 
h_1(0)=h_2(0)=...=h_i(0)=0\,\,\,\text{and}\,\,\,Dh_1(0)=Dh_2(0)=...=Dh_i(0)=I_n
 
\big(h_i\,\circ...\circ\,h_2\,\circ\,h_1\big)(x)=\big(g_1(x),g_2(x),...,g_i(x),x_{i+1},...,x_n\big)
 x\in V_0 h_{i+1}:V_i\rightarrow\Bbb R^n 
h_{i+1}(y):=\Big(y_1,...,y_i,\big(g_{i+1}\,\circ\,h_1^{-1}\,\circ\,h_2^{-1}\,\circ...\circ\,h_i^{-1}\big)(y),y_{i+2},...,y_n\Big)
 y\in V_i g_{i+1}(0)=0 Dg_{i+1}(0)=e_{i+1} h_1^{-1}(0)=h_2^{-1}(0)=...=h_i^{-1}(0)=0 \big(Dh_1(0)\big)^{-1}=\big(Dh_2(0)\big)^{-1}=...=\big(Dh_i(0)\big)^{-1}=I_n 
(g_{i+1}\,\circ\,h_1^{-1}\,\circ\,h_2^{-1}\,\circ...\circ\,h_i^{-1}\big)(0)=0
 
D\big(g_{i+1}\,\circ\,h_1^{-1}\,\circ\,h_2^{-1}\,\circ...\circ\,h_i^{-1}\big)(0)=Dg_{i+1}(0)\cdot Dh_1^{-1}(0)\cdot Dh_2^{-1}(0)\cdot...\cdot Dh_i^{-1}(0)=e_{i+1}\cdot I_n\cdot I_n\cdot...\cdot I_n=e_{i+1}
 h_{i+1}(0)=0 Dh_{i+1}(0)=I_n h_{i+1} 
U_i 0 U_{i+1} h_{i+1}(0)=0 
W_0:=h^{-1}_1\Big[h^{-1}_2\big[...[h^{-1}_i[U_i\cap V_i]...\big]\Big],\,W_1:=h^{-1}_2\big[...[h^{-1}_i[U_i\cap V_i]...\big],\,...,
\\
W_{i-1}:=h^{-1}_i[U_i\cap V_i],\,W_i:=U_i\cap V_i\,\,\,\text{and}\,\,\,W_{i+1}:=h_{i+1}[U_i\cap V_i]
 0 (h_{i+1}|_{W_i}\,\circ\,h_i|_{W_{i-1}}\,\circ\,...\circ\,h_2|_{W_1}\,\circ\,h_1|_{W_0}) W_0 W_{i+1} 
\big(h_{i+1}|_{W_i}\,\circ\,h_i|_{W_{i-1}}\,\circ\,...\circ\,h_2|_{W_1}\,\circ\,h_1|_{W_0}\big)(x)=\big(h_{i+1}\,\circ\,h_i\,\circ\,...\circ\,h_2\,\circ\,h_1\big)(x)=\Big(h_{i+1}\,\circ\big(h_i\,\circ\,...\circ\,h_2\,\circ\,h_1\big)\Big)(x)=h_{i+1}\Big(\big(h_i\,\circ\,...\circ\,h_2\,\circ\,h_1\big)(x)\Big)=h_{i+1}\Big(\big(g_1(x),g_2(x),...,g_i(x),x_{i+1},...,x_n\big)\Big)=
\\
=\Biggl(g_1(x),g_2(x),...,g_i(x),\Big(g_{i+1}\,\circ\,h_1^{-1}\,\circ\,h_2^{-1}\,\circ...\circ\,h_i^{-1}\Big)\Big(\big(g_1(x),g_2(x),...,g_i(x),x_{i+1},...,x_n\big)\Big),x_{i+2},...,x_n\Biggl)=\Biggl(g_1(x),g_2(x),...,g_i(x),\Big(g_{i+1}\,\circ\,h_1^{-1}\,\circ\,h_2^{-1}\,\circ...\circ\,h_i^{-1}\Big)\Big(\big(h_i\,\circ...\circ\,h_2\,\circ\,h_1\big)(x)\Big),x_{i+2},...,x_n\Biggl)=
\\
=\Biggl(g_1(x),g_2(x),...,g_i(x),\Big(g_{i+1}\,\circ\big(h_1^{-1}\,\circ\,h_2^{-1}\,\circ...\circ\,h_i^{-1}\big)\Big)\Big(\big(h_i\,\circ...\circ\,h_2\,\circ\,h_1\big)(x)\Big),x_{i+2},...,x_n\Biggl)=
\\
=\Biggl(g_1(x),g_2(x),...,g_i(x),g_{i+1}\biggl(\Big(h_1^{-1}\,\circ\,h_2^{-1}\,\circ...\circ\,h_i^{-1}\Big)\Big(\big(h_i\,\circ...\circ\,h_2\,\circ\,h_1\big)(x)\Big)\biggl),x_{i+2},...,x_n\Biggl)=
\\
=\Biggl(g_1(x),g_2(x),...,g_i(x),g_{i+1}\biggl(\Big(\big(h_1^{-1}\,\circ\,h_2^{-1}\,\circ...\circ\,h_i^{-1}\big)\circ\big(h_i\,\circ...\circ\,h_2\,\circ\,h_1\big)\Big)(x)\biggl),x_{i+2},...,x_n\Biggl)=
\\
=\Big(g_1(x),g_2(x),...,g_i(x),g_{i+1}(x),x_{i+2},...,x_n\Big)
 x\in W_0 V_0,V_1,...,V_{n-1} n h_1,...,h_n g|_{V_0}\equiv h_n\,\circ...\circ\,h_2\,\circ\,h_1 4","['multivariable-calculus', 'differential-geometry', 'solution-verification', 'alternative-proof', 'diffeomorphism']"
38,Why is gradient in the direction of ascent but not descent?,Why is gradient in the direction of ascent but not descent?,,"I understand that differentiation of a function ($\mathbb{R} \rightarrow \mathbb{R} $) at a point is the rate of change in the output for a slight nudge in the input. And this rate of change could be negative or positive. There is no concept of direction for the single-variable function as obvious. Now, my doubt is in the case of the multivariate function ($\mathbb{R}^n \rightarrow \mathbb{R}$) where differentiation is a gradient. And this gradient representing partial differentiation w.r.t. to each basis becomes a direction. This direction is a direction of ascent but not descent, why?. Why it is a direction is of ascent. My question is not at all related to steepest ascent , about which one can find many answers on this forum and read elaborately at this link . An intuitive explanation would be preferable than mathematical at this link .","I understand that differentiation of a function ($\mathbb{R} \rightarrow \mathbb{R} $) at a point is the rate of change in the output for a slight nudge in the input. And this rate of change could be negative or positive. There is no concept of direction for the single-variable function as obvious. Now, my doubt is in the case of the multivariate function ($\mathbb{R}^n \rightarrow \mathbb{R}$) where differentiation is a gradient. And this gradient representing partial differentiation w.r.t. to each basis becomes a direction. This direction is a direction of ascent but not descent, why?. Why it is a direction is of ascent. My question is not at all related to steepest ascent , about which one can find many answers on this forum and read elaborately at this link . An intuitive explanation would be preferable than mathematical at this link .",,"['multivariable-calculus', 'vector-analysis', 'intuition']"
39,Gradient is NOT the direction that points to the minimum or maximum,Gradient is NOT the direction that points to the minimum or maximum,,"I understand that the gradient is the direction of steepest descent (ref: Why is gradient the direction of steepest ascent? and Gradient of a function as the direction of steepest ascent/descent ). However, I am not able to visualize it. The Blue arrow is the one pointing towards minima/maxima. The gradient  ( black arrow ) is not and that's why we have this zig-zag motion. Then how come gradient is the direction of steepest descent/ascent? I have a related question: Why does gradient ascent/descent have zig-zag motion?","I understand that the gradient is the direction of steepest descent (ref: Why is gradient the direction of steepest ascent? and Gradient of a function as the direction of steepest ascent/descent ). However, I am not able to visualize it. The Blue arrow is the one pointing towards minima/maxima. The gradient  ( black arrow ) is not and that's why we have this zig-zag motion. Then how come gradient is the direction of steepest descent/ascent? I have a related question: Why does gradient ascent/descent have zig-zag motion?",,"['multivariable-calculus', 'optimization', 'convex-optimization', 'visualization', 'gradient-descent']"
40,What is the difference between $\frac{\mathrm{d}}{\mathrm{d}x}$ and $\frac{\partial}{\partial x}$?,What is the difference between  and ?,\frac{\mathrm{d}}{\mathrm{d}x} \frac{\partial}{\partial x},"Is there not any difference between $\frac{\mathrm{d}}{\mathrm{d}x}$ and $\frac{\partial}{\partial x}$ as long as your function has one variable? $f(x) = x^3\implies \left\{\begin{align}&\dfrac{\mathrm{d}}{\mathrm{d}x}f = \dfrac{\mathrm{d}f}{\mathrm{d}x}=\dfrac{\mathrm{d}(x\mapsto x^3)}{\mathrm{d}x} = x\mapsto 3x^2&\color{green}{\checkmark}\\&\dfrac{\partial}{\partial x}f = \dfrac{\partial f}{\partial x}= \dfrac{\partial(x\mapsto x^3)}{\partial x} = x\mapsto 3x^2&\color{green}{\checkmark}\end{align}\right.$ And if so, why does this change with two (or more) variables? $\require{cancel} f(x,y) = yx^3\implies \left\{\begin{align}&\color{grey}{\cancel{\dfrac{\mathrm{d}}{\mathrm{d}x}f = }} \dfrac{\mathrm{d}f}{\mathrm{d}x}=\dfrac{\mathrm{d}((x,y)\mapsto yx^3)}{\mathrm{d}x} \neq x\mapsto 3yx^2&\color{green}{\checkmark}\\&\dfrac{\partial}{\partial x}f = \dfrac{\partial f}{\partial x} =\dfrac{\partial((x,y)\mapsto x^3)}{\partial x} = x\mapsto 3yx^2&\color{red}{\mathcal{X}}\end{align}\right.$ I get that it is supposed to be something like this $f(x,y) = yx^3\implies \left\{\begin{align}&\color{grey}{\cancel{\dfrac{\mathrm{d}}{\mathrm{d}x}f = }} \dfrac{\mathrm{d}f}{\mathrm{d}x}=\dfrac{\mathrm{d}((x,y)\mapsto yx^3)}{\mathrm{d}x} \neq\\&\cdots\quad (x,y)\mapsto 3y\dfrac{\mathrm{d}\color{red}{(x\mapsto x^3)}}{\mathrm{d}x}+\dfrac{\mathrm{d}\color{red}{(y\mapsto y)}}{\mathrm{d}x}x^3 =\\&\cdots\quad (x,y)\mapsto 3y\color{red}{(x\mapsto x^2)}+\dfrac{\mathrm{d}\color{red}{(y\mapsto y)}}{\mathrm{d}x}x^3&\color{green}{\checkmark}\\&\dfrac{\partial}{\partial x}f = \dfrac{\partial f}{\partial x} = \dfrac{\partial(x,y)\mapsto x^3)}{\partial x} = x\mapsto 3yx^2&\color{green}{\checkmark}\end{align}\right.$","Is there not any difference between $\frac{\mathrm{d}}{\mathrm{d}x}$ and $\frac{\partial}{\partial x}$ as long as your function has one variable? $f(x) = x^3\implies \left\{\begin{align}&\dfrac{\mathrm{d}}{\mathrm{d}x}f = \dfrac{\mathrm{d}f}{\mathrm{d}x}=\dfrac{\mathrm{d}(x\mapsto x^3)}{\mathrm{d}x} = x\mapsto 3x^2&\color{green}{\checkmark}\\&\dfrac{\partial}{\partial x}f = \dfrac{\partial f}{\partial x}= \dfrac{\partial(x\mapsto x^3)}{\partial x} = x\mapsto 3x^2&\color{green}{\checkmark}\end{align}\right.$ And if so, why does this change with two (or more) variables? $\require{cancel} f(x,y) = yx^3\implies \left\{\begin{align}&\color{grey}{\cancel{\dfrac{\mathrm{d}}{\mathrm{d}x}f = }} \dfrac{\mathrm{d}f}{\mathrm{d}x}=\dfrac{\mathrm{d}((x,y)\mapsto yx^3)}{\mathrm{d}x} \neq x\mapsto 3yx^2&\color{green}{\checkmark}\\&\dfrac{\partial}{\partial x}f = \dfrac{\partial f}{\partial x} =\dfrac{\partial((x,y)\mapsto x^3)}{\partial x} = x\mapsto 3yx^2&\color{red}{\mathcal{X}}\end{align}\right.$ I get that it is supposed to be something like this $f(x,y) = yx^3\implies \left\{\begin{align}&\color{grey}{\cancel{\dfrac{\mathrm{d}}{\mathrm{d}x}f = }} \dfrac{\mathrm{d}f}{\mathrm{d}x}=\dfrac{\mathrm{d}((x,y)\mapsto yx^3)}{\mathrm{d}x} \neq\\&\cdots\quad (x,y)\mapsto 3y\dfrac{\mathrm{d}\color{red}{(x\mapsto x^3)}}{\mathrm{d}x}+\dfrac{\mathrm{d}\color{red}{(y\mapsto y)}}{\mathrm{d}x}x^3 =\\&\cdots\quad (x,y)\mapsto 3y\color{red}{(x\mapsto x^2)}+\dfrac{\mathrm{d}\color{red}{(y\mapsto y)}}{\mathrm{d}x}x^3&\color{green}{\checkmark}\\&\dfrac{\partial}{\partial x}f = \dfrac{\partial f}{\partial x} = \dfrac{\partial(x,y)\mapsto x^3)}{\partial x} = x\mapsto 3yx^2&\color{green}{\checkmark}\end{align}\right.$",,"['multivariable-calculus', 'derivatives', 'notation', 'partial-derivative']"
41,The meaning of $\lambda$ in Lagrange Multipliers,The meaning of  in Lagrange Multipliers,\lambda,"This is related to two previous questions which I asked about the history of Lagrange Multipliers and intuition behind the gradient giving the direction of steepest ascent. I am wondering if the constant $\lambda$ in the Lagrange equation $$\nabla f=\lambda \nabla g$$ has any significance. For instance, can the sign of $\lambda$ tell us anything about the nature of the critical point? Does its magnitude have any significance?","This is related to two previous questions which I asked about the history of Lagrange Multipliers and intuition behind the gradient giving the direction of steepest ascent. I am wondering if the constant $\lambda$ in the Lagrange equation $$\nabla f=\lambda \nabla g$$ has any significance. For instance, can the sign of $\lambda$ tell us anything about the nature of the critical point? Does its magnitude have any significance?",,['multivariable-calculus']
42,Why can't we usually speak of partial derivatives if the domain is not open?,Why can't we usually speak of partial derivatives if the domain is not open?,,"In the book by Guillemin & Pollack they define a function $f$ from an open subset $U\subset R^n$ to $R^m$ to be smooth if it has continuous partial derivatives of all orders. Then they say ""However, when the domain of $f$ is not open, one usually cannot speak of partial derivatives. (Why?)"" Can someone explain this a little bit more?","In the book by Guillemin & Pollack they define a function $f$ from an open subset $U\subset R^n$ to $R^m$ to be smooth if it has continuous partial derivatives of all orders. Then they say ""However, when the domain of $f$ is not open, one usually cannot speak of partial derivatives. (Why?)"" Can someone explain this a little bit more?",,"['multivariable-calculus', 'differential-geometry', 'differential-topology']"
43,"Are ""differential forms"" an algebraic approach to multivariable calculus?","Are ""differential forms"" an algebraic approach to multivariable calculus?",,"I am recently learning some basic differential geometry. As I understand, differential forms provide a neat way to deal with the topics in calculus such as Stoke's theorem. In order to define the differential forms, one needs to learn the basic concepts in multi-linear algebra. Here are my questions : Are ""differential forms"" basically an algebraic approach to (multivariable) calculus? If the answer is YES, what would be a analysis counterpart and what are the advantages and disadvantages of these two approaches? If the answer is NO, how should I understand it properly? (EDITED: Are differential forms the only approach to multivariable analysis calculus?) MOTIVATION: When I look for a rigorous theoretical approach to multivariable calculus, such as I asked in this question , differential forms are almost always included in the books recommended. I am thus wondering if this is the only approach to multivariable calculus. I may not being asking a good question. Any suggestion for improving the questions above will be really appreciated.","I am recently learning some basic differential geometry. As I understand, differential forms provide a neat way to deal with the topics in calculus such as Stoke's theorem. In order to define the differential forms, one needs to learn the basic concepts in multi-linear algebra. Here are my questions : Are ""differential forms"" basically an algebraic approach to (multivariable) calculus? If the answer is YES, what would be a analysis counterpart and what are the advantages and disadvantages of these two approaches? If the answer is NO, how should I understand it properly? (EDITED: Are differential forms the only approach to multivariable analysis calculus?) MOTIVATION: When I look for a rigorous theoretical approach to multivariable calculus, such as I asked in this question , differential forms are almost always included in the books recommended. I am thus wondering if this is the only approach to multivariable calculus. I may not being asking a good question. Any suggestion for improving the questions above will be really appreciated.",,['differential-geometry']
44,Curl of a vector in spherical coordinates,Curl of a vector in spherical coordinates,,"The curl of a Vector function in curvilinear coordinate system is given by  $$ \nabla \times A =  \frac 1 {h_1 h_2 h_3} \begin{vmatrix} h_1 \hat e_1 & h_2 \hat e_2 & h_3 \hat e_3\\  \partial \over \partial x_1 & \partial \over \partial x_2 & \partial \over \partial x_3\\  h_1 A_1 & h_2 A_2 & h_3 A_3  \end{vmatrix} \hspace{20 mm} \mathbf{(1)}$$ where $h_1, h_2, h_3$ are scale factors. For spherical coordinates  $$h_1 = 1, h_2 = r, h_3 = r\sin\theta$$ However I don't understand (1), which is also not explained in my book. How is it derived?? Can anyone explain me? Even links would be helpful. Thank you!!","The curl of a Vector function in curvilinear coordinate system is given by  $$ \nabla \times A =  \frac 1 {h_1 h_2 h_3} \begin{vmatrix} h_1 \hat e_1 & h_2 \hat e_2 & h_3 \hat e_3\\  \partial \over \partial x_1 & \partial \over \partial x_2 & \partial \over \partial x_3\\  h_1 A_1 & h_2 A_2 & h_3 A_3  \end{vmatrix} \hspace{20 mm} \mathbf{(1)}$$ where $h_1, h_2, h_3$ are scale factors. For spherical coordinates  $$h_1 = 1, h_2 = r, h_3 = r\sin\theta$$ However I don't understand (1), which is also not explained in my book. How is it derived?? Can anyone explain me? Even links would be helpful. Thank you!!",,['multivariable-calculus']
45,Does strictly convex imply invertible gradient?,Does strictly convex imply invertible gradient?,,"If $f:\mathbb R^n \to \mathbb R$ is strictly convex and continuously differentiable, does this imply that $\nabla f$ is a one-to-one mapping? To be precise, can we say that $x, y \in \mathbb R^n$ and $\nabla f(x) = \nabla f(y)$ implies $x = y$ ?","If is strictly convex and continuously differentiable, does this imply that is a one-to-one mapping? To be precise, can we say that and implies ?","f:\mathbb R^n \to \mathbb R \nabla f x, y \in \mathbb R^n \nabla f(x) = \nabla f(y) x = y","['multivariable-calculus', 'convex-analysis', 'inverse', 'inverse-function', 'scalar-fields']"
46,How would one arrive at the formulas for divergence and curl?,How would one arrive at the formulas for divergence and curl?,,"It has been some years since I've taken multivariable calculus now, but there's something I really never understood: how people would discover the expressions for divergence and curl. I mean, the books usually say the formulas and then show that with that it's possible to view divergence as a measure of how much a vector field diverges locally and curl the analog for rotation locally. Now, it's not clear that if you pick those expressions it will give this interpretation. Books usually say: ""we take those formulas because they work"" and well, I know that. What I want to know is: imagining we want to find two operators $\operatorname{div}$ and $\operatorname{curl}$ on vector fields such that $\operatorname{div}$ gives local divergence and $\operatorname{curl}$ gives local rotation, how could we deduce the definitions that would work? I'm questioning this because currently I'm studying differential forms on manifolds, and to appreciate the definition of exterior derivative I thought it would be good to go back and see where the definitions of divergence and curl come from. Based then on the exterior derivative, I've found out that if $v\in \mathfrak{X}(\mathbb{R}^3)$ is a vector field and we consider the usual cartesian coordinates in $\mathbb{R}^3$ then $$\nabla \times v = \sum_{i=1}^3 \nabla v^i \times \dfrac{\partial}{\partial x^i} \qquad \nabla\cdot v = \sum_{i=1}^3 \nabla v^i \cdot \dfrac{\partial}{\partial x^i}$$ I then started to try seeing if these formulas were any easier to find out, but I couldn't get anythin from it. Thanks very much in advance.","It has been some years since I've taken multivariable calculus now, but there's something I really never understood: how people would discover the expressions for divergence and curl. I mean, the books usually say the formulas and then show that with that it's possible to view divergence as a measure of how much a vector field diverges locally and curl the analog for rotation locally. Now, it's not clear that if you pick those expressions it will give this interpretation. Books usually say: ""we take those formulas because they work"" and well, I know that. What I want to know is: imagining we want to find two operators $\operatorname{div}$ and $\operatorname{curl}$ on vector fields such that $\operatorname{div}$ gives local divergence and $\operatorname{curl}$ gives local rotation, how could we deduce the definitions that would work? I'm questioning this because currently I'm studying differential forms on manifolds, and to appreciate the definition of exterior derivative I thought it would be good to go back and see where the definitions of divergence and curl come from. Based then on the exterior derivative, I've found out that if $v\in \mathfrak{X}(\mathbb{R}^3)$ is a vector field and we consider the usual cartesian coordinates in $\mathbb{R}^3$ then $$\nabla \times v = \sum_{i=1}^3 \nabla v^i \times \dfrac{\partial}{\partial x^i} \qquad \nabla\cdot v = \sum_{i=1}^3 \nabla v^i \cdot \dfrac{\partial}{\partial x^i}$$ I then started to try seeing if these formulas were any easier to find out, but I couldn't get anythin from it. Thanks very much in advance.",,"['multivariable-calculus', 'soft-question', 'intuition', 'vector-analysis']"
47,The proof of the Helmholtz decomposition theorem through Neumann boundary value problem,The proof of the Helmholtz decomposition theorem through Neumann boundary value problem,,"As you know, the Helmholtz decomposition theorem is as follows. Let $\Omega$ be open and simply connected, and bounded region in $\mathbb R^3$.    Then the smooth vector field $E : \Omega \to \mathbb R^3$ can be expressed by   $\operatorname{grad}(f)+\operatorname{curl}(A)$ for some $f(\text{scalar field})$, $A(\text{vector field})$. I found the following proof. Text: Mathematical analysis-Several variables(Springer) Proof. In fact, let $u\colon\Omega\to\mathbb R$ be the solution of the Neumann problem $$\Delta u = \operatorname{div} E\text{ in }Ω\quad \text{and}\quad \frac{\partial u}{ \partial n}= E\cdot n\text{ on }\partial\Omega,$$ then $\operatorname{div} (E − \nabla u) = 0$ in $\Omega$, which one can prove to be of class $C^\infty(\Omega)$.  There is a field $H\colon \Omega\subset \mathbb R^3\to\mathbb  R^3$ of class $C^\infty(\Omega)$ such that $E −\nabla u = \operatorname{curl}H$. But, I can't prove the existence of the solution of that PDE (Neumann problem PDE). If we prove that existence, the proof of the HDD is so simple. Does that PDE have a scalar field solution?","As you know, the Helmholtz decomposition theorem is as follows. Let $\Omega$ be open and simply connected, and bounded region in $\mathbb R^3$.    Then the smooth vector field $E : \Omega \to \mathbb R^3$ can be expressed by   $\operatorname{grad}(f)+\operatorname{curl}(A)$ for some $f(\text{scalar field})$, $A(\text{vector field})$. I found the following proof. Text: Mathematical analysis-Several variables(Springer) Proof. In fact, let $u\colon\Omega\to\mathbb R$ be the solution of the Neumann problem $$\Delta u = \operatorname{div} E\text{ in }Ω\quad \text{and}\quad \frac{\partial u}{ \partial n}= E\cdot n\text{ on }\partial\Omega,$$ then $\operatorname{div} (E − \nabla u) = 0$ in $\Omega$, which one can prove to be of class $C^\infty(\Omega)$.  There is a field $H\colon \Omega\subset \mathbb R^3\to\mathbb  R^3$ of class $C^\infty(\Omega)$ such that $E −\nabla u = \operatorname{curl}H$. But, I can't prove the existence of the solution of that PDE (Neumann problem PDE). If we prove that existence, the proof of the HDD is so simple. Does that PDE have a scalar field solution?",,"['multivariable-calculus', 'partial-differential-equations', 'vector-analysis']"
48,Equivalent condition for differentiability on partial derivatives,Equivalent condition for differentiability on partial derivatives,,"I want to extend the concept of derivative of a real function of real variable to a function $f:A\subset \mathbb{R}^n \to \mathbb{R}^m$ with $A$ open. If $x_0 \in A$ then I say that $f$ has derivative $f'(x_0) \in \operatorname{Hom}(\mathbb{R}^n, \mathbb{R}^m)$ if  $$ \lim_{h\to 0} \frac{|f(x_0+h)-f(x_0)-f'(x_0)(h)|}{|h|}=0 .$$  When the function is given in explicit algebraic form as $f(x_1,\dots,x_n) = \sum \limits_{i=1}^m f_{i}(x_1,\dots,x_n)e_i$ and I know that the derivative exists at $x_0\in A$, then I can compute $f'(x_0)$ explicitly because  $$ [f'(x_0)]_{ij} = (D_jf_i)(x_0) $$  is the matrix representation of $f'(x_0)$ relative to standard bases and I know from basic calculus how to compute those partial derivatives. My question is: if I can compute partial derivatives in $x_0$ without knowing if $f'(x_0)$ exists, is there some regularity condition on partial derivatives that is equivalent to the existence of $f'(x_0)$? The existence of partial derivatives isn't sufficient (for $n>1$), for example $\frac{xy}{x^2+y^2}$ has partial derivatives but isn't continuous in $(0,0)$ if is defined $0$ in $(0,0)$ and thus can't be differentiable there. Coming back to the general problem, if partial derivatives exist and are bounded in a neighborhood of $x_0$, then $f$ is continuous in $x_0$ but I believe it could be not differentiable, although I can't write a counterexample. If the partial derivatives are continuous in $x_0$ then $f'(x)$ should exist in a neighborhood of $x_0$ and should be continuous in $x_0$, but this is clearly more than differentiability in $x_0$. NEWS Differentiability seems to be a slippery regularity . I discovered from Rudin ""Principles of Mathematical Analysis"" that an equivalence exists for continuously differentiable functions, in fact $f\in C^1(A)$ if and only if $D_jf_i\in C^1(A)$ for all $i,j$. The question seems difficult because of the fact that the regularity of a multivariable function and that of its partial derivatives seem to have a weak connection, although a general heuristic principle for this type of problems could be: a stronger regularity on partial derivatives implies a weaker regularity on the function .","I want to extend the concept of derivative of a real function of real variable to a function $f:A\subset \mathbb{R}^n \to \mathbb{R}^m$ with $A$ open. If $x_0 \in A$ then I say that $f$ has derivative $f'(x_0) \in \operatorname{Hom}(\mathbb{R}^n, \mathbb{R}^m)$ if  $$ \lim_{h\to 0} \frac{|f(x_0+h)-f(x_0)-f'(x_0)(h)|}{|h|}=0 .$$  When the function is given in explicit algebraic form as $f(x_1,\dots,x_n) = \sum \limits_{i=1}^m f_{i}(x_1,\dots,x_n)e_i$ and I know that the derivative exists at $x_0\in A$, then I can compute $f'(x_0)$ explicitly because  $$ [f'(x_0)]_{ij} = (D_jf_i)(x_0) $$  is the matrix representation of $f'(x_0)$ relative to standard bases and I know from basic calculus how to compute those partial derivatives. My question is: if I can compute partial derivatives in $x_0$ without knowing if $f'(x_0)$ exists, is there some regularity condition on partial derivatives that is equivalent to the existence of $f'(x_0)$? The existence of partial derivatives isn't sufficient (for $n>1$), for example $\frac{xy}{x^2+y^2}$ has partial derivatives but isn't continuous in $(0,0)$ if is defined $0$ in $(0,0)$ and thus can't be differentiable there. Coming back to the general problem, if partial derivatives exist and are bounded in a neighborhood of $x_0$, then $f$ is continuous in $x_0$ but I believe it could be not differentiable, although I can't write a counterexample. If the partial derivatives are continuous in $x_0$ then $f'(x)$ should exist in a neighborhood of $x_0$ and should be continuous in $x_0$, but this is clearly more than differentiability in $x_0$. NEWS Differentiability seems to be a slippery regularity . I discovered from Rudin ""Principles of Mathematical Analysis"" that an equivalence exists for continuously differentiable functions, in fact $f\in C^1(A)$ if and only if $D_jf_i\in C^1(A)$ for all $i,j$. The question seems difficult because of the fact that the regularity of a multivariable function and that of its partial derivatives seem to have a weak connection, although a general heuristic principle for this type of problems could be: a stronger regularity on partial derivatives implies a weaker regularity on the function .",,[]
49,(Non-)Conservative Vector Fields,(Non-)Conservative Vector Fields,,"As anyone who has taken vector calculus (read: most of you) knows, if a vector field is conservative, then it is the gradient of a potential function. In other words, if the vector field is two dimensional, the potential function is a surface and as such some things become really nice, like the line integral of a closed curve being zero. However, there are some very simple vector fields that are not conservative, which leads me to wonder what the corresponding surface would look like and in particular what characteristics of the surface disallow the nice formulations. For instance, the surface corresponding to the vector field $F = y \hat{i} - x \hat{j}$ is continuous but periodic, spiraling along the z-axis. I envision it to look kinda like this: My question is thus: do all non-conservative vector fields (in 2-space) have corresponding surfaces that are periodic or discontinuous? If not, what other cases are there?","As anyone who has taken vector calculus (read: most of you) knows, if a vector field is conservative, then it is the gradient of a potential function. In other words, if the vector field is two dimensional, the potential function is a surface and as such some things become really nice, like the line integral of a closed curve being zero. However, there are some very simple vector fields that are not conservative, which leads me to wonder what the corresponding surface would look like and in particular what characteristics of the surface disallow the nice formulations. For instance, the surface corresponding to the vector field $F = y \hat{i} - x \hat{j}$ is continuous but periodic, spiraling along the z-axis. I envision it to look kinda like this: My question is thus: do all non-conservative vector fields (in 2-space) have corresponding surfaces that are periodic or discontinuous? If not, what other cases are there?",,['multivariable-calculus']
50,What vector field property means “is the curl of another vector field?”,What vector field property means “is the curl of another vector field?”,,"I know that a vector field $\mathbf{F}$ is called irrotational if $\nabla \times \mathbf{F} = \mathbf{0}$  and conservative if there exists a function $g$ such that $\nabla g = \mathbf{F}$.  Under suitable smoothness conditions on the component functions (so that Clairaut's theorem holds), conservative vector fields are irrotational, and under suitable topological conditions on the domain of $\mathbf{F}$, irrotational vector fields are conservative. Moving up one degree, $\mathbf{F}$ is called incompressible if $\nabla \cdot \mathbf{F} = 0$.  If there exists a vector field $\mathbf{G}$ such that $\mathbf{F} = \nabla \times \mathbf{G}$, then (again, under suitable smoothness conditions), $\mathbf{F}$ is incompressible.  And again, under suitable topological conditions (the second cohomology group of the domain must be trivial), if $\mathbf{F}$ is incompressible, there exists a vector field $\mathbf{G}$ such that $\nabla \times\mathbf{G} = \mathbf{F}$. It seems to me there ought to be a word to describe vector fields as shorthand for “is the curl of something” or “has a vector potential.”  But a google search didn't turn anything up, and my colleagues couldn't think of a word either.  Maybe I'm revealing the gap in my physics background.  Does anybody know of such a word?","I know that a vector field $\mathbf{F}$ is called irrotational if $\nabla \times \mathbf{F} = \mathbf{0}$  and conservative if there exists a function $g$ such that $\nabla g = \mathbf{F}$.  Under suitable smoothness conditions on the component functions (so that Clairaut's theorem holds), conservative vector fields are irrotational, and under suitable topological conditions on the domain of $\mathbf{F}$, irrotational vector fields are conservative. Moving up one degree, $\mathbf{F}$ is called incompressible if $\nabla \cdot \mathbf{F} = 0$.  If there exists a vector field $\mathbf{G}$ such that $\mathbf{F} = \nabla \times \mathbf{G}$, then (again, under suitable smoothness conditions), $\mathbf{F}$ is incompressible.  And again, under suitable topological conditions (the second cohomology group of the domain must be trivial), if $\mathbf{F}$ is incompressible, there exists a vector field $\mathbf{G}$ such that $\nabla \times\mathbf{G} = \mathbf{F}$. It seems to me there ought to be a word to describe vector fields as shorthand for “is the curl of something” or “has a vector potential.”  But a google search didn't turn anything up, and my colleagues couldn't think of a word either.  Maybe I'm revealing the gap in my physics background.  Does anybody know of such a word?",,"['multivariable-calculus', 'definition']"
51,Calculate the Hessian of a Vector Function,Calculate the Hessian of a Vector Function,,"I'm working with optimisation. I am trying to obtain the hessian of a vector function: $$ \mathbf{F(X) = 0} \quad \text{or} \quad   \begin{cases}     f_1(x_1,x_2,\dotsc,x_n) = 0,\\     f_2(x_1,x_2,\dotsc,x_n) = 0,\\     \vdots\\     f_n(x_1,x_2,\dotsc,x_n) = 0,\\   \end{cases} $$ I know that the Jacobian for a vector function is calculated as: $$ \mathbf{J}= \begin{bmatrix} \frac{\partial f_1}{\partial x_1} & \dots & \frac{\partial f_1}{\partial x_1} \\ 	\vdots & \ddots & \vdots \\ \frac{\partial f_n}{\partial x_1} & \dots &\frac{\partial f_n}{\partial x_n}  \end{bmatrix}  $$ I also know that the hessian for a single function is calculated as: $$ \mathbf{H}_{f_1}= \begin{bmatrix} \frac{\partial ^2 f_1}{\partial {x_1}^2} & \frac{\partial ^2 f_1}{\partial {x_1}{x_2}} & \dots & \frac{\partial ^2 f_1}{\partial {x_1}{x_n}}  \\  \frac{\partial ^2 f_1}{\partial {x_2}{x_1}} & \frac{\partial ^2 f_1}{\partial {x_2}^2} & \dots & \frac{\partial ^2 f_1}{\partial {x_2}{x_n}}  \\	\vdots & \vdots & \ddots & \vdots \\ \frac{\partial ^2 f_1}{\partial {x_n}{x_1}} & \frac{\partial ^2 f_1}{\partial {x_n}{x_2}} & \dots & \frac{\partial ^2 f_1}{\partial {x_n}^2}   \end{bmatrix}  $$ but I don't have an idea of how does the Hessian for a vector function should look like, neither how to calculate it. My idea was to calculate the hessian of each function, but I have no idea how to structure the result matrix $$ \mathbf{H}_{f_1}, \mathbf{H}_{f_2} , \dots , \mathbf{H}_{f_n} $$","I'm working with optimisation. I am trying to obtain the hessian of a vector function: $$ \mathbf{F(X) = 0} \quad \text{or} \quad   \begin{cases}     f_1(x_1,x_2,\dotsc,x_n) = 0,\\     f_2(x_1,x_2,\dotsc,x_n) = 0,\\     \vdots\\     f_n(x_1,x_2,\dotsc,x_n) = 0,\\   \end{cases} $$ I know that the Jacobian for a vector function is calculated as: $$ \mathbf{J}= \begin{bmatrix} \frac{\partial f_1}{\partial x_1} & \dots & \frac{\partial f_1}{\partial x_1} \\ 	\vdots & \ddots & \vdots \\ \frac{\partial f_n}{\partial x_1} & \dots &\frac{\partial f_n}{\partial x_n}  \end{bmatrix}  $$ I also know that the hessian for a single function is calculated as: $$ \mathbf{H}_{f_1}= \begin{bmatrix} \frac{\partial ^2 f_1}{\partial {x_1}^2} & \frac{\partial ^2 f_1}{\partial {x_1}{x_2}} & \dots & \frac{\partial ^2 f_1}{\partial {x_1}{x_n}}  \\  \frac{\partial ^2 f_1}{\partial {x_2}{x_1}} & \frac{\partial ^2 f_1}{\partial {x_2}^2} & \dots & \frac{\partial ^2 f_1}{\partial {x_2}{x_n}}  \\	\vdots & \vdots & \ddots & \vdots \\ \frac{\partial ^2 f_1}{\partial {x_n}{x_1}} & \frac{\partial ^2 f_1}{\partial {x_n}{x_2}} & \dots & \frac{\partial ^2 f_1}{\partial {x_n}^2}   \end{bmatrix}  $$ but I don't have an idea of how does the Hessian for a vector function should look like, neither how to calculate it. My idea was to calculate the hessian of each function, but I have no idea how to structure the result matrix $$ \mathbf{H}_{f_1}, \mathbf{H}_{f_2} , \dots , \mathbf{H}_{f_n} $$",,"['multivariable-calculus', 'vector-analysis', 'matrix-calculus', 'hessian-matrix']"
52,How prove this $x^3+y^3+z^3+3\ge 2(x^2+y^2+z^2)$,How prove this,x^3+y^3+z^3+3\ge 2(x^2+y^2+z^2),"Question: let $x,y,z>0$ and such $xyz=1$, show that   $$x^3+y^3+z^3+3\ge 2(x^2+y^2+z^2)$$ My idea: use AM-GM inequality $$x^3+x^3+1\ge 3x^2$$ $$y^3+y^3+1\ge 3y^2$$ $$z^3+z^3+1\ge 3z^2$$ so $$2(x^3+y^3+z^3)+3\ge 3(x^2+y^2+z^2)$$ But this is not my inequality,so How prove it? I know this condition is very important.but how use this condition? and this inequality is stronger","Question: let $x,y,z>0$ and such $xyz=1$, show that   $$x^3+y^3+z^3+3\ge 2(x^2+y^2+z^2)$$ My idea: use AM-GM inequality $$x^3+x^3+1\ge 3x^2$$ $$y^3+y^3+1\ge 3y^2$$ $$z^3+z^3+1\ge 3z^2$$ so $$2(x^3+y^3+z^3)+3\ge 3(x^2+y^2+z^2)$$ But this is not my inequality,so How prove it? I know this condition is very important.but how use this condition? and this inequality is stronger",,"['multivariable-calculus', 'inequality', 'substitution', 'uvw', 'mixing-variables']"
53,"Why is this vector field not conservative, even though it has a potential? (what is the actual definition of a conservative vector field?)","Why is this vector field not conservative, even though it has a potential? (what is the actual definition of a conservative vector field?)",,"My question is really ''what is the definition of a conservative vector field''? I've consulted 3 textbooks that all say a vector field $\vec{F}$ is conservative by definition if there exists a scalar potential $\phi$ such that $\nabla \phi = \vec{F}$. Then, they go on to talk about connected domains, path independence and the equality of mixed partials and how they are all related. In particular, they emphasize that e.g. in $\mathbb{R}^2$ given $\vec{F} = \bigl<F_1,\,F_2\bigr>$, if $\frac{\partial F_1}{\partial y} = \frac{\partial F_2}{\partial x}$ on a simply-connected domain, then $\vec{F}$ is conservative on that domain. However, without fail, all of them then offer the example of $\vec{F} = \bigl< \frac{-y}{x^2+y^2},\, \frac{x}{x^2+y^2} \bigr>$, pointing out: it's line integral is not path independent, even though $\frac{\partial F_1}{\partial y} = \frac{\partial F_2}{\partial x}$ and this is explained by pointing out that the domain is not simply connected (if the path contains the origin). That much makes sense to me: we cannot conclude $\vec{F}$ is conservative based on the partial derivatives, because the domain is not simply connected - totally consistent with what has been presented. What none of them address is why $\vec{F} = \bigl< \frac{-y}{x^2+y^2},\, \frac{x}{x^2+y^2} \bigr>$ is not conservative when there exists a potential $\phi = \arctan(y/x)$ such that $\nabla \phi = \vec{F}$ . None of the texts mention any necessary conditions on the scalar potential. So is the existence of a scalar potential the definition of a vector field being conservative or not?","My question is really ''what is the definition of a conservative vector field''? I've consulted 3 textbooks that all say a vector field $\vec{F}$ is conservative by definition if there exists a scalar potential $\phi$ such that $\nabla \phi = \vec{F}$. Then, they go on to talk about connected domains, path independence and the equality of mixed partials and how they are all related. In particular, they emphasize that e.g. in $\mathbb{R}^2$ given $\vec{F} = \bigl<F_1,\,F_2\bigr>$, if $\frac{\partial F_1}{\partial y} = \frac{\partial F_2}{\partial x}$ on a simply-connected domain, then $\vec{F}$ is conservative on that domain. However, without fail, all of them then offer the example of $\vec{F} = \bigl< \frac{-y}{x^2+y^2},\, \frac{x}{x^2+y^2} \bigr>$, pointing out: it's line integral is not path independent, even though $\frac{\partial F_1}{\partial y} = \frac{\partial F_2}{\partial x}$ and this is explained by pointing out that the domain is not simply connected (if the path contains the origin). That much makes sense to me: we cannot conclude $\vec{F}$ is conservative based on the partial derivatives, because the domain is not simply connected - totally consistent with what has been presented. What none of them address is why $\vec{F} = \bigl< \frac{-y}{x^2+y^2},\, \frac{x}{x^2+y^2} \bigr>$ is not conservative when there exists a potential $\phi = \arctan(y/x)$ such that $\nabla \phi = \vec{F}$ . None of the texts mention any necessary conditions on the scalar potential. So is the existence of a scalar potential the definition of a vector field being conservative or not?",,"['multivariable-calculus', 'vector-analysis']"
54,What is the Laplace operator's representation in 3-sphere-coordinates?,What is the Laplace operator's representation in 3-sphere-coordinates?,,"The three-dimensional Laplace operator in spherical coordinates can be expressed as $$\Delta_3 = \frac1{r^2}\partial_r(r^2\partial_r) + \frac1{r^2} L^2$$ where $L^2$ is the squared angular momentum operator $$L^2 = \frac1{\sin\theta}\partial_\theta(\sin\theta\partial_\theta)+\frac1{\sin^2\theta}\partial_\phi^2.$$ Is there a similarly simple representation in four-dimensional hyperspherical coordinates à la $$\Delta_4 = \frac1{r^3}\partial_r(r^3\partial_r) + \frac1{r^2}M^2$$ for some ""hyperangular"" momentum operator that does not depend on the hyper-radius? I know this is related to the Laplace-Beltrami Operator , but what's the explicit form?","The three-dimensional Laplace operator in spherical coordinates can be expressed as $$\Delta_3 = \frac1{r^2}\partial_r(r^2\partial_r) + \frac1{r^2} L^2$$ where $L^2$ is the squared angular momentum operator $$L^2 = \frac1{\sin\theta}\partial_\theta(\sin\theta\partial_\theta)+\frac1{\sin^2\theta}\partial_\phi^2.$$ Is there a similarly simple representation in four-dimensional hyperspherical coordinates à la $$\Delta_4 = \frac1{r^3}\partial_r(r^3\partial_r) + \frac1{r^2}M^2$$ for some ""hyperangular"" momentum operator that does not depend on the hyper-radius? I know this is related to the Laplace-Beltrami Operator , but what's the explicit form?",,"['differential-geometry', 'multivariable-calculus', 'partial-differential-equations']"
55,Is $ds$ a differential form?,Is  a differential form?,ds,"I am somewhat confused as to whether $ds$ (line element) is actually a differential form... we have (in $\mathbb{R}^2$): $$ds^2 = dx^2 + dy^2$$ Differential 1-forms are supposed to be linear combinations of the $dx_i$, but the $ds$ shown above is definitely not a linear combination of $\{dx, dy\}$. So what is it? EDIT The link given in the comments seems to indicate that $ds$ is not a differential form. The following quote is from C. H. Edwards, ""Advanced Calculus of Several Variables"" section V.1 Given an oriented curve $C$ in $\bf R$, its arclength form $ds$   is defined for ${\bf x} \in C$ by   $$ds_{\bf x}({\bf v}) = T({\bf x}) \cdot {\bf v}$$   Thus $ds_{\bf x}({\bf v})$ is simply the component of ${\bf v}$ in the direction of the unit tangent vector $T({\bf x})$. It is clear that $ds_{\bf x}({\bf v})$ is a linear function of ${\bf v} \in {\bf R}^n$, so $ds$ is a differential form on $C$. Is the book wrong (or does ""is a differential form on $C$"" not mean ""is a differential form"" in general)?","I am somewhat confused as to whether $ds$ (line element) is actually a differential form... we have (in $\mathbb{R}^2$): $$ds^2 = dx^2 + dy^2$$ Differential 1-forms are supposed to be linear combinations of the $dx_i$, but the $ds$ shown above is definitely not a linear combination of $\{dx, dy\}$. So what is it? EDIT The link given in the comments seems to indicate that $ds$ is not a differential form. The following quote is from C. H. Edwards, ""Advanced Calculus of Several Variables"" section V.1 Given an oriented curve $C$ in $\bf R$, its arclength form $ds$   is defined for ${\bf x} \in C$ by   $$ds_{\bf x}({\bf v}) = T({\bf x}) \cdot {\bf v}$$   Thus $ds_{\bf x}({\bf v})$ is simply the component of ${\bf v}$ in the direction of the unit tangent vector $T({\bf x})$. It is clear that $ds_{\bf x}({\bf v})$ is a linear function of ${\bf v} \in {\bf R}^n$, so $ds$ is a differential form on $C$. Is the book wrong (or does ""is a differential form on $C$"" not mean ""is a differential form"" in general)?",,"['multivariable-calculus', 'differential-geometry', 'riemannian-geometry', 'differential-forms']"
56,find the maximum $\frac{\frac{x^2_{1}}{x_{2}}+\frac{x^2_{2}}{x_{3}}+\cdots+\frac{x^2_{n-1}}{x_{n}}+\frac{x^2_{n}}{x_{1}}}{x_{1}+x_{2}+\cdots+x_{n}}$,find the maximum,\frac{\frac{x^2_{1}}{x_{2}}+\frac{x^2_{2}}{x_{3}}+\cdots+\frac{x^2_{n-1}}{x_{n}}+\frac{x^2_{n}}{x_{1}}}{x_{1}+x_{2}+\cdots+x_{n}},"give the postive intger $n\ge 2$,and postive real numbers $a<b$ if the real numbers such $x_{1},x_{2},\cdots,x_{n}\in[a,b]$ find the maximum of the value $$\dfrac{\frac{x^2_{1}}{x_{2}}+\frac{x^2_{2}}{x_{3}}+\cdots+\frac{x^2_{n-1}}{x_{n}}+\frac{x^2_{n}}{x_{1}}}{x_{1}+x_{2}+\cdots+x_{n}}$$ it seem the  polya-szego inequality http://journalofinequalitiesandapplications.springeropen.com/articles/10.1186/1029-242X-2013-591","give the postive intger $n\ge 2$,and postive real numbers $a<b$ if the real numbers such $x_{1},x_{2},\cdots,x_{n}\in[a,b]$ find the maximum of the value $$\dfrac{\frac{x^2_{1}}{x_{2}}+\frac{x^2_{2}}{x_{3}}+\cdots+\frac{x^2_{n-1}}{x_{n}}+\frac{x^2_{n}}{x_{1}}}{x_{1}+x_{2}+\cdots+x_{n}}$$ it seem the  polya-szego inequality http://journalofinequalitiesandapplications.springeropen.com/articles/10.1186/1029-242X-2013-591",,"['multivariable-calculus', 'inequality', 'optimization', 'convex-optimization', 'convexity-inequality']"
57,Looking for a rigorous treatment of improper multiple Riemann integrals,Looking for a rigorous treatment of improper multiple Riemann integrals,,"I'm studying undergraduate-level differential and integral calculus and have recently come across the topic of improper Riemann integrals. I'm familiar with the concept for single-variable functions, but I haven't been able to find a rigorous exposition of improper integrals for functions of several variables. What I mean exactly is the 'natural' generalisation of the Riemann integral which arises from dropping one or both of the hypotheses of The function being defined on a set which is not bounded The function itself not being bounded. Being familiar with the equivalent concepts for functions $\mathbb{R} \to \mathbb{R}$, I feel that it wouldn't be too hard for me to generalize the concept from that scenario (which I have found to be a lot better documented) to that of functions defined on $\mathbb{R^{n}}$, but I'm looking for a sort of standard text on it. I just can't seem to find it. Apostol's Mathematical Analysis was a promising source but, again, it only deals with the single-variable case. Marsden & Hoffman's Elementary Classical Analysis does have a short section on it, which I find too shallow and aerial. Finally, Rudin's Principles of Mathematical Analysis doesn't seem to cover what I want either, only mentioning improper integrals for the single-variable case. I am baffled that none of these supposedly standard texts cover the topic I'm after with depth. I can only hope that somebody out here knows any better sources.","I'm studying undergraduate-level differential and integral calculus and have recently come across the topic of improper Riemann integrals. I'm familiar with the concept for single-variable functions, but I haven't been able to find a rigorous exposition of improper integrals for functions of several variables. What I mean exactly is the 'natural' generalisation of the Riemann integral which arises from dropping one or both of the hypotheses of The function being defined on a set which is not bounded The function itself not being bounded. Being familiar with the equivalent concepts for functions $\mathbb{R} \to \mathbb{R}$, I feel that it wouldn't be too hard for me to generalize the concept from that scenario (which I have found to be a lot better documented) to that of functions defined on $\mathbb{R^{n}}$, but I'm looking for a sort of standard text on it. I just can't seem to find it. Apostol's Mathematical Analysis was a promising source but, again, it only deals with the single-variable case. Marsden & Hoffman's Elementary Classical Analysis does have a short section on it, which I find too shallow and aerial. Finally, Rudin's Principles of Mathematical Analysis doesn't seem to cover what I want either, only mentioning improper integrals for the single-variable case. I am baffled that none of these supposedly standard texts cover the topic I'm after with depth. I can only hope that somebody out here knows any better sources.",,['multivariable-calculus']
58,Distance of ellipse to the origin,Distance of ellipse to the origin,,Calculate the minimum distance from the origin to the curve $$3x^2+4xy+3y^2=20$$ The only method I know of is Lagrange multipliers. Is there any other method for questions of such type? Any help appreciated.,Calculate the minimum distance from the origin to the curve $$3x^2+4xy+3y^2=20$$ The only method I know of is Lagrange multipliers. Is there any other method for questions of such type? Any help appreciated.,,"['multivariable-calculus', 'optimization', 'conic-sections', 'qcqp']"
59,Surface integral over ellipsoid,Surface integral over ellipsoid,,"I've problem with this surface integral: $$ \iint\limits_S {\sqrt{ \left(\frac{x^2}{a^4}+\frac{y^2}{b^4}+\frac{z^2}{c^4}\right)}}{dS} $$, where $$ S = \{(x,y,z)\in\mathbb{R}^3: \frac{x^2}{a^2} + \frac{y^2}{b^2} + \frac{z^2}{c^2}= 1\} $$","I've problem with this surface integral: $$ \iint\limits_S {\sqrt{ \left(\frac{x^2}{a^4}+\frac{y^2}{b^4}+\frac{z^2}{c^4}\right)}}{dS} $$, where $$ S = \{(x,y,z)\in\mathbb{R}^3: \frac{x^2}{a^2} + \frac{y^2}{b^2} + \frac{z^2}{c^2}= 1\} $$",,['multivariable-calculus']
60,Proof that gradient is orthogonal to level set,Proof that gradient is orthogonal to level set,,"When we proved the gradient of a function $f:\mathbb{R}^n\rightarrow \mathbb{R}$ is orthogonal to the level sets of the function $f(\vec{x}) = c$ for some constant $c$, my professor was quite explicit in stating that the implicit function theorem (IFT) is needed for the proof without giving a clear reason why. In every other proof I've seen of the theorem however, the implicit function theorem was not used nor even mentioned. This has got me to thinking why exactly the IFT was invoked in our proof or whether it's needed at all. All of the proofs start by taking any differentiable curve, parametrized in $t$, residing in the level set and passing through the point of interest $\vec{a}$. The chain rule guarantees that the tangent to the curve is orthogonal to the gradient at $\vec{a}$. Since this happens for any curve, we can say that the gradient is orthogonal to the surface. I'm thinking that the IFT is needed to prove that such a curve actually exists, but I'm not sure on how exactly it does that. If anyone can shed some light on the subject that would be great. Thanks.","When we proved the gradient of a function $f:\mathbb{R}^n\rightarrow \mathbb{R}$ is orthogonal to the level sets of the function $f(\vec{x}) = c$ for some constant $c$, my professor was quite explicit in stating that the implicit function theorem (IFT) is needed for the proof without giving a clear reason why. In every other proof I've seen of the theorem however, the implicit function theorem was not used nor even mentioned. This has got me to thinking why exactly the IFT was invoked in our proof or whether it's needed at all. All of the proofs start by taking any differentiable curve, parametrized in $t$, residing in the level set and passing through the point of interest $\vec{a}$. The chain rule guarantees that the tangent to the curve is orthogonal to the gradient at $\vec{a}$. Since this happens for any curve, we can say that the gradient is orthogonal to the surface. I'm thinking that the IFT is needed to prove that such a curve actually exists, but I'm not sure on how exactly it does that. If anyone can shed some light on the subject that would be great. Thanks.",,"['multivariable-calculus', 'vector-analysis']"
61,What is difference between all of these derivatives?,What is difference between all of these derivatives?,,"In calculus II we were introduced to a bunch of new derivatives: the gradient, the derivative $D=\begin{bmatrix} \partial_{x_1} \\ \partial_{x_2} \\ \vdots \\ \partial_{x_n}\end{bmatrix}$, the Jacobian, the Hessian, the total differential, the directional derivative, the partial derivative, and something called a Frechet derivative (that one was only mentioned in passing). I can apply the formulas to calculate these things, but what exactly are they?  And how do they relate to each other? For instance, the derivative of a function $f: \Bbb R \to \Bbb R$ gives the slope of the line tangent to $f$.  Which one of the above gives you, for instance, the ""slope"" (I don't even know what to call a $2$-D slope) of a function $g: \Bbb R^2 \to \Bbb R$?  I know that the partial derivatives give you the slope in the $x$, $y$, etc directions, but then what do the others do? And how do they relate to each other?  For instance, how does $D$ relate to say the directional derivative $\partial_{\vec v}$? Thanks.","In calculus II we were introduced to a bunch of new derivatives: the gradient, the derivative $D=\begin{bmatrix} \partial_{x_1} \\ \partial_{x_2} \\ \vdots \\ \partial_{x_n}\end{bmatrix}$, the Jacobian, the Hessian, the total differential, the directional derivative, the partial derivative, and something called a Frechet derivative (that one was only mentioned in passing). I can apply the formulas to calculate these things, but what exactly are they?  And how do they relate to each other? For instance, the derivative of a function $f: \Bbb R \to \Bbb R$ gives the slope of the line tangent to $f$.  Which one of the above gives you, for instance, the ""slope"" (I don't even know what to call a $2$-D slope) of a function $g: \Bbb R^2 \to \Bbb R$?  I know that the partial derivatives give you the slope in the $x$, $y$, etc directions, but then what do the others do? And how do they relate to each other?  For instance, how does $D$ relate to say the directional derivative $\partial_{\vec v}$? Thanks.",,"['multivariable-calculus', 'derivatives']"
62,Double Integral Math Olympiad Problem,Double Integral Math Olympiad Problem,,"I was taking a Math Olympiad test and one of the questions was to calculate the following double integral: $$\int_0^\infty\int_0^\infty\frac{\log|(x+y)(1-xy)|}{(1+x^2)(1+y^2)}\ \mathrm{d}x\ \mathrm{d}y$$ Here, as usual, $\log a$ and $|a|$ are the natural logarithm and absolute value of $a$ respectively. I'm guessing that you're not supposed to solve it analytically, but rather find some symmetry argument or clever simplification that would make it straightforward. Since I don't even know where to start, any help is welcome. In case you want to know, this was taken from the 2016 Rio de Janeiro State Math Olympiad, known in Portuguese as OMERJ.","I was taking a Math Olympiad test and one of the questions was to calculate the following double integral: $$\int_0^\infty\int_0^\infty\frac{\log|(x+y)(1-xy)|}{(1+x^2)(1+y^2)}\ \mathrm{d}x\ \mathrm{d}y$$ Here, as usual, $\log a$ and $|a|$ are the natural logarithm and absolute value of $a$ respectively. I'm guessing that you're not supposed to solve it analytically, but rather find some symmetry argument or clever simplification that would make it straightforward. Since I don't even know where to start, any help is welcome. In case you want to know, this was taken from the 2016 Rio de Janeiro State Math Olympiad, known in Portuguese as OMERJ.",,"['multivariable-calculus', 'contest-math']"
63,Why does the formula for calculating a reflection vector work?,Why does the formula for calculating a reflection vector work?,,"The formula for calculating a reflection vector is as follows: $$ R = V - 2N(V\cdot N) $$ Where V is the incident vector and N is the normal vector on the plane in question. Why does this formula work? I haven't seen any good explanations of it. I don't understand the significance of doubling the normal vector, nor the relevance of taking the dot product.","The formula for calculating a reflection vector is as follows: $$ R = V - 2N(V\cdot N) $$ Where V is the incident vector and N is the normal vector on the plane in question. Why does this formula work? I haven't seen any good explanations of it. I don't understand the significance of doubling the normal vector, nor the relevance of taking the dot product.",,"['multivariable-calculus', 'reflection']"
64,Solving quadratic vector equation,Solving quadratic vector equation,,"Hope it is a right place to ask how to solve the equation on $\mathbf x$: $$ \mathbf x^T \mathbf A\mathbf x + \mathbf x^T \mathbf b + c = 0. $$ where: $\mathbf x$ is an $n\times 1$ column vector $\mathbf A$ is an $n\times n$ matrix $\mathbf b$ is an $n\times 1$ vector $c$ is a scalar Thanks","Hope it is a right place to ask how to solve the equation on $\mathbf x$: $$ \mathbf x^T \mathbf A\mathbf x + \mathbf x^T \mathbf b + c = 0. $$ where: $\mathbf x$ is an $n\times 1$ column vector $\mathbf A$ is an $n\times n$ matrix $\mathbf b$ is an $n\times 1$ vector $c$ is a scalar Thanks",,['multivariable-calculus']
65,Taylor's theorem in Banach spaces,Taylor's theorem in Banach spaces,,"Let $f$ be a real function of a single real variable. Suppose that $f$ is $n$ times differentiable at some $x$, for some integer $n\geq 1$. Making no further assumptions, we have $$ f(x+h) = f(x) + f'(x)h + \frac{f''(x)h^2}{2} + \ldots + \frac{ f^{(n)}(x)h^n}{n!} + o(h^n) $$ although this tells us nothing about the size of the error on any fixed neighbourhood of $x$ (further regularity assumptions are needed to get the traditional forms for the remainder). Wikipedia has a proof of this which relies on, of all things, L'Hôpital's rule for which a proof is provided as well. Recently I've been working through the basic theorems of calculus in the Banach space setting. It's not terribly surprising and I have seen it written in a few places that the expansion $$ f(x+h) = f(x) + f'(x)h + \frac{f''(x)h^2}{2} + \ldots + \frac{ f^{(n)}(x)h^n}{n!} + o(\|h\|^n) $$ is still valid when $f$ is a map of Banach spaces $X \to Y$. In this setting, the $k$th derivative $f^{(k)}(x)$ of $f$ at $x$ is a continuous, symmetric, $k$-linear map $X^k \to Y$ and $h^k$ is short-hand for $(h,\ldots,h) \in X^k$. I would like to know: How do I prove this last expansion is valid? I'm having trouble adapting the proof from the single variable case because I can't seem to prove an appropriate analogue of L'Hôpital's rule. An industry standard proof of L'Hôpital's rule depends on Cauchy's mean value theorem and I am not sure whether this admits a Banach space analogue either. In fact, I think I would also be curious to know: Do Cauchy's mean value theorem or  L'Hôpital's rule have natural generalizations in the context of maps between Banach spaces? Thanks in advance for any replies.","Let $f$ be a real function of a single real variable. Suppose that $f$ is $n$ times differentiable at some $x$, for some integer $n\geq 1$. Making no further assumptions, we have $$ f(x+h) = f(x) + f'(x)h + \frac{f''(x)h^2}{2} + \ldots + \frac{ f^{(n)}(x)h^n}{n!} + o(h^n) $$ although this tells us nothing about the size of the error on any fixed neighbourhood of $x$ (further regularity assumptions are needed to get the traditional forms for the remainder). Wikipedia has a proof of this which relies on, of all things, L'Hôpital's rule for which a proof is provided as well. Recently I've been working through the basic theorems of calculus in the Banach space setting. It's not terribly surprising and I have seen it written in a few places that the expansion $$ f(x+h) = f(x) + f'(x)h + \frac{f''(x)h^2}{2} + \ldots + \frac{ f^{(n)}(x)h^n}{n!} + o(\|h\|^n) $$ is still valid when $f$ is a map of Banach spaces $X \to Y$. In this setting, the $k$th derivative $f^{(k)}(x)$ of $f$ at $x$ is a continuous, symmetric, $k$-linear map $X^k \to Y$ and $h^k$ is short-hand for $(h,\ldots,h) \in X^k$. I would like to know: How do I prove this last expansion is valid? I'm having trouble adapting the proof from the single variable case because I can't seem to prove an appropriate analogue of L'Hôpital's rule. An industry standard proof of L'Hôpital's rule depends on Cauchy's mean value theorem and I am not sure whether this admits a Banach space analogue either. In fact, I think I would also be curious to know: Do Cauchy's mean value theorem or  L'Hôpital's rule have natural generalizations in the context of maps between Banach spaces? Thanks in advance for any replies.",,"['multivariable-calculus', 'banach-spaces']"
66,"Vector fields, line integrals and surface integrals - Why one measures flux across the boundary and the other along?","Vector fields, line integrals and surface integrals - Why one measures flux across the boundary and the other along?",,"Why is it that a line integral of a vector field takes the dot product of the vector field with the tangent? This results in us taking the component of the vector field in the direction of the tangent of the curve we are integrating over. This can give us work done as a physical interpretation. If this curve encloses an area, we can also use Green's theorem over that area instead of the line integral to get the same result. Green's theorem measures flux ALONG the boundary. Now the analogue to that in a higher dimension is the surface integral of a vector field. However here the definition of the surface integral is taking the dot product of the vector field with the normal (more specifically the cross product of the first partial derivatives), with its interpretation as flux across the surface. Here, we can use the divergence theorem also if the surface encloses a volume. Divergence theorem measures the flux ACROSS the boundary. Why is there a difference? So why is a surface integral measuring something across the boundary and the line integral something along? EDIT: We notice the curl is actually a vector field itself. Looking through some notes, if we take a vector field in two dimensions, we have components $$f = (P,Q,0)$$ Hence, curl is equal to $$curl f.k$$ which is just $$\frac{\partial Q}{\partial x} - \frac{\partial P}{\partial y}$$ and an integral with this as its integrand is exactly Green's theorem. The curl in this case is integrated over a flat surface on the xy plane and the normal to the curl is just the k component (standard basis of the z axis). By applying the definition of surface integrals on Stoke's theorem, we get Green's theorem. This shows that Green's theorem is just a special case of Stoke's theorem. So the exact definition of the surface integral can be used to result in Green's theorem, and Green's theorem can be obtained directly by considering circulation over an area which links to our line integral, so there is definitely a link here that I can't seem to grasp. EDIT 2: I would like to add the fact that I mention in my comments. The line integral and surface integral over scalar valued functions makes sense. One could say that the line integral uses a tangential component (the norm of our the derivative of our parametrisation), while the surface integral a normal (the norm of the cross product of the first partial derivatives), but this is only because of its geometrical interpretation. The tangent used in line integral approximates the line, while the norm of the cross product used in surface integrals approximate the surface using the area of a parallelogram - which is exactly the norm of the cross product between arbitrary vectors. So why is the link in vector valued functions less obvious? What am I missing in my thinking?","Why is it that a line integral of a vector field takes the dot product of the vector field with the tangent? This results in us taking the component of the vector field in the direction of the tangent of the curve we are integrating over. This can give us work done as a physical interpretation. If this curve encloses an area, we can also use Green's theorem over that area instead of the line integral to get the same result. Green's theorem measures flux ALONG the boundary. Now the analogue to that in a higher dimension is the surface integral of a vector field. However here the definition of the surface integral is taking the dot product of the vector field with the normal (more specifically the cross product of the first partial derivatives), with its interpretation as flux across the surface. Here, we can use the divergence theorem also if the surface encloses a volume. Divergence theorem measures the flux ACROSS the boundary. Why is there a difference? So why is a surface integral measuring something across the boundary and the line integral something along? EDIT: We notice the curl is actually a vector field itself. Looking through some notes, if we take a vector field in two dimensions, we have components $$f = (P,Q,0)$$ Hence, curl is equal to $$curl f.k$$ which is just $$\frac{\partial Q}{\partial x} - \frac{\partial P}{\partial y}$$ and an integral with this as its integrand is exactly Green's theorem. The curl in this case is integrated over a flat surface on the xy plane and the normal to the curl is just the k component (standard basis of the z axis). By applying the definition of surface integrals on Stoke's theorem, we get Green's theorem. This shows that Green's theorem is just a special case of Stoke's theorem. So the exact definition of the surface integral can be used to result in Green's theorem, and Green's theorem can be obtained directly by considering circulation over an area which links to our line integral, so there is definitely a link here that I can't seem to grasp. EDIT 2: I would like to add the fact that I mention in my comments. The line integral and surface integral over scalar valued functions makes sense. One could say that the line integral uses a tangential component (the norm of our the derivative of our parametrisation), while the surface integral a normal (the norm of the cross product of the first partial derivatives), but this is only because of its geometrical interpretation. The tangent used in line integral approximates the line, while the norm of the cross product used in surface integrals approximate the surface using the area of a parallelogram - which is exactly the norm of the cross product between arbitrary vectors. So why is the link in vector valued functions less obvious? What am I missing in my thinking?",,"['multivariable-calculus', 'vector-fields', 'surface-integrals', 'line-integrals', 'greens-theorem']"
67,Stokes for integration along the fiber,Stokes for integration along the fiber,,"I want to use a version of Stokes theorem for integration along the fiber and I need some help in proving a general statement. Let $F$ be a $k$-manifold with boundary and let $E \to M$ be a smooth fiber bundle with fiber $F$. Let $\omega$ be a differential form on $E$, let $\int_F: \Omega^*(E) \to \Omega^{*-k}(E)$ denote integration along the fiber and $i: \partial F \to F$ the inclusion map. Can I have some help in formalizing and proving a statement of the following form: $d \int_F \omega=\int_F d\omega \pm \int_{\partial F}i^*\omega$ Thanks!","I want to use a version of Stokes theorem for integration along the fiber and I need some help in proving a general statement. Let $F$ be a $k$-manifold with boundary and let $E \to M$ be a smooth fiber bundle with fiber $F$. Let $\omega$ be a differential form on $E$, let $\int_F: \Omega^*(E) \to \Omega^{*-k}(E)$ denote integration along the fiber and $i: \partial F \to F$ the inclusion map. Can I have some help in formalizing and proving a statement of the following form: $d \int_F \omega=\int_F d\omega \pm \int_{\partial F}i^*\omega$ Thanks!",,"['differential-geometry', 'multivariable-calculus', 'differential-topology', 'fiber-bundles']"
68,"What exactly does $\frac{\partial(y_1,\dots,y_m)}{\partial(x_1,\dots,x_n)}$ refer to?",What exactly does  refer to?,"\frac{\partial(y_1,\dots,y_m)}{\partial(x_1,\dots,x_n)}","I have been asking a rather few questions of this nature lately, maybe I'm starting to realise math notation isn't as uniform as I initially thought it would be... Question: Does this notation $$\frac{\partial(y_1,\dots,y_m)}{\partial(x_1,\dots,x_n)}$$ refer to the Jacobian matrix $$ J = \begin{bmatrix} \dfrac{\partial y_1}{\partial x_1} & \cdots & \dfrac{\partial y_1}{\partial x_n} \\ \vdots & \ddots & \vdots \\ \dfrac{\partial y_m}{\partial x_1} & \cdots & \dfrac{\partial y_m}{\partial x_n}  \end{bmatrix},$$ or the Jacobian determinant $\det J$? This answer seems to support the latter interpretation, while this (and Wikipedia) both support the former. I am aware of the ambiguity of ""Jacobian"" being used to refer to either the determinant or the matrix itself, is this a similar case? It's really a bit annoying because when I see things like $$ \left| \frac{\partial(y_1,\dots,y_m)}{\partial(x_1,\dots,x_n)} \right| $$ I don't know if it means the absolute value of the Jacobian determinant, or the determinant of the Jacobian matrix.","I have been asking a rather few questions of this nature lately, maybe I'm starting to realise math notation isn't as uniform as I initially thought it would be... Question: Does this notation $$\frac{\partial(y_1,\dots,y_m)}{\partial(x_1,\dots,x_n)}$$ refer to the Jacobian matrix $$ J = \begin{bmatrix} \dfrac{\partial y_1}{\partial x_1} & \cdots & \dfrac{\partial y_1}{\partial x_n} \\ \vdots & \ddots & \vdots \\ \dfrac{\partial y_m}{\partial x_1} & \cdots & \dfrac{\partial y_m}{\partial x_n}  \end{bmatrix},$$ or the Jacobian determinant $\det J$? This answer seems to support the latter interpretation, while this (and Wikipedia) both support the former. I am aware of the ambiguity of ""Jacobian"" being used to refer to either the determinant or the matrix itself, is this a similar case? It's really a bit annoying because when I see things like $$ \left| \frac{\partial(y_1,\dots,y_m)}{\partial(x_1,\dots,x_n)} \right| $$ I don't know if it means the absolute value of the Jacobian determinant, or the determinant of the Jacobian matrix.",,"['multivariable-calculus', 'notation']"
69,Is there a volume-preserving diffeomorphism of the disk with prescribed singular values?,Is there a volume-preserving diffeomorphism of the disk with prescribed singular values?,,"This question has now been cross-posted at mathoverflow. While working on a variational problem, I have reached to the following question. Let $0<\sigma_1<\sigma_2$ satisfy $\sigma_1\sigma_2=1$ , and let $D \subseteq \mathbb{R}^2$ be the closed unit disk. Does there exist a smooth map $f:D \to D$ such that $df$ has everywhere the fixed singular values $\sigma_1,\sigma_2$ and $\det(df)=1$ ?    Is there such a diffeomorphism of $D$ ? The linear map $x \to \begin{pmatrix} \sigma_1 & 0 \\\ 0 & \sigma_2 \end{pmatrix}x$ does not satisfy the requirement; it gets outside of $D$ , as $ \sigma_2 > 1$ . If we exclude a ray from $D$ , then there is such a map, given by $re^{i \theta} \to \sigma_1re^{i(\sigma_2/\sigma_1) \theta}$ . Furthermore, when $\frac{\sigma_2}{\sigma_1}=n$ is an integer, this map is given by $z \to \frac{z^n}{|z|^{n-1}}$ which is in $W^{1,\infty}(D,\mathbb{R}^2)$ . Thus, if we could approximate such a map by smooth maps having fixed singular values, we would finish. Here is an argument (given by a colleague) showing that $f$ cannot be the gradient of a function: Suppose that $f=\nabla u$ . Then $df=\operatorname{Hess}u$ is symmetric and has real eigenvalues. Since $\det(df)=1$ , at every point of $D$ both eigenvalues are positive or both are negative. Thus $\operatorname{tr}(df) \neq 0$ has a definite sign. By composing $f$ with the map $x \to -x$ we can assume the eigenvalues are always positive. Now, $$ \int_{D} \operatorname{div} f = \int_{\partial D} \langle f, n \rangle \le \operatorname{Vol}(\partial D) =2\operatorname{Vol}( D),$$ where in the inequality we have used the fact that $|f| \le 1$ . We showed that $\operatorname{div} f \le 2$ on average, so there exist a point $x \in D$ where $$ \operatorname{div}f (x)=\lambda_1(df_x) + \lambda_2(df_x) \le 2. $$ Since the eigenvalues are positive, and $df=\operatorname{Hess}u$ is symmetric, we have $\lambda_i=\sigma_i$ , so $\sigma_1+\sigma_2=\sigma_1(df_x) + \sigma_2(df_x) \le 2$ . This contradicts the AM-GM inequality $\frac{\sigma_1+\sigma_2}{2} > \sqrt{\sigma_1 \sigma_2}=1$ , which is strict here since $\sigma_1 \neq \sigma_2$ . I tried using Helmholtz decomposition to treat the general case, but that doesn't seem to lead anywhere.","This question has now been cross-posted at mathoverflow. While working on a variational problem, I have reached to the following question. Let satisfy , and let be the closed unit disk. Does there exist a smooth map such that has everywhere the fixed singular values and ?    Is there such a diffeomorphism of ? The linear map does not satisfy the requirement; it gets outside of , as . If we exclude a ray from , then there is such a map, given by . Furthermore, when is an integer, this map is given by which is in . Thus, if we could approximate such a map by smooth maps having fixed singular values, we would finish. Here is an argument (given by a colleague) showing that cannot be the gradient of a function: Suppose that . Then is symmetric and has real eigenvalues. Since , at every point of both eigenvalues are positive or both are negative. Thus has a definite sign. By composing with the map we can assume the eigenvalues are always positive. Now, where in the inequality we have used the fact that . We showed that on average, so there exist a point where Since the eigenvalues are positive, and is symmetric, we have , so . This contradicts the AM-GM inequality , which is strict here since . I tried using Helmholtz decomposition to treat the general case, but that doesn't seem to lead anywhere.","0<\sigma_1<\sigma_2 \sigma_1\sigma_2=1 D \subseteq \mathbb{R}^2 f:D \to D df \sigma_1,\sigma_2 \det(df)=1 D x \to \begin{pmatrix} \sigma_1 & 0 \\\ 0 & \sigma_2 \end{pmatrix}x D  \sigma_2 > 1 D re^{i \theta} \to \sigma_1re^{i(\sigma_2/\sigma_1) \theta} \frac{\sigma_2}{\sigma_1}=n z \to \frac{z^n}{|z|^{n-1}} W^{1,\infty}(D,\mathbb{R}^2) f f=\nabla u df=\operatorname{Hess}u \det(df)=1 D \operatorname{tr}(df) \neq 0 f x \to -x  \int_{D} \operatorname{div} f = \int_{\partial D} \langle f, n \rangle \le \operatorname{Vol}(\partial D) =2\operatorname{Vol}( D), |f| \le 1 \operatorname{div} f \le 2 x \in D  \operatorname{div}f (x)=\lambda_1(df_x) + \lambda_2(df_x) \le 2.  df=\operatorname{Hess}u \lambda_i=\sigma_i \sigma_1+\sigma_2=\sigma_1(df_x) + \sigma_2(df_x) \le 2 \frac{\sigma_1+\sigma_2}{2} > \sqrt{\sigma_1 \sigma_2}=1 \sigma_1 \neq \sigma_2","['multivariable-calculus', 'differential-geometry', 'riemannian-geometry', 'ergodic-theory', 'svd']"
70,Why do we need vectors and who invented it?,Why do we need vectors and who invented it?,,"It is natural to understand the need for scalars (numbers), but why did we invent vectors? Who invented it and for what? EDIT: As George Lowther pointed out, the problem is too broad; I added the following questions as concrete supplements. It's easy for humans to understand the law of addition of scalar numbers, but why does the vector addition follow the parallelogram rule, and not some other law? (link to physics: How do we know that the addition of forces follows the parallelogram law?) The length of a 2-dimensional vector is the hypotenuse of the triangle constructed from the two components of the vector and the length of 3-dimensional vector also follows this way. But what about 4-dimensional vectors? Why do we define dot products of vectors like it is now? Is it because of its physical essence or its equivalence to the law of cosines?","It is natural to understand the need for scalars (numbers), but why did we invent vectors? Who invented it and for what? EDIT: As George Lowther pointed out, the problem is too broad; I added the following questions as concrete supplements. It's easy for humans to understand the law of addition of scalar numbers, but why does the vector addition follow the parallelogram rule, and not some other law? (link to physics: How do we know that the addition of forces follows the parallelogram law?) The length of a 2-dimensional vector is the hypotenuse of the triangle constructed from the two components of the vector and the length of 3-dimensional vector also follows this way. But what about 4-dimensional vectors? Why do we define dot products of vectors like it is now? Is it because of its physical essence or its equivalence to the law of cosines?",,"['multivariable-calculus', 'math-history']"
71,"Why is it that $\int_a^b \int_c^d f(x)g(y)\,dy\,dx=\int_a^b f(x)\,dx \int_c^d g(y)\,dy$?",Why is it that ?,"\int_a^b \int_c^d f(x)g(y)\,dy\,dx=\int_a^b f(x)\,dx \int_c^d g(y)\,dy","The title sums it up. It's simple to prove, but I'm wondering if there is a geometric interpretation?","The title sums it up. It's simple to prove, but I'm wondering if there is a geometric interpretation?",,"['multivariable-calculus', 'definite-integrals']"
72,Multivariate Normal Difference Distribution,Multivariate Normal Difference Distribution,,"Since the distribution of a difference of two normally distributed variates X and Y with means and variances $(\mu_x,\sigma_x^2)$ and $(\mu_y,\sigma_y^2)$ respectively is given by another normal distribution with mean $\mu_x-\mu_y$ and variance $\sigma_x^2+\sigma_y^2$ http://mathworld.wolfram.com/NormalDifferenceDistribution.html . Assuming that the distributions are independent, can we find a distribution of a difference of two multivariate normally distributed variates?","Since the distribution of a difference of two normally distributed variates X and Y with means and variances $(\mu_x,\sigma_x^2)$ and $(\mu_y,\sigma_y^2)$ respectively is given by another normal distribution with mean $\mu_x-\mu_y$ and variance $\sigma_x^2+\sigma_y^2$ http://mathworld.wolfram.com/NormalDifferenceDistribution.html . Assuming that the distributions are independent, can we find a distribution of a difference of two multivariate normally distributed variates?",,"['multivariable-calculus', 'probability-distributions', 'normal-distribution']"
73,Definition of the gradient for non-Cartesian coordinates,Definition of the gradient for non-Cartesian coordinates,,"The gradient of a function $f: \mathbb{R}^n \to \mathbb{R}$ is defined as the vector of the partial derivatives: $$ \nabla f = \left(\frac{\partial f}{\partial x_1}, ..., \frac{\partial f}{\partial x_n}\right)$$ Recently, I have become somewhat confused over this definition since I realized that if, for example, $f$ is defined in spherical coordinates $(r, \theta, \phi)$ , the gradient is given as $$ \nabla f = \left(\frac{\partial f}{\partial r}, \frac{1}{r} \frac{\partial f}{\partial \theta}, \frac{1}{r \sin \theta} \frac{\partial f}{\partial \phi} \right)$$ rather than $$ \nabla f = \left(\frac{\partial f}{\partial r}, \frac{\partial f}{\partial \theta}, \frac{\partial f}{\partial \phi} \right)$$ I have two questions regarding this: Is a scalar-valued function in spherical coordinates still considered to be  $f: \mathbb{R}^3 \to \mathbb{R}$, or is $\mathbb{R}^3$ reserved for Cartesian coordinates? Does the ""partial derivative"" definition of the gradient in fact require Cartesian coordinates?","The gradient of a function $f: \mathbb{R}^n \to \mathbb{R}$ is defined as the vector of the partial derivatives: $$ \nabla f = \left(\frac{\partial f}{\partial x_1}, ..., \frac{\partial f}{\partial x_n}\right)$$ Recently, I have become somewhat confused over this definition since I realized that if, for example, $f$ is defined in spherical coordinates $(r, \theta, \phi)$ , the gradient is given as $$ \nabla f = \left(\frac{\partial f}{\partial r}, \frac{1}{r} \frac{\partial f}{\partial \theta}, \frac{1}{r \sin \theta} \frac{\partial f}{\partial \phi} \right)$$ rather than $$ \nabla f = \left(\frac{\partial f}{\partial r}, \frac{\partial f}{\partial \theta}, \frac{\partial f}{\partial \phi} \right)$$ I have two questions regarding this: Is a scalar-valued function in spherical coordinates still considered to be  $f: \mathbb{R}^3 \to \mathbb{R}$, or is $\mathbb{R}^3$ reserved for Cartesian coordinates? Does the ""partial derivative"" definition of the gradient in fact require Cartesian coordinates?",,['multivariable-calculus']
74,what to do when the multivariable second derivative test is inconclusive?,what to do when the multivariable second derivative test is inconclusive?,,"What do we do when the second derivative test fails? How do we approach it, and is there a general method to further find whether a critical point is a maximum, minimum or a saddle point? For example, I'm asked to find all the critical points of the function $$f(x,y)=-x^{2012}-y^{2012}$$ and determine the nature of the critical points. The critical point that I have found is at $(0,0)$, but I'm unable to determine its nature as the second derivative test fails here.","What do we do when the second derivative test fails? How do we approach it, and is there a general method to further find whether a critical point is a maximum, minimum or a saddle point? For example, I'm asked to find all the critical points of the function $$f(x,y)=-x^{2012}-y^{2012}$$ and determine the nature of the critical points. The critical point that I have found is at $(0,0)$, but I'm unable to determine its nature as the second derivative test fails here.",,"['multivariable-calculus', 'optimization', 'nonlinear-optimization']"
75,"Prove that hyperspherical coordinates are a diffeomorphism, derive Jacobian","Prove that hyperspherical coordinates are a diffeomorphism, derive Jacobian",,"The explicit form for the transformation into hyperspherical coordinates is $$x_1 = r\sin\theta_1 \sin\theta_2 \dotsb \sin \theta_{n-1} \\ x_2 = r\sin\theta_1 \sin\theta_2 \dotsb \cos \theta_{n-1} \\ x_3 = r\sin\theta_1 \dotsb \cos \theta_{n-2}\\ \vdots \\ x_{n} = r \cos\theta_1$$ for $0 \leq \theta_i \leq \pi \;\;(1\leq i \leq n-2)$ and $0\leq \theta_{n-1} \leq 2\pi$. It has Jacobian $r^{n-1} \sin^{n-2}\theta_1 \sin^{n-3}\theta_2 \dotsb \sin{\theta_{n-2}}.$ I wonder if someone could provide me with a reference for an intuitive explanation as to why this indeed is a diffeomorphism from $\mathbb{R}^n\setminus\{0\} \to \mathbb{R}^n \setminus\{0\}$, and why this is the Jacobian. Or perhaps someone could indicate the idea of a proof. Thanks as always","The explicit form for the transformation into hyperspherical coordinates is $$x_1 = r\sin\theta_1 \sin\theta_2 \dotsb \sin \theta_{n-1} \\ x_2 = r\sin\theta_1 \sin\theta_2 \dotsb \cos \theta_{n-1} \\ x_3 = r\sin\theta_1 \dotsb \cos \theta_{n-2}\\ \vdots \\ x_{n} = r \cos\theta_1$$ for $0 \leq \theta_i \leq \pi \;\;(1\leq i \leq n-2)$ and $0\leq \theta_{n-1} \leq 2\pi$. It has Jacobian $r^{n-1} \sin^{n-2}\theta_1 \sin^{n-3}\theta_2 \dotsb \sin{\theta_{n-2}}.$ I wonder if someone could provide me with a reference for an intuitive explanation as to why this indeed is a diffeomorphism from $\mathbb{R}^n\setminus\{0\} \to \mathbb{R}^n \setminus\{0\}$, and why this is the Jacobian. Or perhaps someone could indicate the idea of a proof. Thanks as always",,"['reference-request', 'multivariable-calculus', 'spherical-coordinates']"
76,Derivation of the multivariate chain rule,Derivation of the multivariate chain rule,,"I can't believe I couldn't find this information online, but could someone provide me a good proof of the multivariate chain rule ? \begin{align} \frac{df}{dt} = \frac{df}{dx}\frac{dx}{dt} + \frac{df}{dy}\frac{dy}{dt} \end{align} I found multiple derivation of this results online using differentials and mean value theorem, but they don't look like rigorous to me. Somehow dividing the differential by $dt$ doesn't make it rigorous for my point of view... This question comes in a more general context where I am trying to understand why deriving a composition is effectively a matrix product. So by understanding this formula, I am able to see why building matrix of derivatives is a good tool to compute derivatives by matrix multiplication. Thanks !","I can't believe I couldn't find this information online, but could someone provide me a good proof of the multivariate chain rule ? \begin{align} \frac{df}{dt} = \frac{df}{dx}\frac{dx}{dt} + \frac{df}{dy}\frac{dy}{dt} \end{align} I found multiple derivation of this results online using differentials and mean value theorem, but they don't look like rigorous to me. Somehow dividing the differential by $dt$ doesn't make it rigorous for my point of view... This question comes in a more general context where I am trying to understand why deriving a composition is effectively a matrix product. So by understanding this formula, I am able to see why building matrix of derivatives is a good tool to compute derivatives by matrix multiplication. Thanks !",,"['multivariable-calculus', 'partial-derivative', 'chain-rule']"
77,Find vector field given curl,Find vector field given curl,,"I have an equation $\nabla \times \vec{B} = \mu_{0}\vec{J}$, where $\vec{J} = \left\langle f(x,y), g(x,y), 0 \right\rangle$ and need to solve for $\vec{B}$. I've looked elsewhere on here for how to ""undo"" the curl operator, but every answer I've found has been very theoretical and abstract, and I was hoping to get a more concrete explanation for this particular problem. Breaking down the curl of $\vec{B}$ into components and partial derivatives, I got: $$\frac{\partial B_{2}}{\partial x} - \frac{\partial B_{1}}{\partial y} = 0$$$$\frac{\partial B_{3}}{\partial y} - \frac{\partial B_{2}}{\partial z} = \mu_{0} f(x, y)$$$$\frac{\partial B_{1}}{\partial z} - \frac{\partial B_{3}}{\partial x} = \mu_{0} g(x, y)$$ And from here I'm stuck. Other examples with explicit functions have used guesswork to figure out the components, but I'm having trouble with the arbitrary functions of $f(x,y)$ and $g(x,y)$.","I have an equation $\nabla \times \vec{B} = \mu_{0}\vec{J}$, where $\vec{J} = \left\langle f(x,y), g(x,y), 0 \right\rangle$ and need to solve for $\vec{B}$. I've looked elsewhere on here for how to ""undo"" the curl operator, but every answer I've found has been very theoretical and abstract, and I was hoping to get a more concrete explanation for this particular problem. Breaking down the curl of $\vec{B}$ into components and partial derivatives, I got: $$\frac{\partial B_{2}}{\partial x} - \frac{\partial B_{1}}{\partial y} = 0$$$$\frac{\partial B_{3}}{\partial y} - \frac{\partial B_{2}}{\partial z} = \mu_{0} f(x, y)$$$$\frac{\partial B_{1}}{\partial z} - \frac{\partial B_{3}}{\partial x} = \mu_{0} g(x, y)$$ And from here I'm stuck. Other examples with explicit functions have used guesswork to figure out the components, but I'm having trouble with the arbitrary functions of $f(x,y)$ and $g(x,y)$.",,"['multivariable-calculus', 'partial-differential-equations', 'vector-analysis', 'vector-fields', 'curl']"
78,"Why is $(0, 0)$ not a minimum of $f(x, y) = (y-3x^2)(y-x^2)$?",Why is  not a minimum of ?,"(0, 0) f(x, y) = (y-3x^2)(y-x^2)","There is an exercise in my lists about those functions: $$f(x, y) = (y-3x^2)(y-x^2) = 3 x^4-4 x^2 y+y^2$$ $$g(t) = f(vt) = f(at, bt); a, b \in \mathbf{R}$$ It asks me to prove that $t = 0$ is a local minimum of $g$ for all $a, b \in \mathbf{R}$ I did it easily: $$g(t) = 3 a^4 t^4-4 a^2 t^2 b t+b^2 t^2$$ $$g'(t) = 2 b^2 t-12 a^2 b t^2+12 a^4 t^3$$ $$g''(t) = 2 b^2-24 a^2 b t+36 a^4 t^2$$ It is a critical point: $$g'(0) = 0; \forall a, b$$ Its increasing for all a, b: $$g''(0) = 2b^2 > 0; \forall b \ne 0$$ and $$b = 0 \implies g(t) = 3 a^4 t^4$$ which has only one minimum, at $0$, and no maximum However, it also asks me to prove that $(0, 0)$ is not a local minimum of $f$.  How can this be possible? I mean, if $(0, 0)$ is a minimum over every straight line that passes through it, then, in this point, $f$ should be increasing in all directions, no?","There is an exercise in my lists about those functions: $$f(x, y) = (y-3x^2)(y-x^2) = 3 x^4-4 x^2 y+y^2$$ $$g(t) = f(vt) = f(at, bt); a, b \in \mathbf{R}$$ It asks me to prove that $t = 0$ is a local minimum of $g$ for all $a, b \in \mathbf{R}$ I did it easily: $$g(t) = 3 a^4 t^4-4 a^2 t^2 b t+b^2 t^2$$ $$g'(t) = 2 b^2 t-12 a^2 b t^2+12 a^4 t^3$$ $$g''(t) = 2 b^2-24 a^2 b t+36 a^4 t^2$$ It is a critical point: $$g'(0) = 0; \forall a, b$$ Its increasing for all a, b: $$g''(0) = 2b^2 > 0; \forall b \ne 0$$ and $$b = 0 \implies g(t) = 3 a^4 t^4$$ which has only one minimum, at $0$, and no maximum However, it also asks me to prove that $(0, 0)$ is not a local minimum of $f$.  How can this be possible? I mean, if $(0, 0)$ is a minimum over every straight line that passes through it, then, in this point, $f$ should be increasing in all directions, no?",,"['multivariable-calculus', 'optimization']"
79,Gradient And Hessian Of General 2-Norm,Gradient And Hessian Of General 2-Norm,,"Given $f(\mathbf{x}) = \|\mathbf{Ax}\|_2 = (\mathbf{x}^\mathrm{T} \mathbf{A}^\mathrm{T} \mathbf{Ax} )^{1/2}$, $\nabla f(\mathbf{x}) = \frac {\mathbf{A}^\mathrm{T} \mathbf{Ax}} {\|\mathbf{Ax}\|_2} = \frac {\mathbf{A}^\mathrm{T} \mathbf{Ax}} {(\mathbf{x}^\mathrm{T} \mathbf{A}^\mathrm{T} \mathbf{Ax} )^{1/2}}$ $\nabla^2 f(\mathbf{x}) = \frac { (\mathbf{x}^\mathrm{T} \mathbf{A}^\mathrm{T} \mathbf{Ax} )^{1/2} \cdot \mathbf{A}^\mathrm{T} \mathbf{A} - (\mathbf{A}^\mathrm{T} \mathbf{Ax})^\mathrm{T} (\mathbf{x}^\mathrm{T} \mathbf{A}^\mathrm{T} \mathbf{Ax} )^{-1/2} \mathbf{A}^\mathrm{T} \mathbf{Ax} } {(\mathbf{x}^\mathrm{T} \mathbf{A}^\mathrm{T} \mathbf{Ax} ) } = \frac { \mathbf{A}^\mathrm{T} \mathbf{A} } { (\mathbf{x}^\mathrm{T} \mathbf{A}^\mathrm{T} \mathbf{Ax} )^{1/2}} - \frac {\mathbf{x}^\mathrm{T} \mathbf{A}^\mathrm{T} \mathbf{A} \mathbf{A}^\mathrm{T} \mathbf{Ax} } { (\mathbf{x}^\mathrm{T} \mathbf{A}^\mathrm{T} \mathbf{Ax} )^{3/2} }$ I guess I am looking for confirmation that I have done the above correctly. The dimensions match up except for the second term of the Hessian is a scalar, which makes me think that something is missing. Edit: Also, the last equality reduces to $\nabla^2 f(\mathbf{x}) = \frac {\mathbf{A}^\mathrm{T} \mathbf{A} - \nabla f(\mathbf{x})^\mathrm{T} \nabla f(\mathbf{x})} {\|\mathbf{Ax}\|_2}$","Given $f(\mathbf{x}) = \|\mathbf{Ax}\|_2 = (\mathbf{x}^\mathrm{T} \mathbf{A}^\mathrm{T} \mathbf{Ax} )^{1/2}$, $\nabla f(\mathbf{x}) = \frac {\mathbf{A}^\mathrm{T} \mathbf{Ax}} {\|\mathbf{Ax}\|_2} = \frac {\mathbf{A}^\mathrm{T} \mathbf{Ax}} {(\mathbf{x}^\mathrm{T} \mathbf{A}^\mathrm{T} \mathbf{Ax} )^{1/2}}$ $\nabla^2 f(\mathbf{x}) = \frac { (\mathbf{x}^\mathrm{T} \mathbf{A}^\mathrm{T} \mathbf{Ax} )^{1/2} \cdot \mathbf{A}^\mathrm{T} \mathbf{A} - (\mathbf{A}^\mathrm{T} \mathbf{Ax})^\mathrm{T} (\mathbf{x}^\mathrm{T} \mathbf{A}^\mathrm{T} \mathbf{Ax} )^{-1/2} \mathbf{A}^\mathrm{T} \mathbf{Ax} } {(\mathbf{x}^\mathrm{T} \mathbf{A}^\mathrm{T} \mathbf{Ax} ) } = \frac { \mathbf{A}^\mathrm{T} \mathbf{A} } { (\mathbf{x}^\mathrm{T} \mathbf{A}^\mathrm{T} \mathbf{Ax} )^{1/2}} - \frac {\mathbf{x}^\mathrm{T} \mathbf{A}^\mathrm{T} \mathbf{A} \mathbf{A}^\mathrm{T} \mathbf{Ax} } { (\mathbf{x}^\mathrm{T} \mathbf{A}^\mathrm{T} \mathbf{Ax} )^{3/2} }$ I guess I am looking for confirmation that I have done the above correctly. The dimensions match up except for the second term of the Hessian is a scalar, which makes me think that something is missing. Edit: Also, the last equality reduces to $\nabla^2 f(\mathbf{x}) = \frac {\mathbf{A}^\mathrm{T} \mathbf{A} - \nabla f(\mathbf{x})^\mathrm{T} \nabla f(\mathbf{x})} {\|\mathbf{Ax}\|_2}$",,['multivariable-calculus']
80,A local diffeomorphism of Euclidean space that is not a diffeomorphism,A local diffeomorphism of Euclidean space that is not a diffeomorphism,,Could someone give me an example of a local diffeomorphism from $\mathbb{R}^p$ to $\mathbb{R}^p$ (function of class say $C^k$ with an invertible differential map in each point) that is not a diffeomorphism.. in the real line (1 dim case) that would mean a function with a continuous non null derivative on an open $V$ of $\mathbb{R}$ that is not bijective which does not make sense thus any local diffeomorphism on the real line is a diffeo.. Could one give me a counterexample in a higher dimension?,Could someone give me an example of a local diffeomorphism from $\mathbb{R}^p$ to $\mathbb{R}^p$ (function of class say $C^k$ with an invertible differential map in each point) that is not a diffeomorphism.. in the real line (1 dim case) that would mean a function with a continuous non null derivative on an open $V$ of $\mathbb{R}$ that is not bijective which does not make sense thus any local diffeomorphism on the real line is a diffeo.. Could one give me a counterexample in a higher dimension?,,"['differential-geometry', 'multivariable-calculus']"
81,How to find the Riemannian Distance on the Sphere $S^n\subset \Bbb{R}^{n+1}$,How to find the Riemannian Distance on the Sphere,S^n\subset \Bbb{R}^{n+1},"I am trying to prove that, if $\rho:S^n\times S^n\to \Bbb{R}$ is the distance induced by the Riemannian metric on the sphere $S^n\subset \Bbb{R}^{n+1}$, then $$\rho(p,q)=\arccos \langle p,q\rangle,\,\,\,\forall p,q\in S^n$$ with $\arccos$ defined from $[-1,1]$ to $[0,\pi]$. If $p=q$, then the formula is trivial. We have then two cases: $p=-q$ and $p\neq- q$. I am having trouble even with the apparently easier case, $p=-q$. If $p=-q$, then let $v\in S^n$ be any vector orthogonal to $p$. Then $\alpha:[0,\pi]\to S^n$, $\alpha(t)=(\cos t)p+(\sin t)v$ is a well defined differentiable path from $p$ to $q$ such that $\ell_0^\pi(\alpha)=\pi$ (""length of $\alpha$""). This guarantees that $\rho(p,q)\leq \pi (=\arccos \langle p,q\rangle$, in this case). In order to show that $\rho(p,q)=\pi$, I must consider an arbitrary differentiable by parts path $\beta:[a,b]\to S^n$ from $p$ to $q$ and show that $$\pi\leq \ell_a^b(\beta)=\int_a^b|\beta'(t)|\,dt.$$ I've done some geometric observations and computations, but without success. How can I do this? I wish I could do it without using any facts about geodesics (they appear later in the book I'm studying with).","I am trying to prove that, if $\rho:S^n\times S^n\to \Bbb{R}$ is the distance induced by the Riemannian metric on the sphere $S^n\subset \Bbb{R}^{n+1}$, then $$\rho(p,q)=\arccos \langle p,q\rangle,\,\,\,\forall p,q\in S^n$$ with $\arccos$ defined from $[-1,1]$ to $[0,\pi]$. If $p=q$, then the formula is trivial. We have then two cases: $p=-q$ and $p\neq- q$. I am having trouble even with the apparently easier case, $p=-q$. If $p=-q$, then let $v\in S^n$ be any vector orthogonal to $p$. Then $\alpha:[0,\pi]\to S^n$, $\alpha(t)=(\cos t)p+(\sin t)v$ is a well defined differentiable path from $p$ to $q$ such that $\ell_0^\pi(\alpha)=\pi$ (""length of $\alpha$""). This guarantees that $\rho(p,q)\leq \pi (=\arccos \langle p,q\rangle$, in this case). In order to show that $\rho(p,q)=\pi$, I must consider an arbitrary differentiable by parts path $\beta:[a,b]\to S^n$ from $p$ to $q$ and show that $$\pi\leq \ell_a^b(\beta)=\int_a^b|\beta'(t)|\,dt.$$ I've done some geometric observations and computations, but without success. How can I do this? I wish I could do it without using any facts about geodesics (they appear later in the book I'm studying with).",,"['multivariable-calculus', 'riemannian-geometry']"
82,"Derivative of $f(x,y)$ with respect to another function of two variables $k(x,y)$",Derivative of  with respect to another function of two variables,"f(x,y) k(x,y)","Suppose that we have a function $f(x,y)$ of two variables: $$f(x,y) = g(x) + h(y) + 5(x-y) = x^2 + y^2 + 5(x-y)$$ where $g(x) = x^2$ and $h(y) = y^2$ are also functions of $x$ and $y$, respectively. How do I take the partial derivative of $f(x,y)$ with respect to another multivariate function $k(x,y) = x-y$, so that: $$\frac{{\partial f(x,y)}}{{\partial k(x,y)}} = 5$$ I suppose that this would be a type of directional derivative , or perhaps even a functional derivative .  Would the chain rule be applied in this type of situation? Can this type of calculation be done in a numerical fashion (i.e. using finite difference derivatives) for $f(x,y)$ as a discrete calculated matrix of values, knowing only $f(x,y)$ at each point of the matrix and variables $x$, $y$?","Suppose that we have a function $f(x,y)$ of two variables: $$f(x,y) = g(x) + h(y) + 5(x-y) = x^2 + y^2 + 5(x-y)$$ where $g(x) = x^2$ and $h(y) = y^2$ are also functions of $x$ and $y$, respectively. How do I take the partial derivative of $f(x,y)$ with respect to another multivariate function $k(x,y) = x-y$, so that: $$\frac{{\partial f(x,y)}}{{\partial k(x,y)}} = 5$$ I suppose that this would be a type of directional derivative , or perhaps even a functional derivative .  Would the chain rule be applied in this type of situation? Can this type of calculation be done in a numerical fashion (i.e. using finite difference derivatives) for $f(x,y)$ as a discrete calculated matrix of values, knowing only $f(x,y)$ at each point of the matrix and variables $x$, $y$?",,"['multivariable-calculus', 'numerical-methods', 'calculus-of-variations']"
83,How can I prove $\nabla \cdot \color{green}{(\mathbf{F} {\times} \mathbf{G})} $ with Einstein Summation Notation?,How can I prove  with Einstein Summation Notation?,\nabla \cdot \color{green}{(\mathbf{F} {\times} \mathbf{G})} ,"Source: Stewart. Calculus: Early Transcendentals (6 edn 2007) . p. 1068. §16.5, Exercise #27. $\nabla \cdot \color{green}{(\mathbf{F} {\times} \mathbf{G})} = \partial_h\color{green}{\epsilon_{hij}F_iG_j}$ $ = \epsilon_{hij}\partial_h[F_iG_j]$ $ = \color{purple}{\epsilon_{hij}G_j\partial_hF_i} \color{red}{+} \epsilon_{hij}F_i\partial_hG_j $ $\color{purple}{1. \text{How do I determine whether } \epsilon_{hij}G_j\partial_hF_i =   \mathbf{G} \times \nabla \mathbf{F} \text{ or } \mathbf{G} \cdot (\nabla \times \mathbf{F}) ?}$ $\color{darkred}{\text{3. The answer shows a negative sign $-$, not +. What did I miss?}}$ I read this . My guess One answer to my question 1 is that the divergence of a vector field, ie $\nabla \mathbf{F}$ , is always a scalar. So taking its cross product is nonsensical. This is why $ \color{purple}{ \mathbf{G} \times \nabla \mathbf{F} } $ is wrong. First Attempt Recanted due to celtschk's Answer: I tried to compute $\operatorname{div}(\mathbf{F} \times \mathbf{G})$ by considering the $j$ th component term in the sum of the divergence operator: $\color{red}{[}\nabla \cdot \color{green}{(\mathbf{F} {\times} \mathbf{G})}\color{red}{]}_\color{red}{\LARGE{j}} = \partial_\color{red}{\LARGE{j}}{\epsilon_{hi\color{red}{\LARGE{j}}}}F_{\huge{\color{green}{i}}}G_{\huge{\color{green}{i}}} = \color{green}{\epsilon_{hi\color{red}{\LARGE{j}}}}\partial_\color{red}{\LARGE{j}}[F_{\huge{\color{green}{i}}}G_{\huge{\color{green}{i}}}] = \color{purple}{\epsilon_{hi\color{red}{\LARGE{j}}}G_i\partial_\color{red}{\LARGE{j}}F_i} \color{brown}{+} \color{gray}{\epsilon_{hi\color{red}{\LARGE{j}}}F_i\partial_\color{red}{\LARGE{j}}G_i} $ Supplement due to Muphrid and celtschk: $1.1.$ Without geometric calculus or wedge products or any more advanced topics, how can I  divine to move $G_h$ to the front, as Muphrid did? $3.1.$ In $\color{purple}{F_i\partial_hG_j}$ , the order of the subscripts is $\color{purple}{(i, h, j)}$ But in $\epsilon_{hij}$ , the order is $(h, i, j)$ . Thus, I must rotate $(h, i, j)$ to obtain $\color{purple}{(i, h, j)},$ by swapping $\color{purple}{h}$ and $\color{purple}{i}$ once $\Longrightarrow \epsilon_{hij} = -\epsilon_{\color{purple}{ihj}} $ . This yields the required answer. Must [the order of the subscripts of the components] = [the order of the subscripts in the Levi-Civita symbol]? Why does this work? Supplement due to Muphrid's Comment on July 17th: $3.2.$ How can I divine to pull $G_j$ to the front, and NOT the back? You did: $\color{purple}{\epsilon_{hij}G_j\partial_hF_i} = \color{purple}{(\epsilon_{hij}\partial_hF_i)G_j} = \color{purple}{G_j\epsilon_{\color{magenta}{jhi}}\partial_hF_i = \mathbf{G} \cdot (\nabla \times \mathbf{F})}. $ I would've computed: $\color{purple}{\epsilon_{hij}G_j\partial_hF_i} = \color{purple}{(\epsilon_{hij}\partial_hF_i)G_j = (\nabla \times \mathbf{F}) \cdot \mathbf{G}. \text{ But this is wrong! }}$ Supplement due to celtschk & Muphrid's Comments on July 22nd From celtschk: It is also not strictly required that the order of indices maps the order in the vector expression. However it helps to do that because it's less error prone... From Muphrid: Yes, the order of indices in the Levi-Civita matters... $3.3. $ Don't these two comments contradict? Must [the order of the subscripts of the components] = [the order of the subscripts in the Levi-Civita symbol] ? $3.4.$ Ought the order for both match? Why or why not?","Source: Stewart. Calculus: Early Transcendentals (6 edn 2007) . p. 1068. §16.5, Exercise #27. I read this . My guess One answer to my question 1 is that the divergence of a vector field, ie , is always a scalar. So taking its cross product is nonsensical. This is why is wrong. First Attempt Recanted due to celtschk's Answer: I tried to compute by considering the th component term in the sum of the divergence operator: Supplement due to Muphrid and celtschk: Without geometric calculus or wedge products or any more advanced topics, how can I  divine to move to the front, as Muphrid did? In , the order of the subscripts is But in , the order is . Thus, I must rotate to obtain by swapping and once . This yields the required answer. Must [the order of the subscripts of the components] = [the order of the subscripts in the Levi-Civita symbol]? Why does this work? Supplement due to Muphrid's Comment on July 17th: How can I divine to pull to the front, and NOT the back? You did: I would've computed: Supplement due to celtschk & Muphrid's Comments on July 22nd From celtschk: It is also not strictly required that the order of indices maps the order in the vector expression. However it helps to do that because it's less error prone... From Muphrid: Yes, the order of indices in the Levi-Civita matters... Don't these two comments contradict? Must [the order of the subscripts of the components] = [the order of the subscripts in the Levi-Civita symbol] ? Ought the order for both match? Why or why not?","\nabla \cdot \color{green}{(\mathbf{F} {\times} \mathbf{G})} = \partial_h\color{green}{\epsilon_{hij}F_iG_j}  = \epsilon_{hij}\partial_h[F_iG_j]  = \color{purple}{\epsilon_{hij}G_j\partial_hF_i} \color{red}{+} \epsilon_{hij}F_i\partial_hG_j  \color{purple}{1. \text{How do I determine whether } \epsilon_{hij}G_j\partial_hF_i =   \mathbf{G} \times \nabla \mathbf{F} \text{ or } \mathbf{G} \cdot (\nabla \times \mathbf{F}) ?} \color{darkred}{\text{3. The answer shows a negative sign -, not +. What did I miss?}} \nabla \mathbf{F}  \color{purple}{ \mathbf{G} \times \nabla \mathbf{F} }  \operatorname{div}(\mathbf{F} \times \mathbf{G}) j \color{red}{[}\nabla \cdot \color{green}{(\mathbf{F} {\times} \mathbf{G})}\color{red}{]}_\color{red}{\LARGE{j}} = \partial_\color{red}{\LARGE{j}}{\epsilon_{hi\color{red}{\LARGE{j}}}}F_{\huge{\color{green}{i}}}G_{\huge{\color{green}{i}}} = \color{green}{\epsilon_{hi\color{red}{\LARGE{j}}}}\partial_\color{red}{\LARGE{j}}[F_{\huge{\color{green}{i}}}G_{\huge{\color{green}{i}}}] = \color{purple}{\epsilon_{hi\color{red}{\LARGE{j}}}G_i\partial_\color{red}{\LARGE{j}}F_i} \color{brown}{+} \color{gray}{\epsilon_{hi\color{red}{\LARGE{j}}}F_i\partial_\color{red}{\LARGE{j}}G_i}  1.1. G_h 3.1. \color{purple}{F_i\partial_hG_j} \color{purple}{(i, h, j)} \epsilon_{hij} (h, i, j) (h, i, j) \color{purple}{(i, h, j)}, \color{purple}{h} \color{purple}{i} \Longrightarrow \epsilon_{hij} = -\epsilon_{\color{purple}{ihj}}  3.2. G_j \color{purple}{\epsilon_{hij}G_j\partial_hF_i} = \color{purple}{(\epsilon_{hij}\partial_hF_i)G_j} = \color{purple}{G_j\epsilon_{\color{magenta}{jhi}}\partial_hF_i = \mathbf{G} \cdot (\nabla \times \mathbf{F})}.  \color{purple}{\epsilon_{hij}G_j\partial_hF_i} = \color{purple}{(\epsilon_{hij}\partial_hF_i)G_j = (\nabla \times \mathbf{F}) \cdot \mathbf{G}. \text{ But this is wrong! }} 3.3.  3.4.",['multivariable-calculus']
84,Un-curl operator?,Un-curl operator?,,"I would like to use Stokes' Theorem to find the area of a surface over a given region. This is given by: $A = \oint\vec{F}\centerdot d\vec{r}$ but only if the following condition holds: $(\vec{\triangledown}\times\vec{F})\centerdot \vec{n} = 1$ where $\vec{n}$ is the normal to the surface. How do I come up with a vector field, $\vec{F}$, that satisfies this condition?  I found a paper that discusses an inverse-curl operator here , but this is only useful if I know what $(\vec{\triangledown}\times\vec{F})$ is and need to find $\vec{F}$.  Any ideas?","I would like to use Stokes' Theorem to find the area of a surface over a given region. This is given by: $A = \oint\vec{F}\centerdot d\vec{r}$ but only if the following condition holds: $(\vec{\triangledown}\times\vec{F})\centerdot \vec{n} = 1$ where $\vec{n}$ is the normal to the surface. How do I come up with a vector field, $\vec{F}$, that satisfies this condition?  I found a paper that discusses an inverse-curl operator here , but this is only useful if I know what $(\vec{\triangledown}\times\vec{F})$ is and need to find $\vec{F}$.  Any ideas?",,['multivariable-calculus']
85,Intuition behind curl identity $ \nabla\times (\nabla\times A)=\nabla (\nabla \cdot A)-\nabla ^2A $,Intuition behind curl identity, \nabla\times (\nabla\times A)=\nabla (\nabla \cdot A)-\nabla ^2A ,"Is there any clear intuition behind the identity $$ \nabla\times (\nabla\times A)=\nabla (\nabla \cdot A)-\nabla ^2A $$ Though the result is useful and not difficult to derive, it doesn't quite appear obvious to me why this should be the case. How could one explain this result in intuitive terms? What does it really mean? Thank you.","Is there any clear intuition behind the identity $$ \nabla\times (\nabla\times A)=\nabla (\nabla \cdot A)-\nabla ^2A $$ Though the result is useful and not difficult to derive, it doesn't quite appear obvious to me why this should be the case. How could one explain this result in intuitive terms? What does it really mean? Thank you.",,"['multivariable-calculus', 'soft-question', 'vector-analysis', 'intuition']"
86,Taking the derivative of a differential equation,Taking the derivative of a differential equation,,"My book jumps from $$\frac{\partial f}{\partial x}(x, g(x)) + \frac{\partial f}{\partial y}(x, g(x))g'(x) = 0 $$ to $$\frac{\partial^2 f}{\partial x^2}(x, g(x)) + 2 \cdot \frac{\partial f}{\partial x \partial y} (x, g(x))g'(x) + \frac{\partial^2 f}{\partial y^2}(x, g(x))(g'(x))^2 + \frac{\partial f}{\partial y}(x, g(x))g''(x) = 0.$$ It is left as an exercise to verify that this new equality can be obtained by differentiating both sides of the first equation. I've been trying to do this, but I haven't been able to get to the desired result. I'm sort of new to partial derivatives, and I would really appreciate it if someone can show me the steps that are taken when differentiating the first equation. I'm pretty sure my setup itself is wrong. I've looked at many examples now, but I still haven't been able to get anywhere, since they are not too similar to what I have I would really appreciate any help. Thanks My try: $$\frac{\partial}{\partial x}\left(\frac{\partial f}{\partial x}(x, g(x)) + \frac{\partial f}{\partial y}(x, g(x))g'(x)\right) $$ $$= \underbrace{\frac{\partial}{\partial x}\left(\frac{\partial f}{\partial x}(x,g(x)) \right)}_{\text{Term 1}} + \underbrace{\frac{\partial}{\partial x}\left(\frac{\partial f}{\partial y}(x, g(x))g'(x)\right)}_{\text{Term 2}}$$ Now computing Term 1: $$\frac{\partial}{\partial x}\left(\frac{\partial f}{\partial x}(x,g(x)) \right) = \frac{\partial^{2}f}{\partial x^{2}}(x, g(x)) \cdot \text{ some chain rule term} $$ What would the chain rule term be? I know in single-variable calculus, if you're doing the derivative of $f(g(x))$ , then you need to multiply by $g'(x)$ . But here, there are two variables.","My book jumps from to It is left as an exercise to verify that this new equality can be obtained by differentiating both sides of the first equation. I've been trying to do this, but I haven't been able to get to the desired result. I'm sort of new to partial derivatives, and I would really appreciate it if someone can show me the steps that are taken when differentiating the first equation. I'm pretty sure my setup itself is wrong. I've looked at many examples now, but I still haven't been able to get anywhere, since they are not too similar to what I have I would really appreciate any help. Thanks My try: Now computing Term 1: What would the chain rule term be? I know in single-variable calculus, if you're doing the derivative of , then you need to multiply by . But here, there are two variables.","\frac{\partial f}{\partial x}(x, g(x)) + \frac{\partial f}{\partial y}(x, g(x))g'(x) = 0  \frac{\partial^2 f}{\partial x^2}(x, g(x)) + 2 \cdot \frac{\partial f}{\partial x \partial y} (x, g(x))g'(x) + \frac{\partial^2 f}{\partial y^2}(x, g(x))(g'(x))^2 + \frac{\partial f}{\partial y}(x, g(x))g''(x) = 0. \frac{\partial}{\partial x}\left(\frac{\partial f}{\partial x}(x, g(x)) + \frac{\partial f}{\partial y}(x, g(x))g'(x)\right)  = \underbrace{\frac{\partial}{\partial x}\left(\frac{\partial f}{\partial x}(x,g(x)) \right)}_{\text{Term 1}} + \underbrace{\frac{\partial}{\partial x}\left(\frac{\partial f}{\partial y}(x, g(x))g'(x)\right)}_{\text{Term 2}} \frac{\partial}{\partial x}\left(\frac{\partial f}{\partial x}(x,g(x)) \right) = \frac{\partial^{2}f}{\partial x^{2}}(x, g(x)) \cdot \text{ some chain rule term}  f(g(x)) g'(x)",['multivariable-calculus']
87,Problem in Hamiltonian system,Problem in Hamiltonian system,,"Not sure if this is too much physics to be here... Consider $$H:\mathbb{R}^{2N+1}\rightarrow\mathbb{R}$$ of class $C^2$, let $H(x,y,z)$ such that $x\in\mathbb{R}^N$, $y\in\mathbb{R}^N$ and $z\in\mathbb{R}$. Let $\varphi$ be the flow associate with the Hamiltonian system $$\dot{x}_i=-\frac{\partial H}{\partial y_i}$$ $$\dot{y}_i=\frac{\partial H}{\partial x_i}$$ $$\dot{z}=1$$ I have to prove that if $\eta$ is a 1-form given by $\eta=\sum_{i=1}^Nx_i \ dy_i-H \ dz$ and $c$ is a closed curve in $\mathbb{R}^{2N+1}$, then for all $s$ we have $$\int_{\varphi(s,c)}\eta=\int_c\eta.$$ Thank you in advance!","Not sure if this is too much physics to be here... Consider $$H:\mathbb{R}^{2N+1}\rightarrow\mathbb{R}$$ of class $C^2$, let $H(x,y,z)$ such that $x\in\mathbb{R}^N$, $y\in\mathbb{R}^N$ and $z\in\mathbb{R}$. Let $\varphi$ be the flow associate with the Hamiltonian system $$\dot{x}_i=-\frac{\partial H}{\partial y_i}$$ $$\dot{y}_i=\frac{\partial H}{\partial x_i}$$ $$\dot{z}=1$$ I have to prove that if $\eta$ is a 1-form given by $\eta=\sum_{i=1}^Nx_i \ dy_i-H \ dz$ and $c$ is a closed curve in $\mathbb{R}^{2N+1}$, then for all $s$ we have $$\int_{\varphi(s,c)}\eta=\int_c\eta.$$ Thank you in advance!",,"['multivariable-calculus', 'differential-geometry', 'manifolds', 'mathematical-physics', 'hamilton-equations']"
88,"Definition of smoothness ""up to boundary""","Definition of smoothness ""up to boundary""",,"Let $U\subseteq \mathbb{R}^n$ be an open set and let $f\in\mathcal{C}^k(U)$ for some positive integer $k$. Are the following definitions of $\mathcal{C}^k$ regularity ""up to boundary"" equivalent? (1) There exists an open set $V$ containing $U$ and a $\mathcal{C}^k$ extension of $f$ to $V$. (2) $D^{\alpha}f$ is uniformly continuous on every bounded subset of $U$ for each $|\alpha|\leq k$. I have always interpreted the class $\mathcal{C}^k\left(\overline{U}\right)$ using definition (1) but Evans-PDE uses definition  (2) in its appendix (at least in the edition I am working from). I can easily show that (1)$\Rightarrow$(2) but it would be nice to know if Evans definition is more general than mine.","Let $U\subseteq \mathbb{R}^n$ be an open set and let $f\in\mathcal{C}^k(U)$ for some positive integer $k$. Are the following definitions of $\mathcal{C}^k$ regularity ""up to boundary"" equivalent? (1) There exists an open set $V$ containing $U$ and a $\mathcal{C}^k$ extension of $f$ to $V$. (2) $D^{\alpha}f$ is uniformly continuous on every bounded subset of $U$ for each $|\alpha|\leq k$. I have always interpreted the class $\mathcal{C}^k\left(\overline{U}\right)$ using definition (1) but Evans-PDE uses definition  (2) in its appendix (at least in the edition I am working from). I can easily show that (1)$\Rightarrow$(2) but it would be nice to know if Evans definition is more general than mine.",,"['differential-geometry', 'multivariable-calculus', 'partial-differential-equations']"
89,How do you graph $x + y + z = 1$ without using graphing devices?,How do you graph  without using graphing devices?,x + y + z = 1,"How can I graph $x + y + z = 1$ without using graphing devices? I equal $z = 0$ to find the graph on the xy plane. So I got a line, $y = 1-x$ But when I equal 0 for either the $x$ or the $y,$ I get $z = 1-y$ or $z = 1-x$ , and those are two different lines from different angles. Different graphing websites were telling me different answers... Please don't show some crazy and complicated methods to graph this. I just want simple steps just as plugging $x,y,z$ as zeros and etc.","How can I graph $x + y + z = 1$ without using graphing devices? I equal $z = 0$ to find the graph on the xy plane. So I got a line, $y = 1-x$ But when I equal 0 for either the $x$ or the $y,$ I get $z = 1-y$ or $z = 1-x$ , and those are two different lines from different angles. Different graphing websites were telling me different answers... Please don't show some crazy and complicated methods to graph this. I just want simple steps just as plugging $x,y,z$ as zeros and etc.",,['multivariable-calculus']
90,Geometric or intuitive proof of the symmetry of second partial derivatives,Geometric or intuitive proof of the symmetry of second partial derivatives,,"What was given in my calc book is a ""consider the function"" proof. That is, the author gives a function out of the blue and would deduce all the nice properties from it. I'd prefer a proof which is motivated (perhaps, intuitive) - you see how the proof is crafted in the mind of the person. So my question is a geometric or intuitive proof of $$\frac{\partial ^2 f}{\partial x \, \partial y} = \frac{\partial^2 f}{\partial y \, \partial x}$$","What was given in my calc book is a ""consider the function"" proof. That is, the author gives a function out of the blue and would deduce all the nice properties from it. I'd prefer a proof which is motivated (perhaps, intuitive) - you see how the proof is crafted in the mind of the person. So my question is a geometric or intuitive proof of $$\frac{\partial ^2 f}{\partial x \, \partial y} = \frac{\partial^2 f}{\partial y \, \partial x}$$",,['multivariable-calculus']
91,Product rule for the derivative of a dot product.,Product rule for the derivative of a dot product.,,"I can't find the reason for this simplification, I understand that the dot product of a vector with itself would give the magnitude of that squared, so that explains the v squared. What I don't understand is where did the 2 under the ""m"" come from. (The bold v's are vectors.) $$m\int \frac{d\mathbf{v}}{dt} \cdot \mathbf{v} dt = \frac{m}{2}\int \frac{d}{dt}(\mathbf{v}^2)dt$$ Thanks. Maybe the book's just wrong and that 2 should't be there...","I can't find the reason for this simplification, I understand that the dot product of a vector with itself would give the magnitude of that squared, so that explains the v squared. What I don't understand is where did the 2 under the ""m"" come from. (The bold v's are vectors.) $$m\int \frac{d\mathbf{v}}{dt} \cdot \mathbf{v} dt = \frac{m}{2}\int \frac{d}{dt}(\mathbf{v}^2)dt$$ Thanks. Maybe the book's just wrong and that 2 should't be there...",,"['multivariable-calculus', 'vector-analysis']"
92,Do functions with the same gradient differ by a constant?,Do functions with the same gradient differ by a constant?,,"Let $f,g:\mathbb{R}^n\to\mathbb{R}$ be such that $\nabla f=\nabla g$ . I believe this implies that $f$ and $g$ only differ by a constant, like in the one-dimensional case. But I'm not sure how to prove it. If it's indeed true, can you give me a hint? Thanks!","Let be such that . I believe this implies that and only differ by a constant, like in the one-dimensional case. But I'm not sure how to prove it. If it's indeed true, can you give me a hint? Thanks!","f,g:\mathbb{R}^n\to\mathbb{R} \nabla f=\nabla g f g","['multivariable-calculus', 'vector-analysis']"
93,Can a vector field always be written as a cross-product of two other vector fields?,Can a vector field always be written as a cross-product of two other vector fields?,,"As the title suggests, can any arbitrary conservative vector field, $\bf{F}$ $= \langle P,Q,R \rangle$, where the component functions are functions of $(x,y,z)$, always be written as a cross-product of two other vector fields, namely $\bf{A} \times \bf{B}$? Going further, let $\bf{A}$ $= \nabla a$, where $a = a(x,y,z)$ and has continuous first partial derivatives. Also, let $\bf{B}$ $=\langle b_1,b_2,b_3\rangle$ where the component functions are functions of $(x,y,z)$. What I have so far is the following: $\cases{ P=\frac{\partial a}{\partial y}b_3-\frac{\partial a}{\partial z}b_2\\ \\ Q=\frac{\partial a}{\partial z}b_1-\frac{\partial a}{\partial x}b_3\\ \\ R=\frac{\partial a}{\partial x}b_2-\frac{\partial a}{\partial y}b_1 }$ The reason I am asking is that I recently solved the following question: Let $f$ be a smooth function defined on $\mathbb{R}^3$. Let $S$ be the level surface, $\{(x,y,z):f(x,y,z)=c\}$ for some $c \in \mathbb{R}$. Assume $\nabla f$ is never the zero vector on $S$ and let $\bf{F}$ $=\nabla f$. Show that the surface integral over $S$ of $\bf{F} \times \bf{G}$ is $0$ for any $\bf{G}$. ($\star$) And I thought I could use this property if I can take a complicated surface integral and attempt to break it down to this case to easily see answers that are zero. So I suppose I am asking for the conditions that arbitrary vector fields need to satisfy to use the result of the property in the above problem. Thanks! $\bf{EDIT}$ Summary of questions: Q1. Can a vector field always be written as a cross-product of two other vector fields? $\bf{Answered.}$ Q2. Given that $\bf{F}$ $=\nabla f$ can be written as a cross-product of two vector fields, what other conditions are necessary to use the result of $(\star)$?    $\bf{Answered.}$","As the title suggests, can any arbitrary conservative vector field, $\bf{F}$ $= \langle P,Q,R \rangle$, where the component functions are functions of $(x,y,z)$, always be written as a cross-product of two other vector fields, namely $\bf{A} \times \bf{B}$? Going further, let $\bf{A}$ $= \nabla a$, where $a = a(x,y,z)$ and has continuous first partial derivatives. Also, let $\bf{B}$ $=\langle b_1,b_2,b_3\rangle$ where the component functions are functions of $(x,y,z)$. What I have so far is the following: $\cases{ P=\frac{\partial a}{\partial y}b_3-\frac{\partial a}{\partial z}b_2\\ \\ Q=\frac{\partial a}{\partial z}b_1-\frac{\partial a}{\partial x}b_3\\ \\ R=\frac{\partial a}{\partial x}b_2-\frac{\partial a}{\partial y}b_1 }$ The reason I am asking is that I recently solved the following question: Let $f$ be a smooth function defined on $\mathbb{R}^3$. Let $S$ be the level surface, $\{(x,y,z):f(x,y,z)=c\}$ for some $c \in \mathbb{R}$. Assume $\nabla f$ is never the zero vector on $S$ and let $\bf{F}$ $=\nabla f$. Show that the surface integral over $S$ of $\bf{F} \times \bf{G}$ is $0$ for any $\bf{G}$. ($\star$) And I thought I could use this property if I can take a complicated surface integral and attempt to break it down to this case to easily see answers that are zero. So I suppose I am asking for the conditions that arbitrary vector fields need to satisfy to use the result of the property in the above problem. Thanks! $\bf{EDIT}$ Summary of questions: Q1. Can a vector field always be written as a cross-product of two other vector fields? $\bf{Answered.}$ Q2. Given that $\bf{F}$ $=\nabla f$ can be written as a cross-product of two vector fields, what other conditions are necessary to use the result of $(\star)$?    $\bf{Answered.}$",,['multivariable-calculus']
94,What is the meaning of evaluating the divergence at a _point_?,What is the meaning of evaluating the divergence at a _point_?,,"Reading this first, Divergence (div) is “flux density”—the amount of flux entering or leaving a point. Think of it as the rate of flux expansion (positive divergence) or flux contraction (negative divergence). If you measure flux in bananas (and c’mon, who doesn’t?), a positive divergence means your location is a source of bananas. You’ve hit the Donkey Kong jackpot. Wikipedia has: In physical terms, the divergence of a three dimensional vector field is the extent to which the vector field flow behaves like a source or a sink at a given point. It is a local measure of its ""outgoingness""—the extent to which there is more exiting an infinitesimal region of space than entering it. What puzzles me is, what's the direction ?  Doesn't divergence need a direction too? For example, say you have this 2d vector field $$ F(x,y) = (x,y) $$ This field is everywhere ""spreading"" from the origin: The divergence is: $$ \nabla \cdot (x,y) = 2 $$ Ie everywhere positive.  Everywhere is a ""source"".  At every point, if you draw an infinitesimally small circle, as the Wikipedia definition has, no matter how small it is, that circle will see more ""stuff"" leaving it than entering it -- ie everywhere is a source. But that only makes sense if you are talking as measured using a vector from the origin .  But nowhere in the definitions of divergence do I see this last bolded comment mentioned, making me uneasy. Take a vector field with values in the opposite direction: $$ F(x,y) = (-x,-y) $$ The divergence everywhere is $$ \nabla \cdot (-x,-y) = -2 $$ Ie everywhere negative.  Everywhere is a ""sink"".  Ie the field ""slows down"" at every point. If you are talking about every point, and the direction of flow is actually towards the origin .  Then and only then, is the function everywhere ""slowing down"". So like, what exactly is the divergence at a point?  Does it really make sense?  Am I right about having to consider the direction as towards the origin in order for the ""sources/sinks"" idea to make sense?  Where is this stated?","Reading this first, Divergence (div) is “flux density”—the amount of flux entering or leaving a point. Think of it as the rate of flux expansion (positive divergence) or flux contraction (negative divergence). If you measure flux in bananas (and c’mon, who doesn’t?), a positive divergence means your location is a source of bananas. You’ve hit the Donkey Kong jackpot. Wikipedia has: In physical terms, the divergence of a three dimensional vector field is the extent to which the vector field flow behaves like a source or a sink at a given point. It is a local measure of its ""outgoingness""—the extent to which there is more exiting an infinitesimal region of space than entering it. What puzzles me is, what's the direction ?  Doesn't divergence need a direction too? For example, say you have this 2d vector field $$ F(x,y) = (x,y) $$ This field is everywhere ""spreading"" from the origin: The divergence is: $$ \nabla \cdot (x,y) = 2 $$ Ie everywhere positive.  Everywhere is a ""source"".  At every point, if you draw an infinitesimally small circle, as the Wikipedia definition has, no matter how small it is, that circle will see more ""stuff"" leaving it than entering it -- ie everywhere is a source. But that only makes sense if you are talking as measured using a vector from the origin .  But nowhere in the definitions of divergence do I see this last bolded comment mentioned, making me uneasy. Take a vector field with values in the opposite direction: $$ F(x,y) = (-x,-y) $$ The divergence everywhere is $$ \nabla \cdot (-x,-y) = -2 $$ Ie everywhere negative.  Everywhere is a ""sink"".  Ie the field ""slows down"" at every point. If you are talking about every point, and the direction of flow is actually towards the origin .  Then and only then, is the function everywhere ""slowing down"". So like, what exactly is the divergence at a point?  Does it really make sense?  Am I right about having to consider the direction as towards the origin in order for the ""sources/sinks"" idea to make sense?  Where is this stated?",,"['multivariable-calculus', 'terminology']"
95,Could you a give a intutive interpretation of curl?,Could you a give a intutive interpretation of curl?,,"Could you a give a intuitive interpretation of curl, geometrical interpretation, real-world example or physical interpretation would be ok. EDIT: Consider a specific vector field  $$\mathbf{v} = -y\mathbf{\hat{x}} + x\mathbf{\hat{y}}$$ $$\nabla\times\mathbf{v} = 2\mathbf{\hat{z}}$$ Could you give a intuitive interpretation about why all points in this field has this curl w? What if  $\mathbf{v} = y\mathbf{\hat{x}} - x\mathbf{\hat{y}}$? Thanks.","Could you a give a intuitive interpretation of curl, geometrical interpretation, real-world example or physical interpretation would be ok. EDIT: Consider a specific vector field  $$\mathbf{v} = -y\mathbf{\hat{x}} + x\mathbf{\hat{y}}$$ $$\nabla\times\mathbf{v} = 2\mathbf{\hat{z}}$$ Could you give a intuitive interpretation about why all points in this field has this curl w? What if  $\mathbf{v} = y\mathbf{\hat{x}} - x\mathbf{\hat{y}}$? Thanks.",,"['intuition', 'multivariable-calculus']"
96,Manipulating Partial Derivatives of Inverse Function,Manipulating Partial Derivatives of Inverse Function,,"In lectures we're told: $$\dfrac {\partial y} {\partial x} = \dfrac 1 {\dfrac {\partial x} {\partial y}}$$ as long as the same variables are being held constant in each partial derivative. The course is 'applied maths', i.e non-rigorous, so don't confuse me. But anyway, if we have: $$\xi = x - y \qquad \eta = x$$ Then $\dfrac {\partial x} {\partial \xi} = 0$ and $\dfrac {\partial \xi} {\partial x} = 1$. The rule presumably fails because one of the partial derivatives is $0$. But then isn't this rule useless? Since I cannot use it without checking that the partial derivative isn't in fact $0$, but then I've worked out the partial derivative manually anyway.","In lectures we're told: $$\dfrac {\partial y} {\partial x} = \dfrac 1 {\dfrac {\partial x} {\partial y}}$$ as long as the same variables are being held constant in each partial derivative. The course is 'applied maths', i.e non-rigorous, so don't confuse me. But anyway, if we have: $$\xi = x - y \qquad \eta = x$$ Then $\dfrac {\partial x} {\partial \xi} = 0$ and $\dfrac {\partial \xi} {\partial x} = 1$. The rule presumably fails because one of the partial derivatives is $0$. But then isn't this rule useless? Since I cannot use it without checking that the partial derivative isn't in fact $0$, but then I've worked out the partial derivative manually anyway.",,"['multivariable-calculus', 'partial-derivative']"
97,Diffeomorphism: Unit Ball vs. Euclidean Space,Diffeomorphism: Unit Ball vs. Euclidean Space,,"In my differential geometry class we are being asked to prove that the open unit ball $B^n$ = { $x$ $\in$ $\mathbb{R}$$^n$ such that |$x$| < $1$} is diffeomorphic to $\mathbb{R}$$^n$ I am having a hard time with this as I am brand new not only to differential geometry, but also topology. I know that I need to construct a smooth, differentiable bijection between the two with a differentiable inverse, but beyond that, I am unsure of where to start. Some guidance in the right direction would be greatly appreciated.","In my differential geometry class we are being asked to prove that the open unit ball $B^n$ = { $x$ $\in$ $\mathbb{R}$$^n$ such that |$x$| < $1$} is diffeomorphic to $\mathbb{R}$$^n$ I am having a hard time with this as I am brand new not only to differential geometry, but also topology. I know that I need to construct a smooth, differentiable bijection between the two with a differentiable inverse, but beyond that, I am unsure of where to start. Some guidance in the right direction would be greatly appreciated.",,"['multivariable-calculus', 'differential-geometry']"
98,Is the total differential the same as the directional derivative?,Is the total differential the same as the directional derivative?,,"The way I understand it, the total differential and the directional derivative are both linear approximations of the change in a function at a certain point. So if I know the change in $x$ and $y$ from the initial point, then I plug those into the total differential to find the approximate change in $z$. But isn't this the same as finding the directional derivative in the direction of $$ v = (\text{change in } x, \text{change in } y)? $$","The way I understand it, the total differential and the directional derivative are both linear approximations of the change in a function at a certain point. So if I know the change in $x$ and $y$ from the initial point, then I plug those into the total differential to find the approximate change in $z$. But isn't this the same as finding the directional derivative in the direction of $$ v = (\text{change in } x, \text{change in } y)? $$",,['multivariable-calculus']
99,Is any divergence-free curl-free vector field necessarily constant?,Is any divergence-free curl-free vector field necessarily constant?,,"I'm wondering, for no particular reason: are there differentiable vector-valued functions $\vec{f}(\vec{x})$ in three dimensions, other than the constant function $\vec{f}(\vec{x}) = \vec{C}$, that have zero divergence and zero curl? If not, how would I prove that one doesn't exist? I had a vague memory of learning some reason that such a function doesn't exist, but there's a pretty good chance my mind is just making things up to trick me ;-) But I thought about it for a little while and couldn't think of another divergence-free curl-free function off the top of my head, so I'm curious whether I was thinking of real math or not.","I'm wondering, for no particular reason: are there differentiable vector-valued functions $\vec{f}(\vec{x})$ in three dimensions, other than the constant function $\vec{f}(\vec{x}) = \vec{C}$, that have zero divergence and zero curl? If not, how would I prove that one doesn't exist? I had a vague memory of learning some reason that such a function doesn't exist, but there's a pretty good chance my mind is just making things up to trick me ;-) But I thought about it for a little while and couldn't think of another divergence-free curl-free function off the top of my head, so I'm curious whether I was thinking of real math or not.",,"['functions', 'multivariable-calculus']"
