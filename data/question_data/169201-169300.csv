,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,What is the best way to test if data is normally distributed?,What is the best way to test if data is normally distributed?,,"I have two samples $X_1 \; (N= 97)$ and $X_2 \; (N=4782)$ drawn from the same population data. I like to test (using Statistical Visualizations such as normplot and qqplot , and Hypothesis Tests such as jbtest , chi2gof , and kstest in MATLAB) if the data from each sample is normally distributed. My First Data is X = [8.13010235400000,13.6713071300000,14.0362434700000,18.4349488200000,26.5650511800000,30.9637565300000,34.3803447200000,40.6012946500000,45,49.3987053500000,58.6713071300000,59.0362434700000,59.0362434700000,59.0362434700000,61.9275130600000,61.9275130600000,63.4349488200000,63.4349488200000,63.4349488200000,63.4349488200000,63.4349488200000,64.4400348300000,71.5650511800000,71.5650511800000,71.5650511800000,71.5650511800000,75.9637565300000,75.9637565300000,75.9637565300000,75.9637565300000,75.9637565300000,75.9637565300000,75.9637565300000,75.9637565300000,75.9637565300000,75.9637565300000,77.4711922900000,77.4711922900000,77.4711922900000,77.4711922900000,77.4711922900000,77.4711922900000,77.4711922900000,77.4711922900000,77.4711922900000,78.6900675300000,90,90,90,90,90,90,90,90,90,90,90,90,90,90,90,93.1798301200000,97.1250163500000,97.7651660200000,102.528807700000,102.528807700000,102.528807700000,102.528807700000,102.528807700000,104.036243500000,104.036243500000,104.036243500000,104.036243500000,104.036243500000,104.036243500000,104.036243500000,105.255118700000,108.434948800000,108.434948800000,108.434948800000,108.434948800000,109.440034800000,116.565051200000,118.072486900000,120.963756500000,127.746805400000,130.601294600000,135,137.489552900000,139.398705400000,139.398705400000,149.036243500000,153.434948800000,159.227745300000,161.565051200000,179.999998800000,180]; The analyses using statistical visualizations in MATLAB, shows that the underlying distribution for both samples are normal. However, from the hypothesis tests, the null hypothesis for the $X_1$ sample is not rejected using the same significance value (except for the chi-square test), but that for the $X_2$ is completely rejected. I am now confused as to how to prove my samples are normally distributed and as well come from the same population data. Please, what can I do in this situation? PS : Sample $X_2$ is too large for me to post, but if there is any suggestion on how I could show this, then I don’t mind. Attaching Image","I have two samples and drawn from the same population data. I like to test (using Statistical Visualizations such as normplot and qqplot , and Hypothesis Tests such as jbtest , chi2gof , and kstest in MATLAB) if the data from each sample is normally distributed. My First Data is X = [8.13010235400000,13.6713071300000,14.0362434700000,18.4349488200000,26.5650511800000,30.9637565300000,34.3803447200000,40.6012946500000,45,49.3987053500000,58.6713071300000,59.0362434700000,59.0362434700000,59.0362434700000,61.9275130600000,61.9275130600000,63.4349488200000,63.4349488200000,63.4349488200000,63.4349488200000,63.4349488200000,64.4400348300000,71.5650511800000,71.5650511800000,71.5650511800000,71.5650511800000,75.9637565300000,75.9637565300000,75.9637565300000,75.9637565300000,75.9637565300000,75.9637565300000,75.9637565300000,75.9637565300000,75.9637565300000,75.9637565300000,77.4711922900000,77.4711922900000,77.4711922900000,77.4711922900000,77.4711922900000,77.4711922900000,77.4711922900000,77.4711922900000,77.4711922900000,78.6900675300000,90,90,90,90,90,90,90,90,90,90,90,90,90,90,90,93.1798301200000,97.1250163500000,97.7651660200000,102.528807700000,102.528807700000,102.528807700000,102.528807700000,102.528807700000,104.036243500000,104.036243500000,104.036243500000,104.036243500000,104.036243500000,104.036243500000,104.036243500000,105.255118700000,108.434948800000,108.434948800000,108.434948800000,108.434948800000,109.440034800000,116.565051200000,118.072486900000,120.963756500000,127.746805400000,130.601294600000,135,137.489552900000,139.398705400000,139.398705400000,149.036243500000,153.434948800000,159.227745300000,161.565051200000,179.999998800000,180]; The analyses using statistical visualizations in MATLAB, shows that the underlying distribution for both samples are normal. However, from the hypothesis tests, the null hypothesis for the sample is not rejected using the same significance value (except for the chi-square test), but that for the is completely rejected. I am now confused as to how to prove my samples are normally distributed and as well come from the same population data. Please, what can I do in this situation? PS : Sample is too large for me to post, but if there is any suggestion on how I could show this, then I don’t mind. Attaching Image",X_1 \; (N= 97) X_2 \; (N=4782) X_1 X_2 X_2,"['statistics', 'normal-distribution', 'statistical-inference', 'matlab', 'visualization']"
1,Confusion in finding conditional expecation of indicator function (Lehmann-Scheffe),Confusion in finding conditional expecation of indicator function (Lehmann-Scheffe),,"I'm trying to find the UMVUE of $e^{-2\lambda}$ with $X_1, X_2,\ldots,X_n \sim \operatorname{Poisson}(\lambda)$ being independent. So since $T(X) := \sum_{i=1}^n X_i$ is a complete sufficient statistic for the Poisson distribution, and also $W(X):= \mathbf{I}(X_1 + X_2 = 0)$ is an unbiased estimator of $e^{-2\lambda}$, then by Lehmann_Scheffe, $$\tau(T)= \mathbb{E}(W(T)\mid T(X)) $$ is the UMVUE of $e^{-2\lambda}$. I'm having trouble computing $\tau$ so I would like to see if this is correct: First, to find the expectation, I need the PDF. $$\mathbb{P}(W(T)=s\mid T(X)=t) = \frac{\mathbb{P}(W(T)=s \ \cap \ T(X) = t)}{\mathbb{P}(T(X) = t)} \\ = \frac{\mathbb{P}(\mathbf{I}(X_1 + X_2 = 0)  = s \ \cap \ X_1 + X_2 + \cdots + X_2 = t)}{\mathbb{P}(X_1 + X_2 + \cdots + X_n = t)}\\ = \frac{\mathbb{P}(\mathbf{I}(X_3 + X_4 + \cdots + X_n = t)=s)}{\mathbb{P}(X_1 + X_2 + \cdots + X_n = t)}   $$ The denominator is just $\frac{e^{-n\lambda}(n\lambda)^t}{t!}$. When $s=1$, we have  $$\mathbb{P}(\mathrm{I}(X_3+X_4+\cdots+X_n=t) =1) = \frac{e^{-(n-2)\lambda}((n-2)\lambda)^t}{t!}   $$ So  $$\mathbb{P}(\mathrm{I}(X_3+X_4+\cdots+X_n=t) =0) = 1 - \frac{e^{-(n-2)\lambda}((n-2)\lambda)^t}{t!}   $$ After this, I'm unsure of how to find the expectation. Is my approach correct?","I'm trying to find the UMVUE of $e^{-2\lambda}$ with $X_1, X_2,\ldots,X_n \sim \operatorname{Poisson}(\lambda)$ being independent. So since $T(X) := \sum_{i=1}^n X_i$ is a complete sufficient statistic for the Poisson distribution, and also $W(X):= \mathbf{I}(X_1 + X_2 = 0)$ is an unbiased estimator of $e^{-2\lambda}$, then by Lehmann_Scheffe, $$\tau(T)= \mathbb{E}(W(T)\mid T(X)) $$ is the UMVUE of $e^{-2\lambda}$. I'm having trouble computing $\tau$ so I would like to see if this is correct: First, to find the expectation, I need the PDF. $$\mathbb{P}(W(T)=s\mid T(X)=t) = \frac{\mathbb{P}(W(T)=s \ \cap \ T(X) = t)}{\mathbb{P}(T(X) = t)} \\ = \frac{\mathbb{P}(\mathbf{I}(X_1 + X_2 = 0)  = s \ \cap \ X_1 + X_2 + \cdots + X_2 = t)}{\mathbb{P}(X_1 + X_2 + \cdots + X_n = t)}\\ = \frac{\mathbb{P}(\mathbf{I}(X_3 + X_4 + \cdots + X_n = t)=s)}{\mathbb{P}(X_1 + X_2 + \cdots + X_n = t)}   $$ The denominator is just $\frac{e^{-n\lambda}(n\lambda)^t}{t!}$. When $s=1$, we have  $$\mathbb{P}(\mathrm{I}(X_3+X_4+\cdots+X_n=t) =1) = \frac{e^{-(n-2)\lambda}((n-2)\lambda)^t}{t!}   $$ So  $$\mathbb{P}(\mathrm{I}(X_3+X_4+\cdots+X_n=t) =0) = 1 - \frac{e^{-(n-2)\lambda}((n-2)\lambda)^t}{t!}   $$ After this, I'm unsure of how to find the expectation. Is my approach correct?",,"['probability-theory', 'statistics', 'statistical-inference', 'parameter-estimation']"
2,Show that a pivotal quantity has a chi-square distribution,Show that a pivotal quantity has a chi-square distribution,,"Let $Y$ be a random variable with an exponential distribution with E(Y)=$\theta$. Show that the pivotal $\frac{2Y}{\theta}$ has a chi-square distribution using moment-generating function technique. What I did: Mu(t) = E(e$^{tu})$ = E(e$\frac{t(2Y)}{\theta})$. Then I integrated: $\int_{0}^{\infty}$e$^\frac{t(2Y)}{\theta}*f(y) dy$. This produces $\frac{-\theta}{2 \beta t-\theta}$ This looks nothing like a chi-square distribution. It then asks how many degrees of freedom it has. Please help! Thanks in advance, oh wise Stats geniuses.","Let $Y$ be a random variable with an exponential distribution with E(Y)=$\theta$. Show that the pivotal $\frac{2Y}{\theta}$ has a chi-square distribution using moment-generating function technique. What I did: Mu(t) = E(e$^{tu})$ = E(e$\frac{t(2Y)}{\theta})$. Then I integrated: $\int_{0}^{\infty}$e$^\frac{t(2Y)}{\theta}*f(y) dy$. This produces $\frac{-\theta}{2 \beta t-\theta}$ This looks nothing like a chi-square distribution. It then asks how many degrees of freedom it has. Please help! Thanks in advance, oh wise Stats geniuses.",,['statistics']
3,"If I know the variance of two numbers, is it possible to find their difference?","If I know the variance of two numbers, is it possible to find their difference?",,"If I know the variance of two numbers, is it possible to find their difference? As in, the only information I have is the variance and that there are only two numbers. How could I work backwards to find out what the difference between the two numbers is?","If I know the variance of two numbers, is it possible to find their difference? As in, the only information I have is the variance and that there are only two numbers. How could I work backwards to find out what the difference between the two numbers is?",,"['statistics', 'variance']"
4,"Difference between ""undefined variance"" and ""infinite variance"" (and likewise moments)","Difference between ""undefined variance"" and ""infinite variance"" (and likewise moments)",,"I'm trying to get my head around the concept of ""stable distribution"" but there is a concept which is not clear to me. The thing that I would like to understand is whether ""Undefined variance"" is equivalent to ""infinite variance"" At the beginning, I thought that the two concepts where equivalent statements but then I looked on wikipedia the page about levy distribution ( https://en.wikipedia.org/wiki/Lévy_distribution ) and I saw that For the Lèvy distribution: Variance = $\infty \qquad$ Kurtosis=Undefined Hence, I would like to understand the difference between an undefined moment and an infinite one","I'm trying to get my head around the concept of ""stable distribution"" but there is a concept which is not clear to me. The thing that I would like to understand is whether ""Undefined variance"" is equivalent to ""infinite variance"" At the beginning, I thought that the two concepts where equivalent statements but then I looked on wikipedia the page about levy distribution ( https://en.wikipedia.org/wiki/Lévy_distribution ) and I saw that For the Lèvy distribution: Variance = $\infty \qquad$ Kurtosis=Undefined Hence, I would like to understand the difference between an undefined moment and an infinite one",,"['probability', 'probability-theory', 'statistics', 'probability-distributions']"
5,What substitution would I make to integrate this?,What substitution would I make to integrate this?,,"Problem: The integral is gonna be: $$\int^{1}_{0}\int^{1}_{0}4xy\sqrt{x^2+y^2} dy \, dx$$ But I'm quite rusty with my calculus, and this is mainly for a statistics course. I know that $x$ and $y$ are bounded within a unit square region in the $xy$-plane, and I was considering making some sort of $\cos$ or $\sin$ substitution to get rid of that pesky square root, but then I'm not sure what limits to use, and I suspect the $dy dx$ would just become $d\theta d\theta$, which doesn't make sense. Any help or guidance is appreciated. Please try to keep your explanation simple, and don't assume I understand things. It's been a while since my last calculus course :) Thank you!","Problem: The integral is gonna be: $$\int^{1}_{0}\int^{1}_{0}4xy\sqrt{x^2+y^2} dy \, dx$$ But I'm quite rusty with my calculus, and this is mainly for a statistics course. I know that $x$ and $y$ are bounded within a unit square region in the $xy$-plane, and I was considering making some sort of $\cos$ or $\sin$ substitution to get rid of that pesky square root, but then I'm not sure what limits to use, and I suspect the $dy dx$ would just become $d\theta d\theta$, which doesn't make sense. Any help or guidance is appreciated. Please try to keep your explanation simple, and don't assume I understand things. It's been a while since my last calculus course :) Thank you!",,"['integration', 'statistics', 'probability-distributions', 'definite-integrals']"
6,How to tell when a data series is a normal distribution,How to tell when a data series is a normal distribution,,"I have a list of data, and I need to know if the values are normally distributed. 4 of the 20 values on the list lie outside of 3 standard deviations of the mean. Is the data normally distributed and why?","I have a list of data, and I need to know if the values are normally distributed. 4 of the 20 values on the list lie outside of 3 standard deviations of the mean. Is the data normally distributed and why?",,"['statistics', 'standard-deviation', 'means', 'data-analysis', 'descriptive-statistics']"
7,Derivation of Variance and Mean of an Indicator Function,Derivation of Variance and Mean of an Indicator Function,,"Consider $Y_1, \ldots, Y_n$ random variables such that $n^{1/2}(\bar{Y}_{n} - \mu ) \xrightarrow{D} \text{iid}~ \mathcal{N}(0,\,\sigma^{2}).$ It holds that $$g_{n}(x_n(c))=1_{[n^{1/2}\frac{(\bar{Y}_{n} - \mu )}{\sigma} \leq c]} \xrightarrow{D} 1_{[Z \leq c]}, \quad Z \sim \mathcal{N}(0,1)$$ where $c$ is some deterministic value. Show: \begin{align} E[g_{n}(x)] &\to P[Z \leq c] = \Phi(c)\quad (n \rightarrow \infty) \\ V[g_{n}(x)] &\to \Phi(c)(1-\Phi(c)) \quad(n \rightarrow \infty) \end{align} Problem: How do I arrive at those expressions? Heuristically it is somehow clear due to the indicator function (Bernoulli variable). I am interested in the exact formal derivation. My Attempt: \begin{align} \lim_n E[g_{n}(x)] &= \lim_n E\left[ 1_{[n^{1/2}\frac{(\bar{Y}_{n} - \mu )}{\sigma} \leq c]} \right] = \lim_n P\left[ n^{1/2}\frac{(\bar{Y}_{n} - \mu )}{\sigma} \leq c \right] = P[Z \leq c] = \Phi(c) \\ \lim_n V[g_{n}(x)] &= \lim_n E \left[ 1^2_{[n^{1/2}\frac{(\bar{Y}_{n} - \mu )}{\sigma} \leq c} \right] - \lim_n E\left[ 1_{[n^{1/2}\frac{(\bar{Y}_{n} - \mu )}{\sigma} \leq c} \right]^2 \\ &= \lim_n P\left[ n^{1/2}\frac{(\bar{Y}_{n} - \mu )}{\sigma} \leq c \right] - \lim_n P\left[ n^{1/2}\frac{(\bar{Y}_{n} - \mu )}{\sigma} \leq c \right]^2 \\ &= P(Z \leq c)-P(Z \leq c)^2= \Phi(c)(1- \Phi(c)) \end{align} (I think I cannot decompose the variance and look at each term separately, can I?) For clarification, I am interested if I have used the convergence in distribution property right. In particular, looking at the variance, I decompose it and apply to each term separately (3rd equality) that the distribution functions converge (due to convergence in distribution of the predictor).  Can I do this (if yes why): $\lim_n P[n^{1/2}\frac{(\bar{Y}_{n} - \mu )}{\sigma} \leq c]^2 = P(Z \leq c)^2$ ?","Consider random variables such that It holds that where is some deterministic value. Show: Problem: How do I arrive at those expressions? Heuristically it is somehow clear due to the indicator function (Bernoulli variable). I am interested in the exact formal derivation. My Attempt: (I think I cannot decompose the variance and look at each term separately, can I?) For clarification, I am interested if I have used the convergence in distribution property right. In particular, looking at the variance, I decompose it and apply to each term separately (3rd equality) that the distribution functions converge (due to convergence in distribution of the predictor).  Can I do this (if yes why): ?","Y_1, \ldots, Y_n n^{1/2}(\bar{Y}_{n} - \mu ) \xrightarrow{D} \text{iid}~ \mathcal{N}(0,\,\sigma^{2}). g_{n}(x_n(c))=1_{[n^{1/2}\frac{(\bar{Y}_{n} - \mu )}{\sigma} \leq c]} \xrightarrow{D} 1_{[Z \leq c]}, \quad
Z \sim \mathcal{N}(0,1) c \begin{align}
E[g_{n}(x)] &\to P[Z \leq c] = \Phi(c)\quad (n \rightarrow \infty) \\
V[g_{n}(x)] &\to \Phi(c)(1-\Phi(c)) \quad(n \rightarrow \infty)
\end{align} \begin{align}
\lim_n E[g_{n}(x)] &= \lim_n E\left[ 1_{[n^{1/2}\frac{(\bar{Y}_{n} - \mu )}{\sigma} \leq c]} \right] = \lim_n P\left[ n^{1/2}\frac{(\bar{Y}_{n} - \mu )}{\sigma} \leq c \right] = P[Z \leq c] = \Phi(c) \\
\lim_n V[g_{n}(x)] &= \lim_n E \left[ 1^2_{[n^{1/2}\frac{(\bar{Y}_{n} - \mu )}{\sigma} \leq c} \right] - \lim_n E\left[ 1_{[n^{1/2}\frac{(\bar{Y}_{n} - \mu )}{\sigma} \leq c} \right]^2 \\
&= \lim_n P\left[ n^{1/2}\frac{(\bar{Y}_{n} - \mu )}{\sigma} \leq c \right] - \lim_n P\left[ n^{1/2}\frac{(\bar{Y}_{n} - \mu )}{\sigma} \leq c \right]^2 \\
&= P(Z \leq c)-P(Z \leq c)^2= \Phi(c)(1- \Phi(c))
\end{align} \lim_n P[n^{1/2}\frac{(\bar{Y}_{n} - \mu )}{\sigma} \leq c]^2 = P(Z \leq c)^2","['integration', 'probability-theory', 'statistics']"
8,Calculating standard deviation without a data set.,Calculating standard deviation without a data set.,,I know how to calculate SD when given data points by using: $ \displaystyle \mathrm{SD} = \sqrt{\sum(x^2 - \text{mean}^2) / n} $. I have been given just the sum of $x$ and sum of $x^2$. How do I calculate SD from this?! An example question I am stuck on: Sum of $x = 1303$ Sum of $x^2 = 123557.$ There are 14 years for which the data is given - I would assume this is n...,I know how to calculate SD when given data points by using: $ \displaystyle \mathrm{SD} = \sqrt{\sum(x^2 - \text{mean}^2) / n} $. I have been given just the sum of $x$ and sum of $x^2$. How do I calculate SD from this?! An example question I am stuck on: Sum of $x = 1303$ Sum of $x^2 = 123557.$ There are 14 years for which the data is given - I would assume this is n...,,"['statistics', 'standard-deviation', 'means']"
9,Calculating Simple Functions of Discrete Random Variables,Calculating Simple Functions of Discrete Random Variables,,"Say I have a random variable $X$ that has some distribution (e.g. the value obtained when rolling a standard dice). I'm trying to find out whether the probability distributions of functions of that variable are calculated by simply taking the probability for $x_1$, $x_2$, $x_3$.... $x_n$ and assigning them to $f(x_1),~ f(x_2),\dots,~f(x_n)$ or whether there is a more complicated process, and that depends perhaps on the functions and types, number of etc r.v.'s themselves? For example, say the player of a game scores points based on the value of the above dice they roll (whose r.v. is $X$), according to: $$f(X) = X^2 + 2X + 2 $$ For the dice: $X = 1, 2, 3, 4, 5$ or $6$. And of course $\Bbb P(X = x_{1-6}) = 1/6$ Calculating $f(X)$ for the dice's values, the possible points you can score are: $5, 10, 17, 26, 37$ and $50$. Is then the probability distribution for the points you can score simply: $$\Bbb P(f(X) = 5) = 1/6; \Bbb P(f(X)=10) = 1/6,\dots, \Bbb P(f(X) = 50) = 1/6 $$? Would it be accurate too to say that the score $f(X)$ is itself a random variable? I'm hoping to extend this concept to things like the addition of different r.v.'s - i.e. to understand what it means to add two r.v.'s $X + Y$ - but I've heard this is much more complicated, so I thought I'd start with this. Many thanks for all your insight, really appreciate it.","Say I have a random variable $X$ that has some distribution (e.g. the value obtained when rolling a standard dice). I'm trying to find out whether the probability distributions of functions of that variable are calculated by simply taking the probability for $x_1$, $x_2$, $x_3$.... $x_n$ and assigning them to $f(x_1),~ f(x_2),\dots,~f(x_n)$ or whether there is a more complicated process, and that depends perhaps on the functions and types, number of etc r.v.'s themselves? For example, say the player of a game scores points based on the value of the above dice they roll (whose r.v. is $X$), according to: $$f(X) = X^2 + 2X + 2 $$ For the dice: $X = 1, 2, 3, 4, 5$ or $6$. And of course $\Bbb P(X = x_{1-6}) = 1/6$ Calculating $f(X)$ for the dice's values, the possible points you can score are: $5, 10, 17, 26, 37$ and $50$. Is then the probability distribution for the points you can score simply: $$\Bbb P(f(X) = 5) = 1/6; \Bbb P(f(X)=10) = 1/6,\dots, \Bbb P(f(X) = 50) = 1/6 $$? Would it be accurate too to say that the score $f(X)$ is itself a random variable? I'm hoping to extend this concept to things like the addition of different r.v.'s - i.e. to understand what it means to add two r.v.'s $X + Y$ - but I've heard this is much more complicated, so I thought I'd start with this. Many thanks for all your insight, really appreciate it.",,"['probability', 'statistics', 'random-variables']"
10,Monte-Carlo method,Monte-Carlo method,,"I have a problem with my statistics task. It sounds like this: ""Using the Monte Carlo method, plot the dependence of a on b for a from -5 to 5, where $P (a < x-y < b) = 0.95$, where $x\sim N(0,1)$, and y is a uniformly distributed random variable from -2 to 2.""  what should I do, where to start? until I have no ideas. To be honest, I did not understand anything in the following answer: ""First find the probability distribution of random variable X−Y using montecarlo method. 1.Pick an x and y randomly according to their densities and then compute x−y. Do this N times. 2.Make a list x−y values. 3.Sort the list in ascending order. 4.Fix an b which is in [−5,5]. 5.From count number of values less than b, call this k. $\cdot$ If k/N>0.95: Count from smallest value till you get the k/N less more or less equal to 0.95. The value you find is the a $\cdot$ If k/N<0.95: No such a exists."" I tried to code it and I have a lot of questions. First of all, I created a list for $x-y$. dif<-x-y Then, I created a list of values of a, which is from -5 to 5 a<-seq(-5, 5, 0.05) But now I must go to the 5-th item. It sounds like this: From count number of values more than a, call this k. What should I do here? I must take values from $x-y$, which are more than a? Then I should count $k/N$ and if it is more than 0.95 what is next?  ""Count from smallest value"", but what smallest value? Thanks for help","I have a problem with my statistics task. It sounds like this: ""Using the Monte Carlo method, plot the dependence of a on b for a from -5 to 5, where $P (a < x-y < b) = 0.95$, where $x\sim N(0,1)$, and y is a uniformly distributed random variable from -2 to 2.""  what should I do, where to start? until I have no ideas. To be honest, I did not understand anything in the following answer: ""First find the probability distribution of random variable X−Y using montecarlo method. 1.Pick an x and y randomly according to their densities and then compute x−y. Do this N times. 2.Make a list x−y values. 3.Sort the list in ascending order. 4.Fix an b which is in [−5,5]. 5.From count number of values less than b, call this k. $\cdot$ If k/N>0.95: Count from smallest value till you get the k/N less more or less equal to 0.95. The value you find is the a $\cdot$ If k/N<0.95: No such a exists."" I tried to code it and I have a lot of questions. First of all, I created a list for $x-y$. dif<-x-y Then, I created a list of values of a, which is from -5 to 5 a<-seq(-5, 5, 0.05) But now I must go to the 5-th item. It sounds like this: From count number of values more than a, call this k. What should I do here? I must take values from $x-y$, which are more than a? Then I should count $k/N$ and if it is more than 0.95 what is next?  ""Count from smallest value"", but what smallest value? Thanks for help",,['statistics']
11,unbiased estimator of sigma in normal distribution,unbiased estimator of sigma in normal distribution,,"It's mathematical statistics, but i couldn't find the category of it. Anyway, i am not sure that i did it right. I need help. r.s is random sample, and u.e is unbiased estimator.","It's mathematical statistics, but i couldn't find the category of it. Anyway, i am not sure that i did it right. I need help. r.s is random sample, and u.e is unbiased estimator.",,"['statistics', 'estimation']"
12,Arithmetic mean is to addition as Harmonic mean is to ...?,Arithmetic mean is to addition as Harmonic mean is to ...?,,"Take $n$ real numbers $x_1,\ldots,x_n.$ The Arithmetic mean $A_n=\frac{1}{n}(x_1+\ldots+x_n)$ is the answer to the question: ""Which number, when added up $n$ times, is equal to the sum of the $x_1,\ldots,x_n?$"" If the numbers are non-negative, then the Geometric mean $G_n=(x_1\ldots x_n)^{1/n}$ is the answer to the question: ""Which number, when multiplied $n$ times by itself, is equal to the product of the $x_1,\ldots,x_n?$"" So the Arithmetic and Geometric and Geometric mean are associated with addition and multiplication, respectively. My question: Is there an operation, call it shmultiplication, such that the Harmonic mean $H_n=\frac{n}{\frac{1}{x_1}+\ldots+\frac{1}{x_n}}$ is the answer to the question: ""Which number, when shmultiplied $n$ times with itself, is equal to the shmultiplication of the  $x_1,\ldots,x_n?$"" (And if yes, does shmultiplication have a proper name?)","Take $n$ real numbers $x_1,\ldots,x_n.$ The Arithmetic mean $A_n=\frac{1}{n}(x_1+\ldots+x_n)$ is the answer to the question: ""Which number, when added up $n$ times, is equal to the sum of the $x_1,\ldots,x_n?$"" If the numbers are non-negative, then the Geometric mean $G_n=(x_1\ldots x_n)^{1/n}$ is the answer to the question: ""Which number, when multiplied $n$ times by itself, is equal to the product of the $x_1,\ldots,x_n?$"" So the Arithmetic and Geometric and Geometric mean are associated with addition and multiplication, respectively. My question: Is there an operation, call it shmultiplication, such that the Harmonic mean $H_n=\frac{n}{\frac{1}{x_1}+\ldots+\frac{1}{x_n}}$ is the answer to the question: ""Which number, when shmultiplied $n$ times with itself, is equal to the shmultiplication of the  $x_1,\ldots,x_n?$"" (And if yes, does shmultiplication have a proper name?)",,"['statistics', 'average', 'means', 'descriptive-statistics']"
13,Statistics. How are standard error and confidence intervals useful without knowing population size?,Statistics. How are standard error and confidence intervals useful without knowing population size?,,"I understand standard error and confidence intervals as formulas, but not as concepts. Can you help me understand them better? A smaller standard deviation (smaller spread of your data) and a larger sample size both give you a smaller standard error. That in turn gives you a narrower confidence interval. In layman's terms: As your data points move closer to the sample mean; and as your sample size (n) gets closer to your population size (N), you can be more confident that your sample statistic matches your population parameter. But how do you calculate your sample confidence is your don't know your population size? An example I threw together in Excel: You want to know how the median sick days workers in your town take each year. You survey companies and get responses for 36 workers. The mean for the 36 workers is 14.64 days (I'm rounding). The standard deviation is 9.30. That gives you a standard error of 1.55 and a 95% confidence interval of +-3.15. You conclude, ""I'm 95% sure that workers in our town take between 11 and 17 sick days per year."" But how do you know that estimate is even close? If your little town has only 100 workers, then a survey of 36 is pretty accurate. If you have 100,000 workers in your town, your sample is probably way off. The formulas for standard error and confidence interval (as well as standard deviation) don't have N in their calculations. In many cases, you don't even know N (number of frogs in a national park; amount of drugs smuggled through an area; tons of ore in a mine). So how do you calculate (percent of frogs with a disease; percentage of drugs stopped; quantity of ore per ton of rock) without knowing N? Is a thousand frogs sufficient? Is a hundred bricks of pot a good job? If we extract 16 tons, what do we get? Corollary to this: If we know N, can we use ti change our statistics for n? This is a repeat of How is it that the required sample size for a specified error and confidence is not dependent on population size? , but I don't grasp the concept of infinite populations.","I understand standard error and confidence intervals as formulas, but not as concepts. Can you help me understand them better? A smaller standard deviation (smaller spread of your data) and a larger sample size both give you a smaller standard error. That in turn gives you a narrower confidence interval. In layman's terms: As your data points move closer to the sample mean; and as your sample size (n) gets closer to your population size (N), you can be more confident that your sample statistic matches your population parameter. But how do you calculate your sample confidence is your don't know your population size? An example I threw together in Excel: You want to know how the median sick days workers in your town take each year. You survey companies and get responses for 36 workers. The mean for the 36 workers is 14.64 days (I'm rounding). The standard deviation is 9.30. That gives you a standard error of 1.55 and a 95% confidence interval of +-3.15. You conclude, ""I'm 95% sure that workers in our town take between 11 and 17 sick days per year."" But how do you know that estimate is even close? If your little town has only 100 workers, then a survey of 36 is pretty accurate. If you have 100,000 workers in your town, your sample is probably way off. The formulas for standard error and confidence interval (as well as standard deviation) don't have N in their calculations. In many cases, you don't even know N (number of frogs in a national park; amount of drugs smuggled through an area; tons of ore in a mine). So how do you calculate (percent of frogs with a disease; percentage of drugs stopped; quantity of ore per ton of rock) without knowing N? Is a thousand frogs sufficient? Is a hundred bricks of pot a good job? If we extract 16 tons, what do we get? Corollary to this: If we know N, can we use ti change our statistics for n? This is a repeat of How is it that the required sample size for a specified error and confidence is not dependent on population size? , but I don't grasp the concept of infinite populations.",,"['statistics', 'standard-deviation', 'confidence-interval', 'standard-error']"
14,Cards are drawn one after another from a standard 52 card deck until the first spade is drawn,Cards are drawn one after another from a standard 52 card deck until the first spade is drawn,,Cards are drawn one after another from a standard 52 card deck until the first spade is drawn. Let the number of necessary draws be represented by X. What is the mean of X? My professor said that this question looks like a Negative Binomial but is not. I can't think of what kind of distribution it could be. Any hints?,Cards are drawn one after another from a standard 52 card deck until the first spade is drawn. Let the number of necessary draws be represented by X. What is the mean of X? My professor said that this question looks like a Negative Binomial but is not. I can't think of what kind of distribution it could be. Any hints?,,"['probability', 'statistics', 'means']"
15,What are all of these statistics and probability symbols?,What are all of these statistics and probability symbols?,,"Given a sample space $\mathcal C=\{c:0<c<10\}$ with $C \subset \mathcal C$. The probability function is $$P(C)=\int_C \frac{1}{10} dz$$ and the variable $X(c)=c^2$. Find the CDF $F(x)$. By an example in the text, it seems that $F(x)=P(x)$. So the CDF should be $$\int_0^{10} \frac{1}{10} dz = 1.$$ There are several problems here. If it equals one, then it's a cumulative function (rather than distrib function) and the book says the answer is $F(x) = \frac{\sqrt x}{10},\ 0<x<100$. Something is being lost. $F(x)$ is not the same as $F(c)$ and $c=\sqrt x$ gives the right answer. But why? What is $X$ vs $x$? What is $F(x)$ vs $F(c)$? Or what am I misunderstanding (I'm not even sure what to ask here, so you can just say anything useful). P.S. I did read the text, but it's rather terse and no examples explain the rationale behind why the integral is done on $\sqrt x$.","Given a sample space $\mathcal C=\{c:0<c<10\}$ with $C \subset \mathcal C$. The probability function is $$P(C)=\int_C \frac{1}{10} dz$$ and the variable $X(c)=c^2$. Find the CDF $F(x)$. By an example in the text, it seems that $F(x)=P(x)$. So the CDF should be $$\int_0^{10} \frac{1}{10} dz = 1.$$ There are several problems here. If it equals one, then it's a cumulative function (rather than distrib function) and the book says the answer is $F(x) = \frac{\sqrt x}{10},\ 0<x<100$. Something is being lost. $F(x)$ is not the same as $F(c)$ and $c=\sqrt x$ gives the right answer. But why? What is $X$ vs $x$? What is $F(x)$ vs $F(c)$? Or what am I misunderstanding (I'm not even sure what to ask here, so you can just say anything useful). P.S. I did read the text, but it's rather terse and no examples explain the rationale behind why the integral is done on $\sqrt x$.",,"['calculus', 'probability', 'statistics']"
16,Generating iid samples of mixture of distributions,Generating iid samples of mixture of distributions,,Let $D_1$ and $D_2$ be distributions. Assume that: - one can always get finite amount of iid samples from $D_1$ - one can always get finite amount of iid samples from $D_2$ Let also $D=\frac12 D_1 + \frac12 D_2$ and $m$ be an integer. How can we get at least $m$ i.i.d. samples from $D$? Is it true that we can just pick $m/2$ iid samples from $D_1$ and $m/2$ iid samples from $D_2$?,Let $D_1$ and $D_2$ be distributions. Assume that: - one can always get finite amount of iid samples from $D_1$ - one can always get finite amount of iid samples from $D_2$ Let also $D=\frac12 D_1 + \frac12 D_2$ and $m$ be an integer. How can we get at least $m$ i.i.d. samples from $D$? Is it true that we can just pick $m/2$ iid samples from $D_1$ and $m/2$ iid samples from $D_2$?,,"['probability', 'probability-theory', 'statistics', 'probability-distributions', 'sampling']"
17,Tricky dice problem,Tricky dice problem,,"We simultaneously roll two dice in one round until at least one of them shows 3 once. What is the expected number of rolls needed? I've calculated probability of getting 3 on the 1st roll  I am really bad in math, sorry 😖 $= (1/6)^2 + 2(5/6 * 1/6) = 11/36$ Then on the second $= (1 - p(\text{on the 1st roll})) \cdot (11/36)$ Third $= (1 - p(\text{on the 2nd})) \cdot (11/36)$ And so on, then I've just calculated the expected number of this distribution and reached the conclusion that I'll need at least $4$ rolls, since we operate only with integers. I am really confused, if this approach is correct? Or should we use the geometric distribution and to get the expected number of rolls we just have to divide $1$ over probability of success, i.e. $11/36$? And consequently how can we find the minimum number of dice needed to get $3$ at least once on at least one dice if we can simultaneously  roll them no more than $2$ times? Thank you in advance.💕","We simultaneously roll two dice in one round until at least one of them shows 3 once. What is the expected number of rolls needed? I've calculated probability of getting 3 on the 1st roll  I am really bad in math, sorry 😖 $= (1/6)^2 + 2(5/6 * 1/6) = 11/36$ Then on the second $= (1 - p(\text{on the 1st roll})) \cdot (11/36)$ Third $= (1 - p(\text{on the 2nd})) \cdot (11/36)$ And so on, then I've just calculated the expected number of this distribution and reached the conclusion that I'll need at least $4$ rolls, since we operate only with integers. I am really confused, if this approach is correct? Or should we use the geometric distribution and to get the expected number of rolls we just have to divide $1$ over probability of success, i.e. $11/36$? And consequently how can we find the minimum number of dice needed to get $3$ at least once on at least one dice if we can simultaneously  roll them no more than $2$ times? Thank you in advance.💕",,"['probability', 'statistics', 'dice']"
18,How to calculate expected loss in money when overbooking?,How to calculate expected loss in money when overbooking?,,"I'm currently stuck on the following problem where we know that for an airline it is a 7% probability that a passenger will not meet up for departure. So to get a better use of the plane capacity the airline overbook the tickets. The plane for this task has 243 seats. The thing is that the airline loses 1000\$ for each non-used seat in the plane. If a passenger does not get a spot because of the overbooking, the passenger will be compensated with 4000\$. We want to find an expression for the expected loss if we take $n$ orders. And by using the expression/graph we want to find the number of orders such that we minimize the expected loss. The numerical answer is given, which is 258 orders which results a loss of $5564\$$. I understand that this will be a binomial distribution if we define a stochastic variable $X$ which says how many passengers that really meet ups for departure... but how do I use this fact to get the desired expression? If we do not overbook, i.e. only take 243 orders, the expected loss is to be $243 \cdot 0.07 \cdot 1000 = 17010 \$$, but if I try to do the same for $n = 258$ orders I do not get the same answer as the solution(which would be $5564\$$), why could I do that for $n = 243$, but not for $n= 258$?","I'm currently stuck on the following problem where we know that for an airline it is a 7% probability that a passenger will not meet up for departure. So to get a better use of the plane capacity the airline overbook the tickets. The plane for this task has 243 seats. The thing is that the airline loses 1000\$ for each non-used seat in the plane. If a passenger does not get a spot because of the overbooking, the passenger will be compensated with 4000\$. We want to find an expression for the expected loss if we take $n$ orders. And by using the expression/graph we want to find the number of orders such that we minimize the expected loss. The numerical answer is given, which is 258 orders which results a loss of $5564\$$. I understand that this will be a binomial distribution if we define a stochastic variable $X$ which says how many passengers that really meet ups for departure... but how do I use this fact to get the desired expression? If we do not overbook, i.e. only take 243 orders, the expected loss is to be $243 \cdot 0.07 \cdot 1000 = 17010 \$$, but if I try to do the same for $n = 258$ orders I do not get the same answer as the solution(which would be $5564\$$), why could I do that for $n = 243$, but not for $n= 258$?",,"['probability', 'statistics', 'binomial-distribution']"
19,Probability of the sample mean of a Bernoulli,Probability of the sample mean of a Bernoulli,,"If $X_{1}, X_{2}, ... ,X_{10}$ denotes an independent and identically distributed sample from a Bernoulli(p = 0.4) population distribution. What is the probability that the sample average $X$ equals exactly $0.3\text{?}$ State your answer to three decimal places. I have simply calculated Is my procedure correct? Am I missing something? Thanks you so much guys,","If $X_{1}, X_{2}, ... ,X_{10}$ denotes an independent and identically distributed sample from a Bernoulli(p = 0.4) population distribution. What is the probability that the sample average $X$ equals exactly $0.3\text{?}$ State your answer to three decimal places. I have simply calculated Is my procedure correct? Am I missing something? Thanks you so much guys,",,"['probability', 'statistics', 'probability-distributions', 'statistical-inference']"
20,Symmetric distribution around zero and median equal to zero,Symmetric distribution around zero and median equal to zero,,"Let $X$ be a random variable and suppose that it is symmetrically distributed around $0$. Is this equivalent to assume that $X$ has median equal to $0$? If not, are the two assumptions completely unrelated or one implies the other?","Let $X$ be a random variable and suppose that it is symmetrically distributed around $0$. Is this equivalent to assume that $X$ has median equal to $0$? If not, are the two assumptions completely unrelated or one implies the other?",,"['statistics', 'random-variables', 'symmetry', 'descriptive-statistics']"
21,How to Interpret Matrices,How to Interpret Matrices,,"It seems to me that a m x n matrix can be interpreted in two ways: a collection of m points, each in n-dimensional space. In this case, a point is a row vector. a collection of n points, each in m-dimensional space. In this case, a point is a column vector. I see in some places they interpret a matrix as 1 and in some places as 2. This causes the notations to be different and makes a lot of thing confusing, at least for me. Add to this the interpretation of a matrix as a linear transformation and things become more confusing. I have a few questions, all on the same theme: What is the standard way to interpret matrices? How to reconcile the notations? For example, If I know some theorem for which I had done the derivation understanding a row to be a point and then in some place it says apply the theorem considering a column to be a point. Is there any trick to understand or write equations that makes the two interpretations equivalent? And finally, are the two interpretations equivalent? Sorry if this is too trivial but this thing seems really confusing.","It seems to me that a m x n matrix can be interpreted in two ways: a collection of m points, each in n-dimensional space. In this case, a point is a row vector. a collection of n points, each in m-dimensional space. In this case, a point is a column vector. I see in some places they interpret a matrix as 1 and in some places as 2. This causes the notations to be different and makes a lot of thing confusing, at least for me. Add to this the interpretation of a matrix as a linear transformation and things become more confusing. I have a few questions, all on the same theme: What is the standard way to interpret matrices? How to reconcile the notations? For example, If I know some theorem for which I had done the derivation understanding a row to be a point and then in some place it says apply the theorem considering a column to be a point. Is there any trick to understand or write equations that makes the two interpretations equivalent? And finally, are the two interpretations equivalent? Sorry if this is too trivial but this thing seems really confusing.",,"['linear-algebra', 'matrices', 'statistics', 'matrix-equations', 'matrix-decomposition']"
22,Why is $(x_i-\bar{x})^2$ in the formula for variance as opposed to $|x_i-\bar{x}|$? [duplicate],Why is  in the formula for variance as opposed to ? [duplicate],(x_i-\bar{x})^2 |x_i-\bar{x}|,"This question already has answers here : Why is variance squared? (12 answers) Closed 6 years ago . For a data set $\bigcup_{i=1}^nx_i$ of $n$ values with mean $\bar{x}$ the variance is defined as $$\sigma^2=\frac{\displaystyle\sum_{i=1}^n(x_i-\bar{x})^2}{n}$$ My textbook says that “the square ensures that each term in the sum is positive, which is why the sum turns out not to be zero.” However wouldn’t $$\sigma^2=\frac{\displaystyle\sum_{i=1}^n|x_i-\bar{x}|}{n}$$ also prevent a sum of zero?","This question already has answers here : Why is variance squared? (12 answers) Closed 6 years ago . For a data set $\bigcup_{i=1}^nx_i$ of $n$ values with mean $\bar{x}$ the variance is defined as $$\sigma^2=\frac{\displaystyle\sum_{i=1}^n(x_i-\bar{x})^2}{n}$$ My textbook says that “the square ensures that each term in the sum is positive, which is why the sum turns out not to be zero.” However wouldn’t $$\sigma^2=\frac{\displaystyle\sum_{i=1}^n|x_i-\bar{x}|}{n}$$ also prevent a sum of zero?",,"['statistics', 'standard-deviation', 'variance']"
23,Normal approximations: What is the probability that player A will have at least 10 points more than player B?,Normal approximations: What is the probability that player A will have at least 10 points more than player B?,,"Two players $A$ and $B$ play the following game. Player $A$ uses a fair 8-sided die with numbers of $1, ...,8$ and rolls it to earn points. For each roll player $A$ collects a numbers of points corresponding to the number shown on the die. Player $B$ uses a fair coin and flips it to earn points. If the outcome is ""heads"", then player B collects $1$ point, if ""tails"" player $B$ collect $8$ points. This game is as follows: Player $A$ rolls the die $n=50$ times and player $B$ flips the coin $n=50$ times, after which player $A$ has collected $Y_A$ points and player B has collected $Y_B$ points. After $n=50$ trials: a) What is the probability that player $A$ will have at least $10$ points more than player $B$? Use normal approximations. b) What is the probability that player $A$ and player $B$ together will have more than $340$ points? Use normal approximations. c) What is the probability that player $B$ will have exactly $225$ points? I have calculated the means/expected values to: mean($Y_A$) = 4,5 and mean($Y_B$) = 4,5, the variances I have calculated to: Var($Y_A$)=5,25 and Var($Y_B$)=12,25. a)  Following @callculus answer:  I have in a table found Φ(.32) to be .6255. So 1 - 0.6255 = 0.3745, will that say that the probability for player A having at least 10 more points than player B is 37% b)  Following the method from a):  $$P(Y_A + Y_B > 340) = 1- P(Y_A + Y_B >= 340) = 1 - Φ((340+0.5-450)/\sqrt{875}) \\= 1 - Φ(-3.7) = 1-0.00009 = 99.991\% \approx 100\%$$ There is almost 100% $P(Y_A+Y_B > 340).$ c) Using the binomial PMF with $50$ trials and $25$ successes. $$(50!)/((25!)*(50-25)! * 0.5^{25} * (1-0.5)^{50-25} = 0.112275173.$$  So $P(Y_B = 225) = 11\%.$","Two players $A$ and $B$ play the following game. Player $A$ uses a fair 8-sided die with numbers of $1, ...,8$ and rolls it to earn points. For each roll player $A$ collects a numbers of points corresponding to the number shown on the die. Player $B$ uses a fair coin and flips it to earn points. If the outcome is ""heads"", then player B collects $1$ point, if ""tails"" player $B$ collect $8$ points. This game is as follows: Player $A$ rolls the die $n=50$ times and player $B$ flips the coin $n=50$ times, after which player $A$ has collected $Y_A$ points and player B has collected $Y_B$ points. After $n=50$ trials: a) What is the probability that player $A$ will have at least $10$ points more than player $B$? Use normal approximations. b) What is the probability that player $A$ and player $B$ together will have more than $340$ points? Use normal approximations. c) What is the probability that player $B$ will have exactly $225$ points? I have calculated the means/expected values to: mean($Y_A$) = 4,5 and mean($Y_B$) = 4,5, the variances I have calculated to: Var($Y_A$)=5,25 and Var($Y_B$)=12,25. a)  Following @callculus answer:  I have in a table found Φ(.32) to be .6255. So 1 - 0.6255 = 0.3745, will that say that the probability for player A having at least 10 more points than player B is 37% b)  Following the method from a):  $$P(Y_A + Y_B > 340) = 1- P(Y_A + Y_B >= 340) = 1 - Φ((340+0.5-450)/\sqrt{875}) \\= 1 - Φ(-3.7) = 1-0.00009 = 99.991\% \approx 100\%$$ There is almost 100% $P(Y_A+Y_B > 340).$ c) Using the binomial PMF with $50$ trials and $25$ successes. $$(50!)/((25!)*(50-25)! * 0.5^{25} * (1-0.5)^{50-25} = 0.112275173.$$  So $P(Y_B = 225) = 11\%.$",,"['probability', 'probability-theory', 'statistics', 'random-variables', 'normal-distribution']"
24,Vetting a random number generator -- Chi Square Tests throws varying p-value results,Vetting a random number generator -- Chi Square Tests throws varying p-value results,,"Suppose I have a random number generator and I want to check with a Chi Square Test whether its pdf is uniform or no. I can write a script that does that and I will run it several times. To my surprise, I get completely different results each times. Sometimes I will get a p-value of 0.3, sometimes 0.987 and other times 0.003. Which is the number I should take? Should I try to get an average of the p-values I get? How do I decide if this generator passes the test or not? So what I try next is using a random number generator that I already ""know"" to be uniform and I run the test on that. And I keep getting absolutely varying results. Even if I increase the number of samples, it keeps varying a lot! From what I understad, the probability if seeing a very low p-value when drawing random samples from a uniform distribution should be low, and the probability of seeing a high p-value should be high. But this doesn't seem to happen. How should I interpret the results I get from this test? This is the Python script I am using: import numpy as np from scipy.stats import chisquare  bins=256 x = np.random.randint(bins, size=bins*100)  h = []; for i in range(bins):   h.append(0);  for n in range(0, len(x)):   h[x[n]] += 1;  print(chisquare(h)) and this are the results I get: Power_divergenceResult(statistic=303.19999999999999, pvalue=0.020572599306529871) Power_divergenceResult(statistic=211.06, pvalue=0.97933788750272888) Power_divergenceResult(statistic=289.66000000000003, pvalue=0.066874498546635575) Power_divergenceResult(statistic=275.63999999999999, pvalue=0.17885688588645363) Power_divergenceResult(statistic=257.86000000000001, pvalue=0.43814613213884313) Power_divergenceResult(statistic=217.07999999999998, pvalue=0.95911527563656596) Even more, I did a histogram of the p-values I got and it looks pretty uniform. If I know the samples I have are drawn from a uniform distribution, shouldn't I get most of the times a high p-value?","Suppose I have a random number generator and I want to check with a Chi Square Test whether its pdf is uniform or no. I can write a script that does that and I will run it several times. To my surprise, I get completely different results each times. Sometimes I will get a p-value of 0.3, sometimes 0.987 and other times 0.003. Which is the number I should take? Should I try to get an average of the p-values I get? How do I decide if this generator passes the test or not? So what I try next is using a random number generator that I already ""know"" to be uniform and I run the test on that. And I keep getting absolutely varying results. Even if I increase the number of samples, it keeps varying a lot! From what I understad, the probability if seeing a very low p-value when drawing random samples from a uniform distribution should be low, and the probability of seeing a high p-value should be high. But this doesn't seem to happen. How should I interpret the results I get from this test? This is the Python script I am using: import numpy as np from scipy.stats import chisquare  bins=256 x = np.random.randint(bins, size=bins*100)  h = []; for i in range(bins):   h.append(0);  for n in range(0, len(x)):   h[x[n]] += 1;  print(chisquare(h)) and this are the results I get: Power_divergenceResult(statistic=303.19999999999999, pvalue=0.020572599306529871) Power_divergenceResult(statistic=211.06, pvalue=0.97933788750272888) Power_divergenceResult(statistic=289.66000000000003, pvalue=0.066874498546635575) Power_divergenceResult(statistic=275.63999999999999, pvalue=0.17885688588645363) Power_divergenceResult(statistic=257.86000000000001, pvalue=0.43814613213884313) Power_divergenceResult(statistic=217.07999999999998, pvalue=0.95911527563656596) Even more, I did a histogram of the p-values I got and it looks pretty uniform. If I know the samples I have are drawn from a uniform distribution, shouldn't I get most of the times a high p-value?",,"['statistics', 'simulation', 'chi-squared']"
25,Defining Binomial distribution centered at any integer,Defining Binomial distribution centered at any integer,,"Given the Binomial distribution: $$\Pr(k;n,p) = \Pr(X = k) = \binom n k p^k (1-p)^{n-k}$$ It is clear that for choices of $n$ and $p$ we can shift the binomial distribution left or right since $\mu = np$. I want to consider the case for $p =0.5$, hence the mean and median are equivalent. Does anyone know if there is an extension of this distribution which allows the distribution to be centered at any integer (including negative of course)? Where centered refers to centering with respect to the mean. This was somewhat addressed in the post but here they are looking for a shift to zero from a programming perspective not analytically. Thanks.","Given the Binomial distribution: $$\Pr(k;n,p) = \Pr(X = k) = \binom n k p^k (1-p)^{n-k}$$ It is clear that for choices of $n$ and $p$ we can shift the binomial distribution left or right since $\mu = np$. I want to consider the case for $p =0.5$, hence the mean and median are equivalent. Does anyone know if there is an extension of this distribution which allows the distribution to be centered at any integer (including negative of course)? Where centered refers to centering with respect to the mean. This was somewhat addressed in the post but here they are looking for a shift to zero from a programming perspective not analytically. Thanks.",,"['probability-theory', 'statistics', 'binomial-distribution']"
26,Formula for MOLS generation when n is prime power - really?,Formula for MOLS generation when n is prime power - really?,,"I remember from the university that there is a procedure to simply generate MOLS when n is a prime number. Basically it is about cyclic rotation of the symbols in the rows, for square k by k positions. Implementation in Java was easy: (k * (i - 1) + (j - 1)) mod n I remember that this was supposed to work only for n that is a prime number. And yet I found in a book that this is ""guaranteed"" to work also for prime powers. Which I doubt (does not work for n=4, e.g.), but I am not really that good at math and would like to understand whether the book is wrong or not. https://books.google.de/books?id=yU-rTcurys8C&pg=PA294&dq=MOLS+construct+software+testing&hl=cs&sa=X&ved=0ahUKEwjluuz6rcrUAhVCDZoKHVuxBnIQ6AEIJzAA#v=onepage&q=MOLS%20construct%20software%20testing&f=false , page 295.","I remember from the university that there is a procedure to simply generate MOLS when n is a prime number. Basically it is about cyclic rotation of the symbols in the rows, for square k by k positions. Implementation in Java was easy: (k * (i - 1) + (j - 1)) mod n I remember that this was supposed to work only for n that is a prime number. And yet I found in a book that this is ""guaranteed"" to work also for prime powers. Which I doubt (does not work for n=4, e.g.), but I am not really that good at math and would like to understand whether the book is wrong or not. https://books.google.de/books?id=yU-rTcurys8C&pg=PA294&dq=MOLS+construct+software+testing&hl=cs&sa=X&ved=0ahUKEwjluuz6rcrUAhVCDZoKHVuxBnIQ6AEIJzAA#v=onepage&q=MOLS%20construct%20software%20testing&f=false , page 295.",,"['statistics', 'combinatorial-designs', 'latin-square']"
27,Is $Y_n=(1-\frac{a}{n})^{X_1+\cdots+X_n}$ an unbiased estimator?,Is  an unbiased estimator?,Y_n=(1-\frac{a}{n})^{X_1+\cdots+X_n},"Let $X_1, X_2, \ldots$ be i.i.d. and have the Poisson distribution with parameter $\lambda$. We define the estimator of $y=e^{-a\lambda}$ (where $a \neq 0$ is some constant value) as:   $$Y_n= \left( 1-\frac a n \right)^{\sum\limits_{i=1}^nX_i}$$   Is it an ubiased estimator of $y$? So $E(X_i) = \lambda$. I'm trying to calculate the bias with: $$E(Y_n)=E\left((1-\frac a n)^{\sum\limits_{i=1}^nX_i}\right) = E\left( (1-\frac{a}{n})^{X_1} \cdots (1-\frac{a}{n})^{X_n} \right)$$ which, since $X_i$ are i.i.d., equals: $$=\left( E\left((1-\frac{a}{n})^{X_1}\right)\right)^n$$ I wanted to go further by creating an additional random variable defined as $Z_i = (1-\frac{a}{n})^{X_i}$, calculating its expected value and then plugging it into the formula above, but I have problems with doing so. How should I approach this? $E(Z_i)=(1-\frac{a}{n})^{x}\cdot \frac{\lambda^x e^{-\lambda}}{x!}$ seems awful to calculate and I'm not even sure if it is the correct way.","Let $X_1, X_2, \ldots$ be i.i.d. and have the Poisson distribution with parameter $\lambda$. We define the estimator of $y=e^{-a\lambda}$ (where $a \neq 0$ is some constant value) as:   $$Y_n= \left( 1-\frac a n \right)^{\sum\limits_{i=1}^nX_i}$$   Is it an ubiased estimator of $y$? So $E(X_i) = \lambda$. I'm trying to calculate the bias with: $$E(Y_n)=E\left((1-\frac a n)^{\sum\limits_{i=1}^nX_i}\right) = E\left( (1-\frac{a}{n})^{X_1} \cdots (1-\frac{a}{n})^{X_n} \right)$$ which, since $X_i$ are i.i.d., equals: $$=\left( E\left((1-\frac{a}{n})^{X_1}\right)\right)^n$$ I wanted to go further by creating an additional random variable defined as $Z_i = (1-\frac{a}{n})^{X_i}$, calculating its expected value and then plugging it into the formula above, but I have problems with doing so. How should I approach this? $E(Z_i)=(1-\frac{a}{n})^{x}\cdot \frac{\lambda^x e^{-\lambda}}{x!}$ seems awful to calculate and I'm not even sure if it is the correct way.",,"['probability', 'probability-theory', 'statistics', 'parameter-estimation']"
28,Test of Independence: Combinatorics for P-values?,Test of Independence: Combinatorics for P-values?,,"I feel bad for posting a whole question, but I'm really stuck on it, and would appreciate anything even a hint to it! I'll show what I tried at the bottom. Question A test is used to determine if a sequence of binary numbers are independent, by counting the number of times $X$ that there is a change in response between consecutive trials in the sequence (from failure to success and vice versa). As an example, a sequence (let $S$ denote success and $F$ denote failure) is $$S \ S \ F \ S \ F \ S \ F \ S \ S \ F.$$ This sequence has $X = 7$ changes. $(i)$ Assume this test is an unconditional test, with some known probaility $p = 0.5$ where $n$ is the number of trials. Explain why $X \sim Bin(n-1,0.5)$. $(ii)$ Consider the 3 sequences: $$(A): \ \ \ F \ F \ S \ S \ S \ S \ S \ F \ F \ S \\ (B): \ \ \ S \ S \ F \ S \ F \ S \ F \ S \ S \ F \\ (C): \ \ \ S \ S \ S \ F \ S \ F \ S \ S \ F \ F.$$ Which test is the least consistent with a sequence of independent failures/successes? We are given the p-values for $X = 1,2,3,4,5,6,7,8,9$ being $0.002, \quad 0.020, \quad 0.090, \quad 0.254,  \quad0.500, \quad 0.746, \quad 0.910, \quad 0.980 , \quad0.998, \quad 1.000$ respectively. My attempts The amount of changes $K$ are dependent on how many $F's$ are squeezed between two adjacent $S's$. So for the case where $F$ does not start at the beginning or end, we require $\frac{K}{2}$ number of failures, where $K$ is even. The number of ways we can do this is: $\begin{pmatrix} n-1 \\ \frac{K}{2}\end{pmatrix}$. Sadly, this is only the case where $K$ is even and when the failures are not adjacent to other failures... $(ii)$ I feel like the most least ""consistent"" is $(A)$, but I'm not sure how to mathematically reason with this. Do we test a hypothesis that $p = 0.5$ vs. $p \neq 0.5$?","I feel bad for posting a whole question, but I'm really stuck on it, and would appreciate anything even a hint to it! I'll show what I tried at the bottom. Question A test is used to determine if a sequence of binary numbers are independent, by counting the number of times $X$ that there is a change in response between consecutive trials in the sequence (from failure to success and vice versa). As an example, a sequence (let $S$ denote success and $F$ denote failure) is $$S \ S \ F \ S \ F \ S \ F \ S \ S \ F.$$ This sequence has $X = 7$ changes. $(i)$ Assume this test is an unconditional test, with some known probaility $p = 0.5$ where $n$ is the number of trials. Explain why $X \sim Bin(n-1,0.5)$. $(ii)$ Consider the 3 sequences: $$(A): \ \ \ F \ F \ S \ S \ S \ S \ S \ F \ F \ S \\ (B): \ \ \ S \ S \ F \ S \ F \ S \ F \ S \ S \ F \\ (C): \ \ \ S \ S \ S \ F \ S \ F \ S \ S \ F \ F.$$ Which test is the least consistent with a sequence of independent failures/successes? We are given the p-values for $X = 1,2,3,4,5,6,7,8,9$ being $0.002, \quad 0.020, \quad 0.090, \quad 0.254,  \quad0.500, \quad 0.746, \quad 0.910, \quad 0.980 , \quad0.998, \quad 1.000$ respectively. My attempts The amount of changes $K$ are dependent on how many $F's$ are squeezed between two adjacent $S's$. So for the case where $F$ does not start at the beginning or end, we require $\frac{K}{2}$ number of failures, where $K$ is even. The number of ways we can do this is: $\begin{pmatrix} n-1 \\ \frac{K}{2}\end{pmatrix}$. Sadly, this is only the case where $K$ is even and when the failures are not adjacent to other failures... $(ii)$ I feel like the most least ""consistent"" is $(A)$, but I'm not sure how to mathematically reason with this. Do we test a hypothesis that $p = 0.5$ vs. $p \neq 0.5$?",,"['combinatorics', 'statistics']"
29,Maximum Likelihood Estimator of two independent random variables that share a mean,Maximum Likelihood Estimator of two independent random variables that share a mean,,"How do I find the MLE of a pair of normal variables $X = N(\bar{x}, \sigma_{X})$, $Y = N(\bar{x}, \sigma_{Y})$? I've found solutions for doing so when sigma is the common variable, but I haven't been able to follow the same process with the mean.","How do I find the MLE of a pair of normal variables $X = N(\bar{x}, \sigma_{X})$, $Y = N(\bar{x}, \sigma_{Y})$? I've found solutions for doing so when sigma is the common variable, but I haven't been able to follow the same process with the mean.",,"['statistics', 'parameter-estimation']"
30,Distribution of sample means explanation,Distribution of sample means explanation,,Say a casino has N slot machines all with the same expected value and standard deviation. Why is it when the standard deviation gets really large you end up with more machines under the expected value than over? I understand that the average of the samples will be approximately the expected value of the slot. But the samples means don't appear to be normally distributed. This seems not to be in line with the central limit theorem. I am wondering what bit I have confused. Thanks!,Say a casino has N slot machines all with the same expected value and standard deviation. Why is it when the standard deviation gets really large you end up with more machines under the expected value than over? I understand that the average of the samples will be approximately the expected value of the slot. But the samples means don't appear to be normally distributed. This seems not to be in line with the central limit theorem. I am wondering what bit I have confused. Thanks!,,['statistics']
31,Which mean is used for standard error?,Which mean is used for standard error?,,"While calculating the standard error, we take the standard deviation of the sampling distribution. But for calculating the standard deviation we must consider a 'mean' to take difference about. In Wikipedia its given that the 'mean' is the population mean while I came across a lecture which said it to be sampling distribution mean. So for standard error do we take the population mean or the sampling distribution mean as the 'mean'?","While calculating the standard error, we take the standard deviation of the sampling distribution. But for calculating the standard deviation we must consider a 'mean' to take difference about. In Wikipedia its given that the 'mean' is the population mean while I came across a lecture which said it to be sampling distribution mean. So for standard error do we take the population mean or the sampling distribution mean as the 'mean'?",,"['statistics', 'statistical-inference', 'standard-deviation', 'descriptive-statistics']"
32,A concise guide to basic statistics,A concise guide to basic statistics,,"I am asking for a reference in basic statistics following, in the same spirit as my similar question for probability here . That is it should: Assume some mathematical maturity and can make use of analysis, linear algebra, etc from page 1. On the other hand it should assume nothing itself on statistics itself. Be concise but still show examples. It should present the basics results of statistics (whatever that means, I am trying to say that I prefer a book focusing on few fundamental things). Not sloppy, but I am not looking for particularly abstract or research oriented viewpoint. What is it for: I would like to get some basic understanding of machine learning and other data science related techniques. Before doing so, I would like to get some basics in statistics. The rough amount of time I would like to dedicate to this is roughly 6-8 weeks. I am not pretending to learn everything which can be useful for data science in such a short time, but rather the most essential things which would allow me to have a look ahead with some real understanding, rather than just following recipes and copy-pasting code found somewhere.","I am asking for a reference in basic statistics following, in the same spirit as my similar question for probability here . That is it should: Assume some mathematical maturity and can make use of analysis, linear algebra, etc from page 1. On the other hand it should assume nothing itself on statistics itself. Be concise but still show examples. It should present the basics results of statistics (whatever that means, I am trying to say that I prefer a book focusing on few fundamental things). Not sloppy, but I am not looking for particularly abstract or research oriented viewpoint. What is it for: I would like to get some basic understanding of machine learning and other data science related techniques. Before doing so, I would like to get some basics in statistics. The rough amount of time I would like to dedicate to this is roughly 6-8 weeks. I am not pretending to learn everything which can be useful for data science in such a short time, but rather the most essential things which would allow me to have a look ahead with some real understanding, rather than just following recipes and copy-pasting code found somewhere.",,"['probability', 'statistics', 'reference-request', 'book-recommendation', 'machine-learning']"
33,What is the Maximum-Likelihood Estimator of this strange distribution?,What is the Maximum-Likelihood Estimator of this strange distribution?,,"Suppose there is a probability distribution for values of $x$ greater than $0$: $$p(x) \propto \frac{m}{(x+1)^{m+1}}$$ And we select from a sample of $\{X_1, X_2, \ldots ,X_n\}$ with all $X_i$ having this distribution. What is the maximum likelihood estimator of $m$? I tried to do this using the log-likelihood function method but it doesn't work because the log-likelihood function is not well behaved so I ended up concluding that the MLE of $m$ is $m=\max(X_n)$, similar to the continuous uniform distribution. Is this correct?","Suppose there is a probability distribution for values of $x$ greater than $0$: $$p(x) \propto \frac{m}{(x+1)^{m+1}}$$ And we select from a sample of $\{X_1, X_2, \ldots ,X_n\}$ with all $X_i$ having this distribution. What is the maximum likelihood estimator of $m$? I tried to do this using the log-likelihood function method but it doesn't work because the log-likelihood function is not well behaved so I ended up concluding that the MLE of $m$ is $m=\max(X_n)$, similar to the continuous uniform distribution. Is this correct?",,"['probability', 'statistics', 'probability-distributions', 'parameter-estimation']"
34,"For the Central Limit Theorem, why does the $\sqrt{n}$ term represent the convergence rate?","For the Central Limit Theorem, why does the  term represent the convergence rate?",\sqrt{n},"Suppose that $X_1, X_2, \ldots, $ is a sequence of iid random variables with $\mathbb{E}(X_i) = \mu$ and $Var(X_i) = \sigma^2 < \infty$. Define $S_n = \sum_{i=1}^{n}X_i$. Then as $n$ approaches infinity, we have that $\sqrt{n}(S_n - \mu)$ converges in distribution to a normal $N(0,\sigma^2)$ distribution: $$ \sqrt{n}\left(\left(\frac{1}{n}\sum_{i=1}^n X_i\right) - \mu\right)\ \xrightarrow{d}\ N\left(0,\sigma^2\right). $$ Assuming that $\sigma>0$, convergence in distribution is taken to be that the cumulative distribution functions of $\sqrt{n}(S_n - \mu)$ converge pointwise to the cdf of the $N(0, \sigma^2)$ distribution in that for every real number $z$: $$ \lim_{n\to\infty} \Pr\left[\sqrt{n}(S_n-\mu) \le z\right] = \Phi\left(\frac{z}{\sigma}\right) , $$ My Question: I am having a hard time understanding what is means when they say the above convergence happens with rate $\sqrt{n}$ . The notion of rate implies that it characterizes how fast something converges, but I am failing to see in the equation above where the slowness or the ""fastness"" is coming in. Can someone help me see this?","Suppose that $X_1, X_2, \ldots, $ is a sequence of iid random variables with $\mathbb{E}(X_i) = \mu$ and $Var(X_i) = \sigma^2 < \infty$. Define $S_n = \sum_{i=1}^{n}X_i$. Then as $n$ approaches infinity, we have that $\sqrt{n}(S_n - \mu)$ converges in distribution to a normal $N(0,\sigma^2)$ distribution: $$ \sqrt{n}\left(\left(\frac{1}{n}\sum_{i=1}^n X_i\right) - \mu\right)\ \xrightarrow{d}\ N\left(0,\sigma^2\right). $$ Assuming that $\sigma>0$, convergence in distribution is taken to be that the cumulative distribution functions of $\sqrt{n}(S_n - \mu)$ converge pointwise to the cdf of the $N(0, \sigma^2)$ distribution in that for every real number $z$: $$ \lim_{n\to\infty} \Pr\left[\sqrt{n}(S_n-\mu) \le z\right] = \Phi\left(\frac{z}{\sigma}\right) , $$ My Question: I am having a hard time understanding what is means when they say the above convergence happens with rate $\sqrt{n}$ . The notion of rate implies that it characterizes how fast something converges, but I am failing to see in the equation above where the slowness or the ""fastness"" is coming in. Can someone help me see this?",,"['probability', 'probability-theory', 'statistics', 'statistical-inference']"
35,"Distribution of $\frac{X}{X+Y}$, exponential","Distribution of , exponential",\frac{X}{X+Y},"On this site the distribution of $\frac{X}{X+Y}$ has been derived. Suppose $X$ and $Y$ are independently distributed exponential random variables with mean $1$. This means $f_X(x)=e^{-x}=P(X\geq x)$. Then it starts with For $t\in (0,1)$ $$P\left(\frac{X}{X+Y}\leq t\right)=P\left(\frac{X+Y}{X}\leq \frac1t\right)$$ $$=P(Y\geq X(\frac1t-1))$$ So far so good. But the next step I really don´t understand. $$=\int_0^{\infty} f_X(x)\cdot P(Y\geq x(\frac1t-1)) \, dx$$ It is obvious that $X \in (0, \infty)$. But the rest is unclear to me. I hope somebody can explain me why the two latter lines are equal. Thanks for your time calculus","On this site the distribution of $\frac{X}{X+Y}$ has been derived. Suppose $X$ and $Y$ are independently distributed exponential random variables with mean $1$. This means $f_X(x)=e^{-x}=P(X\geq x)$. Then it starts with For $t\in (0,1)$ $$P\left(\frac{X}{X+Y}\leq t\right)=P\left(\frac{X+Y}{X}\leq \frac1t\right)$$ $$=P(Y\geq X(\frac1t-1))$$ So far so good. But the next step I really don´t understand. $$=\int_0^{\infty} f_X(x)\cdot P(Y\geq x(\frac1t-1)) \, dx$$ It is obvious that $X \in (0, \infty)$. But the rest is unclear to me. I hope somebody can explain me why the two latter lines are equal. Thanks for your time calculus",,"['statistics', 'probability-distributions']"
36,What is the most efficient way to calculate $R^2$?,What is the most efficient way to calculate ?,R^2,Hello I am working on a question from an old exam paper and wondered what is the best way to tackle parts ii and iii. Given the data it is easy to find $\hat{\beta_0}=-1.071$ and $\hat{\beta_1}=2.741$. Now for part ii) I have the formula $R^2=1-SSE/SST$ where $SST=\sum(y_i-\bar{y})^2$ (easy to work out) and $SSE=\sum e_i^2=\sum (y_i-\hat{y_i})$. Likewise I have for part iii) An unbiased estimate of $\sigma^2$ is $\sum e_i^2/(n-2)$. Question: I wondered if there is a nice and more efficient way to work   out $\sum e_i^2$ or do I have to calculate each predicted value based   on the model take it away from the actual value square that value and   then sum all the values up?,Hello I am working on a question from an old exam paper and wondered what is the best way to tackle parts ii and iii. Given the data it is easy to find $\hat{\beta_0}=-1.071$ and $\hat{\beta_1}=2.741$. Now for part ii) I have the formula $R^2=1-SSE/SST$ where $SST=\sum(y_i-\bar{y})^2$ (easy to work out) and $SSE=\sum e_i^2=\sum (y_i-\hat{y_i})$. Likewise I have for part iii) An unbiased estimate of $\sigma^2$ is $\sum e_i^2/(n-2)$. Question: I wondered if there is a nice and more efficient way to work   out $\sum e_i^2$ or do I have to calculate each predicted value based   on the model take it away from the actual value square that value and   then sum all the values up?,,"['statistics', 'regression']"
37,Theory question: How to use Mean Absolute Error properly in a log scaled linear regression,Theory question: How to use Mean Absolute Error properly in a log scaled linear regression,,"First of all, I had a look here and in a couple of other questions: I couldn't find what I am looking for. So my question is purely theoretical (although I have an example by my hands). Suppose I have some data $(x_i,y_i)$ for $i=1,..,n$.  Suppose I fit the following models with IID $\epsilon_i \sim N(0, \sigma^2)$ for $i=1,..,n$ $M_1: \log(y_i)= \beta_0+\beta_1x_i+\epsilon_i$ $M_2: \log(y_i)= \beta_0+\beta_1x_i+\beta_2x_i^2+\epsilon_i$ $M_3: \log(y_i)= \beta_0+\beta_1x_i+\beta_2x_i^2+\beta_3x_i^3+\epsilon_i$ Now I want to see which of these models is better, so I use the following (maybe weird, but stay with me) method, to evaluate their ""predictive powers"": Use $(x_i, \log(y_i))$ for $i=1,..,\frac{n}{2}$, to fit $M_1, M_2, M_3$ respectively. Now use the fitted model (so $M_1, M_2,M_3$ respectively), to predict $y_i$'s using the $x_i$'s from the remaining $\frac{n}{2}$ data , so from $i = \frac{n}{2}+1, .., n$ (careful, predict $y_i$ not $\log(y_i)$) Use MAE or Mean Absolute Error ( here ) $MAE = \frac{1}{\frac{n}{2}}\sum_{i=\frac{n}{2}+1}^{n}|y_i-\hat{y}_i|$, being careful that $\hat{y}_i$ is in the original scale of values! So now my question: If I do point $1.$ and I fit the three models (hence obtaining estimates for the parameters, their standard errors etc..) and then use these parameters (respectively of course!) to predict the responses of the other $x_i$'s: Will I be predicting $\log(y_i)$'s right? And this is true...  Is it also true that in order to get $\hat{y}_i$'s , instead of $\widehat{\log{(y)}}_i$, I should just take the exponential of those terms? So in general, is it true $\hat{y}_i = e^{\widehat{\log{(y)}}_i}$? Once I find the three MAE's, how do I judge the models? Should I be looking for the one with smaller MAE? EDIT For example suppose I have $1000$ data points. I use the first $500$ to fit model $M_1$. Once I've fitted it, I can predict new values. Hence I predict the new responses of the other $500$ $x_i$'s left. of course, the prediction will be given in logarithmic scale. But I want to calculate MAE on the normal scale. This is the context of my question, of course I would do this procedure for all the three models and compare the MAEs.","First of all, I had a look here and in a couple of other questions: I couldn't find what I am looking for. So my question is purely theoretical (although I have an example by my hands). Suppose I have some data $(x_i,y_i)$ for $i=1,..,n$.  Suppose I fit the following models with IID $\epsilon_i \sim N(0, \sigma^2)$ for $i=1,..,n$ $M_1: \log(y_i)= \beta_0+\beta_1x_i+\epsilon_i$ $M_2: \log(y_i)= \beta_0+\beta_1x_i+\beta_2x_i^2+\epsilon_i$ $M_3: \log(y_i)= \beta_0+\beta_1x_i+\beta_2x_i^2+\beta_3x_i^3+\epsilon_i$ Now I want to see which of these models is better, so I use the following (maybe weird, but stay with me) method, to evaluate their ""predictive powers"": Use $(x_i, \log(y_i))$ for $i=1,..,\frac{n}{2}$, to fit $M_1, M_2, M_3$ respectively. Now use the fitted model (so $M_1, M_2,M_3$ respectively), to predict $y_i$'s using the $x_i$'s from the remaining $\frac{n}{2}$ data , so from $i = \frac{n}{2}+1, .., n$ (careful, predict $y_i$ not $\log(y_i)$) Use MAE or Mean Absolute Error ( here ) $MAE = \frac{1}{\frac{n}{2}}\sum_{i=\frac{n}{2}+1}^{n}|y_i-\hat{y}_i|$, being careful that $\hat{y}_i$ is in the original scale of values! So now my question: If I do point $1.$ and I fit the three models (hence obtaining estimates for the parameters, their standard errors etc..) and then use these parameters (respectively of course!) to predict the responses of the other $x_i$'s: Will I be predicting $\log(y_i)$'s right? And this is true...  Is it also true that in order to get $\hat{y}_i$'s , instead of $\widehat{\log{(y)}}_i$, I should just take the exponential of those terms? So in general, is it true $\hat{y}_i = e^{\widehat{\log{(y)}}_i}$? Once I find the three MAE's, how do I judge the models? Should I be looking for the one with smaller MAE? EDIT For example suppose I have $1000$ data points. I use the first $500$ to fit model $M_1$. Once I've fitted it, I can predict new values. Hence I predict the new responses of the other $500$ $x_i$'s left. of course, the prediction will be given in logarithmic scale. But I want to calculate MAE on the normal scale. This is the context of my question, of course I would do this procedure for all the three models and compare the MAEs.",,"['statistics', 'transformation', 'regression', 'mathematical-modeling']"
38,Total number of possible combinations for a vector of variable length,Total number of possible combinations for a vector of variable length,,"I am trying to understand the formula behind generalizing the computation of the number of unique combinations possible for a vector of length n. Say x=(a,b,c) with n  = 3, the number of possible combinations would be: [3 choose 1 (a,b,c) + 3 choose 2 (ab, ac, bc) + 3 choose 3 (abc) ]. While if n is 4, the number of possible combinations would be [4 choose 1 (a,b,c,d) + 4 choose 2 (ab, ac, ad, bc, bd, cd) + 4 choose 3 (abc, abd, bcd, acd) + 4 choose 1 (abcd)], and so on for increasing number of n. But what is the general formuation for these type of combinations? Thanks for any help!  Fra","I am trying to understand the formula behind generalizing the computation of the number of unique combinations possible for a vector of length n. Say x=(a,b,c) with n  = 3, the number of possible combinations would be: [3 choose 1 (a,b,c) + 3 choose 2 (ab, ac, bc) + 3 choose 3 (abc) ]. While if n is 4, the number of possible combinations would be [4 choose 1 (a,b,c,d) + 4 choose 2 (ab, ac, ad, bc, bd, cd) + 4 choose 3 (abc, abd, bcd, acd) + 4 choose 1 (abcd)], and so on for increasing number of n. But what is the general formuation for these type of combinations? Thanks for any help!  Fra",,"['statistics', 'combinations']"
39,Find Expected value,Find Expected value,,"Suppose, $X \sim \operatorname{Poisson}(\lambda)$, Then what is  $\operatorname{E}\left(\dfrac{1}{X+1}\right)$ ? My attempt: Let $Y = \dfrac{1}{X+1}.$ Then $$ F_Y(t) = \{ Y \leq t\} = \left\{ X \geq \frac{1-t} t \right\} = 1- F_Y \left( \frac{1-t} t \right). $$ is this way okay? Any hint/alternative way to solve this ...","Suppose, $X \sim \operatorname{Poisson}(\lambda)$, Then what is  $\operatorname{E}\left(\dfrac{1}{X+1}\right)$ ? My attempt: Let $Y = \dfrac{1}{X+1}.$ Then $$ F_Y(t) = \{ Y \leq t\} = \left\{ X \geq \frac{1-t} t \right\} = 1- F_Y \left( \frac{1-t} t \right). $$ is this way okay? Any hint/alternative way to solve this ...",,"['statistics', 'statistical-inference', 'descriptive-statistics', 'stationary-processes']"
40,Intuitive understanding of probability puzzle,Intuitive understanding of probability puzzle,,"Question : The graduate school at the University of California,Berkeley, is composed of two departments - call them A and B. At department A the acceptance rate is 0.5 for both men and women. At department B, however, the rate is 0.1 for men and 0.2 for women. Overall acceptance rate at Berkeley, both departments combined, is 0.3 for men but only 0.25 for women. What fraction of female applicants choose department A? My (stupid) answer : I tried to devise a primitive formula: $ a*Pa+b*(1-Pa) $ Where a - Percentage of accepted students in A department, b -Percentage of accepted students in B department, Pa - Percentage of accepted A students in sum of all accepted students. I checked this formula with several different possibilities an it worked, still it may be wrong, but i don't really understand problem intuitively, i don't see the reason behind why the formula is as 'is', if someone understands the problem intuitively, please tell me intuitive way of solving this and similar problems.","Question : The graduate school at the University of California,Berkeley, is composed of two departments - call them A and B. At department A the acceptance rate is 0.5 for both men and women. At department B, however, the rate is 0.1 for men and 0.2 for women. Overall acceptance rate at Berkeley, both departments combined, is 0.3 for men but only 0.25 for women. What fraction of female applicants choose department A? My (stupid) answer : I tried to devise a primitive formula: $ a*Pa+b*(1-Pa) $ Where a - Percentage of accepted students in A department, b -Percentage of accepted students in B department, Pa - Percentage of accepted A students in sum of all accepted students. I checked this formula with several different possibilities an it worked, still it may be wrong, but i don't really understand problem intuitively, i don't see the reason behind why the formula is as 'is', if someone understands the problem intuitively, please tell me intuitive way of solving this and similar problems.",,"['statistics', 'intuition']"
41,Covariance of two jointly continuous random variables,Covariance of two jointly continuous random variables,,"I need to solve the following question. I apologize in advance not quite sure how to note piece wise functions. $$ f(x,y)=\begin{cases}72x^2y(1-x)(1-y),& 0 \leq x\leq1, 0\leq y \leq 1\\ 0,& \mathrm{otherwise.} \end{cases} $$ Now I know that the $\operatorname{Cov}(X)=E[XY]-E[X]E[Y]$ and I have attempted to solve by using the same equation for the above continuous function, however, I'm not sure if I am doing this right. By taking the expected values of $x$ and $y$ seperately, there will be variables left and it won't give an exact constant as an answer. For example: $$E[X]=\int_0^1x\times72x^2y(1-x)(1-y)dx$$ I'm not sure if I'm doing this right. Also, the next question is: Determine $P({X>Y})$. Which I don't know how to solve","I need to solve the following question. I apologize in advance not quite sure how to note piece wise functions. $$ f(x,y)=\begin{cases}72x^2y(1-x)(1-y),& 0 \leq x\leq1, 0\leq y \leq 1\\ 0,& \mathrm{otherwise.} \end{cases} $$ Now I know that the $\operatorname{Cov}(X)=E[XY]-E[X]E[Y]$ and I have attempted to solve by using the same equation for the above continuous function, however, I'm not sure if I am doing this right. By taking the expected values of $x$ and $y$ seperately, there will be variables left and it won't give an exact constant as an answer. For example: $$E[X]=\int_0^1x\times72x^2y(1-x)(1-y)dx$$ I'm not sure if I'm doing this right. Also, the next question is: Determine $P({X>Y})$. Which I don't know how to solve",,"['probability', 'statistics', 'covariance', 'means']"
42,Finding the expected value of a geometric random variable,Finding the expected value of a geometric random variable,,"A monkey is sitting at a simplified keyboard that only includes the keys “a”, “b”, and “c”. The monkey presses the keys at random. Let X be the number of keys pressed until the monkey has pressed all the different keys at least once. For example, if the monkey typed “accaacbcaaac. . . ” then X would equal $7$ whereas if the monkey typed “cbaccaabbcab. . . ” then X would equal $3$ What is the expected value of $X$? I know that to get the expected value of a geometric random variable, its just $\frac{1}{p}$. However, $X$ is just the number of keys pressed until all characters have been pressed. What is the probability of $X$ to begin with?","A monkey is sitting at a simplified keyboard that only includes the keys “a”, “b”, and “c”. The monkey presses the keys at random. Let X be the number of keys pressed until the monkey has pressed all the different keys at least once. For example, if the monkey typed “accaacbcaaac. . . ” then X would equal $7$ whereas if the monkey typed “cbaccaabbcab. . . ” then X would equal $3$ What is the expected value of $X$? I know that to get the expected value of a geometric random variable, its just $\frac{1}{p}$. However, $X$ is just the number of keys pressed until all characters have been pressed. What is the probability of $X$ to begin with?",,"['probability', 'statistics', 'random-variables']"
43,Is there a way to get an uniform distribution from a gauss distribution,Is there a way to get an uniform distribution from a gauss distribution,,"I have seen some implementations of ""true"" random number generators. However they are all producing a gauss distribution. I have asked myself, if it would be possible to run these numbers through some sort of algorithm, so that one can get an uniform distribution in the end? I guess it is impossible, can someone explain to me, why?","I have seen some implementations of ""true"" random number generators. However they are all producing a gauss distribution. I have asked myself, if it would be possible to run these numbers through some sort of algorithm, so that one can get an uniform distribution in the end? I guess it is impossible, can someone explain to me, why?",,"['statistics', 'probability-distributions', 'normal-distribution', 'random']"
44,"Derive method of moments estimator of $\theta$ for a uniform distribution on $(0, \theta)$",Derive method of moments estimator of  for a uniform distribution on,"\theta (0, \theta)","Let $X_1, \ldots, X_n$ be a random sample (i.i.d.) from a uniform distribution on $[0, \theta]$ , where $\theta$ is unknown. Derive the method of moments estimator of $\theta$ and find its expectation and variance. Is this estimator consistent? Justify your answer. Here I got the answer for expectation is $\dfrac{\theta}{2}$ . Can anyone help with the variance please?","Let be a random sample (i.i.d.) from a uniform distribution on , where is unknown. Derive the method of moments estimator of and find its expectation and variance. Is this estimator consistent? Justify your answer. Here I got the answer for expectation is . Can anyone help with the variance please?","X_1, \ldots, X_n [0, \theta] \theta \theta \dfrac{\theta}{2}","['statistics', 'statistical-inference', 'uniform-distribution', 'variance', 'parameter-estimation']"
45,Confusion - Correlation between -1 and 1,Confusion - Correlation between -1 and 1,,"So in the text book, it has the following proof of correlation being between $-1$ and $1$: This proof seems solid, but the only thing I am not clear about is: Why is the variance of the ratio $X$ and $\sigma$, i.e., $Var(X/\sigma$) equal to $Var(X)/\sigma^2_x$?","So in the text book, it has the following proof of correlation being between $-1$ and $1$: This proof seems solid, but the only thing I am not clear about is: Why is the variance of the ratio $X$ and $\sigma$, i.e., $Var(X/\sigma$) equal to $Var(X)/\sigma^2_x$?",,"['statistics', 'proof-verification', 'proof-explanation', 'correlation', 'variance']"
46,Expected value (mean) of $e^Z$ where $Z$ poisson distributed,Expected value (mean) of  where  poisson distributed,e^Z Z,"I have $Z \sim \operatorname{po}(1)$, and must find $E(e^{Z})$. My method was as follows. I know that for a function $p$ with support $\{x_i \mid i \in I\}$: $$E(g(X)) = \sum_{i\in I} g(x_i)\cdot p(x_i) $$ So I plop my transformed variable in, and I get the following: $$E(e^Z) = \sum_{i=0}^\infty e^i \cdot p(z_i)$$ And the poisson probability mass function: $$E(e^Z) = \sum_{i=0}^\infty e^i \cdot e^\lambda \cdot \frac{\lambda^i}{i!}$$ And I'm not sure how to proceed from here... I took a sneak peak on wolfram, and saw that this sum converges on $e^{(e-1)\cdot \lambda}$, and since our lambda parameter is $1$, $e^{(e-1)}$. But I'm not sure how can, in hand, reach this result.","I have $Z \sim \operatorname{po}(1)$, and must find $E(e^{Z})$. My method was as follows. I know that for a function $p$ with support $\{x_i \mid i \in I\}$: $$E(g(X)) = \sum_{i\in I} g(x_i)\cdot p(x_i) $$ So I plop my transformed variable in, and I get the following: $$E(e^Z) = \sum_{i=0}^\infty e^i \cdot p(z_i)$$ And the poisson probability mass function: $$E(e^Z) = \sum_{i=0}^\infty e^i \cdot e^\lambda \cdot \frac{\lambda^i}{i!}$$ And I'm not sure how to proceed from here... I took a sneak peak on wolfram, and saw that this sum converges on $e^{(e-1)\cdot \lambda}$, and since our lambda parameter is $1$, $e^{(e-1)}$. But I'm not sure how can, in hand, reach this result.",,"['sequences-and-series', 'statistics', 'poisson-distribution']"
47,How much variance does the random Mersenne twister algorithm allow,How much variance does the random Mersenne twister algorithm allow,,"I'm sorry if I don't know how to phrase this question properly, but I would like to know when using a random number generator like the Mersenne twister, and I generate  numbers which fall into a range [1..5] for example. I understand that I should then get an even distribution of those numbers (20% of each) but generally there is always a ""variance"" of that count (for example the number ""1"" would come out 18% of the time while ""5"" 22%). How much of this variance is allowed and expected? Is there a proper term for this value? Is this dependent on different random number generator algorithms, and if so which ones are better at giving even distributions?","I'm sorry if I don't know how to phrase this question properly, but I would like to know when using a random number generator like the Mersenne twister, and I generate  numbers which fall into a range [1..5] for example. I understand that I should then get an even distribution of those numbers (20% of each) but generally there is always a ""variance"" of that count (for example the number ""1"" would come out 18% of the time while ""5"" 22%). How much of this variance is allowed and expected? Is there a proper term for this value? Is this dependent on different random number generator algorithms, and if so which ones are better at giving even distributions?",,"['statistics', 'random']"
48,"Chebyshev's Inequality: given probability, find $k$","Chebyshev's Inequality: given probability, find",k,"Edit with Context: Book says the % of data captured within k standard deviations $= 1 - \dfrac{1}{k^2}$ . Dug a bit deeper and found it was derived using Chebyshev's but no direct derivation found $\ldots$ Within how many multiples of standard deviation will capture at least $\boldsymbol{75}$ % of the data in a distribution with a mean $\boldsymbol\mu$ ? I derived the formula below and got that $k$ must be equal to or less than $k$ 2. This doesn't make sense to me as a larger $k$ would capture more and more data, so it should be the other way around. Work is shown below. The inequality derivation: Let $v = |X-\mu|$ . Let $y = k\sigma$ . Then $$P(v \geq y) \leq \frac{1}{k^2} = 1 - P(v < y) \leq \frac{1}{k^2}.$$ So $$k \leq \sqrt\frac{1}{1-P(v<y)}.$$","Edit with Context: Book says the % of data captured within k standard deviations . Dug a bit deeper and found it was derived using Chebyshev's but no direct derivation found Within how many multiples of standard deviation will capture at least % of the data in a distribution with a mean ? I derived the formula below and got that must be equal to or less than 2. This doesn't make sense to me as a larger would capture more and more data, so it should be the other way around. Work is shown below. The inequality derivation: Let . Let . Then So",= 1 - \dfrac{1}{k^2} \ldots \boldsymbol{75} \boldsymbol\mu k k k v = |X-\mu| y = k\sigma P(v \geq y) \leq \frac{1}{k^2} = 1 - P(v < y) \leq \frac{1}{k^2}. k \leq \sqrt\frac{1}{1-P(v<y)}.,"['probability', 'statistics']"
49,"This estimator, why is it good?","This estimator, why is it good?",,"I've been given the following as an estimate for the parameter of a Poisson distribution, and must explain why it is a sensible estimate. $$\log\left(\frac1n \sum_{i=1}^n 2^{x_i} \right)               $$ I'm pretty confused because I've covered method of moments estimators and the MLE, but this looks like neither of those. Any advice as to what I'm missing would be much appreciated. Note: The log is the natural logarithm - they like to use that instead of ln for clarity and say that ln can easily confuse some expressions.","I've been given the following as an estimate for the parameter of a Poisson distribution, and must explain why it is a sensible estimate. $$\log\left(\frac1n \sum_{i=1}^n 2^{x_i} \right)               $$ I'm pretty confused because I've covered method of moments estimators and the MLE, but this looks like neither of those. Any advice as to what I'm missing would be much appreciated. Note: The log is the natural logarithm - they like to use that instead of ln for clarity and say that ln can easily confuse some expressions.",,"['statistics', 'parameter-estimation']"
50,Show that $\left(X_{(1)} + X_{(n)}\right)/2$ is a consistent estimator for $\theta$,Show that  is a consistent estimator for,\left(X_{(1)} + X_{(n)}\right)/2 \theta,"Let $X_1, \ldots , X_n$ be a random sample from the uniform distribution on the interval $(\theta − 1/2, \theta + 1/2)$, where $\theta$ is unknown. Let $X_{(1)} = \min(X_1, \ldots , X_n)$ $X_{(n)} = \max(X_1, \ldots , X_n).$ Show that $\left(X_{(1)} + X_{(n)}\right)/2$ is a consistent estimator for $\theta$. Not really sure where to start with this. I tried finding the MLE and saying that is was a consistent estimator but found that the fisher information is 0. I also tried using the MME but that got me no where.","Let $X_1, \ldots , X_n$ be a random sample from the uniform distribution on the interval $(\theta − 1/2, \theta + 1/2)$, where $\theta$ is unknown. Let $X_{(1)} = \min(X_1, \ldots , X_n)$ $X_{(n)} = \max(X_1, \ldots , X_n).$ Show that $\left(X_{(1)} + X_{(n)}\right)/2$ is a consistent estimator for $\theta$. Not really sure where to start with this. I tried finding the MLE and saying that is was a consistent estimator but found that the fisher information is 0. I also tried using the MME but that got me no where.",,"['probability', 'statistics', 'uniform-distribution', 'parameter-estimation']"
51,Isn't probability a branch of combinatorics?,Isn't probability a branch of combinatorics?,,"I've heard it a lot that Probability is a branch of Statistics(which isn't a branch of mathematics) but as far as I know(High School) it appeals more as the branch of Combinatorics though. So, can you kindly explain the reasoning behind considering Probability as a branch of Statistics and not as Combinatorics(& Maths in general).","I've heard it a lot that Probability is a branch of Statistics(which isn't a branch of mathematics) but as far as I know(High School) it appeals more as the branch of Combinatorics though. So, can you kindly explain the reasoning behind considering Probability as a branch of Statistics and not as Combinatorics(& Maths in general).",,"['probability', 'combinatorics', 'statistics']"
52,"$E(X)$, $Var(X) $, and $Mgf_x(t)$ of a continuous uniform random variable on $[a,b]$",", , and  of a continuous uniform random variable on","E(X) Var(X)  Mgf_x(t) [a,b]","I am wondering if I have found the $Var(X), E(X), Mgf_x(t)$ for a continuous uniform random variable on [a,b] correctly. My solution is below, could someone check whether its correct and correct me if needs be. Thanks","I am wondering if I have found the $Var(X), E(X), Mgf_x(t)$ for a continuous uniform random variable on [a,b] correctly. My solution is below, could someone check whether its correct and correct me if needs be. Thanks",,"['probability', 'statistics', 'proof-writing']"
53,"If we have a full rank positive semi-definite symmetric matrix, will any square partition be full rank as well?","If we have a full rank positive semi-definite symmetric matrix, will any square partition be full rank as well?",,"Suppose we have a matrix of $n$ by $n$ dimension $M$ is that is full rank , symmetric, and positive semi-definite , in that $z^TMz \geq 0$ for all $z \in \mathbb{R}^p$. This can be thought of as a covariance matrix in statistics. If I were to take a square partition, would that square partition still be full rank? In example, suppose that: $$ M = \begin{pmatrix} a_{11} \ldots a_{1n}\\ \ldots\\ a_{n1} \ldots a_{nn}\\ \end{pmatrix} $$ Then a square partition might be: $$ M_{33} = \begin{pmatrix} a_{33} \ldots a_{3n}\\ \ldots\\ a_{n3} \ldots a_{nn}\\ \end{pmatrix} $$ Where I took the bottom right side of the original matrix $M$. Would this be full-rank as well?","Suppose we have a matrix of $n$ by $n$ dimension $M$ is that is full rank , symmetric, and positive semi-definite , in that $z^TMz \geq 0$ for all $z \in \mathbb{R}^p$. This can be thought of as a covariance matrix in statistics. If I were to take a square partition, would that square partition still be full rank? In example, suppose that: $$ M = \begin{pmatrix} a_{11} \ldots a_{1n}\\ \ldots\\ a_{n1} \ldots a_{nn}\\ \end{pmatrix} $$ Then a square partition might be: $$ M_{33} = \begin{pmatrix} a_{33} \ldots a_{3n}\\ \ldots\\ a_{n3} \ldots a_{nn}\\ \end{pmatrix} $$ Where I took the bottom right side of the original matrix $M$. Would this be full-rank as well?",,"['linear-algebra', 'statistics']"
54,For what values of $k$ is $p(x) = k(1-r^2)^x$ a valid probability mass function,For what values of  is  a valid probability mass function,k p(x) = k(1-r^2)^x,"The question is asking for values for $k$ making this a valid probability mass function. where $$P(x) =  k(1-r^2)^x$$ for $x = 0,1,2...$ and $P(x)= 0$ otherwise $r$ is elected from interval $(0,1)$ I'm thinking this is a discrete probability distribution that needs to sum to 1, but the text is very light on and I'm at a bit of a loss how to solve this. Any help appreciated","The question is asking for values for $k$ making this a valid probability mass function. where $$P(x) =  k(1-r^2)^x$$ for $x = 0,1,2...$ and $P(x)= 0$ otherwise $r$ is elected from interval $(0,1)$ I'm thinking this is a discrete probability distribution that needs to sum to 1, but the text is very light on and I'm at a bit of a loss how to solve this. Any help appreciated",,"['probability', 'statistics', 'probability-distributions']"
55,Shouldn't sample standard deviation DECREASE with increased sample size?,Shouldn't sample standard deviation DECREASE with increased sample size?,,"I'm doing an experiment comparing 10,000 rolls of 5 dice (those 5 rolls were then summed, so the population mean is 17.5) and dividing them into sets of ten with sample size N = 10. The same experiment is repeated except with 3000 rolls, but the sample size decreases to N = 3. Both experiments have 1000 samples, just with different sample sizes. Now, I took the AVERAGE sample standard deviation for each experiment and to my great confusion, the experiment with the SMALLER sample size had a SMALLER average sample standard deviation than the experiment with the LARGER sample size. I thought this was a fluke and re-generated the random numbers only to find the same result. In other words, this means that the sample standard deviation for each sample of N =3 is on average smaller than each sample of N = 10.  How is this possible? Intuitively, one would think that with a larger sample size the ""spread"" between values would be decreased because any individual improbable value would have less effect on the spread. Sort of like for the mean. While for a smaller sample size, the variables would be less ""controlled"" and thus on average more spread out. Is my intuition just wrong? And is this plain to see mathematically? FOLLOW UP: I talked to my professor and he said that a weird phenomenon happens in statistics where sample standard deviations tend to be underestimations rather than overestimations of the population standard deviation. And that as sample size goes up, the sample standard deviation goes up because it becomes ""more accurate."" Is this true? And why does that happen? Is there both an intuitive explanation as well as a mathematical proof?","I'm doing an experiment comparing 10,000 rolls of 5 dice (those 5 rolls were then summed, so the population mean is 17.5) and dividing them into sets of ten with sample size N = 10. The same experiment is repeated except with 3000 rolls, but the sample size decreases to N = 3. Both experiments have 1000 samples, just with different sample sizes. Now, I took the AVERAGE sample standard deviation for each experiment and to my great confusion, the experiment with the SMALLER sample size had a SMALLER average sample standard deviation than the experiment with the LARGER sample size. I thought this was a fluke and re-generated the random numbers only to find the same result. In other words, this means that the sample standard deviation for each sample of N =3 is on average smaller than each sample of N = 10.  How is this possible? Intuitively, one would think that with a larger sample size the ""spread"" between values would be decreased because any individual improbable value would have less effect on the spread. Sort of like for the mean. While for a smaller sample size, the variables would be less ""controlled"" and thus on average more spread out. Is my intuition just wrong? And is this plain to see mathematically? FOLLOW UP: I talked to my professor and he said that a weird phenomenon happens in statistics where sample standard deviations tend to be underestimations rather than overestimations of the population standard deviation. And that as sample size goes up, the sample standard deviation goes up because it becomes ""more accurate."" Is this true? And why does that happen? Is there both an intuitive explanation as well as a mathematical proof?",,"['probability', 'statistics']"
56,How to not use stars and bars to solve unordered sampling with replacement?,How to not use stars and bars to solve unordered sampling with replacement?,,"Say I want to find the number of different ways of unordered sampling k elements from n distinguishable elements with replacement, and a common way is to use the method of stars and bars . Although this is artful and easy to understand, I want to know whether there are other methods to solve this problem, and if there are, what are those methods? And the more direct, the rawer, the better! BTW, I have searched in a few textbooks on this problem, and it seems all of them are using the above method.","Say I want to find the number of different ways of unordered sampling k elements from n distinguishable elements with replacement, and a common way is to use the method of stars and bars . Although this is artful and easy to understand, I want to know whether there are other methods to solve this problem, and if there are, what are those methods? And the more direct, the rawer, the better! BTW, I have searched in a few textbooks on this problem, and it seems all of them are using the above method.",,"['combinatorics', 'probability-theory', 'statistics']"
57,Total number of combinations for boxes that can hold a maximum of 2 objects.,Total number of combinations for boxes that can hold a maximum of 2 objects.,,"Consider an $N$ balls than can be placed in $n$ number of boxes. Where: $n > N/2$ Suppose each boxes can only hold a maximum of two balls, what is the formula for finding the total number of combinations if each balls are considered distinguishable?","Consider an $N$ balls than can be placed in $n$ number of boxes. Where: $n > N/2$ Suppose each boxes can only hold a maximum of two balls, what is the formula for finding the total number of combinations if each balls are considered distinguishable?",,"['probability', 'combinatorics', 'statistics']"
58,support on a triangle and uniform marginal distributions,support on a triangle and uniform marginal distributions,,"Suppose $X$ and $Y$ are jointly distributed on the support $\operatorname{conv} \{(0,0),(0,1),(1,0)\}$ with the joint PDF $f>0$ everywhere on the support. Is it possible to find $f$ such that the marginal PDFs are given by $f_X(x)= 1$ for all $x\in[0,1]$ and $f_Y(y)=1$ for all $y\in [0,1]$? Thanks","Suppose $X$ and $Y$ are jointly distributed on the support $\operatorname{conv} \{(0,0),(0,1),(1,0)\}$ with the joint PDF $f>0$ everywhere on the support. Is it possible to find $f$ such that the marginal PDFs are given by $f_X(x)= 1$ for all $x\in[0,1]$ and $f_Y(y)=1$ for all $y\in [0,1]$? Thanks",,"['probability', 'statistics', 'multivariable-calculus', 'probability-distributions']"
59,Computing expected cost for exponential random variable,Computing expected cost for exponential random variable,,"I am reading Probability and Statistics for Engineering and the Sciences . Exercise 63, Chapter 4 says: A consumer is trying to decide between two long-distance calling plans. The first one charges a flat rate of 10¢ per minute, whereas the second charges a flat rate of 99¢ for calls up to 20 minutes in duration and then 10¢ for each additional minute exceeding 20 (assume that calls lasting a noninteger number of minutes are charged proportionately to a whole-minute’s charge). Suppose the consumer’s distribution of call duration is exponential with parameter $\lambda$ . Which plan is better if expected call duration is 10 minutes? 15 minutes? Assuming the first question, when the duration is 10 minutes, I computed the cost of the first plan as: $h_1(x) = 10 * E[x] = 10 * 10 = 100$ However, how do I compute the cost for the second plan ( $h_2(x)$ ) ? I tried with: $h_2(x) = 99 * F(x \leq 20) + 10 * (1 - F(x \leq 20)) \approx  87$ But the correct result is $112.53$ .","I am reading Probability and Statistics for Engineering and the Sciences . Exercise 63, Chapter 4 says: A consumer is trying to decide between two long-distance calling plans. The first one charges a flat rate of 10¢ per minute, whereas the second charges a flat rate of 99¢ for calls up to 20 minutes in duration and then 10¢ for each additional minute exceeding 20 (assume that calls lasting a noninteger number of minutes are charged proportionately to a whole-minute’s charge). Suppose the consumer’s distribution of call duration is exponential with parameter . Which plan is better if expected call duration is 10 minutes? 15 minutes? Assuming the first question, when the duration is 10 minutes, I computed the cost of the first plan as: However, how do I compute the cost for the second plan ( ) ? I tried with: But the correct result is .",\lambda h_1(x) = 10 * E[x] = 10 * 10 = 100 h_2(x) h_2(x) = 99 * F(x \leq 20) + 10 * (1 - F(x \leq 20)) \approx  87 112.53,"['probability', 'statistics', 'exponential-distribution']"
60,Positive semidefinite ordering for covariance matrices,Positive semidefinite ordering for covariance matrices,,"Suppose that X and Z are matrices with the same number of rows. Let $$ D = \left[\begin{array}{cc} X' X & X'Z \\ Z'X & Z'Z \end{array} \right]^{-1} - \left[\begin{array}{cc} (X' X)^{-1} & 0 \\ 0 & 0 \end{array} \right],$$ where all inverses are assumed to exist and the zeros represent zero matrices of suitable dimensions. How can we prove that $D$ is positive semidefinite?","Suppose that X and Z are matrices with the same number of rows. Let $$ D = \left[\begin{array}{cc} X' X & X'Z \\ Z'X & Z'Z \end{array} \right]^{-1} - \left[\begin{array}{cc} (X' X)^{-1} & 0 \\ 0 & 0 \end{array} \right],$$ where all inverses are assumed to exist and the zeros represent zero matrices of suitable dimensions. How can we prove that $D$ is positive semidefinite?",,"['linear-algebra', 'statistics', 'positive-semidefinite']"
61,Calculate probability from a Negative Binomial Distribution with multiple success scenarios - 7th child born is either 3rd or 4th boy,Calculate probability from a Negative Binomial Distribution with multiple success scenarios - 7th child born is either 3rd or 4th boy,,"So the problem I am trying to solve states that, if a newborn is equally likely to be a boy or girl, what is the probability that 1) the third child is the first boy 2) the eighth child is the second girl and 3) the seventh child is the third OR fourth boy. This seems to be a pretty straightforward Negative Binomial distribution problem, with X~NegBin(k,0.5) where k is the number of ""successes,"" and I figured since they are equally likely, asking whether the Xth child is a boy or girl can be interchanged, so for numbers 1 and 2, they can just get plugged right into the equation for Neg. Bin. Distributions. I got 0.125 (1 * 0.5^3) for 1, and 0.02735 (7 * 0.5^8) for 2, but my question is with the changing success scenario on number 3. I was thinking it might involve some sort of intersection operation on k=3 and k=4 for P(X=7), but I'm not sure that that makes sense, or if there is a better way to think about this problem. Any tips on this are appreciated!","So the problem I am trying to solve states that, if a newborn is equally likely to be a boy or girl, what is the probability that 1) the third child is the first boy 2) the eighth child is the second girl and 3) the seventh child is the third OR fourth boy. This seems to be a pretty straightforward Negative Binomial distribution problem, with X~NegBin(k,0.5) where k is the number of ""successes,"" and I figured since they are equally likely, asking whether the Xth child is a boy or girl can be interchanged, so for numbers 1 and 2, they can just get plugged right into the equation for Neg. Bin. Distributions. I got 0.125 (1 * 0.5^3) for 1, and 0.02735 (7 * 0.5^8) for 2, but my question is with the changing success scenario on number 3. I was thinking it might involve some sort of intersection operation on k=3 and k=4 for P(X=7), but I'm not sure that that makes sense, or if there is a better way to think about this problem. Any tips on this are appreciated!",,"['probability', 'statistics']"
62,Total Law of Probability Question,Total Law of Probability Question,,"In a community $25\%$ of the residents are smokers. Suppose $30\%$ of the smokers claim that they don’t smoke, and all non-smokers say they don’t smoke. What is the probability that when someone says that he does not smoke, he is telling the truth? Using the total law of probability, $$P(A \hspace{.1cm} \text{and} \hspace{.1cm} B^c) = 0.25$$ $$P(A \hspace{.1cm} \text{and} \hspace{.1cm} B) = 0.85 + (0.3*0.25) = 0.925$$ $$P(A^c \hspace{.1cm} \text{and} \hspace{.1cm} B) = 0.85$$ from here, I'm stuck because my thought was to take the total number of people claiming they don't smoke over the number of people who don't smoke but that gives a probability greater than $1$ which is incorrect. Any tips?","In a community $25\%$ of the residents are smokers. Suppose $30\%$ of the smokers claim that they don’t smoke, and all non-smokers say they don’t smoke. What is the probability that when someone says that he does not smoke, he is telling the truth? Using the total law of probability, $$P(A \hspace{.1cm} \text{and} \hspace{.1cm} B^c) = 0.25$$ $$P(A \hspace{.1cm} \text{and} \hspace{.1cm} B) = 0.85 + (0.3*0.25) = 0.925$$ $$P(A^c \hspace{.1cm} \text{and} \hspace{.1cm} B) = 0.85$$ from here, I'm stuck because my thought was to take the total number of people claiming they don't smoke over the number of people who don't smoke but that gives a probability greater than $1$ which is incorrect. Any tips?",,"['probability', 'statistics']"
63,"Find $P(Y=5)$ for $Y=X_1+X_2+X_3$, where $X_i$ are mutually indpt Poisson R.V","Find  for , where  are mutually indpt Poisson R.V",P(Y=5) Y=X_1+X_2+X_3 X_i,"In this problem, I am told that $X_1,X_2,X_3$ are mutually independent Poisson random variables with means $2,1,4$ respectively. I am also told to find the moment generating function for $Y=X_1+X_2+X_3$, which I found to be $M(t)=e^{7(e^t-1)}$. My question is how do I find $P(Y=5)$?. Is there a connection between $P(Y=5)$ and $M(t)$? Attempt at a solution: Since $X_1,X_2,X_3$ are mutually indpt. Poisson R.V. with mean $2,1,4$, we know that $$f(X_1)=\frac{e^{-2}2^x}{x!},$$ $$f(X_2)=\frac{e^{-1}1^x}{x!}$$ and $$f(X_3)=\frac{e^{-4}4^x}{x!}$$, which imply that $f(Y)=f(X_1)f(X_2)f(X_3)$ (is this true?I think it is bc of independence) Hence, after computations $$f(Y)=\frac{e^{-7}2^{3x}}{x!x!x!}.$$ To find $P(Y=5)$, do I just plug in $x=5$ into $f(Y)$? or do I take the summation of $f(Y)$ for $x=0$ all the way up to $x=5$ or neither? It's been a while since I have seen this material. Any help will be appreciated. Thanks!","In this problem, I am told that $X_1,X_2,X_3$ are mutually independent Poisson random variables with means $2,1,4$ respectively. I am also told to find the moment generating function for $Y=X_1+X_2+X_3$, which I found to be $M(t)=e^{7(e^t-1)}$. My question is how do I find $P(Y=5)$?. Is there a connection between $P(Y=5)$ and $M(t)$? Attempt at a solution: Since $X_1,X_2,X_3$ are mutually indpt. Poisson R.V. with mean $2,1,4$, we know that $$f(X_1)=\frac{e^{-2}2^x}{x!},$$ $$f(X_2)=\frac{e^{-1}1^x}{x!}$$ and $$f(X_3)=\frac{e^{-4}4^x}{x!}$$, which imply that $f(Y)=f(X_1)f(X_2)f(X_3)$ (is this true?I think it is bc of independence) Hence, after computations $$f(Y)=\frac{e^{-7}2^{3x}}{x!x!x!}.$$ To find $P(Y=5)$, do I just plug in $x=5$ into $f(Y)$? or do I take the summation of $f(Y)$ for $x=0$ all the way up to $x=5$ or neither? It's been a while since I have seen this material. Any help will be appreciated. Thanks!",,['statistics']
64,Distinguishing between unimodal and bimodal normal data,Distinguishing between unimodal and bimodal normal data,,"I have a large number of data sets that have either a unimodal normal distribution or a bimodal normal distribution. I'm not a statistician by any means, so I'm quite limited in my experience. For the bimodal data sets, I have implemented (through a library) the Expectation-Maximization method for identifying the distributions of the two constituents and that works great. The only problem is, when the algorithm is fed a unimodel distribution, it doesn't really converge to just one distribution (or two very close ones). The number I'm mostly interested in, is the delta between the two means, and so in the case of a unimodal distribution, the delta-mean is overestimated. So my question is: Is there a good test for identifying bimodal distributions? Sometimes the means are quite close to one another, in the sense that there is no ""dip"" between the two means. Example images: Bimodal: it works great in this case, identifying the two peaks Unimodal: it identifies two peaks that aren't really there, I would wish the two means were (much) closer Close Bimodal: it identifies this one just fine, I would not want this to be considered unimodal","I have a large number of data sets that have either a unimodal normal distribution or a bimodal normal distribution. I'm not a statistician by any means, so I'm quite limited in my experience. For the bimodal data sets, I have implemented (through a library) the Expectation-Maximization method for identifying the distributions of the two constituents and that works great. The only problem is, when the algorithm is fed a unimodel distribution, it doesn't really converge to just one distribution (or two very close ones). The number I'm mostly interested in, is the delta between the two means, and so in the case of a unimodal distribution, the delta-mean is overestimated. So my question is: Is there a good test for identifying bimodal distributions? Sometimes the means are quite close to one another, in the sense that there is no ""dip"" between the two means. Example images: Bimodal: it works great in this case, identifying the two peaks Unimodal: it identifies two peaks that aren't really there, I would wish the two means were (much) closer Close Bimodal: it identifies this one just fine, I would not want this to be considered unimodal",,"['statistics', 'algorithms', 'normal-distribution']"
65,Least Squares with Singular $AA^T$,Least Squares with Singular,AA^T,"Given the following system, find all least squares solutions: $\begin{bmatrix}1 & 2 & 3\\2 & 3 & 4\\3 & 4 & 5\end{bmatrix}  \vec{x} = \begin{bmatrix}1\\1\\2\end{bmatrix}$ However, after trying to minimize residuals with: $\vec{x} = (A^TA)^{-1}A^T\begin{bmatrix}1\\1\\2\end{bmatrix}$ I found that $det(AA^T)$ is singular... I think this means that their exist infinitely many least squares solutions to the system, but I don't know how to go about describing them all. I am relatively new to linear algebra (Uni level into class at the moment) so any help/explanation would be great!","Given the following system, find all least squares solutions: $\begin{bmatrix}1 & 2 & 3\\2 & 3 & 4\\3 & 4 & 5\end{bmatrix}  \vec{x} = \begin{bmatrix}1\\1\\2\end{bmatrix}$ However, after trying to minimize residuals with: $\vec{x} = (A^TA)^{-1}A^T\begin{bmatrix}1\\1\\2\end{bmatrix}$ I found that $det(AA^T)$ is singular... I think this means that their exist infinitely many least squares solutions to the system, but I don't know how to go about describing them all. I am relatively new to linear algebra (Uni level into class at the moment) so any help/explanation would be great!",,"['linear-algebra', 'statistics', 'least-squares']"
66,Probability Riddle,Probability Riddle,,"I was told a puzzle recently, and I can't figure out how to solve it. It went like this: You are a prisoner. You play a game with the guard many times a day.   This game has a unique probability $p$ for you to win, and it is the   same every time you play. Each time you win, you ""gain a life."" Each   time you lose, you ""lose a life."" You begin with 1 life. What does   $p$ need to equal for you to stay alive for a long time (many years   with you playing multiple times a day)? I know the probability will have to be greater than 0.5, but how much greater than that does it need to be for you to be sure you'll live a long life?","I was told a puzzle recently, and I can't figure out how to solve it. It went like this: You are a prisoner. You play a game with the guard many times a day.   This game has a unique probability $p$ for you to win, and it is the   same every time you play. Each time you win, you ""gain a life."" Each   time you lose, you ""lose a life."" You begin with 1 life. What does   $p$ need to equal for you to stay alive for a long time (many years   with you playing multiple times a day)? I know the probability will have to be greater than 0.5, but how much greater than that does it need to be for you to be sure you'll live a long life?",,"['probability', 'statistics', 'problem-solving', 'puzzle']"
67,"Suppose that X ∼ Exp(1). For α > 0, β > 0, and −∞ < ν < ∞, find the pdf of $Y = αX^ {1/ β} + ν$.","Suppose that X ∼ Exp(1). For α > 0, β > 0, and −∞ < ν < ∞, find the pdf of .",Y = αX^ {1/ β} + ν,"Suppose that X ∼ Exp(1). For α > 0, β > 0, and −∞ < ν < ∞, find a) the pdf of $Y = αX^ {1/β} + ν$. b) E[Y ] Working on this problem right now but it seems complicated so I'm anticipating needing help. Will edit work into this part as I go along. a) $F_Y(y) = P(Y < y) = P(αx^ {1/ β} + ν < y) = P(x < ({y- ν}/α)^β) = F_X(((y- ν)/α)^β) =  \int_0^{((y- ν)/α)^β}e^{-x}dx $ -> integrate & differentiate -> weibull distribution pdf for y > v b) Thanks for the help below!  $E[Y] = E[αX^{1/β} + ν] = αE[X^{1/β}] + ν = α\int_0^\infty x^{1/β}e^{-x}dx + ν = α\int_0^\infty x^{(1/β + 1) - 1}e^{-x}dx + ν = α\Gamma((1/ \beta) + 1) + ν = (\alpha / \beta) \Gamma(1/\beta) + ν$","Suppose that X ∼ Exp(1). For α > 0, β > 0, and −∞ < ν < ∞, find a) the pdf of $Y = αX^ {1/β} + ν$. b) E[Y ] Working on this problem right now but it seems complicated so I'm anticipating needing help. Will edit work into this part as I go along. a) $F_Y(y) = P(Y < y) = P(αx^ {1/ β} + ν < y) = P(x < ({y- ν}/α)^β) = F_X(((y- ν)/α)^β) =  \int_0^{((y- ν)/α)^β}e^{-x}dx $ -> integrate & differentiate -> weibull distribution pdf for y > v b) Thanks for the help below!  $E[Y] = E[αX^{1/β} + ν] = αE[X^{1/β}] + ν = α\int_0^\infty x^{1/β}e^{-x}dx + ν = α\int_0^\infty x^{(1/β + 1) - 1}e^{-x}dx + ν = α\Gamma((1/ \beta) + 1) + ν = (\alpha / \beta) \Gamma(1/\beta) + ν$",,"['probability', 'statistics', 'probability-distributions']"
68,"If $Y_1, \ldots, Y_n \sim Poisson(\lambda)$ are iid, how to show that $E(Y_1 | T = \sum_{i=1}^{n}Y_i) = \frac{T}{n}$?","If  are iid, how to show that ?","Y_1, \ldots, Y_n \sim Poisson(\lambda) E(Y_1 | T = \sum_{i=1}^{n}Y_i) = \frac{T}{n}","Suppose I have that $Y_1, \ldots, Y_n \sim Poisson(\lambda)$ are iid. I saw a line in a book that said that if $T = \sum_{i=1}^{n}Y_i$, then: $$ E(Y_1 | T) = \frac{T}{n} $$ I am lost as to how they obtain this. One approach I did was: $$ E(Y_1 | T) = E\left(Y_1 | \sum_{i=1}^{n}Y_i = t\right)  = E\left(Y_1 | Y_1 = t-\sum_{i=2}^{n}Y_i \right) = E\left(T-\sum_{i=2}^{n}Y_i\right) = T- (n-1)\lambda $$ However, I know this is wrong but dont know why.","Suppose I have that $Y_1, \ldots, Y_n \sim Poisson(\lambda)$ are iid. I saw a line in a book that said that if $T = \sum_{i=1}^{n}Y_i$, then: $$ E(Y_1 | T) = \frac{T}{n} $$ I am lost as to how they obtain this. One approach I did was: $$ E(Y_1 | T) = E\left(Y_1 | \sum_{i=1}^{n}Y_i = t\right)  = E\left(Y_1 | Y_1 = t-\sum_{i=2}^{n}Y_i \right) = E\left(T-\sum_{i=2}^{n}Y_i\right) = T- (n-1)\lambda $$ However, I know this is wrong but dont know why.",,"['probability', 'statistics', 'statistical-inference', 'conditional-expectation', 'poisson-distribution']"
69,Clarification on random variables?,Clarification on random variables?,,"If $X$ and $Y$ are dependent random variables, then it is possible that $Var(X+Y) > Var(X) + Var(Y)$. I only know that the two are equal for independent random variables; for dependent variables, would this be the case? According to forecasts, the end-of-year value in dollars of IBM stock has variance 10. If an investor holds a portfolio containing 5 shares of IBM stock and 240 dollars of idle cash, what is the variance in the end-of-year value in dollars of his portfolio? I would assume it's 50, as variance is additive? If X and Y are random variables such that $P(X=0)=0.5$ and $P(Y=0)=0.1$, then is $P((X+Y)/2=0)$ equal to $P(X=0)/2 + P(Y=0)/2 = 0.3$? I don't believe that it's possible to add probabilities like this, would I multiply instead?","If $X$ and $Y$ are dependent random variables, then it is possible that $Var(X+Y) > Var(X) + Var(Y)$. I only know that the two are equal for independent random variables; for dependent variables, would this be the case? According to forecasts, the end-of-year value in dollars of IBM stock has variance 10. If an investor holds a portfolio containing 5 shares of IBM stock and 240 dollars of idle cash, what is the variance in the end-of-year value in dollars of his portfolio? I would assume it's 50, as variance is additive? If X and Y are random variables such that $P(X=0)=0.5$ and $P(Y=0)=0.1$, then is $P((X+Y)/2=0)$ equal to $P(X=0)/2 + P(Y=0)/2 = 0.3$? I don't believe that it's possible to add probabilities like this, would I multiply instead?",,"['statistics', 'random-variables']"
70,Bayesian Statistics: Finding Sufficient Statistic for Uniform Distribution,Bayesian Statistics: Finding Sufficient Statistic for Uniform Distribution,,"The example: let $y_1,\dots,y_n \overset{\text{i.i.d.}}\sim U([0,\theta])$, where $\theta >0$ is unknown. Find a sufficient statistic for $\theta$. Solution attempt: $$g(y_1,\dots,y_n) = c\quad \text{(constant)}$$ $$P(y_i\mid\theta) = \frac{1}{\theta}\quad \text{ for } 0<y_i<\theta$$ $$P(y_1,\dots,y_n\mid\theta) = \prod_{i=1}^n P(y_i\mid\theta) = \frac{1}{\theta^n}\quad\text{ for } 0<y_1,\dots,y_n<\theta$$ Now this is where I got stuck. I have seen this post about Sufficient Statistic but I am still stuck. Could somebody help me find a sufficient statistic for this problem? (I think maybe taking the average or the maximum value of $y_i$s might work but not sure how to do the next step)","The example: let $y_1,\dots,y_n \overset{\text{i.i.d.}}\sim U([0,\theta])$, where $\theta >0$ is unknown. Find a sufficient statistic for $\theta$. Solution attempt: $$g(y_1,\dots,y_n) = c\quad \text{(constant)}$$ $$P(y_i\mid\theta) = \frac{1}{\theta}\quad \text{ for } 0<y_i<\theta$$ $$P(y_1,\dots,y_n\mid\theta) = \prod_{i=1}^n P(y_i\mid\theta) = \frac{1}{\theta^n}\quad\text{ for } 0<y_1,\dots,y_n<\theta$$ Now this is where I got stuck. I have seen this post about Sufficient Statistic but I am still stuck. Could somebody help me find a sufficient statistic for this problem? (I think maybe taking the average or the maximum value of $y_i$s might work but not sure how to do the next step)",,"['statistics', 'bayesian']"
71,Variance of max - min of 2 exponential random variables,Variance of max - min of 2 exponential random variables,,"Suppose we have 2 random variables, $S\sim Exp(\lambda)$ and $T\sim Exp(\mu)$. Let $U=\min(S,T)$ and $V=\max(S,T)$. What is the variance of $W=V-U$? I calculated: $Var(U) = \frac{1}{(\lambda+\mu)^2}$ $Var(V) = \frac{1}{\lambda^2} + \frac{1}{\mu^2} + \frac{1}{(\lambda+\mu)^2}$ But I don't know how to proceed further.","Suppose we have 2 random variables, $S\sim Exp(\lambda)$ and $T\sim Exp(\mu)$. Let $U=\min(S,T)$ and $V=\max(S,T)$. What is the variance of $W=V-U$? I calculated: $Var(U) = \frac{1}{(\lambda+\mu)^2}$ $Var(V) = \frac{1}{\lambda^2} + \frac{1}{\mu^2} + \frac{1}{(\lambda+\mu)^2}$ But I don't know how to proceed further.",,"['probability', 'statistics', 'random-variables', 'covariance']"
72,linear regression for dummies,linear regression for dummies,,"I am trying to understand linear regression. I have a limited knowledge in math (Algebra I) but I still want to be able to learn and understand what this is. I don't need to know all the math surrounding linear regression but a basic working understanding would be great. Can someone give me an simple formula, an example, and an explanation for what all the symbols and variables are for basic linear regression? Thanks","I am trying to understand linear regression. I have a limited knowledge in math (Algebra I) but I still want to be able to learn and understand what this is. I don't need to know all the math surrounding linear regression but a basic working understanding would be great. Can someone give me an simple formula, an example, and an explanation for what all the symbols and variables are for basic linear regression? Thanks",,"['linear-algebra', 'statistics', 'linear-regression']"
73,expectation of a nonlinear function of a Gaussian random variable [closed],expectation of a nonlinear function of a Gaussian random variable [closed],,"Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 8 years ago . Improve this question I am reading a book on Monte Carlo Simulation and I want to know where the formula below come from. $$E\left[ X^4\right] {\text{ }} = 3\sigma _x^4 + 6\sigma _x^2\mu _x^2 + \mu _x^4$$ Suppose $X$ is a Gaussian random variable: $X \sim N\left({\mu_x},\sigma _x^2\right)$.  Thanks.","Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 8 years ago . Improve this question I am reading a book on Monte Carlo Simulation and I want to know where the formula below come from. $$E\left[ X^4\right] {\text{ }} = 3\sigma _x^4 + 6\sigma _x^2\mu _x^2 + \mu _x^4$$ Suppose $X$ is a Gaussian random variable: $X \sim N\left({\mu_x},\sigma _x^2\right)$.  Thanks.",,"['statistics', 'random-variables', 'expectation']"
74,Two-sided confidence intervals and tests,Two-sided confidence intervals and tests,,"From a sample of 1751 army hospitals, estimate the mean expenses for a full time equivalent employee for all US army hospitals using a 90% confidence interval given x = 6563 and s = 2484 . Work: 1.645(2484/41.845) 1.645(59.362) 97.65 6563+- 97.65 6465.35---6660.65   ANswer I think I did that one right but I'm a bit confused on the wording of part b... In 2007 mean expenses for the entire public was believed to be 7000, conduct a test at a = .10 to determine if the mean expenses for an army hospital employee are equal to that of the public. Any ideas on this one?","From a sample of 1751 army hospitals, estimate the mean expenses for a full time equivalent employee for all US army hospitals using a 90% confidence interval given x = 6563 and s = 2484 . Work: 1.645(2484/41.845) 1.645(59.362) 97.65 6563+- 97.65 6465.35---6660.65   ANswer I think I did that one right but I'm a bit confused on the wording of part b... In 2007 mean expenses for the entire public was believed to be 7000, conduct a test at a = .10 to determine if the mean expenses for an army hospital employee are equal to that of the public. Any ideas on this one?",,"['statistics', 'statistical-inference', 'sampling', 'means', 'confidence-interval']"
75,Calculate$\operatorname{Var}(Y |X = 2)$ for a given pdf.,Calculate for a given pdf.,\operatorname{Var}(Y |X = 2),"A machine has two components and fails when both components fail. The   number of years from now until the first component fails, X, and the   number of years from now until the machine fails, Y , are random   variables with joint density function   $$f(x,y)=\begin{cases}\frac1{18}e^{-(x+y)/6}&\text{if }0<x<y\\ 0&\text{otherwise}\end{cases}$$   Calculate $\operatorname{Var}(Y |X = 2)$. My answer: I can notice that the ftp is a product of two exponentials rv, with $\lambda= \frac16$. And the variance of an exponential is $\frac{1}{\lambda^2}$. Then the variance of this pdf is $1/(1/6)^2=36$. Is that right? The book said it is $36$, but I'm not sure if my argument is correct. My question: How to calculate $\operatorname{Var}(Y |X = 2)$?","A machine has two components and fails when both components fail. The   number of years from now until the first component fails, X, and the   number of years from now until the machine fails, Y , are random   variables with joint density function   $$f(x,y)=\begin{cases}\frac1{18}e^{-(x+y)/6}&\text{if }0<x<y\\ 0&\text{otherwise}\end{cases}$$   Calculate $\operatorname{Var}(Y |X = 2)$. My answer: I can notice that the ftp is a product of two exponentials rv, with $\lambda= \frac16$. And the variance of an exponential is $\frac{1}{\lambda^2}$. Then the variance of this pdf is $1/(1/6)^2=36$. Is that right? The book said it is $36$, but I'm not sure if my argument is correct. My question: How to calculate $\operatorname{Var}(Y |X = 2)$?",,"['probability', 'statistics']"
76,"MGF, Calculate the standard deviation of $X + Y$ . Method.","MGF, Calculate the standard deviation of  . Method.",X + Y,"Two claimants place calls simultaneously to an insurers claims call   center. The times X and Y , in minutes, that elapse before the   respective claimants get to speak with call center representatives are   independently and identically distributed. The moment generating   function for both $X$ and $Y$ is   $$ M(t) = \left( \frac{1}{1-1.5t}\right)^2, \quad t< \frac{2}{3}.$$   Calculate the standard deviation of $X + Y$ . I would like to know what approaches I need to use to solve this exercise. I am practicing for an actuarial exam and I am looking for the method that will minimize the amount of time spent on these types of problems. Is it correct this answer, (I used the coment below my post): X and Y are Erlang(2, 2/3) and independent, then var(x+y)=var(x)+var(y)+2 cov(x,y) because are independent, Var(x+y)= 2 var(x)= 2 n * (alpha)^2  =2 (2) (2/3)^2 Then, satandar deviation= 2 * (alpha) = 4/3","Two claimants place calls simultaneously to an insurers claims call   center. The times X and Y , in minutes, that elapse before the   respective claimants get to speak with call center representatives are   independently and identically distributed. The moment generating   function for both $X$ and $Y$ is   $$ M(t) = \left( \frac{1}{1-1.5t}\right)^2, \quad t< \frac{2}{3}.$$   Calculate the standard deviation of $X + Y$ . I would like to know what approaches I need to use to solve this exercise. I am practicing for an actuarial exam and I am looking for the method that will minimize the amount of time spent on these types of problems. Is it correct this answer, (I used the coment below my post): X and Y are Erlang(2, 2/3) and independent, then var(x+y)=var(x)+var(y)+2 cov(x,y) because are independent, Var(x+y)= 2 var(x)= 2 n * (alpha)^2  =2 (2) (2/3)^2 Then, satandar deviation= 2 * (alpha) = 4/3",,"['probability', 'statistics', 'actuarial-science']"
77,Joint moment generating function problem,Joint moment generating function problem,,"X and Y have the following joint moment generating function: $M_{X,Y}(a,b) = \Large \frac{4}{5}[\frac{1}{(1-a)(1-b)}+\frac{1}{(2-a)(2-b)}]$ Find E(XY) I have gone through this problem several times, first calculating the derivative with respect to a and then subsequently calculating the derivative with respect to b. I think I can spot an exponential distribution, but I'm not sure how to make the computation easier. Edit: I have a final answer of .85. Not sure if this is correct or not.","X and Y have the following joint moment generating function: $M_{X,Y}(a,b) = \Large \frac{4}{5}[\frac{1}{(1-a)(1-b)}+\frac{1}{(2-a)(2-b)}]$ Find E(XY) I have gone through this problem several times, first calculating the derivative with respect to a and then subsequently calculating the derivative with respect to b. I think I can spot an exponential distribution, but I'm not sure how to make the computation easier. Edit: I have a final answer of .85. Not sure if this is correct or not.",,"['probability', 'statistics', 'exponential-function', 'moment-generating-functions']"
78,Using distribution properties to find the exact value of the following sum,Using distribution properties to find the exact value of the following sum,,"$\sum_{x=1}^\infty \frac{x^2}{2^x}$ I believe I should be using geometric properties. So I'm trying to get it to resemble $\sum_{x=1}^\infty xp(1-p)^{x-1}$ But I can't seem to manipulate it how I'd like. Am I incorrect in assuming it's closest to a geometric distribution? I've also tried resembling a binomial, which would get me $2\sum_{x=1}^\infty x^22^{1-x}$ But I don't know where to obtain a $p$ from. Also, doesn't $(1-p)$ need to be $\leq 1$?","$\sum_{x=1}^\infty \frac{x^2}{2^x}$ I believe I should be using geometric properties. So I'm trying to get it to resemble $\sum_{x=1}^\infty xp(1-p)^{x-1}$ But I can't seem to manipulate it how I'd like. Am I incorrect in assuming it's closest to a geometric distribution? I've also tried resembling a binomial, which would get me $2\sum_{x=1}^\infty x^22^{1-x}$ But I don't know where to obtain a $p$ from. Also, doesn't $(1-p)$ need to be $\leq 1$?",,['statistics']
79,Proving induction on a binomial distribution,Proving induction on a binomial distribution,,"I have to show that $p(k+1) = \frac{n-k}{k+1}p(k)$ So far, I have done some manipulation. $p(k+1) = {n \choose k+1}p^{k+1}(1-p)^{n-(k+1)}$ $p(k+1) = \frac{n!}{(k+1)!(n-(k+1))!}p^{k+1}(1-p)^{n-(k+1)}$ $p(k+1) = \frac{n!}{(k+1)!(n-k-1))!}pp^k(1-p)^{n-k-1}$ $p(k+1) = \frac{n!}{(k+1)(k)!\frac{(n-k)!}{n-k}}pp^k\frac{(1-p)^{n-k}}{1-p}$ $p(k+1) = \frac{n-k}{k+1} \cdot \frac{n!}{(k)!(n-k)!}pp^k\frac{(1-p)^{n-k}}{1-p}$ It's close, but I seem to have an extra $p$ at the top, and an extra $1-p$ at the bottom. What am I missing?","I have to show that $p(k+1) = \frac{n-k}{k+1}p(k)$ So far, I have done some manipulation. $p(k+1) = {n \choose k+1}p^{k+1}(1-p)^{n-(k+1)}$ $p(k+1) = \frac{n!}{(k+1)!(n-(k+1))!}p^{k+1}(1-p)^{n-(k+1)}$ $p(k+1) = \frac{n!}{(k+1)!(n-k-1))!}pp^k(1-p)^{n-k-1}$ $p(k+1) = \frac{n!}{(k+1)(k)!\frac{(n-k)!}{n-k}}pp^k\frac{(1-p)^{n-k}}{1-p}$ $p(k+1) = \frac{n-k}{k+1} \cdot \frac{n!}{(k)!(n-k)!}pp^k\frac{(1-p)^{n-k}}{1-p}$ It's close, but I seem to have an extra $p$ at the top, and an extra $1-p$ at the bottom. What am I missing?",,['statistics']
80,Proving $\Gamma (\frac{1}{2}) = \sqrt{\pi}$,Proving,\Gamma (\frac{1}{2}) = \sqrt{\pi},"There are already proofs out there, however, the problem I am working on requires doing the proof by using the gamma function with $y = \sqrt{2x}$ to show that $\Gamma(\frac{1}{2}) = \sqrt{\pi}$ . Here is what I have so far: $\Gamma(\frac{1}{2}) = \frac{1}{\sqrt{\beta}}\int_0^\infty x^{-\frac{1}{2}}e^{-\frac{x}{\beta}} \, dx.$ Substituting $x = \frac{y^2}{2}\, dx = y\,dy$ , we get $\Gamma(\frac{1}{2}) = \frac{\sqrt{2}}{\sqrt{\beta}}\int_0^\infty e^{-\frac{y^2}{2\beta}} \, dy$ . At this point, the book pulled some trick that I don't understand. Can anyone explain to me what the book did above? Thanks.","There are already proofs out there, however, the problem I am working on requires doing the proof by using the gamma function with to show that . Here is what I have so far: Substituting , we get . At this point, the book pulled some trick that I don't understand. Can anyone explain to me what the book did above? Thanks.","y = \sqrt{2x} \Gamma(\frac{1}{2}) = \sqrt{\pi} \Gamma(\frac{1}{2}) = \frac{1}{\sqrt{\beta}}\int_0^\infty x^{-\frac{1}{2}}e^{-\frac{x}{\beta}} \, dx. x = \frac{y^2}{2}\, dx = y\,dy \Gamma(\frac{1}{2}) = \frac{\sqrt{2}}{\sqrt{\beta}}\int_0^\infty e^{-\frac{y^2}{2\beta}} \, dy","['statistics', 'special-functions', 'gamma-function']"
81,"If the probability of a single point of a continuous distribution is zero, why can I obtain density values from the normal?","If the probability of a single point of a continuous distribution is zero, why can I obtain density values from the normal?",,"I was hoping to clarify a possibly wrong notion I have. I understand that for a continuous distribution, say the normal, the probability of a single point, say 0, has zero probability. However, if I plug into the standard normal density, I get 0.3989423 in R. Does anyone know what part I am missing here? Thanks.","I was hoping to clarify a possibly wrong notion I have. I understand that for a continuous distribution, say the normal, the probability of a single point, say 0, has zero probability. However, if I plug into the standard normal density, I get 0.3989423 in R. Does anyone know what part I am missing here? Thanks.",,"['probability', 'statistics']"
82,marginal density without a joint density given,marginal density without a joint density given,,"Let X and Y have a joint uniform distribution on the region described by 0≤y≤1-$x^2$; -1≤x≤1. Find E[X] and E[Y] What I've tried The graph will look like this. I know i need to find the marginal densities, and here's what i've tried $f_X(x)$=$\int_{0}^{1-x^2}f(x,y)dy$ E[X]=$\int_{-1}^{1}$$xf_X(x)dx$ $f_Y(y)$=$\int_{-1}^{1}f(x,y)dx$ E[Y]=$\int_{0}^{1-x^2}$$yf_Y(y)dy$ however when I plug in $1-x^2$ as the density the answers for the expected values come out wrong. I'm using this because I assume the density to be used should be $y=1-x^2; -1≤x≤1$","Let X and Y have a joint uniform distribution on the region described by 0≤y≤1-$x^2$; -1≤x≤1. Find E[X] and E[Y] What I've tried The graph will look like this. I know i need to find the marginal densities, and here's what i've tried $f_X(x)$=$\int_{0}^{1-x^2}f(x,y)dy$ E[X]=$\int_{-1}^{1}$$xf_X(x)dx$ $f_Y(y)$=$\int_{-1}^{1}f(x,y)dx$ E[Y]=$\int_{0}^{1-x^2}$$yf_Y(y)dy$ however when I plug in $1-x^2$ as the density the answers for the expected values come out wrong. I'm using this because I assume the density to be used should be $y=1-x^2; -1≤x≤1$",,"['probability', 'statistics']"
83,Find a sufficient statistic for this parameter of a Pareto distibution,Find a sufficient statistic for this parameter of a Pareto distibution,,"I've got a problem with the following exercise: Let be the following function a distribution: $$f_{\theta,a}(x)=\begin{cases} \theta a^\theta x^{-(\theta+1)} &, x\geq a\\ 0 &, x<a \end{cases}$$ while $a,\theta>0$. Find a sufficient statistic for $a$, while $\theta$ is known. Find a sufficient statistic for $\theta$, while $a$ is known. Could give me a solution for the first sufficient statistic, please?","I've got a problem with the following exercise: Let be the following function a distribution: $$f_{\theta,a}(x)=\begin{cases} \theta a^\theta x^{-(\theta+1)} &, x\geq a\\ 0 &, x<a \end{cases}$$ while $a,\theta>0$. Find a sufficient statistic for $a$, while $\theta$ is known. Find a sufficient statistic for $\theta$, while $a$ is known. Could give me a solution for the first sufficient statistic, please?",,"['statistics', 'probability-distributions']"
84,Corollary of the Frisch-Waugh Theorem,Corollary of the Frisch-Waugh Theorem,,"Consider the following linear regression model: $$y=X \beta + \epsilon = X_1 \beta_1 + X_2 \beta_2 + \epsilon $$ Where we have $n$ obsevations and $k$ variables, and hence $X$ is a matrix $nk$, and $X_1$ and $X_2$ are $nk_1$ and $nk_2$ respectively. Where $k_1+k_2=k$. I am studying the Frisch-Waugh theorem, wich states that the fitted values $\hat{\beta_1}$, and the residuals, obtained by performing the OLS regression are the same for  $$y= X_1 \beta_1 + X_2 \beta_2 + error $$ and, $$M_{X_2}y=M_{X_2} X_1 \beta_1 + error $$ Where, $M_x$ denotes the following symmetric matrix obtained by the orthogonal projector $P_X=X(X'X)^{-1}X'$; $$M_X=I_n-P_X$$ I would like to prove that the fitted value $\hat{\beta_1}$  is also the same when running the regression $$y=M_{X_2} X_1 \beta_1 + error $$ But so far haven't been able to prove it succesfully. What would you suggest?","Consider the following linear regression model: $$y=X \beta + \epsilon = X_1 \beta_1 + X_2 \beta_2 + \epsilon $$ Where we have $n$ obsevations and $k$ variables, and hence $X$ is a matrix $nk$, and $X_1$ and $X_2$ are $nk_1$ and $nk_2$ respectively. Where $k_1+k_2=k$. I am studying the Frisch-Waugh theorem, wich states that the fitted values $\hat{\beta_1}$, and the residuals, obtained by performing the OLS regression are the same for  $$y= X_1 \beta_1 + X_2 \beta_2 + error $$ and, $$M_{X_2}y=M_{X_2} X_1 \beta_1 + error $$ Where, $M_x$ denotes the following symmetric matrix obtained by the orthogonal projector $P_X=X(X'X)^{-1}X'$; $$M_X=I_n-P_X$$ I would like to prove that the fitted value $\hat{\beta_1}$  is also the same when running the regression $$y=M_{X_2} X_1 \beta_1 + error $$ But so far haven't been able to prove it succesfully. What would you suggest?",,"['probability', 'statistics', 'regression', 'economics', 'regression-analysis']"
85,Difference between these two standard deviation formulas?,Difference between these two standard deviation formulas?,,Today I started reading statistics and I came across these two S.D formulas: $$\sqrt{\frac 1 n \sum (X - \overline X)^2} \textrm{ and } \sqrt{\frac 1 {n-1}\sum (X - \overline X)^2}$$ What is the difference between these two and in which situations I should use them?,Today I started reading statistics and I came across these two S.D formulas: $$\sqrt{\frac 1 n \sum (X - \overline X)^2} \textrm{ and } \sqrt{\frac 1 {n-1}\sum (X - \overline X)^2}$$ What is the difference between these two and in which situations I should use them?,,"['statistics', 'average', 'standard-deviation', 'means']"
86,Is $P(n) = \frac{a n }{b}$ or $\frac{(a+1) n}{b + 1}$?,Is  or ?,P(n) = \frac{a n }{b} \frac{(a+1) n}{b + 1},"I investigated Some random data and I was a bit confused. Could be Mathematical coincidence but i'm not sure. Consider the integers $1,2,3,...,a$ Randomly Pick $b$ dinstinct element out of them. The question was ; what is the expected value of the smallest integer we picked ? And the second smallest , third etc Let $P(n)$ denote the expected $n^{th}$ smallest. I started thinking about $P(1)$ and i assumed the following ( and it seemed to agree with my formulas/computations ) $1)$ If $P(n)\gt 0$ Then $P(n) = n P(1)$. That seemed logical ? I know that if $b = 1$ then $P(1)=\frac {a+1}2$. So i assumed $P(n)=\frac {n (a+1)}{(b+1)}$. But a friend told me it is just $P(n)= \frac {n a}b$ for $b\gt 2$ And he defended the idea with The average gap is $\frac ab$. Or at least $P(1)=\frac ab$ and $P(n)$ is an arithmetic progression. I Said to myself : this can not be true. So I took the data and took the average of the first two smallest $\frac { ( P(1) + P(2) )} 2 $ And to my surprise it supported the idea of my friend ?!? Am I wrong ? Or is it coincidence ? Or is there slow convergence to the expected values ?? How fast is the convergence to the expected values in terms of $a,b,n$ anyway ? I started wondering if an analogue to probability and saying The probability of $x$ is twice that of $y$. Exist such that We could say We expect $x$ twice as much as $y$. And if such concepts could help in the Above problem. In other words instead of a ( given or not ) probability distribution , we would have a ( given or not ) "" expecting distribution "". I know the gaussian curve as a probability distribution but perhaps it is An intresting "" expecting distribution "" ... Maybe it already used as such in stat and quantum physics ... If this already exists it probably has a name. Let $x$ be a positive integer in the interval $[1,a]$. I wonder - assuming this exists - what the "" expecting distribution "" $D(x)$ is in the Above problem in terms of $a,b,n$ ?","I investigated Some random data and I was a bit confused. Could be Mathematical coincidence but i'm not sure. Consider the integers $1,2,3,...,a$ Randomly Pick $b$ dinstinct element out of them. The question was ; what is the expected value of the smallest integer we picked ? And the second smallest , third etc Let $P(n)$ denote the expected $n^{th}$ smallest. I started thinking about $P(1)$ and i assumed the following ( and it seemed to agree with my formulas/computations ) $1)$ If $P(n)\gt 0$ Then $P(n) = n P(1)$. That seemed logical ? I know that if $b = 1$ then $P(1)=\frac {a+1}2$. So i assumed $P(n)=\frac {n (a+1)}{(b+1)}$. But a friend told me it is just $P(n)= \frac {n a}b$ for $b\gt 2$ And he defended the idea with The average gap is $\frac ab$. Or at least $P(1)=\frac ab$ and $P(n)$ is an arithmetic progression. I Said to myself : this can not be true. So I took the data and took the average of the first two smallest $\frac { ( P(1) + P(2) )} 2 $ And to my surprise it supported the idea of my friend ?!? Am I wrong ? Or is it coincidence ? Or is there slow convergence to the expected values ?? How fast is the convergence to the expected values in terms of $a,b,n$ anyway ? I started wondering if an analogue to probability and saying The probability of $x$ is twice that of $y$. Exist such that We could say We expect $x$ twice as much as $y$. And if such concepts could help in the Above problem. In other words instead of a ( given or not ) probability distribution , we would have a ( given or not ) "" expecting distribution "". I know the gaussian curve as a probability distribution but perhaps it is An intresting "" expecting distribution "" ... Maybe it already used as such in stat and quantum physics ... If this already exists it probably has a name. Let $x$ be a positive integer in the interval $[1,a]$. I wonder - assuming this exists - what the "" expecting distribution "" $D(x)$ is in the Above problem in terms of $a,b,n$ ?",,"['combinatorics', 'statistics', 'probability-distributions', 'expectation', 'order-statistics']"
87,"Expected value of die rolls - roll $n$, keep $1$","Expected value of die rolls - roll , keep",n 1,"I know how to calculate expected value for a single roll, and I read several other answers about expected value with rerolls, but how does the calculation change if you can make your reroll before choosing which die to keep? For instance, what is the expected value of rolling $2$ fair $6$-sided dice and keeping the higher value?  And can you please generalize to $n$ $x$-sided dice?","I know how to calculate expected value for a single roll, and I read several other answers about expected value with rerolls, but how does the calculation change if you can make your reroll before choosing which die to keep? For instance, what is the expected value of rolling $2$ fair $6$-sided dice and keeping the higher value?  And can you please generalize to $n$ $x$-sided dice?",,"['probability', 'statistics']"
88,Probability of drawing two green balls without replacement,Probability of drawing two green balls without replacement,,"A box contains of some green balls and some white balls. Given that the probability of drawing two green balls without replacement is 0.5, what is the smallest total number of ball inside this box? Hence, state the total number of green and white balls. The smallest total number of balls inside the box should be 4, the total number of green balls is 3 and the total number of white balls is 1. I'm just using the trial and error to get these answer. Is there any other way to do it?","A box contains of some green balls and some white balls. Given that the probability of drawing two green balls without replacement is 0.5, what is the smallest total number of ball inside this box? Hence, state the total number of green and white balls. The smallest total number of balls inside the box should be 4, the total number of green balls is 3 and the total number of white balls is 1. I'm just using the trial and error to get these answer. Is there any other way to do it?",,"['probability', 'statistics']"
89,Expectation of Independent Variables Equals Zero?,Expectation of Independent Variables Equals Zero?,,"Given $n$ independent random variables, $X_1, X_2, ..., X_n$ , each having a normal distribution, why is it that the following expectation holds? $$E[(X_i - \mu)(X_j - \mu)] = 0$$ where $i \neq j$ I saw this statement in a proof explaining why we divide by $n-1$ when computing the sample variance and of course there was no explanation. An intuitive explanation and/or a link to more detailed information about why this is true would be greatly appreciated","Given $n$ independent random variables, $X_1, X_2, ..., X_n$ , each having a normal distribution, why is it that the following expectation holds? $$E[(X_i - \mu)(X_j - \mu)] = 0$$ where $i \neq j$ I saw this statement in a proof explaining why we divide by $n-1$ when computing the sample variance and of course there was no explanation. An intuitive explanation and/or a link to more detailed information about why this is true would be greatly appreciated",,"['probability', 'statistics', 'intuition', 'expectation']"
90,How to calculate probability with Z score not on table?,How to calculate probability with Z score not on table?,,"According to this Z table in my book, anything with a $z$ of over $3.16$ is probability 1, but this is not right. The textbook also has an example where $\mathbb P(Z\geq3.9) = 0.000048$; can somone explain to me how this answer was achieved?","According to this Z table in my book, anything with a $z$ of over $3.16$ is probability 1, but this is not right. The textbook also has an example where $\mathbb P(Z\geq3.9) = 0.000048$; can somone explain to me how this answer was achieved?",,"['statistics', 'probability-distributions', 'statistical-inference']"
91,Variance of least square estimator,Variance of least square estimator,,I have two random variables X and Y with $X\sim Exp(a)$ and $Y \sim Exp(\frac a2)$. I have a least square estimator $a=\frac {2x +y}{2.5}$. I want to calculate the variance of the estimator and to do this I'm trying to find the joint distribution (to calculate $E[XY]$). Is there a way of knowing if they are independent? It confuses me that they have different means.I tried to use the moment generating function on E[XY] but I'm not really sure how to go about it.,I have two random variables X and Y with $X\sim Exp(a)$ and $Y \sim Exp(\frac a2)$. I have a least square estimator $a=\frac {2x +y}{2.5}$. I want to calculate the variance of the estimator and to do this I'm trying to find the joint distribution (to calculate $E[XY]$). Is there a way of knowing if they are independent? It confuses me that they have different means.I tried to use the moment generating function on E[XY] but I'm not really sure how to go about it.,,"['statistics', 'statistical-inference']"
92,What are conditions under which convergence in quadratic mean implies convergence in almost sure sense?,What are conditions under which convergence in quadratic mean implies convergence in almost sure sense?,,"What are the conditions on the sequence on $\{X_n\}$ (apart from the degenerate random variable), under which it can be claim that $||X_n-X||_{L^2(\mathbb{R})}\rightarrow 0$ implies $X_n\rightarrow X$, almost surely? I know that there always exists subsequences along with the above implication hold ( as in the second answer of this question ). But I want to know about the convergence of the whole sequence by imposing some condition on it. Thank in advance.","What are the conditions on the sequence on $\{X_n\}$ (apart from the degenerate random variable), under which it can be claim that $||X_n-X||_{L^2(\mathbb{R})}\rightarrow 0$ implies $X_n\rightarrow X$, almost surely? I know that there always exists subsequences along with the above implication hold ( as in the second answer of this question ). But I want to know about the convergence of the whole sequence by imposing some condition on it. Thank in advance.",,"['probability', 'statistics', 'convergence-divergence', 'asymptotics']"
93,Proof arithmetic mean is bounded,Proof arithmetic mean is bounded,,"Suppose $A$ is the arithmetic mean of the set of real numbers $S$. How would I go about proving that  $$\min<A<\max$$ Where $\min$ and $\max$ are the minimal and maximal element of $S$, respectively.","Suppose $A$ is the arithmetic mean of the set of real numbers $S$. How would I go about proving that  $$\min<A<\max$$ Where $\min$ and $\max$ are the minimal and maximal element of $S$, respectively.",,"['algebra-precalculus', 'statistics']"
94,What's the probability that the first four children born are boys and the last two children born are girls?,What's the probability that the first four children born are boys and the last two children born are girls?,,"I'm having some problems with determining how to calculate a question about the gender proportion in newborns in some random family. A family consists of 6 children. The probability of a boy being born is $1/2$ which is also the probability of a girl being born. So $p = 1/2$. I'm well aware that the variables follow a binomial distribution. I want to determine the probability of the first 4 children born being boys and the last 2 being girls. My ""guesses"" would be either: $$\left(\frac{1}{2}\right)^4 \cdot \left(1- \frac{1}{2}\right)^2 \Longleftrightarrow \left(\frac{1}{2}\right)^6 = \frac{1}{64}$$ or $$\left(\frac{1}{2}\right)^4 + \left(1- \frac{1}{2}\right)^2 \Longleftrightarrow \frac{1}{16} + \frac{1}{4} = \frac{5}{16}$$ I don't know the correct answer, and I'm not quite sure which argument to use.","I'm having some problems with determining how to calculate a question about the gender proportion in newborns in some random family. A family consists of 6 children. The probability of a boy being born is $1/2$ which is also the probability of a girl being born. So $p = 1/2$. I'm well aware that the variables follow a binomial distribution. I want to determine the probability of the first 4 children born being boys and the last 2 being girls. My ""guesses"" would be either: $$\left(\frac{1}{2}\right)^4 \cdot \left(1- \frac{1}{2}\right)^2 \Longleftrightarrow \left(\frac{1}{2}\right)^6 = \frac{1}{64}$$ or $$\left(\frac{1}{2}\right)^4 + \left(1- \frac{1}{2}\right)^2 \Longleftrightarrow \frac{1}{16} + \frac{1}{4} = \frac{5}{16}$$ I don't know the correct answer, and I'm not quite sure which argument to use.",,"['probability', 'statistics', 'binomial-distribution']"
95,"If I flip a coin $n$ times, what is the expected maximum number of heads or tails in a row?","If I flip a coin  times, what is the expected maximum number of heads or tails in a row?",n,"Question: If I flip a coin $n$ times, what is the maximum number of heads or tails in a row that I should expect?","Question: If I flip a coin $n$ times, what is the maximum number of heads or tails in a row that I should expect?",,"['probability', 'statistics', 'probability-theory', 'expectation']"
96,"Joint Random Variable: Given f(x,y), find P(X>Y)","Joint Random Variable: Given f(x,y), find P(X>Y)",,"There are 2 continuous random variables, X and Y. Say the joint pdf of (X,Y) is f(x,y). How do you find the P(X>Y) generally? Like I am not sure where to start with.","There are 2 continuous random variables, X and Y. Say the joint pdf of (X,Y) is f(x,y). How do you find the P(X>Y) generally? Like I am not sure where to start with.",,"['probability', 'statistics', 'random-variables']"
97,How to prove $E[e^{e^y}]=\infty$? y is a normal random variable,How to prove ? y is a normal random variable,E[e^{e^y}]=\infty,"The question is, given $Y\sim N(\mu,\sigma^2)$, how to prove$E[e^{e^Y}]=\infty$? I tried to look Y as some kind of Ito's process and apply Ito's formula to it but it doesn't make sense. Next I tried to use variable change $u=e^y$ and I still can't prove it. Is there anyway to do that?","The question is, given $Y\sim N(\mu,\sigma^2)$, how to prove$E[e^{e^Y}]=\infty$? I tried to look Y as some kind of Ito's process and apply Ito's formula to it but it doesn't make sense. Next I tried to use variable change $u=e^y$ and I still can't prove it. Is there anyway to do that?",,"['probability', 'statistics', 'stochastic-processes', 'normal-distribution']"
98,Moment Generating Function of a combination of 2 RVs,Moment Generating Function of a combination of 2 RVs,,"The number $N$ of cars sold has a poisson distribution with parameter $m$.  Let $T=X_1 + X_2 +\cdots+ X_N$, where $X$ represents the size of the claim with a $\gamma(\alpha,\beta)$. $X_i$ is independently and identically distributed and independent of $N$. Derive the MGF of $T$. The $\operatorname{MGF}_T(e^{tT})$ is simply,  $\operatorname{MGF}_{X_1 + X_2 +\cdots+ X_N}(e^{t(X_1 + X_2 +\cdots+ X_N)})$ Where the MGF summation/product rule can be used. But we have a random component N in here. How does this affect the whole process?","The number $N$ of cars sold has a poisson distribution with parameter $m$.  Let $T=X_1 + X_2 +\cdots+ X_N$, where $X$ represents the size of the claim with a $\gamma(\alpha,\beta)$. $X_i$ is independently and identically distributed and independent of $N$. Derive the MGF of $T$. The $\operatorname{MGF}_T(e^{tT})$ is simply,  $\operatorname{MGF}_{X_1 + X_2 +\cdots+ X_N}(e^{t(X_1 + X_2 +\cdots+ X_N)})$ Where the MGF summation/product rule can be used. But we have a random component N in here. How does this affect the whole process?",,"['statistics', 'probability-theory', 'probability-distributions', 'moment-generating-functions']"
99,How does $\sum (Y_i-\bar{Y})^2 = \sum Y_i^2 - n\bar{Y}^2$?,How does ?,\sum (Y_i-\bar{Y})^2 = \sum Y_i^2 - n\bar{Y}^2,I've tried my algebra backwards and forwards and starting from the left-hand side of the equation below I just can't get to the right-hand side. I'm always left with an extra term $-2Y_i\bar{Y}$. $\sum (Y_i-\bar{Y})^2 = \sum Y_i^2 - n\bar{Y}^2$,I've tried my algebra backwards and forwards and starting from the left-hand side of the equation below I just can't get to the right-hand side. I'm always left with an extra term $-2Y_i\bar{Y}$. $\sum (Y_i-\bar{Y})^2 = \sum Y_i^2 - n\bar{Y}^2$,,"['statistics', 'summation', 'arithmetic']"
