,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,Exterior derivative,Exterior derivative,,"Let $$\omega = \frac{1}{2} \sum_{i,j} \omega_{i,j} dx_i \wedge dx_j$$ be an antisymmetric form, so i.e. $\omega_{i,j} = - \omega_{j,i}.$ Now, in some lecture notes I found that $$d \omega (X_F,X_G,X_H) = \sum_{a,b,c} \left( \frac{\partial \omega_{b,c}}{\partial x_a} +\frac{\partial \omega_{c,a}}{\partial x_b}+\frac{\partial \omega_{a,b}}{\partial x_c} \right)(X_F)_a (X_G)_b (X_H)_c$$  was stated as a result that should be more or less obvious to the reader. The thing is that I just don't see how you get this out of the exterior derivative. Does anybody see it and could describe what has happened here?","Let $$\omega = \frac{1}{2} \sum_{i,j} \omega_{i,j} dx_i \wedge dx_j$$ be an antisymmetric form, so i.e. $\omega_{i,j} = - \omega_{j,i}.$ Now, in some lecture notes I found that $$d \omega (X_F,X_G,X_H) = \sum_{a,b,c} \left( \frac{\partial \omega_{b,c}}{\partial x_a} +\frac{\partial \omega_{c,a}}{\partial x_b}+\frac{\partial \omega_{a,b}}{\partial x_c} \right)(X_F)_a (X_G)_b (X_H)_c$$  was stated as a result that should be more or less obvious to the reader. The thing is that I just don't see how you get this out of the exterior derivative. Does anybody see it and could describe what has happened here?",,"['differential-geometry', 'differential-topology', 'differential-forms', 'symplectic-geometry']"
1,"Show that the tangent surface of the twisted cubic curve $V(y-x^2,z-x^3)$ is not isomorphic to $\mathbb{R}^2$.",Show that the tangent surface of the twisted cubic curve  is not isomorphic to .,"V(y-x^2,z-x^3) \mathbb{R}^2","This is an exercise in Ideals, Varieties and Algorithms by Cox et al. Show that the tangent surface V of the twisted cubic curve $V(y-x^2,z-x^3)$, given by $$x=t+u\\y=t^2+2tu\\z=t^3+3t^2u$$ is not isomorphic to $\mathbb{R}^2$. Following their hints, I found that $V$ is singular at all points on the twisted cubic curve. The singular points are where $f(x,y,z)=0$ and $\nabla f(x,y,z)=0$. From here I can see $V$ is not isomorphic to $\mathbb{R}^2$ since $V$ is not smooth whereas $\mathbb{R}^2$ is. To prove that, the hint says that consider a polynomial map $\alpha: \mathbb{R}^2\rightarrow V$ such that $\alpha(a,b)$ is on the twisted cubic curve. Then the derivative matrix of $\alpha$ must have rank strictly less than $2$ at $(a,b)$. My question : With this mapping, we should have the surface defined by  $$f(\alpha(u,v))=0$$ So $\nabla f \cdot \alpha'$ is the gradient of the surface on $u,v$. Shouldn't this be zero? And $\nabla f$ is zero at $(a,b)$ since it is singular on the twisted cubic. How does that tell the rank of the derivative matrix? Thanks for any help!","This is an exercise in Ideals, Varieties and Algorithms by Cox et al. Show that the tangent surface V of the twisted cubic curve $V(y-x^2,z-x^3)$, given by $$x=t+u\\y=t^2+2tu\\z=t^3+3t^2u$$ is not isomorphic to $\mathbb{R}^2$. Following their hints, I found that $V$ is singular at all points on the twisted cubic curve. The singular points are where $f(x,y,z)=0$ and $\nabla f(x,y,z)=0$. From here I can see $V$ is not isomorphic to $\mathbb{R}^2$ since $V$ is not smooth whereas $\mathbb{R}^2$ is. To prove that, the hint says that consider a polynomial map $\alpha: \mathbb{R}^2\rightarrow V$ such that $\alpha(a,b)$ is on the twisted cubic curve. Then the derivative matrix of $\alpha$ must have rank strictly less than $2$ at $(a,b)$. My question : With this mapping, we should have the surface defined by  $$f(\alpha(u,v))=0$$ So $\nabla f \cdot \alpha'$ is the gradient of the surface on $u,v$. Shouldn't this be zero? And $\nabla f$ is zero at $(a,b)$ since it is singular on the twisted cubic. How does that tell the rank of the derivative matrix? Thanks for any help!",,"['algebraic-geometry', 'differential-geometry']"
2,Show that a nonconstant subharmonic function on a manifold cannot attain its supremum,Show that a nonconstant subharmonic function on a manifold cannot attain its supremum,,"PROBLEM: Suppose $f$ is a smooth non-constant function on a connected Riemann manifold $M$ of dimension 2  such that $f$ is bounded and $\Delta_M f \ge0$. Show $f$ cannot attain its supremum. I try to reduce the problem to the special case $M=\mathbb R^2$. In this case we can use mean value property. What's more, can this problem be generalized to any connected Riemann manifold $M$ of an arbitrary dimension n?","PROBLEM: Suppose $f$ is a smooth non-constant function on a connected Riemann manifold $M$ of dimension 2  such that $f$ is bounded and $\Delta_M f \ge0$. Show $f$ cannot attain its supremum. I try to reduce the problem to the special case $M=\mathbb R^2$. In this case we can use mean value property. What's more, can this problem be generalized to any connected Riemann manifold $M$ of an arbitrary dimension n?",,"['differential-geometry', 'riemannian-geometry', 'harmonic-functions', 'laplacian']"
3,Tangent space of the space of compatible complex structures,Tangent space of the space of compatible complex structures,,"Let $(M,\omega)$ be a symplectic manifold, and let $\mathcal{J}(M,\omega)$ denote the space of complex structures on $M$ which are compatible with $\omega$. I have been told the following fact: We have an isomorphism $T_J\mathcal{J}(M,\omega)\cong\Omega^{0,1}(M)$, where $\Omega^{0,1}(M)$ is intended with respect to the complex structure $J\in\mathcal{J}(M,\omega)$. I am trying to prove this fact, but without success until now. Does anyone have ideas, or better still a nice reference? I will post if I find anything. Note: The case that interests me the most is when $M$ is a compact, oriented surface (of genus $g_M\ge2$).","Let $(M,\omega)$ be a symplectic manifold, and let $\mathcal{J}(M,\omega)$ denote the space of complex structures on $M$ which are compatible with $\omega$. I have been told the following fact: We have an isomorphism $T_J\mathcal{J}(M,\omega)\cong\Omega^{0,1}(M)$, where $\Omega^{0,1}(M)$ is intended with respect to the complex structure $J\in\mathcal{J}(M,\omega)$. I am trying to prove this fact, but without success until now. Does anyone have ideas, or better still a nice reference? I will post if I find anything. Note: The case that interests me the most is when $M$ is a compact, oriented surface (of genus $g_M\ge2$).",,"['differential-geometry', 'complex-geometry', 'symplectic-geometry', 'kahler-manifolds']"
4,Normal coordinate parallel along radial geodesics?,Normal coordinate parallel along radial geodesics?,,"A radial geodesic in normal coordinates is given by $\gamma:t \mapsto t(V_1,....,V_n).$ Is it then true that any normal coordinate $\partial_x|_{\gamma}$ is parallel along $\gamma,$ i.e. $\nabla_{\gamma'}\partial_x=0$?  I implicitly saw this once in a calculation, but I have never seen this anywhere as a property of normal coordinates, so I guess it is false, but I don't really know. If anything is unclear, please let me know.","A radial geodesic in normal coordinates is given by $\gamma:t \mapsto t(V_1,....,V_n).$ Is it then true that any normal coordinate $\partial_x|_{\gamma}$ is parallel along $\gamma,$ i.e. $\nabla_{\gamma'}\partial_x=0$?  I implicitly saw this once in a calculation, but I have never seen this anywhere as a property of normal coordinates, so I guess it is false, but I don't really know. If anything is unclear, please let me know.",,"['real-analysis', 'differential-geometry', 'manifolds', 'riemannian-geometry']"
5,Surface of constant mean curvature,Surface of constant mean curvature,,"From PDE Evans, 2nd edition: Chapter 8, Exercise 12: Assume $u$ is a smooth minimizer of the area integral $$I[w]=\int_U (1+|Dw|^2)^{1/2} \, dx,$$ subject to given boundary conditions $w=g$ on $\partial U$ and the constraint $$J[w] = \int_U w \, dx = 1.$$ Prove that graph of $u$ is a surface of constant mean curvature. (Hint: Recall Example 4 in ยง8.1.2.) First, I will print Example 4 from the textbook (page 457) below: Example 4 (Minimal surfaces) . Let $$L(p,z,x)=(1+|p|^2)^{1/2},$$ so that $$I[w] = \int_U (1+|Dw|^2)^{1/2} \, dx$$ is the area of the graph of the function $w : U \to \mathbb{R}$ . The associated Euler-Lagrange equation is $$\sum_{i=1}^n \left(\frac{u_{x_i}}{(1+|Du|^2)^{1/2}} \right)_{x_i}=0 \quad \text{in }U.$$ $\quad$ This partial differential equation is the minimal surface equation . The expression $\operatorname{div} \left(\frac{Du}{(1+|Du|^2)^{1/2}} \right)$ on the left side of $(10)$ is $n$ times the mean curvature of the graph of $u$ . Thus a minimal surface has zero mean curvature . I don't have much work started on this, but this question looks interesting. (This is not a homework assignment, as with all my other PDE Evans questions.) But I am asking here because I do not understand fundamentally how mean curvature is applied to Euler-Lagrange equations. In particular, this question uses concepts of differential geometry, which I have not taken any courses in yet in my academic career. Now, what I do know so far, is that maybe I should show that the graph of $u$ is a minimal surface. This would mean $u$ has zero mean curvature, and hence a constant mean curvature (zero is constant, obviously). How should I start about this problem?","From PDE Evans, 2nd edition: Chapter 8, Exercise 12: Assume is a smooth minimizer of the area integral subject to given boundary conditions on and the constraint Prove that graph of is a surface of constant mean curvature. (Hint: Recall Example 4 in ยง8.1.2.) First, I will print Example 4 from the textbook (page 457) below: Example 4 (Minimal surfaces) . Let so that is the area of the graph of the function . The associated Euler-Lagrange equation is This partial differential equation is the minimal surface equation . The expression on the left side of is times the mean curvature of the graph of . Thus a minimal surface has zero mean curvature . I don't have much work started on this, but this question looks interesting. (This is not a homework assignment, as with all my other PDE Evans questions.) But I am asking here because I do not understand fundamentally how mean curvature is applied to Euler-Lagrange equations. In particular, this question uses concepts of differential geometry, which I have not taken any courses in yet in my academic career. Now, what I do know so far, is that maybe I should show that the graph of is a minimal surface. This would mean has zero mean curvature, and hence a constant mean curvature (zero is constant, obviously). How should I start about this problem?","u I[w]=\int_U (1+|Dw|^2)^{1/2} \, dx, w=g \partial U J[w] = \int_U w \, dx = 1. u L(p,z,x)=(1+|p|^2)^{1/2}, I[w] = \int_U (1+|Dw|^2)^{1/2} \, dx w : U \to \mathbb{R} \sum_{i=1}^n \left(\frac{u_{x_i}}{(1+|Du|^2)^{1/2}} \right)_{x_i}=0 \quad \text{in }U. \quad \operatorname{div} \left(\frac{Du}{(1+|Du|^2)^{1/2}} \right) (10) n u u u","['differential-geometry', 'partial-differential-equations', 'calculus-of-variations', 'euler-lagrange-equation']"
6,Coordinates on the sphere not global?,Coordinates on the sphere not global?,,"I'm reading a book on differential geometry and some part of the introduction I do not understand but I'm curious to understand it. Maybe someone can try to explain those parts to me. ""Each point on the earth has a latitude and a longitude which determines its position. The coordinates are not global. "" Q: What does it mean that the coordinates are not global? Intuitively, I would call a coordinate system 'global' if every point on earth can be described by them. Then it continues: ""There are four domains on the earth, bounded by the Equator and the Greenwich meridian, which are naturally coordinated . The points on the Equator and the Greenwich meridian admit two different coordinatizations."" I do not understand what the author has in mind when he talks about ""naturally coordinated"" domain. Further, the points on the Equator should have also unique coordinates, one latitude and a longitude coordinate. Am I wrong? Any help will be very appreciated! Best wishes","I'm reading a book on differential geometry and some part of the introduction I do not understand but I'm curious to understand it. Maybe someone can try to explain those parts to me. ""Each point on the earth has a latitude and a longitude which determines its position. The coordinates are not global. "" Q: What does it mean that the coordinates are not global? Intuitively, I would call a coordinate system 'global' if every point on earth can be described by them. Then it continues: ""There are four domains on the earth, bounded by the Equator and the Greenwich meridian, which are naturally coordinated . The points on the Equator and the Greenwich meridian admit two different coordinatizations."" I do not understand what the author has in mind when he talks about ""naturally coordinated"" domain. Further, the points on the Equator should have also unique coordinates, one latitude and a longitude coordinate. Am I wrong? Any help will be very appreciated! Best wishes",,"['differential-geometry', 'coordinate-systems', 'spherical-coordinates']"
7,Relation between jet and singularities of a curve,Relation between jet and singularities of a curve,,"I am trying to understand the relationship between a jet and the singularities of a curve. Concretely I am trying to understand the following example: Let $\gamma(t) = (\cos t , b \sin t)$ where $b > 0$ be an ellipse. Let $f$ be the distance squared function from $({1\over 2},0)$ to $\gamma (t)$ i.e. $$ f(t) = ({1\over 2} - \cos t)^2 + b^2 \sin^2 t$$ For each $t_0$ consider the $2$-jet at $t_0$ of $f$: $$ (\sin t_0 + (b^2 -1)\sin 2 t_0)t + (\cos t_0 + 2(b^2 -1)\cos 2 t_0) t^2$$ Writing this as $c_1 t + c_2 t^2$ the points $(c_1, c_2)$ will trace out a curve $\delta$ as $t_0$ varies. This curve will cross the $c_2-$axis at points where $f$ has a singularity $A_{\ge 1}$ at $t_0$ and will pass through the origin where $f$ has an $A_{\ge2}$ singularity at $t_0$. The geometrical interpretation of this is that $f$ has an $A_{\ge1}$ singularity at $t_0$ iff the normal to the ellipse at $\gamma (t_0)$ passes through $({1\over 2},0)$ and an $A_{\ge 2}$ singularity iff $({1\over 2},0)$ is the centre of curvature at $\gamma (t_0)$. I have absolutely no intuition about why the coefficients of the jet   should have the property that they trace out a curve and this curves   contains information about the singularities of the original curve.   Please could someone help me understand why this is true? This example can be found here on page 57.","I am trying to understand the relationship between a jet and the singularities of a curve. Concretely I am trying to understand the following example: Let $\gamma(t) = (\cos t , b \sin t)$ where $b > 0$ be an ellipse. Let $f$ be the distance squared function from $({1\over 2},0)$ to $\gamma (t)$ i.e. $$ f(t) = ({1\over 2} - \cos t)^2 + b^2 \sin^2 t$$ For each $t_0$ consider the $2$-jet at $t_0$ of $f$: $$ (\sin t_0 + (b^2 -1)\sin 2 t_0)t + (\cos t_0 + 2(b^2 -1)\cos 2 t_0) t^2$$ Writing this as $c_1 t + c_2 t^2$ the points $(c_1, c_2)$ will trace out a curve $\delta$ as $t_0$ varies. This curve will cross the $c_2-$axis at points where $f$ has a singularity $A_{\ge 1}$ at $t_0$ and will pass through the origin where $f$ has an $A_{\ge2}$ singularity at $t_0$. The geometrical interpretation of this is that $f$ has an $A_{\ge1}$ singularity at $t_0$ iff the normal to the ellipse at $\gamma (t_0)$ passes through $({1\over 2},0)$ and an $A_{\ge 2}$ singularity iff $({1\over 2},0)$ is the centre of curvature at $\gamma (t_0)$. I have absolutely no intuition about why the coefficients of the jet   should have the property that they trace out a curve and this curves   contains information about the singularities of the original curve.   Please could someone help me understand why this is true? This example can be found here on page 57.",,['differential-geometry']
8,"Gauss-Bonnet Theorem, External Angles and Orientation","Gauss-Bonnet Theorem, External Angles and Orientation",,"The Global Gauss-Bonnet Theorem states: Let $R\subset S$ be a regular region and $C_1,\ldots,C_r$ be closed, simple, piecewise regular curves forming the boundary of $R$. Suposse $C_i$ is positively oriented and $\theta_1,\ldots,\theta_n$ be the external angles of the curves. Then: \begin{equation*} \sum\limits_{i=1}^{r} \int_{C_i} k_g^{C_i}(s) ds + \iint_R K d\sigma + \sum\limits_{j=1}^{n} \theta_j = 2\pi\chi(R) \end{equation*} My questions are: What is the easiest way to orientate positively the curves? Should we compute the external angles before or after the orientation of the curves? How do we compute easily the sign of the external angles? Suppose we have a region $R$ with is homeomorphic to a square like this: Is it true that $\chi(R)=1-n$, where $n$ denote the number of holes?","The Global Gauss-Bonnet Theorem states: Let $R\subset S$ be a regular region and $C_1,\ldots,C_r$ be closed, simple, piecewise regular curves forming the boundary of $R$. Suposse $C_i$ is positively oriented and $\theta_1,\ldots,\theta_n$ be the external angles of the curves. Then: \begin{equation*} \sum\limits_{i=1}^{r} \int_{C_i} k_g^{C_i}(s) ds + \iint_R K d\sigma + \sum\limits_{j=1}^{n} \theta_j = 2\pi\chi(R) \end{equation*} My questions are: What is the easiest way to orientate positively the curves? Should we compute the external angles before or after the orientation of the curves? How do we compute easily the sign of the external angles? Suppose we have a region $R$ with is homeomorphic to a square like this: Is it true that $\chi(R)=1-n$, where $n$ denote the number of holes?",,"['differential-geometry', 'surfaces']"
9,Every submanifold of $\mathbb R^n$ is locally a level set,Every submanifold of  is locally a level set,\mathbb R^n,"Is it true a very submanifold $M$ of $\mathbb R^n$  is locally a level set? Given a chart $\phi$ about $p \in M$, how can we construct a smooth function $f$ s.t. $f^{-1}(0)=  M \cap U$ for some open neighbourhood $U$ of $p$?","Is it true a very submanifold $M$ of $\mathbb R^n$  is locally a level set? Given a chart $\phi$ about $p \in M$, how can we construct a smooth function $f$ s.t. $f^{-1}(0)=  M \cap U$ for some open neighbourhood $U$ of $p$?",,"['differential-geometry', 'manifolds']"
10,Prerequisites for the Gauss-Green theorem,Prerequisites for the Gauss-Green theorem,,"Consider the following theorem from Appendix C from Evans' PDE book: I know about integration in $\mathbb{R}^n$ but not about how to make sense of the integrals on the right-hand side. As my time is very limited, I can't read 50+ pages on the general theory of integration on manifolds. I am in desperate need of a text that in a minimum amount of space manages to get myself acquainted with those integrals and the proof of (i) from above. It's a plus if the text has some worked examples (the proofs from those statements of the text, that are intuitively plausible, I want to skip anyway because I don't have sufficient time). Note: This question is a total re-edit. For further details please consult the edit history.","Consider the following theorem from Appendix C from Evans' PDE book: I know about integration in but not about how to make sense of the integrals on the right-hand side. As my time is very limited, I can't read 50+ pages on the general theory of integration on manifolds. I am in desperate need of a text that in a minimum amount of space manages to get myself acquainted with those integrals and the proof of (i) from above. It's a plus if the text has some worked examples (the proofs from those statements of the text, that are intuitively plausible, I want to skip anyway because I don't have sufficient time). Note: This question is a total re-edit. For further details please consult the edit history.",\mathbb{R}^n,"['differential-geometry', 'partial-differential-equations']"
11,Proof of a tensor identity involved in the derivation of the Einstein field equations?,Proof of a tensor identity involved in the derivation of the Einstein field equations?,,"On the wikipedia page for the einstein-hilbert action, the section for the derivation of the einstein field equations cites this identity: $$ \sqrt{g} \nabla_\mu A^\mu = \partial_\mu (\sqrt{g} A^\mu) $$ Where $g$ is the determinant of the metric tensor, $\nabla_\mu$ is the covariant derivative, $\partial_\mu$ is the ordinary derivative with respect to the $x^\mu$ coordinate, $A^\mu$ is an arbitrary rank 1 tensor, Einstein summation notation is implied.  So I tryed to prove this and started by first expanding the right side: $$ \partial_\mu (\sqrt{g} A^\mu) = \frac{\partial_\mu g}{2\sqrt{g}} + \sqrt{g}\partial_\mu A^\mu = \sqrt{g}(\frac{\partial_\mu g}{2g}A^\mu + \partial_\mu A^\mu) $$ From here one must simply show that the expression in parentheses is equal to the definition of $\nabla_\mu A^\mu$ which is $$ \partial_\mu A^\mu + \Gamma^\mu_{\mu \nu} A^\nu $$ Since we see both terms have the partial derivatives we just have to prove that $$\frac{\partial_\mu g}{2g}A^\mu = \Gamma^\mu_{\mu \nu}A^\nu $$ or $$ \frac{\partial_\nu g}{g} = g^{\mu \alpha}(\partial_\mu g_{\nu \alpha} + \partial_\nu g_{\mu \alpha} - \partial_\alpha g_{\mu \nu})$$ I've tried this by plugging in the definition of the metric tensor's determinant both in tensor notation and fully written out.  Is there some formula in tensor notation for the components of the inverse of the metric tensor?  I feel like that would help a lot.  Otherwise, could you direct me to a proof of the identity itself?  Thank you!","On the wikipedia page for the einstein-hilbert action, the section for the derivation of the einstein field equations cites this identity: $$ \sqrt{g} \nabla_\mu A^\mu = \partial_\mu (\sqrt{g} A^\mu) $$ Where $g$ is the determinant of the metric tensor, $\nabla_\mu$ is the covariant derivative, $\partial_\mu$ is the ordinary derivative with respect to the $x^\mu$ coordinate, $A^\mu$ is an arbitrary rank 1 tensor, Einstein summation notation is implied.  So I tryed to prove this and started by first expanding the right side: $$ \partial_\mu (\sqrt{g} A^\mu) = \frac{\partial_\mu g}{2\sqrt{g}} + \sqrt{g}\partial_\mu A^\mu = \sqrt{g}(\frac{\partial_\mu g}{2g}A^\mu + \partial_\mu A^\mu) $$ From here one must simply show that the expression in parentheses is equal to the definition of $\nabla_\mu A^\mu$ which is $$ \partial_\mu A^\mu + \Gamma^\mu_{\mu \nu} A^\nu $$ Since we see both terms have the partial derivatives we just have to prove that $$\frac{\partial_\mu g}{2g}A^\mu = \Gamma^\mu_{\mu \nu}A^\nu $$ or $$ \frac{\partial_\nu g}{g} = g^{\mu \alpha}(\partial_\mu g_{\nu \alpha} + \partial_\nu g_{\mu \alpha} - \partial_\alpha g_{\mu \nu})$$ I've tried this by plugging in the definition of the metric tensor's determinant both in tensor notation and fully written out.  Is there some formula in tensor notation for the components of the inverse of the metric tensor?  I feel like that would help a lot.  Otherwise, could you direct me to a proof of the identity itself?  Thank you!",,"['calculus', 'differential-geometry', 'tensors']"
12,Signature of the norm on $k$-forms.,Signature of the norm on -forms.,k,"I wonder about the signature of the inner product on forms on a vector space equipped with a non-degenerate bilinear of signature $(t,s)$. For definiteness, let $(V_{6}, g)$ be an oriented six-dimensional vector space equipped with a non-degenerate metric $g$ of signature $(s,t)$ and let $W=\Lambda^{3}V^{\ast}_{6}$. In $W$ there is an induced metric $<\cdot,\cdot>$ given by $< h_{1}, h_{2}> \Omega = h_{1}\wedge \ast h_{2}\, , \qquad h_{1}, h_{2} \in W$ where $\Omega$ is the standard volume form induced by $g$ and $\ast$ is the Hodge dual associated to $g$. I would like to know what is the signature of $<\cdot,\cdot>$ assuming that the signature of $g$ is $(t,s)$. If $t=6$ and $s=0$ then $<\cdot,\cdot>$ is definite positive, but that is not the case in general. For example, if $(t,s)=(1,5)$ then $< h, h> = 0$ if $h = \ast h$, namely if $h\in W$ is self-dual (by the way, is this an if and only if?) Thanks","I wonder about the signature of the inner product on forms on a vector space equipped with a non-degenerate bilinear of signature $(t,s)$. For definiteness, let $(V_{6}, g)$ be an oriented six-dimensional vector space equipped with a non-degenerate metric $g$ of signature $(s,t)$ and let $W=\Lambda^{3}V^{\ast}_{6}$. In $W$ there is an induced metric $<\cdot,\cdot>$ given by $< h_{1}, h_{2}> \Omega = h_{1}\wedge \ast h_{2}\, , \qquad h_{1}, h_{2} \in W$ where $\Omega$ is the standard volume form induced by $g$ and $\ast$ is the Hodge dual associated to $g$. I would like to know what is the signature of $<\cdot,\cdot>$ assuming that the signature of $g$ is $(t,s)$. If $t=6$ and $s=0$ then $<\cdot,\cdot>$ is definite positive, but that is not the case in general. For example, if $(t,s)=(1,5)$ then $< h, h> = 0$ if $h = \ast h$, namely if $h\in W$ is self-dual (by the way, is this an if and only if?) Thanks",,"['linear-algebra', 'differential-geometry', 'vector-spaces']"
13,Tensor calculus on the frame bundle,Tensor calculus on the frame bundle,,"Let $M$ be a manifold and let $g$ be a tensor on it, say for example a metric $g\in\Gamma(T^{\ast}M\otimes T^{\ast}M)$. I know how to perform any computation on $g$. For instance, taking its derivative respect to the a connection $\nabla$, evaluating it at a point, taking its Lie derivative, obtaining the curvature of the Levi-Civita connection etc. However, there is a dual formulation on the frame bundle $F(M)$ of $M$, but I never knew how to do the same calculations on the frame bundle, and as I understand it is sometimes simpler to work on the frame bundle. I would like to know how a tensor on $M$ is represented from the point of view of the frame bundle, and how are the typical operations (curvature, Lie derivative etc) implemented. A tensor in $M$ is a section of the corresponding tensor vector bundle. How is this mapped to the frame bundle? For example, given an open set $U$ of the atlas of $M$ I can write $g$ in coordinates as follows $g = g_{ab}\,dx^{a}\otimes dx^{b}$ What would be the analog local expression from the point of view of the frame bundle? Finally, I would like to know a reference where these things are explained in detail. Thanks.","Let $M$ be a manifold and let $g$ be a tensor on it, say for example a metric $g\in\Gamma(T^{\ast}M\otimes T^{\ast}M)$. I know how to perform any computation on $g$. For instance, taking its derivative respect to the a connection $\nabla$, evaluating it at a point, taking its Lie derivative, obtaining the curvature of the Levi-Civita connection etc. However, there is a dual formulation on the frame bundle $F(M)$ of $M$, but I never knew how to do the same calculations on the frame bundle, and as I understand it is sometimes simpler to work on the frame bundle. I would like to know how a tensor on $M$ is represented from the point of view of the frame bundle, and how are the typical operations (curvature, Lie derivative etc) implemented. A tensor in $M$ is a section of the corresponding tensor vector bundle. How is this mapped to the frame bundle? For example, given an open set $U$ of the atlas of $M$ I can write $g$ in coordinates as follows $g = g_{ab}\,dx^{a}\otimes dx^{b}$ What would be the analog local expression from the point of view of the frame bundle? Finally, I would like to know a reference where these things are explained in detail. Thanks.",,"['algebraic-geometry', 'differential-geometry']"
14,"mutually transverse embedded submanifolds, natural bundle surjections, direct sum, isomorphism","mutually transverse embedded submanifolds, natural bundle surjections, direct sum, isomorphism",,"Let $N$ be a manifold and let $M_1, \dots, M_n \hookrightarrow N$ be mutually transverse embedded submanifolds, so $M = \cap M_i$ is an embedded submanifold of $N$ with $\text{T}_m(M) = \cap T_m(M_i)$ inside of $T_m(N)$. Let $j_i: M \hookrightarrow M_i$ be the embedding. Using the natural bundle surjections $\text{N}_{M/N} \to j_i^*(\text{N}_{M_i/N})$, show that the resulting map$$\text{N}_{M/N} \to \bigoplus_i j_i^*(\text{N}_{M_i/N})$$to the direct sum is an isomorphism. (What does it say on fibers?) In the special case $N = \mathbb{R}^n$ with its standard inner product, use the identification of normal bundles with orthogonal subbundles of tangent bundles to identify this isomorphism as an equality of subbundles of the pullback of $TN$ to $M$. (What does it say on fibers?) In words and pictures, explain the geometric meaning of this equality in terms of ""directions perpendicular to submanifolds."" What are some hints to get started in the right direction? Any help would be appreciated, thanks.","Let $N$ be a manifold and let $M_1, \dots, M_n \hookrightarrow N$ be mutually transverse embedded submanifolds, so $M = \cap M_i$ is an embedded submanifold of $N$ with $\text{T}_m(M) = \cap T_m(M_i)$ inside of $T_m(N)$. Let $j_i: M \hookrightarrow M_i$ be the embedding. Using the natural bundle surjections $\text{N}_{M/N} \to j_i^*(\text{N}_{M_i/N})$, show that the resulting map$$\text{N}_{M/N} \to \bigoplus_i j_i^*(\text{N}_{M_i/N})$$to the direct sum is an isomorphism. (What does it say on fibers?) In the special case $N = \mathbb{R}^n$ with its standard inner product, use the identification of normal bundles with orthogonal subbundles of tangent bundles to identify this isomorphism as an equality of subbundles of the pullback of $TN$ to $M$. (What does it say on fibers?) In words and pictures, explain the geometric meaning of this equality in terms of ""directions perpendicular to submanifolds."" What are some hints to get started in the right direction? Any help would be appreciated, thanks.",,"['differential-geometry', 'manifolds']"
15,Showing a property of a curvature tensor in $S^2$,Showing a property of a curvature tensor in,S^2,"Consider $S^2 \subset \mathbb{R}^3$. I need to show that if $$R_{ijkl} = -g(R(\partial_i,\partial_j)\partial_k,\partial_l)$$ is a curvature tensor in $S^2$ and $g$ is a metric also in $S^2$, then $$R_{ijkl} = k(g_{ik}g_{jl} - g_{il}g_{jk})$$ where $k$ is the curvature of $S^2$. Attempt: I managed to show that $R_{ijkl} = -R_{jikl}$ and $$R_{ijkl} + R_{jkil} + R_{kilj} = 0 $$ (Bianchi's First Identity) I couldn't figure out though how to get that identiy. I was thingking maybe use that $R_{ijkl} = g_{im}R^{m}_{ijk}$. Edit: I got one more step using the properties above $$R_{ijkl} = \frac{1}{2}\Big(R_{jkil}+R_{kilj}-R_{ikjl}-R_{kjli}\Big)$$ Any help, please? I've been searching and found that this could be derived from Bianchi's Identities.","Consider $S^2 \subset \mathbb{R}^3$. I need to show that if $$R_{ijkl} = -g(R(\partial_i,\partial_j)\partial_k,\partial_l)$$ is a curvature tensor in $S^2$ and $g$ is a metric also in $S^2$, then $$R_{ijkl} = k(g_{ik}g_{jl} - g_{il}g_{jk})$$ where $k$ is the curvature of $S^2$. Attempt: I managed to show that $R_{ijkl} = -R_{jikl}$ and $$R_{ijkl} + R_{jkil} + R_{kilj} = 0 $$ (Bianchi's First Identity) I couldn't figure out though how to get that identiy. I was thingking maybe use that $R_{ijkl} = g_{im}R^{m}_{ijk}$. Edit: I got one more step using the properties above $$R_{ijkl} = \frac{1}{2}\Big(R_{jkil}+R_{kilj}-R_{ikjl}-R_{kjli}\Big)$$ Any help, please? I've been searching and found that this could be derived from Bianchi's Identities.",,"['differential-geometry', 'riemannian-geometry', 'tensors', 'curvature']"
16,Simply-connected Lorentzian manifold and event horizon,Simply-connected Lorentzian manifold and event horizon,,Can a simply connected Lorentzian manifold admit an event horizon? Or does the event horizon makes it non-simply connected?,Can a simply connected Lorentzian manifold admit an event horizon? Or does the event horizon makes it non-simply connected?,,"['differential-geometry', 'mathematical-physics', 'general-relativity']"
17,Induced Lie group action on a tangent bundle $TG\times TM\to TM$ and an example concerning Adjoint action,Induced Lie group action on a tangent bundle  and an example concerning Adjoint action,TG\times TM\to TM,"Suppose I have a Lie group action  $$ G\times M\to M, (g,m)\mapsto g\cdot m. $$  which is transitive on $M$, then the tangent functor $T$ induces a corresponding map: $$ TG\times TM\to TM, (\delta g,\delta m)\mapsto \delta g\cdot m+g\cdot\delta m. $$ Note that given the group operation of $G$: $$ G\times G\to G, (g,h)\mapsto g\cdot h. $$ $TG$ can be induced a Lie group structure by: $$ \delta g\star\delta h:=\delta (g\cdot h)=\delta g\cdot h+g\cdot\delta h. $$ The group axioms can be verified as follows: closure: trivial; identity: $0_e$ where $e$ is the identity of $G$; inverse: $(\delta g)^{-1}=\delta(g^{-1})$; associativity:  $$ (\delta g\star\delta h)\star\delta k=\delta(g\cdot h)\star\delta k=\delta (g\cdot h\cdot k)=\delta g\star\delta(h\cdot k)=\delta g\star(\delta h\star\delta k). $$ Under this particular group operation of $TG$, the induced map $TG\times TM\to TM$ defines a Lie group action of $TG$ on $TM$, which is evident from Leibniz rule. And in particular it should be transitive too. Questions: I would like someone to verify for me my above claims first. If there is a good reference about it, I would very much like to know. Is it possible to define an induced action $TSO(3)\times TS^2\to TS^2$ from: $$ SO(3)\times S^2\to S^2, (R,r)\mapsto Rr $$  such that it corresponds exactly to the Adjoint action $SE(3)\times se(3)\to se(3)$? (see the comments below) Some follow-up comments: One of the main applications is a generalization of the action $SO(3)\times S^2\to S^2$ (with $SO(3)$ considered as a Lie subgroup of $SE(3)$) to the whole of $SE(3)\simeq TSO(3)$ on $TS^2$: $$ \begin{split} \psi:SE(3)\times TS^2&\to TS^2, \\ (\hat pR,\delta r)&\mapsto \hat pRr+R\delta r. \end{split} $$ where $p\in\mathbb R^3, R\in SO(3), r\in S^2$ and $\wedge$ is defined by: $$ \wedge:(x,y,z)^T\mapsto\left[\begin{array}{ccc} 0 & -z & y\\ z & 0 & -x\\ -y & x & 0 \end{array}\right] $$ and investigate its connection to Adjoint transformation: $$ Ad:SE(3)\to\mathfrak{gl}(se(3)) $$ To see the connection, define a $\xi\in se(3)$ for $\delta r\in TS^2$ by: $$ \xi:=\left[\begin{array}{cc}\hat r & r\times\delta r\\ 0 & 0  \end{array}\right], r\in S^2. $$ Unfortunately, the induced action $TSO(3)\times TS^2\to TS^2$ is not consistent with the Adjoint action. I am looking for an alternative action $TSO(3)\times TS^2\to TS^2$ such that it is.","Suppose I have a Lie group action  $$ G\times M\to M, (g,m)\mapsto g\cdot m. $$  which is transitive on $M$, then the tangent functor $T$ induces a corresponding map: $$ TG\times TM\to TM, (\delta g,\delta m)\mapsto \delta g\cdot m+g\cdot\delta m. $$ Note that given the group operation of $G$: $$ G\times G\to G, (g,h)\mapsto g\cdot h. $$ $TG$ can be induced a Lie group structure by: $$ \delta g\star\delta h:=\delta (g\cdot h)=\delta g\cdot h+g\cdot\delta h. $$ The group axioms can be verified as follows: closure: trivial; identity: $0_e$ where $e$ is the identity of $G$; inverse: $(\delta g)^{-1}=\delta(g^{-1})$; associativity:  $$ (\delta g\star\delta h)\star\delta k=\delta(g\cdot h)\star\delta k=\delta (g\cdot h\cdot k)=\delta g\star\delta(h\cdot k)=\delta g\star(\delta h\star\delta k). $$ Under this particular group operation of $TG$, the induced map $TG\times TM\to TM$ defines a Lie group action of $TG$ on $TM$, which is evident from Leibniz rule. And in particular it should be transitive too. Questions: I would like someone to verify for me my above claims first. If there is a good reference about it, I would very much like to know. Is it possible to define an induced action $TSO(3)\times TS^2\to TS^2$ from: $$ SO(3)\times S^2\to S^2, (R,r)\mapsto Rr $$  such that it corresponds exactly to the Adjoint action $SE(3)\times se(3)\to se(3)$? (see the comments below) Some follow-up comments: One of the main applications is a generalization of the action $SO(3)\times S^2\to S^2$ (with $SO(3)$ considered as a Lie subgroup of $SE(3)$) to the whole of $SE(3)\simeq TSO(3)$ on $TS^2$: $$ \begin{split} \psi:SE(3)\times TS^2&\to TS^2, \\ (\hat pR,\delta r)&\mapsto \hat pRr+R\delta r. \end{split} $$ where $p\in\mathbb R^3, R\in SO(3), r\in S^2$ and $\wedge$ is defined by: $$ \wedge:(x,y,z)^T\mapsto\left[\begin{array}{ccc} 0 & -z & y\\ z & 0 & -x\\ -y & x & 0 \end{array}\right] $$ and investigate its connection to Adjoint transformation: $$ Ad:SE(3)\to\mathfrak{gl}(se(3)) $$ To see the connection, define a $\xi\in se(3)$ for $\delta r\in TS^2$ by: $$ \xi:=\left[\begin{array}{cc}\hat r & r\times\delta r\\ 0 & 0  \end{array}\right], r\in S^2. $$ Unfortunately, the induced action $TSO(3)\times TS^2\to TS^2$ is not consistent with the Adjoint action. I am looking for an alternative action $TSO(3)\times TS^2\to TS^2$ such that it is.",,"['differential-geometry', 'lie-groups']"
18,Why aren't those spaces diffeomorphic?,Why aren't those spaces diffeomorphic?,,"(Taken from Bredon - Topology and Geometry): Let $X$ be the graph of the real valued function $\theta(x)=|x|$ of a real variable $x$. Define a functional structure on $X$ by taking $f \in F(U) \iff f$ is the restriction to $U$ of a $C^{ \infty}$ function on some open set $V$ in the plane with $U=V \cap X$. Show that $X$ with this structure is not diffeomorphic to the real line with usual $C^{\infty}$ structure. Well, I arrived at the fact that a diffeomorphism between them would have to be the restriction of a differentiable function with non-zero derivative. I imagine I have to use the Implicit Function Theorem and conclude that $\theta$ would have to be the graph of a $C^1$ function near $0$, but I don't know how to proceed.","(Taken from Bredon - Topology and Geometry): Let $X$ be the graph of the real valued function $\theta(x)=|x|$ of a real variable $x$. Define a functional structure on $X$ by taking $f \in F(U) \iff f$ is the restriction to $U$ of a $C^{ \infty}$ function on some open set $V$ in the plane with $U=V \cap X$. Show that $X$ with this structure is not diffeomorphic to the real line with usual $C^{\infty}$ structure. Well, I arrived at the fact that a diffeomorphism between them would have to be the restriction of a differentiable function with non-zero derivative. I imagine I have to use the Implicit Function Theorem and conclude that $\theta$ would have to be the graph of a $C^1$ function near $0$, but I don't know how to proceed.",,"['differential-geometry', 'differential-topology']"
19,Prove: $(\delta^\nabla\text{d}^\nabla+\text{d}^\nabla\delta^\nabla)h=\nabla^*\nabla h-\mathring{R}_gh+h\circ\text{Ricc}_g$,Prove:,(\delta^\nabla\text{d}^\nabla+\text{d}^\nabla\delta^\nabla)h=\nabla^*\nabla h-\mathring{R}_gh+h\circ\text{Ricc}_g,"Let $(M,g)$ be a Riemannian manifold with Levi-Civita connection $\nabla$ and adjoint $\nabla^*$, and exterior derivative $\text{d}^\nabla$ and adjoint $\delta^\nabla$.  For a symmetric 2-covariant tensor $h\in\Gamma(S^2M)\subset\Gamma(T^*M\otimes T^*M)\cong\Omega^1(T^*M)$, viewed as a $T^*M$-valued 1-form on $M$, section 12.69. in Besse Einstein Manifolds says that the following is an easy calculation: $$ (\delta^\nabla\text{d}^\nabla+\text{d}^\nabla\delta^\nabla)h=\nabla^*\nabla h-\mathring{R}_gh+h\circ\text{Ricc}_g. $$ Can someone please present a proof or point to where (a detailed) one is available in the literature?","Let $(M,g)$ be a Riemannian manifold with Levi-Civita connection $\nabla$ and adjoint $\nabla^*$, and exterior derivative $\text{d}^\nabla$ and adjoint $\delta^\nabla$.  For a symmetric 2-covariant tensor $h\in\Gamma(S^2M)\subset\Gamma(T^*M\otimes T^*M)\cong\Omega^1(T^*M)$, viewed as a $T^*M$-valued 1-form on $M$, section 12.69. in Besse Einstein Manifolds says that the following is an easy calculation: $$ (\delta^\nabla\text{d}^\nabla+\text{d}^\nabla\delta^\nabla)h=\nabla^*\nabla h-\mathring{R}_gh+h\circ\text{Ricc}_g. $$ Can someone please present a proof or point to where (a detailed) one is available in the literature?",,"['differential-geometry', 'riemannian-geometry', 'general-relativity']"
20,Question regarding character varieties on a torus with compact gauge group,Question regarding character varieties on a torus with compact gauge group,,"Let $G$ be a compact, connected Lie group. Let $x, y \in G$ be an arbitrary pair of commuting elements. Is there necessarily a torus $T \leq G$ containing $x$ and $y$? Apparently not: Commutativity and Maximal Tori in Connected, Compact Lie Groups The issue arises due to discrete abelian subgroups. Consider the $SO_3(\mathbb{R})$ example from the link. Thinking geometrically, a maximal torus can be described as the circle subgroup of rotations fixing a particular direction in $\mathbb{R}^3$. Consider two orthogonal directions. Then rotation by $\pi$ radians about these two axes will commute, but they certainly do not live in a common maximal torus. My concern comes from trying to compute the following character variety on a torus: $\chi_G(\Sigma_1)=\{\rho: \pi_1(\Sigma_1) \to G \ \vert \rho \ \text{is a group homomorphism}\}/\text{conjugation by} \ G$ where $\Sigma_1 \cong S^1 \times S^1$ is a torus. Since $\pi_1(\Sigma_1)$ is free abelian of rank 2, it follows that such a homomorphism is determined by a choice of commuting elements $x, y \in G$. What should I make of the following argument, particularly the part characterizing a flat connection on $S^1 \times S^1$? [Edit: The link has been updated to reflect this issue] http://ncatlab.org/nlab/show/moduli+space+of+connections#FlatConnectionsOverATorus It seems that there are oddball homomorphisms not fitting into this general setup for some compact groups. I would appreciate more examples (in other compact, connected gauge groups) of commuting elements that fail to live in a common maximal torus. Of course, $SO_3(\mathbb{R})$ is not simply connected, but the gauge groups I am working with all are. I don't know enough about discrete abelian subgroups of compact Lie groups. Can this issue be avoided by further assuming that the gauge group is simply connected?","Let $G$ be a compact, connected Lie group. Let $x, y \in G$ be an arbitrary pair of commuting elements. Is there necessarily a torus $T \leq G$ containing $x$ and $y$? Apparently not: Commutativity and Maximal Tori in Connected, Compact Lie Groups The issue arises due to discrete abelian subgroups. Consider the $SO_3(\mathbb{R})$ example from the link. Thinking geometrically, a maximal torus can be described as the circle subgroup of rotations fixing a particular direction in $\mathbb{R}^3$. Consider two orthogonal directions. Then rotation by $\pi$ radians about these two axes will commute, but they certainly do not live in a common maximal torus. My concern comes from trying to compute the following character variety on a torus: $\chi_G(\Sigma_1)=\{\rho: \pi_1(\Sigma_1) \to G \ \vert \rho \ \text{is a group homomorphism}\}/\text{conjugation by} \ G$ where $\Sigma_1 \cong S^1 \times S^1$ is a torus. Since $\pi_1(\Sigma_1)$ is free abelian of rank 2, it follows that such a homomorphism is determined by a choice of commuting elements $x, y \in G$. What should I make of the following argument, particularly the part characterizing a flat connection on $S^1 \times S^1$? [Edit: The link has been updated to reflect this issue] http://ncatlab.org/nlab/show/moduli+space+of+connections#FlatConnectionsOverATorus It seems that there are oddball homomorphisms not fitting into this general setup for some compact groups. I would appreciate more examples (in other compact, connected gauge groups) of commuting elements that fail to live in a common maximal torus. Of course, $SO_3(\mathbb{R})$ is not simply connected, but the gauge groups I am working with all are. I don't know enough about discrete abelian subgroups of compact Lie groups. Can this issue be avoided by further assuming that the gauge group is simply connected?",,"['abstract-algebra', 'differential-geometry', 'algebraic-topology', 'lie-groups', 'topological-groups']"
21,If $\operatorname{Def} X = \frac{1}{2} \mathcal L_X g$ then what is $\operatorname{Def}^* \operatorname{Def} X$?,If  then what is ?,\operatorname{Def} X = \frac{1}{2} \mathcal L_X g \operatorname{Def}^* \operatorname{Def} X,"Let us define a deformation operator $\operatorname{Def}$ on a Riemannian manifold $(M,g)$, acting on divergence-free vector fields as $$   \operatorname{Def} X = \frac{1}{2} \mathcal L_X g, $$ where $\mathcal L_X g$ is a Lie derivative of metric tensor $g$. In coordinates  $$   \left(L_X g\right)_{lk} = g_{kl} {X^l}_{;\,j} + g_{jl} {X^l}_{;\,k}, \qquad {X^l}_{;\,j} = {(\nabla X)^l}_{j}. $$ Above by $\nabla$ is denoted Levi-Civita connection and the whole relation is taken from Taylor's ""Partial Differential Equations I"" (3.33). Then we may took deformation operator's formal adjoint $\operatorname{Def}^*$: $$   \int_M \left< \operatorname{Def} Y, \operatorname{Def} X \right> \, \mu = \int_M \left< Y, \operatorname{Def} ^* \operatorname{Def} X \right> \, \mu. $$ What is an explicit expression for $\operatorname{Def}^* \operatorname{Def} X$ if $\operatorname{div} X = 0$? It should somehow be related to divergence of $\operatorname{Def} X$ or Laplacian plus Ricci tensor of $X$. Even better though is to get $\left(\operatorname{Def}^* \operatorname{Def} X\right)^\flat$ --- a corresponding covector field.","Let us define a deformation operator $\operatorname{Def}$ on a Riemannian manifold $(M,g)$, acting on divergence-free vector fields as $$   \operatorname{Def} X = \frac{1}{2} \mathcal L_X g, $$ where $\mathcal L_X g$ is a Lie derivative of metric tensor $g$. In coordinates  $$   \left(L_X g\right)_{lk} = g_{kl} {X^l}_{;\,j} + g_{jl} {X^l}_{;\,k}, \qquad {X^l}_{;\,j} = {(\nabla X)^l}_{j}. $$ Above by $\nabla$ is denoted Levi-Civita connection and the whole relation is taken from Taylor's ""Partial Differential Equations I"" (3.33). Then we may took deformation operator's formal adjoint $\operatorname{Def}^*$: $$   \int_M \left< \operatorname{Def} Y, \operatorname{Def} X \right> \, \mu = \int_M \left< Y, \operatorname{Def} ^* \operatorname{Def} X \right> \, \mu. $$ What is an explicit expression for $\operatorname{Def}^* \operatorname{Def} X$ if $\operatorname{div} X = 0$? It should somehow be related to divergence of $\operatorname{Def} X$ or Laplacian plus Ricci tensor of $X$. Even better though is to get $\left(\operatorname{Def}^* \operatorname{Def} X\right)^\flat$ --- a corresponding covector field.",,"['differential-geometry', 'riemannian-geometry', 'fluid-dynamics']"
22,How to define derivative in Minkowski space,How to define derivative in Minkowski space,,"My understanding of derivative is like this: it is the unique linear mapping that sends the difference in $x$ to the difference in $f(x)$ when the difference in $x$ is small. To put it more rigorously, $g$ is the derivative of $f$ if and only if for every positive real number $\epsilon$, there exists a positive real number $\delta$ such that $$\lVert f(x+\Delta x)-f(x)-g(x)(\Delta x)\rVert \le \epsilon \lVert\Delta x\rVert$$ for every $\lVert\Delta x\rVert < \delta$. Is it the same in Minkowski space where $\lVert \Delta x \rVert$ may be equal to $0$?","My understanding of derivative is like this: it is the unique linear mapping that sends the difference in $x$ to the difference in $f(x)$ when the difference in $x$ is small. To put it more rigorously, $g$ is the derivative of $f$ if and only if for every positive real number $\epsilon$, there exists a positive real number $\delta$ such that $$\lVert f(x+\Delta x)-f(x)-g(x)(\Delta x)\rVert \le \epsilon \lVert\Delta x\rVert$$ for every $\lVert\Delta x\rVert < \delta$. Is it the same in Minkowski space where $\lVert \Delta x \rVert$ may be equal to $0$?",,"['calculus', 'differential-geometry', 'metric-spaces']"
23,Containment of two varieties with a lot of intersection,Containment of two varieties with a lot of intersection,,"Given a projective variety $X\subset \mathbb P^n$ and a curve $C\subset \mathbb P^n$, when can I conclude that $C\subset X$, from the fact that $C$ and $X$ have 'many' points in common. I.e., is there a number $N$ (depending on numerical invariants of the embeddings of $X$ and $C$) so that if $X\cap C$ contains at least $N$ points, then $C\subset X$?","Given a projective variety $X\subset \mathbb P^n$ and a curve $C\subset \mathbb P^n$, when can I conclude that $C\subset X$, from the fact that $C$ and $X$ have 'many' points in common. I.e., is there a number $N$ (depending on numerical invariants of the embeddings of $X$ and $C$) so that if $X\cap C$ contains at least $N$ points, then $C\subset X$?",,"['algebraic-geometry', 'differential-geometry', 'intersection-theory']"
24,Do Anosov flows exist on two dimensional compact manifolds?,Do Anosov flows exist on two dimensional compact manifolds?,,"Question: Let $M$ denote a two-dimensional, smooth, compact Riemannian manifold (without boundary). If $\phi:\mathbb{R}\times M \rightarrow M$ is a smooth flow, then prove that $\phi$ is not an Anosov flow. Motivation: I am new to the field of dynamical systems. It seems to be implicit in the literature that Anosov flows do not exist on two dimensional manifolds (see http://www.sciencedirect.com/science/article/pii/S0022039605002767 ) but I would like to know the proof. My current understanding: Assume $\phi$ is Anosov and argue for a contradiction. There is a splitting of the tangent space $T_x M = E_x^s \oplus E_x^u \oplus E_x^c$ at each $x\in M$, where $E_x^c$ is the one-dimensional subspace spanned by the direction of the flow, and the stable and unstable spaces have dimension independent of $x$. Thus, without loss of generality, suppose that $E_x^u = \{0\}$ and $\text{dim}(E_x^s)=1$. The differential of the vectors $v$ in $E_x^s$ satisfy $\|d\phi_tv\| \lesssim e^{-\gamma t} \|v\|$ for all $t\geq 0$ and some $\gamma >0$. I guess that I now need to use the fact that $M$ is compact to obtain a contradiction but I don't see how to proceed. Any help would be much appreciated.","Question: Let $M$ denote a two-dimensional, smooth, compact Riemannian manifold (without boundary). If $\phi:\mathbb{R}\times M \rightarrow M$ is a smooth flow, then prove that $\phi$ is not an Anosov flow. Motivation: I am new to the field of dynamical systems. It seems to be implicit in the literature that Anosov flows do not exist on two dimensional manifolds (see http://www.sciencedirect.com/science/article/pii/S0022039605002767 ) but I would like to know the proof. My current understanding: Assume $\phi$ is Anosov and argue for a contradiction. There is a splitting of the tangent space $T_x M = E_x^s \oplus E_x^u \oplus E_x^c$ at each $x\in M$, where $E_x^c$ is the one-dimensional subspace spanned by the direction of the flow, and the stable and unstable spaces have dimension independent of $x$. Thus, without loss of generality, suppose that $E_x^u = \{0\}$ and $\text{dim}(E_x^s)=1$. The differential of the vectors $v$ in $E_x^s$ satisfy $\|d\phi_tv\| \lesssim e^{-\gamma t} \|v\|$ for all $t\geq 0$ and some $\gamma >0$. I guess that I now need to use the fact that $M$ is compact to obtain a contradiction but I don't see how to proceed. Any help would be much appreciated.",,"['differential-geometry', 'dynamical-systems', 'ergodic-theory']"
25,How is partial time derivative $\frac{\partial}{\partial t}$ defined for vector flows?,How is partial time derivative  defined for vector flows?,\frac{\partial}{\partial t},"This question emerged when I was thinking about Liouville's theorem of classical mechanics. As far as I understand, the change of any function along the integral curves of Hamiltonian vector field is $$\frac{d \rho}{d t} = \{\rho, H \} = \mathcal L_X \rho $$ where $\{\,,\}$ is Poisson bracket, $X$ is a vector field associated with $dH$ and $\mathcal L$ is Lie derivative. I have the following understanding of $\frac{d f}{d t}$ --- as we move on the manifold $M$ with a speed $X$, the landscape ($f$ visioned as mountains on $M$) changes. The mountains are still, it us who is moving. On the other hand, in certain formulations of Liouville's theorem $\frac{\partial}{\partial t}$ is used: $$\frac{\partial \rho}{\partial t} = - \{\rho, H \} $$ (The question is about the definition of $\frac{\partial}{\partial t}$, not about the contents of Liouville's theorem). I am familiar with the notion of material derivative and partial time derivative from usual formulations of fluid dynamics, but I am not sure of their counterparts in differential geometry. What is partial time derivative $\frac{\partial}{\partial t}$ and how is it defined invariantly for functions, vector fields and differential forms? How is it related to $X$ and $\mathcal L_X$?","This question emerged when I was thinking about Liouville's theorem of classical mechanics. As far as I understand, the change of any function along the integral curves of Hamiltonian vector field is $$\frac{d \rho}{d t} = \{\rho, H \} = \mathcal L_X \rho $$ where $\{\,,\}$ is Poisson bracket, $X$ is a vector field associated with $dH$ and $\mathcal L$ is Lie derivative. I have the following understanding of $\frac{d f}{d t}$ --- as we move on the manifold $M$ with a speed $X$, the landscape ($f$ visioned as mountains on $M$) changes. The mountains are still, it us who is moving. On the other hand, in certain formulations of Liouville's theorem $\frac{\partial}{\partial t}$ is used: $$\frac{\partial \rho}{\partial t} = - \{\rho, H \} $$ (The question is about the definition of $\frac{\partial}{\partial t}$, not about the contents of Liouville's theorem). I am familiar with the notion of material derivative and partial time derivative from usual formulations of fluid dynamics, but I am not sure of their counterparts in differential geometry. What is partial time derivative $\frac{\partial}{\partial t}$ and how is it defined invariantly for functions, vector fields and differential forms? How is it related to $X$ and $\mathcal L_X$?",,"['differential-geometry', 'classical-mechanics', 'lie-derivative']"
26,Orientation double cover,Orientation double cover,,"Let $M$ be a manifold and let $\bigwedge^\text{top}TM$ be the top exterior product of the tangent bundle. Then this becomes a line bundle. Let $g$ be any metric on $\bigwedge^\text{top}TM$ and define $\hat{M}:=\{x\in\bigwedge^\text{top}TM:g(x,x)=1\}$, thus $\hat{M}$ is the space of all unit vectors. My question is: Why is $\pi:\hat{M}\rightarrow M$ a smooth double cover which is independent of the choice of the metric and why $\hat{M}$ has a natural orientation. I thought that the independence is implied by the fact that we have a line bundle. Moreover: $M$ is orientable iff $\hat{M}=M\coprod M$. Can someone help me, with this because i have no idea, how to start. Thanks.","Let $M$ be a manifold and let $\bigwedge^\text{top}TM$ be the top exterior product of the tangent bundle. Then this becomes a line bundle. Let $g$ be any metric on $\bigwedge^\text{top}TM$ and define $\hat{M}:=\{x\in\bigwedge^\text{top}TM:g(x,x)=1\}$, thus $\hat{M}$ is the space of all unit vectors. My question is: Why is $\pi:\hat{M}\rightarrow M$ a smooth double cover which is independent of the choice of the metric and why $\hat{M}$ has a natural orientation. I thought that the independence is implied by the fact that we have a line bundle. Moreover: $M$ is orientable iff $\hat{M}=M\coprod M$. Can someone help me, with this because i have no idea, how to start. Thanks.",,['differential-geometry']
27,DeRham Cohomology of the Circle and the Torus,DeRham Cohomology of the Circle and the Torus,,"I want to compute the first DeRham cohomology group of the circle. In symbols $H^1_{dR}(S^1)$. Let $p(x)=e^{ix}$ the map from $\mathbb{R}$ to the circle $S^1$, $\Omega^1(S^1)$ the set of all $1$-forms on $S^1$. Then for $\alpha\in\Omega^1(S^1)$ we have $p^*\alpha=f(t)dt$ for some function $f$. I want to deduce an isomorphism $\psi:H^1_{dR}(M)\rightarrow\mathbb{R}$. Therefore define $\tilde{\psi}:\Omega^1(S^1)\rightarrow\mathbb{R}$ by $\tilde{\psi}(\alpha)=\int_0^{2\pi}f(t)$. The claim is that this determines an isomorphism $\psi:H^1_{dR}(S^1)\rightarrow\mathbb{R}$. I already show that this map $\psi$ is well-defined and $\mathbb{R}$ linear. I now have to show that if $\psi(\alpha)=0$ that this implies that $\alpha$ is exact. I think you have to consider the function $p^*g(t)=\int_0^tf(\tau)d\tau$ but I don't see how. Can someone say me how to get the conclusion I want? Does all this imply directly that we have the isomorphism? Can i also do the same argument for $H^1_{dR}(S^1\times S^1)$?","I want to compute the first DeRham cohomology group of the circle. In symbols $H^1_{dR}(S^1)$. Let $p(x)=e^{ix}$ the map from $\mathbb{R}$ to the circle $S^1$, $\Omega^1(S^1)$ the set of all $1$-forms on $S^1$. Then for $\alpha\in\Omega^1(S^1)$ we have $p^*\alpha=f(t)dt$ for some function $f$. I want to deduce an isomorphism $\psi:H^1_{dR}(M)\rightarrow\mathbb{R}$. Therefore define $\tilde{\psi}:\Omega^1(S^1)\rightarrow\mathbb{R}$ by $\tilde{\psi}(\alpha)=\int_0^{2\pi}f(t)$. The claim is that this determines an isomorphism $\psi:H^1_{dR}(S^1)\rightarrow\mathbb{R}$. I already show that this map $\psi$ is well-defined and $\mathbb{R}$ linear. I now have to show that if $\psi(\alpha)=0$ that this implies that $\alpha$ is exact. I think you have to consider the function $p^*g(t)=\int_0^tf(\tau)d\tau$ but I don't see how. Can someone say me how to get the conclusion I want? Does all this imply directly that we have the isomorphism? Can i also do the same argument for $H^1_{dR}(S^1\times S^1)$?",,"['differential-geometry', 'differential-topology']"
28,Normal bundle of the two-dimensional sphere manifold embedded in $\mathbb R^4$,Normal bundle of the two-dimensional sphere manifold embedded in,\mathbb R^4,"Let $M \subset \mathbb R^4$ be a smooth manifold diffeomorphic to $S^2$. How can one prove that normal bundle of $M$ has at least one non-vanishing global section. I think that $M$ should be parametrizable and hence has one non-vanishing global section, however I can not prove rigorously that $M$ is parametrizable.","Let $M \subset \mathbb R^4$ be a smooth manifold diffeomorphic to $S^2$. How can one prove that normal bundle of $M$ has at least one non-vanishing global section. I think that $M$ should be parametrizable and hence has one non-vanishing global section, however I can not prove rigorously that $M$ is parametrizable.",,"['differential-geometry', 'differential-topology']"
29,Existence of positive solutions of a linear PDE on closed manifolds,Existence of positive solutions of a linear PDE on closed manifolds,,"I was wondering is there a sufficient condition (or sufficient and necessary condition) for the existence of positive solutions of the following linear PDE on a closed manifold $(M, g)$, \begin{equation*} \Delta u +\nabla u\nabla f +hu=0. \end{equation*} where $f, h\in C^{\infty}(M)$. I got some necessary conditions using the Stokes formula, but I couldn't find a statement for sufficient condition, or sufficient and necessary condition. Thank you very much for any suggestions.","I was wondering is there a sufficient condition (or sufficient and necessary condition) for the existence of positive solutions of the following linear PDE on a closed manifold $(M, g)$, \begin{equation*} \Delta u +\nabla u\nabla f +hu=0. \end{equation*} where $f, h\in C^{\infty}(M)$. I got some necessary conditions using the Stokes formula, but I couldn't find a statement for sufficient condition, or sufficient and necessary condition. Thank you very much for any suggestions.",,"['differential-geometry', 'partial-differential-equations']"
30,Homework: calculation about differential form,Homework: calculation about differential form,,"Here is the question: Let $\omega = A dy\wedge dz + B dz \wedge dx + C dx \wedge dy$ in $\mathbf{R}^3$, and $d\omega = 0$. Denote \begin{eqnarray} \alpha = \int_0^1 tA(tx,ty,tz)dt\cdot(ydz-zdy)\\ +\int_0^1 tB(tx,ty,tz)dt\cdot(zdx-xdz)\\ +\int_0^1 tC(tx,ty,tz)dt\cdot(xdy-ydx). \end{eqnarray} Then show that $d\alpha = \omega$. Can you teach me how to do this calculation?","Here is the question: Let $\omega = A dy\wedge dz + B dz \wedge dx + C dx \wedge dy$ in $\mathbf{R}^3$, and $d\omega = 0$. Denote \begin{eqnarray} \alpha = \int_0^1 tA(tx,ty,tz)dt\cdot(ydz-zdy)\\ +\int_0^1 tB(tx,ty,tz)dt\cdot(zdx-xdz)\\ +\int_0^1 tC(tx,ty,tz)dt\cdot(xdy-ydx). \end{eqnarray} Then show that $d\alpha = \omega$. Can you teach me how to do this calculation?",,"['differential-geometry', 'smooth-manifolds', 'differential-forms']"
31,Differentiable parameterization of a curve $\Gamma$,Differentiable parameterization of a curve,\Gamma,"If $\alpha:I\longrightarrow \Gamma$ and $\beta:J\longrightarrow \Gamma$ are two bijective differentiable regular parameterizations of the curve  $\Gamma\subset\mathbb{R}^2$ (not necessarily of class $C^1$) where $I,J\subset\mathbb{R}$(compact intervals). Can we say that $\theta=\beta^{-1}\circ \alpha$ is differentiable ? Note that $\alpha ,\beta,\theta$ are homeomorphisms. Any hints would be appreciated.","If $\alpha:I\longrightarrow \Gamma$ and $\beta:J\longrightarrow \Gamma$ are two bijective differentiable regular parameterizations of the curve  $\Gamma\subset\mathbb{R}^2$ (not necessarily of class $C^1$) where $I,J\subset\mathbb{R}$(compact intervals). Can we say that $\theta=\beta^{-1}\circ \alpha$ is differentiable ? Note that $\alpha ,\beta,\theta$ are homeomorphisms. Any hints would be appreciated.",,"['real-analysis', 'differential-geometry']"
32,$X\in \mathfrak{g}$ means flow commutes with left-translation,means flow commutes with left-translation,X\in \mathfrak{g},"Suppose $X\in \mathfrak{g}$ is a left invariant vector field on a Lie group G. In this article it mentions that The fact that our vector fields satisfy $L^*_gX = X$ implies that the   flow commutes with left-translation: $\Phi_t\circ L_g = L_g \circ  \Phi_t$. It makes intuitive sense to me why this would be true, but I can't seem to formulate it.  Let $h\in G$, then we want to show $\Phi_X^t(L_g(h)) = g\cdot \Phi^t_X(h)$. We have $X_{h} = X_{L_{g^{-1}}(gh)}= L_{g^{-1}}^*(X_{gh})$, but now I am getting lost again...","Suppose $X\in \mathfrak{g}$ is a left invariant vector field on a Lie group G. In this article it mentions that The fact that our vector fields satisfy $L^*_gX = X$ implies that the   flow commutes with left-translation: $\Phi_t\circ L_g = L_g \circ  \Phi_t$. It makes intuitive sense to me why this would be true, but I can't seem to formulate it.  Let $h\in G$, then we want to show $\Phi_X^t(L_g(h)) = g\cdot \Phi^t_X(h)$. We have $X_{h} = X_{L_{g^{-1}}(gh)}= L_{g^{-1}}^*(X_{gh})$, but now I am getting lost again...",,"['differential-geometry', 'lie-groups', 'vector-fields']"
33,Does local flow of left-invariant vector field commute with the left-translation operator?,Does local flow of left-invariant vector field commute with the left-translation operator?,,"Let $G$ be a Lie group and $X$ a left-invariant vector field over $G$ (i.e. $\forall g,p\in G: (D_p l_g)(X_p) = X_{gp}$ whereby $l_g$ is the map $G\rightarrow G:p\mapsto gp$). Let $\phi_t$ be the local flow of $X$. Is it true, that $\phi_t$ and $l_g$ commute for every $t\in \mathbb R$ and $g\in G$? (Reason for my question: I want to solve an exercise. I have found a solution if the above statement is true. So I wonder whether from $\forall g\in G: (D_p l_g)(X_p) = X_{gp}$ follows $l_g\circ\phi_t=\phi_t\circ l_g$).","Let $G$ be a Lie group and $X$ a left-invariant vector field over $G$ (i.e. $\forall g,p\in G: (D_p l_g)(X_p) = X_{gp}$ whereby $l_g$ is the map $G\rightarrow G:p\mapsto gp$). Let $\phi_t$ be the local flow of $X$. Is it true, that $\phi_t$ and $l_g$ commute for every $t\in \mathbb R$ and $g\in G$? (Reason for my question: I want to solve an exercise. I have found a solution if the above statement is true. So I wonder whether from $\forall g\in G: (D_p l_g)(X_p) = X_{gp}$ follows $l_g\circ\phi_t=\phi_t\circ l_g$).",,"['differential-geometry', 'lie-groups', 'vector-fields']"
34,Is invariance of a multi-linear form required for co/contra variance?,Is invariance of a multi-linear form required for co/contra variance?,,"I'm reading the book: The Absolute Differential Calculus by Levi-Civita to get an idea of the history behind the development of tensor calculus. On page 71 he states: An m-fold covariant is an m-fold system which is transformed in the same way as the coefficients of a multilinear form in point variables; an m-fold contravariant is one which is transformed in the same way as the coefficients of a multilinear form in dual variables. The few pages beforehand has required the multilinear form to be invariant, so has this been incorrectly left out in the above definition?","I'm reading the book: The Absolute Differential Calculus by Levi-Civita to get an idea of the history behind the development of tensor calculus. On page 71 he states: An m-fold covariant is an m-fold system which is transformed in the same way as the coefficients of a multilinear form in point variables; an m-fold contravariant is one which is transformed in the same way as the coefficients of a multilinear form in dual variables. The few pages beforehand has required the multilinear form to be invariant, so has this been incorrectly left out in the above definition?",,"['differential-geometry', 'tensors', 'multilinear-algebra']"
35,Morphism of vector bundles,Morphism of vector bundles,,"I have some doubts on the definition of morphism of vector bundles. My notes say: "" Let $ E, E'$ be vector bundles over the same B. A morphism of vector bundles is a morphism of bundles $f:E\rightarrow E'$ such that for each $p\in B$ the map $f|_{E_p}:E_{p}\rightarrow E'_{f(p)}$ is linear. "" My first doubt is the following: if $f$ is restricted to $E_p$, then it's clear that $f|_{E_p}$ has domain $E_p$, but I think that it has target in $E'_p$, and not in $E'_{f(p)}$. I've thought this because, if $\pi$ and $\pi'$ are the projections on $B$ of $E$ and $E'$ respectively, then, since $f$ is a morphism of bundles, $f={\pi'}^{-1}\circ \pi$. So $f(E_p)=f(\pi^{-1}(p))={\pi'}^{-1}(\pi(\pi^{-1}(p)))={\pi'}^{-1}(p)=E'_p$. Am I wrong? Can someone give me some clarifications?","I have some doubts on the definition of morphism of vector bundles. My notes say: "" Let $ E, E'$ be vector bundles over the same B. A morphism of vector bundles is a morphism of bundles $f:E\rightarrow E'$ such that for each $p\in B$ the map $f|_{E_p}:E_{p}\rightarrow E'_{f(p)}$ is linear. "" My first doubt is the following: if $f$ is restricted to $E_p$, then it's clear that $f|_{E_p}$ has domain $E_p$, but I think that it has target in $E'_p$, and not in $E'_{f(p)}$. I've thought this because, if $\pi$ and $\pi'$ are the projections on $B$ of $E$ and $E'$ respectively, then, since $f$ is a morphism of bundles, $f={\pi'}^{-1}\circ \pi$. So $f(E_p)=f(\pi^{-1}(p))={\pi'}^{-1}(\pi(\pi^{-1}(p)))={\pi'}^{-1}(p)=E'_p$. Am I wrong? Can someone give me some clarifications?",,"['algebraic-geometry', 'differential-geometry']"
36,The degree of every smooth map $\mathbb{R}^n \to \mathbb{R}^n$ is one...,The degree of every smooth map  is one...,\mathbb{R}^n \to \mathbb{R}^n,"Let $\varphi : M^n \to N^n$ be a proper smooth map between two connected smooth manifolds. Then $\varphi$ induces a linear map $\varphi^* : H_c^n(N) \simeq \mathbb{R} \to H_c^n(M) \simeq \mathbb{R}$ (where $H_c^n$ is the $n$-th de Rham cohomology group with compact support), so there exists $d \in \mathbb{R}$ so that $\varphi^* : x \mapsto d \cdot x$; $d$ is called the degree of $\varphi$. Moreover, it can be shown that if $\varphi : M \to N$ and $\phi : M\to N$ are smoothly homotopic, then $\varphi^*= \phi^*$ hence $\deg(\varphi)= \deg(\phi)$. However, every proper smooth map $f : \mathbb{R}^n \to \mathbb{R}^n$ and $g : \mathbb{R}^n \to \mathbb{R}^n$ are smoothly homotopic (thanks to $H(t,x)= tf(x)+(1-t)g(x)$), so every proper smooth map $\mathbb{R}^n \to \mathbb{R}^n$ should have the same degree... It is probably a silly question, but where does my argument fail? Edit: Sean Eberhard found a first problem; in fact, to conclude that $\varphi^*=\phi^*$ it is needed the homotopy be proper at any $t$. However, here is another contradiction: If $P(z)=z^n$ (where $\mathbb{R}^2$ and $\mathbb{C}$ are not distinguised), $\deg(P)=n$ and $P$ is proper if $n \geq 1$. Taking $H(t,z)=tz^m+(1-t)z^n$ with $m>n>1$, $m= \deg(H(1, \cdot))= \deg(H(0,\cdot))=n$, a contradiction.","Let $\varphi : M^n \to N^n$ be a proper smooth map between two connected smooth manifolds. Then $\varphi$ induces a linear map $\varphi^* : H_c^n(N) \simeq \mathbb{R} \to H_c^n(M) \simeq \mathbb{R}$ (where $H_c^n$ is the $n$-th de Rham cohomology group with compact support), so there exists $d \in \mathbb{R}$ so that $\varphi^* : x \mapsto d \cdot x$; $d$ is called the degree of $\varphi$. Moreover, it can be shown that if $\varphi : M \to N$ and $\phi : M\to N$ are smoothly homotopic, then $\varphi^*= \phi^*$ hence $\deg(\varphi)= \deg(\phi)$. However, every proper smooth map $f : \mathbb{R}^n \to \mathbb{R}^n$ and $g : \mathbb{R}^n \to \mathbb{R}^n$ are smoothly homotopic (thanks to $H(t,x)= tf(x)+(1-t)g(x)$), so every proper smooth map $\mathbb{R}^n \to \mathbb{R}^n$ should have the same degree... It is probably a silly question, but where does my argument fail? Edit: Sean Eberhard found a first problem; in fact, to conclude that $\varphi^*=\phi^*$ it is needed the homotopy be proper at any $t$. However, here is another contradiction: If $P(z)=z^n$ (where $\mathbb{R}^2$ and $\mathbb{C}$ are not distinguised), $\deg(P)=n$ and $P$ is proper if $n \geq 1$. Taking $H(t,z)=tz^m+(1-t)z^n$ with $m>n>1$, $m= \deg(H(1, \cdot))= \deg(H(0,\cdot))=n$, a contradiction.",,"['differential-geometry', 'differential-topology', 'homology-cohomology']"
37,Algebraic Tangent Space and Vector - an intuitive understanding?,Algebraic Tangent Space and Vector - an intuitive understanding?,,"In this question I am looking for help in understanding the Algebraic Tangent vector and what the difference is between it and the ""regular"" Tangent vector. A ""differentiable function"" near p is a pair $(f,U)$ where $U \subset$ is open with $p \in U$, and $f:U \rightarrow \mathbb{R}$ is a differentiable function.   We say that two such pairs $(f,U), (g,W)$ are ""equivalent"" - the notation is $(f,U) \sim (g,W)$, if and only if there exists an open $V \subset U \cap W$ with $p \in V$ such that $f|_{V}$ and $g|_{V}$.  So f and g coincide in a small neighborhood of p. $\sim$ is an equivalence relation. Note:  I am thinking that this is an equivalence relation (intuitively) because we are just looking at some neighborhood of point p - that which is  $V \subset U \cap W$ and so other mapping $h: Y \rightarrow \mathbb{R}$ where $Z \subset U \cap W$ is also in the equivalence relation defined by: $$E_{p} \equiv \left\{(f,U)|(f,U) \text{ is a differential function near } p \right\}/\sim$$ Now the equivalence class of a pair $(f,U)$ is denoted by $[f,U]$ and is called the ""germ"" of a differentiable function at  $p$. Note: From my understanding $[f,U]$, the germ, is showing that locally these functions $\in [f,U]$ are differentiable.  I am guessing that is just scratching the surface - but furthermore other than knowing something has to be differentiable to find the tangent (at least in the sense of being $C^{1}$). While I am trying to wrap my head around the above, it seems to get worse.. An element $v$ of the dual space of $E_{p}, v \in Hom(E_{p},\mathbb{R}) =\left\{v:E_{p} \rightarrow \mathbb{R} | \text{ is linear} \right\}$ is called a ""derivation"" if $\forall [f,U],[g,W] \in E_{p}$ the product rule $$v([f,U] \cdot [g,W]) = v([f,U]) \cdot [g,W](p) + [f,U] \cdot v([g,W])$$ holds Defn:  The Algebraic Tangent space of a space X at p is: $T_{p}^{alg}X  \equiv \left\{v \in E_{p}^{*} | v \text{ a derivation}\right\}$ An element of $T_{p}^{alg}X$ is called an algebraic tangent vector of X at p. So what is the point of the Algebraic Tangent vector and space? Thanks much for any thoughts, Brian","In this question I am looking for help in understanding the Algebraic Tangent vector and what the difference is between it and the ""regular"" Tangent vector. A ""differentiable function"" near p is a pair $(f,U)$ where $U \subset$ is open with $p \in U$, and $f:U \rightarrow \mathbb{R}$ is a differentiable function.   We say that two such pairs $(f,U), (g,W)$ are ""equivalent"" - the notation is $(f,U) \sim (g,W)$, if and only if there exists an open $V \subset U \cap W$ with $p \in V$ such that $f|_{V}$ and $g|_{V}$.  So f and g coincide in a small neighborhood of p. $\sim$ is an equivalence relation. Note:  I am thinking that this is an equivalence relation (intuitively) because we are just looking at some neighborhood of point p - that which is  $V \subset U \cap W$ and so other mapping $h: Y \rightarrow \mathbb{R}$ where $Z \subset U \cap W$ is also in the equivalence relation defined by: $$E_{p} \equiv \left\{(f,U)|(f,U) \text{ is a differential function near } p \right\}/\sim$$ Now the equivalence class of a pair $(f,U)$ is denoted by $[f,U]$ and is called the ""germ"" of a differentiable function at  $p$. Note: From my understanding $[f,U]$, the germ, is showing that locally these functions $\in [f,U]$ are differentiable.  I am guessing that is just scratching the surface - but furthermore other than knowing something has to be differentiable to find the tangent (at least in the sense of being $C^{1}$). While I am trying to wrap my head around the above, it seems to get worse.. An element $v$ of the dual space of $E_{p}, v \in Hom(E_{p},\mathbb{R}) =\left\{v:E_{p} \rightarrow \mathbb{R} | \text{ is linear} \right\}$ is called a ""derivation"" if $\forall [f,U],[g,W] \in E_{p}$ the product rule $$v([f,U] \cdot [g,W]) = v([f,U]) \cdot [g,W](p) + [f,U] \cdot v([g,W])$$ holds Defn:  The Algebraic Tangent space of a space X at p is: $T_{p}^{alg}X  \equiv \left\{v \in E_{p}^{*} | v \text{ a derivation}\right\}$ An element of $T_{p}^{alg}X$ is called an algebraic tangent vector of X at p. So what is the point of the Algebraic Tangent vector and space? Thanks much for any thoughts, Brian",,"['differential-geometry', 'manifolds']"
38,curves and surfaces. curvature of a regular curve,curves and surfaces. curvature of a regular curve,,"Let $\gamma(t)$ be a regular curve lies on a sphere $S^2$ with center $(0, 0, 0)$ (origin) and radius $r$. Show that the curvature of $\gamma$ is non-zero, i.e., $ฮบ \ne 0$. Furthermore, if the torsion of the curve $\tau \ne 0$ we have: $\gamma(t)= - p \overrightarrow{n} - p'\alpha \overrightarrow{b} $ where: $p=1/k, \\ \alpha=1/\tau, \\ r^2=p^2+(p'\alpha)^2  $","Let $\gamma(t)$ be a regular curve lies on a sphere $S^2$ with center $(0, 0, 0)$ (origin) and radius $r$. Show that the curvature of $\gamma$ is non-zero, i.e., $ฮบ \ne 0$. Furthermore, if the torsion of the curve $\tau \ne 0$ we have: $\gamma(t)= - p \overrightarrow{n} - p'\alpha \overrightarrow{b} $ where: $p=1/k, \\ \alpha=1/\tau, \\ r^2=p^2+(p'\alpha)^2  $",,"['differential-geometry', 'algebraic-curves']"
39,"How are the isometries $h:(\mathbb{R}^n,||\cdot||_p)\longrightarrow(\mathbb{R}^n,||\cdot||_p)\;$?",How are the isometries ?,"h:(\mathbb{R}^n,||\cdot||_p)\longrightarrow(\mathbb{R}^n,||\cdot||_p)\;","An isometry of $\mathbb{R}^n$ is a function $h:\mathbb{R}^n\longrightarrow\mathbb{R}^n$ that preserves the distance between vectors: $$||h(x)-h(y)||_p=||x-y||_p\;\;, \;\;p\ge1$$ for all $x$ and $y$ in $\mathbb{R}^n$, where $||(x_1,x_2,\cdots,x_n)||_p=(x_1^p+x_2^p+\cdots+x_n^p)^{\frac{1}{p}}$ If $p=2$ $\Longrightarrow$ $h(x)=A\cdot x+a$ , where $A$ is a ortogonal matrix If $p\neq2$ how are the isometries $h$ ? Any hints would be appreciated.","An isometry of $\mathbb{R}^n$ is a function $h:\mathbb{R}^n\longrightarrow\mathbb{R}^n$ that preserves the distance between vectors: $$||h(x)-h(y)||_p=||x-y||_p\;\;, \;\;p\ge1$$ for all $x$ and $y$ in $\mathbb{R}^n$, where $||(x_1,x_2,\cdots,x_n)||_p=(x_1^p+x_2^p+\cdots+x_n^p)^{\frac{1}{p}}$ If $p=2$ $\Longrightarrow$ $h(x)=A\cdot x+a$ , where $A$ is a ortogonal matrix If $p\neq2$ how are the isometries $h$ ? Any hints would be appreciated.",,"['real-analysis', 'differential-geometry']"
40,Vector fields as section of tangent bundle,Vector fields as section of tangent bundle,,"We can define vector fields on manifolds in two ways. The way I first saw was that a vector field was a linear map $C^\infty(M) \to C^\infty(M)$ satisfying the Leibniz rule (aka product rule). We can also define a vector field to be a smooth section of $TM \to M$. I get that given a section $s$ of $TM \to M$, we can define $\hat{s}(f)(p) = s(p)(f)$, but I don't understand why $\hat{}:Sec(TM \to M) \to Vect(M)$ is an isomorphism. It's clearly linear, but I don't see why it's injective or surjective. $s \in Sec(TM \to M)$ has a left inverse (the inverse being the projection). Maybe that is used to show $\hat{} :Sec(TM \to M) \to Vect(M)$ is injective?","We can define vector fields on manifolds in two ways. The way I first saw was that a vector field was a linear map $C^\infty(M) \to C^\infty(M)$ satisfying the Leibniz rule (aka product rule). We can also define a vector field to be a smooth section of $TM \to M$. I get that given a section $s$ of $TM \to M$, we can define $\hat{s}(f)(p) = s(p)(f)$, but I don't understand why $\hat{}:Sec(TM \to M) \to Vect(M)$ is an isomorphism. It's clearly linear, but I don't see why it's injective or surjective. $s \in Sec(TM \to M)$ has a left inverse (the inverse being the projection). Maybe that is used to show $\hat{} :Sec(TM \to M) \to Vect(M)$ is injective?",,"['differential-geometry', 'vector-fields']"
41,Zeros of the second fundamental form,Zeros of the second fundamental form,,Let $ f:M \rightarrow N $ be a minimal immersion (of arbitrary codimension or an hypersurface if it is necessary) and let $ |A| $ be the norm of its second fundametal form.If $ A $ is not identically zero is it true that the zeros of $ |A| $ are isolated? If $ f:M \rightarrow R^3 $ is a minimal immersion then the conjecture above is true. Briefly this case follows from the fact that $ f $ can be locally represented as a conformal minimal immersion $ X: \Omega \subset R^2 \rightarrow R^3 $ and for conformal minimal immersions the statement above is true (see Osserman 'Minimal surfaces') Thanks,Let $ f:M \rightarrow N $ be a minimal immersion (of arbitrary codimension or an hypersurface if it is necessary) and let $ |A| $ be the norm of its second fundametal form.If $ A $ is not identically zero is it true that the zeros of $ |A| $ are isolated? If $ f:M \rightarrow R^3 $ is a minimal immersion then the conjecture above is true. Briefly this case follows from the fact that $ f $ can be locally represented as a conformal minimal immersion $ X: \Omega \subset R^2 \rightarrow R^3 $ and for conformal minimal immersions the statement above is true (see Osserman 'Minimal surfaces') Thanks,,['differential-geometry']
42,Topology of manifolds,Topology of manifolds,,"Where can I find a stricter presentation of topology of manifolds, then in section 0.4 in Griffiths-Harris? For example, they define the map $H_k \times H_{n-k}$ by presenting a cycle by a submanifold and intersection form. But it's known, that not all homology classes are representable by a submanifold. It can contain singularities in codimension 2. Griffiths and Harris wtite nothing about it. But in the classical topology all is defined strictly, but it's very difficult to calculate anything, using their definition (for example, in Hatcher, Algebraic topology). Then I want to justify the approach of Griffiths-Harris. Also they use, that we can compute homology groups buy smooth simplexes or using triangulations and simplexes, contained in triangulation. The fact of existence of triangulation, as far as I know, is quite difficult, and I don't want to use it without proof. By the wyay, Griffiths and Harris writes, that their book is quite self-contained.","Where can I find a stricter presentation of topology of manifolds, then in section 0.4 in Griffiths-Harris? For example, they define the map $H_k \times H_{n-k}$ by presenting a cycle by a submanifold and intersection form. But it's known, that not all homology classes are representable by a submanifold. It can contain singularities in codimension 2. Griffiths and Harris wtite nothing about it. But in the classical topology all is defined strictly, but it's very difficult to calculate anything, using their definition (for example, in Hatcher, Algebraic topology). Then I want to justify the approach of Griffiths-Harris. Also they use, that we can compute homology groups buy smooth simplexes or using triangulations and simplexes, contained in triangulation. The fact of existence of triangulation, as far as I know, is quite difficult, and I don't want to use it without proof. By the wyay, Griffiths and Harris writes, that their book is quite self-contained.",,"['algebraic-geometry', 'differential-geometry', 'algebraic-topology', 'manifolds']"
43,Regular compact domains of a Riemannian manifolds,Regular compact domains of a Riemannian manifolds,,"In a Riemannian manifold $ M $ a regular compact domain $ D $ is a compact subset of $ M $ with non empty interior and such that for every $ p \in \partial D $ there exists $ \left(U,\varphi\right) $ coordinate neighbourhood of $ p $ such that $ \varphi(U\cap \partial D)\subseteq \partial \mathbb{R}^{n}_{+} $ and $ \varphi (U\cap D)\subseteq \mathbb{R}_{+}^{n} $ where $ \mathbb{R}^{n}_{+} $ is the set such that $ x_n\geq 0 $. Now let $ R<R' $. My question:  is there a regular compact domain $ D $ such that $ B_R \subset D $ and $ \partial D \subset B_{R'} $ (where $ B_R $ is the metric ball of $ M$ )? I'm thinking that if we can construct a smooth function $ \varphi $ such that $ \varphi = 1 $ on $ B_R $, $ supp(\varphi)\subset B_{R'} $ and $ |\nabla \varphi |\neq 0 $ on $ interior( supp(\varphi)) \diagdown \overline{B_R} $ then the subset $\Omega=\{p \in M:\varphi(p)\geq \frac{1}{2}\} $ is the regular compact domain looked for. We have to assume completeness of $ M $ in my argument. But if we replace the metric balls with two compact subsets $ K \subset interior K' $ it should work without completeness.","In a Riemannian manifold $ M $ a regular compact domain $ D $ is a compact subset of $ M $ with non empty interior and such that for every $ p \in \partial D $ there exists $ \left(U,\varphi\right) $ coordinate neighbourhood of $ p $ such that $ \varphi(U\cap \partial D)\subseteq \partial \mathbb{R}^{n}_{+} $ and $ \varphi (U\cap D)\subseteq \mathbb{R}_{+}^{n} $ where $ \mathbb{R}^{n}_{+} $ is the set such that $ x_n\geq 0 $. Now let $ R<R' $. My question:  is there a regular compact domain $ D $ such that $ B_R \subset D $ and $ \partial D \subset B_{R'} $ (where $ B_R $ is the metric ball of $ M$ )? I'm thinking that if we can construct a smooth function $ \varphi $ such that $ \varphi = 1 $ on $ B_R $, $ supp(\varphi)\subset B_{R'} $ and $ |\nabla \varphi |\neq 0 $ on $ interior( supp(\varphi)) \diagdown \overline{B_R} $ then the subset $\Omega=\{p \in M:\varphi(p)\geq \frac{1}{2}\} $ is the regular compact domain looked for. We have to assume completeness of $ M $ in my argument. But if we replace the metric balls with two compact subsets $ K \subset interior K' $ it should work without completeness.",,['differential-geometry']
44,Periodic parametric curve on cylinder [duplicate],Periodic parametric curve on cylinder [duplicate],,"This question already has answers here : Parametric curve on cylinder surface (2 answers) Closed 9 years ago . Given a cylinder surface $S=\{(x,y,z):x^2+2y^2=C\}$. Let $\gamma(t)=(x(t),y(t),z(t))$ satisfy $\gamma'(t)=(2y(t)(z(t)-1),-x(t)(z(t)-1),x(t)y(t))$. Could we guarante that $\gamma$ always on $S$ and periodic if $\gamma(0)$ on $S$?","This question already has answers here : Parametric curve on cylinder surface (2 answers) Closed 9 years ago . Given a cylinder surface $S=\{(x,y,z):x^2+2y^2=C\}$. Let $\gamma(t)=(x(t),y(t),z(t))$ satisfy $\gamma'(t)=(2y(t)(z(t)-1),-x(t)(z(t)-1),x(t)y(t))$. Could we guarante that $\gamma$ always on $S$ and periodic if $\gamma(0)$ on $S$?",,['differential-geometry']
45,Doubly Ruled Surfaces,Doubly Ruled Surfaces,,"I get stuck in the following exercice: Let $S$ be a doubly ruled surfaces by orthogonal lines. Use the Gauss equations to prove that $K\equiv 0$; Conclude that $S$ is a plane. I tried to use a parametrization $X(u,v)=\alpha(u)+v\beta(u)$, usinge the fact that the lines are orthogonal  and calculate the Christoffel symbols, but it did't worked.","I get stuck in the following exercice: Let $S$ be a doubly ruled surfaces by orthogonal lines. Use the Gauss equations to prove that $K\equiv 0$; Conclude that $S$ is a plane. I tried to use a parametrization $X(u,v)=\alpha(u)+v\beta(u)$, usinge the fact that the lines are orthogonal  and calculate the Christoffel symbols, but it did't worked.",,['differential-geometry']
46,Chern number require G-bundles?,Chern number require G-bundles?,,"Do we need a principle G-bundle to define a Chern number or is it enough to have a vector bundle? (and in the case this needs to have complex fibers?) That is, is it mandatory to consider the action on the fibers of a Lie group? I'm asking this because the wikipedia pages on Chern numbers do not mention the group action, but in the book in which I've found the proof that Chern number do not depend on the connection, the framework are principle G- bundles. Hope somebody can answer my question!","Do we need a principle G-bundle to define a Chern number or is it enough to have a vector bundle? (and in the case this needs to have complex fibers?) That is, is it mandatory to consider the action on the fibers of a Lie group? I'm asking this because the wikipedia pages on Chern numbers do not mention the group action, but in the book in which I've found the proof that Chern number do not depend on the connection, the framework are principle G- bundles. Hope somebody can answer my question!",,['differential-geometry']
47,Derivative of vector field,Derivative of vector field,,"I have just started learning differential geometry and I am trying to answer the following exercise (it's not for homework): Let $M$ be an $n$-dimensional manifold with chart $(U, \varphi_U)$ and let $(x^1, \dots, x^n)$ be the corresponding local coordinates. Define a vector field on $U$ by $X = X^i\frac{\partial}{\partial x^i}$. We will try to define a ""derivative of vector fields"" by $$d_Y X = d_Y X^i \frac{\partial}{\partial x^i}$$   By changing coordinates to $(\bar{x}^1, \dots, \bar{x}^n)$, show that unlike $df$, this operator is not independent of the choice of local coordinates. I think I understand the method, but I am not certain of the details. I believe what I have to do is first apply $d_Y$ to $X$ and then change coordinates, and then change coordinates and apply $d_Y$ to $X$, and show that the two results are not then same. Here is what I have attempted: Let $Y = Y^i\frac{\partial}{\partial x^i}$ (the question doesn't define $Y$ but presumably it's just another vector field). Then $$d_Y X = d_Y X^i \frac{\partial}{\partial x^i}$$ $$= dX^i(Y) \frac{\partial}{\partial x^i}$$ $$= \frac{\partial X^i}{\partial x^j}Y^j \frac{\partial}{\partial x^i}$$ This is just another vector field, so transforming it according to the contravariant transformation we get the following messy expression: $$\frac{\partial \bar{x}^i}{\partial x^j} \frac{\partial X^j}{\partial x^k} Y^k \frac{\partial}{\partial \bar{x}^i},$$ which is $d_Y X$ in $\bar{x}^i$ coordinates (if I'm doing this correctly). Now I am trying to first change coordinates and apply $d_Y$ to $X$: first, in the new coordinates the vector field $X$ will be $\bar{X} = \frac{\partial \bar{x}^i}{\partial x^j}X^j \frac{\partial}{\partial \bar{x}^i}$ (contravariant transformation). So applying $d_Y$ to this we have $$d_Y \bar{X} = d_Y \left(\frac{\partial \bar{x}^i}{\partial x^j}X^j \right) \frac{\partial}{\partial \bar{x}^i}$$ Now I'm not sure what to do because it looks like $d_Y \left(\frac{\partial \bar{x}^i}{\partial x^j}X^j \right)$ will be terribly messy, and I don't really know how to evaluate it. Also, I don't know if I'm supposed to transform $Y$ to new coordinates as well (this would make the calculation even uglier) or if it's just $X$ that is transformed. Could anyone help?","I have just started learning differential geometry and I am trying to answer the following exercise (it's not for homework): Let $M$ be an $n$-dimensional manifold with chart $(U, \varphi_U)$ and let $(x^1, \dots, x^n)$ be the corresponding local coordinates. Define a vector field on $U$ by $X = X^i\frac{\partial}{\partial x^i}$. We will try to define a ""derivative of vector fields"" by $$d_Y X = d_Y X^i \frac{\partial}{\partial x^i}$$   By changing coordinates to $(\bar{x}^1, \dots, \bar{x}^n)$, show that unlike $df$, this operator is not independent of the choice of local coordinates. I think I understand the method, but I am not certain of the details. I believe what I have to do is first apply $d_Y$ to $X$ and then change coordinates, and then change coordinates and apply $d_Y$ to $X$, and show that the two results are not then same. Here is what I have attempted: Let $Y = Y^i\frac{\partial}{\partial x^i}$ (the question doesn't define $Y$ but presumably it's just another vector field). Then $$d_Y X = d_Y X^i \frac{\partial}{\partial x^i}$$ $$= dX^i(Y) \frac{\partial}{\partial x^i}$$ $$= \frac{\partial X^i}{\partial x^j}Y^j \frac{\partial}{\partial x^i}$$ This is just another vector field, so transforming it according to the contravariant transformation we get the following messy expression: $$\frac{\partial \bar{x}^i}{\partial x^j} \frac{\partial X^j}{\partial x^k} Y^k \frac{\partial}{\partial \bar{x}^i},$$ which is $d_Y X$ in $\bar{x}^i$ coordinates (if I'm doing this correctly). Now I am trying to first change coordinates and apply $d_Y$ to $X$: first, in the new coordinates the vector field $X$ will be $\bar{X} = \frac{\partial \bar{x}^i}{\partial x^j}X^j \frac{\partial}{\partial \bar{x}^i}$ (contravariant transformation). So applying $d_Y$ to this we have $$d_Y \bar{X} = d_Y \left(\frac{\partial \bar{x}^i}{\partial x^j}X^j \right) \frac{\partial}{\partial \bar{x}^i}$$ Now I'm not sure what to do because it looks like $d_Y \left(\frac{\partial \bar{x}^i}{\partial x^j}X^j \right)$ will be terribly messy, and I don't really know how to evaluate it. Also, I don't know if I'm supposed to transform $Y$ to new coordinates as well (this would make the calculation even uglier) or if it's just $X$ that is transformed. Could anyone help?",,['differential-geometry']
48,Linear independence regarding Exterior Power .,Linear independence regarding Exterior Power .,,I have been trying to learn the proof of dimension of exterior power from this text : http://www.thehcmr.org/issue1_2/poincare_lemma.pdf .( Page 16) I am not able to understand the part of linear independence ( how its been assured) . It would be nice if you could explain .  thank you .,I have been trying to learn the proof of dimension of exterior power from this text : http://www.thehcmr.org/issue1_2/poincare_lemma.pdf .( Page 16) I am not able to understand the part of linear independence ( how its been assured) . It would be nice if you could explain .  thank you .,,"['differential-geometry', 'multilinear-algebra']"
49,Did I integrate a differential form correctly?,Did I integrate a differential form correctly?,,"I start with 1-form $\omega=f\,dx$ on $\left[0,1\right]$ where $f\left(0\right)=f\left(1\right)$ and a $g:\left[0,1\right]\to R$ with $g\left(0\right)=g\left(1\right)$ and I want to integrate $\omega-\lambda \, dx=dg$ on $\left[0,1\right]$. So I write $\int\limits_0^1 \omega-\lambda \,dx = \int\limits_0^1 \left(f-\lambda\right)\,dx =\int\limits_0^1 f\,dx-\lambda$ and $\int\limits_0^1 \omega-\lambda \, dx =\int\limits_0^1 dg = g\left(1\right)-g\left(0\right)=0$ and find out that $$\lambda=\int\limits_0^1 f\,dx.$$ Is this correct? What would happen if I parametrized the path from $0$ to $1$ another way? Is $\int\limits_0^1 dg=g\left(1\right)-g\left(0\right)$ legal - I'm using dg as a differential form and maybe I'm supposed to check some precondition?","I start with 1-form $\omega=f\,dx$ on $\left[0,1\right]$ where $f\left(0\right)=f\left(1\right)$ and a $g:\left[0,1\right]\to R$ with $g\left(0\right)=g\left(1\right)$ and I want to integrate $\omega-\lambda \, dx=dg$ on $\left[0,1\right]$. So I write $\int\limits_0^1 \omega-\lambda \,dx = \int\limits_0^1 \left(f-\lambda\right)\,dx =\int\limits_0^1 f\,dx-\lambda$ and $\int\limits_0^1 \omega-\lambda \, dx =\int\limits_0^1 dg = g\left(1\right)-g\left(0\right)=0$ and find out that $$\lambda=\int\limits_0^1 f\,dx.$$ Is this correct? What would happen if I parametrized the path from $0$ to $1$ another way? Is $\int\limits_0^1 dg=g\left(1\right)-g\left(0\right)$ legal - I'm using dg as a differential form and maybe I'm supposed to check some precondition?",,['differential-geometry']
50,Generalized Laws of Cosines and Sines,Generalized Laws of Cosines and Sines,,"I wonder the ""laws of sines and cosines"" in the two cases below and how to derive them. (or any related sources) (i) For geodesic triangles on a sphere of radius $R>0$. (so constant curvature $1/R^2$) (ii) On the upper half-plane, say $\{(x,y):y>0\}$ with metric $\frac{R^2}{y^2}\pmatrix{ 1& 0\\\ 0& 1}.$ (which has curvature $-1/R^2$) Thank you so much.","I wonder the ""laws of sines and cosines"" in the two cases below and how to derive them. (or any related sources) (i) For geodesic triangles on a sphere of radius $R>0$. (so constant curvature $1/R^2$) (ii) On the upper half-plane, say $\{(x,y):y>0\}$ with metric $\frac{R^2}{y^2}\pmatrix{ 1& 0\\\ 0& 1}.$ (which has curvature $-1/R^2$) Thank you so much.",,"['trigonometry', 'differential-geometry', 'hyperbolic-geometry']"
51,How to prove a surface is smooth,How to prove a surface is smooth,,"I am given the function $F:\mathbb{R}^3\to\mathbb{R}$ where $F(x,y,z)=(x^2+y^2+z^2-5)^2+16z^2-16$ and then asked to prove that $M:=F^{-1}(0)$ is a smooth surface. Problem is, I wasn't given a definition of a smooth surface in my lecture and can't seem to find a good one via Googling. I am told that a smooth curve is one for which all higher derivatives exist for each point on the curve, so is this the same for surfaces? Either way could someone let me know how to do this question? Thanks","I am given the function $F:\mathbb{R}^3\to\mathbb{R}$ where $F(x,y,z)=(x^2+y^2+z^2-5)^2+16z^2-16$ and then asked to prove that $M:=F^{-1}(0)$ is a smooth surface. Problem is, I wasn't given a definition of a smooth surface in my lecture and can't seem to find a good one via Googling. I am told that a smooth curve is one for which all higher derivatives exist for each point on the curve, so is this the same for surfaces? Either way could someone let me know how to do this question? Thanks",,['differential-geometry']
52,Can the Differential be Considered as a Covariant Functor?,Can the Differential be Considered as a Covariant Functor?,,"First, I apologize if this question is poorly-worded or otherwise vague, I'll try to be as clear as possible. If $F:N\rightarrow M$ is a smooth map between smooth manifolds $N$ and $M$, then at each point $p \in N$ the map $F$ induces the derivation $F_{*p}:T_pN \rightarrow  T_{F(p)}M$ between tangent spaces, called the differential, that is determined by $F_{*p}(X_p)f = X_p(f \circ F)$ for all smooth real-valued functions $f$ on $M$. To me, this seems like a covariant functor from the category of smooth manifolds to the category (?) of tangent spaces. My understanding though if it is to be a functor  it must also assign, for example, a manifold $M$ to a tangent space $T_pM$. Are there additional aspects of defining the differential that would facilitate this? Is there a way, perhaps, that this can be achieved with the inclusion maps $i_N$ and $i_M$ of $N$ and $M$ into $T_pN$ and $T_pN$ since the differential, satisfies $F_{*p} \circ i_N = i_N \circ F$?","First, I apologize if this question is poorly-worded or otherwise vague, I'll try to be as clear as possible. If $F:N\rightarrow M$ is a smooth map between smooth manifolds $N$ and $M$, then at each point $p \in N$ the map $F$ induces the derivation $F_{*p}:T_pN \rightarrow  T_{F(p)}M$ between tangent spaces, called the differential, that is determined by $F_{*p}(X_p)f = X_p(f \circ F)$ for all smooth real-valued functions $f$ on $M$. To me, this seems like a covariant functor from the category of smooth manifolds to the category (?) of tangent spaces. My understanding though if it is to be a functor  it must also assign, for example, a manifold $M$ to a tangent space $T_pM$. Are there additional aspects of defining the differential that would facilitate this? Is there a way, perhaps, that this can be achieved with the inclusion maps $i_N$ and $i_M$ of $N$ and $M$ into $T_pN$ and $T_pN$ since the differential, satisfies $F_{*p} \circ i_N = i_N \circ F$?",,"['differential-geometry', 'category-theory', 'manifolds', 'functors']"
53,The extension of Manifold,The extension of Manifold,,"If $M \subset \mathbb R^n$ is a compact smooth manifold with boundary, and ${M_\varepsilon }$ is the closed $\varepsilon$-neighborhood of $M$ in $\mathbb R^n$, then whether for sufficiently small $\varepsilon$, ${M_\varepsilon }$ is a smooth manifold?","If $M \subset \mathbb R^n$ is a compact smooth manifold with boundary, and ${M_\varepsilon }$ is the closed $\varepsilon$-neighborhood of $M$ in $\mathbb R^n$, then whether for sufficiently small $\varepsilon$, ${M_\varepsilon }$ is a smooth manifold?",,['differential-geometry']
54,Conformally immersed Riemann surfaces and foliations,Conformally immersed Riemann surfaces and foliations,,"I want to show that conformally immersed Riemann surfaces in $\mathbb{R}^4$ are leaves of a 2-foliation $\mathcal{F}$. I start with the generalized Weierstrass representation of the surfaces: take 4 holomorphic functions $ \{\phi(z)_\alpha, \psi(z)_\alpha\},~\alpha=1,2$ that satisfy a Dirac equation $\partial_z \phi_\alpha=p\psi_\alpha,~\partial_{\bar{z}}\psi_\alpha=-p\phi_\alpha$ with real-valued $p(z,\bar{z})$. These define a conformal immersion into $\mathbb{R}^4$ with coordinates $X_a(z,\bar{z}), a=1,2,3,4$ that satisfy $dX_1=\frac{i}{2}(\bar\phi_1\bar\phi_2+\psi_1\psi_2)dz+c.c.$ $dX_2=\frac{1}{2}(\bar\phi_1\bar\phi_2-\psi_1\psi_2)dz+c.c.$ $dX_3=-\frac{1}{2}(\bar\phi_1\psi_2+\bar\phi_1\psi_2)dz+c.c.$ $dX_4=\frac{i}{2}(\bar\phi_1\psi_2-\bar\phi_1\psi_2)dz+c.c.$ (for details on this ""generalized Weierstrass representation"", see Konopelchenko & Landolfi ). Call this ""immersed submanifold"" $\Sigma$. Now I want to show these surfaces define a foliation, so I want to show the field of tangent planes is integrable. In these local coordinates I can write a vector field as $ V=V^a \frac{\partial}{\partial X^a}=V^a(\frac{\partial z}{\partial X^a}\frac{\partial }{\partial z}+\frac{\partial \bar{z}}{\partial X^a}\frac{\partial}{\partial \bar{z}})$ It seems pretty obvious to me then that $[V,W]$ is a vector field on $\Sigma$ if $V,W$ are, and by Frobenius' Theorem this defines a 2-foliation. On the other hand, I know that any old surface in a smooth manifold does not necessarily define a 2-foliation and it didn't look like I did anything special except use coordinates which depend smoothly on the complex coordinate $z$. So am I right about this or did I do something strange?","I want to show that conformally immersed Riemann surfaces in $\mathbb{R}^4$ are leaves of a 2-foliation $\mathcal{F}$. I start with the generalized Weierstrass representation of the surfaces: take 4 holomorphic functions $ \{\phi(z)_\alpha, \psi(z)_\alpha\},~\alpha=1,2$ that satisfy a Dirac equation $\partial_z \phi_\alpha=p\psi_\alpha,~\partial_{\bar{z}}\psi_\alpha=-p\phi_\alpha$ with real-valued $p(z,\bar{z})$. These define a conformal immersion into $\mathbb{R}^4$ with coordinates $X_a(z,\bar{z}), a=1,2,3,4$ that satisfy $dX_1=\frac{i}{2}(\bar\phi_1\bar\phi_2+\psi_1\psi_2)dz+c.c.$ $dX_2=\frac{1}{2}(\bar\phi_1\bar\phi_2-\psi_1\psi_2)dz+c.c.$ $dX_3=-\frac{1}{2}(\bar\phi_1\psi_2+\bar\phi_1\psi_2)dz+c.c.$ $dX_4=\frac{i}{2}(\bar\phi_1\psi_2-\bar\phi_1\psi_2)dz+c.c.$ (for details on this ""generalized Weierstrass representation"", see Konopelchenko & Landolfi ). Call this ""immersed submanifold"" $\Sigma$. Now I want to show these surfaces define a foliation, so I want to show the field of tangent planes is integrable. In these local coordinates I can write a vector field as $ V=V^a \frac{\partial}{\partial X^a}=V^a(\frac{\partial z}{\partial X^a}\frac{\partial }{\partial z}+\frac{\partial \bar{z}}{\partial X^a}\frac{\partial}{\partial \bar{z}})$ It seems pretty obvious to me then that $[V,W]$ is a vector field on $\Sigma$ if $V,W$ are, and by Frobenius' Theorem this defines a 2-foliation. On the other hand, I know that any old surface in a smooth manifold does not necessarily define a 2-foliation and it didn't look like I did anything special except use coordinates which depend smoothly on the complex coordinate $z$. So am I right about this or did I do something strange?",,"['riemann-surfaces', 'differential-geometry']"
55,"Product of Smooth Covering Maps a Smooth Covering Map? ( J.Lee, 2-12)","Product of Smooth Covering Maps a Smooth Covering Map? ( J.Lee, 2-12)",,"I am trying to review my differential Geometry. In J.Lee's Smooth Manifolds , there is an exercise in which one has to show that the product of smooth covering maps is a smooth covering map. A smooth covering map is a smooth cover $\pi: M' \rightarrow M$, where $M, \, M'$ are smooth manifolds, and in which every $p$ in $M'$ has a neighborhood $U_p$ in $M$ such that $p|_{p^{-1}} (U_p) \rightarrow U_p$ (i.e., the restriction of $p$ to the inverse image of $U_p$ is a diffeomorphism) . I think this exercise is straightforward, (basically we just use the fact that the product of diffeomorphisms is a diffeomorphism) but I am kind of rusty, and would appreciate your inputs. Now, we need to show that , given covers $p_1:M' \rightarrow M$ and $p_2:N' \rightarrow {N}$ that for any pair $(p,q)$ there is a neighborhood $W$ of $(p, q)$ such that $p_1 \times p_2 |_{p_1^{-1} \times p_2^{-1} (W)} : p_1^{-1} \times p_2^{-1} (W) \rightarrow W$ is a diffeomorphism. But we know that for $p$ in $M$ there is a $U_p$ with $p_1|_{p_1^{-1} (U_p)} : p_1^{-1} (U_p) \rightarrow U_p$ and for any $q$ in $N$ there is a $U_q$ with $p_2|_{{p_2}^{-1} (U_q)} : p_2^{-1} (U_q) \rightarrow U_q$ such that both are diffeomorphisms. Then the product map is a diffeomorphism automatically, isn't it, i.e., isn't the map: $$(p_1, p_2)|_{(p_1^{-1} \times p_2^{-1}) (U_q)} : (p_1^{-1} \times p_2^{-1}) (U_q) \rightarrow U_p \times U_q$$ a diffeomorphism? We know that if $f = (f_1(x_1,\ldots,x_n),f_2(x_1,\ldots,x_n))$ is differentiable with derivative $(f_1',f_2')$ and$f_1^{-1}$ and $f_2^{-1}$ are each differentiable (by the assumption of diffeomorphism), then  I think the differentiable inverse here is given by the identities fof -1 =Id. I imagine there may be some issue in showing that the above is true for manifolds, and not just for subsets of $\mathbb{R}^n$. Other than that, is my setup correct? Basically, I am curious as to whether this problem comes down to the fact that the product of diffeomorphisms is a diffeomorphism. Thanks.","I am trying to review my differential Geometry. In J.Lee's Smooth Manifolds , there is an exercise in which one has to show that the product of smooth covering maps is a smooth covering map. A smooth covering map is a smooth cover $\pi: M' \rightarrow M$, where $M, \, M'$ are smooth manifolds, and in which every $p$ in $M'$ has a neighborhood $U_p$ in $M$ such that $p|_{p^{-1}} (U_p) \rightarrow U_p$ (i.e., the restriction of $p$ to the inverse image of $U_p$ is a diffeomorphism) . I think this exercise is straightforward, (basically we just use the fact that the product of diffeomorphisms is a diffeomorphism) but I am kind of rusty, and would appreciate your inputs. Now, we need to show that , given covers $p_1:M' \rightarrow M$ and $p_2:N' \rightarrow {N}$ that for any pair $(p,q)$ there is a neighborhood $W$ of $(p, q)$ such that $p_1 \times p_2 |_{p_1^{-1} \times p_2^{-1} (W)} : p_1^{-1} \times p_2^{-1} (W) \rightarrow W$ is a diffeomorphism. But we know that for $p$ in $M$ there is a $U_p$ with $p_1|_{p_1^{-1} (U_p)} : p_1^{-1} (U_p) \rightarrow U_p$ and for any $q$ in $N$ there is a $U_q$ with $p_2|_{{p_2}^{-1} (U_q)} : p_2^{-1} (U_q) \rightarrow U_q$ such that both are diffeomorphisms. Then the product map is a diffeomorphism automatically, isn't it, i.e., isn't the map: $$(p_1, p_2)|_{(p_1^{-1} \times p_2^{-1}) (U_q)} : (p_1^{-1} \times p_2^{-1}) (U_q) \rightarrow U_p \times U_q$$ a diffeomorphism? We know that if $f = (f_1(x_1,\ldots,x_n),f_2(x_1,\ldots,x_n))$ is differentiable with derivative $(f_1',f_2')$ and$f_1^{-1}$ and $f_2^{-1}$ are each differentiable (by the assumption of diffeomorphism), then  I think the differentiable inverse here is given by the identities fof -1 =Id. I imagine there may be some issue in showing that the above is true for manifolds, and not just for subsets of $\mathbb{R}^n$. Other than that, is my setup correct? Basically, I am curious as to whether this problem comes down to the fact that the product of diffeomorphisms is a diffeomorphism. Thanks.",,[]
56,Defining the Ricci tensor on a hermitian manifold,Defining the Ricci tensor on a hermitian manifold,,"The following is an excerpt from a set of notes I'm following. Let $M$ be a complex manifold of dimension $n$ and let $g$ be an Hermitian structure in $T M$ . It is called an Hermitian metric on $M$ . A complex manifold $M$ with an Hermitian metric $g$ is called an Hermitian manifold. Then we can write $$ \begin{equation*} g=\sum g_{i \bar{j}} d z^{i} d \bar{z}^{j}, \quad \text { where } \quad g_{i \bar{j}}=g\left(\partial / \partial z^{i}, \partial / \partial \bar{z}^{j}\right) \tag{1.7.7} \end{equation*} $$ (Following the tradition, we write $d z^{i} d \bar{z}^{j}$ instead of $d z^{i} \otimes d \bar{z}^{j}$ ). Let $D$ be the Hermitian connection of $g$ . Let $R$ be the curvature of $D$ ; it is in $A^{1,1}(\operatorname{End}(T M))$ . In terms of the frame field $\left(\partial / \partial z^{1}, \cdots, \partial / \partial z^{n}\right)$ and its dual $\left(d z^{1}, \cdots, d z^{n}\right)$ , the curvature can be expressed as $$ \begin{equation*} R=\sum \Omega_{j}^{i} d z^{j} \otimes \frac{\partial}{\partial z^{i}}, \quad \text { where } \quad \Omega_{j}^{i}=\sum R_{j k \bar{h}}^{i} d z^{k} \wedge d \bar{z}^{h} \tag{1.7.8} \end{equation*} $$ Expressing (1.4.13) in terms of local coordinates, we have $$ \begin{equation*} R_{j a \bar{b}}^{i}=-\sum g^{i \bar{k}} \frac{\partial^{2} g_{j \bar{k}}}{\partial z^{a} \partial \bar{z}^{b}}+\sum g^{i \bar{k}} g^{p \bar{q}} \frac{\partial g_{p \bar{k}}}{\partial z^{a}} \frac{\partial g_{j \bar{q}}}{\partial \bar{z}^{\theta}} \tag{1.7.9} \end{equation*} $$ If we set $$ \begin{equation*} R_{j \bar{k} a \bar{b}}=\sum g_{i \bar{k}} R_{j a \bar{b}}^{i} \tag{1.7.10} \end{equation*} $$ then (1.7.9) reads as follows: $$ \begin{equation*} R_{j \bar{k} a \bar{b}}=-\frac{\partial^{2} g_{j \bar{k}}}{\partial z^{a} \partial \bar{z}^{b}}+\sum g^{p \bar{q}} \frac{\partial g_{p \bar{k}}}{\partial z^{a}} \frac{\partial g_{j \bar{q}}}{\partial \bar{z}^{b}} \tag{1.7.11} \end{equation*} $$ We define two Hermitian tensor fields contracting the curvature tensor $R$ in two different ways. We set $$ \begin{gather*} R_{k \bar{h}}=\sum R_{i k \bar{h}}^{i}=\sum g^{i \bar{j}} R_{i \bar{j} k \bar{h}}, \quad R i c=\sum R_{k \bar{h}} d z^{k} \otimes d \bar{z}^{h}  \tag{1.7.12}\\ K_{i \bar{j}}=\sum g^{k \bar{h}} R_{i \bar{j} k \bar{h}}, \quad \hat{K}=\sum K_{i \bar{j}} d z^{i} \otimes d \bar{z}^{j} \tag{1.7.13} \end{gather*} $$ I have two questions. Does the author here have an unconventional way of defining the Ricci tensor? From the looks of it, they seem to define it as the contraction of the last two indices $j$ and $i$ in $$R^i_{jkl} dz^k \wedge d\bar{z}^l\otimes dz^j \otimes \frac{\partial}{\partial z^i}$$ which would differ from the standard using the first and last index. Why isn't $\hat{K}$ identically zero given the symmetries of $R$ ?","The following is an excerpt from a set of notes I'm following. Let be a complex manifold of dimension and let be an Hermitian structure in . It is called an Hermitian metric on . A complex manifold with an Hermitian metric is called an Hermitian manifold. Then we can write (Following the tradition, we write instead of ). Let be the Hermitian connection of . Let be the curvature of ; it is in . In terms of the frame field and its dual , the curvature can be expressed as Expressing (1.4.13) in terms of local coordinates, we have If we set then (1.7.9) reads as follows: We define two Hermitian tensor fields contracting the curvature tensor in two different ways. We set I have two questions. Does the author here have an unconventional way of defining the Ricci tensor? From the looks of it, they seem to define it as the contraction of the last two indices and in which would differ from the standard using the first and last index. Why isn't identically zero given the symmetries of ?","M n g T M M M g  \begin{equation*}
g=\sum g_{i \bar{j}} d z^{i} d \bar{z}^{j}, \quad \text { where } \quad g_{i \bar{j}}=g\left(\partial / \partial z^{i}, \partial / \partial \bar{z}^{j}\right) \tag{1.7.7}
\end{equation*}
 d z^{i} d \bar{z}^{j} d z^{i} \otimes d \bar{z}^{j} D g R D A^{1,1}(\operatorname{End}(T M)) \left(\partial / \partial z^{1}, \cdots, \partial / \partial z^{n}\right) \left(d z^{1}, \cdots, d z^{n}\right) 
\begin{equation*}
R=\sum \Omega_{j}^{i} d z^{j} \otimes \frac{\partial}{\partial z^{i}}, \quad \text { where } \quad \Omega_{j}^{i}=\sum R_{j k \bar{h}}^{i} d z^{k} \wedge d \bar{z}^{h} \tag{1.7.8}
\end{equation*}
 
\begin{equation*}
R_{j a \bar{b}}^{i}=-\sum g^{i \bar{k}} \frac{\partial^{2} g_{j \bar{k}}}{\partial z^{a} \partial \bar{z}^{b}}+\sum g^{i \bar{k}} g^{p \bar{q}} \frac{\partial g_{p \bar{k}}}{\partial z^{a}} \frac{\partial g_{j \bar{q}}}{\partial \bar{z}^{\theta}} \tag{1.7.9}
\end{equation*}
 
\begin{equation*}
R_{j \bar{k} a \bar{b}}=\sum g_{i \bar{k}} R_{j a \bar{b}}^{i} \tag{1.7.10}
\end{equation*}
 
\begin{equation*}
R_{j \bar{k} a \bar{b}}=-\frac{\partial^{2} g_{j \bar{k}}}{\partial z^{a} \partial \bar{z}^{b}}+\sum g^{p \bar{q}} \frac{\partial g_{p \bar{k}}}{\partial z^{a}} \frac{\partial g_{j \bar{q}}}{\partial \bar{z}^{b}} \tag{1.7.11}
\end{equation*}
 R 
\begin{gather*}
R_{k \bar{h}}=\sum R_{i k \bar{h}}^{i}=\sum g^{i \bar{j}} R_{i \bar{j} k \bar{h}}, \quad R i c=\sum R_{k \bar{h}} d z^{k} \otimes d \bar{z}^{h}  \tag{1.7.12}\\
K_{i \bar{j}}=\sum g^{k \bar{h}} R_{i \bar{j} k \bar{h}}, \quad \hat{K}=\sum K_{i \bar{j}} d z^{i} \otimes d \bar{z}^{j} \tag{1.7.13}
\end{gather*}
 j i R^i_{jkl} dz^k \wedge d\bar{z}^l\otimes dz^j \otimes \frac{\partial}{\partial z^i} \hat{K} R","['differential-geometry', 'complex-geometry']"
57,Two definitons of a pullback of a differential form,Two definitons of a pullback of a differential form,,"Let $f: X \to Y$ be a morphism of spaces with admitting differential forms (e.g. real manifold, complex manifold, smooth algebraic variety, schemes). Let $\Omega^n_Y$ denote the sheaf of $n$ -forms on $Y$ . When one speaks of a ""pullback of a differential form,"" there are at least two possible interpretations. The first is the one from differential geometry: given a section on $\Omega^n_Y(U)$ , we get a section on $\Omega^n_X(f^{-1}(U))$ defined locally by $$g \; dy_1 \wedge \dots \wedge dy_n \mapsto (g \circ f) \; d(y_1 \circ f) \wedge \dots \wedge d(y_n \circ f).$$ This gives us a bona fide section of $\Omega^n_X$ ; using the chain rule, we can check this map respects the respective transition functions for $\Omega^{n}_Y$ and $\Omega^n_X$ , and this is what most non-algebraic geometers seem to mean when referring to pullback of differential forms. However, there is another common usage of the word ""pullback"": the canonical map on sections $H^0(Y, \Omega^n_Y) \to H^0(X, f^*\Omega^n_Y)$ , or more generally $H^0(U, \Omega^n_Y) \to H^0(f^{-1}(U), f^*\Omega^n_Y).$ I really only know how describe this map affine-locally in the algebraic category. For example, if $X = \text{Spec } A$ and $Y = \text{Spec } B$ are $k$ -varieties, then $\Omega_Y$ is the sheaf associated the module of Kรคhler differentials $\Omega_{B/k}$ , and $f^*\Omega_Y$ is the sheaf associated to the $A$ -module $A \otimes_B \Omega_{B/k}$ . Then the map on sections is given locally by sending $\omega \in \Omega_{B/k}$ to $1 \otimes \omega \in A \otimes_B \Omega_{B/k}$ . These two definitions of pullback cannot be literally the same, since in general $f^*\Omega^n_Y \not \simeq \Omega^n_X$ โthe outputted sections live on entirely different sheaves. But I have deeply confused myself trying to think about how they are related and whether we can easily convert between the two versions. Is there something I am missing, or a better way of thinking about this? I worry because I cannot find any literature that seems to address this conflict, so I feel like there is some obvious thing that is being left unsaid.","Let be a morphism of spaces with admitting differential forms (e.g. real manifold, complex manifold, smooth algebraic variety, schemes). Let denote the sheaf of -forms on . When one speaks of a ""pullback of a differential form,"" there are at least two possible interpretations. The first is the one from differential geometry: given a section on , we get a section on defined locally by This gives us a bona fide section of ; using the chain rule, we can check this map respects the respective transition functions for and , and this is what most non-algebraic geometers seem to mean when referring to pullback of differential forms. However, there is another common usage of the word ""pullback"": the canonical map on sections , or more generally I really only know how describe this map affine-locally in the algebraic category. For example, if and are -varieties, then is the sheaf associated the module of Kรคhler differentials , and is the sheaf associated to the -module . Then the map on sections is given locally by sending to . These two definitions of pullback cannot be literally the same, since in general โthe outputted sections live on entirely different sheaves. But I have deeply confused myself trying to think about how they are related and whether we can easily convert between the two versions. Is there something I am missing, or a better way of thinking about this? I worry because I cannot find any literature that seems to address this conflict, so I feel like there is some obvious thing that is being left unsaid.","f: X \to Y \Omega^n_Y n Y \Omega^n_Y(U) \Omega^n_X(f^{-1}(U)) g \; dy_1 \wedge \dots \wedge dy_n \mapsto (g \circ f) \; d(y_1 \circ f) \wedge \dots \wedge d(y_n \circ f). \Omega^n_X \Omega^{n}_Y \Omega^n_X H^0(Y, \Omega^n_Y) \to H^0(X, f^*\Omega^n_Y) H^0(U, \Omega^n_Y) \to H^0(f^{-1}(U), f^*\Omega^n_Y). X = \text{Spec } A Y = \text{Spec } B k \Omega_Y \Omega_{B/k} f^*\Omega_Y A A \otimes_B \Omega_{B/k} \omega \in \Omega_{B/k} 1 \otimes \omega \in A \otimes_B \Omega_{B/k} f^*\Omega^n_Y \not \simeq \Omega^n_X","['differential-geometry', 'algebraic-geometry', 'complex-geometry', 'differential-forms', 'pullback']"
58,Existence of special transversal on foliation,Existence of special transversal on foliation,,"This is a somewhat technical question about a line in Sharpe's book Differential Geometry: Cartan's Generalization of Klein's Erlangen Program in the proof of the structure theorem, Theorem 8.3. We have assumed that the leaf space of a foliation $M/\sim$ is Hausdorff. Because of this, each leaf $\mathcal{L}$ is closed and is in particular an embedded submanifold. Using the hypotheses that a foliation has trivial holonomy (w.r.t. transversals) and each leaf has finitely generated fundamental group, he concludes that a given leaf has a transversal that meets each leaf at most once. How can I justify this? Because each leaf is embedded, we can certainly choose a transveral intersecting a given leaf exactly once. But the tricky part is doing this uniformly for all the leaves meeting the transversal. If we replaced the assumption that leaves $\mathcal{L}$ have finitely generated fundamental group $\pi_1(\mathcal{L})$ with the stronger assumption that each leaf is compact, then Theorem 7.8 of Sharpe guarantees the existence of a tubular neighborhood $U$ of $\mathcal{L}$ with a leaf-preserving diffeomorphism $U\cong \mathcal{L}\times T$ for some transversal $T$ , which would give a transversal meeting each leaf at most once. The main application of finitely generated fundamental group and trivial holonomy I've seen is that, given a transveral of a leaf, we can pick a single open subset of the transversal where the entire fundamental group acts as the identity. Thanks!","This is a somewhat technical question about a line in Sharpe's book Differential Geometry: Cartan's Generalization of Klein's Erlangen Program in the proof of the structure theorem, Theorem 8.3. We have assumed that the leaf space of a foliation is Hausdorff. Because of this, each leaf is closed and is in particular an embedded submanifold. Using the hypotheses that a foliation has trivial holonomy (w.r.t. transversals) and each leaf has finitely generated fundamental group, he concludes that a given leaf has a transversal that meets each leaf at most once. How can I justify this? Because each leaf is embedded, we can certainly choose a transveral intersecting a given leaf exactly once. But the tricky part is doing this uniformly for all the leaves meeting the transversal. If we replaced the assumption that leaves have finitely generated fundamental group with the stronger assumption that each leaf is compact, then Theorem 7.8 of Sharpe guarantees the existence of a tubular neighborhood of with a leaf-preserving diffeomorphism for some transversal , which would give a transversal meeting each leaf at most once. The main application of finitely generated fundamental group and trivial holonomy I've seen is that, given a transveral of a leaf, we can pick a single open subset of the transversal where the entire fundamental group acts as the identity. Thanks!",M/\sim \mathcal{L} \mathcal{L} \pi_1(\mathcal{L}) U \mathcal{L} U\cong \mathcal{L}\times T T,"['differential-geometry', 'fundamental-groups', 'foliations', 'holonomy', 'cartan-geometry']"
59,Geometry of electrodynamics,Geometry of electrodynamics,,"In this question, I'd like to go over the physics - math dictionary occurring in the geometric structure (Principal bundle/spin bundles etc.) of Maxwell electrodynamics and the Dirac field. Consider the action for classical electrodynamics on Minkowski spacetime $(M,\eta)$ : $$S_{QED} = \int_{M} \big[ -\frac{1}{4} F_{\mu \nu} F^{\mu \nu} + \psi^{\dagger} \gamma^{0}(i\gamma^{\mu} D_{\mu} - m) \psi \big] dV_{\eta}$$ where the $\gamma$ symbols are the Dirac matrices, $D_\mu = \partial_\mu - ieA_\mu$ is a so-called gauge covariant derivative in which $A_\mu$ is the electromagnetic four potential . (I will italicize physics terminology that we will translate). The above theory is a $U(1)$ - gauge theory , i.e. a geometrical physical theory formulated using a principal bundle - thus one which must ""respect"" the mathematical redundancy of describing a principal bundle (its isomorphism class). This is analogous to the fact that isometric Lorentzian manifolds (if there are fields, we pull these back naturally too) describe the same spacetime (and fields). Namely that $$S_{EH}[(M,g)] = S_{EH}[(N,h)]$$ when $(M,g)$ is isometric to $(N,h)$ . Anyway here is the structure which I so far see regarding QED before quantization: We have a principal $U(1)$ bundle over $\mathbf{R}^4$ (so necessarily trivial $\equiv$ there exists a global smooth section) on which there is an principal $U(1)$ connection $\omega$ . Now  a trivialization of this bundle $(E,\pi,M)$ over a neighboorhood $\mathcal{U} \subset M$ is called a local gauge . A local gauge transformation is a change in this trivialization and hence equivalently the related section (the local smooth section equivalent to the trivialization, which can also be called a local gauge or mathematically, a trivializing section). Anyway, it is a standard fact that such local gauge transformations are in one to one correspondence with bundle automorphisms of $\pi^{-1}(\mathcal{U})$ . The $A_\mu$ above is the pullback of the principal $U(1)$ connection to the trivializing neighborhood by some trivializing section, so $s^{*}\omega$ . As in the above discussion, if one wants to do a local gauge transformation, one needs only change the local trivializing section. So we replace $s$ by the section $s^g$ where pointwise it is defined by $s^g = s \cdot g$ where $g: \mathcal{U} \to E$ is a smooth function (all local sections are given this way). From here we get the famous formula $$(s^g)^{*}\omega = Ad(g)^{-1} s^{*} \omega + g^{-1} dg$$ . This is the transformation formula for the connection one forms . Next, recall the Faraday tensor is simply the pullback of the curvature of $\omega$ . Question one Given this set up, how do we show geometrically that the Faraday tensor term is ""gauge invariant"". Question two Considering the Dirac term is geometrically formulated in terms of a (associate) spin bundle and all that machinery, how does the gauge transformation act on that end? Also what is the gauge covariant derivative geometrically?","In this question, I'd like to go over the physics - math dictionary occurring in the geometric structure (Principal bundle/spin bundles etc.) of Maxwell electrodynamics and the Dirac field. Consider the action for classical electrodynamics on Minkowski spacetime : where the symbols are the Dirac matrices, is a so-called gauge covariant derivative in which is the electromagnetic four potential . (I will italicize physics terminology that we will translate). The above theory is a - gauge theory , i.e. a geometrical physical theory formulated using a principal bundle - thus one which must ""respect"" the mathematical redundancy of describing a principal bundle (its isomorphism class). This is analogous to the fact that isometric Lorentzian manifolds (if there are fields, we pull these back naturally too) describe the same spacetime (and fields). Namely that when is isometric to . Anyway here is the structure which I so far see regarding QED before quantization: We have a principal bundle over (so necessarily trivial there exists a global smooth section) on which there is an principal connection . Now  a trivialization of this bundle over a neighboorhood is called a local gauge . A local gauge transformation is a change in this trivialization and hence equivalently the related section (the local smooth section equivalent to the trivialization, which can also be called a local gauge or mathematically, a trivializing section). Anyway, it is a standard fact that such local gauge transformations are in one to one correspondence with bundle automorphisms of . The above is the pullback of the principal connection to the trivializing neighborhood by some trivializing section, so . As in the above discussion, if one wants to do a local gauge transformation, one needs only change the local trivializing section. So we replace by the section where pointwise it is defined by where is a smooth function (all local sections are given this way). From here we get the famous formula . This is the transformation formula for the connection one forms . Next, recall the Faraday tensor is simply the pullback of the curvature of . Question one Given this set up, how do we show geometrically that the Faraday tensor term is ""gauge invariant"". Question two Considering the Dirac term is geometrically formulated in terms of a (associate) spin bundle and all that machinery, how does the gauge transformation act on that end? Also what is the gauge covariant derivative geometrically?","(M,\eta) S_{QED} = \int_{M} \big[ -\frac{1}{4} F_{\mu \nu} F^{\mu \nu} + \psi^{\dagger} \gamma^{0}(i\gamma^{\mu} D_{\mu} - m) \psi \big] dV_{\eta} \gamma D_\mu = \partial_\mu - ieA_\mu A_\mu U(1) S_{EH}[(M,g)] = S_{EH}[(N,h)] (M,g) (N,h) U(1) \mathbf{R}^4 \equiv U(1) \omega (E,\pi,M) \mathcal{U} \subset M \pi^{-1}(\mathcal{U}) A_\mu U(1) s^{*}\omega s s^g s^g = s \cdot g g: \mathcal{U} \to E (s^g)^{*}\omega = Ad(g)^{-1} s^{*} \omega + g^{-1} dg \omega","['differential-geometry', 'mathematical-physics', 'principal-bundles', 'gauge-theory']"
60,Simple question on differential forms,Simple question on differential forms,,"Let $\omega = dp_i \wedge dq^i$ be the standard symplectic form on $\mathbb{R}^{2n}(q^i, p_i)$ . (We use the Einstein summation convention throughout). Assume $dq^i = h^{ij} dp_j$ on a Lagrangian near $0$ for some function $h^{ij} = h^{ij}(p)$ . Then, in particular, $h^{ij} = h^{ji}$ . I want to show that there exists a function $f$ such that $$ h^{ij} = \frac{\partial f}{\partial p_i \partial p_j} $$ and $\frac{\partial f}{\partial p_i}(0) = 0$ . I am stuck: Clearly, $0 = d^2 q^i = dh^{ij} \wedge dp_j$ and hence, $$ \frac{\partial h^{ij}}{\partial p_k} dp_k \wedge dp_j = 0. $$ How to conclude from here? Edit: We see that $h^{ij} = \frac{\partial q^i}{\partial p_j}$ . Is it always true that in the case where $h^{ij} = h^{ji}$ , we have $h^{ij}$ is a second partial derivative of a function (due to Schwarz)?","Let be the standard symplectic form on . (We use the Einstein summation convention throughout). Assume on a Lagrangian near for some function . Then, in particular, . I want to show that there exists a function such that and . I am stuck: Clearly, and hence, How to conclude from here? Edit: We see that . Is it always true that in the case where , we have is a second partial derivative of a function (due to Schwarz)?","\omega = dp_i \wedge dq^i \mathbb{R}^{2n}(q^i, p_i) dq^i = h^{ij} dp_j 0 h^{ij} = h^{ij}(p) h^{ij} = h^{ji} f 
h^{ij} = \frac{\partial f}{\partial p_i \partial p_j}
 \frac{\partial f}{\partial p_i}(0) = 0 0 = d^2 q^i = dh^{ij} \wedge dp_j 
\frac{\partial h^{ij}}{\partial p_k} dp_k \wedge dp_j = 0.
 h^{ij} = \frac{\partial q^i}{\partial p_j} h^{ij} = h^{ji} h^{ij}",['differential-geometry']
61,Exponential map of an $SO(n)$-invariant metric on $\mathbb R^n$ is diffeomorphism,Exponential map of an -invariant metric on  is diffeomorphism,SO(n) \mathbb R^n,"Let $g$ be a complete $SO(n)$ -invariant metric on $\mathbb R^n$ . That is, for each $A \in SO(n)$ , the map $\varphi_A:\mathbb R^n \rightarrow \mathbb R^n$ given by $x \mapsto Ax$ is a Riemannian isometry of $(\mathbb R^n,g)$ . Let $\exp_0:T_0\mathbb R^n \rightarrow \mathbb R^n$ denote the Riemannian exponential map at zero. Recall that $\exp_0(v) := \gamma_v(1)$ , where $\gamma_v:\mathbb R \rightarrow \mathbb R^n$ is the unique maximal geodesic starting at zero with initial velocity $v$ . Since $g$ is complete, $\exp_0$ is surjective (there exists a geodesic connecting any point in $\mathbb R^n$ to zero). It is well-known that $\exp_0$ is a diffeomorphism if restricted to a sufficiently small neighbourhood around zero. Question: Is $\exp_0$ a diffeomorphism? (I know that it remains to show $\exp_0$ is injective and a local diffeomorphism, but I'm not sure how to show these.) EDIT: I believe the answer is yes and I have come up with a solution. Please let me know if I have made a mistake somewhere. Consider the inner product space $(T_0 \mathbb R^n,g_0)$ . Observe that the isotropy subgroup of $0 \in \mathbb R^n$ is the entire group $SO(n)$ . Thus, $SO(n)$ acts by linear isometries on $(T_0 \mathbb R^n,g_0)$ via $A \cdot v := d \varphi_A(v)$ . For each $r > 0$ , let us define $$S_r(0) := \left\{v \in T_0 \mathbb R^n \mid g_0(v,v) = r^2\right\}.$$ Lemma 1. For any $r > 0$ , $SO(n)$ acts transitively on $S_r(0)$ . Proof. First, let us identify $T_0 \mathbb R^n$ with $\mathbb R^n$ via the linear isomorphism $\partial_i|_0 \mapsto e_i$ . Here, $\partial_i|_0$ are the coordinate vectors induced by the identity chart, and $e_i$ denote the standard basis for $\mathbb R^n$ . Recall that under this identification, for each $A \in SO(n)$ , the differential $d\varphi_A|_0:T_0 \mathbb R^n \rightarrow T_0\mathbb R^n$ is just $\varphi_A:\mathbb R^n \rightarrow \mathbb R^n$ , because $\varphi_A$ is linear. Thus, under this identification, the action of $SO(n)$ on $T_0 \mathbb R^n \cong \mathbb R^n$ is exactly the same as the original action. We know that $SO(n)$ acts transitively on the Euclidean spheres $\mathbb S^{n-1}(R)$ . Thus, we are done if we show that $g_0$ is a multiple of the standard dot product $\bullet$ on $\mathbb R^n$ . Set $c := g_0(e_1,e_1)$ . Fix an arbitrary element in $\mathbb R^n$ , which can be written as $tv$ , with $t \geq  0$ and $v\bullet v = 1$ . Since $SO(n)$ acts transitively on $\mathbb S^{n-1}(1)$ , there exists $A \in SO(n)$ such that $Av = e_1$ . Therefore, since each $d\varphi_A|_0=\varphi_A$ is a linear isometry of $(T_0 \mathbb R^n\cong \mathbb R^n,g_0)$ , we find $$g_0(tv,tv) = g_0(Atv, Atv) = t^2 g_0(Av,Av) = t^2 g_0(e_1,e_1) =  t^2 c = c (tv \bullet tv),$$ as desired. $\square$ Lemma 2. Fix $R > 0$ . Then there exists some $r > 0$ such that the restriction $\exp_0|_{S_r(0)}:S_r(0)  \rightarrow  \mathbb S^{n-1}(R)$ is a surjective smooth submersion. Proof. Since $\exp_0$ is surjective, there exists a unit speed geodesic $\gamma:\mathbb R \rightarrow \mathbb R^n$ such that $\gamma(0) = 0$ and $\gamma(r) \in \mathbb S^{n-1}(R)$ for some $r > 0$ . Let $v := \gamma'(0)$ . First, let us show that the image of $S_r(0)$ is contained in $S^{n-1}(R)$ . To this end,fix an arbitrary element of $S_r(0)$ , which can be written as $rw$ , where $r > 0$ and $w \in S_1(0)$ . We wish to show that $\exp_0(rw) \in \mathbb S^{n-1}(R)$ . By the previous lemma, we know that $SO(n)$ acts transitively on $S_1(0)$ , so there exists some $A \in SO(n)$ such that $w = d \varphi_A(v) = (\varphi_A \circ \gamma)'(0).$ Uniqueness of geodesics implies that the geodesic $\varphi_A \circ \gamma$ is given by $t \mapsto \exp_0(tw)$ . In particular, $A \gamma(r) = (\varphi_A \circ \gamma)(r) = \exp_0(rw)$ . Thus, $\exp_0(rw)$ belongs to $\mathbb S^{n-1}(R)$ . Next, let us show surjectivity. Suppose $p \in \mathbb S^{n-1}(R)$ . Then there exists $A \in SO(n)$ such that $p = A\gamma(r) = (\varphi_A \circ \gamma)(r)$ . We know that $\varphi_A \circ \gamma$ is a unit speed geodesic starting at zero. Set $w := (\varphi_A \circ \gamma)'(0)$ . By uniqueness of geodesics, we know that $\varphi_A \circ \gamma$ is given by $t \mapsto \exp_0(tw)$ . In particular, $\exp_0(rw) = (\varphi_A \circ \gamma)(r) = p$ . Now, let us show that $\exp_0|_{S_r(0)}:S_r(0)  \rightarrow  \mathbb S^{n-1}(R)$ is a smooth submersion. Since we have shown surjectivity, the Global Rank Theorem tells us that it suffices to show that $\exp_0|_{S_r(0)}$ has constant rank. By Lemma 1, we know that $SO(n)$ acts transitively on the domain $S_r(0)$ . Thus, by the Equivariant Rank Theorem, it suffices to show that $\exp_0|_{S_r(0)}:S_r(0)  \rightarrow  \mathbb S^{n-1}(R)$ is $SO(n)$ -equivariant. Fix $rv \in S_r(0)$ , where $r > 0$ and $v \in S_1(0)$ . Then $$A\cdot \exp_0(rv) = (\varphi_A \circ \exp_0)(rv) = (\exp_0 \circ d \varphi_A |_0)(rv) = \exp_0(A \cdot rv),$$ where we have used the naturality of the exponential map in the second equality. $\square$ Lemma 3. Fix $R > 0$ . Let $\gamma: \mathbb R \rightarrow \mathbb R^n$ be a unit speed geodesic such that $\gamma(0) = 0$ and $\gamma(r) \in \mathbb S^{n-1}(R)$ . Then $\gamma'(r)$ is orthogonal to $T_{\gamma(r)}S^{n-1}(R)$ . Proof. Let $v := \gamma'(0)$ . By Lemma 2, we know that $$d (\exp_0|_{S_r(0)})|_{rv}: T_{rv} S_r(0) \rightarrow T_{\gamma(r)} \mathbb S^{n-1}(R)$$ is a linear isomorphism. Let $w \in T_{\gamma(r)} \mathbb S^{n-1}(R)$ . Then there exists $u \in T_{rv} S_r(0)$ such that $dexp_0|_{rv}(u) = w$ . The Gauss Lemma tells us that $$g_{\gamma(r)}(w, \gamma'(r)) = g_{\gamma(r)}(d\exp_0|_{rv}(u),d\exp_0|_{rv}(v) ) = g_0(u,v) = 0.$$ This completes the proof. $\square$ Lemma 4. Let $\gamma: \mathbb R \rightarrow \mathbb R^n$ be a geodesic such that $\gamma(0) = 0$ . Then $\gamma(r) \neq 0$ for all $r > 0$ . Proof. For the sake of contradiction, suppose that $\gamma(r) = 0$ for some $r > 0$ . Since $[0,r]$ is compact and $\gamma$ is continuous, we know that the image $\gamma([0,r])$ is bounded in the Euclidean norm $\| \cdot \|$ . Thus, there exists $R > 0$ such that $\|\gamma(t) \| < R$ for all $t \in [0,r]$ . Now, fix $p \in \mathbb S^{n-1}(R)$ . Since $(\mathbb R^n,g)$ is complete, there exists a unit speed geodesic $\alpha: \mathbb R \rightarrow \mathbb R^n$ such that $\alpha(0) = 0$ , $\alpha(s) = p$ for some $s > 0$ , and $\alpha|_{[0,s]}$ is distance minimising with respect to $g$ . Choose $A \in SO(n)$ so that $\gamma'(0) = d\varphi_A(\alpha'(0)) = (\varphi_A \circ \alpha)'(0)$ . By uniqueness of geodesics, we know that $\gamma = \varphi_A \circ \alpha$ . Therefore, $\gamma|_{[0,s]}$ minimises the distance (with respect to the metric $g$ ) between $0$ and $\gamma(s) \in \mathbb S^{n-1}(R)$ . It follows that $r < s$ . Consider the reparameterisation $\beta: \mathbb R \rightarrow \mathbb R^n$ given by $\beta(t) := \gamma(t + r)$ . Then $\beta$ is a unit speed geodesic such that $\beta(0) = 0$ and $\beta(s-r) = \gamma(s-r + r) = \gamma(s)$ . However, observe that $$L_g(\beta|_{[0,s-r]}) = s-r < s = L_g(\gamma|_{[0,s]}),$$ so this contradicts the fact that $\gamma$ is distance minimising. $\square$ Proposition. The exponential map $\exp_0:T_0 \mathbb R^n \rightarrow \mathbb R^n$ is injective. Proof. For the sake of contradiction, suppose that $\exp_0(rv) = \exp_0(sw)$ for some $r,s \geq 0$ and $v,w \in S_1(0)$ . Let $\alpha,\beta:\mathbb R \rightarrow \mathbb R^n$ denote the unit speed geodesics given by $\alpha:t \mapsto \exp_0(tv)$ and $\beta:t \mapsto \exp_0(tw)$ . Then $\alpha(r) = \beta(s)$ . If $\alpha(r) = \beta(s) = 0$ , then Lemma 4 implies that $r = s$ , so $rv = sw$ . Thus, let us suppose $\alpha(r) =\beta(s)$ is non-zero. Then $r,s > 0$ . Fix $R>0$ such that $\alpha(r) = \beta(s)$ belongs to $\mathbb S^{n-1}(R)$ .  By Lemma 3, we know that $\alpha'(r)$ and $\beta'(s)$ are both orthogonal to $T_{\alpha(r)}\mathbb S^{n-1}(R)$ . Since $\mathbb S^{n-1}(R)$ is a codimension one embedded submanifold, we conclude that either $\alpha'(r) = \beta'(s)$ or $\alpha'(r) = - \beta'(s)$ . Let us show that in $rv = sw$ in the first case, and the second case leads to contradiction. Now, suppose $\alpha'(r) = \beta'(s)$ . Without loss of generality, suppose $r \leq  s$ . Define $\gamma: \mathbb R \rightarrow \mathbb R^n$ to be the unit speed geodesic given by $\gamma(t) := \beta(t + s - r)$ . Now, observe that $\gamma(r) = \beta(s) = \alpha(r)$ and $\gamma'(r) = \beta'(s) = \alpha'(r)$ . Uniqueness of geodesics implies that $\gamma = \alpha$ . In particular, $0 = \beta(0) = \gamma(s-r) = \alpha(s-r)$ . Thus, by Lemma 4, we must have $s = r$ . Therefore, $\alpha = \gamma = \beta$ . In particular, $v = \alpha'(0) = \beta'(0) = w$ , so $sv = rw$ , as desired. Finally, suppose that $\alpha'(r) = -\beta'(s)$ . Let $\gamma: \mathbb R \rightarrow \mathbb R^n$ be the unit speed geodesic given by $\gamma(t) := \beta(-t + s + r)$ . Then observe that $\gamma(r) = \beta(-r + s + r) = \beta(s) = \alpha(r)$ and $\gamma'(r) = - \beta'(-r + s + r) = -\beta'(s) = \alpha'(r)$ , so uniqueness of geodesics implies that $\gamma = \alpha$ . In particular, $\alpha(s + r) = 0$ . However, Lemma 4 implies that this is a contradiction, since $s + r > 0$ . $\square$ Proposition. The exponential map $\exp_0:T_0 \mathbb R^n \rightarrow \mathbb R^n$ is a diffeomorphism. Proof. We have shown that $\exp_0:T_0 \mathbb R^n \rightarrow \mathbb R^n$ is bijective. By the Global Rank Theorem, it suffices to show that $\exp_0$ has constant rank. Thus, fix $r > 0$ and $v \in S_1(0)$ , and let $\gamma: \mathbb R \rightarrow \mathbb R^n$ denote the unit speed geodesic given by $\gamma(t) := \exp_0(tv)$ . Let $R > 0$ be such that $\gamma(r) \in \mathbb S^{n-1}(R)$ . Consider the differential $d\exp_0|_{rv}:T_0 \mathbb R^n \rightarrow T_{\gamma(r)} \mathbb R^n$ . Let $e_2,\ldots,e_n$ denote unit vectors in $T_0 \mathbb R^n$ such that $\gamma'(0),e_2,\ldots,e_n$ forms an orthonormal basis for $T_0 \mathbb R^n$ . Then we know that $e_2,\ldots,e_n$ must belong to $T_{rv} S_r(0)$ . By Lemma 2, we know that $\exp_0|_{S_r(0)}:S_r(0)  \rightarrow  \mathbb S^{n-1}(R)$ is a submersion, so in particular, $d \exp_0|_{rv}(e_2),\ldots,d \exp_0|_{rv}(e_n)$ forms a basis of $T_{\gamma(r)} \mathbb S^{n-1}(R)$ . Since $\gamma'(r) = d \exp_0|_{v}$ is orthogonal to these vectors, it follows that $d\exp_0|_{rv}$ is a linear isomorphism. This completes the proof. $\square$","Let be a complete -invariant metric on . That is, for each , the map given by is a Riemannian isometry of . Let denote the Riemannian exponential map at zero. Recall that , where is the unique maximal geodesic starting at zero with initial velocity . Since is complete, is surjective (there exists a geodesic connecting any point in to zero). It is well-known that is a diffeomorphism if restricted to a sufficiently small neighbourhood around zero. Question: Is a diffeomorphism? (I know that it remains to show is injective and a local diffeomorphism, but I'm not sure how to show these.) EDIT: I believe the answer is yes and I have come up with a solution. Please let me know if I have made a mistake somewhere. Consider the inner product space . Observe that the isotropy subgroup of is the entire group . Thus, acts by linear isometries on via . For each , let us define Lemma 1. For any , acts transitively on . Proof. First, let us identify with via the linear isomorphism . Here, are the coordinate vectors induced by the identity chart, and denote the standard basis for . Recall that under this identification, for each , the differential is just , because is linear. Thus, under this identification, the action of on is exactly the same as the original action. We know that acts transitively on the Euclidean spheres . Thus, we are done if we show that is a multiple of the standard dot product on . Set . Fix an arbitrary element in , which can be written as , with and . Since acts transitively on , there exists such that . Therefore, since each is a linear isometry of , we find as desired. Lemma 2. Fix . Then there exists some such that the restriction is a surjective smooth submersion. Proof. Since is surjective, there exists a unit speed geodesic such that and for some . Let . First, let us show that the image of is contained in . To this end,fix an arbitrary element of , which can be written as , where and . We wish to show that . By the previous lemma, we know that acts transitively on , so there exists some such that Uniqueness of geodesics implies that the geodesic is given by . In particular, . Thus, belongs to . Next, let us show surjectivity. Suppose . Then there exists such that . We know that is a unit speed geodesic starting at zero. Set . By uniqueness of geodesics, we know that is given by . In particular, . Now, let us show that is a smooth submersion. Since we have shown surjectivity, the Global Rank Theorem tells us that it suffices to show that has constant rank. By Lemma 1, we know that acts transitively on the domain . Thus, by the Equivariant Rank Theorem, it suffices to show that is -equivariant. Fix , where and . Then where we have used the naturality of the exponential map in the second equality. Lemma 3. Fix . Let be a unit speed geodesic such that and . Then is orthogonal to . Proof. Let . By Lemma 2, we know that is a linear isomorphism. Let . Then there exists such that . The Gauss Lemma tells us that This completes the proof. Lemma 4. Let be a geodesic such that . Then for all . Proof. For the sake of contradiction, suppose that for some . Since is compact and is continuous, we know that the image is bounded in the Euclidean norm . Thus, there exists such that for all . Now, fix . Since is complete, there exists a unit speed geodesic such that , for some , and is distance minimising with respect to . Choose so that . By uniqueness of geodesics, we know that . Therefore, minimises the distance (with respect to the metric ) between and . It follows that . Consider the reparameterisation given by . Then is a unit speed geodesic such that and . However, observe that so this contradicts the fact that is distance minimising. Proposition. The exponential map is injective. Proof. For the sake of contradiction, suppose that for some and . Let denote the unit speed geodesics given by and . Then . If , then Lemma 4 implies that , so . Thus, let us suppose is non-zero. Then . Fix such that belongs to .  By Lemma 3, we know that and are both orthogonal to . Since is a codimension one embedded submanifold, we conclude that either or . Let us show that in in the first case, and the second case leads to contradiction. Now, suppose . Without loss of generality, suppose . Define to be the unit speed geodesic given by . Now, observe that and . Uniqueness of geodesics implies that . In particular, . Thus, by Lemma 4, we must have . Therefore, . In particular, , so , as desired. Finally, suppose that . Let be the unit speed geodesic given by . Then observe that and , so uniqueness of geodesics implies that . In particular, . However, Lemma 4 implies that this is a contradiction, since . Proposition. The exponential map is a diffeomorphism. Proof. We have shown that is bijective. By the Global Rank Theorem, it suffices to show that has constant rank. Thus, fix and , and let denote the unit speed geodesic given by . Let be such that . Consider the differential . Let denote unit vectors in such that forms an orthonormal basis for . Then we know that must belong to . By Lemma 2, we know that is a submersion, so in particular, forms a basis of . Since is orthogonal to these vectors, it follows that is a linear isomorphism. This completes the proof.","g SO(n) \mathbb R^n A \in SO(n) \varphi_A:\mathbb R^n \rightarrow \mathbb R^n x \mapsto Ax (\mathbb R^n,g) \exp_0:T_0\mathbb R^n \rightarrow \mathbb R^n \exp_0(v) := \gamma_v(1) \gamma_v:\mathbb R \rightarrow \mathbb R^n v g \exp_0 \mathbb R^n \exp_0 \exp_0 \exp_0 (T_0 \mathbb R^n,g_0) 0 \in \mathbb R^n SO(n) SO(n) (T_0 \mathbb R^n,g_0) A \cdot v := d \varphi_A(v) r > 0 S_r(0) := \left\{v \in T_0 \mathbb R^n \mid g_0(v,v) = r^2\right\}. r > 0 SO(n) S_r(0) T_0 \mathbb R^n \mathbb R^n \partial_i|_0 \mapsto e_i \partial_i|_0 e_i \mathbb R^n A \in SO(n) d\varphi_A|_0:T_0 \mathbb R^n \rightarrow T_0\mathbb R^n \varphi_A:\mathbb R^n \rightarrow \mathbb R^n \varphi_A SO(n) T_0 \mathbb R^n \cong \mathbb R^n SO(n) \mathbb S^{n-1}(R) g_0 \bullet \mathbb R^n c := g_0(e_1,e_1) \mathbb R^n tv t \geq  0 v\bullet v = 1 SO(n) \mathbb S^{n-1}(1) A \in SO(n) Av = e_1 d\varphi_A|_0=\varphi_A (T_0 \mathbb R^n\cong \mathbb R^n,g_0) g_0(tv,tv) = g_0(Atv, Atv) = t^2 g_0(Av,Av) = t^2 g_0(e_1,e_1) =  t^2 c = c (tv \bullet tv), \square R > 0 r > 0 \exp_0|_{S_r(0)}:S_r(0)  \rightarrow  \mathbb S^{n-1}(R) \exp_0 \gamma:\mathbb R \rightarrow \mathbb R^n \gamma(0) = 0 \gamma(r) \in \mathbb S^{n-1}(R) r > 0 v := \gamma'(0) S_r(0) S^{n-1}(R) S_r(0) rw r > 0 w \in S_1(0) \exp_0(rw) \in \mathbb S^{n-1}(R) SO(n) S_1(0) A \in SO(n) w = d \varphi_A(v) = (\varphi_A \circ \gamma)'(0). \varphi_A \circ \gamma t \mapsto \exp_0(tw) A \gamma(r) = (\varphi_A \circ \gamma)(r) = \exp_0(rw) \exp_0(rw) \mathbb S^{n-1}(R) p \in \mathbb S^{n-1}(R) A \in SO(n) p = A\gamma(r) = (\varphi_A \circ \gamma)(r) \varphi_A \circ \gamma w := (\varphi_A \circ \gamma)'(0) \varphi_A \circ \gamma t \mapsto \exp_0(tw) \exp_0(rw) = (\varphi_A \circ \gamma)(r) = p \exp_0|_{S_r(0)}:S_r(0)  \rightarrow  \mathbb S^{n-1}(R) \exp_0|_{S_r(0)} SO(n) S_r(0) \exp_0|_{S_r(0)}:S_r(0)  \rightarrow  \mathbb S^{n-1}(R) SO(n) rv \in S_r(0) r > 0 v \in S_1(0) A\cdot \exp_0(rv) = (\varphi_A \circ \exp_0)(rv) = (\exp_0 \circ d \varphi_A |_0)(rv) = \exp_0(A \cdot rv), \square R > 0 \gamma: \mathbb R \rightarrow \mathbb R^n \gamma(0) = 0 \gamma(r) \in \mathbb S^{n-1}(R) \gamma'(r) T_{\gamma(r)}S^{n-1}(R) v := \gamma'(0) d (\exp_0|_{S_r(0)})|_{rv}: T_{rv} S_r(0) \rightarrow T_{\gamma(r)} \mathbb S^{n-1}(R) w \in T_{\gamma(r)} \mathbb S^{n-1}(R) u \in T_{rv} S_r(0) dexp_0|_{rv}(u) = w g_{\gamma(r)}(w, \gamma'(r)) = g_{\gamma(r)}(d\exp_0|_{rv}(u),d\exp_0|_{rv}(v) ) = g_0(u,v) = 0. \square \gamma: \mathbb R \rightarrow \mathbb R^n \gamma(0) = 0 \gamma(r) \neq 0 r > 0 \gamma(r) = 0 r > 0 [0,r] \gamma \gamma([0,r]) \| \cdot \| R > 0 \|\gamma(t) \| < R t \in [0,r] p \in \mathbb S^{n-1}(R) (\mathbb R^n,g) \alpha: \mathbb R \rightarrow \mathbb R^n \alpha(0) = 0 \alpha(s) = p s > 0 \alpha|_{[0,s]} g A \in SO(n) \gamma'(0) = d\varphi_A(\alpha'(0)) = (\varphi_A \circ \alpha)'(0) \gamma = \varphi_A \circ \alpha \gamma|_{[0,s]} g 0 \gamma(s) \in \mathbb S^{n-1}(R) r < s \beta: \mathbb R \rightarrow \mathbb R^n \beta(t) := \gamma(t + r) \beta \beta(0) = 0 \beta(s-r) = \gamma(s-r + r) = \gamma(s) L_g(\beta|_{[0,s-r]}) = s-r < s = L_g(\gamma|_{[0,s]}), \gamma \square \exp_0:T_0 \mathbb R^n \rightarrow \mathbb R^n \exp_0(rv) = \exp_0(sw) r,s \geq 0 v,w \in S_1(0) \alpha,\beta:\mathbb R \rightarrow \mathbb R^n \alpha:t \mapsto \exp_0(tv) \beta:t \mapsto \exp_0(tw) \alpha(r) = \beta(s) \alpha(r) = \beta(s) = 0 r = s rv = sw \alpha(r) =\beta(s) r,s > 0 R>0 \alpha(r) = \beta(s) \mathbb S^{n-1}(R) \alpha'(r) \beta'(s) T_{\alpha(r)}\mathbb S^{n-1}(R) \mathbb S^{n-1}(R) \alpha'(r) = \beta'(s) \alpha'(r) = - \beta'(s) rv = sw \alpha'(r) = \beta'(s) r \leq  s \gamma: \mathbb R \rightarrow \mathbb R^n \gamma(t) := \beta(t + s - r) \gamma(r) = \beta(s) = \alpha(r) \gamma'(r) = \beta'(s) = \alpha'(r) \gamma = \alpha 0 = \beta(0) = \gamma(s-r) = \alpha(s-r) s = r \alpha = \gamma = \beta v = \alpha'(0) = \beta'(0) = w sv = rw \alpha'(r) = -\beta'(s) \gamma: \mathbb R \rightarrow \mathbb R^n \gamma(t) := \beta(-t + s + r) \gamma(r) = \beta(-r + s + r) = \beta(s) = \alpha(r) \gamma'(r) = - \beta'(-r + s + r) = -\beta'(s) = \alpha'(r) \gamma = \alpha \alpha(s + r) = 0 s + r > 0 \square \exp_0:T_0 \mathbb R^n \rightarrow \mathbb R^n \exp_0:T_0 \mathbb R^n \rightarrow \mathbb R^n \exp_0 r > 0 v \in S_1(0) \gamma: \mathbb R \rightarrow \mathbb R^n \gamma(t) := \exp_0(tv) R > 0 \gamma(r) \in \mathbb S^{n-1}(R) d\exp_0|_{rv}:T_0 \mathbb R^n \rightarrow T_{\gamma(r)} \mathbb R^n e_2,\ldots,e_n T_0 \mathbb R^n \gamma'(0),e_2,\ldots,e_n T_0 \mathbb R^n e_2,\ldots,e_n T_{rv} S_r(0) \exp_0|_{S_r(0)}:S_r(0)  \rightarrow  \mathbb S^{n-1}(R) d \exp_0|_{rv}(e_2),\ldots,d \exp_0|_{rv}(e_n) T_{\gamma(r)} \mathbb S^{n-1}(R) \gamma'(r) = d \exp_0|_{v} d\exp_0|_{rv} \square","['differential-geometry', 'riemannian-geometry', 'smooth-manifolds', 'geodesic', 'diffeomorphism']"
62,Is $y=|x|$ a differential manifold?,Is  a differential manifold?,y=|x|,"In the Euclidean space $\mathbb R^2$ , the function of absolute values $y=|x|$ determines a $1$ -dimentional topological manifold $M$ . Define the natural projection map $$p:\mathbb R^2\to \mathbb R,$$ $$(x,y)\mapsto x,$$ let $f:=p|_M$ be the restriction of $p$ on $M$ , then $M$ can be covered by two charts $U,V$ , where $U=f^{-1}(-1,+\infty)$ and $V=f^{-1}(-\infty,1)$ , the transition map between $U$ and $V$ is the identity map, which is clearly a smooth function, so according to the definition of differential (or smooth) manifolds (see for example Wiki page ), can we get the conclusion: the function image of $y=|x|$ is a differential manifold?","In the Euclidean space , the function of absolute values determines a -dimentional topological manifold . Define the natural projection map let be the restriction of on , then can be covered by two charts , where and , the transition map between and is the identity map, which is clearly a smooth function, so according to the definition of differential (or smooth) manifolds (see for example Wiki page ), can we get the conclusion: the function image of is a differential manifold?","\mathbb R^2 y=|x| 1 M p:\mathbb R^2\to \mathbb R, (x,y)\mapsto x, f:=p|_M p M M U,V U=f^{-1}(-1,+\infty) V=f^{-1}(-\infty,1) U V y=|x|",['differential-geometry']
63,Why Ricci Flow Always Defines a Riemannian Metric As Long as it Exists,Why Ricci Flow Always Defines a Riemannian Metric As Long as it Exists,,"From what I understand, the Ricci flow \begin{equation} \frac{\partial g}{\partial t} = -2Ric(g) \end{equation} always defines a Riemannian metric as long as it exists. I know that the Riemannian condition is an open condition in the space of symmetric two-tensors, but is that all there is to this statement? I know that certain curvature-related quantities must blow up at the first singular time for the Ricci flow, but may this also be understood in terms of when the two-tensor solution for the Ricci flow looses its definiteness?","From what I understand, the Ricci flow always defines a Riemannian metric as long as it exists. I know that the Riemannian condition is an open condition in the space of symmetric two-tensors, but is that all there is to this statement? I know that certain curvature-related quantities must blow up at the first singular time for the Ricci flow, but may this also be understood in terms of when the two-tensor solution for the Ricci flow looses its definiteness?","\begin{equation}
\frac{\partial g}{\partial t} = -2Ric(g)
\end{equation}","['differential-geometry', 'ricci-flow']"
64,A System of Two Linear PDEs,A System of Two Linear PDEs,,"I'm researching on discrete/semi-discrete/smooth differential geometry. Recently, I could simplify one of my geometric problems (in the smooth scenario) into the solutions of a system of linear PDEs that is $$  e_v = L_u\,g,\quad\quad\quad g_u = L_v\,e $$ where $L = \ln(\tan{(\frac{\omega}{2})})$ and $e(u,v),g(u,v),\omega(u,v): [0,a]\times [0,b] \subset \mathbb{R}^2 \rightarrow \mathbb{R}$ are smooth functions on $(0,a)\times (0,b)$ . Furthermore, I have the functions $e(u,0)$ and $g(0,v)$ and $\omega(u,v)$ as well. My goal is to show that the system has a solution for $e(u,v)$ and $g(u,v)$ . Unfortunately, my knowledge of PDEs is very limited but just from observing the counterpart of the above system in discrete and semi-discrete scenarios I have this kind of feeling that there should be a solution for the above system (with the boundaries that I mentioned). Can someone clarify how I can prove that there exists a unique solution to the above system. I would very much appreciate if you can also go to details, describing non-characteristic curves and explaining why $e(u,0)$ and $g(0,v)$ are actually enough as boundary conditions (if they are). I think I also have to assume $e(u,v)$ and $g(u,v)$ are analytic functions. Is there any way to avoid this? Please let me know if you need more details so that I can edit the question.","I'm researching on discrete/semi-discrete/smooth differential geometry. Recently, I could simplify one of my geometric problems (in the smooth scenario) into the solutions of a system of linear PDEs that is where and are smooth functions on . Furthermore, I have the functions and and as well. My goal is to show that the system has a solution for and . Unfortunately, my knowledge of PDEs is very limited but just from observing the counterpart of the above system in discrete and semi-discrete scenarios I have this kind of feeling that there should be a solution for the above system (with the boundaries that I mentioned). Can someone clarify how I can prove that there exists a unique solution to the above system. I would very much appreciate if you can also go to details, describing non-characteristic curves and explaining why and are actually enough as boundary conditions (if they are). I think I also have to assume and are analytic functions. Is there any way to avoid this? Please let me know if you need more details so that I can edit the question."," 
e_v = L_u\,g,\quad\quad\quad g_u = L_v\,e
 L = \ln(\tan{(\frac{\omega}{2})}) e(u,v),g(u,v),\omega(u,v): [0,a]\times [0,b] \subset \mathbb{R}^2 \rightarrow \mathbb{R} (0,a)\times (0,b) e(u,0) g(0,v) \omega(u,v) e(u,v) g(u,v) e(u,0) g(0,v) e(u,v) g(u,v)","['differential-geometry', 'partial-differential-equations', 'systems-of-equations']"
65,Question about the existence of a diffeomorphism between nonsmooth domains in $\mathbb{R}^2$,Question about the existence of a diffeomorphism between nonsmooth domains in,\mathbb{R}^2,"Suppose $\Omega_1 \subset \mathbb{R}^2$ is an $n$ -sided convex polygon while $\Omega_2 \subset \mathbb{R}^2$ is simply connected with a piecewise smooth Lipschitz boundary comprised of $n$ smooth curves. My question is: under what circumstances is it possible to find a diffeomorphism in the neighbourhood of $\overline{\Omega}_1$ that, restricted to $\overline{\Omega}_1$ , maps $\overline{\Omega}_1$ onto $\overline{\Omega}_2$ (and hence $\partial \Omega_1$ onto $\partial \Omega_2$ ). Assigning the $n$ (straight) sides of $\partial \Omega_1$ to the $n$ sides of $\partial \Omega_2$ in, say, counter-clockwise orientation, my intuition tells me that a diffeomorphism could exist when the convex corners of $\partial \Omega_1$ are mapped onto convex corners of $\partial \Omega_2$ , i.e., the consecutive sides of $\partial \Omega_2$ must create a convex corner where they attach. Is my intuition correct ? If so, where can I find a proof ? In case my intuition fails me, is it possible to say something about the existence of a ""weak"" diffeomorphism, i.e., one that satisfies $\det Du(x) > 0$ almost everywhere in $\overline{\Omega}_1$ ?","Suppose is an -sided convex polygon while is simply connected with a piecewise smooth Lipschitz boundary comprised of smooth curves. My question is: under what circumstances is it possible to find a diffeomorphism in the neighbourhood of that, restricted to , maps onto (and hence onto ). Assigning the (straight) sides of to the sides of in, say, counter-clockwise orientation, my intuition tells me that a diffeomorphism could exist when the convex corners of are mapped onto convex corners of , i.e., the consecutive sides of must create a convex corner where they attach. Is my intuition correct ? If so, where can I find a proof ? In case my intuition fails me, is it possible to say something about the existence of a ""weak"" diffeomorphism, i.e., one that satisfies almost everywhere in ?",\Omega_1 \subset \mathbb{R}^2 n \Omega_2 \subset \mathbb{R}^2 n \overline{\Omega}_1 \overline{\Omega}_1 \overline{\Omega}_1 \overline{\Omega}_2 \partial \Omega_1 \partial \Omega_2 n \partial \Omega_1 n \partial \Omega_2 \partial \Omega_1 \partial \Omega_2 \partial \Omega_2 \det Du(x) > 0 \overline{\Omega}_1,"['differential-geometry', 'diffeomorphism']"
66,How to compute the exterior derivative of a 1-form on projectivization of a vector space,How to compute the exterior derivative of a 1-form on projectivization of a vector space,,"Let $V$ be a complex vector space, and let $\mathbb{P}(V)$ denote the projectivization of $V$ (i.e. space of 1-dimensional subspaces, i.e. 1st Grassmanian). Suppose further that $V$ is endowed with a non-degenerate two-form $\omega$ . We know that the tangent space $T_v\mathbb{P}(V)$ is canonically isomorphic to $V/\left<v\right>$ . We can thus define a 1-form $\alpha$ by the maps $$\alpha_v(\cdot)=\omega(v,\cdot),$$ which is well defined since $\omega(v,v)=0$ . My question is how I can explicitly compute the exterior derivative $d\alpha$ . The reason I ask this is that I am trying to do Exercise 10.1 in Voisin's ""Hodge theory and complex algebraic geometry i"", where they ask to show that the top dimensional form $\alpha \wedge (d\alpha)^{n-1}$ does not vanish at any point, and I feel that I can't do the exercise if I don't know how to compute $d\alpha$ . EDIT: Below is the exercise in question. Let $V$ be a complex vector space endowed with a non-degenerate 2-form $\omega$ . Recall that $T_{\mathbb{P}(V),v}$ is isomorphic to $V/\left<v\right>$ . Show that the form $\alpha$ defined (up to multiplicative coefficient) by $$\alpha_v(\cdot)=\omega(v,\cdot)$$ provides a contact structure on $\mathbb{P}(V)$ Contact structures are defined above as follows: Let $X$ be a complex manifold of dimension $2n-1$ a contact structure on $X$ is determined by the local datum of a holomorphic $1$ -form $\alpha$ which is well defined up to multiplication by an invertible holomorphic function and which satisfies the condition that the $(2n-1)$ -form $$\alpha\wedge (d\alpha)^{n-1}\in K_X$$ does not vanish at any point.","Let be a complex vector space, and let denote the projectivization of (i.e. space of 1-dimensional subspaces, i.e. 1st Grassmanian). Suppose further that is endowed with a non-degenerate two-form . We know that the tangent space is canonically isomorphic to . We can thus define a 1-form by the maps which is well defined since . My question is how I can explicitly compute the exterior derivative . The reason I ask this is that I am trying to do Exercise 10.1 in Voisin's ""Hodge theory and complex algebraic geometry i"", where they ask to show that the top dimensional form does not vanish at any point, and I feel that I can't do the exercise if I don't know how to compute . EDIT: Below is the exercise in question. Let be a complex vector space endowed with a non-degenerate 2-form . Recall that is isomorphic to . Show that the form defined (up to multiplicative coefficient) by provides a contact structure on Contact structures are defined above as follows: Let be a complex manifold of dimension a contact structure on is determined by the local datum of a holomorphic -form which is well defined up to multiplication by an invertible holomorphic function and which satisfies the condition that the -form does not vanish at any point.","V \mathbb{P}(V) V V \omega T_v\mathbb{P}(V) V/\left<v\right> \alpha \alpha_v(\cdot)=\omega(v,\cdot), \omega(v,v)=0 d\alpha \alpha \wedge (d\alpha)^{n-1} d\alpha V \omega T_{\mathbb{P}(V),v} V/\left<v\right> \alpha \alpha_v(\cdot)=\omega(v,\cdot) \mathbb{P}(V) X 2n-1 X 1 \alpha (2n-1) \alpha\wedge (d\alpha)^{n-1}\in K_X","['differential-geometry', 'complex-geometry', 'hodge-theory']"
67,What is the metric for this product manifold?,What is the metric for this product manifold?,,"Consider a spacetime $(\zeta^{3,1},g)$ where $$g=\frac{dudv}{uv}-\frac{dr^2}{r^2}-\frac{dw^2}{w^2} \quad \forall u,v,w,r \in (0,1)$$ Now this is just Minkowski space in different coordinates (related to Dirac/Light cone coordinates/null coordinates). I'm looking to take a Cauchy foliation of $\zeta^{3,1},$ change the metric back to $g'=dudv-dw^2-dr^2,$ and use the induced measure from $g'$ by means of the volume form to transform the foliated past light cone region onto a new manifold $(\Psi^{~3,1},d).$ I'm not sure how this will work so I will try to construct a $(1+1)$ dimensional example and then define a product manifold to acheive a $(3+1)$ dimensional spacetime. But the idea is similar. Let's look at the steps involved for the $(1+1)$ dimensional case $(\zeta^{1,1},u)$ where $u=\frac{dxdq}{xq}.$ The Cauchy foliation is simply $\ln(b)\ln(y)=t$ (solving for $y$ gives an explicit representation) which can then be transformed via Mellin-like transform: $$\Phi_s(t)= \int_{S=(0,1)} \exp {\frac{t s}{\ln b}}~db = \int_0^1 tb^{t-1} \exp \frac{s}{\ln b}~db = 2\sqrt{ts}K_1(2\sqrt{ts})$$ Observe that this is an unnormalized K-distribution and it's inverse transform yields an unnormalized distribution call it the ""Zeta-distribution"" (our Cauchy foliation). Observe that the Fisher information metric of the Zeta-distribution is $\Phi_s(t)$ up to a factor. Observe that the Zeta-distribution yields solutions to the Killing field $X=\langle x \ln x, -y\ln y \rangle.$ It also provides a distributional solution to the backwards heat equation with diffusivity depending on both space and time: $$\frac{\partial^2}{\partial t^2}\Phi(t,b)=-\frac{b}{t}\frac{\partial}{\partial b}\Phi(t,b)$$ In short, $\Phi_1(t)=v$ is a Lorentzian metric on the smooth $(1+0)$ manifold $(\Psi^{~1,0}, v).$ We have that $\Psi^{~3,1}=\Psi^{~1,1}\times \Psi^{~1,0} \times \Psi^{~1,0}$ so we can define the product metric on $\Psi^{~3,1}.$ What is the exact form of the metric (line element) for $(\Psi^{~3,1},v)?$ I believe it is of the form $v=v_1 \oplus v_2 \oplus v_3$ and should be expressed in terms of $K_1$ (Bessel functions). I think I am not too far off from getting the precise form. Am I on the right track here?","Consider a spacetime where Now this is just Minkowski space in different coordinates (related to Dirac/Light cone coordinates/null coordinates). I'm looking to take a Cauchy foliation of change the metric back to and use the induced measure from by means of the volume form to transform the foliated past light cone region onto a new manifold I'm not sure how this will work so I will try to construct a dimensional example and then define a product manifold to acheive a dimensional spacetime. But the idea is similar. Let's look at the steps involved for the dimensional case where The Cauchy foliation is simply (solving for gives an explicit representation) which can then be transformed via Mellin-like transform: Observe that this is an unnormalized K-distribution and it's inverse transform yields an unnormalized distribution call it the ""Zeta-distribution"" (our Cauchy foliation). Observe that the Fisher information metric of the Zeta-distribution is up to a factor. Observe that the Zeta-distribution yields solutions to the Killing field It also provides a distributional solution to the backwards heat equation with diffusivity depending on both space and time: In short, is a Lorentzian metric on the smooth manifold We have that so we can define the product metric on What is the exact form of the metric (line element) for I believe it is of the form and should be expressed in terms of (Bessel functions). I think I am not too far off from getting the precise form. Am I on the right track here?","(\zeta^{3,1},g) g=\frac{dudv}{uv}-\frac{dr^2}{r^2}-\frac{dw^2}{w^2} \quad \forall u,v,w,r \in (0,1) \zeta^{3,1}, g'=dudv-dw^2-dr^2, g' (\Psi^{~3,1},d). (1+1) (3+1) (1+1) (\zeta^{1,1},u) u=\frac{dxdq}{xq}. \ln(b)\ln(y)=t y \Phi_s(t)= \int_{S=(0,1)} \exp {\frac{t s}{\ln b}}~db = \int_0^1 tb^{t-1} \exp \frac{s}{\ln b}~db = 2\sqrt{ts}K_1(2\sqrt{ts}) \Phi_s(t) X=\langle x \ln x, -y\ln y \rangle. \frac{\partial^2}{\partial t^2}\Phi(t,b)=-\frac{b}{t}\frac{\partial}{\partial b}\Phi(t,b) \Phi_1(t)=v (1+0) (\Psi^{~1,0}, v). \Psi^{~3,1}=\Psi^{~1,1}\times \Psi^{~1,0} \times \Psi^{~1,0} \Psi^{~3,1}. (\Psi^{~3,1},v)? v=v_1 \oplus v_2 \oplus v_3 K_1","['differential-geometry', 'manifolds', 'physics', 'mathematical-physics', 'semi-riemannian-geometry']"
68,Stokes Theorem on manifolds with dense corners,Stokes Theorem on manifolds with dense corners,,"I am currently working on a project, where I would (ideally) like to apply Stokes theorem on a Manifold with corners. I have found various sources, which justify this application. Except one thing: In the book ""Stokes's Theorem and Whitney Manifolds"" by Anthony W. Knapp, on page 108, the author uses an indicator function $I_k(x)$ which is 1 on the subset $D(x,E)\geq2^{-k}$ and 0 elsewhere, where E is the set of ""exceptional points"" such as corners, and D is the distance function (exceptional points are the points on the boundary, which make the boundary non-differentiable). He later arrives at the conclusion, that, if E has the condition $$\operatorname*{lim}_{\delta\downarrow0}\;\delta^{-1}\vert\{x\in\mathbb{R}^{m}\mid D(x,E)\lt \delta\}=0,$$ then Stokes theorem $\int_{B-E}\omega=\int_{U}d\omega$ holds. The problem is that this proof doesn't work, if E is dense, due to the fact that the set of points which satisfies $D(x,E)\geq2^{-k}$ is empty. So, my question is: Is Stokes theorem applicable on manifolds with dense corners (or better, with a dense set of exceptional points)? If so, why/why not?","I am currently working on a project, where I would (ideally) like to apply Stokes theorem on a Manifold with corners. I have found various sources, which justify this application. Except one thing: In the book ""Stokes's Theorem and Whitney Manifolds"" by Anthony W. Knapp, on page 108, the author uses an indicator function which is 1 on the subset and 0 elsewhere, where E is the set of ""exceptional points"" such as corners, and D is the distance function (exceptional points are the points on the boundary, which make the boundary non-differentiable). He later arrives at the conclusion, that, if E has the condition then Stokes theorem holds. The problem is that this proof doesn't work, if E is dense, due to the fact that the set of points which satisfies is empty. So, my question is: Is Stokes theorem applicable on manifolds with dense corners (or better, with a dense set of exceptional points)? If so, why/why not?","I_k(x) D(x,E)\geq2^{-k} \operatorname*{lim}_{\delta\downarrow0}\;\delta^{-1}\vert\{x\in\mathbb{R}^{m}\mid D(x,E)\lt \delta\}=0, \int_{B-E}\omega=\int_{U}d\omega D(x,E)\geq2^{-k}","['integration', 'differential-geometry', 'stokes-theorem']"
69,Family of spin Dirac operators coming from path of metrics,Family of spin Dirac operators coming from path of metrics,,"Let $\mathbb{R}\ni t\mapsto g_t$ be a continuous path (in the $C^{\infty}$ -topology of $\Gamma(\text{Sym}^2(T^*M))$ ) of smooth metrics on a compact spin manifold $M$ . For each $t\in \mathbb{R}$ , we can define the Dirac operator $D_t$ associated to the metric $g_t$ over a spinor bundle $S_t\to M$ , in the usual way. I'm trying to understand under which conditions one can perceive these Dirac operators as defined on the same spinor bundle $S$ , with the final goal of showing that the map $t\mapsto D_t \in \mathcal{B}(W^1(S),L^2(S))$ is norm-continuous, where $W^1(S)$ denotes the first Sobolev space. I already know that for two conformally related metrics one can find an isometry between their spinor bundles, so this would definitely be enough, but too restrictive. I can see on Dependence of spinor bundle on choice of metric that the answer seems to be positive if one picks two metric $g_0$ and $g_1$ and looks at the line segment along them, but I cannot seem to be able to prove it. Any help on that would be appreciated!","Let be a continuous path (in the -topology of ) of smooth metrics on a compact spin manifold . For each , we can define the Dirac operator associated to the metric over a spinor bundle , in the usual way. I'm trying to understand under which conditions one can perceive these Dirac operators as defined on the same spinor bundle , with the final goal of showing that the map is norm-continuous, where denotes the first Sobolev space. I already know that for two conformally related metrics one can find an isometry between their spinor bundles, so this would definitely be enough, but too restrictive. I can see on Dependence of spinor bundle on choice of metric that the answer seems to be positive if one picks two metric and and looks at the line segment along them, but I cannot seem to be able to prove it. Any help on that would be appreciated!","\mathbb{R}\ni t\mapsto g_t C^{\infty} \Gamma(\text{Sym}^2(T^*M)) M t\in \mathbb{R} D_t g_t S_t\to M S t\mapsto D_t \in \mathcal{B}(W^1(S),L^2(S)) W^1(S) g_0 g_1","['differential-geometry', 'spin-geometry', 'elliptic-operators']"
70,Does the set of diffeomorphisms which are induced by flows form a group?,Does the set of diffeomorphisms which are induced by flows form a group?,,"Let $M$ be a smooth manifold.  Consider the set of diffeomorphisms which are induced by flows of vector fields. (which are not time-dependent) Is this set a subgroup of $\text{Diff}(M)$? (Note that not every diffeomorphism which is isotopic to the identity is induced by a flow of a vector field, see here for details). ""A naive attempt"": Maybe it's possible to construct a counter-example when taking $M=\mathbb{S}^2$. Every vector field on $\mathbb{S}^2$ vanishes at some point, hence every flow-diffeomorphism has a fixed point. Maybe we can find two vector fields, such that the composition of their flows is a diffeomorphism without fixed points.","Let $M$ be a smooth manifold.  Consider the set of diffeomorphisms which are induced by flows of vector fields. (which are not time-dependent) Is this set a subgroup of $\text{Diff}(M)$? (Note that not every diffeomorphism which is isotopic to the identity is induced by a flow of a vector field, see here for details). ""A naive attempt"": Maybe it's possible to construct a counter-example when taking $M=\mathbb{S}^2$. Every vector field on $\mathbb{S}^2$ vanishes at some point, hence every flow-diffeomorphism has a fixed point. Maybe we can find two vector fields, such that the composition of their flows is a diffeomorphism without fixed points.",,"['differential-topology', 'smooth-manifolds']"
71,What is the relation between Fubini-Study metric and the standard metric on hyperspheres?,What is the relation between Fubini-Study metric and the standard metric on hyperspheres?,,"The relevant Wikipedia page mentions that "" the Fubini-Study metric is the metric induced on the quotient $\mathbb{CP}^n=S^{2n+1}/S^1$ , where $S^{2n+1}$ carries the so-called ""round metric"" endowed upon it by restriction of the standard Euclidean metric to the unit hypersphere "". Furthermore, slightly above, they mention that the standard Hermitian metric on $\mathbb{C}^{n+1}$ , given by $ds^2=d\mathbf Z\otimes d\mathbf Z$ , is invariant under the diagonal action of $S^1$ , and therefore to derive the metric on $\mathbb{CP}^n$ we need only figure out the metric on $S^{2n+1}$ . I'm a bit confused by these statements. It would seem like we're saying that to derive the metric on $\mathbb{CP}^n$ all we need to do is use the standard metric on $S^{2n+1}\subset \mathbb{R}^{2n+2}$ , and this metric will automatically be invariant under the suitable action of $S^1$ , and thus define a metric on the projective space. But if I were to follow this reasoning, I'd just take a complex ray $\psi\in\mathbb{CP}^n$ , represent it as some real normalised vector $f(\psi)\in S^{2n+1}\subset\mathbb{R}^{2n+2}$ , and then define the metric between two variations $\delta_i\psi$ as the standard Euclidean inner product between tangent vectors of $S^{2n+1}$ . And because we're dealing with an inner product in a (subset of a) Euclidean space, shouldn't the metric just be $g(\partial_i,\partial_j)=\delta_{ij}$ on all points of the sphere? Granted, I know that here $\partial_i$ would be tangent vectors attached to a specific point, and there's no global way to define these with a single chart. Still, it would mean that the Fubini-Study metric is ""locally trivial"", which doesn't seem to be the case. I can see that part of the apparent complexity in the expressions for the Fubini-Study metric (as discussed e.g. here or here ) is probably due to expressing the metric in stereographic coordinates. But I can't quite figure out whether these expressions are also compatible with saying that the metric is indeed locally trivial (or if this is even true), and whether the metric is really just the standard metric on a hypersphere expressed in stereographic coordinates.","The relevant Wikipedia page mentions that "" the Fubini-Study metric is the metric induced on the quotient , where carries the so-called ""round metric"" endowed upon it by restriction of the standard Euclidean metric to the unit hypersphere "". Furthermore, slightly above, they mention that the standard Hermitian metric on , given by , is invariant under the diagonal action of , and therefore to derive the metric on we need only figure out the metric on . I'm a bit confused by these statements. It would seem like we're saying that to derive the metric on all we need to do is use the standard metric on , and this metric will automatically be invariant under the suitable action of , and thus define a metric on the projective space. But if I were to follow this reasoning, I'd just take a complex ray , represent it as some real normalised vector , and then define the metric between two variations as the standard Euclidean inner product between tangent vectors of . And because we're dealing with an inner product in a (subset of a) Euclidean space, shouldn't the metric just be on all points of the sphere? Granted, I know that here would be tangent vectors attached to a specific point, and there's no global way to define these with a single chart. Still, it would mean that the Fubini-Study metric is ""locally trivial"", which doesn't seem to be the case. I can see that part of the apparent complexity in the expressions for the Fubini-Study metric (as discussed e.g. here or here ) is probably due to expressing the metric in stereographic coordinates. But I can't quite figure out whether these expressions are also compatible with saying that the metric is indeed locally trivial (or if this is even true), and whether the metric is really just the standard metric on a hypersphere expressed in stereographic coordinates.","\mathbb{CP}^n=S^{2n+1}/S^1 S^{2n+1} \mathbb{C}^{n+1} ds^2=d\mathbf Z\otimes d\mathbf Z S^1 \mathbb{CP}^n S^{2n+1} \mathbb{CP}^n S^{2n+1}\subset \mathbb{R}^{2n+2} S^1 \psi\in\mathbb{CP}^n f(\psi)\in S^{2n+1}\subset\mathbb{R}^{2n+2} \delta_i\psi S^{2n+1} g(\partial_i,\partial_j)=\delta_{ij} \partial_i","['differential-geometry', 'riemannian-geometry', 'projective-space', 'information-geometry']"
72,"Is $\{(\mathbb R,\phi)\}$ a smooth atlas for $\mathbb R$, where $\phi(x)=\left\{ \begin{array}{ll} x & x<0, \\ 2x& x\geq0~? \end{array} \right.$","Is  a smooth atlas for , where","\{(\mathbb R,\phi)\} \mathbb R \phi(x)=\left\{ \begin{array}{ll} x & x<0, \\ 2x& x\geq0~? \end{array} \right.","Is $\{(\mathbb R,\phi)\}$ a smooth atlas for $\mathbb R$ , where $\phi(x)=\left\{ \begin{array}{ll} x & x<0,  \\ 2x& x\geq0~? \end{array} \right.$ Let $M$ be a topological $ n $ -manifold. A coordinate chart (or just a chart ) on $ M $ is a pair $ (U, \varphi) $ , where $ U $ is an open subset of $ M $ and $ \varphi: U \rightarrow \widehat{U} $ is a homeomorphism from $ U $ to an open subset $ \widehat{U}=\varphi(U) \subseteq \mathbb{R}^{n} $ . If $ U $ and $ V $ are open subsets of Euclidean spaces $ \mathbb{R}^{n} $ and $ \mathbb{R}^{m} $ , respectively, a function $ F: U \rightarrow V $ is said to be smooth if each of its component functions has continuous partial derivatives of all orders. If in addition $ F $ is bijective and has a smooth inverse map, it is called a diffeomorphism . Two charts $ (U, \varphi) $ and $ (V, \psi) $ are said to be smoothly compatible if either $ U \cap V=\varnothing $ or the transition map $ \psi \circ \varphi^{-1} $ is a diffeomorphism. An atlas for $ M $ is a collection of charts whose domains cover $ M $ . An atlas $ \mathcal{A} $ is called a smooth atlas if any two charts in $ \mathcal{A} $ are smoothly compatible with each other. I have carefully checked the definition of each concept related to this question, but I have not found anything contrary to a certain definition. Although it is obvious that $\phi$ itself is not smooth, my understanding is that the atlas $\{(\mathbb R,\phi)\}$ has only a single coordinate chart $(\mathbb R,\phi)$ , and no other coordinate chart to determine whether it is smoothly compatible with $(\mathbb R,\phi)$ . So, is it a smooth atlas? This is a question out of curiosity. Any help would be appreciated.","Is a smooth atlas for , where Let be a topological -manifold. A coordinate chart (or just a chart ) on is a pair , where is an open subset of and is a homeomorphism from to an open subset . If and are open subsets of Euclidean spaces and , respectively, a function is said to be smooth if each of its component functions has continuous partial derivatives of all orders. If in addition is bijective and has a smooth inverse map, it is called a diffeomorphism . Two charts and are said to be smoothly compatible if either or the transition map is a diffeomorphism. An atlas for is a collection of charts whose domains cover . An atlas is called a smooth atlas if any two charts in are smoothly compatible with each other. I have carefully checked the definition of each concept related to this question, but I have not found anything contrary to a certain definition. Although it is obvious that itself is not smooth, my understanding is that the atlas has only a single coordinate chart , and no other coordinate chart to determine whether it is smoothly compatible with . So, is it a smooth atlas? This is a question out of curiosity. Any help would be appreciated.","\{(\mathbb R,\phi)\} \mathbb R \phi(x)=\left\{ \begin{array}{ll} x & x<0,  \\ 2x& x\geq0~? \end{array} \right. M  n   M   (U, \varphi)   U   M   \varphi: U \rightarrow \widehat{U}   U   \widehat{U}=\varphi(U) \subseteq \mathbb{R}^{n}   U   V   \mathbb{R}^{n}   \mathbb{R}^{m}   F: U \rightarrow V   F   (U, \varphi)   (V, \psi)   U \cap V=\varnothing   \psi \circ \varphi^{-1}   M   M   \mathcal{A}   \mathcal{A}  \phi \{(\mathbb R,\phi)\} (\mathbb R,\phi) (\mathbb R,\phi)","['differential-geometry', 'manifolds', 'differential-topology', 'smooth-manifolds']"
73,How to define linearization of a dynamical system on a manifold with affine connection?,How to define linearization of a dynamical system on a manifold with affine connection?,,"In Euclidean space, if I have a smooth dynamical system $\dot{x}=F(x)$ , it's linearization about a solution $x(t)$ is $\dot{v}(t) = DF(x(t))v(t)$ , where $DF(x)$ is the Jacobian matrix of $F$ at $x$ . I am curious on how linearization works on smooth manifolds. Suppose $(M,\nabla)$ is a smooth manifold with affine connection $\nabla$ and $\dot{p}=F(p)$ describes a dynamical system on $M$ , where $F:M \to TM$ is a smooth vector field. What exactly is the ""right way"" to define the linearization of the system about a solution $p(t)$ ? By right, I guess the most intrinsic-to-the-manifold way. In the Euclidean set up, recall $DF(x)v$ can be interpreted as the directional derivative of $F$ along $v$ at $x$ . This leads me to say that the linearization of $\dot{p}=F(p)$ about the solution $p(t)$ is $\dot{v}(t) = \nabla_{v(t)} F|_{p(t)}$ . However, this seems a bit odd.  Under those dynamics (if they are even well-defined) $v(t) \in T_{p(t)}M$ . So, $v(t)$ is a tangent vector constantly varying through the tangent spaces. Is this the right way to do it? Is this even well-defined? I guess if $(E_i)$ is a local frame and $(\Gamma_{ij}^k)$ are the Christoffel symbols, we can write locally $\dot{v}^k = v(F^k)+v^iF^j(p)\Gamma_{ij}^k(p)$ .","In Euclidean space, if I have a smooth dynamical system , it's linearization about a solution is , where is the Jacobian matrix of at . I am curious on how linearization works on smooth manifolds. Suppose is a smooth manifold with affine connection and describes a dynamical system on , where is a smooth vector field. What exactly is the ""right way"" to define the linearization of the system about a solution ? By right, I guess the most intrinsic-to-the-manifold way. In the Euclidean set up, recall can be interpreted as the directional derivative of along at . This leads me to say that the linearization of about the solution is . However, this seems a bit odd.  Under those dynamics (if they are even well-defined) . So, is a tangent vector constantly varying through the tangent spaces. Is this the right way to do it? Is this even well-defined? I guess if is a local frame and are the Christoffel symbols, we can write locally .","\dot{x}=F(x) x(t) \dot{v}(t) = DF(x(t))v(t) DF(x) F x (M,\nabla) \nabla \dot{p}=F(p) M F:M \to TM p(t) DF(x)v F v x \dot{p}=F(p) p(t) \dot{v}(t) = \nabla_{v(t)} F|_{p(t)} v(t) \in T_{p(t)}M v(t) (E_i) (\Gamma_{ij}^k) \dot{v}^k = v(F^k)+v^iF^j(p)\Gamma_{ij}^k(p)","['differential-geometry', 'dynamical-systems', 'smooth-manifolds', 'linearization']"
74,Notion of `normal bundle' of a submanifold in a Finsler manifold,Notion of `normal bundle' of a submanifold in a Finsler manifold,,"Given a submanifold $N$ in a Riemannian manifold $(M, g)$ , we have the notion of normal bundle $$\nu(N) := \{(p,v) \in TM \; | \; p \in N, \, g(v, w) = 0 \, \forall w \in T_p N\}.$$ It is well-known that a distance minimimizing geodesic from $N$ has to start in the normal bundle. That is, if $\gamma :[0,1] \to M$ is a unit speed geodesic satisfying $\gamma(0) \in N$ and $\text{dist}(N, \gamma(t)) = t$ for some time $t \in [0,t_0] \subset[0,1]$ , then $\dot\gamma(0) \in \nu(N)|_{\gamma(0)}$ . Here $\text{dist}(\_,\_)$ is the induced distance. Now, suppose we have a Finsler manifold $(M,F)$ and a submanifold $N \subset M$ . Is there any notion of 'normal bundle' of $N$ here, so that the initial velocity of any distance minimizing geodesic from $N$ must lie in this normal bundle? Any references or comments regarding this will be highly appreciated. Cheers!","Given a submanifold in a Riemannian manifold , we have the notion of normal bundle It is well-known that a distance minimimizing geodesic from has to start in the normal bundle. That is, if is a unit speed geodesic satisfying and for some time , then . Here is the induced distance. Now, suppose we have a Finsler manifold and a submanifold . Is there any notion of 'normal bundle' of here, so that the initial velocity of any distance minimizing geodesic from must lie in this normal bundle? Any references or comments regarding this will be highly appreciated. Cheers!","N (M, g) \nu(N) := \{(p,v) \in TM \; | \; p \in N, \, g(v, w) = 0 \, \forall w \in T_p N\}. N \gamma :[0,1] \to M \gamma(0) \in N \text{dist}(N, \gamma(t)) = t t \in [0,t_0] \subset[0,1] \dot\gamma(0) \in \nu(N)|_{\gamma(0)} \text{dist}(\_,\_) (M,F) N \subset M N N","['differential-geometry', 'riemannian-geometry', 'finsler-geometry']"
75,What exactly is Geometric Analysis?,What exactly is Geometric Analysis?,,"I don't really know if here's a good place to ask this question, but I'd appreciate it if someone can guide me here so that I can choose the right path going forward! I recently got my masters degree and now I am trying to figure out what is best for me to do in my PhD and I have bunch of questions about Geometric Analysis and would be happy if professionals can help me through this! What exactly is geometric analysis and what's the difference between this field and Differential Geometry? What are the fundamental courses you need to go through to be called a geometric analyst? What are its applications? Is it mostly applied in physics or does it also have some applications in the industry? If so, what are some examples of the projects in this field and can one apply it to machine learning for instance? Thanks for everyone's help!","I don't really know if here's a good place to ask this question, but I'd appreciate it if someone can guide me here so that I can choose the right path going forward! I recently got my masters degree and now I am trying to figure out what is best for me to do in my PhD and I have bunch of questions about Geometric Analysis and would be happy if professionals can help me through this! What exactly is geometric analysis and what's the difference between this field and Differential Geometry? What are the fundamental courses you need to go through to be called a geometric analyst? What are its applications? Is it mostly applied in physics or does it also have some applications in the industry? If so, what are some examples of the projects in this field and can one apply it to machine learning for instance? Thanks for everyone's help!",,"['differential-geometry', 'soft-question']"
76,Can we think of the Riemann curvature tensor as a tensor-valued 2-form and integrate it over a surface?,Can we think of the Riemann curvature tensor as a tensor-valued 2-form and integrate it over a surface?,,"The usual physical interpretation of the Reimann tensor $R^\mu_{\ \ \nu \rho \sigma}$ at a given point $p$ on a manifold $M$ is that it inputs an infinitesimal vector $v^\nu$ and two other infinitesimal vectors $x^\rho$ and $x^\sigma$ , the latter of which determine an infinitesimal parallelogram-shaped loop in $M$ anchored at $p$ , and outputs the amount by which $v^\nu$ changes after being parallel-transported around the infinitesimal loop. That is, I believe it gives the holonomy of infinitesimal oriented loops with base point $p$ lying in the $x^\rho$ - $x^\sigma$ plane. There are other, equivalent ways to think about what type of mathematical object the Riemann curvature tensor is. E.g. one way to think of it as a self-adjoint operator on the real inner product space of two-forms. (This interpretation is arguably more natural for the ""all-lowered"" version $R_{\mu \nu \rho \sigma}$ .) Is it also possible to think of the Riemann tensor as a linear-operator-valued two-form in its last two indices $\rho$ and $\sigma$ , and therefore integrate it over a 2D surface in the manifold? I've only ever seen the Riemann tensor described in terms of infinitesimal loops in the manifold. But is it legitimate to think of it as a two-form and integrate it (over $\rho$ and $\sigma$ ) over a finite surface $S$ , and get the holonomy of the oriented boundary loop $\partial S$ ? On the one hand, this seems plausible by the general heuristic picture for Green's theorem: On the other hand, I doubt that it's correct, because (a) I can't find any sources saying that it is, and (b) more concretely, it seems to me that the holonomy of an oriented loop $\partial S$ should depend on the base point (although I'm not sure about that). But the formal integral $$ \iint_S R^\mu_{\ \ \nu \rho \sigma} dx^\rho \wedge dx^\sigma $$ does not pick out any distinguished point on $\partial S$ . Is there any sense in which this is a legitimate operation? If not, is there any other way to construct the (pseudo-)Riemannian holonomy of arbitrary oriented loops (with base point) in $M$ from the Riemann tensor? Or does that require knowing the connection and/or the full metric tensor? I know very little about tensor-valued differential forms, so this might all be complete nonsense. I suspect the fact that the Riemann tensor is an actual tensor means that this doesn't work, because for tensor-valued forms the ""form"" indices and the ""tensor"" indices somehow transform differently under coordinate transformations. Edit: Apparently the Riemann tensor can be thought of as a Lie-algebra-valued two-form (that takes values in $\mathfrak{so}(n)$ ) called the curvature form . I don't know how integration of Lie-algebra-valued differential forms works, though.","The usual physical interpretation of the Reimann tensor at a given point on a manifold is that it inputs an infinitesimal vector and two other infinitesimal vectors and , the latter of which determine an infinitesimal parallelogram-shaped loop in anchored at , and outputs the amount by which changes after being parallel-transported around the infinitesimal loop. That is, I believe it gives the holonomy of infinitesimal oriented loops with base point lying in the - plane. There are other, equivalent ways to think about what type of mathematical object the Riemann curvature tensor is. E.g. one way to think of it as a self-adjoint operator on the real inner product space of two-forms. (This interpretation is arguably more natural for the ""all-lowered"" version .) Is it also possible to think of the Riemann tensor as a linear-operator-valued two-form in its last two indices and , and therefore integrate it over a 2D surface in the manifold? I've only ever seen the Riemann tensor described in terms of infinitesimal loops in the manifold. But is it legitimate to think of it as a two-form and integrate it (over and ) over a finite surface , and get the holonomy of the oriented boundary loop ? On the one hand, this seems plausible by the general heuristic picture for Green's theorem: On the other hand, I doubt that it's correct, because (a) I can't find any sources saying that it is, and (b) more concretely, it seems to me that the holonomy of an oriented loop should depend on the base point (although I'm not sure about that). But the formal integral does not pick out any distinguished point on . Is there any sense in which this is a legitimate operation? If not, is there any other way to construct the (pseudo-)Riemannian holonomy of arbitrary oriented loops (with base point) in from the Riemann tensor? Or does that require knowing the connection and/or the full metric tensor? I know very little about tensor-valued differential forms, so this might all be complete nonsense. I suspect the fact that the Riemann tensor is an actual tensor means that this doesn't work, because for tensor-valued forms the ""form"" indices and the ""tensor"" indices somehow transform differently under coordinate transformations. Edit: Apparently the Riemann tensor can be thought of as a Lie-algebra-valued two-form (that takes values in ) called the curvature form . I don't know how integration of Lie-algebra-valued differential forms works, though.","R^\mu_{\ \ \nu \rho \sigma} p M v^\nu x^\rho x^\sigma M p v^\nu p x^\rho x^\sigma R_{\mu \nu \rho \sigma} \rho \sigma \rho \sigma S \partial S \partial S 
\iint_S R^\mu_{\ \ \nu \rho \sigma} dx^\rho \wedge dx^\sigma
 \partial S M \mathfrak{so}(n)","['differential-geometry', 'differential-forms', 'curvature', 'holonomy']"
77,Maximum symmetry metric on Cayley Plane $ F_4/Spin(9) $,Maximum symmetry metric on Cayley Plane, F_4/Spin(9) ,"The maximum symmetry metric on real projective space $ \mathbb{RP}^n $ is the round metric. The maximum symmetry metric on complex projective space $ \mathbb{CP}^n $ is the Fubini-Study metric. https://mathoverflow.net/questions/433847/maximum-symmetry-metric-on-mathbbcpn Let $ M $ be a compact connected manifold. The degree of symmetry of $ M $ , denoted $ N(M) $ , is the maximum of the dimensions of the isometry groups of all possible Riemannian structures on $ M $ . See for example https://www.ams.org/journals/tran/1969-146-00/S0002-9947-1969-0250340-1/S0002-9947-1969-0250340-1.pdf Two metrics are considered to be equivalent if they are isometric up to a constant multiple. I'm interested in manifolds $ M $ for which there is a unique up to equivalence metric with isometry group of dimension $ N(M) $ . Is the pushforward of the biinvariant metric of $ F_4 $ onto the Cayley projective plane $$ \mathbb{OP}^2 \cong F_4/Spin(9) $$ a maximum symmetry metric in this sense?","The maximum symmetry metric on real projective space is the round metric. The maximum symmetry metric on complex projective space is the Fubini-Study metric. https://mathoverflow.net/questions/433847/maximum-symmetry-metric-on-mathbbcpn Let be a compact connected manifold. The degree of symmetry of , denoted , is the maximum of the dimensions of the isometry groups of all possible Riemannian structures on . See for example https://www.ams.org/journals/tran/1969-146-00/S0002-9947-1969-0250340-1/S0002-9947-1969-0250340-1.pdf Two metrics are considered to be equivalent if they are isometric up to a constant multiple. I'm interested in manifolds for which there is a unique up to equivalence metric with isometry group of dimension . Is the pushforward of the biinvariant metric of onto the Cayley projective plane a maximum symmetry metric in this sense?"," \mathbb{RP}^n   \mathbb{CP}^n   M   M   N(M)   M   M   N(M)   F_4  
\mathbb{OP}^2 \cong F_4/Spin(9)
","['differential-geometry', 'riemannian-geometry', 'lie-groups', 'symmetric-spaces']"
78,$1$-forms integrating to integers and circle-valued maps,-forms integrating to integers and circle-valued maps,1,"I'm looking for a reference for the following. Fix a connected, closed differentiable manifold $M$ . Let $d\theta$ be the angle form $S^1$ . On one hand, given a differentiable map $f : M \to S^1$ , we get a closed $1$ -form $f^*(d\theta)$ ; this $1$ -form has the property that it integrates to an integer on any closed path. On the other hand, given a closed $1$ -form $\omega$ on $M$ with the property that it integrates to an integer on any closed path, one can get a differentiable map $f : M \to S^1$ as follows: pick $x \in M$ arbitrarily, and for all $y \in M$ , pick any path $p : [0,1] \to M$ from $x$ to $y$ and let $$     f(y) = \left(\int_0^1 \omega_{p(t)}(p'(t)) dt\right) \mod \mathbb{Z}, $$ where $\mod \mathbb{Z}$ refers to the fact that I am defining $S^1 = \mathbb{R}/\mathbb{Z}$ . This defines a bijection between the set of closed $1$ -forms of $M$ that integrate to an integer on any closed path, on one hand, and the set of differentiable maps $M \to S^1$ modulo rotations of $S^1$ , on the other hand.","I'm looking for a reference for the following. Fix a connected, closed differentiable manifold . Let be the angle form . On one hand, given a differentiable map , we get a closed -form ; this -form has the property that it integrates to an integer on any closed path. On the other hand, given a closed -form on with the property that it integrates to an integer on any closed path, one can get a differentiable map as follows: pick arbitrarily, and for all , pick any path from to and let where refers to the fact that I am defining . This defines a bijection between the set of closed -forms of that integrate to an integer on any closed path, on one hand, and the set of differentiable maps modulo rotations of , on the other hand.","M d\theta S^1 f : M \to S^1 1 f^*(d\theta) 1 1 \omega M f : M \to S^1 x \in M y \in M p : [0,1] \to M x y 
    f(y) = \left(\int_0^1 \omega_{p(t)}(p'(t)) dt\right) \mod \mathbb{Z},
 \mod \mathbb{Z} S^1 = \mathbb{R}/\mathbb{Z} 1 M M \to S^1 S^1","['differential-geometry', 'reference-request', 'differential-forms']"
79,Compute the gradient of polar basis vectors in tensor calculus,Compute the gradient of polar basis vectors in tensor calculus,,"$(1)$ Compute the gradient of polar basis vectors, \begin{equation} \tilde{\nabla} e_\rho=\frac{1}{\rho} \widetilde{e}_\rho \otimes e_\theta \text { and } \tilde{\nabla} e_\theta=\frac{1}{\rho} \tilde{e}^\rho \otimes e_\theta-\tilde{\rho} \tilde{e}^\theta \otimes e_\rho \end{equation} $(2)$ What are the gradients of the vectors $e_\rho$ and $e_\theta$ on a cylinder? (Hints: A cylinder is a surface of constant $\rho$ in cylindrical coordinates in $\mathbb{R}$ ) I know, \begin{align} [g_{ij}] =  \begin{pmatrix} g_{rr} & g_{r\theta}\\ g_{\theta r} & g_{\theta \theta} \end{pmatrix} =  \begin{pmatrix} 1 & 0 \\ 0 & r^2 \end{pmatrix} \end{align} \begin{align} [g^{ij}] =  \begin{pmatrix} g^{rr} & g^{r\theta}\\ g^{\theta r} & g^{\theta \theta} \end{pmatrix} =  \begin{pmatrix} 1 & 0 \\ 0 & 1/r^2 \end{pmatrix} \end{align} and gradient in polar coordinates, \begin{align} \text{grad}(f) &= g^{rr}\dfrac{\partial f}{\partial r}\dfrac{\partial }{\partial r} + g^{\theta \theta}\dfrac{\partial f}{\partial \theta}\dfrac{\partial }{\partial \theta} \\ &= \dfrac{\partial f}{\partial r}\dfrac{\partial }{\partial r} + \dfrac{1}{r^2}\dfrac{\partial f}{\partial \theta}\dfrac{\partial }{\partial \theta} \tag{$**$} \end{align} But I didn't see from these information how can I compute the given one? From here I knew that there are two kind of basis. Covariant basis and orthonormal basis (which we mostly familiar with linear algebra). From the question context I couldn't understand how to interpret which basis was used to compute the gradient. Any help will be appreciated. TIA. As @Paul Sinclair suggested, I need to use gradient for vector field. but what I get was scalar gradient in my book only. I have the following definitions in my book . $$ \begin{align} \text{grad}(\phi) &= \phi_{,i} = \frac{\partial \phi}{\partial x^{i}}\\ \text{div}(A^{i}) &= A^{i}_{,i} = \frac{1}{\sqrt g}\frac{\partial }{\partial x^{k}}(\sqrt g A^k)\\ \text{curl}(A_i) &= A_{i,j} - A_{j,i} \end{align} $$ Where I see any of them can't help me to get the solution. And I was wondering why $\text{curl}$ operator only defined for covariant tensor? Where $\text{div}$ defined for both covariant and contravarint tensor $\text{div}(A^{i})=\text{div}(A_{i})$ What bother me a lot that from vector calculus we taught the operations grad (which maps functions to vector fields), curl (which maps vector fields to vector fields), and div (which maps vector fields to functions). But in tensor calculus we are talking about gradient of vector field, isn't that seem wrong?","Compute the gradient of polar basis vectors, What are the gradients of the vectors and on a cylinder? (Hints: A cylinder is a surface of constant in cylindrical coordinates in ) I know, and gradient in polar coordinates, But I didn't see from these information how can I compute the given one? From here I knew that there are two kind of basis. Covariant basis and orthonormal basis (which we mostly familiar with linear algebra). From the question context I couldn't understand how to interpret which basis was used to compute the gradient. Any help will be appreciated. TIA. As @Paul Sinclair suggested, I need to use gradient for vector field. but what I get was scalar gradient in my book only. I have the following definitions in my book . Where I see any of them can't help me to get the solution. And I was wondering why operator only defined for covariant tensor? Where defined for both covariant and contravarint tensor What bother me a lot that from vector calculus we taught the operations grad (which maps functions to vector fields), curl (which maps vector fields to vector fields), and div (which maps vector fields to functions). But in tensor calculus we are talking about gradient of vector field, isn't that seem wrong?","(1) \begin{equation}
\tilde{\nabla} e_\rho=\frac{1}{\rho} \widetilde{e}_\rho \otimes e_\theta \text { and } \tilde{\nabla} e_\theta=\frac{1}{\rho} \tilde{e}^\rho \otimes e_\theta-\tilde{\rho} \tilde{e}^\theta \otimes e_\rho
\end{equation} (2) e_\rho e_\theta \rho \mathbb{R} \begin{align}
[g_{ij}] = 
\begin{pmatrix}
g_{rr} & g_{r\theta}\\
g_{\theta r} & g_{\theta \theta}
\end{pmatrix} = 
\begin{pmatrix}
1 & 0 \\
0 & r^2
\end{pmatrix}
\end{align} \begin{align}
[g^{ij}] = 
\begin{pmatrix}
g^{rr} & g^{r\theta}\\
g^{\theta r} & g^{\theta \theta}
\end{pmatrix} = 
\begin{pmatrix}
1 & 0 \\
0 & 1/r^2
\end{pmatrix}
\end{align} \begin{align}
\text{grad}(f) &= g^{rr}\dfrac{\partial f}{\partial r}\dfrac{\partial }{\partial r} + g^{\theta \theta}\dfrac{\partial f}{\partial \theta}\dfrac{\partial }{\partial \theta} \\
&= \dfrac{\partial f}{\partial r}\dfrac{\partial }{\partial r} + \dfrac{1}{r^2}\dfrac{\partial f}{\partial \theta}\dfrac{\partial }{\partial \theta} \tag{**}
\end{align} 
\begin{align}
\text{grad}(\phi) &= \phi_{,i} = \frac{\partial \phi}{\partial x^{i}}\\
\text{div}(A^{i}) &= A^{i}_{,i} = \frac{1}{\sqrt g}\frac{\partial }{\partial x^{k}}(\sqrt g A^k)\\
\text{curl}(A_i) &= A_{i,j} - A_{j,i}
\end{align}
 \text{curl} \text{div} \text{div}(A^{i})=\text{div}(A_{i})","['differential-geometry', 'tensors', 'polar-coordinates']"
80,Understanding differential forms on manifolds (Bachman example 52),Understanding differential forms on manifolds (Bachman example 52),,"In David Bachman's ""A Geometric Approach to Differential Forms"", in example 52, he lists what integrating a particular 1-form is like on $S^1$ . My question is, how does he calculate the ranges for the integrals that he does? Example 52 Let $S^1$ , $U_i$ , $\phi_i$ , and $w$ be defined as in Examples 49 and 50. A partition of unity subordinate to the cover ${\phi_i(U_i)}$ is as follows. \begin{align*} f_1(x,y) = y^2 \text{ if } y \geq 0 & \text{ else } 0 \\ f_2(x,y) = y^2 \text{ if } y < 0 & \text{ else } 0 \\ f_3(x,y) = x^2 \text{ if } x \geq 0 & \text{ else } 0 \\ f_4(x,y) = x^2 \text{ if } x < 0 & \text{ else } 0. \end{align*} (Check this!) Let $\mu: [0, \pi] \rightarrow S^1$ be defined by $\mu(\theta) = (\cos(\theta), \sin(\theta))$ . Then the image of $\mu$ is a 1-cell, $\sigma$ , in $S^1$ . Let's integrate $w$ over $\sigma$ . $$ \int_\sigma \omega = \sum_{i=1}^4 \int_{\phi^{-1}(\sigma)} \phi_i^*(f_iw). $$ In the next step he shows, $$ \int_{-(-1,1)} -\sqrt{1-t^2}dt + 0 + \int_{[0,1)} \sqrt{1-t^2}dt + \int_{-[0,1)} -\sqrt{1-t^2}dt. $$ In general, I do not understand how he gets the range of the four integrals. I would expect it to all be $(-1,1)$ . Since $\sigma$ is the entire $S^1$ image and the partitions of unity would limit the integrand to be valid only for the points in $S^1$ that each $\phi_i$ outputs to, I don't see why it wouldn't always be $(-1,1)$ . References $$ \omega = -y\ dx + x\ dy $$ Let $U_i = (-1,1)$ for $i=1,2,3,4$ . \begin{align*} \phi_1(t) ={} & (t, \sqrt{1-t^2}) \\ \phi_2(t) ={} & (t, -\sqrt{1-t^2}) \\ \phi_3(t) ={} & (\sqrt{1-t^2}, t) \\ \phi_4(t) ={} & (-\sqrt{1-t^2},t). \end{align*} \begin{align*} \phi_1^* \omega = \phi_4^* \omega  ={} & \frac{-1}{\sqrt{1-t^2}}dt \\ \phi_2^* \omega =\phi_3^* \omega ={} & \frac{-1}{\sqrt{1-t^2}}dt. \end{align*}","In David Bachman's ""A Geometric Approach to Differential Forms"", in example 52, he lists what integrating a particular 1-form is like on . My question is, how does he calculate the ranges for the integrals that he does? Example 52 Let , , , and be defined as in Examples 49 and 50. A partition of unity subordinate to the cover is as follows. (Check this!) Let be defined by . Then the image of is a 1-cell, , in . Let's integrate over . In the next step he shows, In general, I do not understand how he gets the range of the four integrals. I would expect it to all be . Since is the entire image and the partitions of unity would limit the integrand to be valid only for the points in that each outputs to, I don't see why it wouldn't always be . References Let for .","S^1 S^1 U_i \phi_i w {\phi_i(U_i)} \begin{align*}
f_1(x,y) = y^2 \text{ if } y \geq 0 & \text{ else } 0
\\
f_2(x,y) = y^2 \text{ if } y < 0 & \text{ else } 0
\\
f_3(x,y) = x^2 \text{ if } x \geq 0 & \text{ else } 0
\\
f_4(x,y) = x^2 \text{ if } x < 0 & \text{ else } 0.
\end{align*} \mu: [0, \pi] \rightarrow S^1 \mu(\theta) = (\cos(\theta), \sin(\theta)) \mu \sigma S^1 w \sigma 
\int_\sigma \omega = \sum_{i=1}^4 \int_{\phi^{-1}(\sigma)} \phi_i^*(f_iw).
 
\int_{-(-1,1)} -\sqrt{1-t^2}dt + 0 + \int_{[0,1)} \sqrt{1-t^2}dt + \int_{-[0,1)} -\sqrt{1-t^2}dt.
 (-1,1) \sigma S^1 S^1 \phi_i (-1,1) 
\omega = -y\ dx + x\ dy
 U_i = (-1,1) i=1,2,3,4 \begin{align*}
\phi_1(t) ={} & (t, \sqrt{1-t^2}) \\
\phi_2(t) ={} & (t, -\sqrt{1-t^2}) \\
\phi_3(t) ={} & (\sqrt{1-t^2}, t) \\
\phi_4(t) ={} & (-\sqrt{1-t^2},t).
\end{align*} \begin{align*}
\phi_1^* \omega = \phi_4^* \omega  ={} & \frac{-1}{\sqrt{1-t^2}}dt
\\
\phi_2^* \omega =\phi_3^* \omega ={} & \frac{-1}{\sqrt{1-t^2}}dt.
\end{align*}","['differential-geometry', 'differential-forms']"
81,Is $w$ Killing for some semi-Riemannian metric?,Is  Killing for some semi-Riemannian metric?,w,"Consider the vector field $w=(x\log x,-y\log y,-z\log z)$ for $0<x,y,z<1.$ I'm wondering if $w$ is Killing for some semi-Riemannian metric. If we consider a lower dimensional version of $w$ i.e. $v=(x\log x,-y\log y)$ then it's not hard to show that $v$ is Killing and preserves $g=\frac{dxdy}{xy}.$ Furthermore it can be shown that the pair $(M,g)$ is equivalent to $\Bbb M^{1,1}$ (Minkowski plane). A quick sketch of how to see the equivalence is to solve for the integral curves of $v$ and then inspect them, revealing that they are rectangular hyperbolae under a change in coordinates. Or you can start with the Minkowski metric in standard coordinates, perform a rotation, then pushforward the metric to get $g.$ A similar approach with $w$ yields integral curves of the form $(e^{-t_1e^{s}},e^{-t_2e^{-s}},e^{-t_3e^{-s}})$ parametrized by $s\in (-\infty,\infty)$ and parameters $t_1,t_2,t_3.$ Inspecting this parametrisation leads me to believe that these integral curves foliate a semi-Riemannian manifold that I think would be equivalent to $\Bbb M^{2,1}.$ Am I on the right track to showing that the integral curves of $w$ foliate a semi-Riemannian manifold equivalent to a component of Minkowski $3-$ space?","Consider the vector field for I'm wondering if is Killing for some semi-Riemannian metric. If we consider a lower dimensional version of i.e. then it's not hard to show that is Killing and preserves Furthermore it can be shown that the pair is equivalent to (Minkowski plane). A quick sketch of how to see the equivalence is to solve for the integral curves of and then inspect them, revealing that they are rectangular hyperbolae under a change in coordinates. Or you can start with the Minkowski metric in standard coordinates, perform a rotation, then pushforward the metric to get A similar approach with yields integral curves of the form parametrized by and parameters Inspecting this parametrisation leads me to believe that these integral curves foliate a semi-Riemannian manifold that I think would be equivalent to Am I on the right track to showing that the integral curves of foliate a semi-Riemannian manifold equivalent to a component of Minkowski space?","w=(x\log x,-y\log y,-z\log z) 0<x,y,z<1. w w v=(x\log x,-y\log y) v g=\frac{dxdy}{xy}. (M,g) \Bbb M^{1,1} v g. w (e^{-t_1e^{s}},e^{-t_2e^{-s}},e^{-t_3e^{-s}}) s\in (-\infty,\infty) t_1,t_2,t_3. \Bbb M^{2,1}. w 3-","['differential-geometry', 'vector-fields', 'semi-riemannian-geometry']"
82,Regular submanifolds in practice.,Regular submanifolds in practice.,,"A subset $S$ of a smooth $m$ -manifold $M$ is called regular submanifold of codimension $k$ if for every $p\in S$ , there is a coordinate neighborhood $(U, \phi)= (U, x^1, \dots, x^m)$ in the atlas of $M$ such that $x^1 = \dots = x^k =0$ on $U \cap S$ . Then, there are theorems like: Let $g: M\to \mathbb{R}$ be a $C^\infty$ -function. Then a non-empty regular level set $S=g^{-1}(c)$ is a regular submanifold of $M$ of codimension $1$ . Question : (From someone ignorant about differential geometry!) What is the practical use of knowing that a certain subset $S\subseteq M$ is a regular submanifold? We know it is a manifold in some natural way, but do we actually understand its structure? It is my understanding that the charts of such a submanifold are constructed using abstract results like the inverse function theorem, so we don't really understand how an atlas of this submanifold looks like. Say, for example, we are given a regular submanifold $S\subseteq M$ which we know is a submanifold by the above theorem. We have a function $S \to N$ where $N$ is some other manifold. We don't have an explicit atlas for $S$ , so how do we for example show that the function $S\to N$ is smooth?","A subset of a smooth -manifold is called regular submanifold of codimension if for every , there is a coordinate neighborhood in the atlas of such that on . Then, there are theorems like: Let be a -function. Then a non-empty regular level set is a regular submanifold of of codimension . Question : (From someone ignorant about differential geometry!) What is the practical use of knowing that a certain subset is a regular submanifold? We know it is a manifold in some natural way, but do we actually understand its structure? It is my understanding that the charts of such a submanifold are constructed using abstract results like the inverse function theorem, so we don't really understand how an atlas of this submanifold looks like. Say, for example, we are given a regular submanifold which we know is a submanifold by the above theorem. We have a function where is some other manifold. We don't have an explicit atlas for , so how do we for example show that the function is smooth?","S m M k p\in S (U, \phi)= (U, x^1, \dots, x^m) M x^1 = \dots = x^k =0 U \cap S g: M\to \mathbb{R} C^\infty S=g^{-1}(c) M 1 S\subseteq M S\subseteq M S \to N N S S\to N","['differential-geometry', 'manifolds', 'smooth-manifolds', 'submanifold']"
83,"Geometric interpretation of the Lie bracket: Sign of the ""gap""?","Geometric interpretation of the Lie bracket: Sign of the ""gap""?",,"The short of it: Concerning Lie brackets of vector fields: Who's right with the sign of the ""gap vector""? Does $[V=d/d\mu, W=d/d\lambda]$ point from $A$ to $B$ or from $B$ to $A$ ? Adapted from Schutz's ""Geometrical methods in mathematical physics"": But in other sources the sign differs, e.g. Hehl 2007 : Who is right? The long of it: I'm reading Bernard Schutz's ""Geometrical methods in mathematical physics"", which -- being a physicist myself -- strikes a great balance for me between explanatory detail and rigor. I'd like to prefix my question with some things that I think I understand, so that it's easier for others to point to where I might have erred. I added two figures to illustrate what I'm trying to get to. I cannot find an answer to my specific question, though I have seen similar arguments as the following posted. I can understand how vector fields can be seen as operators acting on functions. The coordinate representation of the Lie bracket $[V, W] = \left(V^j \frac{W^i}{\partial x_j} - W^j \frac{V^i}{\partial x_j} \right)\partial_i$ is clear to me and using the product rule, I can derive the ""product rule"" $[fX, gY] = fg[X, Y] + fX(Y) - gY(f)X$ . Schutz describes the geometric interpretation of the Lie bracket by using the exponentiation of vector fields that describe finite motions along integral curves: Let $V = d / d\lambda$ and $W = d / d \mu$ be vector fields. Starting at a point $P$ , moving $\Delta \lambda = \epsilon$ along the V curve through $P$ to a point $R$ , and then moving $\Delta \mu = \epsilon$ along a W curve, ending at a point $A$ . Then start again at $P$ , go first $\Delta \mu$ along $W$ and then $\Delta \lambda$ along $V$ ending at a point $B \neq A$ . The vector from $A$ to be $B$ is $\epsilon^2[V, W]$ , to lowest order in $\varepsilon$ , since $x^i(R) = \exp\left(\epsilon \frac{d}{d\lambda}\right)x^i|_P \\ x^i(A) = \exp\left(\epsilon \frac{d}{d\mu}\right) \exp\left(\epsilon \frac{d}{d\lambda}\right)x^i|_P$ and analogously $x^i(B) = \exp\left(\epsilon \frac{d}{d\lambda}\right) \exp\left(\epsilon \frac{d}{d\mu}\right)x^i|_P$ . Using the Taylor series of the exponentials yields $x^i(B) - x^i(A) = \left[\exp\left(\epsilon \frac{d}{d\lambda}\right) \exp\left(\epsilon \frac{d}{d\mu}\right), \exp\left(\epsilon \frac{d}{d\mu}\right) \exp\left(\epsilon \frac{d}{d\lambda}\right)\right]x^i|_P \\  = \epsilon^2\left[V, W\right]|_P$ . This makes sense to me, the exponentials act from the left on $x^i$ so the rightmost exponential acts first.  You from $P$ to $A$ via $WV$ and you get from $P$ to $B$ via $VW$ . So all of this should be easy to verify with an example, right? Lets take polar coordinates $(r, \phi)$ in 2d and three vector fields: $V := \cos(\phi) \partial_x + \sin(\phi) \partial_y = \frac{x}{r} \partial_x + \frac{y}{r} = \partial_r\\ W := -r\sin(\phi) \partial_x + r \cos(\phi) \partial_y = -y \partial_x + x \partial_y = \partial_{\phi}\\ Z := -\sin(\phi) \partial_x +  \cos(\phi) \partial_y = \frac{1}{r} \partial_{\phi}$ The vector fields $V$ and $W$ are the usual ones for polar coordinates and form a coordinate basis, since they commute with each other: $[V, W] = 0$ (just plug in the definition of the Lie bracket and use the product rule for the Lie bracket together with commuting partial derivatives $[\partial_x, \partial_y] = 0$ . Ok. Using the product rule for Lie brackets yet again yields straightforwardly: $[V, Z] = [V, \frac{1}{r} W] = \frac{1}{r} [V, W] + \frac{\partial r^{-1}}{\partial_r} \partial_\phi = -\frac{1}{r^2} \partial_\phi= -\frac{1}{r} Z \neq 0$ I get the same result by separately computing $V(Z) = ... = 0$ and $Z(V) = \frac{1}{r^2}\partial_\phi$ and again $[V, Z] = -\frac{1}{r^2} \partial_\phi$ . So the vector field $[V, Z]$ points opposite to $\partial_\phi$ . Finally my question: How does this connect to $[V, Z]$ pointing from $A$ to $B$ ? Following the above reasoning of following the flows, I get to $A$ by following first $V$ and then $Z$ , so as an operator it's $ZV$ ; and I get to $B$ by following first $Z$ and then $V$ , so as an operator it's $VZ$ . Sketching this for small $\epsilon$ (or large radii so that the circle segments look almost like straight lines), starting at $P = (x=1, y=0)$ results in $B$ being almost directly above $A$ , so $\vec B - \vec A = \epsilon^2 [V, Z] + O(\epsilon^4)$ points almost completely in positive $y$ direction. But from my calculations I see that $[V, Z] = - \frac{1}{r^2}\partial_\phi$ so evaluated at $P = (1, 0)$ this yields $-\partial_y$ . Exactly opposite of what I expected! I can follow the derivation of why the Lie bracket points from $A$ to $B$ , but the example yields the opposite! Where am I going wrong? Sorry for the rather long post, but I think my question may be unclear without context. Thanks for your help!","The short of it: Concerning Lie brackets of vector fields: Who's right with the sign of the ""gap vector""? Does point from to or from to ? Adapted from Schutz's ""Geometrical methods in mathematical physics"": But in other sources the sign differs, e.g. Hehl 2007 : Who is right? The long of it: I'm reading Bernard Schutz's ""Geometrical methods in mathematical physics"", which -- being a physicist myself -- strikes a great balance for me between explanatory detail and rigor. I'd like to prefix my question with some things that I think I understand, so that it's easier for others to point to where I might have erred. I added two figures to illustrate what I'm trying to get to. I cannot find an answer to my specific question, though I have seen similar arguments as the following posted. I can understand how vector fields can be seen as operators acting on functions. The coordinate representation of the Lie bracket is clear to me and using the product rule, I can derive the ""product rule"" . Schutz describes the geometric interpretation of the Lie bracket by using the exponentiation of vector fields that describe finite motions along integral curves: Let and be vector fields. Starting at a point , moving along the V curve through to a point , and then moving along a W curve, ending at a point . Then start again at , go first along and then along ending at a point . The vector from to be is , to lowest order in , since and analogously . Using the Taylor series of the exponentials yields . This makes sense to me, the exponentials act from the left on so the rightmost exponential acts first.  You from to via and you get from to via . So all of this should be easy to verify with an example, right? Lets take polar coordinates in 2d and three vector fields: The vector fields and are the usual ones for polar coordinates and form a coordinate basis, since they commute with each other: (just plug in the definition of the Lie bracket and use the product rule for the Lie bracket together with commuting partial derivatives . Ok. Using the product rule for Lie brackets yet again yields straightforwardly: I get the same result by separately computing and and again . So the vector field points opposite to . Finally my question: How does this connect to pointing from to ? Following the above reasoning of following the flows, I get to by following first and then , so as an operator it's ; and I get to by following first and then , so as an operator it's . Sketching this for small (or large radii so that the circle segments look almost like straight lines), starting at results in being almost directly above , so points almost completely in positive direction. But from my calculations I see that so evaluated at this yields . Exactly opposite of what I expected! I can follow the derivation of why the Lie bracket points from to , but the example yields the opposite! Where am I going wrong? Sorry for the rather long post, but I think my question may be unclear without context. Thanks for your help!","[V=d/d\mu, W=d/d\lambda] A B B A [V, W] = \left(V^j \frac{W^i}{\partial x_j} - W^j \frac{V^i}{\partial x_j} \right)\partial_i [fX, gY] = fg[X, Y] + fX(Y) - gY(f)X V = d / d\lambda W = d / d \mu P \Delta \lambda = \epsilon P R \Delta \mu = \epsilon A P \Delta \mu W \Delta \lambda V B \neq A A B \epsilon^2[V, W] \varepsilon x^i(R) = \exp\left(\epsilon \frac{d}{d\lambda}\right)x^i|_P \\
x^i(A) = \exp\left(\epsilon \frac{d}{d\mu}\right) \exp\left(\epsilon \frac{d}{d\lambda}\right)x^i|_P x^i(B) = \exp\left(\epsilon \frac{d}{d\lambda}\right) \exp\left(\epsilon \frac{d}{d\mu}\right)x^i|_P x^i(B) - x^i(A) = \left[\exp\left(\epsilon \frac{d}{d\lambda}\right) \exp\left(\epsilon \frac{d}{d\mu}\right), \exp\left(\epsilon \frac{d}{d\mu}\right) \exp\left(\epsilon \frac{d}{d\lambda}\right)\right]x^i|_P \\
 = \epsilon^2\left[V, W\right]|_P x^i P A WV P B VW (r, \phi) V := \cos(\phi) \partial_x + \sin(\phi) \partial_y = \frac{x}{r} \partial_x + \frac{y}{r} = \partial_r\\
W := -r\sin(\phi) \partial_x + r \cos(\phi) \partial_y = -y \partial_x + x \partial_y = \partial_{\phi}\\
Z := -\sin(\phi) \partial_x +  \cos(\phi) \partial_y = \frac{1}{r} \partial_{\phi} V W [V, W] = 0 [\partial_x, \partial_y] = 0 [V, Z] = [V, \frac{1}{r} W] = \frac{1}{r} [V, W] + \frac{\partial r^{-1}}{\partial_r} \partial_\phi = -\frac{1}{r^2} \partial_\phi= -\frac{1}{r} Z \neq 0 V(Z) = ... = 0 Z(V) = \frac{1}{r^2}\partial_\phi [V, Z] = -\frac{1}{r^2} \partial_\phi [V, Z] \partial_\phi [V, Z] A B A V Z ZV B Z V VZ \epsilon P = (x=1, y=0) B A \vec B - \vec A = \epsilon^2 [V, Z] + O(\epsilon^4) y [V, Z] = - \frac{1}{r^2}\partial_\phi P = (1, 0) -\partial_y A B","['differential-geometry', 'vector-fields', 'visualization', 'lie-derivative']"
84,When is the leaf of a foliation the level set of a function?,When is the leaf of a foliation the level set of a function?,,"Suppose I have a smooth (say $C^1$ ) codimension one foliation of $P^n$ (open subset of $R^n$ consisting of vectors with all positive components) arising from a smooth $(n-1)$ -plane field satisfying the Frobenius conditions. Now take a single leaf $L$ ; I would like to show that $L$ is the graph of a function defined on some open subset of $P^{n-1}$ . If I knew that $L$ was the level set of some function $F$ on $P^n$ , say $F(x_1,\ldots,x_n) = 0, \forall x \in P^n$ , and I knew that $F_n(p) > 0 \; \forall p \in L$ , then I could use the ""basic"" implicit function theorem to conclude that there exists a function $u$ such that $F(x_1,\ldots,x_{n-1},u(x_1,\ldots,u_{n-1})) = 0$ locally, then proceed to extend it since I know $F_n(p) > 0$ everywhere on $L$ . My question is: Do I know that $L$ is the level set of some function $F$ ? For reasons outside the scope of this question I do know that the normal vector to the tangent plane to $L$ , at every point of $L$ , has $n$ th component $> 0$ (I can compute this manually since I know the vector fields giving rise to the distribution), but to me that doesn't seem to make things directly amenable to an application of the implicit function theorem. I have a feeling that there is some way to look at this which makes the answer to my question very obvious (and the question itself silly), but I have tried to work this out and being very obsessive about my proofs I wanted to ask here. Any info or advice appreciated. Edit to add information. : I understand that $L$ , being an integral manifold, is a graph, locally, say on some domain $U$ . I believe my condition that the last component of the normal to the tangent plane, at every point $p \in L$ , means that $U$ can be extended to a maximal domain $D$ by means of the implicit function theorem. I just am searching for the proper application of that theorem given that I don't know that $L$ can be  given by a function of the form $F(x_1,\ldots,x_n) = 0$ . Edit to clarify my objective I want to show that any leaf $L$ of a particular foliation (arising from a specific $n$ -plane distribution on $P^n$ ) is the graph of some function $u : D \rightarrow P$ where $D$ is an open subset of $P^{n-1}$ . This specific distribution gives me the condition on the normals mentioned in the previous edit. I believe the way to prove that is to show that any neighborhood $U$ on which we know $L$ is a graph (there has to be one since an integral manifold is locally a graph) can always be extended to a maximal domain unless $u$ goes to infinity or zero on its boundary, and I believe the way to do that is via the implicit function theorem, utilizing the condition on the normals. I just don't know how to prove it this way. Edit to give specifics : Let $n=3$ ; I have two strictly positive, $C^2$ functions on $P^3$ , $m^1(x,y,z)$ and $m^2(x,y,z)$ . The vector fields are $X^1 = \partial/\partial x - m^1 \partial/\partial z$ and $X^2 = \partial/\partial y - m^2 \partial/\partial z$ and I impose  the following condition on the derivatives of $m^1,m^2$ : $ -\partial m^2/\partial x + \partial m^1/\partial y + m^1 \partial m^2/\partial z - m^2 \partial m^1/\partial z = 0$ . This should make the $2$ plane distribution generated by $X,Y$ involutive and so by Frobenious there is a smooth foliation $\mathscr{F}$ of $P^3$ by integral manifolds of the distribution. The normal vector to the plane tangent to a leaf $L$ at a point $p$ can be computed as $\pm(m^1(p),m^2(p),1)$ , I take the positive normal. My understanding is that under these conditions (in particular, that the normal has strictly positive last component at every point of $P^3$ ) each leaf $L \in \mathscr{F}$ is the graph of some function $u : D \rightarrow P$ , where $D$ is some open set of $P^2$ . I am looking for details on proving (if possible) my understanding, and my idea was to use the implicit function theorem.","Suppose I have a smooth (say ) codimension one foliation of (open subset of consisting of vectors with all positive components) arising from a smooth -plane field satisfying the Frobenius conditions. Now take a single leaf ; I would like to show that is the graph of a function defined on some open subset of . If I knew that was the level set of some function on , say , and I knew that , then I could use the ""basic"" implicit function theorem to conclude that there exists a function such that locally, then proceed to extend it since I know everywhere on . My question is: Do I know that is the level set of some function ? For reasons outside the scope of this question I do know that the normal vector to the tangent plane to , at every point of , has th component (I can compute this manually since I know the vector fields giving rise to the distribution), but to me that doesn't seem to make things directly amenable to an application of the implicit function theorem. I have a feeling that there is some way to look at this which makes the answer to my question very obvious (and the question itself silly), but I have tried to work this out and being very obsessive about my proofs I wanted to ask here. Any info or advice appreciated. Edit to add information. : I understand that , being an integral manifold, is a graph, locally, say on some domain . I believe my condition that the last component of the normal to the tangent plane, at every point , means that can be extended to a maximal domain by means of the implicit function theorem. I just am searching for the proper application of that theorem given that I don't know that can be  given by a function of the form . Edit to clarify my objective I want to show that any leaf of a particular foliation (arising from a specific -plane distribution on ) is the graph of some function where is an open subset of . This specific distribution gives me the condition on the normals mentioned in the previous edit. I believe the way to prove that is to show that any neighborhood on which we know is a graph (there has to be one since an integral manifold is locally a graph) can always be extended to a maximal domain unless goes to infinity or zero on its boundary, and I believe the way to do that is via the implicit function theorem, utilizing the condition on the normals. I just don't know how to prove it this way. Edit to give specifics : Let ; I have two strictly positive, functions on , and . The vector fields are and and I impose  the following condition on the derivatives of : . This should make the plane distribution generated by involutive and so by Frobenious there is a smooth foliation of by integral manifolds of the distribution. The normal vector to the plane tangent to a leaf at a point can be computed as , I take the positive normal. My understanding is that under these conditions (in particular, that the normal has strictly positive last component at every point of ) each leaf is the graph of some function , where is some open set of . I am looking for details on proving (if possible) my understanding, and my idea was to use the implicit function theorem.","C^1 P^n R^n (n-1) L L P^{n-1} L F P^n F(x_1,\ldots,x_n) = 0, \forall x \in P^n F_n(p) > 0 \; \forall p \in L u F(x_1,\ldots,x_{n-1},u(x_1,\ldots,u_{n-1})) = 0 F_n(p) > 0 L L F L L n > 0 L U p \in L U D L F(x_1,\ldots,x_n) = 0 L n P^n u : D \rightarrow P D P^{n-1} U L u n=3 C^2 P^3 m^1(x,y,z) m^2(x,y,z) X^1 = \partial/\partial x - m^1 \partial/\partial z X^2 = \partial/\partial y - m^2 \partial/\partial z m^1,m^2  -\partial m^2/\partial x + \partial m^1/\partial y + m^1 \partial m^2/\partial z - m^2 \partial m^1/\partial z = 0 2 X,Y \mathscr{F} P^3 L p \pm(m^1(p),m^2(p),1) P^3 L \in \mathscr{F} u : D \rightarrow P D P^2","['differential-geometry', 'foliations']"
85,Divergence of tensor fields,Divergence of tensor fields,,"I have found numerous definitions for the divergence of a tensor which makes me confused as to trust which one to use. In Itskov's Tensor Algebra and Tensor Analysis for Engineers , he begins with Gauss's theorem to define \begin{equation} \text{div} ~\boldsymbol{S} = \lim_{V \to 0} \frac{1}{V} \int_{\partial V} \boldsymbol{S} ~\boldsymbol{n} ~da \end{equation} which, resorting to some coordinates system, gives \begin{equation} \text{div} ~\boldsymbol{S} = \boldsymbol{S}_{,i} ~\boldsymbol{g}^i = S_{j}^{~~i} |_i ~\boldsymbol{g}^j \end{equation} I actually like this definition because of its naturalness from beginning with Gauss's theorem. However, it requires choosing a basis. To define a coordinate-free divergence, I have come across multiple definitions: One from this wiki article defines the divergence as \begin{equation} (\boldsymbol{\nabla \cdot S}) \boldsymbol{\cdot a}  =   \boldsymbol{\nabla \cdot} ~(\boldsymbol{S ~a}) \end{equation} where $\boldsymbol{a}$ is an arbitrary constant vector. This gives \begin{equation} \boldsymbol{\nabla \cdot S}  =  S^{i}_{~~j} |_i ~\boldsymbol{g}^j \end{equation} where the first index is contracted. Yet, another wiki article defines \begin{equation}   (\boldsymbol{\nabla}\cdot\boldsymbol{T})\cdot\mathbf{c} =     \boldsymbol{\nabla}\cdot\left(\mathbf{c}\cdot\boldsymbol{T}^\textsf{T}\right) \end{equation} to give the exact same result as the other wiki article. (Here I presume that Reddy's notation were used, where he uses dot product for denoting any product he can find! One problem with Reddy's notation is that I cannot figure out how he dot products a vector into a dyad, as in $\boldsymbol{e}_k \cdot \boldsymbol{e}_i\otimes\boldsymbol{e}_j$ , so please do not advise me using his notation. This being said, I don't know what $\mathbf{c}\cdot\boldsymbol{T}^\textsf{T}$ means; is it $\mathbf{c}~\boldsymbol{T}^\textsf{T}$ where $\boldsymbol{T}^\textsf{T}$ is acting from the left on the vector $\mathbf{c}$ ? If so, I don't think this holds for a general curvilinear basis. I guess $\mathbf{c}~\boldsymbol{\cdot}\boldsymbol{T}^\textsf{T} = \boldsymbol{c}^\textsf{T}\boldsymbol{T}^\textsf{T}$ is more appropreate, but I don't reckon Reddy means this way.) This article also says that Itskov's result (contracting the second index) is actually true only for symmetric tensors, which Itskov never assumes. Abeyratne's lecture notes (p. 64) uses this definition \begin{equation} (\text{div} ~\boldsymbol{T})\cdot\mathbf{c} =     \text{div} \left(\boldsymbol{T}^\textsf{T}\mathbf{c}\right) \end{equation} where he claims that the second index gets contracted. I don't know whether $\text{div}$ and $\boldsymbol{\nabla \cdot}$ are different or the same. Ogden's ""Nonlinear Elastic deformations"" puts it in a very nice way: that there are three possible contractions for the gradient of a 2nd rank tensor $\boldsymbol{\nabla}\otimes \boldsymbol{T}$ , so defining the divergence is a matter of convention. He contracts the first index. But still, which one should one choose for a throughout consistency in his calculations. What is the definition of divergence? Kelly's lecture notes were a little helpful, yet because of its different notations from other, I always get caught wondering if I am doing the right way. For example, he finds for the gradient of a tensor field that $\text{grad}~\boldsymbol{v} = (\boldsymbol{\nabla}\otimes\boldsymbol{v})^\textsf{T}$ , but Ogden finds it with the transpose, and I believe they have used the same definitions to start with, namely the directional derivative. This will make much mess for me, as to define the divergence of the vector field whether as $\boldsymbol{\nabla\cdot v} = \text{tr} (\boldsymbol{\nabla}\otimes\boldsymbol{v})^\textsf{T}$ or as $\boldsymbol{\nabla\cdot v} = \text{tr} (\boldsymbol{\nabla}\otimes\boldsymbol{v})$ . Please help me organize my mind on the subject, and share with me your experience regarding the same notation conflictions and how you have overcome them.","I have found numerous definitions for the divergence of a tensor which makes me confused as to trust which one to use. In Itskov's Tensor Algebra and Tensor Analysis for Engineers , he begins with Gauss's theorem to define which, resorting to some coordinates system, gives I actually like this definition because of its naturalness from beginning with Gauss's theorem. However, it requires choosing a basis. To define a coordinate-free divergence, I have come across multiple definitions: One from this wiki article defines the divergence as where is an arbitrary constant vector. This gives where the first index is contracted. Yet, another wiki article defines to give the exact same result as the other wiki article. (Here I presume that Reddy's notation were used, where he uses dot product for denoting any product he can find! One problem with Reddy's notation is that I cannot figure out how he dot products a vector into a dyad, as in , so please do not advise me using his notation. This being said, I don't know what means; is it where is acting from the left on the vector ? If so, I don't think this holds for a general curvilinear basis. I guess is more appropreate, but I don't reckon Reddy means this way.) This article also says that Itskov's result (contracting the second index) is actually true only for symmetric tensors, which Itskov never assumes. Abeyratne's lecture notes (p. 64) uses this definition where he claims that the second index gets contracted. I don't know whether and are different or the same. Ogden's ""Nonlinear Elastic deformations"" puts it in a very nice way: that there are three possible contractions for the gradient of a 2nd rank tensor , so defining the divergence is a matter of convention. He contracts the first index. But still, which one should one choose for a throughout consistency in his calculations. What is the definition of divergence? Kelly's lecture notes were a little helpful, yet because of its different notations from other, I always get caught wondering if I am doing the right way. For example, he finds for the gradient of a tensor field that , but Ogden finds it with the transpose, and I believe they have used the same definitions to start with, namely the directional derivative. This will make much mess for me, as to define the divergence of the vector field whether as or as . Please help me organize my mind on the subject, and share with me your experience regarding the same notation conflictions and how you have overcome them.","\begin{equation}
\text{div} ~\boldsymbol{S} = \lim_{V \to 0} \frac{1}{V} \int_{\partial V} \boldsymbol{S} ~\boldsymbol{n} ~da
\end{equation} \begin{equation}
\text{div} ~\boldsymbol{S} = \boldsymbol{S}_{,i} ~\boldsymbol{g}^i = S_{j}^{~~i} |_i ~\boldsymbol{g}^j
\end{equation} \begin{equation}
(\boldsymbol{\nabla \cdot S}) \boldsymbol{\cdot a}  =   \boldsymbol{\nabla \cdot} ~(\boldsymbol{S ~a})
\end{equation} \boldsymbol{a} \begin{equation}
\boldsymbol{\nabla \cdot S}  =  S^{i}_{~~j} |_i ~\boldsymbol{g}^j
\end{equation} \begin{equation}
  (\boldsymbol{\nabla}\cdot\boldsymbol{T})\cdot\mathbf{c} =
    \boldsymbol{\nabla}\cdot\left(\mathbf{c}\cdot\boldsymbol{T}^\textsf{T}\right)
\end{equation} \boldsymbol{e}_k \cdot \boldsymbol{e}_i\otimes\boldsymbol{e}_j \mathbf{c}\cdot\boldsymbol{T}^\textsf{T} \mathbf{c}~\boldsymbol{T}^\textsf{T} \boldsymbol{T}^\textsf{T} \mathbf{c} \mathbf{c}~\boldsymbol{\cdot}\boldsymbol{T}^\textsf{T} = \boldsymbol{c}^\textsf{T}\boldsymbol{T}^\textsf{T} \begin{equation}
(\text{div} ~\boldsymbol{T})\cdot\mathbf{c} =
    \text{div} \left(\boldsymbol{T}^\textsf{T}\mathbf{c}\right)
\end{equation} \text{div} \boldsymbol{\nabla \cdot} \boldsymbol{\nabla}\otimes \boldsymbol{T} \text{grad}~\boldsymbol{v} = (\boldsymbol{\nabla}\otimes\boldsymbol{v})^\textsf{T} \boldsymbol{\nabla\cdot v} = \text{tr} (\boldsymbol{\nabla}\otimes\boldsymbol{v})^\textsf{T} \boldsymbol{\nabla\cdot v} = \text{tr} (\boldsymbol{\nabla}\otimes\boldsymbol{v})","['differential-geometry', 'differential-forms', 'continuum-theory']"
86,bound on the distance of an isometry to the identity,bound on the distance of an isometry to the identity,,"Consider the space $$ \mathbb{S}^{3,1} := \{ x \in \mathbb{R}^5 : (x_1)^2 + (x_2)^2 + (x_3)^2 +(x_4)^2=1+(x_5)^2 \} $$ with the lorentz metric on $\mathbb{R}^5$ : $$ \langle x,y\rangle _{lor} := x_1 y_1 + x_2y_2 + x_3 y_3 +x_4 y_4 - x_5 y_5 $$ We denote $SO(4,1)$ the group of matrices that preserve this lorentzian scalar product. In particular, they are isometries of $\mathbb{S}^{3,1}$ . On $\mathbb{S}^{3,1}$ , I consider the quantity (which is a kind of lorentzian distance) $$ d(x,y) = \inf \left\{ \int_0^1 |\langle \dot{\gamma}(t), \dot{\gamma}(t) \rangle_{lor}|^{1/2} dt\ \Big| \ \gamma \in C^1([0,1]; \mathbb{S}^{3,1}),\ \gamma(0)=x,\ \gamma(1)=y \right\} $$ The question is : is the quantity $\sup_{x\in \mathbb{S}^{3,1}} d(Mx,x)$ finite ? And can we show that there exists $C>1$ such that for any $M\in SO(4,1)$ : $$ C^{-1} |M-I_5| <\sup_{x\in \mathbb{S}^{3,1}} d(Mx,x) < C|M-I_5| $$ where $|\cdot|$ is some matrix norm, and $I_5$ is the identity matrix ? An other question is : if we have a given Riemannian manifold $(X,g)$ with an isometry $F : X\to X$ , can we estimate $\text{dist}(F(x),x)$ in terms of $F$ and any geometric quantity of $X$ , in the case where $X$ is a homogeneous space or a Lie group ?","Consider the space with the lorentz metric on : We denote the group of matrices that preserve this lorentzian scalar product. In particular, they are isometries of . On , I consider the quantity (which is a kind of lorentzian distance) The question is : is the quantity finite ? And can we show that there exists such that for any : where is some matrix norm, and is the identity matrix ? An other question is : if we have a given Riemannian manifold with an isometry , can we estimate in terms of and any geometric quantity of , in the case where is a homogeneous space or a Lie group ?","
\mathbb{S}^{3,1} := \{ x \in \mathbb{R}^5 : (x_1)^2 + (x_2)^2 + (x_3)^2 +(x_4)^2=1+(x_5)^2 \}
 \mathbb{R}^5 
\langle x,y\rangle _{lor} := x_1 y_1 + x_2y_2 + x_3 y_3 +x_4 y_4 - x_5 y_5
 SO(4,1) \mathbb{S}^{3,1} \mathbb{S}^{3,1} 
d(x,y) = \inf \left\{ \int_0^1 |\langle \dot{\gamma}(t), \dot{\gamma}(t) \rangle_{lor}|^{1/2} dt\ \Big| \ \gamma \in C^1([0,1]; \mathbb{S}^{3,1}),\ \gamma(0)=x,\ \gamma(1)=y \right\}
 \sup_{x\in \mathbb{S}^{3,1}} d(Mx,x) C>1 M\in SO(4,1) 
C^{-1} |M-I_5| <\sup_{x\in \mathbb{S}^{3,1}} d(Mx,x) < C|M-I_5|
 |\cdot| I_5 (X,g) F : X\to X \text{dist}(F(x),x) F X X","['differential-geometry', 'isometry', 'semi-riemannian-geometry']"
87,Evolution of the energy density under the harmonic map heat flow (The Ricci Flow in Riemannian Geometry),Evolution of the energy density under the harmonic map heat flow (The Ricci Flow in Riemannian Geometry),,"I am reading now the book The Ricci Flow in Riemannian Geometry by Ben Andrews and Christopher Hopper. It has been a while since I did not use pullback bundles and other objects, and they still look complicated to me. I have trouble regarding sections 3.2.1 and 3.2.2. First let me introduce some notations and context: They consider a vector subbundle of $T(M \times I)$ , where $I \subset \mathbb{R}$ is a time interval, consisting of vector tangents to the first factor $M$ in the sense that $(\pi_2)_* = 0$ . On that bundle, a connection $\nabla$ is defined via the one on $M$ denoted by $^M\nabla$ augmented by its effect on the time component $\nabla_{\partial_t} u = [\partial_t, u]$ for vectors on this bundle, and extended by zero outside the bundle. We are given a smooth map between Riemannian manifolds $f : (M, g) \to (N, h)$ and consider its time dependent variant $f : M \times  I \to N$ . The map induces the pullback bundle $f^*TN$ over $M \times I$ , and consider on it the pullback connection. The Energy $E$ is defined as the total density $$E(f) = \frac{1}{2}\int_M e(f) \: d\mu(g) = \frac{1}{2}\int_M\langle f_*, f_*\rangle \: d\mu(g)$$ We use coordinates $\{x_i\}$ on $M \times I$ and $\{y^\alpha\}$ on $N$ . We also use the indices to denote components. For instance in coordinates, we have $f_* = (f_*)_i^\alpha dx^i \otimes (\partial_\alpha)_f$ . I have a question regarding the motivation behind the gradient descent flow : By a direct computation as done in section 3.2.1, the authors show that $$ \frac{d}{dt} E(f) = -\int_M \langle f_*\partial_t, \Delta_{g,h}f\rangle \: d\mu(g). $$ Then they go on like this Note that $f_*\partial_t$ is the variation of $f$ . Hence the gradient of E, with respect to the inner product of $f^*TN$ , is $-\Delta_{g,h}f$ and the gradient descent flow is $$ f_*\partial_t = \Delta_{g,h}f.$$ First, I do not understand why is the gradient of $E$ is $-\Delta_{g,h}f$ . Second, how does this imply the following of the statement? I have another question regarding the evolution of the energy density : The authors show that the evolution equation for $f_*$ is given by $$ (\Delta_{\partial_t} f_*)(\partial_i) = (\Delta_* f)(\partial_i) + g^{kl}R^N(f_*\partial_k, f_*\partial_i)(f_*\partial_l) - g^{kl}f_*(R^M(\partial_k, \partial_l)).$$ From that, they infer that $$\frac{d}{dt}e = \langle f_*, \Delta f_*\rangle + g^{kl}g^{ij}R^N(f_*\partial_k, f_*\partial_i, f_*\partial_l, f_*\partial_j) - g^{kl}h_f\left(f_*\left((^M\mathrm{Ric})_k^{\:\,m}\partial_p\right), f_*\partial_l\right). $$ Here is my attempt for the second term : By unravelling the definition for the scalar product, we get \begin{multline} g^{kl}g^{ij}(h_{\alpha\beta})_f(f_*)_i^\alpha[R^N(f_*\partial_k, f_*\cdot)(f_*\partial_l)]_j^\beta = g^{kl}g^{ij}h_f\left((f_*)_i,R^N(f_*\partial_k, f_*\partial_j)(f_*\partial_l)\right) = g^{kl}g^{ij}R^N(f_*\partial_k, f_*\partial_j, f_*\partial_l, f_*\partial_i).\end{multline} For the third term. Here is my attempt : By unravelling again, we get \begin{multline} g^{kl}g^{ij}(h_{\alpha\beta})_f (f_*)_i^\alpha[f_*(R^M(\partial_k, \cdot)\partial_l)]_j^\beta = g^{ij}h_f\left((f_*)_i, g^{kl}f_*((R^M)_{kjl}^{\:\:\:\:m}\partial_m\right) \\ = - g^{ij}h_f\left((f_*)_i, f_*(g^{kl}(R^M)_{jkl}^{\:\:\:\:m}\partial_m\right) = g^{ij}h_f\left((f_*)_i, f_*(g^{kl}(R^M)_{jk}\!\,^m_{\:\:\,l}\partial_m\right) = g^{ij}h_f\left((f_*)_i, f_*((^M\mathrm{Ric})_j^{\:\,m}\partial_m)\right) = g^{kl}h_f\left(f_*\partial_l, f_*((^M\mathrm{Ric})_k^{\:\,m}\partial_m)\right). \end{multline} Are those two computations correct?","I am reading now the book The Ricci Flow in Riemannian Geometry by Ben Andrews and Christopher Hopper. It has been a while since I did not use pullback bundles and other objects, and they still look complicated to me. I have trouble regarding sections 3.2.1 and 3.2.2. First let me introduce some notations and context: They consider a vector subbundle of , where is a time interval, consisting of vector tangents to the first factor in the sense that . On that bundle, a connection is defined via the one on denoted by augmented by its effect on the time component for vectors on this bundle, and extended by zero outside the bundle. We are given a smooth map between Riemannian manifolds and consider its time dependent variant . The map induces the pullback bundle over , and consider on it the pullback connection. The Energy is defined as the total density We use coordinates on and on . We also use the indices to denote components. For instance in coordinates, we have . I have a question regarding the motivation behind the gradient descent flow : By a direct computation as done in section 3.2.1, the authors show that Then they go on like this Note that is the variation of . Hence the gradient of E, with respect to the inner product of , is and the gradient descent flow is First, I do not understand why is the gradient of is . Second, how does this imply the following of the statement? I have another question regarding the evolution of the energy density : The authors show that the evolution equation for is given by From that, they infer that Here is my attempt for the second term : By unravelling the definition for the scalar product, we get For the third term. Here is my attempt : By unravelling again, we get Are those two computations correct?","T(M \times I) I \subset \mathbb{R} M (\pi_2)_* = 0 \nabla M ^M\nabla \nabla_{\partial_t} u = [\partial_t, u] f : (M, g) \to (N, h) f : M \times  I \to N f^*TN M \times I E E(f) = \frac{1}{2}\int_M e(f) \: d\mu(g) = \frac{1}{2}\int_M\langle f_*, f_*\rangle \: d\mu(g) \{x_i\} M \times I \{y^\alpha\} N f_* = (f_*)_i^\alpha dx^i \otimes (\partial_\alpha)_f  \frac{d}{dt} E(f) = -\int_M \langle f_*\partial_t, \Delta_{g,h}f\rangle \: d\mu(g).  f_*\partial_t f f^*TN -\Delta_{g,h}f  f_*\partial_t = \Delta_{g,h}f. E -\Delta_{g,h}f f_*  (\Delta_{\partial_t} f_*)(\partial_i) = (\Delta_* f)(\partial_i) + g^{kl}R^N(f_*\partial_k, f_*\partial_i)(f_*\partial_l) - g^{kl}f_*(R^M(\partial_k, \partial_l)). \frac{d}{dt}e = \langle f_*, \Delta f_*\rangle + g^{kl}g^{ij}R^N(f_*\partial_k, f_*\partial_i, f_*\partial_l, f_*\partial_j) - g^{kl}h_f\left(f_*\left((^M\mathrm{Ric})_k^{\:\,m}\partial_p\right), f_*\partial_l\right).  \begin{multline} g^{kl}g^{ij}(h_{\alpha\beta})_f(f_*)_i^\alpha[R^N(f_*\partial_k, f_*\cdot)(f_*\partial_l)]_j^\beta = g^{kl}g^{ij}h_f\left((f_*)_i,R^N(f_*\partial_k, f_*\partial_j)(f_*\partial_l)\right) = g^{kl}g^{ij}R^N(f_*\partial_k, f_*\partial_j, f_*\partial_l, f_*\partial_i).\end{multline} \begin{multline} g^{kl}g^{ij}(h_{\alpha\beta})_f (f_*)_i^\alpha[f_*(R^M(\partial_k, \cdot)\partial_l)]_j^\beta = g^{ij}h_f\left((f_*)_i, g^{kl}f_*((R^M)_{kjl}^{\:\:\:\:m}\partial_m\right) \\ = - g^{ij}h_f\left((f_*)_i, f_*(g^{kl}(R^M)_{jkl}^{\:\:\:\:m}\partial_m\right) = g^{ij}h_f\left((f_*)_i, f_*(g^{kl}(R^M)_{jk}\!\,^m_{\:\:\,l}\partial_m\right) = g^{ij}h_f\left((f_*)_i, f_*((^M\mathrm{Ric})_j^{\:\,m}\partial_m)\right) = g^{kl}h_f\left(f_*\partial_l, f_*((^M\mathrm{Ric})_k^{\:\,m}\partial_m)\right). \end{multline}","['differential-geometry', 'solution-verification', 'riemannian-geometry', 'heat-equation', 'ricci-flow']"
88,Do linearly independent vector fields with vanishing Lie bracket always have integral manifolds which are level sets?,Do linearly independent vector fields with vanishing Lie bracket always have integral manifolds which are level sets?,,"Suppose I have a smooth distribution on an open subset of $M = R^n_{++}$ consisting of $n-1$ vector fields $X^i$ . I know three things about this distribution: At each point $p \in M$ the set $(X^1_p,\ldots,X^{n-1}_p)$ is linearly independent and so spans an $(n-1)$ -dimensional subspace of $R^n$ . For each pair $i,j, \; 1 \leq i < j \leq n-1, \; [X^i,X^j] = 0$ (so the system is in involution). At each point of $M$ the normal to the subspace spanned by this system is non-negative, with the last coordinate strictly positive. By 2, Frobenius tells us this system is integrable with its integral manifolds forming a foliation of $M$ . I understand this also means that there are local coordinates $x^i$ such that $X^i = \partial/\partial x^i$ . I have two questions: In this particular case does there always exist a single function $f: M \rightarrow R$ (smooth or not) such that the integral manifolds (hypersurfaces) are level sets of $f$ ; i.e. each leaf $L$ of the foliation is of the form $f^{-1}(x)$ for some $x \in R$ ? Is each leaf $L$ of the foliation globally the graph of some function $h:D \rightarrow R$ (i.e. $L = (x_1,\ldots,x_{n-1},h(x_1,\ldots,x_{n-1}))$ where $D$ is some domain in $R^{n-1}$ ? My sense is that the first statement is not true but (because of the third assumption) that the second is. As to 1., it has been suggested to me that if I looked at the distribution from the perspective of the normal field (i.e. 1-form approach) that my assumption that the Lie bracket vanishes is equivalent to the ""well-known"" integrability conditions insuring there exists a function f with the required property, but I'm not sure this is right. I would appreciate any insight into this (surely) simple issue that I'm just not seeing. Edit : In case it helps I can give a concrete example of the vector fields I'm working with when $n=3$ . $X^1 = (-m(x_1,x_2,x_3),1, 0), \; X^2 = (-n(x_1,x_2,x_3),0,1)$ . Here the smooth functions $m$ and $n$ are strictly positive everywhere on $M$ and bounded away from zero and $\infty$ on every compact subset of $M$ . I also simply impose the condition $[X^1,X^2] = mn_1 - nm_1 -n_2 +m_3 = 0$ . Note that the normal field is $N_{(x_1,x_2,x_3)} = (1,m(x_1,x_2,x_3),n(x_1,x_2,x_3)) \gg 0$ . Further clarification : Assumptions on m are, for each fixed $x_2,x_3$ that $\lim_{x \rightarrow \infty}m(x,x_2,x_3) = 0, \lim_{x \rightarrow 0}m(x,x_2,x_3) = \infty$ . A similar assumption holds for $n$ ; i.e. for each fixed $x_1,x_3$ $\lim_{x \rightarrow \infty}n(x_1,x,x_3) = 0, \lim_{x \rightarrow 0}n(x_1,x,x_3) = \infty$ .","Suppose I have a smooth distribution on an open subset of consisting of vector fields . I know three things about this distribution: At each point the set is linearly independent and so spans an -dimensional subspace of . For each pair (so the system is in involution). At each point of the normal to the subspace spanned by this system is non-negative, with the last coordinate strictly positive. By 2, Frobenius tells us this system is integrable with its integral manifolds forming a foliation of . I understand this also means that there are local coordinates such that . I have two questions: In this particular case does there always exist a single function (smooth or not) such that the integral manifolds (hypersurfaces) are level sets of ; i.e. each leaf of the foliation is of the form for some ? Is each leaf of the foliation globally the graph of some function (i.e. where is some domain in ? My sense is that the first statement is not true but (because of the third assumption) that the second is. As to 1., it has been suggested to me that if I looked at the distribution from the perspective of the normal field (i.e. 1-form approach) that my assumption that the Lie bracket vanishes is equivalent to the ""well-known"" integrability conditions insuring there exists a function f with the required property, but I'm not sure this is right. I would appreciate any insight into this (surely) simple issue that I'm just not seeing. Edit : In case it helps I can give a concrete example of the vector fields I'm working with when . . Here the smooth functions and are strictly positive everywhere on and bounded away from zero and on every compact subset of . I also simply impose the condition . Note that the normal field is . Further clarification : Assumptions on m are, for each fixed that . A similar assumption holds for ; i.e. for each fixed .","M = R^n_{++} n-1 X^i p \in M (X^1_p,\ldots,X^{n-1}_p) (n-1) R^n i,j, \; 1 \leq i < j \leq n-1, \; [X^i,X^j] = 0 M M x^i X^i = \partial/\partial x^i f: M \rightarrow R f L f^{-1}(x) x \in R L h:D \rightarrow R L = (x_1,\ldots,x_{n-1},h(x_1,\ldots,x_{n-1})) D R^{n-1} n=3 X^1 = (-m(x_1,x_2,x_3),1, 0), \; X^2 = (-n(x_1,x_2,x_3),0,1) m n M \infty M [X^1,X^2] = mn_1 - nm_1 -n_2 +m_3 = 0 N_{(x_1,x_2,x_3)} = (1,m(x_1,x_2,x_3),n(x_1,x_2,x_3)) \gg 0 x_2,x_3 \lim_{x \rightarrow \infty}m(x,x_2,x_3) = 0, \lim_{x \rightarrow 0}m(x,x_2,x_3) = \infty n x_1,x_3 \lim_{x \rightarrow \infty}n(x_1,x,x_3) = 0, \lim_{x \rightarrow 0}n(x_1,x,x_3) = \infty","['differential-geometry', 'partial-differential-equations', 'foliations']"
89,Convex function on Riemannian manifold,Convex function on Riemannian manifold,,"When I read the 9.5 of Topping's Lectures on the Ricci flow, I have some problem. Assume $W$ is a vector bundle over manifold $(M,g)$ , and $A$ is connection on $W$ . $\{e_1,...,e_l\}$ is a frame of $W_p$ , and extend it to a local frame for $W$ by radial parallel translation using the connection $A$ ,  and $E\in \Gamma(W)$ . $\Psi :W\rightarrow \mathbb R$ is parallel function, namely if $\omega_1\in W$ can be parallel translated (using the connection A) into $\omega_2\in W$ , then $\Psi(\omega_1)=\Psi(\omega_2)$ . Denote the restriction of $\Psi$ to the fibre $W_p$ as $\Psi_p$ , then there is $$ \nabla d (\Psi\circ E)(p)= Hess(\Psi_p)(E(p)) (AE(p), AE(p))   \\ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ + d\Psi_p(E(p))(A^2E(p)+\frac{1}{2}R_A(\cdot,\cdot)E(p))   \tag{9.5.5} $$ If assume $\Psi_p$ weakly convex, after taking the trace of $(9.5.5)$ , how to get $$ \Delta_M(\Psi\circ E)(p)\ge d\Psi_p(E(p))(\Delta_A E(p))  ~~~?  \tag{9.5.6} $$ What I try: First, the weak convex means $$ \text{Hess}(\Psi_p)(E(p)) (AE(p), AE(p))  \ge 0   \tag{1} $$ but I don't know how to deal $$ \text{tr } d\Psi_p(E(p))R_A(\cdot,\cdot)E(p))   \ge 0   \tag{2} $$ I guess, the convex on manifold is not same with linear space. Maybe the weak convex on manifold mean $(1)$ and $(2)$ . But I can't find the definition of convex on Riemannian manifold. PS(2022-6-25):  After some thinking, I think it is irrelevant with convex on manifold, since $W_p$ is vector space.  Besides, by maltreat the Weitzenbock formula, I give a rough process : Weitzenbock formula: assuming $\omega=\omega_i dx^i$ and $\overline\omega$ is the dual vector of $\omega$ , then $$ \Delta \omega (Y) = tr \nabla^2 \omega(Y) - R(\overline\omega,Y) $$ Roughly, (or similarly), I have $$ (\Delta_A E)(\omega) = (tr A^2 E)\omega - g^{ij} R(e_i,\omega)E(e_j)  $$ By Bianchi, I have $$ R(e_i, \omega) E(e_j) + R(\omega, E)e_i(e_j) + R(E,e_i)\omega(e_j)=0 $$ Therefore,  (treat $R(e_i, \omega) E(e_j)$ as $\langle R(e_i, \omega) E, e_j \rangle $ ) $$ g^{ij}R(e_i,\omega) E(e_j) = -\frac{1}{2}g^{ij} R(e_i, e_j)E(\omega) $$ At last, I get $$ \Delta_A E = tr A^2 E+ \frac{1}{2} tr R(\cdot, \cdot)E $$ So, the $(2)$ is redundant. And since it is ordinary convex, $(1)$ must be right.  Therefore, we can get $(9.5.6)$ . But the process  is very rough. Who can rigorous it? Thanks.","When I read the 9.5 of Topping's Lectures on the Ricci flow, I have some problem. Assume is a vector bundle over manifold , and is connection on . is a frame of , and extend it to a local frame for by radial parallel translation using the connection ,  and . is parallel function, namely if can be parallel translated (using the connection A) into , then . Denote the restriction of to the fibre as , then there is If assume weakly convex, after taking the trace of , how to get What I try: First, the weak convex means but I don't know how to deal I guess, the convex on manifold is not same with linear space. Maybe the weak convex on manifold mean and . But I can't find the definition of convex on Riemannian manifold. PS(2022-6-25):  After some thinking, I think it is irrelevant with convex on manifold, since is vector space.  Besides, by maltreat the Weitzenbock formula, I give a rough process : Weitzenbock formula: assuming and is the dual vector of , then Roughly, (or similarly), I have By Bianchi, I have Therefore,  (treat as ) At last, I get So, the is redundant. And since it is ordinary convex, must be right.  Therefore, we can get . But the process  is very rough. Who can rigorous it? Thanks.","W (M,g) A W \{e_1,...,e_l\} W_p W A E\in \Gamma(W) \Psi :W\rightarrow \mathbb R \omega_1\in W \omega_2\in W \Psi(\omega_1)=\Psi(\omega_2) \Psi W_p \Psi_p 
\nabla d (\Psi\circ E)(p)= Hess(\Psi_p)(E(p)) (AE(p), AE(p))   \\
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
+ d\Psi_p(E(p))(A^2E(p)+\frac{1}{2}R_A(\cdot,\cdot)E(p))  
\tag{9.5.5}
 \Psi_p (9.5.5) 
\Delta_M(\Psi\circ E)(p)\ge d\Psi_p(E(p))(\Delta_A E(p))  ~~~?  \tag{9.5.6}
 
\text{Hess}(\Psi_p)(E(p)) (AE(p), AE(p))  \ge 0   \tag{1}
 
\text{tr } d\Psi_p(E(p))R_A(\cdot,\cdot)E(p))   \ge 0   \tag{2}
 (1) (2) W_p \omega=\omega_i dx^i \overline\omega \omega 
\Delta \omega (Y) = tr \nabla^2 \omega(Y) - R(\overline\omega,Y)
 
(\Delta_A E)(\omega) = (tr A^2 E)\omega - g^{ij} R(e_i,\omega)E(e_j) 
 
R(e_i, \omega) E(e_j) + R(\omega, E)e_i(e_j) + R(E,e_i)\omega(e_j)=0
 R(e_i, \omega) E(e_j) \langle R(e_i, \omega) E, e_j \rangle  
g^{ij}R(e_i,\omega) E(e_j) = -\frac{1}{2}g^{ij} R(e_i, e_j)E(\omega)
 
\Delta_A E = tr A^2 E+ \frac{1}{2} tr R(\cdot, \cdot)E
 (2) (1) (9.5.6)","['differential-geometry', 'riemannian-geometry', 'ricci-flow']"
90,"How to show $A^2e_i=-\frac{1}{2} R_A(\cdot, \cdot)e_i$ in Topping's Lectures on the Ricci flow",How to show  in Topping's Lectures on the Ricci flow,"A^2e_i=-\frac{1}{2} R_A(\cdot, \cdot)e_i","When I read the 9.5 of Topping's Lectures on the Ricci flow, I have some problem. Assume $W$ is a vector bundle over manifold $(M,g)$ , and $A$ is connection on $W$ . $\{e_1,...,e_l\}$ is a frame of $W_p$ , and extend it to a local frame for $W$ by radial parallel translation using the connection $A$ , then we have $Ae_i=0 $ , and $$ A^2e_i=-\frac{1}{2} R_A(\cdot, \cdot)e_i   \tag{1} $$ where $R_A$ is the curvature of the connection $A$ .   I don't know how to get $(1)$ . In my view, for any $v_a,v_b\in T_pM$ $$ R_A(v_a,v_b)e_k= A_a A_b e_k - A_b A_a e_k $$ where $A_ae_k=A_{v_a}e_k$ .  How it imply $(1)$ ? PS(2022-7-4): I use the notation of chapter 4 of   S.S. Chern and W.H. Chen's ใๅพฎๅๅไฝ่ฎฒไนใ. Assume the connection matrix of $W$ is $$ \omega= \begin{pmatrix} \omega_1^1 ,...\omega_1^l \\ ~~\\ \omega_1^l ,...\omega_l^l \end{pmatrix} $$ where $\omega_a^b=\Gamma_{ai}^b dx^i$ . And the curvature matrix is $\Omega= d\omega -\omega\wedge \omega$ , namely $$ \Omega= \begin{pmatrix} d\Gamma_{ai}^b \wedge dx^i  - \omega_a^r\wedge \omega_r^b \end{pmatrix} $$ Therefore, the curvature operator is $$ R_A(\cdot, \cdot) e_a = \begin{pmatrix} d\Gamma_{ai}^b \wedge dx^i  - \omega_a^r\wedge \omega_r^b \end{pmatrix} \otimes e_b $$ where $e_a=\frac{\partial}{\partial x_a}$ is tangent vector. I use the definition $$ e_1\wedge e_2 = e_1 \otimes e_2 - e_2 \otimes e_1 $$ Then, I have $$ R_A(\cdot, \cdot) e_a =   d\Gamma_{ai}^r\otimes dx^i \otimes e_r - dx^i \otimes d\Gamma_{ai}^r\otimes e_r  \\ -\Gamma_{ai}^b \Gamma_{bj}^r dx^i\otimes dx^j\otimes e_r  +\Gamma_{bi}^r \Gamma_{aj}^b dx^i\otimes dx^j\otimes e_r \\ =\left( \frac{\partial \Gamma_{aj}^r}{\partial x_i} -\frac{\partial \Gamma_{ai}^r}{\partial x_j} -\Gamma_{ai}^b\Gamma_{bj}^r +\Gamma_{bi}^r\Gamma_{aj}^b \right) dx^i\otimes dx^j \otimes e_r   \tag{2} $$ On the other hand, first, I have $$ A e_a = \Gamma_{ai}^b dx^i\otimes e_b $$ Then $$ A^2 e_a =  A( \Gamma_{ai}^b dx^i \otimes e_b )\\ = d\Gamma_{ai}^b \otimes dx^i \otimes e_b  - \Gamma_{ai}^b  \Gamma_{cd}^i dx^c \otimes dx^d\otimes e_b  + \Gamma_{ai}^b \Gamma_{bk}^r dx^i \otimes dx^k \otimes e_r \\ =\left( \frac{\partial \Gamma_{aj}^r}{\partial x_i} -\Gamma_{ak}^r\Gamma_{ij}^k +\Gamma_{ai}^b\Gamma_{bj}^r \right)dx^i\otimes dx^j \otimes e_r   \tag{3} $$ Then, I think I should use an analogue of Bianchi to get $(1)$ from $(2)$ and $(3)$ . But now, I still don't know how to use a suitable Bianchi. Besides, of course, if assume  local trivialization, (2) and (3) will become much simple. PS(2022-7-5): If I use the geodesic normal coordinate at $p$ , then (2) and (3) become $$ R_A(\cdot, \cdot) e_a = \begin{pmatrix} d\Gamma_{ai}^b \wedge dx^i  - \omega_a^r\wedge \omega_r^b \end{pmatrix} \otimes e_b $$ where $e_a=\frac{\partial}{\partial x_a}$ is tangent vector. I use the definition $$ e_1\wedge e_2 = e_1 \otimes e_2 - e_2 \otimes e_1 $$ Then, I have $$ R_A(\cdot, \cdot) e_a  =   \left( \frac{\partial \Gamma_{aj}^r}{\partial x_i} -\frac{\partial \Gamma_{ai}^r}{\partial x_j} \right) dx^i\otimes dx^j \otimes e_r   \tag{4} $$ $$ A^2 e_a  = \frac{\partial \Gamma_{aj}^r}{\partial x_i} dx^i\otimes dx^j \otimes e_r   \tag{5} $$ The power series expansion of the metric in the coordinate is $$ h_{ij}(x) = \delta_{ij} -\frac{1}{3} R_{ikjl}(p) x^k x^l  + O(|x|^3) \tag{6} $$ where $x$ is in the geodesic normal coordinate of $p$ , $h$ is the metric of $W$ , and $R$ is cuvature of connection $A$ . But I want to show (1) at $p$ , seemly, it has nothing to the point $x$ . PS(2022-7-10): In fact, the next calculation is finished  in two or three days ago. Just since feeling boring, I  stopped. First,  since $Ah=0$ , we have $$ \Gamma_{ij}^m = \frac{1}{2}h^{lm} (\partial_i h_{jl}+ \partial_j h_{li} - \partial_l h_{ij}) $$ Therefore, in the geodesic normal coordinate of $p$ , we have $$ \partial_k \Gamma_{ij}^m(p) =  \frac{1}{2}h^{lm} (\partial_k\partial_i h_{jl}+ \partial_k\partial_j h_{li} - \partial_k\partial_l h_{ij})|_{x=p} $$ Using (6) to calculate $\partial_k\partial_i h_{jl} (p)$ and $h^{lm} =\delta^{lm}$ ,we have $$ \partial_k \Gamma_{ij}^m(p) = -\frac{1}{3} (R_{mijk}(p) +R_{mjik}(p)) \tag{7} $$ From (4),(5), we have $$ R_A(\cdot, \cdot) e_a =R_{raij} dx^i \otimes dx^j \otimes e_r \tag{8} $$ $$ A^2e_a = \frac{1}{3}(R_{raij} - R_{rjai}) dx^i\otimes dx^j\otimes e_r \tag{9} $$ Obviously, they are not  sustaining the (1).  Besides, I am sure (7) is right, since before I calculated it , I found it in Wiki . The possible error is the miscalculate of (8) and (9). But, in fact, I have done 5 or 6 times, and not  sustaining the (1). Therefore, I feel boring...","When I read the 9.5 of Topping's Lectures on the Ricci flow, I have some problem. Assume is a vector bundle over manifold , and is connection on . is a frame of , and extend it to a local frame for by radial parallel translation using the connection , then we have , and where is the curvature of the connection .   I don't know how to get . In my view, for any where .  How it imply ? PS(2022-7-4): I use the notation of chapter 4 of   S.S. Chern and W.H. Chen's ใๅพฎๅๅไฝ่ฎฒไนใ. Assume the connection matrix of is where . And the curvature matrix is , namely Therefore, the curvature operator is where is tangent vector. I use the definition Then, I have On the other hand, first, I have Then Then, I think I should use an analogue of Bianchi to get from and . But now, I still don't know how to use a suitable Bianchi. Besides, of course, if assume  local trivialization, (2) and (3) will become much simple. PS(2022-7-5): If I use the geodesic normal coordinate at , then (2) and (3) become where is tangent vector. I use the definition Then, I have The power series expansion of the metric in the coordinate is where is in the geodesic normal coordinate of , is the metric of , and is cuvature of connection . But I want to show (1) at , seemly, it has nothing to the point . PS(2022-7-10): In fact, the next calculation is finished  in two or three days ago. Just since feeling boring, I  stopped. First,  since , we have Therefore, in the geodesic normal coordinate of , we have Using (6) to calculate and ,we have From (4),(5), we have Obviously, they are not  sustaining the (1).  Besides, I am sure (7) is right, since before I calculated it , I found it in Wiki . The possible error is the miscalculate of (8) and (9). But, in fact, I have done 5 or 6 times, and not  sustaining the (1). Therefore, I feel boring...","W (M,g) A W \{e_1,...,e_l\} W_p W A Ae_i=0  
A^2e_i=-\frac{1}{2} R_A(\cdot, \cdot)e_i   \tag{1}
 R_A A (1) v_a,v_b\in T_pM 
R_A(v_a,v_b)e_k= A_a A_b e_k - A_b A_a e_k
 A_ae_k=A_{v_a}e_k (1) W 
\omega=
\begin{pmatrix}
\omega_1^1 ,...\omega_1^l
\\
~~\\
\omega_1^l ,...\omega_l^l
\end{pmatrix}
 \omega_a^b=\Gamma_{ai}^b dx^i \Omega= d\omega -\omega\wedge \omega 
\Omega=
\begin{pmatrix}
d\Gamma_{ai}^b \wedge dx^i  - \omega_a^r\wedge \omega_r^b
\end{pmatrix}
 
R_A(\cdot, \cdot) e_a
=
\begin{pmatrix}
d\Gamma_{ai}^b \wedge dx^i  - \omega_a^r\wedge \omega_r^b
\end{pmatrix} \otimes e_b
 e_a=\frac{\partial}{\partial x_a} 
e_1\wedge e_2 = e_1 \otimes e_2 - e_2 \otimes e_1
 
R_A(\cdot, \cdot) e_a =  
d\Gamma_{ai}^r\otimes dx^i \otimes e_r
- dx^i \otimes d\Gamma_{ai}^r\otimes e_r  \\
-\Gamma_{ai}^b \Gamma_{bj}^r dx^i\otimes dx^j\otimes e_r 
+\Gamma_{bi}^r \Gamma_{aj}^b dx^i\otimes dx^j\otimes e_r
\\
=\left(
\frac{\partial \Gamma_{aj}^r}{\partial x_i}
-\frac{\partial \Gamma_{ai}^r}{\partial x_j}
-\Gamma_{ai}^b\Gamma_{bj}^r
+\Gamma_{bi}^r\Gamma_{aj}^b
\right) dx^i\otimes dx^j \otimes e_r  
\tag{2}
 
A e_a = \Gamma_{ai}^b dx^i\otimes e_b
 
A^2 e_a = 
A(
\Gamma_{ai}^b dx^i \otimes e_b
)\\
=
d\Gamma_{ai}^b \otimes dx^i \otimes e_b 
-
\Gamma_{ai}^b  \Gamma_{cd}^i dx^c \otimes dx^d\otimes e_b 
+
\Gamma_{ai}^b \Gamma_{bk}^r dx^i \otimes dx^k \otimes e_r
\\
=\left(
\frac{\partial \Gamma_{aj}^r}{\partial x_i}
-\Gamma_{ak}^r\Gamma_{ij}^k
+\Gamma_{ai}^b\Gamma_{bj}^r
\right)dx^i\otimes dx^j \otimes e_r  
\tag{3}
 (1) (2) (3) p 
R_A(\cdot, \cdot) e_a
=
\begin{pmatrix}
d\Gamma_{ai}^b \wedge dx^i  - \omega_a^r\wedge \omega_r^b
\end{pmatrix} \otimes e_b
 e_a=\frac{\partial}{\partial x_a} 
e_1\wedge e_2 = e_1 \otimes e_2 - e_2 \otimes e_1
 
R_A(\cdot, \cdot) e_a 
=  
\left(
\frac{\partial \Gamma_{aj}^r}{\partial x_i}
-\frac{\partial \Gamma_{ai}^r}{\partial x_j}
\right) dx^i\otimes dx^j \otimes e_r  
\tag{4}
 
A^2 e_a 
=
\frac{\partial \Gamma_{aj}^r}{\partial x_i}
dx^i\otimes dx^j \otimes e_r  
\tag{5}
 
h_{ij}(x) = \delta_{ij} -\frac{1}{3} R_{ikjl}(p) x^k x^l  + O(|x|^3)
\tag{6}
 x p h W R A p x Ah=0 
\Gamma_{ij}^m = \frac{1}{2}h^{lm} (\partial_i h_{jl}+ \partial_j h_{li} - \partial_l h_{ij})
 p 
\partial_k \Gamma_{ij}^m(p)
= 
\frac{1}{2}h^{lm} (\partial_k\partial_i h_{jl}+ \partial_k\partial_j h_{li} - \partial_k\partial_l h_{ij})|_{x=p}
 \partial_k\partial_i h_{jl} (p) h^{lm} =\delta^{lm} 
\partial_k \Gamma_{ij}^m(p) = -\frac{1}{3} (R_{mijk}(p) +R_{mjik}(p))
\tag{7}
 
R_A(\cdot, \cdot) e_a =R_{raij} dx^i \otimes dx^j \otimes e_r
\tag{8}
 
A^2e_a = \frac{1}{3}(R_{raij} - R_{rjai}) dx^i\otimes dx^j\otimes e_r
\tag{9}
","['differential-geometry', 'riemannian-geometry', 'ricci-flow']"
91,Reference for topological invariants written in integrals,Reference for topological invariants written in integrals,,"When studying physics, I came up with various integrals that only takes integer values due to topological reasons. Winding number $S^1 \to S^1$ is the most elementary example, which moreover provides an isomorphism $\pi_1(S^1) \to \mathbb Z$ . More generally, homotopy class of a map from $S^d$ or $T^d$ to a Lie group $G$ is often represented by an integral, as I saw in various physics literature (one example is Homotopy and quantization in condensed matter physics , written by mathematical physicists). I want a systematic theory on this matter. Already in this site, there are at least two questions related to my question, which focuses on a specific choice of $X$ and $Y$ when classifying maps $X\to Y$ : Why is the winding number of a matrix an integer? Cartan 3-form on a Lie group G What I want to address is more general, since I feel that there is a more general theory of obtaining homotopy invariant as an integral. Let $X = S^d$ or $T^d$ , and let $G$ be a ""nice"" Lie group (say, compact). What is the homotopy class $[X, G]$ ? Can we associate an invariant written in terms of an integral over $X$ that takes only integer values? If such invariant exists, when does this invariant completely characterizes the homotopy class? In particular, in the link Why is the winding number of a matrix an integer? , a map $f:S^1โU(N)$ is classified by the degree of the map $\det\circ f:S^1\to S^1$ . Then, does this invariant completely characterizes $[S^1,U(N)]$ ? Any answers or references on this question is welcomed. I have a basic knowledge of algebraic topology and differential geometry. Note added : After several comments, I can make my question more concrete. According to a comment: If $f:XโG$ is smooth and $\omega$ is a $k$ -form representing an integer cohomology class on $G$ , then $f^โ\omega$ will be a $k$ -form on $X$ representing an integer cohomology class. In particular, integrating it against an integer homology class will give an integer invariant of $f$ . My question following the comment is: How to find a $k$ -form representing an integer cohomology class? What is the ""integer homology class""? Is it correct to say that $\int_N f^*\omega \in \mathbb Z$ whenever $N$ is a $k$ -dimensional submanifold of $M$ ?","When studying physics, I came up with various integrals that only takes integer values due to topological reasons. Winding number is the most elementary example, which moreover provides an isomorphism . More generally, homotopy class of a map from or to a Lie group is often represented by an integral, as I saw in various physics literature (one example is Homotopy and quantization in condensed matter physics , written by mathematical physicists). I want a systematic theory on this matter. Already in this site, there are at least two questions related to my question, which focuses on a specific choice of and when classifying maps : Why is the winding number of a matrix an integer? Cartan 3-form on a Lie group G What I want to address is more general, since I feel that there is a more general theory of obtaining homotopy invariant as an integral. Let or , and let be a ""nice"" Lie group (say, compact). What is the homotopy class ? Can we associate an invariant written in terms of an integral over that takes only integer values? If such invariant exists, when does this invariant completely characterizes the homotopy class? In particular, in the link Why is the winding number of a matrix an integer? , a map is classified by the degree of the map . Then, does this invariant completely characterizes ? Any answers or references on this question is welcomed. I have a basic knowledge of algebraic topology and differential geometry. Note added : After several comments, I can make my question more concrete. According to a comment: If is smooth and is a -form representing an integer cohomology class on , then will be a -form on representing an integer cohomology class. In particular, integrating it against an integer homology class will give an integer invariant of . My question following the comment is: How to find a -form representing an integer cohomology class? What is the ""integer homology class""? Is it correct to say that whenever is a -dimensional submanifold of ?","S^1 \to S^1 \pi_1(S^1) \to \mathbb Z S^d T^d G X Y X\to Y X = S^d T^d G [X, G] X f:S^1โU(N) \det\circ f:S^1\to S^1 [S^1,U(N)] f:XโG \omega k G f^โ\omega k X f k \int_N f^*\omega \in \mathbb Z N k M","['differential-geometry', 'algebraic-topology', 'homotopy-theory']"
92,Show that $M :=\{J\in M_{2n}(\mathbb{R}); J^2 = -I\}$ is a submanifold of $M_{2n}(\mathbb{R})$,Show that  is a submanifold of,M :=\{J\in M_{2n}(\mathbb{R}); J^2 = -I\} M_{2n}(\mathbb{R}),"I want to show that: $$  M := \{J \in M_{2n}(\mathbb{R}) \mid J^2 = -I \} $$ is the submanifold of $M_{2n}(\mathbb{R})$ , and I am given a hint to use Theorem 1. Theorem 1. Let $m,n,l \geq 0$ . Suppose that $O$ is an open set of $\mathbb{R}^{n}$ , and $O'$ is a open set of $\mathbb{R}^m$ , and the mappings $F: O \to O'$ and $G: O' \to \mathbb{R}^l$ are of the class $C^{\infty}$ . Suppose that the points $a_0 \in O$ , $\,b_0 \in O'$ , and $c_0 \in \mathbb{R}^{l}$ satisfy the following conditions: ( i ) $F(a_0) = b_0$ , ( ii ) $GF(O) = c_0$ , and ( iii ) the sequence of linear mappings $$ \mathbb{R}^n \xrightarrow{(JF)_{a_0}}  \mathbb{R}^m \xrightarrow{(JG)_{b_0}} \mathbb{R}^l  $$ is exact. Then, there is a open set $W \subseteq \mathbb{R}^m$ that satisfies $b_0 \in W \subseteq O'$ , and we have $$ W \cap F(O) = W \cap G^{-1}(c_0)  $$ and $W \cap F(O)$ will be a $C^{\infty}$ -submanifold of $\mathbb{R}^m$ . However, I don't know how to use this theorem. In my opinion, I can consider $G$ as $\det$ , and $\det^{-1} \{\pm{1}\}$ will be $M$ , but I don't know what $J(\det)$ is. Please help me!!!!!  I am Japanese, so I am not good at $\LaTeX$ and English. There may be a lot of mistakes in my context, please forgive me.","I want to show that: is the submanifold of , and I am given a hint to use Theorem 1. Theorem 1. Let . Suppose that is an open set of , and is a open set of , and the mappings and are of the class . Suppose that the points , , and satisfy the following conditions: ( i ) , ( ii ) , and ( iii ) the sequence of linear mappings is exact. Then, there is a open set that satisfies , and we have and will be a -submanifold of . However, I don't know how to use this theorem. In my opinion, I can consider as , and will be , but I don't know what is. Please help me!!!!!  I am Japanese, so I am not good at and English. There may be a lot of mistakes in my context, please forgive me."," 
M := \{J \in M_{2n}(\mathbb{R}) \mid J^2 = -I \}
 M_{2n}(\mathbb{R}) m,n,l \geq 0 O \mathbb{R}^{n} O' \mathbb{R}^m F: O \to O' G: O' \to \mathbb{R}^l C^{\infty} a_0 \in O \,b_0 \in O' c_0 \in \mathbb{R}^{l} F(a_0) = b_0 GF(O) = c_0 
\mathbb{R}^n \xrightarrow{(JF)_{a_0}} 
\mathbb{R}^m \xrightarrow{(JG)_{b_0}} \mathbb{R}^l 
 W \subseteq \mathbb{R}^m b_0 \in W \subseteq O' 
W \cap F(O) = W \cap G^{-1}(c_0) 
 W \cap F(O) C^{\infty} \mathbb{R}^m G \det \det^{-1} \{\pm{1}\} M J(\det) \LaTeX","['differential-geometry', 'smooth-manifolds', 'geometric-topology']"
93,Structure induced by a sheaf,Structure induced by a sheaf,,"Let $n\geq 1$ and $F$ be a sheaf on $\mathbb{R}^n$ of continuous functions. Assume for every open subsets $U,V$ of $\mathbb{R}^n$ and every $f\in F(U)$ , $g\in F(V)$ , $g\circ f\in F(U\cap f^{-1}(V))$ . $\bullet$ Is there a name to qualify such a sheaf ? Let $X$ is a set, $(U_i)_{i\in I}$ a collection of non empty subsets covering $X$ and for $i\in I$ , let $z_i:U_i\to\mathbb{R}^n$ be and injective function with an open image. If we assume that for every $(i,j)\in I^2$ , $z_i(U_i\cap U_j)$ is open and $z_j\circ z_i^{-1}\in F(z_i(U_i\cap U_j))$ then there is a unique topology on X such that each $z_i$ is a homeomorphism onto it's image. $\bullet$ Is there a name to define such structure on $X$ induced by $F$ ? A $F$ -manifold or something like that ? For example if $n=2$ and $F$ is the sheaf of holomorphic maps on $\mathbb{C}$ then a Riemann surface is any set $X$ with a Hausdorff connected topology induced by $F$ . Smooth manifolds are defined is a similar way.","Let and be a sheaf on of continuous functions. Assume for every open subsets of and every , , . Is there a name to qualify such a sheaf ? Let is a set, a collection of non empty subsets covering and for , let be and injective function with an open image. If we assume that for every , is open and then there is a unique topology on X such that each is a homeomorphism onto it's image. Is there a name to define such structure on induced by ? A -manifold or something like that ? For example if and is the sheaf of holomorphic maps on then a Riemann surface is any set with a Hausdorff connected topology induced by . Smooth manifolds are defined is a similar way.","n\geq 1 F \mathbb{R}^n U,V \mathbb{R}^n f\in F(U) g\in F(V) g\circ f\in F(U\cap f^{-1}(V)) \bullet X (U_i)_{i\in I} X i\in I z_i:U_i\to\mathbb{R}^n (i,j)\in I^2 z_i(U_i\cap U_j) z_j\circ z_i^{-1}\in F(z_i(U_i\cap U_j)) z_i \bullet X F F n=2 F \mathbb{C} X F","['differential-geometry', 'manifolds', 'sheaf-theory', 'riemann-surfaces']"
94,The Lie algebra of vector fields preserving a tensor,The Lie algebra of vector fields preserving a tensor,,"Let $\alpha$ be a tensor field on a manifold $M$ , compact and without boundary. Denote by $\mathrm{Diff}(M,\alpha)$ the group of diffeomorphisms of $M$ preserving $\alpha$ , i.e. $f:M\to M$ such that $f^*\alpha=\alpha$ . I am interested in the Lie algebra of this group; the tangent space at the identity is given by a subset of vector fields, those satisfying $\mathcal{L}_X\alpha=0$ . I am trying to prove that this set of vector fields is a Lie subalgebra of the vector fields on $M$ ; in other words, I want to show that if $X$ and $Y$ preserve $\alpha$ then also $\mathcal{L}_{[X,Y]}\alpha=0$ . One possible way to do it is to express the flow of $[X,Y]$ using the flows of $X$ and $Y$ , as $$\mathcal{L}_{[X,Y]}\alpha=\partial_{t=0}\Phi^{[X,Y]}_t{}^*\alpha.$$ It might be difficult to express $\Phi^{[X,Y]}_t$ as a combination of $\Phi^X_t$ and $\Phi^Y_t$ , but in fact we just have to do it to first order in $t$ , as is explained in this question , for example. Writing $$\Phi_t^{[X,Y]}=\Phi^X_{\sqrt{t}}\circ\Phi^Y_{\sqrt{t}}\circ\Phi^X_{-\sqrt{t}}\circ\Phi^Y_{-\sqrt{t}}+O(t^2)$$ it seems to follow easily that $\partial_{t=0}\Phi^{[X,Y]}_t{}^*\alpha=0$ , since each of the flows preserves $\alpha$ . So, a first question is: is this reasoning correct? I am however interested in proving the identity directly, through a computation in local coordinates. This is motivated by the fact that, in the context of symplectic geometry, it is possible to directly compute in coordinates for example that Hamiltonian vector fields form a Lie algebra, even showing identites like $[X^\omega_f,X^\omega_g]=X^\omega_{\{f,g\}}$ (up to a sign depending on your conventions). So, let's say that $\alpha$ is a closed $2$ -form, and $X$ and $Y$ are vector fields such that $\alpha_{ij}X^i\mathrm{d}x^j$ and $\alpha_{ij}Y^i\mathrm{d}x^j$ are closed; how do I compute that also $$\alpha_{ij}\left(X^k\partial_k Y^i-Y^k\partial_kX^i\right)\mathrm{d}x^j$$ is closed? As $\alpha_{ij}\partial_kX^i$ is symmetric in $j$ and $k$ (and the same holds for $Y$ ), we have $$\alpha_{ij}X^k\partial_k Y^i=\partial_j(X^k Y^i)\alpha_{ik}+\alpha_{ij}Y^k\partial_kX^i$$ so that $$\alpha_{ij}\left(X^k\partial_k Y^i-Y^k\partial_kX^i\right)\mathrm{d}x^j=\partial_j(X^k Y^i)\alpha_{ik}\mathrm{d}x^j.$$ At this point however I am unable to check that the differential of this expression vanishes. We get then at my second question : can you proceed from here to show that $\mathrm{d}\left([X,Y]\lrcorner\alpha\right)=0$ ?","Let be a tensor field on a manifold , compact and without boundary. Denote by the group of diffeomorphisms of preserving , i.e. such that . I am interested in the Lie algebra of this group; the tangent space at the identity is given by a subset of vector fields, those satisfying . I am trying to prove that this set of vector fields is a Lie subalgebra of the vector fields on ; in other words, I want to show that if and preserve then also . One possible way to do it is to express the flow of using the flows of and , as It might be difficult to express as a combination of and , but in fact we just have to do it to first order in , as is explained in this question , for example. Writing it seems to follow easily that , since each of the flows preserves . So, a first question is: is this reasoning correct? I am however interested in proving the identity directly, through a computation in local coordinates. This is motivated by the fact that, in the context of symplectic geometry, it is possible to directly compute in coordinates for example that Hamiltonian vector fields form a Lie algebra, even showing identites like (up to a sign depending on your conventions). So, let's say that is a closed -form, and and are vector fields such that and are closed; how do I compute that also is closed? As is symmetric in and (and the same holds for ), we have so that At this point however I am unable to check that the differential of this expression vanishes. We get then at my second question : can you proceed from here to show that ?","\alpha M \mathrm{Diff}(M,\alpha) M \alpha f:M\to M f^*\alpha=\alpha \mathcal{L}_X\alpha=0 M X Y \alpha \mathcal{L}_{[X,Y]}\alpha=0 [X,Y] X Y \mathcal{L}_{[X,Y]}\alpha=\partial_{t=0}\Phi^{[X,Y]}_t{}^*\alpha. \Phi^{[X,Y]}_t \Phi^X_t \Phi^Y_t t \Phi_t^{[X,Y]}=\Phi^X_{\sqrt{t}}\circ\Phi^Y_{\sqrt{t}}\circ\Phi^X_{-\sqrt{t}}\circ\Phi^Y_{-\sqrt{t}}+O(t^2) \partial_{t=0}\Phi^{[X,Y]}_t{}^*\alpha=0 \alpha [X^\omega_f,X^\omega_g]=X^\omega_{\{f,g\}} \alpha 2 X Y \alpha_{ij}X^i\mathrm{d}x^j \alpha_{ij}Y^i\mathrm{d}x^j \alpha_{ij}\left(X^k\partial_k Y^i-Y^k\partial_kX^i\right)\mathrm{d}x^j \alpha_{ij}\partial_kX^i j k Y \alpha_{ij}X^k\partial_k Y^i=\partial_j(X^k Y^i)\alpha_{ik}+\alpha_{ij}Y^k\partial_kX^i \alpha_{ij}\left(X^k\partial_k Y^i-Y^k\partial_kX^i\right)\mathrm{d}x^j=\partial_j(X^k Y^i)\alpha_{ik}\mathrm{d}x^j. \mathrm{d}\left([X,Y]\lrcorner\alpha\right)=0","['differential-geometry', 'solution-verification', 'lie-groups', 'vector-fields', 'symplectic-geometry']"
95,Why is there a 1/2 coefficient for the curvature formula in the Wikipedia Curvature Form article?,Why is there a 1/2 coefficient for the curvature formula in the Wikipedia Curvature Form article?,,"The formula for the principal curvature form in the Wikipedia article Curvature Form, when applied to the $X, Y$ tangent vectors to the principal bundle $P$ , goes on like $$\Omega(X, Y)=d\omega(X, Y)+\frac12[\omega(X), \omega(Y)]$$ where $\omega$ is the connection form with value in the lie algebra of the structure group. It only depends on the lie bracket of the lie algebra, not on any conventional definition of the wedge product of forms Having learned that there is no $\frac12$ coefficient, I was about to edit the article when I saw a caveat from the authors ""do not remove the $\frac12$ coefficient"". They did motivate its presence by a correct calculation made from the definition given in the article Lie-Algebra Valued Forms of Wikipedia where there is a $\frac1{(p+q)!}$ coefficient in front of the definition of the wedge product of two lie-algebras valued forms. This definition is different from the one I saw in Michor's book Natural Operations in Differential Geometry, Cap. 19, or other textbooks, where the coefficient is $\frac1{p!q!}$ . This explain the difference in the formula for the curvature. What is right? It seems to me that there should be no such a coefficient $\frac12$ and that I should edit the article on Lie-Algebra Valued Forms. Am I right? Edit 2 Well, the only explanation that makes sense for me is the following: the authors of the article are using an ""old"" convention for the definition of the exterior derivative of a one form, following there the one given in the Kobayashi & Nomizu book they referred to in a note. In this book, the convention used put a $\frac1{p+1}$ in front of the definition of the exterior derivative of a $p$ -form compare to the ""modern"" definition. So, for the connection form , we have a $\frac12$ coefficient in the formula $$d\omega(X,Y)=\frac12(X.\omega(Y)-Y.\omega(X)-\omega([X,Y]))$$ With this convention, it is easy to see that the formula for the curvature given in the Wikipedia article is right, albeit giving a curvature that is half of the modern definition for it. Having two competing definitions for such an important and basic operator is a source of confusion and waste of time IMHO. I will definitely go for the ""modern"" approach. Thus, I just edited the wikipedia article with a reference to the choice of convention made for the exterior derivative definition, following my answer. It seems so me the safest path of action, since getting rid of the $\frac12$ coefficient would have implied a change in other articles. Therefore, my remaining questions are now 1/ is my reasoning right? 2/ is there any rationale behind the choice of the Kobayashi convention for the exterior derivative versus the modern one or is it just a matter of convention? In any case, the choices of some wikipedia articles (like Curvature Form, or Lie-Algebra Valued form, etc.) of the Kobayashi convention is confusing when it is not clearly specified.","The formula for the principal curvature form in the Wikipedia article Curvature Form, when applied to the tangent vectors to the principal bundle , goes on like where is the connection form with value in the lie algebra of the structure group. It only depends on the lie bracket of the lie algebra, not on any conventional definition of the wedge product of forms Having learned that there is no coefficient, I was about to edit the article when I saw a caveat from the authors ""do not remove the coefficient"". They did motivate its presence by a correct calculation made from the definition given in the article Lie-Algebra Valued Forms of Wikipedia where there is a coefficient in front of the definition of the wedge product of two lie-algebras valued forms. This definition is different from the one I saw in Michor's book Natural Operations in Differential Geometry, Cap. 19, or other textbooks, where the coefficient is . This explain the difference in the formula for the curvature. What is right? It seems to me that there should be no such a coefficient and that I should edit the article on Lie-Algebra Valued Forms. Am I right? Edit 2 Well, the only explanation that makes sense for me is the following: the authors of the article are using an ""old"" convention for the definition of the exterior derivative of a one form, following there the one given in the Kobayashi & Nomizu book they referred to in a note. In this book, the convention used put a in front of the definition of the exterior derivative of a -form compare to the ""modern"" definition. So, for the connection form , we have a coefficient in the formula With this convention, it is easy to see that the formula for the curvature given in the Wikipedia article is right, albeit giving a curvature that is half of the modern definition for it. Having two competing definitions for such an important and basic operator is a source of confusion and waste of time IMHO. I will definitely go for the ""modern"" approach. Thus, I just edited the wikipedia article with a reference to the choice of convention made for the exterior derivative definition, following my answer. It seems so me the safest path of action, since getting rid of the coefficient would have implied a change in other articles. Therefore, my remaining questions are now 1/ is my reasoning right? 2/ is there any rationale behind the choice of the Kobayashi convention for the exterior derivative versus the modern one or is it just a matter of convention? In any case, the choices of some wikipedia articles (like Curvature Form, or Lie-Algebra Valued form, etc.) of the Kobayashi convention is confusing when it is not clearly specified.","X, Y P \Omega(X, Y)=d\omega(X, Y)+\frac12[\omega(X), \omega(Y)] \omega \frac12 \frac12 \frac1{(p+q)!} \frac1{p!q!} \frac12 \frac1{p+1} p \frac12 d\omega(X,Y)=\frac12(X.\omega(Y)-Y.\omega(X)-\omega([X,Y])) \frac12","['differential-geometry', 'curvature', 'principal-bundles']"
96,Splitting the Tangent Bundle of a Vector Bundle along the Zero Section,Splitting the Tangent Bundle of a Vector Bundle along the Zero Section,,"Good evening everyone, I have a small question: Assume we have a vector bundle $E = \bigcup\limits_{x\in M} E_x$ over a manifold $M$ . I now want to show the following well-known equation: $$TE_M \cong TM \oplus E$$ I've seen it done with short exact sequences, but I wonder if it can be done more elementary: Let $s : M \hookrightarrow E$ be the zero section (i.e. $s(x) = 0 \in E_x$ ), then we get for each $x\in M$ the direct sum $E_x = \{0\} \oplus E_x = s(x) \oplus E_x$ , so: $$E= s(M) \oplus E$$ For the tangent bundle, this means (identifying $x = s(x)$ ) $$T_{x}E = T_{x}s(M) \oplus T_{x}E \cong T_xM \oplus E_x,$$ since $s(M) \cong M$ and $T_xE = T_x E_x \cong E_x$ as it is a vector space. So, my question is: Does anyone see a mistake or something essential missing from my ""proof""?","Good evening everyone, I have a small question: Assume we have a vector bundle over a manifold . I now want to show the following well-known equation: I've seen it done with short exact sequences, but I wonder if it can be done more elementary: Let be the zero section (i.e. ), then we get for each the direct sum , so: For the tangent bundle, this means (identifying ) since and as it is a vector space. So, my question is: Does anyone see a mistake or something essential missing from my ""proof""?","E = \bigcup\limits_{x\in M} E_x M TE_M \cong TM \oplus E s : M \hookrightarrow E s(x) = 0 \in E_x x\in M E_x = \{0\} \oplus E_x = s(x) \oplus E_x E= s(M) \oplus E x = s(x) T_{x}E = T_{x}s(M) \oplus T_{x}E \cong T_xM \oplus E_x, s(M) \cong M T_xE = T_x E_x \cong E_x","['differential-geometry', 'algebraic-topology']"
97,Mayer-Vietoris Sequence and Poincare dual,Mayer-Vietoris Sequence and Poincare dual,,"Given a $4$ -dimensional simply connected manifold $M$ and open sets $U,V\subseteq M$ such that $U\cup V=M$ we can compute the deRham cohomology in terms of the Mayer-Vietoris sequence: \begin{align*} 0\rightarrow H^1(U)\oplus H^1(V) \rightarrow H^1(U\cap V) \xrightarrow{\delta} H^2(X) \rightarrow H^2(U)\oplus H^2(V) \end{align*} Now assume that the intersection $U\cap V$ is homotopy equivalent to a $2$ -dimensional surface $S\subseteq M$ . My Question: Given the Poincare dual $\omega_S\in H^2(X)$ of $S$ , is there a relation between $\omega_S$ and the image of $\delta:H^1(U\cap V)\rightarrow H^2(X)$ ?","Given a -dimensional simply connected manifold and open sets such that we can compute the deRham cohomology in terms of the Mayer-Vietoris sequence: Now assume that the intersection is homotopy equivalent to a -dimensional surface . My Question: Given the Poincare dual of , is there a relation between and the image of ?","4 M U,V\subseteq M U\cup V=M \begin{align*}
0\rightarrow H^1(U)\oplus H^1(V) \rightarrow H^1(U\cap V) \xrightarrow{\delta} H^2(X) \rightarrow H^2(U)\oplus H^2(V)
\end{align*} U\cap V 2 S\subseteq M \omega_S\in H^2(X) S \omega_S \delta:H^1(U\cap V)\rightarrow H^2(X)","['differential-geometry', 'algebraic-topology', 'poincare-duality', 'mayer-vietoris-sequence']"
98,Compactness of the linear system when varying complex structure,Compactness of the linear system when varying complex structure,,"Let $M$ be a compact, simply connected smooth manifold and $L\rightarrow M$ a complex line bundle over $M$ . Assume there is a continuous path of Kรคhler structures $(g_t,I_t)\; t\in [0,1]$ on $M$ together with a compatible path of holomorphic structures $J_t$ of the line bundle $L$ . Now we know, that for each fixed $t\in [0,1]$ , the space of holomorhic sections $H^0(M,L)_t$ is finite dimensional, hence the linear system $|L|_t := \mathbb{P}(H^0(M,L)_t)$ is a finite dimensional projective space. Choosing a hermitian metric on $L$ , we can equip the space of all smooth sections $\Gamma(X,L)$ with the topology induced by the supremums norm, inducing a topology on the space $\mathbb{P}(\Gamma(X,L))$ . My question now is: Is the set $\cup_{t\in [0,1]} |L|_t \subseteq \mathbb{P}(\Gamma(X,L))$ compact?","Let be a compact, simply connected smooth manifold and a complex line bundle over . Assume there is a continuous path of Kรคhler structures on together with a compatible path of holomorphic structures of the line bundle . Now we know, that for each fixed , the space of holomorhic sections is finite dimensional, hence the linear system is a finite dimensional projective space. Choosing a hermitian metric on , we can equip the space of all smooth sections with the topology induced by the supremums norm, inducing a topology on the space . My question now is: Is the set compact?","M L\rightarrow M M (g_t,I_t)\; t\in [0,1] M J_t L t\in [0,1] H^0(M,L)_t |L|_t := \mathbb{P}(H^0(M,L)_t) L \Gamma(X,L) \mathbb{P}(\Gamma(X,L)) \cup_{t\in [0,1]} |L|_t \subseteq \mathbb{P}(\Gamma(X,L))","['differential-geometry', 'algebraic-geometry', 'complex-geometry', 'deformation-theory']"
99,Do Carmo Chapter 8 Exercise 2,Do Carmo Chapter 8 Exercise 2,,"The statement is : Show that if $M^k$ is a closed, totally geodesic submanifold of $H^n$ (hyperbolic $n$ space), then $M^k$ is isometric to $H^k$ . My friend and I thought to use the ""Cartan Theorem"". This requires setting notation. Let $M$ and $\tilde{M}$ be two $n$ -dimensional Riemannian manifolds, say $p \in M$ and $\tilde{p} \in \tilde{M}$ are arbitrary points. Choose a linear isometry $i:T_pM \rightarrow T_{\tilde{p}}\tilde{M}$ Let $V \subset M$ be a normal neighborhood of $p$ small enough so that $\exp_{\tilde{p}}$ is defined on $i \circ \exp_p^{-1}(V)$ . Define a map $f:V \rightarrow \tilde{M}$ by $f(q) = \exp_{\tilde{p}} \circ i \circ \exp_p^{-1}(q)$ for $q \in V$ . Now, for all $q \in V$ , there exists a unique unit-speed geodesic $\gamma:[0,t] \rightarrow M$ such that $\gamma(0) = p$ and $\gamma(t) = q$ . Denote by $P_t$ the parallel transport along $\gamma$ from $\gamma(0)$ to $\gamma(t)$ . Define another map $\phi_t:T_qM \rightarrow T_{f(q)}\tilde{M}$ by: $\phi_t(v) = \tilde{P}_t \circ i \circ P_t^{-1}(v)$ for all $v \in T_qM$ where $\tilde{P}_t$ is the parallel transport along the normalized geodesic $\tilde{\gamma}:[0,t] \rightarrow \tilde{M}$ given by $\tilde{\gamma}(0) = \tilde{p}$ , $\tilde{\gamma}'(0) = i(\gamma'(0))$ . Finally, denote by $R$ and $\tilde{R}$ the curvature tensors for $M$ and $\tilde{M}$ respectively. We can now state Cartan's Theorem: With notation as above, if for all $q \in V$ and all $x,y,u,v \in T_qM$ we have: $\langle R(x,y)u,v \rangle = \langle \tilde{R}(\phi_t(x), \phi_t(y))\phi_t(u), \phi_t(v) \rangle$ then $f:V \rightarrow f(V) \subset \tilde{M}$ is a local isometry, and $df_p = i$ . Our work: I've heard that in negative curvature, things like conjugate points aren't a problem, and the injectivity radius is large or infinity, so I figured $\exp$ would be a global diffeomorphism for $H^n$ (which I didn't confirm). The hope was that the $f$ in Cartan's Theorem could give us an isometry onto $H^n$ . We looked at choosing the linear isometry $i$ to be the inclusion of the tangent space of the submanifold $T_pM^k$ restricted to its image in $T_pH^n$ . In that case, since the second fundamental form vanishes because $M^k$ is totally geodesic, the curvature tensors are equal via the Gauss formula. $\phi_t$ ends up being the identity (again, vanishing second fundamental form -> same covariant derivatives -> same parallel transport) with this choice of $i$ , and so the hypotheses of the theorem are met, and we get that $f$ is an isometry. Because of the way $f$ is defined, and since we chose the tangent space inclusion as $i$ , I believe this only shows that $M^k$ is isometric to its image in $H^n$ , which is not enough. I thought that maybe we could choose $i$ to be a different isometry whose image is all of $T_pH^n$ , but it seems obvious that this would fail to preserve length, or even be a diffeomorphism if $n \neq k$ . Maybe another approach would be to show that the image of $f$ is clopen in a connected manifold? We aren't sure where to go next. Everything we've done so far has only relied on ""totally geodesic"", so we need to bring in properties of hyperbolic space somehow. Thoughts? Edit: I've also realized that they aren't asking for an isometry onto $H^n$ , they're asking for an isometry onto $H^k$ . I think our proof works when $n = k$ because the tangent space inclusion is a linear isomorphism (if $\exp$ really is a global diffeo). I wonder what you get when exponentiating subspaces of a tangent space in $H^n$ ? Things that are isometric to lower-dimensional hyperbolic spaces perhaps?","The statement is : Show that if is a closed, totally geodesic submanifold of (hyperbolic space), then is isometric to . My friend and I thought to use the ""Cartan Theorem"". This requires setting notation. Let and be two -dimensional Riemannian manifolds, say and are arbitrary points. Choose a linear isometry Let be a normal neighborhood of small enough so that is defined on . Define a map by for . Now, for all , there exists a unique unit-speed geodesic such that and . Denote by the parallel transport along from to . Define another map by: for all where is the parallel transport along the normalized geodesic given by , . Finally, denote by and the curvature tensors for and respectively. We can now state Cartan's Theorem: With notation as above, if for all and all we have: then is a local isometry, and . Our work: I've heard that in negative curvature, things like conjugate points aren't a problem, and the injectivity radius is large or infinity, so I figured would be a global diffeomorphism for (which I didn't confirm). The hope was that the in Cartan's Theorem could give us an isometry onto . We looked at choosing the linear isometry to be the inclusion of the tangent space of the submanifold restricted to its image in . In that case, since the second fundamental form vanishes because is totally geodesic, the curvature tensors are equal via the Gauss formula. ends up being the identity (again, vanishing second fundamental form -> same covariant derivatives -> same parallel transport) with this choice of , and so the hypotheses of the theorem are met, and we get that is an isometry. Because of the way is defined, and since we chose the tangent space inclusion as , I believe this only shows that is isometric to its image in , which is not enough. I thought that maybe we could choose to be a different isometry whose image is all of , but it seems obvious that this would fail to preserve length, or even be a diffeomorphism if . Maybe another approach would be to show that the image of is clopen in a connected manifold? We aren't sure where to go next. Everything we've done so far has only relied on ""totally geodesic"", so we need to bring in properties of hyperbolic space somehow. Thoughts? Edit: I've also realized that they aren't asking for an isometry onto , they're asking for an isometry onto . I think our proof works when because the tangent space inclusion is a linear isomorphism (if really is a global diffeo). I wonder what you get when exponentiating subspaces of a tangent space in ? Things that are isometric to lower-dimensional hyperbolic spaces perhaps?","M^k H^n n M^k H^k M \tilde{M} n p \in M \tilde{p} \in \tilde{M} i:T_pM \rightarrow T_{\tilde{p}}\tilde{M} V \subset M p \exp_{\tilde{p}} i \circ \exp_p^{-1}(V) f:V \rightarrow \tilde{M} f(q) = \exp_{\tilde{p}} \circ i \circ \exp_p^{-1}(q) q \in V q \in V \gamma:[0,t] \rightarrow M \gamma(0) = p \gamma(t) = q P_t \gamma \gamma(0) \gamma(t) \phi_t:T_qM \rightarrow T_{f(q)}\tilde{M} \phi_t(v) = \tilde{P}_t \circ i \circ P_t^{-1}(v) v \in T_qM \tilde{P}_t \tilde{\gamma}:[0,t] \rightarrow \tilde{M} \tilde{\gamma}(0) = \tilde{p} \tilde{\gamma}'(0) = i(\gamma'(0)) R \tilde{R} M \tilde{M} q \in V x,y,u,v \in T_qM \langle R(x,y)u,v \rangle = \langle \tilde{R}(\phi_t(x), \phi_t(y))\phi_t(u), \phi_t(v) \rangle f:V \rightarrow f(V) \subset \tilde{M} df_p = i \exp H^n f H^n i T_pM^k T_pH^n M^k \phi_t i f f i M^k H^n i T_pH^n n \neq k f H^n H^k n = k \exp H^n","['differential-geometry', 'riemannian-geometry', 'hyperbolic-geometry']"
