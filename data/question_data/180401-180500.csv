,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,When can a differential be replaced with a gradient (del) operator?,When can a differential be replaced with a gradient (del) operator?,,"I'm going through some old engineering lecture notes. I've already spotted some errors in the notes. In an important part of a derivation, the lecturer did the following: $Tds = du + pd(1/\rho)$ can be rewritten as $T\nabla s = \nabla u + p\nabla(1/\rho)$ The equation is the second law of thermodynamics where s, u, p, and $\rho$ are the entropy, internal energy, pressure, and density: all scalars, and all state variables (that do not depend on the path taken to get from point a to point b). I've seen similar transformations done for the material derivative: $TDs/Dt = Du/Dt + pD(1/\rho)/Dt$ where $Dg/Dt = \partial g/\partial t + \vec{u}\bullet\nabla g$ My question is: When are these forms of taking a derivative interchangeable or not (for a scalar in 3D Cartesian space)? Or do you know of a handy online reference that explains it? Take a more general case (still scalar variables): If I can write, $dg = (\partial g/\partial a)da + (\partial g/\partial b)db$ I get the sense that I can't just swap ""d"" for $\nabla$... about notation: let's stick with x,y,z as spatial coordinates, t is time, everything else is an arbitrary scalar, and $\vec u$ is a vector.","I'm going through some old engineering lecture notes. I've already spotted some errors in the notes. In an important part of a derivation, the lecturer did the following: $Tds = du + pd(1/\rho)$ can be rewritten as $T\nabla s = \nabla u + p\nabla(1/\rho)$ The equation is the second law of thermodynamics where s, u, p, and $\rho$ are the entropy, internal energy, pressure, and density: all scalars, and all state variables (that do not depend on the path taken to get from point a to point b). I've seen similar transformations done for the material derivative: $TDs/Dt = Du/Dt + pD(1/\rho)/Dt$ where $Dg/Dt = \partial g/\partial t + \vec{u}\bullet\nabla g$ My question is: When are these forms of taking a derivative interchangeable or not (for a scalar in 3D Cartesian space)? Or do you know of a handy online reference that explains it? Take a more general case (still scalar variables): If I can write, $dg = (\partial g/\partial a)da + (\partial g/\partial b)db$ I get the sense that I can't just swap ""d"" for $\nabla$... about notation: let's stick with x,y,z as spatial coordinates, t is time, everything else is an arbitrary scalar, and $\vec u$ is a vector.",,['multivariable-calculus']
1,To show that the function $f:M_n(\mathbb R) \to M_n(\mathbb R)$ given by $f(A)=AA^t$ is differentiable and evaluate its derivative,To show that the function  given by  is differentiable and evaluate its derivative,f:M_n(\mathbb R) \to M_n(\mathbb R) f(A)=AA^t,How to show that the function  $f:M_n(\mathbb R)  \to M_n(\mathbb R)$ given by $f(A)=AA^t$ is differentiable and how to find the total differential at a point  $X$ i.e. how to find $D f_A(X)$ ?,How to show that the function  $f:M_n(\mathbb R)  \to M_n(\mathbb R)$ given by $f(A)=AA^t$ is differentiable and how to find the total differential at a point  $X$ i.e. how to find $D f_A(X)$ ?,,"['analysis', 'multivariable-calculus']"
2,Simple questions about the line integral.,Simple questions about the line integral.,,"I learned very recently about line integrals in my class, and I've been given the definition: $$\int _\Gamma \vec {F} \cdot \vec {ds}=\int _a^b \vec F(\vec \alpha(t))||\alpha'(t)||dt$$ Where $\alpha$ is an injective parametrization of $\Gamma$. My questions are: Given this definition, it seems like the only way to calculate a line integral (besides easy ones where you may use geometric tricks) is to parametrize the curve of domain, is that so? What kind of information does the line integral give about a vector field ($\vec F: \Bbb R^3\to \Bbb R^3$)? Why do we need a new symbol for ""closed path"" line integrals? I know that when talking about functions $g:\Bbb R^2 \to \Bbb R$, it     gives the area between the curve and the function, but couldn't get     the intuition on what it tells for functions from $\Bbb R^n \to \Bbb     R^m$. Also, if we were treating functions with units (as in physics) what units would the line integrated function get?","I learned very recently about line integrals in my class, and I've been given the definition: $$\int _\Gamma \vec {F} \cdot \vec {ds}=\int _a^b \vec F(\vec \alpha(t))||\alpha'(t)||dt$$ Where $\alpha$ is an injective parametrization of $\Gamma$. My questions are: Given this definition, it seems like the only way to calculate a line integral (besides easy ones where you may use geometric tricks) is to parametrize the curve of domain, is that so? What kind of information does the line integral give about a vector field ($\vec F: \Bbb R^3\to \Bbb R^3$)? Why do we need a new symbol for ""closed path"" line integrals? I know that when talking about functions $g:\Bbb R^2 \to \Bbb R$, it     gives the area between the curve and the function, but couldn't get     the intuition on what it tells for functions from $\Bbb R^n \to \Bbb     R^m$. Also, if we were treating functions with units (as in physics) what units would the line integrated function get?",,['multivariable-calculus']
3,Finding the directional derivative.,Finding the directional derivative.,,"We need to find the directional derivative of the function , $f(x,y) = x^{2}+y^{2}+xy$ at $P(1,-1)$ in the direction towards origin. The direction towards origin form the point $(1,-1)$ is represented by a unit vector $u$ , Is it correct to take $u=\dfrac{-i-j}{\sqrt{2}}$ ?","We need to find the directional derivative of the function , $f(x,y) = x^{2}+y^{2}+xy$ at $P(1,-1)$ in the direction towards origin. The direction towards origin form the point $(1,-1)$ is represented by a unit vector $u$ , Is it correct to take $u=\dfrac{-i-j}{\sqrt{2}}$ ?",,"['calculus', 'multivariable-calculus']"
4,Metric tensor for n-sphere in ambient coordinates,Metric tensor for n-sphere in ambient coordinates,,"Let $S^n$ be the unit n-sphere embedded in $\mathbb{R}^{n+1}$: $$ S^n = \{ a \in \mathbb{R}^{n+1} \mid a \cdot a = 1 \} $$ What is the induced metric tensor for the sphere, in $\mathbb{R}^{n+1}$ coordinates? $$ g(x_1, x_2, \ldots, x_n, x_{n+1})$$ Note that $$ \text{arclength}(a,b) = \arccos(a \cdot b) $$ How do I obtain the metric tensor components from this expression? Should I take $b=a+\delta a$?","Let $S^n$ be the unit n-sphere embedded in $\mathbb{R}^{n+1}$: $$ S^n = \{ a \in \mathbb{R}^{n+1} \mid a \cdot a = 1 \} $$ What is the induced metric tensor for the sphere, in $\mathbb{R}^{n+1}$ coordinates? $$ g(x_1, x_2, \ldots, x_n, x_{n+1})$$ Note that $$ \text{arclength}(a,b) = \arccos(a \cdot b) $$ How do I obtain the metric tensor components from this expression? Should I take $b=a+\delta a$?",,"['geometry', 'multivariable-calculus', 'differential-geometry', 'riemannian-geometry', 'tensors']"
5,Inequality for the gradient of a power of absolute value,Inequality for the gradient of a power of absolute value,,"Let $U \subset \mathbb{R}^2$ be open, and let $f : U \to \mathbb{C}$ be a smooth complex-valued function which does not vanish anywhere on $U$.  Let $r > 0$ be a real constant. Does the inequality $$\left| \nabla (|f|^r) \right| \le r |f|^{r-1} |\nabla f|$$   hold pointwise?  Is there a nice elegant proof? For $f > 0$ the inequality is an equality, since $\nabla (f^r) = r f^{r-1} \nabla f$ by the chain rule.  If we identify $\mathbb{R}^2$ with $\mathbb{C}$ and take $f$ holomorphic, it is not hard to show that $\left| \nabla (|f|^r) \right| = \frac{1}{\sqrt{2}} r |f|^{r-1} |\nabla f|$ so the inequality can be strict. For the general case, I am getting bogged down in real and imaginary parts - there must be a better way. If there is a proof that works for $\mathbb{R}^d$ instead of $\mathbb{R}^2$ that is even better. Notation, as requested: $\nabla f : U \to \mathbb{C}^2$ is, as usual, the function $\nabla f = ( \partial_x f, \partial_y f)$.  The partial derivatives $\partial_x f, \partial_y f$ are defined by taking the partial derivatives of the real and imaginary parts of $f$ separately. So if $f = u+iv$ where $u,v  : U \to \mathbb{R}$ are the real and imaginary parts, we have $\partial_x f = \partial_x u + i \partial_x v$, etc.  In particular, we have $$|\nabla f| = \sqrt{(\partial_x u)^2 + (\partial_y u)^2 + (\partial_x v)^2 + (\partial_y v)^2}.$$","Let $U \subset \mathbb{R}^2$ be open, and let $f : U \to \mathbb{C}$ be a smooth complex-valued function which does not vanish anywhere on $U$.  Let $r > 0$ be a real constant. Does the inequality $$\left| \nabla (|f|^r) \right| \le r |f|^{r-1} |\nabla f|$$   hold pointwise?  Is there a nice elegant proof? For $f > 0$ the inequality is an equality, since $\nabla (f^r) = r f^{r-1} \nabla f$ by the chain rule.  If we identify $\mathbb{R}^2$ with $\mathbb{C}$ and take $f$ holomorphic, it is not hard to show that $\left| \nabla (|f|^r) \right| = \frac{1}{\sqrt{2}} r |f|^{r-1} |\nabla f|$ so the inequality can be strict. For the general case, I am getting bogged down in real and imaginary parts - there must be a better way. If there is a proof that works for $\mathbb{R}^d$ instead of $\mathbb{R}^2$ that is even better. Notation, as requested: $\nabla f : U \to \mathbb{C}^2$ is, as usual, the function $\nabla f = ( \partial_x f, \partial_y f)$.  The partial derivatives $\partial_x f, \partial_y f$ are defined by taking the partial derivatives of the real and imaginary parts of $f$ separately. So if $f = u+iv$ where $u,v  : U \to \mathbb{R}$ are the real and imaginary parts, we have $\partial_x f = \partial_x u + i \partial_x v$, etc.  In particular, we have $$|\nabla f| = \sqrt{(\partial_x u)^2 + (\partial_y u)^2 + (\partial_x v)^2 + (\partial_y v)^2}.$$",,"['multivariable-calculus', 'inequality', 'complex-numbers']"
6,Proving that a set is open using epsilons.,Proving that a set is open using epsilons.,,"I'm trying to prove that the set $$A=\{x=(x_{1},x_{2})\in\mathbb{R}^2:x_{1}^{2}+x_{2}^{2}>1\}$$ is open in $\mathbb{R}^2$ with the usual norm is open with the definition of ""epsilons"". My attempt is based in taking $\epsilon=x_{1}^{2}+x_{2}^{2}-1,$ but simply don't achieve to prove that $B(x,\epsilon)\subset A.$ I tried too using another norm because of the equivalence among norms, but it's useless. Another form to attack the problem is taking complement over A, so $A^c$ is closed that's why is an closed ball. However I'd know the path using epsilon definition. Anyone could help me? Thanks in advance.","I'm trying to prove that the set $$A=\{x=(x_{1},x_{2})\in\mathbb{R}^2:x_{1}^{2}+x_{2}^{2}>1\}$$ is open in $\mathbb{R}^2$ with the usual norm is open with the definition of ""epsilons"". My attempt is based in taking $\epsilon=x_{1}^{2}+x_{2}^{2}-1,$ but simply don't achieve to prove that $B(x,\epsilon)\subset A.$ I tried too using another norm because of the equivalence among norms, but it's useless. Another form to attack the problem is taking complement over A, so $A^c$ is closed that's why is an closed ball. However I'd know the path using epsilon definition. Anyone could help me? Thanks in advance.",,"['general-topology', 'multivariable-calculus', 'metric-spaces']"
7,"Given local smooth extensions, construct a global smooth extension","Given local smooth extensions, construct a global smooth extension",,"In Spivak's A Comprehensive Introduction to Differential Geometry, Vol. 1 , he defines a function from a half-space $H^n$ to be $C^\infty$ if there is an extension to a neighborhood of $H^n$ that is $C^\infty$. On page 54, exercise 10(b) asks If $f: H^n \to \mathbb{R}$ is locally $C^\infty$, then $f$ is   $C^\infty$, i.e., $f$ can be extended to a $C^\infty$ function on a   neighborhood of $H^n$. Perhaps an argument from connectedness would work? Given a point $x$ on the boundary, the condition on a point $y$ that there exists a smooth extension containing both $x$ and $y$ is certainly an open condition. I can't see why it should be a closed one.","In Spivak's A Comprehensive Introduction to Differential Geometry, Vol. 1 , he defines a function from a half-space $H^n$ to be $C^\infty$ if there is an extension to a neighborhood of $H^n$ that is $C^\infty$. On page 54, exercise 10(b) asks If $f: H^n \to \mathbb{R}$ is locally $C^\infty$, then $f$ is   $C^\infty$, i.e., $f$ can be extended to a $C^\infty$ function on a   neighborhood of $H^n$. Perhaps an argument from connectedness would work? Given a point $x$ on the boundary, the condition on a point $y$ that there exists a smooth extension containing both $x$ and $y$ is certainly an open condition. I can't see why it should be a closed one.",,"['multivariable-calculus', 'differential-geometry', 'smooth-manifolds']"
8,"Lagrange Multiplier Method: Why is the Langragian function defined as $f(x,y)+\lambda \cdot g(x,y)$?",Lagrange Multiplier Method: Why is the Langragian function defined as ?,"f(x,y)+\lambda \cdot g(x,y)","Edit: As AlexR points out in this comment, there is no mathematical reason behind defining the Lagrangian, except because it makes the Lagrange Multiplier Method easier to memorize. I find this confusing; for me, it is easier to find the critical values of the system of equations. I still haven't found an explanation why is the Lagrange function defined as: $$\Lambda(x,y,\lambda) = f(x,y)+\lambda \cdot g(x,y)$$ Every author doesn't explain the procedure and I don't think is necessary to learn Lagrangian Mechanics to understand the formula, some examples: Wikipedia : To incorporate these conditions into one equation, we introduce an auxiliary function The Idea Shop : Now, if we're clever we can write a single equation that will capture this idea. This is where the familiar Lagrangian equation comes in: $L=f-\lambda(g-c)$ Lagrange Multipliers Without Permanent Scarring by Dan Kein : We can compactly represent both equations at once by writing the Lagrangian: $\Lambda(x,\lambda)=f(x)-\lambda g(x)$ The list goes on, the thing is that ""Lagrangian"" seems to have different meanings depending on the context: https://en.wikipedia.org/wiki/Lagrangian_%28disambiguation%29 So in this context, What does ""Lagrangian function"" means, and what are the steps to get to that function ? Thanks in advance.","Edit: As AlexR points out in this comment, there is no mathematical reason behind defining the Lagrangian, except because it makes the Lagrange Multiplier Method easier to memorize. I find this confusing; for me, it is easier to find the critical values of the system of equations. I still haven't found an explanation why is the Lagrange function defined as: Every author doesn't explain the procedure and I don't think is necessary to learn Lagrangian Mechanics to understand the formula, some examples: Wikipedia : To incorporate these conditions into one equation, we introduce an auxiliary function The Idea Shop : Now, if we're clever we can write a single equation that will capture this idea. This is where the familiar Lagrangian equation comes in: Lagrange Multipliers Without Permanent Scarring by Dan Kein : We can compactly represent both equations at once by writing the Lagrangian: The list goes on, the thing is that ""Lagrangian"" seems to have different meanings depending on the context: https://en.wikipedia.org/wiki/Lagrangian_%28disambiguation%29 So in this context, What does ""Lagrangian function"" means, and what are the steps to get to that function ? Thanks in advance.","\Lambda(x,y,\lambda) = f(x,y)+\lambda \cdot g(x,y) L=f-\lambda(g-c) \Lambda(x,\lambda)=f(x)-\lambda g(x)","['multivariable-calculus', 'lagrange-multiplier']"
9,Tangent space of manifold has two unit vectors orthogonal to tangent space of its boundary,Tangent space of manifold has two unit vectors orthogonal to tangent space of its boundary,,"I'm reading spivak calculus on manifolds and got stuck. Let M be a k-dimensional manifold with boundary in $\mathbb{R^{n}}$, and $M_{x}$ is the tangent space of M at x with dimension k, then $\partial M_{x}$ is the (k-1) dimensional subspace of $M_{x}$  Spivak says that this implies, there are only two unit vectors in $M_{x}$ that are orthogonal to $\partial M_{x}$. I am confused as to why only two unit vectors?","I'm reading spivak calculus on manifolds and got stuck. Let M be a k-dimensional manifold with boundary in $\mathbb{R^{n}}$, and $M_{x}$ is the tangent space of M at x with dimension k, then $\partial M_{x}$ is the (k-1) dimensional subspace of $M_{x}$  Spivak says that this implies, there are only two unit vectors in $M_{x}$ that are orthogonal to $\partial M_{x}$. I am confused as to why only two unit vectors?",,"['multivariable-calculus', 'manifolds-with-boundary']"
10,Significance of the derivative of a scalar field,Significance of the derivative of a scalar field,,I read somewhere that if the temperatures of all points of a huge room were plotted then the derivative at a certain point would give a vector whose direction points in the direction of the hottest point in the room with its magnitude being equal to the temperature at that point. Later on I learned that finding the grad of a scalar field(a surface) gives you a vector perpendicular to the surface(the normal). But the normal points upwards and not in the direction of the largest scalar values being generated by the scalar field. I'm confused...,I read somewhere that if the temperatures of all points of a huge room were plotted then the derivative at a certain point would give a vector whose direction points in the direction of the hottest point in the room with its magnitude being equal to the temperature at that point. Later on I learned that finding the grad of a scalar field(a surface) gives you a vector perpendicular to the surface(the normal). But the normal points upwards and not in the direction of the largest scalar values being generated by the scalar field. I'm confused...,,"['multivariable-calculus', 'scalar-fields']"
11,Function of several variables which is continuous at single point,Function of several variables which is continuous at single point,,Examples of functions on $\mathbb{R}$ which are continuous at a single point are well known. But what about $f:\mathbb{R}^2\to \mathbb{R}$ which is continuous at a single point? I tried to proceed as the one dimension case. I wanted to define $f=g_1$ on $\mathbb{Q}\times \mathbb{Q}$ and $f=g_2$ on $(\mathbb{R}\setminus \mathbb{Q})\times (\mathbb{R}\setminus \mathbb{Q})$ where $g_1=g_2$ has a unique solution. But in here I am leaving out $\mathbb{Q}\times (\mathbb{R}\setminus \mathbb{Q})$ and $(\mathbb{R}\setminus \mathbb{Q})\times \mathbb{Q}$. And I don't know what to define on those sets. Some ideas? I want to generalize such examples to $f :\mathbb{R}^n\to \mathbb{R}^m$. Thanks a lot.,Examples of functions on $\mathbb{R}$ which are continuous at a single point are well known. But what about $f:\mathbb{R}^2\to \mathbb{R}$ which is continuous at a single point? I tried to proceed as the one dimension case. I wanted to define $f=g_1$ on $\mathbb{Q}\times \mathbb{Q}$ and $f=g_2$ on $(\mathbb{R}\setminus \mathbb{Q})\times (\mathbb{R}\setminus \mathbb{Q})$ where $g_1=g_2$ has a unique solution. But in here I am leaving out $\mathbb{Q}\times (\mathbb{R}\setminus \mathbb{Q})$ and $(\mathbb{R}\setminus \mathbb{Q})\times \mathbb{Q}$. And I don't know what to define on those sets. Some ideas? I want to generalize such examples to $f :\mathbb{R}^n\to \mathbb{R}^m$. Thanks a lot.,,"['multivariable-calculus', 'continuity']"
12,Properties of Curl.,Properties of Curl.,,Could anyone please let me know which of these statements is true. 1)$$\text{curl}~{\vec{F}}=0 \implies \vec{F} ~\text{is conservative.}$$ 2) $$\text{curl}~{\vec{F}}=0 \impliedby \vec{F} ~\text{is conservative.}$$ 3) $$\text{curl}~{\vec{F}}=0 \iff\vec{F} ~\text{is conservative.}$$ I have a sneaking suspicion that they are all true in which case the statement $$\text{curl}~{\vec{F}}=0 \iff\vec{F} ~\text{is conservative.}$$ will be very handy to know. PS: Would I be correct in thinking that curl and divergence are only used with vector fields as opposed to scalar fields? Thanks.,Could anyone please let me know which of these statements is true. 1)$$\text{curl}~{\vec{F}}=0 \implies \vec{F} ~\text{is conservative.}$$ 2) $$\text{curl}~{\vec{F}}=0 \impliedby \vec{F} ~\text{is conservative.}$$ 3) $$\text{curl}~{\vec{F}}=0 \iff\vec{F} ~\text{is conservative.}$$ I have a sneaking suspicion that they are all true in which case the statement $$\text{curl}~{\vec{F}}=0 \iff\vec{F} ~\text{is conservative.}$$ will be very handy to know. PS: Would I be correct in thinking that curl and divergence are only used with vector fields as opposed to scalar fields? Thanks.,,['multivariable-calculus']
13,Volume of Solid Defined by Inequalities,Volume of Solid Defined by Inequalities,,"How can I find the volume of a solid defined only by inequalities? For example, in this case I have: $$0\le z \le y \le x \le 1$$ Can someone please explain to me step-by-step on how I can do this. This is a very new concept to me.","How can I find the volume of a solid defined only by inequalities? For example, in this case I have: $$0\le z \le y \le x \le 1$$ Can someone please explain to me step-by-step on how I can do this. This is a very new concept to me.",,['multivariable-calculus']
14,Help finding specific book,Help finding specific book,,"I'm studying Engineering and I'm in my second year, studying Multivariable Calculus, but my University is kind of hard teaching me fresh calculus with topology and analysis, and is kind of hard, so I would like to find a good book, a recent one if possible with Multivariable calculus seen from the perspective of a fresh student but with enough advancement to approach the subject with analysis and topology. Any suggestion? PS: The problem I find with multivariable analysis is that I don't understand the ""importance"" or ""where am I going with this?"" with all those theorems and proofs....is like...is a constant bombarding of theorems and proofs, but without telling me where am I going, or how I use them, or show me a broader perspective of the theory so I can look at it simpler...","I'm studying Engineering and I'm in my second year, studying Multivariable Calculus, but my University is kind of hard teaching me fresh calculus with topology and analysis, and is kind of hard, so I would like to find a good book, a recent one if possible with Multivariable calculus seen from the perspective of a fresh student but with enough advancement to approach the subject with analysis and topology. Any suggestion? PS: The problem I find with multivariable analysis is that I don't understand the ""importance"" or ""where am I going with this?"" with all those theorems and proofs....is like...is a constant bombarding of theorems and proofs, but without telling me where am I going, or how I use them, or show me a broader perspective of the theory so I can look at it simpler...",,"['calculus', 'multivariable-calculus', 'reference-request', 'soft-question', 'book-recommendation']"
15,Minimize Total Cost of Box,Minimize Total Cost of Box,,"So there is a rectangular box that has a volume of $8 m^3$. The top and bottom of the box is made with some material that has a cost of $8$ dollars per square meter. The sides are made with another material that costs $1$ dollar per square meter. How can I find the dimensions of the box that would minimize the total cost? My thoughts: I first started off with the equation, $V=lwh$. I think that we need to find an equation for cost in which we substitute $h$ into and then solve for two variables, then use the first derivative. I can get the concept but can't get how to work on the mechanics.","So there is a rectangular box that has a volume of $8 m^3$. The top and bottom of the box is made with some material that has a cost of $8$ dollars per square meter. The sides are made with another material that costs $1$ dollar per square meter. How can I find the dimensions of the box that would minimize the total cost? My thoughts: I first started off with the equation, $V=lwh$. I think that we need to find an equation for cost in which we substitute $h$ into and then solve for two variables, then use the first derivative. I can get the concept but can't get how to work on the mechanics.",,['multivariable-calculus']
16,Can the calculation of the surface integral of a specific vector field be simplified?,Can the calculation of the surface integral of a specific vector field be simplified?,,"Suppose the two vector fields are $F(x,y,z)=(x^2,0,0)$ and $G(x,y,z)=(0,0,x z)$ respectively. The surface $S$ is a triangle determined by three points $A:(a_1,a_2,a_3)$, $B:(b_1,b_2,b_3)$ and $C: (c_1,c_2,c_3)$ as below: Can the surface integrals of $F$ and $G$ on $S$: $$\iint\limits_S\; F(x,y,z)\cdot {\bf n}{\rm d}S=\iint\limits_S\;x^2\;{\rm d}y\wedge{\rm d}z$$ and $$\iint\limits_S\; G(x,y,z)\cdot {\bf n}{\rm d}S=\iint\limits_S\;x z\;{\rm d}x\wedge{\rm d}y$$ be simplified so that a double integral is unnecessary? I guess there should be a very simple result such that both the integral can be represented as $\sum\limits_{k=1}^3\; f(a_k,b_k,c_k)$, but don't know how to prove it.","Suppose the two vector fields are $F(x,y,z)=(x^2,0,0)$ and $G(x,y,z)=(0,0,x z)$ respectively. The surface $S$ is a triangle determined by three points $A:(a_1,a_2,a_3)$, $B:(b_1,b_2,b_3)$ and $C: (c_1,c_2,c_3)$ as below: Can the surface integrals of $F$ and $G$ on $S$: $$\iint\limits_S\; F(x,y,z)\cdot {\bf n}{\rm d}S=\iint\limits_S\;x^2\;{\rm d}y\wedge{\rm d}z$$ and $$\iint\limits_S\; G(x,y,z)\cdot {\bf n}{\rm d}S=\iint\limits_S\;x z\;{\rm d}x\wedge{\rm d}y$$ be simplified so that a double integral is unnecessary? I guess there should be a very simple result such that both the integral can be represented as $\sum\limits_{k=1}^3\; f(a_k,b_k,c_k)$, but don't know how to prove it.",,"['integration', 'multivariable-calculus', 'definite-integrals', 'vector-analysis', 'surface-integrals']"
17,Integration on 4th dimension unit ball,Integration on 4th dimension unit ball,,I'm trying to calculate \begin{equation*}\int _B e^{x^2+y^2-z^2-w^2}\end{equation*}where \begin{equation*}B=\{\vec x\in\mathbb R^4:||x||\le 1\}\end{equation*}is the unit ball in 4 dimensions. I tried using the standard ball coordinates and rewriting the function as \begin{equation*}e^{x^2+y^2-z^2-w^2}=e^{r^2-2(z^2+w^2)}\end{equation*}where $r^2=x^2+y^2+z^2+w^2$. It did eliminate some of the variables but I still don't know how to integrate it.,I'm trying to calculate \begin{equation*}\int _B e^{x^2+y^2-z^2-w^2}\end{equation*}where \begin{equation*}B=\{\vec x\in\mathbb R^4:||x||\le 1\}\end{equation*}is the unit ball in 4 dimensions. I tried using the standard ball coordinates and rewriting the function as \begin{equation*}e^{x^2+y^2-z^2-w^2}=e^{r^2-2(z^2+w^2)}\end{equation*}where $r^2=x^2+y^2+z^2+w^2$. It did eliminate some of the variables but I still don't know how to integrate it.,,"['integration', 'multivariable-calculus']"
18,"Choosing a vector normal to a jordan curve that points ""inside""","Choosing a vector normal to a jordan curve that points ""inside""",,"Let $\gamma=\partial K_1(0,0)$ be the circle with radius $r=1$ and origin $(0,0)$ in $\mathbb R^2$. Then for any $t_0$ we have $\gamma'(t_0)\neq \begin{pmatrix} 0 \\ 0\end{pmatrix}$. Let $v=\begin{pmatrix} \gamma_1'(t_0) \\ \gamma_2'(t_0) \end{pmatrix}$ be the tangent vector to the curve at $\gamma(t_0)$. Choose $$\gamma:[0,2\pi]\rightarrow \mathbb R^2,~t\mapsto \begin{pmatrix} \cos(t) \\ \sin(t) \end{pmatrix}$$ as a parametric equation of the circle. This yields $$v=\begin{pmatrix}-\sin(t_0) \\ \cos(t_0)\end{pmatrix}$$ for the tangent vector. Thus $$n_1=\begin{pmatrix} \cos(t_0) \\ \sin(t_0) \end{pmatrix},~n_2=\begin{pmatrix} -\cos(t_0) \\ -\sin(t_0) \end{pmatrix}$$ are the two ""obvious"" choices for vectors which are  perpendicular to $v$. For $t_0=0$ we would get $$\gamma(0)=\begin{pmatrix} 1 \\ 0\end{pmatrix}~v=\begin{pmatrix} 0 \\ 1 \end{pmatrix},~n_1=\begin{pmatrix} 1 \\ 0 \end{pmatrix},~n_2=\begin{pmatrix} -1 \\ 0 \end{pmatrix}.$$ Now: if we attach $n_1$ and $n_2$ to $\gamma(0)$, only one vector will point into the circle (in this case $n_2$) while the other vector will point out of the circle. For a circle with positiv orientation it is easy to get the one vector pointing inside, as the curvature doesn't change. Same goes for a circle with negative orientation. But is it possible to choose this vector for an arbitrary jordan curve? In other words: Let $\gamma:[0,1]\rightarrow\mathbb R^2$ be a smooth jordan curve, i.e. $\gamma$ is a differentiable simple closed curve with $\gamma'(t)\neq (0,0)$. Let $$v=\begin{pmatrix} \gamma_1'(t_0) \\ \gamma_2'(t_0)\end{pmatrix}$$ be the tangent vector to the curve at $\gamma_(t_0)$ and $$n_1=\begin{pmatrix} \gamma_2'(t_0) \\ -\gamma_1'(t_0)\end{pmatrix},~n_2=\begin{pmatrix} -\gamma_2'(t_0) \\ \gamma_1'(t_0)\end{pmatrix}$$ be vectors perpendicular to $v$. Is it possible to determine which vector $\varepsilon \cdot n_1$ or $\varepsilon \cdot n_2$ will lie completely in the interior of $\gamma$ if we attach it to $\gamma(t_0)$? ($\varepsilon>0$ is used to shrink the vector if necessary so it doesn't come out ""on the other side"")","Let $\gamma=\partial K_1(0,0)$ be the circle with radius $r=1$ and origin $(0,0)$ in $\mathbb R^2$. Then for any $t_0$ we have $\gamma'(t_0)\neq \begin{pmatrix} 0 \\ 0\end{pmatrix}$. Let $v=\begin{pmatrix} \gamma_1'(t_0) \\ \gamma_2'(t_0) \end{pmatrix}$ be the tangent vector to the curve at $\gamma(t_0)$. Choose $$\gamma:[0,2\pi]\rightarrow \mathbb R^2,~t\mapsto \begin{pmatrix} \cos(t) \\ \sin(t) \end{pmatrix}$$ as a parametric equation of the circle. This yields $$v=\begin{pmatrix}-\sin(t_0) \\ \cos(t_0)\end{pmatrix}$$ for the tangent vector. Thus $$n_1=\begin{pmatrix} \cos(t_0) \\ \sin(t_0) \end{pmatrix},~n_2=\begin{pmatrix} -\cos(t_0) \\ -\sin(t_0) \end{pmatrix}$$ are the two ""obvious"" choices for vectors which are  perpendicular to $v$. For $t_0=0$ we would get $$\gamma(0)=\begin{pmatrix} 1 \\ 0\end{pmatrix}~v=\begin{pmatrix} 0 \\ 1 \end{pmatrix},~n_1=\begin{pmatrix} 1 \\ 0 \end{pmatrix},~n_2=\begin{pmatrix} -1 \\ 0 \end{pmatrix}.$$ Now: if we attach $n_1$ and $n_2$ to $\gamma(0)$, only one vector will point into the circle (in this case $n_2$) while the other vector will point out of the circle. For a circle with positiv orientation it is easy to get the one vector pointing inside, as the curvature doesn't change. Same goes for a circle with negative orientation. But is it possible to choose this vector for an arbitrary jordan curve? In other words: Let $\gamma:[0,1]\rightarrow\mathbb R^2$ be a smooth jordan curve, i.e. $\gamma$ is a differentiable simple closed curve with $\gamma'(t)\neq (0,0)$. Let $$v=\begin{pmatrix} \gamma_1'(t_0) \\ \gamma_2'(t_0)\end{pmatrix}$$ be the tangent vector to the curve at $\gamma_(t_0)$ and $$n_1=\begin{pmatrix} \gamma_2'(t_0) \\ -\gamma_1'(t_0)\end{pmatrix},~n_2=\begin{pmatrix} -\gamma_2'(t_0) \\ \gamma_1'(t_0)\end{pmatrix}$$ be vectors perpendicular to $v$. Is it possible to determine which vector $\varepsilon \cdot n_1$ or $\varepsilon \cdot n_2$ will lie completely in the interior of $\gamma$ if we attach it to $\gamma(t_0)$? ($\varepsilon>0$ is used to shrink the vector if necessary so it doesn't come out ""on the other side"")",,"['multivariable-calculus', 'differential-geometry']"
19,What is $\nabla Au$ for $A:\mathbb{R}^n\to\mathbb{R}^{n\times n}$ and $u:\mathbb{R}^n\to\mathbb{R}$?,What is  for  and ?,\nabla Au A:\mathbb{R}^n\to\mathbb{R}^{n\times n} u:\mathbb{R}^n\to\mathbb{R},"Let $A:\mathbb{R}^n\to\mathbb{R}^{n\times n}$ and $u:\mathbb{R}^n\to\mathbb{R}$. How can we compute $\nabla Au$? I assume we need to apply some kind of product rule, but I wasn't able to figure out how exactly.","Let $A:\mathbb{R}^n\to\mathbb{R}^{n\times n}$ and $u:\mathbb{R}^n\to\mathbb{R}$. How can we compute $\nabla Au$? I assume we need to apply some kind of product rule, but I wasn't able to figure out how exactly.",,"['calculus', 'real-analysis', 'analysis', 'multivariable-calculus', 'derivatives']"
20,The prerequisites for a change of variables in a double integral,The prerequisites for a change of variables in a double integral,,"Given a double integral, I want to find out what should I prove for the equality: $$ \int \int _\Omega f(x,y) dx dy = \int \int _{\Omega_{new}} f(x(u,v),y(u,v))\cdot J dudv $$ My dilemma is as follows: I know that the condition $J\neq 0$ in the domain $\Omega$ implies the validity of the inverse function theorem , and in particular that my mapping $u=u(x,y), v=v(x,y)$ is injective. But, if so, why does all the statements that I find of the theorem of changing variables has the two conditions: $ J\neq 0 $ and our mapping in injective ? In addition, why does for linear mappings $u,v$ it is enough to check the Jacobian does not vanish ?","Given a double integral, I want to find out what should I prove for the equality: $$ \int \int _\Omega f(x,y) dx dy = \int \int _{\Omega_{new}} f(x(u,v),y(u,v))\cdot J dudv $$ My dilemma is as follows: I know that the condition $J\neq 0$ in the domain $\Omega$ implies the validity of the inverse function theorem , and in particular that my mapping $u=u(x,y), v=v(x,y)$ is injective. But, if so, why does all the statements that I find of the theorem of changing variables has the two conditions: $ J\neq 0 $ and our mapping in injective ? In addition, why does for linear mappings $u,v$ it is enough to check the Jacobian does not vanish ?",,['multivariable-calculus']
21,Determining the shock solutions to a PDE.,Determining the shock solutions to a PDE.,,"I'm confused by the question below. Particularly, sketching the base characteristics at the discontinuities in $u(x,0)$ and thus finding the shock solutions. Some advice would be appreciated. Problem My Attempt Using the method of characteristics I find that $t=\tau$ as $t(0)=0$, $x=-g(\psi)^2 + \psi$ as $x(0)=\psi$ as $u=g(\psi)$ as $u(0)=\psi$. So, this means $$x=\begin{cases}-\frac{1}{4}t+\psi & \psi \le 0, \psi \ge 1 \\ -t+\psi & x \lt\psi\lt1 \end{cases}$$ and $$u=\begin{cases}-\frac{1}{2} & \psi \le 0 \\ 1 & \psi \lt\psi\lt1 \\ \frac{1}{2} & \psi \ge 1 \end{cases}$$ Now, when considering the discontinuity in $g(x)$ I get confused. I begin by noticing $-\frac{1}{2} \le g(\psi) \le 1$ at $\psi=0$ and $\frac{1}{2} \le g(\psi) \le 1$ at $\psi=1$, but don't know how to continue from here. In past problems I would determine what would happen and plot a x-t plots of the characteristics to see whether is are any multivalued solutions, etc.","I'm confused by the question below. Particularly, sketching the base characteristics at the discontinuities in $u(x,0)$ and thus finding the shock solutions. Some advice would be appreciated. Problem My Attempt Using the method of characteristics I find that $t=\tau$ as $t(0)=0$, $x=-g(\psi)^2 + \psi$ as $x(0)=\psi$ as $u=g(\psi)$ as $u(0)=\psi$. So, this means $$x=\begin{cases}-\frac{1}{4}t+\psi & \psi \le 0, \psi \ge 1 \\ -t+\psi & x \lt\psi\lt1 \end{cases}$$ and $$u=\begin{cases}-\frac{1}{2} & \psi \le 0 \\ 1 & \psi \lt\psi\lt1 \\ \frac{1}{2} & \psi \ge 1 \end{cases}$$ Now, when considering the discontinuity in $g(x)$ I get confused. I begin by noticing $-\frac{1}{2} \le g(\psi) \le 1$ at $\psi=0$ and $\frac{1}{2} \le g(\psi) \le 1$ at $\psi=1$, but don't know how to continue from here. In past problems I would determine what would happen and plot a x-t plots of the characteristics to see whether is are any multivalued solutions, etc.",,"['multivariable-calculus', 'partial-differential-equations']"
22,"How can the limit exist for a function on an interval (a,b) but not be continuous on that interval?","How can the limit exist for a function on an interval (a,b) but not be continuous on that interval?",,"This is from a practice test true/false question. The statement I was given is that If $\lim_{(x,y)→(a,b)} f(x, y)$ exists, then $f(x, y)$ is continuous at $(a, b)$. I put True but the answer is False. I don't see how how this can be untrue?","This is from a practice test true/false question. The statement I was given is that If $\lim_{(x,y)→(a,b)} f(x, y)$ exists, then $f(x, y)$ is continuous at $(a, b)$. I put True but the answer is False. I don't see how how this can be untrue?",,"['calculus', 'limits', 'multivariable-calculus']"
23,What kind of derivative is this?,What kind of derivative is this?,,In this question here the equation of the derivative of the projection map $\pi_i : \mathbb R^n \to \mathbb R$ is given by $$\pi_i(X+H)-\pi_i(X)=\textrm{grad}\ \pi_i(X)\cdot H+||H||g(H)$$ What definition of derivative is used here? I thought it should be the Frechet derivative but it seems not.,In this question here the equation of the derivative of the projection map $\pi_i : \mathbb R^n \to \mathbb R$ is given by $$\pi_i(X+H)-\pi_i(X)=\textrm{grad}\ \pi_i(X)\cdot H+||H||g(H)$$ What definition of derivative is used here? I thought it should be the Frechet derivative but it seems not.,,"['multivariable-calculus', 'derivatives']"
24,"Let $\mathbf{r}=(x,y,z)$,$r=||\mathbf{r}||$. Show the following equation on $B\cdot \nabla (A\cdot \nabla (\frac{1}{r}))$","Let ,. Show the following equation on","\mathbf{r}=(x,y,z) r=||\mathbf{r}|| B\cdot \nabla (A\cdot \nabla (\frac{1}{r}))","Let $\mathbf{r}=(x,y,z)$ and let $r=||\mathbf{r}||$. If $A$ and $B$ are constant vectors show that: $$B\cdot \left(\nabla \left (A\cdot \nabla \left(\frac{1}{r}\right)\right)\right)=\frac{3A\cdot \mathbf{r}B\cdot \mathbf{r}}{r^5}-\frac{A\cdot B}{r^3}$$ I've found so far that $A\cdot \nabla\left(\frac{1}{r}\right)=-\frac{A\cdot \mathbf{r}}{r^3}$. However, I have not been able to show the equation above.  I would greatly appreciate any solutions, suggestions, or hints.","Let $\mathbf{r}=(x,y,z)$ and let $r=||\mathbf{r}||$. If $A$ and $B$ are constant vectors show that: $$B\cdot \left(\nabla \left (A\cdot \nabla \left(\frac{1}{r}\right)\right)\right)=\frac{3A\cdot \mathbf{r}B\cdot \mathbf{r}}{r^5}-\frac{A\cdot B}{r^3}$$ I've found so far that $A\cdot \nabla\left(\frac{1}{r}\right)=-\frac{A\cdot \mathbf{r}}{r^3}$. However, I have not been able to show the equation above.  I would greatly appreciate any solutions, suggestions, or hints.",,"['multivariable-calculus', 'vector-analysis']"
25,Gradient of an absolute value [closed],Gradient of an absolute value [closed],,"Closed . This question needs details or clarity . It is not currently accepting answers. Want to improve this question? Add details and clarify the problem by editing this post . Closed 9 years ago . Improve this question What is the gradient of $|\vec{x}|^2$? Is it simply $2\vec{x}$, or does the answer get expressed using absolute value notation?","Closed . This question needs details or clarity . It is not currently accepting answers. Want to improve this question? Add details and clarify the problem by editing this post . Closed 9 years ago . Improve this question What is the gradient of $|\vec{x}|^2$? Is it simply $2\vec{x}$, or does the answer get expressed using absolute value notation?",,"['multivariable-calculus', 'derivatives']"
26,Does map induced by rotation preserve the volume form?,Does map induced by rotation preserve the volume form?,,"Let $A: \mathbb{R}^n \to \mathbb{R}^n$ be a rotation. My question is, does the map of $S^{n-1}$ onto $S^{n-1}$ induced by $A$ necessarily preserve the volume form?","Let $A: \mathbb{R}^n \to \mathbb{R}^n$ be a rotation. My question is, does the map of $S^{n-1}$ onto $S^{n-1}$ induced by $A$ necessarily preserve the volume form?",,"['real-analysis', 'general-topology', 'multivariable-calculus', 'differential-geometry', 'manifolds']"
27,Application Stokes's Theorem,Application Stokes's Theorem,,"I am a bit unsure the way Stoke's theorem is applied in this case. Evaluate $\oint\limits_C {xydx + yzdy + zxdz} $ around the triangle with vertices $(1,0,0), (0,1,0), and (0,0,1)$, oriented clockwise as seen from the point $(1,1,1)$ My reasoning Since Stokes's theorem says that we can sum up individual microscopic rotations, i.e. we can turn a line integral into a double integral: $$\oint\limits_C {xydx + yzdy + zxdz}  = \iint\limits_D {\nabla  \times \vec F \cdot \hat NdS} =  - \iint\limits_D {(y,z,x) \cdot \hat NdS}$$ Then, I used parametrization $r = (u,v,1 - u)$. The surface element turns out to be $$d{\mathbf{S}} = \frac{{\partial (x,y,z)}}{{\partial (u,v)}}dudv  = (1,0,1)dudv$$ So, the integral is $$\iint\limits_{D'} {u + vdudv} = \int\limits_0^1 {\int\limits_0^{1 - v} {u + vdudv = \int\limits_0^1 {(1 - v)v + \frac{{{{(1 - v)}^2}}}{2}dv}  = \frac{1}{2}\int\limits_0^1 {1 - {v^2}dv}  = \frac{1}{2}\int\limits_0^1 {1 - {v^2}dv}  = \frac{1}{3}} } $$ According to the answer, it should be $1/2$. Is this correct application of Stokes's theorem?","I am a bit unsure the way Stoke's theorem is applied in this case. Evaluate $\oint\limits_C {xydx + yzdy + zxdz} $ around the triangle with vertices $(1,0,0), (0,1,0), and (0,0,1)$, oriented clockwise as seen from the point $(1,1,1)$ My reasoning Since Stokes's theorem says that we can sum up individual microscopic rotations, i.e. we can turn a line integral into a double integral: $$\oint\limits_C {xydx + yzdy + zxdz}  = \iint\limits_D {\nabla  \times \vec F \cdot \hat NdS} =  - \iint\limits_D {(y,z,x) \cdot \hat NdS}$$ Then, I used parametrization $r = (u,v,1 - u)$. The surface element turns out to be $$d{\mathbf{S}} = \frac{{\partial (x,y,z)}}{{\partial (u,v)}}dudv  = (1,0,1)dudv$$ So, the integral is $$\iint\limits_{D'} {u + vdudv} = \int\limits_0^1 {\int\limits_0^{1 - v} {u + vdudv = \int\limits_0^1 {(1 - v)v + \frac{{{{(1 - v)}^2}}}{2}dv}  = \frac{1}{2}\int\limits_0^1 {1 - {v^2}dv}  = \frac{1}{2}\int\limits_0^1 {1 - {v^2}dv}  = \frac{1}{3}} } $$ According to the answer, it should be $1/2$. Is this correct application of Stokes's theorem?",,"['calculus', 'multivariable-calculus', 'surface-integrals']"
28,Find minimum value of multivariable-function,Find minimum value of multivariable-function,,"A tent with 2 rectangle shaped sides (no floor) and 2 isosceles triangles shaped gables with the volume $V$ is to be constructed. Determine the height so that the minimum amount of cloth is needed. The tent is a prism with isosceles triangle bases. Let the height of the triangle (i.e. the height of the tent) be denoted $x$, the base $2y$ and the length of the tent $L$. Then the volume is $$V(x,y,L)=xyL$$ and the area of the tent will be $$A(x,y,L) = 2(xy+L\sqrt{x^2+y^2}).$$ Since A is a continuous function on a compact set (or can this actually be said, since V is not a boundary but a function of the variables?), there will be a minimum and maximum value. These are found when $grad \, V \, || \, grad \, A$. Since $$ grad \, V = (yL, xL, xy)$$ and $$grad \, A =2(y + \frac{xL}{\sqrt{x^2+y^2}}, x + \frac{yL}{\sqrt{x^2+y^2}}, \sqrt{x^2+y^2})$$ we must find $\lambda$ such that \begin{cases} y+\frac{xL}{\sqrt{x^2+y^2}} = \lambda yL \\ x+ \frac{yL}{\sqrt{x^2+y^2}} = \lambda xL \\ \sqrt{x^2+y^2} = \lambda xy \end{cases} I have no idea how to solve this or if this even would be the correct approach. Any help is appreciated.","A tent with 2 rectangle shaped sides (no floor) and 2 isosceles triangles shaped gables with the volume $V$ is to be constructed. Determine the height so that the minimum amount of cloth is needed. The tent is a prism with isosceles triangle bases. Let the height of the triangle (i.e. the height of the tent) be denoted $x$, the base $2y$ and the length of the tent $L$. Then the volume is $$V(x,y,L)=xyL$$ and the area of the tent will be $$A(x,y,L) = 2(xy+L\sqrt{x^2+y^2}).$$ Since A is a continuous function on a compact set (or can this actually be said, since V is not a boundary but a function of the variables?), there will be a minimum and maximum value. These are found when $grad \, V \, || \, grad \, A$. Since $$ grad \, V = (yL, xL, xy)$$ and $$grad \, A =2(y + \frac{xL}{\sqrt{x^2+y^2}}, x + \frac{yL}{\sqrt{x^2+y^2}}, \sqrt{x^2+y^2})$$ we must find $\lambda$ such that \begin{cases} y+\frac{xL}{\sqrt{x^2+y^2}} = \lambda yL \\ x+ \frac{yL}{\sqrt{x^2+y^2}} = \lambda xL \\ \sqrt{x^2+y^2} = \lambda xy \end{cases} I have no idea how to solve this or if this even would be the correct approach. Any help is appreciated.",,['multivariable-calculus']
29,Evaluating a triple integral by inspection,Evaluating a triple integral by inspection,,"I would like to evaluate the triple integral: $$\iiint\limits_D {2 + 3{x^2} + 3{y^2}dV}$$ where $D$ is a conic domain with vertex $(0,0,b)$ and axis along the $z$-axis with a base (disk) with radius $a$ in the $xy$-plane My working: Since integration is a linear operation, we get: $$2\iiint\limits_D {dV + 3\iiint\limits_D {{x^2} + {y^2}dV}}$$ The first part represents the volume of the domain, hence $2\times1/3\times a^2b\pi$ (formula for volume of a cone). The second part is a bit tricky. It seems like we could change variables, but this is where I am stuck. Does anyone know how to find the value of the second integral? Edit: Using substitution (cylindrical coordinates) $$3\iiint\limits_D {{x^2} + {y^2}dV} = 3\iiint\limits_{D'} {{r^3}drd\theta dz} = 3\int_0^b {dz} \int_0^{2\pi } {d\theta } \int_0^a {{r^3}dr = a{b^3}\pi /6} $$ Unfortunately, the answer suggests $$\frac{3}{{10}}\pi {a^4}b + \pi {a^2}$$","I would like to evaluate the triple integral: $$\iiint\limits_D {2 + 3{x^2} + 3{y^2}dV}$$ where $D$ is a conic domain with vertex $(0,0,b)$ and axis along the $z$-axis with a base (disk) with radius $a$ in the $xy$-plane My working: Since integration is a linear operation, we get: $$2\iiint\limits_D {dV + 3\iiint\limits_D {{x^2} + {y^2}dV}}$$ The first part represents the volume of the domain, hence $2\times1/3\times a^2b\pi$ (formula for volume of a cone). The second part is a bit tricky. It seems like we could change variables, but this is where I am stuck. Does anyone know how to find the value of the second integral? Edit: Using substitution (cylindrical coordinates) $$3\iiint\limits_D {{x^2} + {y^2}dV} = 3\iiint\limits_{D'} {{r^3}drd\theta dz} = 3\int_0^b {dz} \int_0^{2\pi } {d\theta } \int_0^a {{r^3}dr = a{b^3}\pi /6} $$ Unfortunately, the answer suggests $$\frac{3}{{10}}\pi {a^4}b + \pi {a^2}$$",,"['calculus', 'integration', 'multivariable-calculus']"
30,Understanding a step in Yi Fang's Lectures on Minimal Surfaces,Understanding a step in Yi Fang's Lectures on Minimal Surfaces,,"In Yi Fang's Lectures on Minimal Surfaces, page $94$, there's a step that I didn't understand, and that perhaps is wrong. I'll estabilish some notation first. We have that $X$ is a minimal surface, $X(t)$ is a variation (understand here that $X(0) = X$), $g_{ij}(t)$ is the first fundamental form, $g^{ij}(t)$ is its inverse, and $h_{ij}(t)$ is the second fundamental form. We have the variational field $$E(t) = \frac{\partial X(t)}{\partial t},$$ and $E(0) \equiv E = \alpha X_1 + \beta X_2 + \gamma N$. We also assume isothermal coordinates $g_{ij} = \Lambda^2 \delta_{ij}$. This implies that $g^{ij} = \Lambda^{-2}\delta_{ij}$. He states that: $$\frac{{\rm d}g^{ij}(t)}{{\rm d}t}\Bigg|_{t=0} = -\Lambda^{-4}(E_i \cdot X_j + E_j \cdot X_i),$$ ok. Then he says: Using $h_{11} = -h_{22}$ and $X_{11}\cdot X_1 = \frac{1}{2}\Lambda_1^2$, $X_{11}\cdot X_2 = -\frac{1}{2}\Lambda_2^2$, etc, we have: $$\frac{1}{2}\sum_{i,j}\frac{{\rm d}g^{ij}(t)}{{\rm d}t}\Bigg|_{t=0}h_{ij} = \gamma \Lambda^{-4}\sum_{i,j}h_{ij}^2 - \Lambda^{-2}(\alpha_1h_{11}+(\alpha_2+\beta_1)h_{12}+\beta_2h_{22})$$ I just don't follow that. We could say that: $$X_1 \cdot X_1 = \Lambda^2 \implies 2 X_{11}\cdot X_1 = 2\Lambda_1\Lambda \implies X_{11}\cdot X_1 = \Lambda_1\Lambda,$$ and I don't see how he got that expression. Can someone explain to me how to get the highlighted expression please? If you need me to explain some more of the notation please say it.","In Yi Fang's Lectures on Minimal Surfaces, page $94$, there's a step that I didn't understand, and that perhaps is wrong. I'll estabilish some notation first. We have that $X$ is a minimal surface, $X(t)$ is a variation (understand here that $X(0) = X$), $g_{ij}(t)$ is the first fundamental form, $g^{ij}(t)$ is its inverse, and $h_{ij}(t)$ is the second fundamental form. We have the variational field $$E(t) = \frac{\partial X(t)}{\partial t},$$ and $E(0) \equiv E = \alpha X_1 + \beta X_2 + \gamma N$. We also assume isothermal coordinates $g_{ij} = \Lambda^2 \delta_{ij}$. This implies that $g^{ij} = \Lambda^{-2}\delta_{ij}$. He states that: $$\frac{{\rm d}g^{ij}(t)}{{\rm d}t}\Bigg|_{t=0} = -\Lambda^{-4}(E_i \cdot X_j + E_j \cdot X_i),$$ ok. Then he says: Using $h_{11} = -h_{22}$ and $X_{11}\cdot X_1 = \frac{1}{2}\Lambda_1^2$, $X_{11}\cdot X_2 = -\frac{1}{2}\Lambda_2^2$, etc, we have: $$\frac{1}{2}\sum_{i,j}\frac{{\rm d}g^{ij}(t)}{{\rm d}t}\Bigg|_{t=0}h_{ij} = \gamma \Lambda^{-4}\sum_{i,j}h_{ij}^2 - \Lambda^{-2}(\alpha_1h_{11}+(\alpha_2+\beta_1)h_{12}+\beta_2h_{22})$$ I just don't follow that. We could say that: $$X_1 \cdot X_1 = \Lambda^2 \implies 2 X_{11}\cdot X_1 = 2\Lambda_1\Lambda \implies X_{11}\cdot X_1 = \Lambda_1\Lambda,$$ and I don't see how he got that expression. Can someone explain to me how to get the highlighted expression please? If you need me to explain some more of the notation please say it.",,"['multivariable-calculus', 'differential-geometry', 'minimal-surfaces']"
31,Determining what set of points a curve can be expressed as a singlevariable-function,Determining what set of points a curve can be expressed as a singlevariable-function,,"The curve $$x^2y^3-3xy^2-9y+9=0$$ is given. I want to determine what points on the curve, for a neighbourhood to said points, $y$ can safely be expressed as a function of $x$. I guess what this means is that I need to find sets $A$ such that for $x_i \in A$ yields one and only one corresponding y-term $y_i$. Furthermore, this means that the y-term in the gradient in every such point is separate from zero, i.e. $$\frac{\partial f}{\partial y} (x_i, y_i) \neq 0$$ where $f$ is the the function $f : x \rightarrow y$. Would this be accurate?","The curve $$x^2y^3-3xy^2-9y+9=0$$ is given. I want to determine what points on the curve, for a neighbourhood to said points, $y$ can safely be expressed as a function of $x$. I guess what this means is that I need to find sets $A$ such that for $x_i \in A$ yields one and only one corresponding y-term $y_i$. Furthermore, this means that the y-term in the gradient in every such point is separate from zero, i.e. $$\frac{\partial f}{\partial y} (x_i, y_i) \neq 0$$ where $f$ is the the function $f : x \rightarrow y$. Would this be accurate?",,['multivariable-calculus']
32,Limit in multivariable-calculus,Limit in multivariable-calculus,,"Let $\ell$ be a straight line through origo. Determine the limit to the restriction of $$f(x,y)=xye^{-x^2y^2}$$ to $\ell$ when $x^2+y^2 \to \infty$. Also, investigate the limit $$\lim_{x^2+y^2 \to \infty} f(x,y). $$ How should I think about the restriction? As I understand it, we have a line $\ell(t) = kt, k \in \mathbb R$ and I want $f$ to follow this path which would give us $$f(t,kt) = \frac{tkt}{e^{t^2(kt)^2}} $$ and if either $t$ or $kt$ is fixed to its minimum value ($0$) we get, by letting the other variable approach $\infty$,  $$\lim_{t^2 \to \infty} f(t,0) = \frac{t \cdot 0}{e^{t^2(0)^2}} =0 $$ and $$\lim_{(kt)^2 \to \infty} f(0,kt) = \frac{0 \cdot kt}{e^{0^2(kt)^2}} = 0$$  and so the limit should be equal to 0. Is this correct? And how do I ""realize"" that the second limit doesn't exist?","Let $\ell$ be a straight line through origo. Determine the limit to the restriction of $$f(x,y)=xye^{-x^2y^2}$$ to $\ell$ when $x^2+y^2 \to \infty$. Also, investigate the limit $$\lim_{x^2+y^2 \to \infty} f(x,y). $$ How should I think about the restriction? As I understand it, we have a line $\ell(t) = kt, k \in \mathbb R$ and I want $f$ to follow this path which would give us $$f(t,kt) = \frac{tkt}{e^{t^2(kt)^2}} $$ and if either $t$ or $kt$ is fixed to its minimum value ($0$) we get, by letting the other variable approach $\infty$,  $$\lim_{t^2 \to \infty} f(t,0) = \frac{t \cdot 0}{e^{t^2(0)^2}} =0 $$ and $$\lim_{(kt)^2 \to \infty} f(0,kt) = \frac{0 \cdot kt}{e^{0^2(kt)^2}} = 0$$  and so the limit should be equal to 0. Is this correct? And how do I ""realize"" that the second limit doesn't exist?",,"['calculus', 'limits', 'multivariable-calculus']"
33,Difference between path and vector field,Difference between path and vector field,,"What is the difference between a path and a vector field? From what I understand the unit vectors $\mathbf i$, $\mathbf j$, and $\mathbf k$ are actually vector fields (constant vector fields to be exact).  Then if we have a path $$\mathbf r(t) = x(t)\mathbf i + y(t)\mathbf j + z(t)\mathbf k$$ it's just a linear combination of vector fields. $1)$ So doesn't that make it a vector field as well? $2)$ So then are paths just a specific type of vector field or are they different concepts?","What is the difference between a path and a vector field? From what I understand the unit vectors $\mathbf i$, $\mathbf j$, and $\mathbf k$ are actually vector fields (constant vector fields to be exact).  Then if we have a path $$\mathbf r(t) = x(t)\mathbf i + y(t)\mathbf j + z(t)\mathbf k$$ it's just a linear combination of vector fields. $1)$ So doesn't that make it a vector field as well? $2)$ So then are paths just a specific type of vector field or are they different concepts?",,['multivariable-calculus']
34,"Verify that the set $\Omega = \lbrace (u,v) \in \mathbb{R}^2 \mid |u| + |v| \leq 1 \rbrace$ is Jordan measurable",Verify that the set  is Jordan measurable,"\Omega = \lbrace (u,v) \in \mathbb{R}^2 \mid |u| + |v| \leq 1 \rbrace","Motivation : I am currently in a rather uncomfortable spot in my Analysis studies. In class we introduced the Jordan measure in a very vague way, meaning no proofs, no examples. (Because next Semester we study the better Lebesgue-Measure theory) I do understand the concepts of it as a generalization of the 1 Dimensional case. That is, approximating a given bounded set $\Omega\subset \mathbb{R}^n$ from above and below by rectangles (boxes, simple sets) and if it happens that the smallest of all the approximations from above is equal to the largest of all the approximations from below, we say by definition that $\Omega$ is Jordan measurable. However when it comes to applying said intuitive idea I run into troubles Problem : Show that the set $\Omega := \lbrace (u,v) \in \mathbb{R}^2 \mid |u| + |v| \leq 1 \rbrace \subset \mathbb{R}^2$ is Jordan measurable. My approach : Clearly the set is bounded, from below by $0$ and from above $1$ . I don't know how to work with the definition as explained above in the motivation to show that said set is Jordan measurable. The best thing I could do was to cheat my way around by computing the following picture: The red line shows the boundary $\partial \Omega$ of the set. Since it is only a line segment in $\mathbb{R}^2$ it has Jordan measure zero respective to the topology in $\mathbb{R}^2$ . According to How to prove $E\subset R^n$ Jordan measurable is equivalent to $\bar{E}-E$ is Jordan measured null this would 'show' that $\Omega$ is Jordan measurable. My question is if I can formalize this idea into an actual proof rather than my hand wavy  explanation above. Or even better, is it possible to show that $\Omega$ is Jordan measurable by just relying on the definition of above/below approximations and check the equality of the infimum and supremum? Because said definition is so far what I understand best, the iff statement linked is only covered in the $\implies$ direction in my course.","Motivation : I am currently in a rather uncomfortable spot in my Analysis studies. In class we introduced the Jordan measure in a very vague way, meaning no proofs, no examples. (Because next Semester we study the better Lebesgue-Measure theory) I do understand the concepts of it as a generalization of the 1 Dimensional case. That is, approximating a given bounded set from above and below by rectangles (boxes, simple sets) and if it happens that the smallest of all the approximations from above is equal to the largest of all the approximations from below, we say by definition that is Jordan measurable. However when it comes to applying said intuitive idea I run into troubles Problem : Show that the set is Jordan measurable. My approach : Clearly the set is bounded, from below by and from above . I don't know how to work with the definition as explained above in the motivation to show that said set is Jordan measurable. The best thing I could do was to cheat my way around by computing the following picture: The red line shows the boundary of the set. Since it is only a line segment in it has Jordan measure zero respective to the topology in . According to How to prove $E\subset R^n$ Jordan measurable is equivalent to $\bar{E}-E$ is Jordan measured null this would 'show' that is Jordan measurable. My question is if I can formalize this idea into an actual proof rather than my hand wavy  explanation above. Or even better, is it possible to show that is Jordan measurable by just relying on the definition of above/below approximations and check the equality of the infimum and supremum? Because said definition is so far what I understand best, the iff statement linked is only covered in the direction in my course.","\Omega\subset \mathbb{R}^n \Omega \Omega := \lbrace (u,v) \in \mathbb{R}^2 \mid |u| + |v| \leq 1 \rbrace \subset \mathbb{R}^2 0 1 \partial \Omega \mathbb{R}^2 \mathbb{R}^2 \Omega \Omega \implies","['analysis', 'measure-theory', 'multivariable-calculus']"
35,Derivative of function of matrix vector product,Derivative of function of matrix vector product,,"Suppose we have $$ f(W) = g(Wx) $$ with $g:\mathbb{R}^n \rightarrow \mathbb{R}$, $W \in \mathbb{R}^{n \times n}, x \in \mathbb{R}^n$. I know that the Jacobian w/r/t $W$ is: $$ J_{W} (f) = x J_{(Wx)}(g)$$ One can derive this using elementwise reasoning. However, I find this confusing because the ""typical case"" is that the Jacobian of a composition is just the successive multiplication of Jacobians in the same order as written in the composition. For example: $$ J_{x} (g(Wx)) = J_{(Wx)}(g)W$$ I have to use formulations like this alot, and I have taken to more or less ""memorizing"" the reversed order of $J_W(f)$ as a special case -- even though I could re-derive it, it sticks out as nonintuitive. I have a notion that this might be explainable either using tensor products or using differentials (and indeed that if tensor products are used, we could somehow show that the Jacobian tensor is being left multiplied). But I'm not sure how to proceed, even after consulting Wikipedia and the Matrix Cookbook. Without reasoning on an element-by-element basis, how could you derive $J_W(f)$ and explain the apparently puzzling ""swap"" in order relative to the ""usual"" case?","Suppose we have $$ f(W) = g(Wx) $$ with $g:\mathbb{R}^n \rightarrow \mathbb{R}$, $W \in \mathbb{R}^{n \times n}, x \in \mathbb{R}^n$. I know that the Jacobian w/r/t $W$ is: $$ J_{W} (f) = x J_{(Wx)}(g)$$ One can derive this using elementwise reasoning. However, I find this confusing because the ""typical case"" is that the Jacobian of a composition is just the successive multiplication of Jacobians in the same order as written in the composition. For example: $$ J_{x} (g(Wx)) = J_{(Wx)}(g)W$$ I have to use formulations like this alot, and I have taken to more or less ""memorizing"" the reversed order of $J_W(f)$ as a special case -- even though I could re-derive it, it sticks out as nonintuitive. I have a notion that this might be explainable either using tensor products or using differentials (and indeed that if tensor products are used, we could somehow show that the Jacobian tensor is being left multiplied). But I'm not sure how to proceed, even after consulting Wikipedia and the Matrix Cookbook. Without reasoning on an element-by-element basis, how could you derive $J_W(f)$ and explain the apparently puzzling ""swap"" in order relative to the ""usual"" case?",,"['multivariable-calculus', 'derivatives']"
36,What is the Hessian matrix of $x\mapsto f(Ax+b)$?,What is the Hessian matrix of ?,x\mapsto f(Ax+b),Let $A\in\mathbb{R}^{n\times n}$ and $b\in\mathbb{R}^n$ $f\in C^2(\mathbb{R}^n)$ and $\tilde{f}(x):=f(Ax+b)$ for $x\in\mathbb{R}^n$ It's easy to prove that $$\nabla\tilde{f}(x)=A^T\nabla f(x)$$ But I'm not able to prove that the Hessian matrix $$\nabla^2\tilde{f}(x)=A^T\nabla^2 f(x)A$$ Shouldn't we have $$\nabla^2\tilde{f}(x)=\left(\begin{matrix}\frac{\partial}{\partial x_1}\\\vdots\\\frac{\partial}{\partial x_n} \end{matrix}\right)A^T\nabla f(x)=A\left(\begin{matrix}\frac{\partial}{\partial x_1}\\\vdots\\\frac{\partial}{\partial x_n} \end{matrix}\right)\nabla f(x)=A\nabla^2f(x)\;?$$,Let $A\in\mathbb{R}^{n\times n}$ and $b\in\mathbb{R}^n$ $f\in C^2(\mathbb{R}^n)$ and $\tilde{f}(x):=f(Ax+b)$ for $x\in\mathbb{R}^n$ It's easy to prove that $$\nabla\tilde{f}(x)=A^T\nabla f(x)$$ But I'm not able to prove that the Hessian matrix $$\nabla^2\tilde{f}(x)=A^T\nabla^2 f(x)A$$ Shouldn't we have $$\nabla^2\tilde{f}(x)=\left(\begin{matrix}\frac{\partial}{\partial x_1}\\\vdots\\\frac{\partial}{\partial x_n} \end{matrix}\right)A^T\nabla f(x)=A\left(\begin{matrix}\frac{\partial}{\partial x_1}\\\vdots\\\frac{\partial}{\partial x_n} \end{matrix}\right)\nabla f(x)=A\nabla^2f(x)\;?$$,,"['calculus', 'analysis', 'multivariable-calculus', 'derivatives']"
37,Continuity of the integral as a function of the domain,Continuity of the integral as a function of the domain,,"Let $f: \mathbb{R}^n \to \mathbb{R}$ be integrable. Let $C \subset \mathbb{R}^n$ be measurable. Is $$ r \mapsto \int_{rC} f \, \mathrm{d} \mu, $$ where $rC = \left\{ rc \: \middle| \: c \in C \right\}$ continuous? If not, is it under some more restricted contiditons on $f$ or $C$ i.e. compactness?","Let $f: \mathbb{R}^n \to \mathbb{R}$ be integrable. Let $C \subset \mathbb{R}^n$ be measurable. Is $$ r \mapsto \int_{rC} f \, \mathrm{d} \mu, $$ where $rC = \left\{ rc \: \middle| \: c \in C \right\}$ continuous? If not, is it under some more restricted contiditons on $f$ or $C$ i.e. compactness?",,"['calculus', 'multivariable-calculus']"
38,"Find $\nabla \cdot (f\textbf r)$ and $\nabla \times (f\textbf r)$ of the function $f(x,y,z) = (x^2+y^2)\log(1-z)$",Find  and  of the function,"\nabla \cdot (f\textbf r) \nabla \times (f\textbf r) f(x,y,z) = (x^2+y^2)\log(1-z)","I have been given the function $f(x,y,z) = (x^2+y^2)\log(1-z)$ and I need to find the divergence $\nabla \cdot (f\textbf r)$ and curl $\nabla \times (f\textbf r)$ where $\textbf r$ is the position vector. I understand that $$\nabla \cdot F = \frac{\partial F_x}{\partial x} + \frac{\partial F_y}{\partial y} + \frac{\partial F_z}{\partial z}$$ and that $$\nabla \times F = \begin{bmatrix}                                 i & j & k \\                                 \frac{\partial}{\partial x} & \frac{\partial}{\partial y} & \frac{\partial}{\partial z} \\                                 F_x & F_y & F_z \\                             \end{bmatrix}                             $$ I just don't know how to proceed with this question.","I have been given the function $f(x,y,z) = (x^2+y^2)\log(1-z)$ and I need to find the divergence $\nabla \cdot (f\textbf r)$ and curl $\nabla \times (f\textbf r)$ where $\textbf r$ is the position vector. I understand that $$\nabla \cdot F = \frac{\partial F_x}{\partial x} + \frac{\partial F_y}{\partial y} + \frac{\partial F_z}{\partial z}$$ and that $$\nabla \times F = \begin{bmatrix}                                 i & j & k \\                                 \frac{\partial}{\partial x} & \frac{\partial}{\partial y} & \frac{\partial}{\partial z} \\                                 F_x & F_y & F_z \\                             \end{bmatrix}                             $$ I just don't know how to proceed with this question.",,['multivariable-calculus']
39,An Application of Stokes's Theorem,An Application of Stokes's Theorem,,"Let $D^2=\{(x,y)\in \mathbf R^2: x^2+y^2\leq 1\}$ be the unit disc in $\mathbf R^2$, and $D^3=\{(x,y,z)\in \mathbf R^3: x^2+y^2+y^2\leq 1\}$ be the unit disc in $\mathbf R^3$. Let $i_{\pm}:D^2\to D^3$ be two maps defined as   $$ i_{\pm}(x, y)=\left(x, y, \pm\sqrt{(1-x^2-y^2)}\right) $$   Let $\omega$ be a closed $2$-form in a smooth manifold $M$, and $F:D^3\to M$ be a smooth map.   Then I need to show that   $$ \int_{D^2}(F\circ i_+)^*\omega = \int_{D^2} (F\circ i_-)^*\omega $$   where $D^2$ has the standard orientation governed by the form $dx\wedge dy$. Attempt: Since $\omega$ is a closed form, it seems that Stokes's Theorem might be useful. Define $H_+=\{\left(x, y, \sqrt{1-x^2-y^2}\right): (x, y)\in D^2\}$ and $H_-=\{\left(x, y, -\sqrt{1-x^2-y^2}\right): (x,y)\in D^2\}$. Now $H_+\cup D^2$ is the boundary of the ""upper solid half unit disc"" $D^3_+$ in $\mathbf R^3$ and $H_-\cup D^2$ is the boundary of the ""lower solid half unit disc"" $D^3_-$ in $\mathbf R^3$. By Stokes we have $$ \int_{H_+\cup D^2}i_+^*(F^* \omega)= \int_{D^3_+}d(F^* \omega))=\int_{D^3_+} F^* d\omega = 0 $$ Therefore $$ \int_{D^2}i_+^*(F^*\omega) = -\int_{H_+} i_+^*(F^*\omega) $$ Similarly $$ \int_{D^2}i_-^*(F^*\omega) = -\int_{H_-} i_-^*(F^*\omega) $$ How do I proceed from here? Thanks.","Let $D^2=\{(x,y)\in \mathbf R^2: x^2+y^2\leq 1\}$ be the unit disc in $\mathbf R^2$, and $D^3=\{(x,y,z)\in \mathbf R^3: x^2+y^2+y^2\leq 1\}$ be the unit disc in $\mathbf R^3$. Let $i_{\pm}:D^2\to D^3$ be two maps defined as   $$ i_{\pm}(x, y)=\left(x, y, \pm\sqrt{(1-x^2-y^2)}\right) $$   Let $\omega$ be a closed $2$-form in a smooth manifold $M$, and $F:D^3\to M$ be a smooth map.   Then I need to show that   $$ \int_{D^2}(F\circ i_+)^*\omega = \int_{D^2} (F\circ i_-)^*\omega $$   where $D^2$ has the standard orientation governed by the form $dx\wedge dy$. Attempt: Since $\omega$ is a closed form, it seems that Stokes's Theorem might be useful. Define $H_+=\{\left(x, y, \sqrt{1-x^2-y^2}\right): (x, y)\in D^2\}$ and $H_-=\{\left(x, y, -\sqrt{1-x^2-y^2}\right): (x,y)\in D^2\}$. Now $H_+\cup D^2$ is the boundary of the ""upper solid half unit disc"" $D^3_+$ in $\mathbf R^3$ and $H_-\cup D^2$ is the boundary of the ""lower solid half unit disc"" $D^3_-$ in $\mathbf R^3$. By Stokes we have $$ \int_{H_+\cup D^2}i_+^*(F^* \omega)= \int_{D^3_+}d(F^* \omega))=\int_{D^3_+} F^* d\omega = 0 $$ Therefore $$ \int_{D^2}i_+^*(F^*\omega) = -\int_{H_+} i_+^*(F^*\omega) $$ Similarly $$ \int_{D^2}i_-^*(F^*\omega) = -\int_{H_-} i_-^*(F^*\omega) $$ How do I proceed from here? Thanks.",,"['integration', 'multivariable-calculus', 'differential-geometry', 'differential-forms', 'smooth-manifolds']"
40,Divergence and Curl (involving constant vectors),Divergence and Curl (involving constant vectors),,"How find the divergence and Curl of the following: $(\vec{a} \cdot \vec{r}) \vec{b}$, where $\vec{a}$ and $\vec{b}$ are the constant vectors and $\vec{r}$ is the radius vector. I have tried solving this by supposing $\vec{r} = (x,y,z)$ and got answer as div($(\vec{a} \cdot \vec{r}) \vec{b}$) = $\vec{a} \cdot \vec{b}$ but I was wondering if anybody could help me to solve it by using the formulas involving Nabla Operator. Thanks","How find the divergence and Curl of the following: $(\vec{a} \cdot \vec{r}) \vec{b}$, where $\vec{a}$ and $\vec{b}$ are the constant vectors and $\vec{r}$ is the radius vector. I have tried solving this by supposing $\vec{r} = (x,y,z)$ and got answer as div($(\vec{a} \cdot \vec{r}) \vec{b}$) = $\vec{a} \cdot \vec{b}$ but I was wondering if anybody could help me to solve it by using the formulas involving Nabla Operator. Thanks",,"['multivariable-calculus', 'vector-analysis']"
41,Cartesian into polar integral.,Cartesian into polar integral.,,"I have set up a double integral in course of proving Gauss theorem in physics. I am considering a gaussian cube of edge $a$ and I supposed that mid point of cube is at origin and a charge is placed at this origin. The integral is as follow: $$\dfrac{qa}{2(4\pi\epsilon_0)}\int_{-a/2}^{a/2}\int_{-a/2}^{a/2}\dfrac{dx\;dy}{\left(\frac{a^2}{4}+x^2+y^2\right)^{\frac{3}{2}}}$$ In this integral, $z$ coordinate is fixed on the top surface and is at a height of $\dfrac{a}{2}$ away from origin. Sorry for extremely poor quality of the image, I am on a bad laptop at the moment. Basically, I could integrated this in Cartesian coordinates and it do yield the correct answer. But I want to obtain the result in spherical 3D coordinates. And I am really having trouble changing the limits. Is there anyway to do it in polar?  I took the following substitution $x=r\cos\theta \sin\phi$ , $y=r\sin\theta \sin\phi$ , $\dfrac{a}{2}\text{(z=constant)}=r\cos\phi$ But it didn't yield anything good. And please try to keep the answer simple, as i am high school student, thank you.","I have set up a double integral in course of proving Gauss theorem in physics. I am considering a gaussian cube of edge and I supposed that mid point of cube is at origin and a charge is placed at this origin. The integral is as follow: In this integral, coordinate is fixed on the top surface and is at a height of away from origin. Sorry for extremely poor quality of the image, I am on a bad laptop at the moment. Basically, I could integrated this in Cartesian coordinates and it do yield the correct answer. But I want to obtain the result in spherical 3D coordinates. And I am really having trouble changing the limits. Is there anyway to do it in polar?  I took the following substitution , , But it didn't yield anything good. And please try to keep the answer simple, as i am high school student, thank you.",a \dfrac{qa}{2(4\pi\epsilon_0)}\int_{-a/2}^{a/2}\int_{-a/2}^{a/2}\dfrac{dx\;dy}{\left(\frac{a^2}{4}+x^2+y^2\right)^{\frac{3}{2}}} z \dfrac{a}{2} x=r\cos\theta \sin\phi y=r\sin\theta \sin\phi \dfrac{a}{2}\text{(z=constant)}=r\cos\phi,"['integration', 'multivariable-calculus', 'polar-coordinates']"
42,Riemann Integral on $\mathbb{R}^2$,Riemann Integral on,\mathbb{R}^2,"I have the following question. Find a function $f(x,y)$ that is integrable on rectangle $[0,1] \times [0,1]$, such that $g(y) = f(\frac{1}{2}, y)$ is not integrable for $y \in [0,1]$, or prove that it is impossible. I think it is not possible. The reason is that if $f$ is integrable on $[0,1] \times [0,1]$, then $g$ will be a function over the set $\{(\frac{1}{2}, y) : y \in [0,1]\}$. What I have tried is following. Let $R = [0,1] \times [0,1]$. Since $f$ is integrable on $R$, there is a partition $P = \{ 0 = x_0, \ldots , x_n = 1 ; 0 = y_0 , \ldots , y_m = 1 \}$ such that the lower Riemann sum $s_Pf$ and the upper Riemann sum $S_Pf$ so that the lower and upper integrals are equal. I am stuck here and I am not even sure if I am on the right track. I am was thinking that I could prove it if I fix the partition $P$ such that $P = \{ \frac{1}{2} ; 0 = y_0 , \ldots , y_m = 1 \}$ and show that $g$ is integrable using the Cauchy Criterion for integrability. Can anyone help me with this?","I have the following question. Find a function $f(x,y)$ that is integrable on rectangle $[0,1] \times [0,1]$, such that $g(y) = f(\frac{1}{2}, y)$ is not integrable for $y \in [0,1]$, or prove that it is impossible. I think it is not possible. The reason is that if $f$ is integrable on $[0,1] \times [0,1]$, then $g$ will be a function over the set $\{(\frac{1}{2}, y) : y \in [0,1]\}$. What I have tried is following. Let $R = [0,1] \times [0,1]$. Since $f$ is integrable on $R$, there is a partition $P = \{ 0 = x_0, \ldots , x_n = 1 ; 0 = y_0 , \ldots , y_m = 1 \}$ such that the lower Riemann sum $s_Pf$ and the upper Riemann sum $S_Pf$ so that the lower and upper integrals are equal. I am stuck here and I am not even sure if I am on the right track. I am was thinking that I could prove it if I fix the partition $P$ such that $P = \{ \frac{1}{2} ; 0 = y_0 , \ldots , y_m = 1 \}$ and show that $g$ is integrable using the Cauchy Criterion for integrability. Can anyone help me with this?",,"['integration', 'multivariable-calculus']"
43,Is it possible to have $|f(x) - f(y)| \leq M\| x - y \|$ under such conditions?,Is it possible to have  under such conditions?,|f(x) - f(y)| \leq M\| x - y \|,"Let $f: A \to \mathbb{R}$ be differentiable on an open convex $A \subset \mathbb{R}^{n}.$ If $\| \nabla f \| \leq M$ on $A$ for some $M > 0,$ is it possible to have $$|f(x) - f(y)| \leq M \| x - y \|$$ for all $x, y \in A$? I do not think so. I went thus far, as the following indicates. By the mean-value theorem, given $x, y \in A,$ there is a $c$ in the line segment joining $x$ and $y$ such that $$|f(x) - f(y)| = \big| \nabla f(c)\cdot (x-y) \big| = \bigg| \big[ D_{1}f(c), \dots, D_{n}f(c) \big] \big[ x_{1} - y_{1}, \dots, x_{n} - y_{n} \big]^{\top}\bigg|\\ = \bigg| \sum_{1}^{n}D_{i}f(c)(x_{i} - y_{i}) \bigg| \leq \bigg| \sum_{1}^{n}D_{i}f(c) \bigg| \bigg| \sum_{1}^{n}(x_{i} - y_{i}) \bigg|.$$ Since $\| \nabla f \| \leq M,$ we have  $$\sum_{1}^{n}\big[ D_{i}f(c) \big]^{2} \leq M.$$ But since $\bigg| \sum_{1}^{n}D_{i}f(c) \bigg|$ need not be less than $\sum_{1}^{n}\big[ D_{i}f(c) \big]^{2}$ and $\bigg| \sum_{1}^{n}(x_{i} - y_{i})\bigg| $ need not be less than $\sum_{1}^{n}(x_{i} - y_{i})^{2},$ so we need not have the desired conclusion, ?","Let $f: A \to \mathbb{R}$ be differentiable on an open convex $A \subset \mathbb{R}^{n}.$ If $\| \nabla f \| \leq M$ on $A$ for some $M > 0,$ is it possible to have $$|f(x) - f(y)| \leq M \| x - y \|$$ for all $x, y \in A$? I do not think so. I went thus far, as the following indicates. By the mean-value theorem, given $x, y \in A,$ there is a $c$ in the line segment joining $x$ and $y$ such that $$|f(x) - f(y)| = \big| \nabla f(c)\cdot (x-y) \big| = \bigg| \big[ D_{1}f(c), \dots, D_{n}f(c) \big] \big[ x_{1} - y_{1}, \dots, x_{n} - y_{n} \big]^{\top}\bigg|\\ = \bigg| \sum_{1}^{n}D_{i}f(c)(x_{i} - y_{i}) \bigg| \leq \bigg| \sum_{1}^{n}D_{i}f(c) \bigg| \bigg| \sum_{1}^{n}(x_{i} - y_{i}) \bigg|.$$ Since $\| \nabla f \| \leq M,$ we have  $$\sum_{1}^{n}\big[ D_{i}f(c) \big]^{2} \leq M.$$ But since $\bigg| \sum_{1}^{n}D_{i}f(c) \bigg|$ need not be less than $\sum_{1}^{n}\big[ D_{i}f(c) \big]^{2}$ and $\bigg| \sum_{1}^{n}(x_{i} - y_{i})\bigg| $ need not be less than $\sum_{1}^{n}(x_{i} - y_{i})^{2},$ so we need not have the desired conclusion, ?",,"['multivariable-calculus', 'proof-verification']"
44,How do I prove that $\alpha/\lVert\alpha\rVert$ is differentiable?,How do I prove that  is differentiable?,\alpha/\lVert\alpha\rVert,Let $\alpha\colon I\rightarrow \mathbb{R}^3$ be a $C^2$-curve such that $\alpha(t)\neq 0$. How do I prove that $\alpha/\lVert\alpha\lVert$ is differentiable?,Let $\alpha\colon I\rightarrow \mathbb{R}^3$ be a $C^2$-curve such that $\alpha(t)\neq 0$. How do I prove that $\alpha/\lVert\alpha\lVert$ is differentiable?,,"['analysis', 'multivariable-calculus']"
45,The path of the shock,The path of the shock,,Here I am using the shock speed to work out the path the shock takes. I don't understand why we cannot take the value of $u_{-}$ at $t=1/u_0$  i.e $u_{-}=u_0$. and calculate the speed of the shock there ($\frac{ds}{dt}$) and hence work out the shock path $x=s(t)$. Why do we need the value of $u_-$ in its more general form?,Here I am using the shock speed to work out the path the shock takes. I don't understand why we cannot take the value of $u_{-}$ at $t=1/u_0$  i.e $u_{-}=u_0$. and calculate the speed of the shock there ($\frac{ds}{dt}$) and hence work out the shock path $x=s(t)$. Why do we need the value of $u_-$ in its more general form?,,['multivariable-calculus']
46,Show that an open subset $U \subset \mathbb{H^n}$ is open in $\mathbb{R^n}$ iff $U \cap \partial\mathbb{H^n}=\phi$,Show that an open subset  is open in  iff,U \subset \mathbb{H^n} \mathbb{R^n} U \cap \partial\mathbb{H^n}=\phi,"Consider the $n-$dimensional closed half-space $\mathbb{H^n}=\{x \in \mathbb{R^n}|x_1 \le 0 \}$ and let $\partial \mathbb{H^n}=\{x \in \mathbb{H}|x_1=0\}$ be its boundary. Show that an open subset $U \subset \mathbb{H^n}$ is open in $\mathbb{R^n}$ iff $U \cap \partial\mathbb{H^n}=\phi$ My try: Suppose that an open subset $U \subset \mathbb{H^n}$ is open in $\mathbb{R^n}$. Then let's assume that $\vec{x} \in U \cap \partial\mathbb{H^n}$.Since $U$ is open in $\mathbb{R^n}$, there exists $r \gt 0$ such that $B_{d}(\vec{x},r) \subset U$.  Let $\vec{y}=(y_1,y')\in \mathbb{H^n} \cap B_{d}(\vec{x},r) $ such that $y_1 \lt 0$.   $z=(-y_1,y') \in B_{d}(\vec{x},r) $ but $z \not \in B_d(\vec{x},r) \cap \mathbb{H^n}$.  I believe this should lead to a contradiction at the point $x$. For some reason I am unable to see it. For the other side since $ U$ is open in $\mathbb{H^n}$, $U$ can be written as some $V \cap \mathbb{H^n}$, where $V$ is open in $\mathbb{R^n}$. Since  $U \cap \partial\mathbb{H^n}=\phi$  , we  have $V \cap \partial\mathbb{H^n}=\phi$.Thus $V \subset \mathbb{H^n}$. Thus $U=V$. Is this part alright? Thanks for the help!!","Consider the $n-$dimensional closed half-space $\mathbb{H^n}=\{x \in \mathbb{R^n}|x_1 \le 0 \}$ and let $\partial \mathbb{H^n}=\{x \in \mathbb{H}|x_1=0\}$ be its boundary. Show that an open subset $U \subset \mathbb{H^n}$ is open in $\mathbb{R^n}$ iff $U \cap \partial\mathbb{H^n}=\phi$ My try: Suppose that an open subset $U \subset \mathbb{H^n}$ is open in $\mathbb{R^n}$. Then let's assume that $\vec{x} \in U \cap \partial\mathbb{H^n}$.Since $U$ is open in $\mathbb{R^n}$, there exists $r \gt 0$ such that $B_{d}(\vec{x},r) \subset U$.  Let $\vec{y}=(y_1,y')\in \mathbb{H^n} \cap B_{d}(\vec{x},r) $ such that $y_1 \lt 0$.   $z=(-y_1,y') \in B_{d}(\vec{x},r) $ but $z \not \in B_d(\vec{x},r) \cap \mathbb{H^n}$.  I believe this should lead to a contradiction at the point $x$. For some reason I am unable to see it. For the other side since $ U$ is open in $\mathbb{H^n}$, $U$ can be written as some $V \cap \mathbb{H^n}$, where $V$ is open in $\mathbb{R^n}$. Since  $U \cap \partial\mathbb{H^n}=\phi$  , we  have $V \cap \partial\mathbb{H^n}=\phi$.Thus $V \subset \mathbb{H^n}$. Thus $U=V$. Is this part alright? Thanks for the help!!",,"['general-topology', 'analysis', 'multivariable-calculus']"
47,How to solve the line integral?,How to solve the line integral?,,"Evaluate the integral $$ \int_c (x+xy+y) \, ds $$ where $C$ is the path of the arc along the circle given by $x^2+y^2=4$ starting at the point $(2,0)$ going counterclockwise making an inscribed angle of $\frac76\pi$ I started this problem by letting $x=2\cos t$ and $y=2\sin t$. I know that because it is counterclockwise it is positive. Then I started by taking the integral $$ \int_0^{\frac76\pi} (2\cos t +4\cos t\sin t + 2\sin t)\,dt $$ I solve the integral and got $-2.5+\sqrt3$ but this answer is incorrect. I think the values I have for $x$ and $y$ is correct but am I supposed to take the intergral from $0$ to $2$? Can someone help me?","Evaluate the integral $$ \int_c (x+xy+y) \, ds $$ where $C$ is the path of the arc along the circle given by $x^2+y^2=4$ starting at the point $(2,0)$ going counterclockwise making an inscribed angle of $\frac76\pi$ I started this problem by letting $x=2\cos t$ and $y=2\sin t$. I know that because it is counterclockwise it is positive. Then I started by taking the integral $$ \int_0^{\frac76\pi} (2\cos t +4\cos t\sin t + 2\sin t)\,dt $$ I solve the integral and got $-2.5+\sqrt3$ but this answer is incorrect. I think the values I have for $x$ and $y$ is correct but am I supposed to take the intergral from $0$ to $2$? Can someone help me?",,"['integration', 'multivariable-calculus']"
48,Finding a line integral along the curve of intersection of two surfaces,Finding a line integral along the curve of intersection of two surfaces,,"Find \begin{align*} \int_C \sqrt{1+4x^2 z^2} ds, \end{align*} where $C$ is the curve of intersection of the surfaces $x^2 + z^2 = 1$ and $y = x^2$. Attempt at solution: So first I need a parametrization of this curve. I let $x = t$. Then we have $y = t^2$ and $z = +- \sqrt{1-t^2}$. But I'm not sure what sign I should pick here, and what my integration bounds are? Any help would be appreciated.","Find \begin{align*} \int_C \sqrt{1+4x^2 z^2} ds, \end{align*} where $C$ is the curve of intersection of the surfaces $x^2 + z^2 = 1$ and $y = x^2$. Attempt at solution: So first I need a parametrization of this curve. I let $x = t$. Then we have $y = t^2$ and $z = +- \sqrt{1-t^2}$. But I'm not sure what sign I should pick here, and what my integration bounds are? Any help would be appreciated.",,"['calculus', 'multivariable-calculus']"
49,Generalization of $\int_0^\infty \frac{e^{-cx} - e^{-dx}}{x} dx = \log(d/c)$ for $0<c<d$,Generalization of  for,\int_0^\infty \frac{e^{-cx} - e^{-dx}}{x} dx = \log(d/c) 0<c<d,"I am trying to procure a generlization of the following result: $\displaystyle \int_0^\infty \frac{e^{-cx} - e^{-dx}}{x} dx = \log(d/c)$ for $0<c<d$ This result is obtained by considering $$\int_a^b \int_c^d e^{-xy} dy dx = \int_c^d \int_a^b e^{-xy} dx dy$$ And the hint tells us to begin with $$\int_a^b \int_c^d \varphi' (xy) dy dx$$ and introduce hypotheses in $\varphi$ where needed to justify the argument. The problem I have is that this problem is very vague (doesn't tell us what result we should get, and doesn't tell us what ""suitable hypotheses"" are), and I don't see a natural generalization of the result. However, this is what I have so far: The first hypothesis I introduce is that $\frac{d}{dy} \varphi = \frac{d}{dx} \varphi$, so that I can switch integrals. So, I begin by integrating the inner integrals: $$\int_a^b \varphi(dx) - \varphi(cx) dx = \int_c^d \varphi(ay) - \varphi(by) dy.$$ Let $a \to 0$ and $b \to \infty$. So I suppose that $\lim \limits_{a \to 0} \varphi(ay) = A$ and $\lim \limits_{b \to \infty} \varphi(by) = B$ for fixed, finite $y$. Now this becomes $$\int_0^\infty \varphi(dx) - \varphi(cx) dx = \int_c^d (A - B) dy$$ but now I feel like I've gone off track because the result is different from the result that we're supposed to be generalizing.","I am trying to procure a generlization of the following result: $\displaystyle \int_0^\infty \frac{e^{-cx} - e^{-dx}}{x} dx = \log(d/c)$ for $0<c<d$ This result is obtained by considering $$\int_a^b \int_c^d e^{-xy} dy dx = \int_c^d \int_a^b e^{-xy} dx dy$$ And the hint tells us to begin with $$\int_a^b \int_c^d \varphi' (xy) dy dx$$ and introduce hypotheses in $\varphi$ where needed to justify the argument. The problem I have is that this problem is very vague (doesn't tell us what result we should get, and doesn't tell us what ""suitable hypotheses"" are), and I don't see a natural generalization of the result. However, this is what I have so far: The first hypothesis I introduce is that $\frac{d}{dy} \varphi = \frac{d}{dx} \varphi$, so that I can switch integrals. So, I begin by integrating the inner integrals: $$\int_a^b \varphi(dx) - \varphi(cx) dx = \int_c^d \varphi(ay) - \varphi(by) dy.$$ Let $a \to 0$ and $b \to \infty$. So I suppose that $\lim \limits_{a \to 0} \varphi(ay) = A$ and $\lim \limits_{b \to \infty} \varphi(by) = B$ for fixed, finite $y$. Now this becomes $$\int_0^\infty \varphi(dx) - \varphi(cx) dx = \int_c^d (A - B) dy$$ but now I feel like I've gone off track because the result is different from the result that we're supposed to be generalizing.",,['multivariable-calculus']
50,Orthogonal Level Sets and a generalization of harmonic functions,Orthogonal Level Sets and a generalization of harmonic functions,,"Forgive my ignorance of differential equations and analysis.  I was playing around with orthogonal level curves of real valued functions in the plane, and realized this is one way a person could be led to harmonic functions: The function $f:\mathbb{R}^2 \rightarrow \mathbb{R}$ has gradient $$ \nabla f(x,y) = <f_x,f_y> $$ If we wanted a function $g$ so that the level sets of $g$ are orthogonal to those of $f$ then we can just take their gradients to be orthogonal.  The easiest way to do that is to find a $g$ so that $$\nabla g = <-f_y,f_x> $$ But, any conservative vector field $<P,Q>$ must satisfy $P_y = Q_x$, which leads to the condition that $$-f_{yy} = f_{xx}$$ i.e. that $\Delta f$, the Laplacian, must be zero. Now, suppose that we didn't assume that $\nabla g$ were not exactly $<-f_y,f_x>$, but instead some arbitrary multiple at each point, so that the level sets are still orthogonal.  Does this give you a more general class of functions? Then the condition becomes $$\nabla g = \lambda(x,y) <-f_y,f_x>$$ for some positive function $\lambda$.  Then, by the product rule, and using the same $P_y = Q_x$ condition above, we get that  $$-\lambda_y f_y -\lambda f_{yy} = \lambda_x f_x + \lambda f_{xx}$$  Or in another form $$\lambda \Delta f +\nabla \lambda \cdot \nabla f = 0$$ Dividing through by $\lambda$ and subtracting over, one could also write this as $$\Delta f = \nabla (-log(\lambda)) \cdot \nabla f$$ So the question becomes, for a function $f$, does there exist some function $h$ so that  $$\Delta f = \nabla(f) \cdot \nabla(h)?$$ Clearly, harmonic functions satisfy this with $\lambda = 1$ and $h = 0$.  Are there functions that satisfy this that aren't harmonic? Is this a well studied class of functions?  Again, forgive me as this might be a stupid question.","Forgive my ignorance of differential equations and analysis.  I was playing around with orthogonal level curves of real valued functions in the plane, and realized this is one way a person could be led to harmonic functions: The function $f:\mathbb{R}^2 \rightarrow \mathbb{R}$ has gradient $$ \nabla f(x,y) = <f_x,f_y> $$ If we wanted a function $g$ so that the level sets of $g$ are orthogonal to those of $f$ then we can just take their gradients to be orthogonal.  The easiest way to do that is to find a $g$ so that $$\nabla g = <-f_y,f_x> $$ But, any conservative vector field $<P,Q>$ must satisfy $P_y = Q_x$, which leads to the condition that $$-f_{yy} = f_{xx}$$ i.e. that $\Delta f$, the Laplacian, must be zero. Now, suppose that we didn't assume that $\nabla g$ were not exactly $<-f_y,f_x>$, but instead some arbitrary multiple at each point, so that the level sets are still orthogonal.  Does this give you a more general class of functions? Then the condition becomes $$\nabla g = \lambda(x,y) <-f_y,f_x>$$ for some positive function $\lambda$.  Then, by the product rule, and using the same $P_y = Q_x$ condition above, we get that  $$-\lambda_y f_y -\lambda f_{yy} = \lambda_x f_x + \lambda f_{xx}$$  Or in another form $$\lambda \Delta f +\nabla \lambda \cdot \nabla f = 0$$ Dividing through by $\lambda$ and subtracting over, one could also write this as $$\Delta f = \nabla (-log(\lambda)) \cdot \nabla f$$ So the question becomes, for a function $f$, does there exist some function $h$ so that  $$\Delta f = \nabla(f) \cdot \nabla(h)?$$ Clearly, harmonic functions satisfy this with $\lambda = 1$ and $h = 0$.  Are there functions that satisfy this that aren't harmonic? Is this a well studied class of functions?  Again, forgive me as this might be a stupid question.",,"['multivariable-calculus', 'partial-differential-equations']"
51,A question about multiple integral,A question about multiple integral,,"How to compute the multiple integral $$\int \int...\int_{(D)}  dx_1dx_2...dx_n, \ \ D:-1\le x_1,x_2,\ldots ,x_n\le1, -1\le x_1+x_2+\cdots +x_n\le1.$$ Thanks in advanced for your help!","How to compute the multiple integral $$\int \int...\int_{(D)}  dx_1dx_2...dx_n, \ \ D:-1\le x_1,x_2,\ldots ,x_n\le1, -1\le x_1+x_2+\cdots +x_n\le1.$$ Thanks in advanced for your help!",,"['integration', 'multivariable-calculus', 'volume']"
52,Continuity of functions of several variables,Continuity of functions of several variables,,"trying to understand this example of continuity of a a function on $R^2$ $$f(x,y) := \begin{cases}     \frac{xy^2}{x^2 + y^2}& \text{if } (x,y) \neq (0,0)\\     0              & \text{if } (x,y) = (0,0) \end{cases} $$ to prove continuity at (0,0) we need to show: $$ \left|{\frac{x_ny_n^2}{x_n^2 + y_n^2} - 0}\right| = \left|{\frac{x_ny_n^2}{x_n^2 + y_n^2}}\right|  \xrightarrow{n \to \infty} 0 ~~~~~~~~~ \text{whenever} (x_n, y_n) \xrightarrow{n \to \infty} (0,0) $$ can someone explain the above criterion, I am struggling to make sense of it.  The definition of continuity states $ \lim_{x \to x_0}  f(x) = f(x_0)  $ In this case $x_0~~ is ~~(0,0),$ we need to show that the limit from every direction is approaching $f(x_0)$. Is that right? Thanks for the answers, I get the idea when x, y are scalar variables, but in the text they are vectors defined on a set $D \subset R^n$ with values in $R^k $  and my text has 3 conditions out of which one needs to be satisfied to prove continuity. using the epsilon-delta definition if every component function $f_i~~of ~~ \pmb{f} = (f_1, ...., f_k) $ is continuous at $x_0$ $\pmb{f}(x_n) \to \pmb{f}(x_0)$ as $ n \to \infty $ for every sequence $x_n$ in D for which $x_n \to {x_0}$ I believe it is the 3rd option they are using in the text for the above question and that is the one I am having most difficulty understanding. Can someone please explain the 3rd criterion in plain english.","trying to understand this example of continuity of a a function on $R^2$ $$f(x,y) := \begin{cases}     \frac{xy^2}{x^2 + y^2}& \text{if } (x,y) \neq (0,0)\\     0              & \text{if } (x,y) = (0,0) \end{cases} $$ to prove continuity at (0,0) we need to show: $$ \left|{\frac{x_ny_n^2}{x_n^2 + y_n^2} - 0}\right| = \left|{\frac{x_ny_n^2}{x_n^2 + y_n^2}}\right|  \xrightarrow{n \to \infty} 0 ~~~~~~~~~ \text{whenever} (x_n, y_n) \xrightarrow{n \to \infty} (0,0) $$ can someone explain the above criterion, I am struggling to make sense of it.  The definition of continuity states $ \lim_{x \to x_0}  f(x) = f(x_0)  $ In this case $x_0~~ is ~~(0,0),$ we need to show that the limit from every direction is approaching $f(x_0)$. Is that right? Thanks for the answers, I get the idea when x, y are scalar variables, but in the text they are vectors defined on a set $D \subset R^n$ with values in $R^k $  and my text has 3 conditions out of which one needs to be satisfied to prove continuity. using the epsilon-delta definition if every component function $f_i~~of ~~ \pmb{f} = (f_1, ...., f_k) $ is continuous at $x_0$ $\pmb{f}(x_n) \to \pmb{f}(x_0)$ as $ n \to \infty $ for every sequence $x_n$ in D for which $x_n \to {x_0}$ I believe it is the 3rd option they are using in the text for the above question and that is the one I am having most difficulty understanding. Can someone please explain the 3rd criterion in plain english.",,"['sequences-and-series', 'limits', 'multivariable-calculus', 'continuity']"
53,Application of the chain rule for curves,Application of the chain rule for curves,,"Problem : Let $f: \mathbb{R}^3 \to \mathbb{R}$ be a differentiable function such that $$y \frac{\partial f}{\partial x}(x,y,z) -x \frac{\partial f}{\partial y}(x,y,z) + \frac{\partial f}{\partial z}(x,y,z) \geq a>0, \forall(x,y,z) \in \mathbb{R}^3 \tag{*} $$   Let $\gamma: \mathbb{R}_+ \to \mathbb{R}^3$ a differentiable curve given by $\gamma(t)=(- \cos t, \sin t ,t), \ t \geq 0$ Show for $g(t):=f(\gamma(t))$ that $\lim_{t \to + \infty} g(t) = + \infty$ My approach : My idea was to make use of the fact that $g$ is differentiable and the derivative is given by $$g'(t)=< \nabla f(\gamma(t)), \dot\gamma(t)> $$ because the result will very much look like (*), if I can manage to show that $g'(t)\geq a >0$ it would follow that for all $t\geq 0$ I have that $g$ is strictly monotone increasing, if I then could prove that there exists no upper bound, this would finish the exercise. I have $\dot \gamma(t)=(\sin t, \cos t ,1)$, my next step was to compute $g'(t)$, I obtained that $$g'(t)= \sin t \frac{\partial f( \gamma(t))}{\partial x}+ \cos t \frac{\partial f ( \gamma(t))}{\partial y} + \frac{\partial f(\gamma(t))}{\partial z} $$ Which looks a lot like (*), however not quite, because there is this annoying minus sign missing, which might be due to a typing error in C. Michels Analysis II Exercises. Also I fail to make sense of the partial derivatives above, I left the $x,y,z$ in place in order to establish the connection with (*), however if I naively substitute $x= - \cos t, y = \sin t, z = t$ I get very weird partial derivatives that make no sense at all to me. Is my approach to this exercise (in general) right? Or did I misguide myself into trap because (*) looks so much like the chain rule for curves.","Problem : Let $f: \mathbb{R}^3 \to \mathbb{R}$ be a differentiable function such that $$y \frac{\partial f}{\partial x}(x,y,z) -x \frac{\partial f}{\partial y}(x,y,z) + \frac{\partial f}{\partial z}(x,y,z) \geq a>0, \forall(x,y,z) \in \mathbb{R}^3 \tag{*} $$   Let $\gamma: \mathbb{R}_+ \to \mathbb{R}^3$ a differentiable curve given by $\gamma(t)=(- \cos t, \sin t ,t), \ t \geq 0$ Show for $g(t):=f(\gamma(t))$ that $\lim_{t \to + \infty} g(t) = + \infty$ My approach : My idea was to make use of the fact that $g$ is differentiable and the derivative is given by $$g'(t)=< \nabla f(\gamma(t)), \dot\gamma(t)> $$ because the result will very much look like (*), if I can manage to show that $g'(t)\geq a >0$ it would follow that for all $t\geq 0$ I have that $g$ is strictly monotone increasing, if I then could prove that there exists no upper bound, this would finish the exercise. I have $\dot \gamma(t)=(\sin t, \cos t ,1)$, my next step was to compute $g'(t)$, I obtained that $$g'(t)= \sin t \frac{\partial f( \gamma(t))}{\partial x}+ \cos t \frac{\partial f ( \gamma(t))}{\partial y} + \frac{\partial f(\gamma(t))}{\partial z} $$ Which looks a lot like (*), however not quite, because there is this annoying minus sign missing, which might be due to a typing error in C. Michels Analysis II Exercises. Also I fail to make sense of the partial derivatives above, I left the $x,y,z$ in place in order to establish the connection with (*), however if I naively substitute $x= - \cos t, y = \sin t, z = t$ I get very weird partial derivatives that make no sense at all to me. Is my approach to this exercise (in general) right? Or did I misguide myself into trap because (*) looks so much like the chain rule for curves.",,"['calculus', 'multivariable-calculus', 'self-learning']"
54,"evaluate the double integral $\int_0^4 \int_{\sqrt{y}}^2 \sqrt{x^2+y}\, dxdy$",evaluate the double integral,"\int_0^4 \int_{\sqrt{y}}^2 \sqrt{x^2+y}\, dxdy","evaluate the double integral $\int_0^4 \int_{\sqrt{y}}^2 \sqrt{x^2+y}\, dxdy$ Hi all, could someone give me a hint on this question? I've actually tried converting to polar coordinates but i cant seem to get the limits. But if polar coordinates are the way to go i'll just keep working on it. Thanks in advance. edit: so the trick is to change the order of integration. $\int_0^4 \int_{\sqrt{y}}^2 \sqrt{x^2+y}\, dxdy$ =$\int_0^2 \int_0^{x^2} \sqrt{x^2+y}\, dydx$ =$ \frac{2}{3}\int_0^2 (2x^2)^{3/2}-x^3\,dx$ =$ \frac{2}{3}(2\sqrt2-1) \int_0^2 x^3\,dx$ =$\frac{8}{3}(2\sqrt2-1)$","evaluate the double integral $\int_0^4 \int_{\sqrt{y}}^2 \sqrt{x^2+y}\, dxdy$ Hi all, could someone give me a hint on this question? I've actually tried converting to polar coordinates but i cant seem to get the limits. But if polar coordinates are the way to go i'll just keep working on it. Thanks in advance. edit: so the trick is to change the order of integration. $\int_0^4 \int_{\sqrt{y}}^2 \sqrt{x^2+y}\, dxdy$ =$\int_0^2 \int_0^{x^2} \sqrt{x^2+y}\, dydx$ =$ \frac{2}{3}\int_0^2 (2x^2)^{3/2}-x^3\,dx$ =$ \frac{2}{3}(2\sqrt2-1) \int_0^2 x^3\,dx$ =$\frac{8}{3}(2\sqrt2-1)$",,['multivariable-calculus']
55,"Find the volume V of the solid bounded by the cylinder $x^2 +y^2 = 1$, the xy-plane and the plane $x + z = 1 $.","Find the volume V of the solid bounded by the cylinder , the xy-plane and the plane .",x^2 +y^2 = 1 x + z = 1 ,"Find the volume V of the solid bounded by the cylinder $x^2 +y^2 = 1$, the xy-plane and the plane $x + z = 1 $. Hi all, i cant seem to get the correct answer for this question. The answer is $\pi$ but i got $2\pi$ . Was hoping that someone could check to see what i'm doing wrong. Thanks in advance. I tried doing this with polar coordinates. so $x^2+y^2=r^2$ , $x=rcos\theta$ , $y=rsin\theta$ $0\le \theta \le 2\pi$ , $0 \le r \le 1 $ and $ 0 \le z \le 1 - r cos\theta$ Did the integration like this $\int_0^{2\pi}\int_0^1\int_0^{1-rcos\theta} dz dr d\theta$ = $\int_0^{2\pi}\int_0^1 (1-rcos\theta) dr d\theta$ = $\int_0^{2\pi} (1- \frac{cos\theta}{2}) d\theta$ =$2\pi$ edit: mistake was that r is missing eg.$\int_0^{2\pi}\int_0^1\int_0^{1-rcos\theta} r dz dr d\theta$","Find the volume V of the solid bounded by the cylinder $x^2 +y^2 = 1$, the xy-plane and the plane $x + z = 1 $. Hi all, i cant seem to get the correct answer for this question. The answer is $\pi$ but i got $2\pi$ . Was hoping that someone could check to see what i'm doing wrong. Thanks in advance. I tried doing this with polar coordinates. so $x^2+y^2=r^2$ , $x=rcos\theta$ , $y=rsin\theta$ $0\le \theta \le 2\pi$ , $0 \le r \le 1 $ and $ 0 \le z \le 1 - r cos\theta$ Did the integration like this $\int_0^{2\pi}\int_0^1\int_0^{1-rcos\theta} dz dr d\theta$ = $\int_0^{2\pi}\int_0^1 (1-rcos\theta) dr d\theta$ = $\int_0^{2\pi} (1- \frac{cos\theta}{2}) d\theta$ =$2\pi$ edit: mistake was that r is missing eg.$\int_0^{2\pi}\int_0^1\int_0^{1-rcos\theta} r dz dr d\theta$",,"['integration', 'multivariable-calculus', 'volume']"
56,How can I show that double integral exists?,How can I show that double integral exists?,,"For $Q= [0,1] \times [0,1]$ $$f(x,y) = \begin{cases} 1, \text{ if $x = y$} \\ 0 ,\text{ if $x \neq y$} \end{cases}$$ prove that double integral exists and equals to $ 0$ So I tried to prove this by showing that function is bounded $0<f<1$ so there is sup and inf. If I can show sup and inf are the same, then I can say double integral exists. But how can I show that they are the same?","For $Q= [0,1] \times [0,1]$ $$f(x,y) = \begin{cases} 1, \text{ if $x = y$} \\ 0 ,\text{ if $x \neq y$} \end{cases}$$ prove that double integral exists and equals to $ 0$ So I tried to prove this by showing that function is bounded $0<f<1$ so there is sup and inf. If I can show sup and inf are the same, then I can say double integral exists. But how can I show that they are the same?",,"['real-analysis', 'integration', 'multivariable-calculus']"
57,Limit of a function with two variables,Limit of a function with two variables,,"Could anyone help me with step-by-step solution of this limit? $$\lim_{x \to 2, y \to 3} \frac{3x-2y}{y-x-1} $$ I used a calculator that gave me $-3$ as answer. Thanks a lot.","Could anyone help me with step-by-step solution of this limit? $$\lim_{x \to 2, y \to 3} \frac{3x-2y}{y-x-1} $$ I used a calculator that gave me $-3$ as answer. Thanks a lot.",,"['limits', 'multivariable-calculus']"
58,"Question on extremums, can anyone provide some help?","Question on extremums, can anyone provide some help?",,"The real question: Let $f \colon \mathbb{R}^3 \to \mathbb{R}$;  $f(x,y_1,y_2)=x^3−xy_1y_2+y_2^2−16$. Show that there exists a differentiable real function $g$ so that in some neighboorhood $(1,4)$ $$f(g(y_1,y_2),y_1,y_2)=0 $$Find $g′(1,4)$ and ${\partial^2g\over \partial x^2}(1,4).$ My theorem of implicit function is:$$$$ $$\large {Implicit\ Theorem:}$$ $$\begin {cases} a.)X,Y,Z ;Banach- spaces, W\subset X \times Y. \\                  b.)F:W \to Z; F\in C^1; F(a,b)=0.\\                  c.)D_yF(a,b):Y\to Z -isomorphism-Y-and-Z \end {cases}$$ $Conclusions:$ $$There\ exists\ an \ open\ neighboorhood\ U(a)\in X,\ an \ open \ neighboor\  W'=W'(a,b)\subset W\subset X \times Y and\ a\ function\ f:U\to Y, that: $$ $$\begin {cases} 1.)(x,y)\in W';F(x,y)=0 \iff x\in U ;y=f(x);f(a)=b.\\                  2.)f\in C^1;f'(a)=-D_yF^{-1}\circ D_xF. \\                  3.)If\ F \in C^k \implies f\in C^k,(k\geq1).  \end {cases}$$","The real question: Let $f \colon \mathbb{R}^3 \to \mathbb{R}$;  $f(x,y_1,y_2)=x^3−xy_1y_2+y_2^2−16$. Show that there exists a differentiable real function $g$ so that in some neighboorhood $(1,4)$ $$f(g(y_1,y_2),y_1,y_2)=0 $$Find $g′(1,4)$ and ${\partial^2g\over \partial x^2}(1,4).$ My theorem of implicit function is:$$$$ $$\large {Implicit\ Theorem:}$$ $$\begin {cases} a.)X,Y,Z ;Banach- spaces, W\subset X \times Y. \\                  b.)F:W \to Z; F\in C^1; F(a,b)=0.\\                  c.)D_yF(a,b):Y\to Z -isomorphism-Y-and-Z \end {cases}$$ $Conclusions:$ $$There\ exists\ an \ open\ neighboorhood\ U(a)\in X,\ an \ open \ neighboor\  W'=W'(a,b)\subset W\subset X \times Y and\ a\ function\ f:U\to Y, that: $$ $$\begin {cases} 1.)(x,y)\in W';F(x,y)=0 \iff x\in U ;y=f(x);f(a)=b.\\                  2.)f\in C^1;f'(a)=-D_yF^{-1}\circ D_xF. \\                  3.)If\ F \in C^k \implies f\in C^k,(k\geq1).  \end {cases}$$",,"['multivariable-calculus', 'implicit-differentiation', 'implicit-function-theorem']"
59,Changing order of integration in a double integral,Changing order of integration in a double integral,,"The question was to sketch the region of integration and change the order of integration. $$\int^{3}_{0} \int^{\sqrt{9-y}}_{0} f(x,y) dxdy$$ When I sketch the region of integration I do not see a way that it is possible to change the order of integration.  My region is bounded by, $y=3$, $y=0$, $x=0$ and $x=\sqrt{9-y}$. Any insight would help, if there is a way to do this then I guess I have it sketched wrong. My image is below of what I drew","The question was to sketch the region of integration and change the order of integration. $$\int^{3}_{0} \int^{\sqrt{9-y}}_{0} f(x,y) dxdy$$ When I sketch the region of integration I do not see a way that it is possible to change the order of integration.  My region is bounded by, $y=3$, $y=0$, $x=0$ and $x=\sqrt{9-y}$. Any insight would help, if there is a way to do this then I guess I have it sketched wrong. My image is below of what I drew",,"['integration', 'multivariable-calculus']"
60,What is the gradient of a gradient?,What is the gradient of a gradient?,,"I'm a student, trying to re-derive a result found in a paper by calculating the following in spherical coordinates: $$\mathbf{I}+ \frac{\nabla\nabla}{\mathrm{constant}},$$ where $\mathbf{I}$ is a $3\times 3$ identity matrix. The paper that I've seen writes the result as \begin{bmatrix} 0 & 0 & 0\\  \cos(\Theta)\cos(\Phi) & \cos(\Theta)\sin(\Phi) & -\sin(\Theta)\\  -\sin(\Phi) & \cos(\Theta) & 0 \end{bmatrix} How do they get that? What is the gradient of a gradient in spherical coordinates? Is it a Hessian? Please see equations 11 and 13 of the following paper: http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.369.921&rep=rep1&type=pdf","I'm a student, trying to re-derive a result found in a paper by calculating the following in spherical coordinates: where is a identity matrix. The paper that I've seen writes the result as How do they get that? What is the gradient of a gradient in spherical coordinates? Is it a Hessian? Please see equations 11 and 13 of the following paper: http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.369.921&rep=rep1&type=pdf","\mathbf{I}+ \frac{\nabla\nabla}{\mathrm{constant}}, \mathbf{I} 3\times 3 \begin{bmatrix}
0 & 0 & 0\\ 
\cos(\Theta)\cos(\Phi) & \cos(\Theta)\sin(\Phi) & -\sin(\Theta)\\ 
-\sin(\Phi) & \cos(\Theta) & 0
\end{bmatrix}",['multivariable-calculus']
61,How to prove the existence of a minimum of a quadratic function of two variables?,How to prove the existence of a minimum of a quadratic function of two variables?,,"I am given function $$ f(x,y)=Ax^2+2Bxy+Cy^2+2Dx+2Ey+F,\quad\text{where }A>0\text{ and }B^2<AC . $$ Prove that a point $(a,b)$ exists which $f$ has a minimum. I figured out that there is no stationary point for this equation. So, Hessian Matrix seems not helpful. In my book, it says that ""change quadratic part to sum of squares but, Can't think of any way to change it to sum of squares. Also, Why $f(a,b)=Da+Eb+F$ is at this minimum..?","I am given function $$ f(x,y)=Ax^2+2Bxy+Cy^2+2Dx+2Ey+F,\quad\text{where }A>0\text{ and }B^2<AC . $$ Prove that a point $(a,b)$ exists which $f$ has a minimum. I figured out that there is no stationary point for this equation. So, Hessian Matrix seems not helpful. In my book, it says that ""change quadratic part to sum of squares but, Can't think of any way to change it to sum of squares. Also, Why $f(a,b)=Da+Eb+F$ is at this minimum..?",,"['real-analysis', 'multivariable-calculus', 'optimization', 'partial-derivative', 'quadratics']"
62,Piecewise Iterated Integral,Piecewise Iterated Integral,,"I have tried to find these integrals, I'm not sure how to deal with the if conditions, or gain some sort of geometric interpretation of the if conditions. I suspect that f does not contradict Tonelli's Theorem because f is not integrable on the unit square. Any hints as to approach this problem would be greatly appreciated. Thank you.","I have tried to find these integrals, I'm not sure how to deal with the if conditions, or gain some sort of geometric interpretation of the if conditions. I suspect that f does not contradict Tonelli's Theorem because f is not integrable on the unit square. Any hints as to approach this problem would be greatly appreciated. Thank you.",,"['real-analysis', 'integration', 'multivariable-calculus']"
63,Second Partial Derivative Test for a Three-Variable Function,Second Partial Derivative Test for a Three-Variable Function,,"Here is the function in question: $$f(x,y,z) = x^2 + x^2y + y^2z + z^2 - 4z$$ I need to find all critical points and use the second derivative test to determine if each one is a local minimum, maximum, or saddle point (or state if the test cannot determine the answer). So, my plan is to find all of the partial derivates, find the critical points, then construct the Hessian of f at those critical points. However, I'm stuck on the second step. I found the following: $$\dfrac{\partial f}{\partial x} = 2x + 2xy$$ $$\dfrac{\partial f}{\partial y} = x^2 +2yz$$ $$\dfrac{\partial f}{\partial z} = y^2 + 2z - 4$$ Now, I assume I'm supposed to set all of those equal to 0, and find the solutions for x, y, z for which that is true. But, since there are three non-linear equations with cross terms, I don't really know the best way to do that. In fact, I'm starting to question whether or not that's the right approach at all. Obviously, I can see that (0,2,0) is a critical point, just from inspection, but I assume there's a more comprehensive way to go about this. Any help would be appreciated. Thanks!","Here is the function in question: $$f(x,y,z) = x^2 + x^2y + y^2z + z^2 - 4z$$ I need to find all critical points and use the second derivative test to determine if each one is a local minimum, maximum, or saddle point (or state if the test cannot determine the answer). So, my plan is to find all of the partial derivates, find the critical points, then construct the Hessian of f at those critical points. However, I'm stuck on the second step. I found the following: $$\dfrac{\partial f}{\partial x} = 2x + 2xy$$ $$\dfrac{\partial f}{\partial y} = x^2 +2yz$$ $$\dfrac{\partial f}{\partial z} = y^2 + 2z - 4$$ Now, I assume I'm supposed to set all of those equal to 0, and find the solutions for x, y, z for which that is true. But, since there are three non-linear equations with cross terms, I don't really know the best way to do that. In fact, I'm starting to question whether or not that's the right approach at all. Obviously, I can see that (0,2,0) is a critical point, just from inspection, but I assume there's a more comprehensive way to go about this. Any help would be appreciated. Thanks!",,"['matrices', 'multivariable-calculus', 'optimization']"
64,Can a curve's unit normal vector be determined using the second derivative?,Can a curve's unit normal vector be determined using the second derivative?,,Because $$T = \frac {r'}{|r'|}$$ I was wondering whether or not it was also valid to solve for the unit normal vector with the second derivative without first solving for T: $$N = \frac {r''}{|r''|}$$,Because $$T = \frac {r'}{|r'|}$$ I was wondering whether or not it was also valid to solve for the unit normal vector with the second derivative without first solving for T: $$N = \frac {r''}{|r''|}$$,,"['calculus', 'multivariable-calculus']"
65,Applying geometric calculus to a matrix expression,Applying geometric calculus to a matrix expression,,"Let $f(\mathbf x) = \mathbf x^T A \mathbf x + \mathbf b^T\mathbf x + c$, where $A$ is a square matrix, $\mathbf x, \mathbf b$ are column matrices and $c$ is a constant.  Then the gradient of this expression is $\nabla f(\mathbf x) = (A+A^T)\mathbf x + \mathbf b$.  I see how to derive this by interpretting the gradient as the matrix operator $D$.  I'm wondering if we could get the same answer if we used the geometric gradient defined by $\nabla = \sum_i \mathbf e^i \partial_i$.  So I'd like to know how to use geometric calculus on this matrix expression.  Is there a way to do this?  Thanks.","Let $f(\mathbf x) = \mathbf x^T A \mathbf x + \mathbf b^T\mathbf x + c$, where $A$ is a square matrix, $\mathbf x, \mathbf b$ are column matrices and $c$ is a constant.  Then the gradient of this expression is $\nabla f(\mathbf x) = (A+A^T)\mathbf x + \mathbf b$.  I see how to derive this by interpretting the gradient as the matrix operator $D$.  I'm wondering if we could get the same answer if we used the geometric gradient defined by $\nabla = \sum_i \mathbf e^i \partial_i$.  So I'd like to know how to use geometric calculus on this matrix expression.  Is there a way to do this?  Thanks.",,"['multivariable-calculus', 'geometric-algebras']"
66,Find $\int_C ydx+zdy+xdz~$ where $C$ is the curve of the intersection of the two surfaces $z=xy$ and $x^2+y^2=1$,Find  where  is the curve of the intersection of the two surfaces  and,\int_C ydx+zdy+xdz~ C z=xy x^2+y^2=1,"Find $\int_c ydx+zdy+xdz~$ where $C$ is the curve of the intersection of the two surfaces $z=xy$ and $x^2+y^2=1$ , traversed once in a direction that appears counterclockwise when viewed from high above the $xy$ plane. ( Without using Stokes Theorem) Solution Attempt: I seem to have no idea how to represent the curve of the intersection of the two surfaces $z=xy$ and $x^2+y^2=1$ in mathematical terms. However, here's what I tried with the given equations : Differentiating them : $dz = x dy+y dx$ and $xdx+ydy=0$ . I thought of substituting the above results back into the integrand, but this doesn't seem a very good idea. Could anyone tell me how do I proceed from here? Thank you for reading through!","Find $\int_c ydx+zdy+xdz~$ where $C$ is the curve of the intersection of the two surfaces $z=xy$ and $x^2+y^2=1$ , traversed once in a direction that appears counterclockwise when viewed from high above the $xy$ plane. ( Without using Stokes Theorem) Solution Attempt: I seem to have no idea how to represent the curve of the intersection of the two surfaces $z=xy$ and $x^2+y^2=1$ in mathematical terms. However, here's what I tried with the given equations : Differentiating them : $dz = x dy+y dx$ and $xdx+ydy=0$ . I thought of substituting the above results back into the integrand, but this doesn't seem a very good idea. Could anyone tell me how do I proceed from here? Thank you for reading through!",,['multivariable-calculus']
67,Evaluate 2D integral (by change of variable),Evaluate 2D integral (by change of variable),,"The question asks to evaluate integral $$\iint_D \Big[3-\frac12( \frac{x^2}{a^2}+\frac{y^2}{b^2})\Big]  \, dx \, dy \ $$ where D is the region $$\frac{x^2}{a^2}+\frac{y^2}{b^2} \le 4 $$ I believe this requires me to convert it to polar coordinates, using the Jacobian etc. but it gets messy really quickly and does not lead me to anything. Any clues/ideas? PS. The final integral I've got is $$ \int_0^\pi \frac{a^2b^2}{(bcos(\theta))^2+(asin(\theta))^2} \, d \theta \ $$ which might even be correct, or might have some algebra messed up. but How to approach this?","The question asks to evaluate integral $$\iint_D \Big[3-\frac12( \frac{x^2}{a^2}+\frac{y^2}{b^2})\Big]  \, dx \, dy \ $$ where D is the region $$\frac{x^2}{a^2}+\frac{y^2}{b^2} \le 4 $$ I believe this requires me to convert it to polar coordinates, using the Jacobian etc. but it gets messy really quickly and does not lead me to anything. Any clues/ideas? PS. The final integral I've got is $$ \int_0^\pi \frac{a^2b^2}{(bcos(\theta))^2+(asin(\theta))^2} \, d \theta \ $$ which might even be correct, or might have some algebra messed up. but How to approach this?",,"['integration', 'multivariable-calculus', 'polar-coordinates']"
68,"Find $F'(t)$, where F is an integral","Find , where F is an integral",F'(t),"I need to find $F'(t)$, where $F(t)=\int_{[0,t]^2}e^{\frac{tx}{y^2}}dxdy$. My first approach: Let's observe that $\int e^{\frac{tx}{y^2}}dx=\frac{y^2}{t}e^{\frac{tx}{y^2}}+C$. So I get: $$F(t)=\int_{[0,t]^2}e^{\frac{tx}{y^2}}dxdy=\int_{0}^{t}\frac{y^2}{t}e^{\frac{t^2}{y^2}}-\frac{y^2}{t}dy$$ But now the integral isn't so easy. Has anybody got any ideas? Is there a better way to find the derviative of $F$ or do I need to calculate it like that?","I need to find $F'(t)$, where $F(t)=\int_{[0,t]^2}e^{\frac{tx}{y^2}}dxdy$. My first approach: Let's observe that $\int e^{\frac{tx}{y^2}}dx=\frac{y^2}{t}e^{\frac{tx}{y^2}}+C$. So I get: $$F(t)=\int_{[0,t]^2}e^{\frac{tx}{y^2}}dxdy=\int_{0}^{t}\frac{y^2}{t}e^{\frac{t^2}{y^2}}-\frac{y^2}{t}dy$$ But now the integral isn't so easy. Has anybody got any ideas? Is there a better way to find the derviative of $F$ or do I need to calculate it like that?",,"['calculus', 'analysis', 'multivariable-calculus']"
69,"How to tell if $f(x,y)$, defined as $f(x,y) = x^2 + y^2$ when $x,y\neq0$ and $f(x,y) = 2$ when $x,y = (0,0)$, is continuous at $(0,0)$?","How to tell if , defined as  when  and  when , is continuous at ?","f(x,y) f(x,y) = x^2 + y^2 x,y\neq0 f(x,y) = 2 x,y = (0,0) (0,0)","I have to decide if the following function is continuous at $(0,0)$ . $$f(x,y)= \begin{cases}        x^2+y^2 & x,y\neq 0 \\       2 & x,y= (0,0)    \end{cases}$$ so for the first one, I assume it is continuous, because there isn't a fraction, so there is no way for there to be an error when computing this (like dividing by a 0) but for the second one, I am not sure where to go when all it gives me is the number 2 with the condition ""if (x,y) = (0,0)","I have to decide if the following function is continuous at . so for the first one, I assume it is continuous, because there isn't a fraction, so there is no way for there to be an error when computing this (like dividing by a 0) but for the second one, I am not sure where to go when all it gives me is the number 2 with the condition ""if (x,y) = (0,0)","(0,0) f(x,y)= \begin{cases} 
      x^2+y^2 & x,y\neq 0 \\
      2 & x,y= (0,0)
   \end{cases}","['limits', 'multivariable-calculus', 'functions', 'continuity']"
70,Extending Taylor's theorem from one to several variables,Extending Taylor's theorem from one to several variables,,"In my calculus class we are dealing with Taylor´s theorem in several variables. When we were looking at the function $f(x,y)=\sin(xy)$ my teacher said that instead of applying the theorem in several variables we can just apply Taylor´s theorem in one variable and then replace the variable by $xy$: $$\sin(t)=t-{t^3\over 3!}+{t^5\over 5!}+ R_5(t)$$ which yields: $$\sin(xy)=xy-{(xy)^3\over 3!}+{(xy)^5\over 5!}+ R_5(xy)$$ My question is: when can we do this kind of replacement? What conditions does the function must satisfy? Or we just do it whenever we can? I would really appreciate your help :)","In my calculus class we are dealing with Taylor´s theorem in several variables. When we were looking at the function $f(x,y)=\sin(xy)$ my teacher said that instead of applying the theorem in several variables we can just apply Taylor´s theorem in one variable and then replace the variable by $xy$: $$\sin(t)=t-{t^3\over 3!}+{t^5\over 5!}+ R_5(t)$$ which yields: $$\sin(xy)=xy-{(xy)^3\over 3!}+{(xy)^5\over 5!}+ R_5(xy)$$ My question is: when can we do this kind of replacement? What conditions does the function must satisfy? Or we just do it whenever we can? I would really appreciate your help :)",,"['multivariable-calculus', 'taylor-expansion']"
71,Points at which partial derivatives exist and are continuous,Points at which partial derivatives exist and are continuous,,"Given $ E = (xy : xy \neq 0 ) $ . Let $ f : \mathbb R^{2}\longrightarrow \mathbb R$ be defined by  $$f(x,y)=\cases{0,& if $xy=0$\\y\sin\left(\frac{1}{x}\right)+x\sin\left(\frac{1}{y}\right),& otherwise} .$$ Let $ S_1  $ be set of points in $ \mathbb R^{2}$  , where $f_x$ exists and $S_2 $ be set of points where $ f_y$ exists . Let $E_1$ be points where partial wrt x is continous and $E_2$ be set of points where $f_y$ is continous I heed to determine what $S_i$ and $E_i$ are, $i=1,2 $ ATTEMPT : I calculated partial derivatives as $f_x$ = $ -(y/x^{2}) \cos(1/x) + x\sin(1/y)$ $f_y = \sin1/x - (x/y^{2} )\cdot\cos (1/y) $ These both partials do not exist at X axis and Y axis and origin. At other points in xy plane they exist. Same for continuity. Is that enough, I'm not confident? Can anyone please elaborate? Thanks","Given $ E = (xy : xy \neq 0 ) $ . Let $ f : \mathbb R^{2}\longrightarrow \mathbb R$ be defined by  $$f(x,y)=\cases{0,& if $xy=0$\\y\sin\left(\frac{1}{x}\right)+x\sin\left(\frac{1}{y}\right),& otherwise} .$$ Let $ S_1  $ be set of points in $ \mathbb R^{2}$  , where $f_x$ exists and $S_2 $ be set of points where $ f_y$ exists . Let $E_1$ be points where partial wrt x is continous and $E_2$ be set of points where $f_y$ is continous I heed to determine what $S_i$ and $E_i$ are, $i=1,2 $ ATTEMPT : I calculated partial derivatives as $f_x$ = $ -(y/x^{2}) \cos(1/x) + x\sin(1/y)$ $f_y = \sin1/x - (x/y^{2} )\cdot\cos (1/y) $ These both partials do not exist at X axis and Y axis and origin. At other points in xy plane they exist. Same for continuity. Is that enough, I'm not confident? Can anyone please elaborate? Thanks",,"['multivariable-calculus', 'partial-derivative']"
72,Inverse gradient as line integral in Mathematica,Inverse gradient as line integral in Mathematica,,"I found a nice paper about inverse vector operators here . I have successfully defined a Mathematica function for inverse curl and inverse divergence, however I can't figure out how to do inverse gradient (page 7 in the paper). According the paper, the inverse gradient can be computed as a path independent line integral, but in my attempts I got improper results. The paper doesn't show up an example for it and I'm not really familiar with line integrals. If anyone could help me I would be really thankful. In the answer please be aware the fact that I'm not a math guy.","I found a nice paper about inverse vector operators here . I have successfully defined a Mathematica function for inverse curl and inverse divergence, however I can't figure out how to do inverse gradient (page 7 in the paper). According the paper, the inverse gradient can be computed as a path independent line integral, but in my attempts I got improper results. The paper doesn't show up an example for it and I'm not really familiar with line integrals. If anyone could help me I would be really thankful. In the answer please be aware the fact that I'm not a math guy.",,"['integration', 'multivariable-calculus', 'vector-spaces', 'vector-analysis', 'mathematica']"
73,Parametrization of the intersection of a cone and plane.,Parametrization of the intersection of a cone and plane.,,"EDITED with new progress updates. As the title states, I'm trying to parametrize the intersection of a cone and a plane. The equations are: $z^2 = 2x^2+2y^2$ and $2x+y+3z=4\implies z=\frac{1}{3}(4-2x-y)$ If it were either a plane parallel to the x-y plane or a cylinder instead of a cone, I would simply project the intersection onto the x-y plane, parametrize the projection for x and y then insert those equations into the equation of the plane to get the parametrization of z. I'm thinking a similar approach would work here but I can't figure out how to find the equation of the projection. I now have what I believe is the projected ellipse onto the x-y plane. The ellipse equation is: $0 = \frac{1}{9}(4-2x-y)^2-2x^2-2y^2$ My problem is now that if I expand out the squared portion, I end up with $4xy$ in the equation. I have no idea how to parametrize an ellipse with a cross term in it. Could someone help with that? Any help would be greatly appreciated. Thank you, Eric","EDITED with new progress updates. As the title states, I'm trying to parametrize the intersection of a cone and a plane. The equations are: $z^2 = 2x^2+2y^2$ and $2x+y+3z=4\implies z=\frac{1}{3}(4-2x-y)$ If it were either a plane parallel to the x-y plane or a cylinder instead of a cone, I would simply project the intersection onto the x-y plane, parametrize the projection for x and y then insert those equations into the equation of the plane to get the parametrization of z. I'm thinking a similar approach would work here but I can't figure out how to find the equation of the projection. I now have what I believe is the projected ellipse onto the x-y plane. The ellipse equation is: $0 = \frac{1}{9}(4-2x-y)^2-2x^2-2y^2$ My problem is now that if I expand out the squared portion, I end up with $4xy$ in the equation. I have no idea how to parametrize an ellipse with a cross term in it. Could someone help with that? Any help would be greatly appreciated. Thank you, Eric",,"['multivariable-calculus', 'parametric']"
74,Real/Complex differentials forms,Real/Complex differentials forms,,"Given $f:\Bbb C^{n}\to\Bbb C$ identified with $f:\Bbb R^{2n}\to\Bbb C$, in a book I read that $$ \partial_x f\,dx+\partial_yf\,dy=\partial_zf\,dz+\partial_{\bar z}f\,d\bar z $$ and that this could be rewritten as $$ df=\partial f+\bar{\partial}f $$ and this last equation defines $\partial f$ and $\bar{\partial}f$. My problem is to write explicitly the last two objects. Now $\partial_x:=(\partial_{x_1},\dots,\partial_{x_n})$ and $dx:=(dx_1,\dots,dx_n)$ and so on for the other variables. Hence we can think at $\partial_x f\,dx$ as a scalar product between $\partial_x f$ and the formal vector $dx$. In this way, writing formally $dz=dx+idy$ and $d\bar z=dx-idy$ we deduce easily the first equation. However I would like to write explicitly both its sides. LHS it's obviously $df$ (just a computation). Now it would be nice to write explicitly even RHS, in order to understand how to define properly $\partial f$ and $\bar{\partial}f$. We know that $\partial_z=\frac12(\partial_x-i\partial_y)$ and $\partial_\bar z=\frac12(\partial_x+i\partial_y)$. My problem is to understand how $dz$ works. We can assume wlog $n=1$. As a differential form, $dx$ is a covector: $dx=(1,0)$ and similarly for $dy=(0,1)$. What about $dz$? We know it's equal to $dx+idy=(1,0)+i(0,1)$, but the ""ambient"" in which $dz$ lives is 1-dim covector. I think the point is inside the last observation. The only way to obtain the first equality directly is to write $dz=(1,-1)$ and $d\bar z=(1,1)$, i.e. $i(0,1)=(0,-1)$. How can this be possibile? I need someone shading lights on this! Thank you all!","Given $f:\Bbb C^{n}\to\Bbb C$ identified with $f:\Bbb R^{2n}\to\Bbb C$, in a book I read that $$ \partial_x f\,dx+\partial_yf\,dy=\partial_zf\,dz+\partial_{\bar z}f\,d\bar z $$ and that this could be rewritten as $$ df=\partial f+\bar{\partial}f $$ and this last equation defines $\partial f$ and $\bar{\partial}f$. My problem is to write explicitly the last two objects. Now $\partial_x:=(\partial_{x_1},\dots,\partial_{x_n})$ and $dx:=(dx_1,\dots,dx_n)$ and so on for the other variables. Hence we can think at $\partial_x f\,dx$ as a scalar product between $\partial_x f$ and the formal vector $dx$. In this way, writing formally $dz=dx+idy$ and $d\bar z=dx-idy$ we deduce easily the first equation. However I would like to write explicitly both its sides. LHS it's obviously $df$ (just a computation). Now it would be nice to write explicitly even RHS, in order to understand how to define properly $\partial f$ and $\bar{\partial}f$. We know that $\partial_z=\frac12(\partial_x-i\partial_y)$ and $\partial_\bar z=\frac12(\partial_x+i\partial_y)$. My problem is to understand how $dz$ works. We can assume wlog $n=1$. As a differential form, $dx$ is a covector: $dx=(1,0)$ and similarly for $dy=(0,1)$. What about $dz$? We know it's equal to $dx+idy=(1,0)+i(0,1)$, but the ""ambient"" in which $dz$ lives is 1-dim covector. I think the point is inside the last observation. The only way to obtain the first equality directly is to write $dz=(1,-1)$ and $d\bar z=(1,1)$, i.e. $i(0,1)=(0,-1)$. How can this be possibile? I need someone shading lights on this! Thank you all!",,"['complex-analysis', 'analysis', 'multivariable-calculus', 'differential-forms']"
75,"Calculating the limit $\lim \limits_{(x,y) \rightarrow (0,0)} \frac{f(x,y)}{g(x,y)}$",Calculating the limit,"\lim \limits_{(x,y) \rightarrow (0,0)} \frac{f(x,y)}{g(x,y)}","I want to prove that $\displaystyle\lim \limits_{(x,y) \rightarrow (0,0)} \frac{f(x,y)}{g(x,y)}$ is $0$ if the degree of $f(x,y)$ is greater than the degree of $g(x,y)$. Here $f(x,y)$ and $g(x,y)$ are homogeneous polynomials in $x$ and $y$ with no constant term. I have no idea as to how to go about it. If anyone can give a hint it would be great. I was able to prove it in special cases like $\displaystyle \frac{x^2y}{x^2+y^2}$ by using AM-GM inequality.But I am unable to get any general method to solve this problem.","I want to prove that $\displaystyle\lim \limits_{(x,y) \rightarrow (0,0)} \frac{f(x,y)}{g(x,y)}$ is $0$ if the degree of $f(x,y)$ is greater than the degree of $g(x,y)$. Here $f(x,y)$ and $g(x,y)$ are homogeneous polynomials in $x$ and $y$ with no constant term. I have no idea as to how to go about it. If anyone can give a hint it would be great. I was able to prove it in special cases like $\displaystyle \frac{x^2y}{x^2+y^2}$ by using AM-GM inequality.But I am unable to get any general method to solve this problem.",,"['limits', 'multivariable-calculus']"
76,An Interesting Resource Allocation Problem,An Interesting Resource Allocation Problem,,"Here is the problem: \begin{array}{ll} \text{minimize} & \sum_{i=1}^N \frac{1}{1 + \textrm{exp}(C_i + x_i)}\\ \text{subject to} & \sum_{i=1}^N x_i \le R \\ & x_i \ge 0, ~ i = 1,2,...,N \end{array} $C_1,C_2,...,C_N$ are real numbers and can be positives, zeros and negatives.The problem seems to be easy, but after several days I still have no idea on solving it. Thanks for any comments.","Here is the problem: \begin{array}{ll} \text{minimize} & \sum_{i=1}^N \frac{1}{1 + \textrm{exp}(C_i + x_i)}\\ \text{subject to} & \sum_{i=1}^N x_i \le R \\ & x_i \ge 0, ~ i = 1,2,...,N \end{array} $C_1,C_2,...,C_N$ are real numbers and can be positives, zeros and negatives.The problem seems to be easy, but after several days I still have no idea on solving it. Thanks for any comments.",,"['multivariable-calculus', 'optimization', 'nonlinear-optimization']"
77,Textbook suggestion-Vector Analysis,Textbook suggestion-Vector Analysis,,"I took a course in vector analysis this year. It was a two fold course. The first part covered linear algebra and basic euclidean geometry. The second took to more advanced areas such as differential geometry, and the integration theorems. We used vector analysis by Schaum(author: Murray Speigel). I was wondering if there are more books on the subject? Please make sure you state the things covered in the textbooks you mention.","I took a course in vector analysis this year. It was a two fold course. The first part covered linear algebra and basic euclidean geometry. The second took to more advanced areas such as differential geometry, and the integration theorems. We used vector analysis by Schaum(author: Murray Speigel). I was wondering if there are more books on the subject? Please make sure you state the things covered in the textbooks you mention.",,"['multivariable-calculus', 'vector-analysis', 'book-recommendation']"
78,Application of Stoke's Theorem: Wrong parametrization?,Application of Stoke's Theorem: Wrong parametrization?,,"Show that the path $\mathbf{x}(t)=( \cos t, \sin t, \sin 2t )$ lies on the surface $z=2xy$. Evaluate $$\oint_C (y^3+ \cos x)\, dx+(\sin y+z^2)\, dy+x\,dz$$ where $C$ is the closed curve parametrized and oriented by the path   $\mathbf{x}$ in 1. Question $1$ is trivial. It is pure substitution and double-angle formula. I am stuck with question $2$. I think it should be an application of the Stoke's Theorem: $$ \iint_S \nabla \times \mathbf{F}\cdot d\mathbf{S}=\oint_C \mathbf{F}\cdot d\mathbf{s}$$ where $S$ is the surface bounded and $C$ is the corresponding boundary. Now, $$\nabla \times \mathbf{F}=(2z, -1, -3y^2)$$ and then I parametrize the bounded area: $\mathbf{X}(s,t)=(s \cos t, s \sin t, 2s^2 \sin t \cos t)$ where $0 \leq s \leq 1, 0 \leq t \leq 2\pi$. But it leads me to a messy looking vector surface integral. I suppose it is a wrong way to tackle this question. I also try to solve this vector surface integral without parametrizing the surface. I then find $$ \iint_S \frac{-8xy^2 + 2x -3y^2}{\sqrt{4y^2+4x^2+1}}\,dS.$$ Again, messy! Any help will be appreciated!","Show that the path $\mathbf{x}(t)=( \cos t, \sin t, \sin 2t )$ lies on the surface $z=2xy$. Evaluate $$\oint_C (y^3+ \cos x)\, dx+(\sin y+z^2)\, dy+x\,dz$$ where $C$ is the closed curve parametrized and oriented by the path   $\mathbf{x}$ in 1. Question $1$ is trivial. It is pure substitution and double-angle formula. I am stuck with question $2$. I think it should be an application of the Stoke's Theorem: $$ \iint_S \nabla \times \mathbf{F}\cdot d\mathbf{S}=\oint_C \mathbf{F}\cdot d\mathbf{s}$$ where $S$ is the surface bounded and $C$ is the corresponding boundary. Now, $$\nabla \times \mathbf{F}=(2z, -1, -3y^2)$$ and then I parametrize the bounded area: $\mathbf{X}(s,t)=(s \cos t, s \sin t, 2s^2 \sin t \cos t)$ where $0 \leq s \leq 1, 0 \leq t \leq 2\pi$. But it leads me to a messy looking vector surface integral. I suppose it is a wrong way to tackle this question. I also try to solve this vector surface integral without parametrizing the surface. I then find $$ \iint_S \frac{-8xy^2 + 2x -3y^2}{\sqrt{4y^2+4x^2+1}}\,dS.$$ Again, messy! Any help will be appreciated!",,"['integration', 'multivariable-calculus']"
79,Surface Integral without Parametrization,Surface Integral without Parametrization,,"Let $S$ be the sphere $x^2 + y^2 + z^2 = a^2$. Use symmetry considerations to evaluate $\iint_Sx\,dS$ without resorting to parametrizing the sphere. Let F $= (1, 1, 1)$. Use symmetry to determine the vector surface integral $\iint_S$ F $\cdot \, dS$ without parametrizing   the sphere. I find it difficult to understand question 1. What does it mean by 'symmetry considerations'? For question 2, without parametrizing the sphere, I find out that the integral should be equal to $$\frac1a \iint_S(x+y+z)dS.$$ Now symmetry comes in - a sphere is symmetric in all axes, thus the answer should be $\frac3a$ times the answer in part a. Am I correct? Any help will be appreciated.","Let $S$ be the sphere $x^2 + y^2 + z^2 = a^2$. Use symmetry considerations to evaluate $\iint_Sx\,dS$ without resorting to parametrizing the sphere. Let F $= (1, 1, 1)$. Use symmetry to determine the vector surface integral $\iint_S$ F $\cdot \, dS$ without parametrizing   the sphere. I find it difficult to understand question 1. What does it mean by 'symmetry considerations'? For question 2, without parametrizing the sphere, I find out that the integral should be equal to $$\frac1a \iint_S(x+y+z)dS.$$ Now symmetry comes in - a sphere is symmetric in all axes, thus the answer should be $\frac3a$ times the answer in part a. Am I correct? Any help will be appreciated.",,"['integration', 'multivariable-calculus']"
80,Integrating certain functions over the unit sphere $\mathbb{S}^2$,Integrating certain functions over the unit sphere,\mathbb{S}^2,"Let $ \mathbb{S}^2$ the unit sphere, and $  \vec a$, $  \vec b$ two constant vectors. I have to prove that:   $$  \iint\limits_{\mathbb{S}^2}  \langle \vec x , \vec a \rangle \langle \vec x , \vec b \rangle \, d \sigma= \frac43 π \langle \vec a , \vec b \rangle $$ and by using this to prove that: $$  \iint\limits_{\mathbb{S}^2}  \langle A\vec x , \vec x \rangle \, d \sigma = \frac{4}{3}  π  \operatorname{tr}(A) $$ where $A$ is a matrix with order $3 \times 3$. Can anyone give me an idea about the solution?","Let $ \mathbb{S}^2$ the unit sphere, and $  \vec a$, $  \vec b$ two constant vectors. I have to prove that:   $$  \iint\limits_{\mathbb{S}^2}  \langle \vec x , \vec a \rangle \langle \vec x , \vec b \rangle \, d \sigma= \frac43 π \langle \vec a , \vec b \rangle $$ and by using this to prove that: $$  \iint\limits_{\mathbb{S}^2}  \langle A\vec x , \vec x \rangle \, d \sigma = \frac{4}{3}  π  \operatorname{tr}(A) $$ where $A$ is a matrix with order $3 \times 3$. Can anyone give me an idea about the solution?",,"['multivariable-calculus', 'vector-analysis', 'surface-integrals']"
81,how to determine the outward pointing normal (gauss divergence theorem),how to determine the outward pointing normal (gauss divergence theorem),,"I have a cone defined by $x^2+y^2=(1-z)^2$ i was trying to work out the normal vector on surface $s_1$ indicated on the plot On $s_1$: r =$\left<x,y,0\right>$ since $z=0$ on $x-y$ plane $\dfrac{\partial}{\partial x}=1;$  $\dfrac{\partial}{\partial y}=1;$  $\dfrac{\partial}{\partial z}=0$ $\therefore$ n =$\dfrac{\partial}{\partial x}$×$\dfrac{\partial}{\partial y}$  =$\left<0,0,1\right>$, however my textbook ""thinks"" on $s_1$ this normal vector points inwards and the outward pointing normal is given by n =$\dfrac{\partial}{\partial x}$×$\dfrac{\partial}{\partial y}$  =$\left<0,0,-1\right>$. Can someone please explain why this is, how do i know where the normal vector is pointing -Thanks. EDIT: i also dont't understand why this is wrong for $s_2$: $r$=$\left<x,y,1-\sqrt{x^2+y^2}\right>$ note that $x^2+y^2=(1-z^2)$ for this question: $r_x$=$\left<1,0,\dfrac{-x}{\sqrt{x^2+y^2}}\right>$ and $r_y$=$\left<0,1,\dfrac{-y}{\sqrt{x^2+y^2}}\right>$ $r_x$×$r_y$   =$\left<\dfrac{x}{\sqrt{{x^2+y^2}}}, \dfrac{y}{\sqrt{{x^2+y^2}}},1 \right>$ $\implies n$=$\left<\dfrac{-x}{\sqrt{{x^2+y^2}}}, \dfrac{-y}{\sqrt{{x^2+y^2}}},-1 \right>$ shouldn't $-\left<\hat{i},\hat{j},\hat{k}\right>$ point away from the surface at $s_1$?","I have a cone defined by $x^2+y^2=(1-z)^2$ i was trying to work out the normal vector on surface $s_1$ indicated on the plot On $s_1$: r =$\left<x,y,0\right>$ since $z=0$ on $x-y$ plane $\dfrac{\partial}{\partial x}=1;$  $\dfrac{\partial}{\partial y}=1;$  $\dfrac{\partial}{\partial z}=0$ $\therefore$ n =$\dfrac{\partial}{\partial x}$×$\dfrac{\partial}{\partial y}$  =$\left<0,0,1\right>$, however my textbook ""thinks"" on $s_1$ this normal vector points inwards and the outward pointing normal is given by n =$\dfrac{\partial}{\partial x}$×$\dfrac{\partial}{\partial y}$  =$\left<0,0,-1\right>$. Can someone please explain why this is, how do i know where the normal vector is pointing -Thanks. EDIT: i also dont't understand why this is wrong for $s_2$: $r$=$\left<x,y,1-\sqrt{x^2+y^2}\right>$ note that $x^2+y^2=(1-z^2)$ for this question: $r_x$=$\left<1,0,\dfrac{-x}{\sqrt{x^2+y^2}}\right>$ and $r_y$=$\left<0,1,\dfrac{-y}{\sqrt{x^2+y^2}}\right>$ $r_x$×$r_y$   =$\left<\dfrac{x}{\sqrt{{x^2+y^2}}}, \dfrac{y}{\sqrt{{x^2+y^2}}},1 \right>$ $\implies n$=$\left<\dfrac{-x}{\sqrt{{x^2+y^2}}}, \dfrac{-y}{\sqrt{{x^2+y^2}}},-1 \right>$ shouldn't $-\left<\hat{i},\hat{j},\hat{k}\right>$ point away from the surface at $s_1$?",,"['calculus', 'multivariable-calculus', 'vector-analysis', 'surfaces']"
82,"Find the volume bound by the curve $\gamma(t) = (e^{\sin(t)}\cos(t),e^{\cos(t)}\sin(t))$",Find the volume bound by the curve,"\gamma(t) = (e^{\sin(t)}\cos(t),e^{\cos(t)}\sin(t))","Find the volume bound by the curve $\gamma(t) = (e^{\sin(t)}\cos(t),e^{\cos(t)}\sin(t))$ I've tried computing directly with $\int_0^{2\pi} \int_0^R r \, dr \, d\theta$ where $R = \sqrt{(e^{\sin(t)}\cos(t))^2+(e^{\cos(t)}\sin(t))^2}$ but this seems unfruitful. I've also tried considering the path as a line integral over the vector field $F=(-y,x)$, which gives us the area by Green's theorem. Here's why... \begin{equation} \int_\gamma F \cdot dr = \int_\gamma -y\,dx+x\,dy = \iint\limits_\Omega dA = \mathrm{Area}(\Omega) \end{equation} Where $\Omega$ is the region bound by the curve. But this attempt also seems unfruitful. To be honest I'm not sure if this is even possible without numerical methods. Thanks for the help in advance.","Find the volume bound by the curve $\gamma(t) = (e^{\sin(t)}\cos(t),e^{\cos(t)}\sin(t))$ I've tried computing directly with $\int_0^{2\pi} \int_0^R r \, dr \, d\theta$ where $R = \sqrt{(e^{\sin(t)}\cos(t))^2+(e^{\cos(t)}\sin(t))^2}$ but this seems unfruitful. I've also tried considering the path as a line integral over the vector field $F=(-y,x)$, which gives us the area by Green's theorem. Here's why... \begin{equation} \int_\gamma F \cdot dr = \int_\gamma -y\,dx+x\,dy = \iint\limits_\Omega dA = \mathrm{Area}(\Omega) \end{equation} Where $\Omega$ is the region bound by the curve. But this attempt also seems unfruitful. To be honest I'm not sure if this is even possible without numerical methods. Thanks for the help in advance.",,"['real-analysis', 'multivariable-calculus', 'bessel-functions']"
83,Finding the area between two curves and my attempt,Finding the area between two curves and my attempt,,"Question is to find the area between two curves given by $r=1$ and $r^{2} = \cos(2\theta)$ and the $\theta$ bounds are from $0$ to $\pi/2$. $$\begin{align}\int_{0}^{\frac{\pi}{2}}\int_{1}^{\sqrt{\cos 2\theta}} r \ dr\  d\theta &=\frac{1}{2}\int_{0}^{\frac{\pi}{2}} \cos 2 \theta\ -1\  d\theta \\&=\frac{1}{2}\Bigg[\frac{\sin 2 \theta}{2} - \theta\Bigg]_{0}^{\frac{\pi}{2}}\\&=\frac{1}{2}\Bigg[-\frac{\pi}{2} - 0\Bigg]\\=& -\frac{\pi}{4}\end{align}$$ If I am correct,why is the answer coming negative?","Question is to find the area between two curves given by $r=1$ and $r^{2} = \cos(2\theta)$ and the $\theta$ bounds are from $0$ to $\pi/2$. $$\begin{align}\int_{0}^{\frac{\pi}{2}}\int_{1}^{\sqrt{\cos 2\theta}} r \ dr\  d\theta &=\frac{1}{2}\int_{0}^{\frac{\pi}{2}} \cos 2 \theta\ -1\  d\theta \\&=\frac{1}{2}\Bigg[\frac{\sin 2 \theta}{2} - \theta\Bigg]_{0}^{\frac{\pi}{2}}\\&=\frac{1}{2}\Bigg[-\frac{\pi}{2} - 0\Bigg]\\=& -\frac{\pi}{4}\end{align}$$ If I am correct,why is the answer coming negative?",,"['calculus', 'multivariable-calculus']"
84,Show that the set $\{x\in\mathbb{R}^N:\nabla f(x)=0 \}$ is convex,Show that the set  is convex,\{x\in\mathbb{R}^N:\nabla f(x)=0 \},Let $f:\mathbb{R}^N\rightarrow \mathbb{R}$ be a $C^1$ convex function. Show that $\{x\in\mathbb{R}^N:\nabla f(x)=0 \}$ is convex (we assume that empty set is convex). Any hint?,Let $f:\mathbb{R}^N\rightarrow \mathbb{R}$ be a $C^1$ convex function. Show that $\{x\in\mathbb{R}^N:\nabla f(x)=0 \}$ is convex (we assume that empty set is convex). Any hint?,,"['real-analysis', 'multivariable-calculus', 'convex-analysis']"
85,Evaluate Surface Integral over this triangular surface,Evaluate Surface Integral over this triangular surface,,"When I solving the practice exercise problems at the end of the section, I stumbled upon this problem, which I have been trying to figure out how to compute the integral, but couldn't. Can someone please help? Integrate $G(x,y,z) = xyz$ over the triangular surface with vertices $(1,0,0),\,(0,2,0)$ and $(0,1,1)$ . What I have tried: I found the surface in question: $f(x,y,z) = 2x+y+z = 2$","When I solving the practice exercise problems at the end of the section, I stumbled upon this problem, which I have been trying to figure out how to compute the integral, but couldn't. Can someone please help? Integrate over the triangular surface with vertices and . What I have tried: I found the surface in question:","G(x,y,z) = xyz (1,0,0),\,(0,2,0) (0,1,1) f(x,y,z) = 2x+y+z = 2","['multivariable-calculus', 'surfaces', 'surface-integrals']"
86,Gradient of $x^T B^T B x - x^T B^T b - b^T Bx$ [duplicate],Gradient of  [duplicate],x^T B^T B x - x^T B^T b - b^T Bx,"This question already has answers here : How to take the gradient of the quadratic form? (6 answers) Closed 3 years ago . I want to compute the gradient $\nabla_x f(x)$ of $f(x) = x^T B^T B x - x^T B^T b - b^T Bx$ with respect to the vector $x$. So far I have tried below. But when I try to add them together, I couldn't see they come together. Edits Now I think I got it. They does come together as the following answers. Since $C=B^TB$, $C$ is symmetric. Then $c_{ij} = c_{ji}$, so  $$\frac{\partial D}{\partial x} = 2\Big[\sum_{i=1}^n c_{1i} x_i \ \ \sum_{i=1}^n c_{2i} x_i \ \ \cdots \ \ \sum_{i=1}^n c_{ni} x_i\Big] =2 C x.$$ Hence $$\frac{\partial x^TBb}{\partial x} = \Big[ \sum_{i=1}^n b_{i1} b_i \ \ \sum_{i=1}^n b_{i2} b_i \ \ \ \ \cdots \ \ \sum_{i=1}^n b_{in} b_i \ \ \Big] = B^T b.$$ So $\frac{d}{dx} (x^T B^T B x - 2x^T B^T b) = 2B^T B - 2B^Tb$. Thank you! Let $C = B^T B$. \begin{align*} x^T C x =&  \begin{pmatrix} x_1 & x_2 & \cdots & x_n\end{pmatrix} \begin{pmatrix}  c_{11} & c_{12} & \cdots & c_{1m}\\  c_{21} & c_{22} & \cdots & c_{2m}\\ \vdots & \vdots& \ddots & \vdots \\  c_{n1} & c_{n2} & \cdots & c_{nm}\\  \end{pmatrix} \begin{pmatrix} x_1 \\ x_2 \\ \vdots \\x_n\end{pmatrix}\\ =&  \begin{pmatrix} x_1 & x_2 & \cdots & x_n\end{pmatrix} \begin{pmatrix}  c_{11} x_1 + c_{12} x_2 + \cdots c_{1n} x_n\\ c_{21} x_1 + c_{22} x_2 + \cdots c_{2n} x_n\\ \vdots\\ c_{n1} x_1 + a_{n2} x_2 + \cdots a_{nn} x_n\\ \end{pmatrix}\\ =& x_1(c_{11} x_1 + c_{12} x_2 + \cdots c_{1n} x_n)+x_2(c_{21} x_1 + c_{22} x_2 + \cdots c_{2n} x_n) + \cdots + x_n(c_{n1} x_1 + a_{n2} x_2 + \cdots a_{nn} x_n) \end{align*} Since the derivative of the scalar $D = x^T C x$ by a vector $x$ is $$\frac{\partial D}{\partial x} = \bigg(\frac{\partial D}{\partial x_1} \frac{\partial D}{\partial x_2} \cdots  \frac{\partial D}{\partial x_n}\bigg),$$ I have: \begin{align*} \frac{\partial D}{\partial x_1}  &= c_{11} x_1 + c_{12} x_2 + \cdots c_{1n} x_n  + c_{11} x_1 + c_{21} x_2 + \cdots + c_{n1} x_n = \sum_{i=1}^n (c_{1i} + c_{i1})x_i \end{align*} Hence $$\frac{\partial D}{\partial x} = \Big[\sum_{i=1}^n (c_{1i} + c_{i1})x_i \ \ \sum_{i=1}^n (c_{2i}+c_{i2})x_i \ \ \cdots \ \ \sum_{i=1}^n (c_{ni}+c_{in})x_i\Big].$$ \begin{align*} x^T B b =&  \begin{pmatrix} x_1 & x_2 & \cdots & x_n\end{pmatrix} \begin{pmatrix}  b_{11} b_1 + b_{12} b_2 + \cdots b_{1n} b_n\\ b_{21} b_1 + b_{22} b_2 + \cdots b_{2n} b_n\\ \vdots\\ b_{n1} b_1 + b_{n2} b_2 + \cdots b_{nn} b_n\\ \end{pmatrix}\\ =&  x_1(b_{11} b_1 + b_{12} b_2 + \cdots b_{1n} b_n)+ x_2(b_{21} b_1 + b_{22} b_2 + \cdots b_{2n} b_n)+\cdots+ x_n(b_{n1} b_1 + b_{n2} b_2 + \cdots b_{nn} b_n) \end{align*} Hence $$\frac{\partial x^TBb}{\partial x} = \Big[ \sum_{i=1}^n b_{1i} b_i \ \ \sum_{i=1}^n b_{2i} b_i \ \ \ \ \cdots \ \ \sum_{i=1}^n b_{ni} b_i \ \ \Big].$$","This question already has answers here : How to take the gradient of the quadratic form? (6 answers) Closed 3 years ago . I want to compute the gradient $\nabla_x f(x)$ of $f(x) = x^T B^T B x - x^T B^T b - b^T Bx$ with respect to the vector $x$. So far I have tried below. But when I try to add them together, I couldn't see they come together. Edits Now I think I got it. They does come together as the following answers. Since $C=B^TB$, $C$ is symmetric. Then $c_{ij} = c_{ji}$, so  $$\frac{\partial D}{\partial x} = 2\Big[\sum_{i=1}^n c_{1i} x_i \ \ \sum_{i=1}^n c_{2i} x_i \ \ \cdots \ \ \sum_{i=1}^n c_{ni} x_i\Big] =2 C x.$$ Hence $$\frac{\partial x^TBb}{\partial x} = \Big[ \sum_{i=1}^n b_{i1} b_i \ \ \sum_{i=1}^n b_{i2} b_i \ \ \ \ \cdots \ \ \sum_{i=1}^n b_{in} b_i \ \ \Big] = B^T b.$$ So $\frac{d}{dx} (x^T B^T B x - 2x^T B^T b) = 2B^T B - 2B^Tb$. Thank you! Let $C = B^T B$. \begin{align*} x^T C x =&  \begin{pmatrix} x_1 & x_2 & \cdots & x_n\end{pmatrix} \begin{pmatrix}  c_{11} & c_{12} & \cdots & c_{1m}\\  c_{21} & c_{22} & \cdots & c_{2m}\\ \vdots & \vdots& \ddots & \vdots \\  c_{n1} & c_{n2} & \cdots & c_{nm}\\  \end{pmatrix} \begin{pmatrix} x_1 \\ x_2 \\ \vdots \\x_n\end{pmatrix}\\ =&  \begin{pmatrix} x_1 & x_2 & \cdots & x_n\end{pmatrix} \begin{pmatrix}  c_{11} x_1 + c_{12} x_2 + \cdots c_{1n} x_n\\ c_{21} x_1 + c_{22} x_2 + \cdots c_{2n} x_n\\ \vdots\\ c_{n1} x_1 + a_{n2} x_2 + \cdots a_{nn} x_n\\ \end{pmatrix}\\ =& x_1(c_{11} x_1 + c_{12} x_2 + \cdots c_{1n} x_n)+x_2(c_{21} x_1 + c_{22} x_2 + \cdots c_{2n} x_n) + \cdots + x_n(c_{n1} x_1 + a_{n2} x_2 + \cdots a_{nn} x_n) \end{align*} Since the derivative of the scalar $D = x^T C x$ by a vector $x$ is $$\frac{\partial D}{\partial x} = \bigg(\frac{\partial D}{\partial x_1} \frac{\partial D}{\partial x_2} \cdots  \frac{\partial D}{\partial x_n}\bigg),$$ I have: \begin{align*} \frac{\partial D}{\partial x_1}  &= c_{11} x_1 + c_{12} x_2 + \cdots c_{1n} x_n  + c_{11} x_1 + c_{21} x_2 + \cdots + c_{n1} x_n = \sum_{i=1}^n (c_{1i} + c_{i1})x_i \end{align*} Hence $$\frac{\partial D}{\partial x} = \Big[\sum_{i=1}^n (c_{1i} + c_{i1})x_i \ \ \sum_{i=1}^n (c_{2i}+c_{i2})x_i \ \ \cdots \ \ \sum_{i=1}^n (c_{ni}+c_{in})x_i\Big].$$ \begin{align*} x^T B b =&  \begin{pmatrix} x_1 & x_2 & \cdots & x_n\end{pmatrix} \begin{pmatrix}  b_{11} b_1 + b_{12} b_2 + \cdots b_{1n} b_n\\ b_{21} b_1 + b_{22} b_2 + \cdots b_{2n} b_n\\ \vdots\\ b_{n1} b_1 + b_{n2} b_2 + \cdots b_{nn} b_n\\ \end{pmatrix}\\ =&  x_1(b_{11} b_1 + b_{12} b_2 + \cdots b_{1n} b_n)+ x_2(b_{21} b_1 + b_{22} b_2 + \cdots b_{2n} b_n)+\cdots+ x_n(b_{n1} b_1 + b_{n2} b_2 + \cdots b_{nn} b_n) \end{align*} Hence $$\frac{\partial x^TBb}{\partial x} = \Big[ \sum_{i=1}^n b_{1i} b_i \ \ \sum_{i=1}^n b_{2i} b_i \ \ \ \ \cdots \ \ \sum_{i=1}^n b_{ni} b_i \ \ \Big].$$",,"['matrices', 'multivariable-calculus', 'quadratic-forms', 'scalar-fields']"
87,"If $f: U \rightarrow \mathbb{R}^n$ differentiable such that $|f(x)-f(y)| \geq c |x-y|$ for all $x,y \in U$, then $\det \mathbf{J}_f(x) \neq 0$","If  differentiable such that  for all , then","f: U \rightarrow \mathbb{R}^n |f(x)-f(y)| \geq c |x-y| x,y \in U \det \mathbf{J}_f(x) \neq 0","Let $f: U \rightarrow \mathbb{R}^n$ be a differentiable function on an open subset $U$ of $\mathbb{R}^n$. Assume that there exists $c>0$ such that $|f(x)-f(y)| \geq c |x-y|$ for all $x,y \in U$. Then I want to show that $\det \mathbf{J}_f(x)$ is nonzero for all $x \in U$. I think I need to apply the Inverse function theorem but I cannot see how.","Let $f: U \rightarrow \mathbb{R}^n$ be a differentiable function on an open subset $U$ of $\mathbb{R}^n$. Assume that there exists $c>0$ such that $|f(x)-f(y)| \geq c |x-y|$ for all $x,y \in U$. Then I want to show that $\det \mathbf{J}_f(x)$ is nonzero for all $x \in U$. I think I need to apply the Inverse function theorem but I cannot see how.",,"['real-analysis', 'multivariable-calculus', 'derivatives']"
88,Triple integration using spherical coordinates,Triple integration using spherical coordinates,,"Write a triple integral including limits of integration that gives the volume of the cap of the solid sphere $$x^2+y^2+z^2 ≤ 2$$ cut off by the plane z=1 and restricted to the first octant. Note: In your answer(s), type theta, rho, and phi in place of θ, ρ and ϕ, as needed Progress: I've managed to find some of the boundaries for integration using spherical coordinates, but for some reason following examples in textbooks I cannot arrive at the rest. Can someone kindly explain?","Write a triple integral including limits of integration that gives the volume of the cap of the solid sphere $$x^2+y^2+z^2 ≤ 2$$ cut off by the plane z=1 and restricted to the first octant. Note: In your answer(s), type theta, rho, and phi in place of θ, ρ and ϕ, as needed Progress: I've managed to find some of the boundaries for integration using spherical coordinates, but for some reason following examples in textbooks I cannot arrive at the rest. Can someone kindly explain?",,['multivariable-calculus']
89,Evaluate the integral along the stated curve,Evaluate the integral along the stated curve,,"$\int_{C}{(3x+2y) \, dx + (2x-y) \, dy}$ along the curve y = sin($\pi*x\over2$) from (0,0) to (1,1). (Given that the curve is smooth). Approach: I attempted this problem by parametrizing x = $\pi*t\over2$ and y = sin(t), but that wasn't working out since I got this: ∫3t + 2sin($\pi*t\over2$) + ($\pi$) t cos($\pi*t\over2$) - ($\pi\over2$)*cos($\pi*t\over2$)*sin($\pi*t\over2$). I then attempted x = t and y = sin($\pi*t\over2$), which didn't help. I'm having trouble finding which parametrization works (the integration should follow easy from there). Can some help out with the setup of the parameters? Thanks","$\int_{C}{(3x+2y) \, dx + (2x-y) \, dy}$ along the curve y = sin($\pi*x\over2$) from (0,0) to (1,1). (Given that the curve is smooth). Approach: I attempted this problem by parametrizing x = $\pi*t\over2$ and y = sin(t), but that wasn't working out since I got this: ∫3t + 2sin($\pi*t\over2$) + ($\pi$) t cos($\pi*t\over2$) - ($\pi\over2$)*cos($\pi*t\over2$)*sin($\pi*t\over2$). I then attempted x = t and y = sin($\pi*t\over2$), which didn't help. I'm having trouble finding which parametrization works (the integration should follow easy from there). Can some help out with the setup of the parameters? Thanks",,['multivariable-calculus']
90,How to represent real algebraic numbers with period integrals,How to represent real algebraic numbers with period integrals,,"Background: A real period is defined to be the value of an integral of the form $$\int_D R(x_1,\cdots,x_n)dV$$ where $R$ is a rational function with rational ceofficients, and $D\subseteq\Bbb R^n$ is a set of points satisfying a system of inequalities involving rational functions also with rational coefficients (combined via the Boolean operators and and or ), and the integral converges absolutely. A period is then a complex number whose real and imaginary parts are real periods. The set of periods is called $\cal P$. In my other question here , I've asked why we can simplify this definition to only volume integrals over domains defined by polynomial inequalities, and why we can generalize the definition by replacing both instances of the phrase ""rational coefficients"" with ""(real) algebraic coefficients."" So, question : how do we express an arbitrary (wlog real) algebraic number as a period integral? I know using the argument principle from complex analysis, if $w$ is a root of $f(z)$ then $$\frac{1}{2\pi i}\oint_{\partial D}z\frac{f'(z)}{f(z)}dz=w$$ where $D\subset\Bbb C$ is a disc of small enough radius around $w$ to not include any other zeros. (This is assuming that $w$ has multiplicity one, for instance take $f$ to be its minimal polynomial, and the boundary of the disc $\partial D$ is oriented clockwise.) But it's not obvious to me how to convert this into a real (multidimensional) integral. Perhaps I could invoke a continuum of different $D$ whose $\partial D$s form there own disk (like an onion), and then convert via polar coordinates? There's also the issue of $2\pi i$ to contend with as well. (Conjecturally, $\frac{1}{2\pi i}$ is not a period.) (I can figure out why $\cal P$ is closed under addition: we can always write $$\int_A R(\vec{x})dV+\int_B S(\vec{x})dV=\int_{\large A\sqcup(B+\vec{v})} \hskip -0.5in R(\vec{x})+S(\vec{x}-\vec{v})\,dV $$ for some rational-coordinate displacement vector $\vec{v}$ of sufficiently large magnitude, in which case $A\sqcup(B+\vec{v})$ can be described in the obvious way. Furthermore we can write $$\int_A \hskip -0.05in R(x_1,\cdots,x_n)dV \hskip -0.05in \times \hskip -0.05in \int_BS(x_1,\cdots,x_k)dV \hskip -0.05in = \hskip -0.05in \int_{A\times B} \hskip -0.2in R(x_1,\cdots,x_n)S(x_{n+1},\cdots,x_{n+k})dV $$ describing $A\times B$ in the obvious way as well, so I know how $\cal P$ is closed under multiplication.)","Background: A real period is defined to be the value of an integral of the form $$\int_D R(x_1,\cdots,x_n)dV$$ where $R$ is a rational function with rational ceofficients, and $D\subseteq\Bbb R^n$ is a set of points satisfying a system of inequalities involving rational functions also with rational coefficients (combined via the Boolean operators and and or ), and the integral converges absolutely. A period is then a complex number whose real and imaginary parts are real periods. The set of periods is called $\cal P$. In my other question here , I've asked why we can simplify this definition to only volume integrals over domains defined by polynomial inequalities, and why we can generalize the definition by replacing both instances of the phrase ""rational coefficients"" with ""(real) algebraic coefficients."" So, question : how do we express an arbitrary (wlog real) algebraic number as a period integral? I know using the argument principle from complex analysis, if $w$ is a root of $f(z)$ then $$\frac{1}{2\pi i}\oint_{\partial D}z\frac{f'(z)}{f(z)}dz=w$$ where $D\subset\Bbb C$ is a disc of small enough radius around $w$ to not include any other zeros. (This is assuming that $w$ has multiplicity one, for instance take $f$ to be its minimal polynomial, and the boundary of the disc $\partial D$ is oriented clockwise.) But it's not obvious to me how to convert this into a real (multidimensional) integral. Perhaps I could invoke a continuum of different $D$ whose $\partial D$s form there own disk (like an onion), and then convert via polar coordinates? There's also the issue of $2\pi i$ to contend with as well. (Conjecturally, $\frac{1}{2\pi i}$ is not a period.) (I can figure out why $\cal P$ is closed under addition: we can always write $$\int_A R(\vec{x})dV+\int_B S(\vec{x})dV=\int_{\large A\sqcup(B+\vec{v})} \hskip -0.5in R(\vec{x})+S(\vec{x}-\vec{v})\,dV $$ for some rational-coordinate displacement vector $\vec{v}$ of sufficiently large magnitude, in which case $A\sqcup(B+\vec{v})$ can be described in the obvious way. Furthermore we can write $$\int_A \hskip -0.05in R(x_1,\cdots,x_n)dV \hskip -0.05in \times \hskip -0.05in \int_BS(x_1,\cdots,x_k)dV \hskip -0.05in = \hskip -0.05in \int_{A\times B} \hskip -0.2in R(x_1,\cdots,x_n)S(x_{n+1},\cdots,x_{n+k})dV $$ describing $A\times B$ in the obvious way as well, so I know how $\cal P$ is closed under multiplication.)",,"['multivariable-calculus', 'algebraic-geometry']"
91,Showing that normal line passes through a point.,Showing that normal line passes through a point.,,"I need to show that a line passes through a point. How should I go about doing this? The question is: Let $L$ be the normal line at $(1,1,1)$ to the level surface of $f(x,y,z) = x^2 - z$ that passes through $(1,1,1)$. Show that $L$ passes through the point $(3,1,0)$. Thanks in advance! Edit: I tried taking the gradient, where the gradient is (2,0,-1). So L would be 2(x-1)-(z-1) = 0. Is this right? If it is, how do I show that it passes through the point? Edit 2: How do I show that it passes through (3,1,0)? (1+2(3),1,1-(0))? Edit 3: So you're saying that t_0 = 1? I'm confused as to how does this show that L passes through the point. Edit 4: Can you please show an example where the point is NOT in L? Thanks! Edit 5: So conflicting values of t? Thank you very much for your help!","I need to show that a line passes through a point. How should I go about doing this? The question is: Let $L$ be the normal line at $(1,1,1)$ to the level surface of $f(x,y,z) = x^2 - z$ that passes through $(1,1,1)$. Show that $L$ passes through the point $(3,1,0)$. Thanks in advance! Edit: I tried taking the gradient, where the gradient is (2,0,-1). So L would be 2(x-1)-(z-1) = 0. Is this right? If it is, how do I show that it passes through the point? Edit 2: How do I show that it passes through (3,1,0)? (1+2(3),1,1-(0))? Edit 3: So you're saying that t_0 = 1? I'm confused as to how does this show that L passes through the point. Edit 4: Can you please show an example where the point is NOT in L? Thanks! Edit 5: So conflicting values of t? Thank you very much for your help!",,"['multivariable-calculus', 'analytic-geometry']"
92,First derivative of a multivariable function,First derivative of a multivariable function,,"This question was featured on my calculus mid-term today : Find the first derivative of $g(x,y)$ where : $$  g(x,y) = f(x^2  + y^2 ,xy) $$ This is the exact text of the problem. I just don't get it. How are we supposed to find the first derivative of a function that has no expression ? Secondly, how cand I find the derivative when we don't know in respect with which variable to differentiate ? Can somebody please explain and show a solution ?","This question was featured on my calculus mid-term today : Find the first derivative of $g(x,y)$ where : $$  g(x,y) = f(x^2  + y^2 ,xy) $$ This is the exact text of the problem. I just don't get it. How are we supposed to find the first derivative of a function that has no expression ? Secondly, how cand I find the derivative when we don't know in respect with which variable to differentiate ? Can somebody please explain and show a solution ?",,"['multivariable-calculus', 'derivatives']"
93,Find the center of mass of soda can?,Find the center of mass of soda can?,,"Help finding center of mass of soda can? If you represent the soda can as a right-circular cylinder radius=4 cm  height =12 cm We are told to neglect the mass of the can itself. When the can is full the center of mass is at 6cm above the base, halfway along the axis of the can. As the can is drained and air replaces the soda, the center of mass descends towards the bottom. However when the can is completely empty the center of mass is still at 6cm. Assuming the density of soda is 1 gram per cubic cm and the density of air is 0.001 grams per cubic cm. Find the depth of soda in the can for which the center of mass is at it's lowest point. I am not sure where to begin, I was trying to think of this as a linear case where the fulcrum is at a point with masses on either end, but I am extremely lost. Thank you for the help","Help finding center of mass of soda can? If you represent the soda can as a right-circular cylinder radius=4 cm  height =12 cm We are told to neglect the mass of the can itself. When the can is full the center of mass is at 6cm above the base, halfway along the axis of the can. As the can is drained and air replaces the soda, the center of mass descends towards the bottom. However when the can is completely empty the center of mass is still at 6cm. Assuming the density of soda is 1 gram per cubic cm and the density of air is 0.001 grams per cubic cm. Find the depth of soda in the can for which the center of mass is at it's lowest point. I am not sure where to begin, I was trying to think of this as a linear case where the fulcrum is at a point with masses on either end, but I am extremely lost. Thank you for the help",,['multivariable-calculus']
94,Why $E^2$ instead of $\Bbb R^2$,Why  instead of,E^2 \Bbb R^2,"I was reading a paper earlier where whenever the author would discuss vectors in polar coordinates, he'd call the space $E^2$.  He'd even give a function of vectors in that space as $f:E^2 \to \Bbb R$.  I'm wondering why he didn't just call that space $\Bbb R^2$.  Is there some difference between the spaces themselves if you decide to use polar rather than Cartesian coordinates?  Is this a standard notation?  Does the $E$ stand for anything (like $\Bbb R$ is short for ""reals"")?","I was reading a paper earlier where whenever the author would discuss vectors in polar coordinates, he'd call the space $E^2$.  He'd even give a function of vectors in that space as $f:E^2 \to \Bbb R$.  I'm wondering why he didn't just call that space $\Bbb R^2$.  Is there some difference between the spaces themselves if you decide to use polar rather than Cartesian coordinates?  Is this a standard notation?  Does the $E$ stand for anything (like $\Bbb R$ is short for ""reals"")?",,"['multivariable-calculus', 'notation']"
95,Evaluate the surface integral from the paraboloid,Evaluate the surface integral from the paraboloid,,"Evaluate the surface integral $$\iint\limits_S xy \sqrt{x^2+y^2+1}\,\mathrm d\sigma,$$ where $S$ is the surface cut from the paraboloid $2z=x^2+y^2$ by the plane $z=1$. Is it possible for the answer to be $0$ ? I am not so sure. Would anyone mind telling me the answer?","Evaluate the surface integral $$\iint\limits_S xy \sqrt{x^2+y^2+1}\,\mathrm d\sigma,$$ where $S$ is the surface cut from the paraboloid $2z=x^2+y^2$ by the plane $z=1$. Is it possible for the answer to be $0$ ? I am not so sure. Would anyone mind telling me the answer?",,"['integration', 'multivariable-calculus', 'surface-integrals']"
96,Determine if the following function is one-to-one and/or onto,Determine if the following function is one-to-one and/or onto,,"$T(x,y,z) = (xy,yz,xz)$ For one to one, I made $(x,y,z)=(u,v,w)$ and solved. $$xy=uv\to y=\frac{uv}{x}$$ $$\frac{uz}{x}=w$$ $$xz = uw \to x = u$$ $$uy = uv \to y = v$$ $$vz = vw \to z = w$$ So since every $(x,y,z)$ maps to a unique point $(u,v,w)$, $T(x,y,z)$ is one-to-one. Correct? However, I'm not sure how to prove that the function is onto. I know that $T(D*) = D$, and  $T\vec x = A\vec x$ where A is a $3\times3$ matrix such that $\det(A) \neq 0$. But I don't know how to work with this to solve without having some points to work with. By looking at the function, I am pretty sure that it is onto because every point in D* will map to somewhere in D. But that's not sufficient. Could I chose arbitrary points and make equations out of it and solve those? For example: $$\begin{bmatrix}a & b & c\\d & e &f\\g &h&i\end{bmatrix}\begin{bmatrix}x\\y\\z\end{bmatrix}=\begin{bmatrix}u\\v\\w\end{bmatrix}$$ $$ax+by+cz = u$$ $$dx+ey+fz = v$$ $$gx+hy+iz = w$$ Then choose arbitrary points for $x,y,z$ and note that from earlier $u=x,v=y,w=z$. Then solve. But, T is already given, so this may be futile. Any help?","$T(x,y,z) = (xy,yz,xz)$ For one to one, I made $(x,y,z)=(u,v,w)$ and solved. $$xy=uv\to y=\frac{uv}{x}$$ $$\frac{uz}{x}=w$$ $$xz = uw \to x = u$$ $$uy = uv \to y = v$$ $$vz = vw \to z = w$$ So since every $(x,y,z)$ maps to a unique point $(u,v,w)$, $T(x,y,z)$ is one-to-one. Correct? However, I'm not sure how to prove that the function is onto. I know that $T(D*) = D$, and  $T\vec x = A\vec x$ where A is a $3\times3$ matrix such that $\det(A) \neq 0$. But I don't know how to work with this to solve without having some points to work with. By looking at the function, I am pretty sure that it is onto because every point in D* will map to somewhere in D. But that's not sufficient. Could I chose arbitrary points and make equations out of it and solve those? For example: $$\begin{bmatrix}a & b & c\\d & e &f\\g &h&i\end{bmatrix}\begin{bmatrix}x\\y\\z\end{bmatrix}=\begin{bmatrix}u\\v\\w\end{bmatrix}$$ $$ax+by+cz = u$$ $$dx+ey+fz = v$$ $$gx+hy+iz = w$$ Then choose arbitrary points for $x,y,z$ and note that from earlier $u=x,v=y,w=z$. Then solve. But, T is already given, so this may be futile. Any help?",,"['linear-algebra', 'multivariable-calculus', 'transformation']"
97,Stream functions and divergence?,Stream functions and divergence?,,"We see that the existence of a stream function guarantees that the vector field has zero    divergence or, equivalently, is source free. The converse is also true on simply connected    regions of $\mathbb{R}^2$ Why does the region have to be simply connected on $\mathbb{R}^2$ for the converse to be true? If the vector field has zero divergence, $$\frac{\partial{f}}{\partial{x}} + \frac{\partial{g}}{\partial{y}} = 0$$ $$\frac{\partial{f}}{\partial{x}} = -\frac{\partial{g}}{\partial{y}}$$ Integrating either side with respect to the appropriate variable would give you the stream line function. Doesn't this show that if a vector field has zero divergence it has a stream function no matter what?","We see that the existence of a stream function guarantees that the vector field has zero    divergence or, equivalently, is source free. The converse is also true on simply connected    regions of $\mathbb{R}^2$ Why does the region have to be simply connected on $\mathbb{R}^2$ for the converse to be true? If the vector field has zero divergence, $$\frac{\partial{f}}{\partial{x}} + \frac{\partial{g}}{\partial{y}} = 0$$ $$\frac{\partial{f}}{\partial{x}} = -\frac{\partial{g}}{\partial{y}}$$ Integrating either side with respect to the appropriate variable would give you the stream line function. Doesn't this show that if a vector field has zero divergence it has a stream function no matter what?",,"['calculus', 'multivariable-calculus', 'derivatives']"
98,"""Inverse"" Chain Rule","""Inverse"" Chain Rule",,"Green's Theorem, as we know, takes $\frac{\partial Q}{\partial x}$ and $\frac{\partial P}{\partial y}$. However, my functions $P$ and $Q$ aren't easily derived in $x$ and $y$ (actually I'm lazy) but are easily so in $r$ and $\theta$. How can I write those partials in terms of the easier ones? I looked online but didn't find much. It's like an ""inverse"" Chain Rule, since normally the Chain Rule is used to find $\frac{\partial P}{\partial r}$ and $\frac{\partial P}{\partial \theta}$ in terms of $\frac{\partial P}{\partial x}$ and $\frac{\partial P}{\partial y}$, but in this case I want the opposite. MY WORK: I gave it a shot and arrived at the following (note, $t$ is $\theta$): $$\frac{\partial Q}{\partial x} = \frac{\frac{\partial Q}{\partial r}\frac{\partial y}{\partial t}-\frac{\partial Q}{\partial t}\frac{\partial y}{\partial r}} {\frac{\partial x}{\partial r}\frac{\partial y}{\partial t}-\frac{\partial x}{\partial t}\frac{\partial y}{\partial r}}$$ This works for me, but is this correct? Can it be simplified further? Also, is this something common? EDIT: the equation above has been confirmed by @CameronBuie's answer; I applied the same thinking for the 3-dimensional case (that is, a scalar field $u(x,y,z)$ with the coordinate change $(x,y,z)=\mathbf r(r,s,t)$ and arrived at this -- I don't intend this to be useful (it's unwieldy) or correct (didn't check, but seems right), but I thought I might store it here: $$\frac{\partial u}{\partial z} = \frac{ \left( \frac{\partial x}{\partial t}\frac{\partial u}{\partial t} - \frac{\partial x}{\partial s}\frac{\partial u}{\partial r} \right) \left( \frac{\partial x}{\partial r}\frac{\partial y}{\partial s} - \frac{\partial x}{\partial s}\frac{\partial y}{\partial r} \right) - \left( \frac{\partial x}{\partial r}\frac{\partial u}{\partial s} - \frac{\partial x}{\partial s}\frac{\partial u}{\partial r} \right) \left( \frac{\partial x}{\partial t}\frac{\partial y}{\partial t} - \frac{\partial x}{\partial s}\frac{\partial y}{\partial r} \right) }{ \left( \frac{\partial x}{\partial t} \frac{\partial z}{\partial t} - \frac{\partial x}{\partial s} \frac{\partial z}{\partial r} \right) \left( \frac{\partial x}{\partial r} \frac{\partial y}{\partial s} - \frac{\partial x}{\partial s} \frac{\partial y}{\partial r} \right) - \left( \frac{\partial x}{\partial r} \frac{\partial z}{\partial s} - \frac{\partial x}{\partial s} \frac{\partial z}{\partial r} \right) \left( \frac{\partial x}{\partial t} \frac{\partial y}{\partial t} - \frac{\partial x}{\partial s} \frac{\partial y}{\partial r} \right) } $$ $\frac{\partial u}{\partial x}$ and $\frac{\partial u}{\partial y}$ can now be found solving the 2x2 system below (chain rules), or by finding a pattern on the formula above. $$\frac{\partial u}{\partial r}=\frac{\partial u}{\partial x}\frac{\partial x}{\partial r}+\frac{\partial u}{\partial y}\frac{\partial y}{\partial r}+\frac{\partial u}{\partial z}\frac{\partial z}{\partial r}$$ $$\frac{\partial u}{\partial s}=\frac{\partial u}{\partial x}\frac{\partial x}{\partial s}+\frac{\partial u}{\partial y}\frac{\partial y}{\partial s}+\frac{\partial u}{\partial z}\frac{\partial z}{\partial s}$$","Green's Theorem, as we know, takes $\frac{\partial Q}{\partial x}$ and $\frac{\partial P}{\partial y}$. However, my functions $P$ and $Q$ aren't easily derived in $x$ and $y$ (actually I'm lazy) but are easily so in $r$ and $\theta$. How can I write those partials in terms of the easier ones? I looked online but didn't find much. It's like an ""inverse"" Chain Rule, since normally the Chain Rule is used to find $\frac{\partial P}{\partial r}$ and $\frac{\partial P}{\partial \theta}$ in terms of $\frac{\partial P}{\partial x}$ and $\frac{\partial P}{\partial y}$, but in this case I want the opposite. MY WORK: I gave it a shot and arrived at the following (note, $t$ is $\theta$): $$\frac{\partial Q}{\partial x} = \frac{\frac{\partial Q}{\partial r}\frac{\partial y}{\partial t}-\frac{\partial Q}{\partial t}\frac{\partial y}{\partial r}} {\frac{\partial x}{\partial r}\frac{\partial y}{\partial t}-\frac{\partial x}{\partial t}\frac{\partial y}{\partial r}}$$ This works for me, but is this correct? Can it be simplified further? Also, is this something common? EDIT: the equation above has been confirmed by @CameronBuie's answer; I applied the same thinking for the 3-dimensional case (that is, a scalar field $u(x,y,z)$ with the coordinate change $(x,y,z)=\mathbf r(r,s,t)$ and arrived at this -- I don't intend this to be useful (it's unwieldy) or correct (didn't check, but seems right), but I thought I might store it here: $$\frac{\partial u}{\partial z} = \frac{ \left( \frac{\partial x}{\partial t}\frac{\partial u}{\partial t} - \frac{\partial x}{\partial s}\frac{\partial u}{\partial r} \right) \left( \frac{\partial x}{\partial r}\frac{\partial y}{\partial s} - \frac{\partial x}{\partial s}\frac{\partial y}{\partial r} \right) - \left( \frac{\partial x}{\partial r}\frac{\partial u}{\partial s} - \frac{\partial x}{\partial s}\frac{\partial u}{\partial r} \right) \left( \frac{\partial x}{\partial t}\frac{\partial y}{\partial t} - \frac{\partial x}{\partial s}\frac{\partial y}{\partial r} \right) }{ \left( \frac{\partial x}{\partial t} \frac{\partial z}{\partial t} - \frac{\partial x}{\partial s} \frac{\partial z}{\partial r} \right) \left( \frac{\partial x}{\partial r} \frac{\partial y}{\partial s} - \frac{\partial x}{\partial s} \frac{\partial y}{\partial r} \right) - \left( \frac{\partial x}{\partial r} \frac{\partial z}{\partial s} - \frac{\partial x}{\partial s} \frac{\partial z}{\partial r} \right) \left( \frac{\partial x}{\partial t} \frac{\partial y}{\partial t} - \frac{\partial x}{\partial s} \frac{\partial y}{\partial r} \right) } $$ $\frac{\partial u}{\partial x}$ and $\frac{\partial u}{\partial y}$ can now be found solving the 2x2 system below (chain rules), or by finding a pattern on the formula above. $$\frac{\partial u}{\partial r}=\frac{\partial u}{\partial x}\frac{\partial x}{\partial r}+\frac{\partial u}{\partial y}\frac{\partial y}{\partial r}+\frac{\partial u}{\partial z}\frac{\partial z}{\partial r}$$ $$\frac{\partial u}{\partial s}=\frac{\partial u}{\partial x}\frac{\partial x}{\partial s}+\frac{\partial u}{\partial y}\frac{\partial y}{\partial s}+\frac{\partial u}{\partial z}\frac{\partial z}{\partial s}$$",,"['multivariable-calculus', 'partial-derivative']"
99,heat equation with perfectly insulated end,heat equation with perfectly insulated end,,"In one of my tutorial question about $1$-dim heat equation,a question about heat equation with pefectly insulated end at $x=0$ and $x=l$ with ${\rm u}\left(\, x,t\,\right)$ as temperature function,TAs used as perfectly insulated end implies $\frac{\partial u}{\partial x}|_{(0,t)}=0$ and $\frac{\partial u}{\partial x}|_{(l,t)}=0$,but if perfectly insulated end means temperature at that point remains constant,then correct expression should be like $\frac{\partial u}{\partial t}|_{(0,t)}=0 $and $\frac{\partial u}{\partial t}|_{(l,t)}=0$ right? (it means $u$ is independent of t at that point, am I right?) How did the former equation represent perfectly insulated ends? What is the physical meaning of these each expression, i.e., $$\frac{\partial u}{\partial x}|_{(0,t)}=0 \ \hbox{ and  } \frac{\partial u}{\partial t}|_{(0,t)}=0$$ also similar expression like $\frac{\partial u}{\partial x}|_{(x,0)}=0$ and $\frac{\partial u}{\partial t}|_{(x,0)}=0$ in context of heat transfer environment, what does each quantity represent? What is wrong with my understanding about partial derivative? please help me... note:$u(x,t)$ is temperature at cordinate $x$ at time $t$","In one of my tutorial question about $1$-dim heat equation,a question about heat equation with pefectly insulated end at $x=0$ and $x=l$ with ${\rm u}\left(\, x,t\,\right)$ as temperature function,TAs used as perfectly insulated end implies $\frac{\partial u}{\partial x}|_{(0,t)}=0$ and $\frac{\partial u}{\partial x}|_{(l,t)}=0$,but if perfectly insulated end means temperature at that point remains constant,then correct expression should be like $\frac{\partial u}{\partial t}|_{(0,t)}=0 $and $\frac{\partial u}{\partial t}|_{(l,t)}=0$ right? (it means $u$ is independent of t at that point, am I right?) How did the former equation represent perfectly insulated ends? What is the physical meaning of these each expression, i.e., $$\frac{\partial u}{\partial x}|_{(0,t)}=0 \ \hbox{ and  } \frac{\partial u}{\partial t}|_{(0,t)}=0$$ also similar expression like $\frac{\partial u}{\partial x}|_{(x,0)}=0$ and $\frac{\partial u}{\partial t}|_{(x,0)}=0$ in context of heat transfer environment, what does each quantity represent? What is wrong with my understanding about partial derivative? please help me... note:$u(x,t)$ is temperature at cordinate $x$ at time $t$",,"['multivariable-calculus', 'partial-differential-equations', 'physics', 'partial-derivative', 'heat-equation']"
