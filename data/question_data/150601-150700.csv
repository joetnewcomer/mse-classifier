,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,Series for reciprocal of polylogarithm?,Series for reciprocal of polylogarithm?,,The series representation of a polylogarithm of order $s$ is given by $$\text{Li}_s(z) = \sum_{k=1}^{\infty}\frac{z^k}{k^s}$$ Are there any simplified expressions for $\dfrac{1}{\text{Li}_s(z)}$? Google doesn't seem to know.,The series representation of a polylogarithm of order $s$ is given by $$\text{Li}_s(z) = \sum_{k=1}^{\infty}\frac{z^k}{k^s}$$ Are there any simplified expressions for $\dfrac{1}{\text{Li}_s(z)}$? Google doesn't seem to know.,,"['sequences-and-series', 'analysis', 'polylogarithm']"
1,Characterization of differentiable functions from $\mathbb{R}^m$ to $\mathbb{R}^n$.,Characterization of differentiable functions from  to .,\mathbb{R}^m \mathbb{R}^n,"Let $U\subset\mathbb{R}^m$ be an open set. Consider a function $f:U\to\mathbb{R}^n$ and a point $a\in U$. I need help to prove that the following sentences are equivalents. (a) There exists a linear map $f'(a):U\to\mathbb{R}^n$ such that   $$f(a+v)-f(a)=f'(a)\cdot v+r(v),$$   where the ""remainder"" $r(v)$ satisfies $$\lim_{v\to 0}\frac{\|r(v)\|}{\|v\|}=0.$$ (b) For each $h\in\mathbb{R}^m$ such that $a+h\in U$, there exists a linear map $A(h):\mathbb{R}^m\to\mathbb{R}^n$ such that $A(h)\cdot h=f(a+h)-f(a)$ and $h\mapsto A(h)$ is continuous at $h=0$. The first sentence is the common definition of differentiability at a point. Could someone give me some ideias to solve it? Thanks.","Let $U\subset\mathbb{R}^m$ be an open set. Consider a function $f:U\to\mathbb{R}^n$ and a point $a\in U$. I need help to prove that the following sentences are equivalents. (a) There exists a linear map $f'(a):U\to\mathbb{R}^n$ such that   $$f(a+v)-f(a)=f'(a)\cdot v+r(v),$$   where the ""remainder"" $r(v)$ satisfies $$\lim_{v\to 0}\frac{\|r(v)\|}{\|v\|}=0.$$ (b) For each $h\in\mathbb{R}^m$ such that $a+h\in U$, there exists a linear map $A(h):\mathbb{R}^m\to\mathbb{R}^n$ such that $A(h)\cdot h=f(a+h)-f(a)$ and $h\mapsto A(h)$ is continuous at $h=0$. The first sentence is the common definition of differentiability at a point. Could someone give me some ideias to solve it? Thanks.",,"['real-analysis', 'analysis', 'multivariable-calculus', 'derivatives']"
2,Prove that the a modified Cantor Set is not Jordan-Measurable,Prove that the a modified Cantor Set is not Jordan-Measurable,,"Let $C_0 = [0,1]$ and if $C_n$ is given as a disjoint union of intervals, construct $C_{n+1}$  by removing from each interval $I$ an open interval of length $(n+2)^{-2}|I|$ in the middle of each interval, and then define $C$ as the intersection of all those intervals. I want to show that this set is not Jordan-measurable. Note that contrary to the ""standard"" Cantor set, where we remove $1/3 \cdot |I|$ in the middle of each interval, here we remove $(n+2)^{-2} \cdot |I|$ in each stage. Using some Theory about Lebesgue-measurable sets, it is easy to show this, because counting what we remove we have $$  1 - \sum_{n=0}^{\infty} \frac{1}{(n+2)^2} = 1 - \left( \sum_{n=1}^{\infty} \frac{1}{n^2} - 1 - 1/2 \right) = \frac{5}{2} - \frac{\pi^2}{6} > 0 $$ and so it has positive Lebesgue-meassure. Also if it is Jordan-measurable, then its Jordan-measure equals its Lebesgue-measure, and so it would have positive Jordan-measure. But because it equals its boundary, its boundary has positive Jordan-measure, which implies it is not Jordan-measurable, and by this contradiction it cannot be Jordan-measurable. But I want to proof this fact without using the Lebesgue-measure , so do you know a proof? Some facts about Jordan-measurable (J-measurable for short) sets I know: a set is J-measurable iff its inner and outer J-measure coincide (definition) a set is $M$ J-measureable iff for each $\varepsilon > 0$ there exists sets $S,T$ which could be written as a union of a finite number of intervals such that $$  S \subseteq M \subseteq T, \quad |T| - |S| < \varepsilon $$ a set $M$ is J-measureable iff $|\partial M| = 0$. and also a fact about approximations if we successively partition $\mathbb R$ in intervals of length $1/2^k, k = 1,2,3,\ldots$. But I do not see a way to use this facts in any useful way here.","Let $C_0 = [0,1]$ and if $C_n$ is given as a disjoint union of intervals, construct $C_{n+1}$  by removing from each interval $I$ an open interval of length $(n+2)^{-2}|I|$ in the middle of each interval, and then define $C$ as the intersection of all those intervals. I want to show that this set is not Jordan-measurable. Note that contrary to the ""standard"" Cantor set, where we remove $1/3 \cdot |I|$ in the middle of each interval, here we remove $(n+2)^{-2} \cdot |I|$ in each stage. Using some Theory about Lebesgue-measurable sets, it is easy to show this, because counting what we remove we have $$  1 - \sum_{n=0}^{\infty} \frac{1}{(n+2)^2} = 1 - \left( \sum_{n=1}^{\infty} \frac{1}{n^2} - 1 - 1/2 \right) = \frac{5}{2} - \frac{\pi^2}{6} > 0 $$ and so it has positive Lebesgue-meassure. Also if it is Jordan-measurable, then its Jordan-measure equals its Lebesgue-measure, and so it would have positive Jordan-measure. But because it equals its boundary, its boundary has positive Jordan-measure, which implies it is not Jordan-measurable, and by this contradiction it cannot be Jordan-measurable. But I want to proof this fact without using the Lebesgue-measure , so do you know a proof? Some facts about Jordan-measurable (J-measurable for short) sets I know: a set is J-measurable iff its inner and outer J-measure coincide (definition) a set is $M$ J-measureable iff for each $\varepsilon > 0$ there exists sets $S,T$ which could be written as a union of a finite number of intervals such that $$  S \subseteq M \subseteq T, \quad |T| - |S| < \varepsilon $$ a set $M$ is J-measureable iff $|\partial M| = 0$. and also a fact about approximations if we successively partition $\mathbb R$ in intervals of length $1/2^k, k = 1,2,3,\ldots$. But I do not see a way to use this facts in any useful way here.",,"['real-analysis', 'general-topology', 'analysis']"
3,Prove the inequality based on an infinite series,Prove the inequality based on an infinite series,,"Define $$f(x)=\sum_{n=1}^{\infty}\frac{nx^n}{1-x^n}.$$ It is easy to see that this series converges for $x\in(-1,1).$ Now we are asked to show that $(1-x)^2f(x)\geq x,$ for $x\in[0,1).$ I tried defining $g(x)=(1-x)^2f(x)-x$ and differentiating $g$ but could not proceed further. Any ideas?","Define $$f(x)=\sum_{n=1}^{\infty}\frac{nx^n}{1-x^n}.$$ It is easy to see that this series converges for $x\in(-1,1).$ Now we are asked to show that $(1-x)^2f(x)\geq x,$ for $x\in[0,1).$ I tried defining $g(x)=(1-x)^2f(x)-x$ and differentiating $g$ but could not proceed further. Any ideas?",,"['sequences-and-series', 'analysis', 'convergence-divergence']"
4,line integral: anticlockwise parametrisation in $\mathbb R^3$,line integral: anticlockwise parametrisation in,\mathbb R^3,"Consider $\gamma$ given by the sides of the triangle with vertices $(0,0,1)^t$, $(0,1,0)^t$ and $(1,0,0)^t$. So $\gamma$ runs through the sides of the triangle. Let $f(x,y,z)=(y,xz,x^2)$. I want to calculate $\int_\gamma f(\gamma(t))\cdot\dot\gamma(t)\,\mathrm{d}t$. So I have to calculate this integral for every side of the triangle. I know I have to go anticlockwise. So do we get $\gamma_1(s)=(1,0,0)^ts+(0,1,0)^t(1-s)$ or $\gamma_1(s)=(0,1,0)^ts+(1,0,0)^t(1-s)$ for $s\in[0,1]$? From where do you consider the ""anticlockwise direction""? I would guess we are looking from $(0,0,0)^t$. But if you're looking from outside the triangle we get the other way around. I am a bit confussed. In one or two dimensions it's easy to see but in higher dimensions not. So from where are you looking at the triangle?","Consider $\gamma$ given by the sides of the triangle with vertices $(0,0,1)^t$, $(0,1,0)^t$ and $(1,0,0)^t$. So $\gamma$ runs through the sides of the triangle. Let $f(x,y,z)=(y,xz,x^2)$. I want to calculate $\int_\gamma f(\gamma(t))\cdot\dot\gamma(t)\,\mathrm{d}t$. So I have to calculate this integral for every side of the triangle. I know I have to go anticlockwise. So do we get $\gamma_1(s)=(1,0,0)^ts+(0,1,0)^t(1-s)$ or $\gamma_1(s)=(0,1,0)^ts+(1,0,0)^t(1-s)$ for $s\in[0,1]$? From where do you consider the ""anticlockwise direction""? I would guess we are looking from $(0,0,0)^t$. But if you're looking from outside the triangle we get the other way around. I am a bit confussed. In one or two dimensions it's easy to see but in higher dimensions not. So from where are you looking at the triangle?",,"['calculus', 'real-analysis', 'integration', 'analysis', 'vector-analysis']"
5,How can we prove this integral inequality ? $\int_{0}^{\frac{\pi}{2}}\left|\frac{\sin{(2n+1)t}}{\sin{t}}\right|dt<\pi\left(1+\frac{\ln{n}}{2}\right)$,How can we prove this integral inequality ?,\int_{0}^{\frac{\pi}{2}}\left|\frac{\sin{(2n+1)t}}{\sin{t}}\right|dt<\pi\left(1+\frac{\ln{n}}{2}\right),"Use this  $$\dfrac{1}{2}+\sum_{k=1}^{n}\cos{(kx)}=\dfrac{\sin{\left(n+\dfrac{1}{2}\right)x}}{2\sin{\dfrac{x}{2}}},x\neq 2m\pi,m\in\mathbb{Z}$$ to show that $$\int_{0}^{\dfrac{\pi}{2}}\left|\dfrac{\sin{(2n+1)t}}{\sin{t}}\right|dt<\pi\left(1+\dfrac{\ln{n}}{2}\right)\text{ for }n\ge 3$$","Use this  $$\dfrac{1}{2}+\sum_{k=1}^{n}\cos{(kx)}=\dfrac{\sin{\left(n+\dfrac{1}{2}\right)x}}{2\sin{\dfrac{x}{2}}},x\neq 2m\pi,m\in\mathbb{Z}$$ to show that $$\int_{0}^{\dfrac{\pi}{2}}\left|\dfrac{\sin{(2n+1)t}}{\sin{t}}\right|dt<\pi\left(1+\dfrac{\ln{n}}{2}\right)\text{ for }n\ge 3$$",,"['real-analysis', 'integration', 'analysis', 'integral-inequality']"
6,Is $\overline{\{x: f(x) < a\}} = \{x: f(x) \leq a\}$?,Is ?,\overline{\{x: f(x) < a\}} = \{x: f(x) \leq a\},"Let $f: \mathbb{R}^{2} \rightarrow \mathbb{R}$ be a continuous function. Is $\overline{A} =\overline{\{x: f(x) < a\}} = \{x: f(x) \leq a\} = B$? Since $B$ is closed and contains $A$, $\overline{A} \subset B$ as $\overline{A}$ is the smallest closed subset containing $A$. It remains to show $B \subset \overline{A}$. Let $x \in B$. If $f(x) < a$, then we are done. Now suppose $f(x) = a$. There exists $x_{n} \in \mathbb{R}^{2}$ such that $x_{n} \rightarrow x$. However, I cannot guarantee that $f(x_{n}) < a$.","Let $f: \mathbb{R}^{2} \rightarrow \mathbb{R}$ be a continuous function. Is $\overline{A} =\overline{\{x: f(x) < a\}} = \{x: f(x) \leq a\} = B$? Since $B$ is closed and contains $A$, $\overline{A} \subset B$ as $\overline{A}$ is the smallest closed subset containing $A$. It remains to show $B \subset \overline{A}$. Let $x \in B$. If $f(x) < a$, then we are done. Now suppose $f(x) = a$. There exists $x_{n} \in \mathbb{R}^{2}$ such that $x_{n} \rightarrow x$. However, I cannot guarantee that $f(x_{n}) < a$.",,"['real-analysis', 'analysis']"
7,how to prove $\lim\limits_{x \to 0+} f'(x)$ = $\lim\limits_{x \to 0-} f'(x)$ implies $f'(0)$ exists?,how to prove  =  implies  exists?,\lim\limits_{x \to 0+} f'(x) \lim\limits_{x \to 0-} f'(x) f'(0),"$f:(-1,1) \to R$ is continous and $f'(x)$ exists for all $x \in (-1,1)$ except $x=0$ if $\lim\limits_{x \to 0+} f'(x)$, $\lim\limits_{x \to 0-} f'(x)$ exists and same how to prove $f'(0)$ exists?","$f:(-1,1) \to R$ is continous and $f'(x)$ exists for all $x \in (-1,1)$ except $x=0$ if $\lim\limits_{x \to 0+} f'(x)$, $\lim\limits_{x \to 0-} f'(x)$ exists and same how to prove $f'(0)$ exists?",,"['calculus', 'analysis']"
8,Closed expression for infinite series,Closed expression for infinite series,,"How can I find the value of the series $$\sum_{n=1}^\infty \frac{1}{(b+n)(a+n)}$$ where $a,b\in[0,1)$? It is obvious by standard arguments that the series converges, but how can I derive the explicit value dependent on $a$ and $b$. Thanks EDIT: $a,b$ can be assumed to be different.","How can I find the value of the series $$\sum_{n=1}^\infty \frac{1}{(b+n)(a+n)}$$ where $a,b\in[0,1)$? It is obvious by standard arguments that the series converges, but how can I derive the explicit value dependent on $a$ and $b$. Thanks EDIT: $a,b$ can be assumed to be different.",,"['sequences-and-series', 'analysis', 'convergence-divergence']"
9,"Equivalent norms, identity function","Equivalent norms, identity function",,"If $||*||_{1}$ and $||*||_{2}$ are norms on $X$ and $I:(X,||*||_{1}) \rightarrow  (X,||*||_{2})$ is the identity function and it is continous... are $||*||_{1}$ and $||*||_{2}$ equivalent? It is continous, so it is bounded... in that way $||x||_{2}=||I(x)||_{2}\leq ||I||||x||_{1}\leq M||x||_{1} $. what about a $N$ such that $N||x||_{1}\leq ||x||_{2}$ ? does it always exists? is $I^{-1}$ continous (or $I$ open)?","If $||*||_{1}$ and $||*||_{2}$ are norms on $X$ and $I:(X,||*||_{1}) \rightarrow  (X,||*||_{2})$ is the identity function and it is continous... are $||*||_{1}$ and $||*||_{2}$ equivalent? It is continous, so it is bounded... in that way $||x||_{2}=||I(x)||_{2}\leq ||I||||x||_{1}\leq M||x||_{1} $. what about a $N$ such that $N||x||_{1}\leq ||x||_{2}$ ? does it always exists? is $I^{-1}$ continous (or $I$ open)?",,"['general-topology', 'analysis', 'functional-analysis']"
10,$a_1=3$ and $a_{n+1}=\frac{a_n}{2} + \frac{1}{a_n}$. Show that it monotonically decreases and find the limit.,and . Show that it monotonically decreases and find the limit.,a_1=3 a_{n+1}=\frac{a_n}{2} + \frac{1}{a_n},"What I've done so far: I have proved that this sequence is bounded below by 0, which is a very rough estimate. I know that the infimum is $\sqrt2$. Anyway, the question first asks me to prove that the sequence decreases monotonically. And I've tried the following: Suppose $a_{n+1} \le a_n$, then we would get $\frac{1}{a_n} \le \frac{a_n}{2}$. After some algebra, I ended up with $\frac{1}{a_n^2 +2}\le\frac{a_n^2 +2}{2}$. Now, $a_{n+2} - a_{n+1}=\frac{2a_n}{a_n^2 +2}-\frac{a_n^2 +2}{4a_n}$. I was looking for some similarities between these two expressions but it doesn't seem to work out. A more general question is, if the sequence is defined recursively, what are some common strategies to find the limit? I just started studying analysis and this kind of questions kind of troubles me. Thank you guys.","What I've done so far: I have proved that this sequence is bounded below by 0, which is a very rough estimate. I know that the infimum is $\sqrt2$. Anyway, the question first asks me to prove that the sequence decreases monotonically. And I've tried the following: Suppose $a_{n+1} \le a_n$, then we would get $\frac{1}{a_n} \le \frac{a_n}{2}$. After some algebra, I ended up with $\frac{1}{a_n^2 +2}\le\frac{a_n^2 +2}{2}$. Now, $a_{n+2} - a_{n+1}=\frac{2a_n}{a_n^2 +2}-\frac{a_n^2 +2}{4a_n}$. I was looking for some similarities between these two expressions but it doesn't seem to work out. A more general question is, if the sequence is defined recursively, what are some common strategies to find the limit? I just started studying analysis and this kind of questions kind of troubles me. Thank you guys.",,"['real-analysis', 'sequences-and-series', 'analysis']"
11,On the equation $\exp(a x+b)=\ln(x)$,On the equation,\exp(a x+b)=\ln(x),"I am confronted with: $$\exp(a x+b)=\ln(x)$$ for $a,b$ reals and $a<0$, $b>0$. I need the (unique) solution for $x$. My first target is (if it exists) an analytic solution in terms of elementary functions (perhaps augmented a bit  e.g. with Lambert's W function ). Otherwise I will have to work out some approximation scheme. Regarding the analytic solution, my research has not revealed anything until now so any pointer for further study (that I might have missed) is more than welcome. On the approximation case, due to the mixing of the exponential and the logarithm, things are not obvious. So any opinion or clue would be very useful.","I am confronted with: $$\exp(a x+b)=\ln(x)$$ for $a,b$ reals and $a<0$, $b>0$. I need the (unique) solution for $x$. My first target is (if it exists) an analytic solution in terms of elementary functions (perhaps augmented a bit  e.g. with Lambert's W function ). Otherwise I will have to work out some approximation scheme. Regarding the analytic solution, my research has not revealed anything until now so any pointer for further study (that I might have missed) is more than welcome. On the approximation case, due to the mixing of the exponential and the logarithm, things are not obvious. So any opinion or clue would be very useful.",,"['real-analysis', 'analysis', 'logarithms', 'exponentiation', 'exponential-function']"
12,Is this double integral always positive on nonzero continuous functions?,Is this double integral always positive on nonzero continuous functions?,,"Is this double integral $$(f,g)=\int_{x=0}^1\int_{y=0}^1\frac{f(x)g(y)}{|y-x|^{\frac14}}dydx$$ an inner product on continuous functions on $[0,1]$? Namely, is $(f,f)$ always positive for all nonzero continuous functions $f$? I don't know if this is true, but I conjecture it being correct.","Is this double integral $$(f,g)=\int_{x=0}^1\int_{y=0}^1\frac{f(x)g(y)}{|y-x|^{\frac14}}dydx$$ an inner product on continuous functions on $[0,1]$? Namely, is $(f,f)$ always positive for all nonzero continuous functions $f$? I don't know if this is true, but I conjecture it being correct.",,['analysis']
13,Absolute and conditional convergence of function series,Absolute and conditional convergence of function series,,"I have a problem. I have to explore absolute and conditional convergence of this function series Unfortunately. I didn't find in my reference any words about ""absolute and conditional"", Instead I've just seen ""uniform convergence, amount and balance"" of function series. I've jusy started to learn it. Help me find out the steps of solving and solve my example. Thank You so much!","I have a problem. I have to explore absolute and conditional convergence of this function series Unfortunately. I didn't find in my reference any words about ""absolute and conditional"", Instead I've just seen ""uniform convergence, amount and balance"" of function series. I've jusy started to learn it. Help me find out the steps of solving and solve my example. Thank You so much!",,"['sequences-and-series', 'analysis', 'convergence-divergence', 'absolute-convergence']"
14,Solve $\frac{1}{1^z}+\frac{1}{3^z}+\frac{1}{5^z}+\cdots=\frac{1}{2^z}+\frac{1}{4^z}+\frac{1}{6^z}+\cdots$ for $z\in \mathbb C$,Solve  for,\frac{1}{1^z}+\frac{1}{3^z}+\frac{1}{5^z}+\cdots=\frac{1}{2^z}+\frac{1}{4^z}+\frac{1}{6^z}+\cdots z\in \mathbb C,My professor gave us this problem. Find all complex numbers $z\in \mathbb C$ such that   $$\frac{1}{1^z}+\frac{1}{3^z}+\frac{1}{5^z}+\cdots=\frac{1}{2^z}+\frac{1}{4^z}+\frac{1}{6^z}+\cdots$$ I removed my try because it's wrong.,My professor gave us this problem. Find all complex numbers $z\in \mathbb C$ such that   $$\frac{1}{1^z}+\frac{1}{3^z}+\frac{1}{5^z}+\cdots=\frac{1}{2^z}+\frac{1}{4^z}+\frac{1}{6^z}+\cdots$$ I removed my try because it's wrong.,,['analysis']
15,"Why is $-\Delta+1$ an isomorphism between Sobolev space $W^{2,p}$ and $L^p\,$?",Why is  an isomorphism between Sobolev space  and ?,"-\Delta+1 W^{2,p} L^p\,","Consider the linear elliptic operator $L: W^{2,p}(\mathbb{R}^N)\to L^p(\mathbb{R}^N)$ defined by $Lu=-\Delta u + u$. Can we prove that $L$ is an isomorphism for all $p\geq 1$? It can be proved that it is true when $p=2$ (by classical Fourier analysis), however when $p\neq2$ the usual Fourier analysis does not work well (Fourier transform is not a isomorphism on $L^p(\mathbb{R}^N)$ if $p\neq2$).","Consider the linear elliptic operator $L: W^{2,p}(\mathbb{R}^N)\to L^p(\mathbb{R}^N)$ defined by $Lu=-\Delta u + u$. Can we prove that $L$ is an isomorphism for all $p\geq 1$? It can be proved that it is true when $p=2$ (by classical Fourier analysis), however when $p\neq2$ the usual Fourier analysis does not work well (Fourier transform is not a isomorphism on $L^p(\mathbb{R}^N)$ if $p\neq2$).",,"['analysis', 'functional-analysis', 'partial-differential-equations', 'sobolev-spaces']"
16,Riemann-integrable functions and pointwise convergence,Riemann-integrable functions and pointwise convergence,,"Hello, I was hoping for some advice on finding a function which will satisfy this. I think I am okay with the actual execution of the answer, but I don't know how I'm supposed to find a suitable function. Thank you","Hello, I was hoping for some advice on finding a function which will satisfy this. I think I am okay with the actual execution of the answer, but I don't know how I'm supposed to find a suitable function. Thank you",,"['integration', 'analysis', 'functions']"
17,Show that $L^1\subsetneq (L^\infty)^*$ [duplicate],Show that  [duplicate],L^1\subsetneq (L^\infty)^*,This question already has an answer here : Show that $L^1$ is strictly contained in $(L^\infty)^*$ (1 answer) Closed 10 years ago . How does one show that $L^1\subsetneq (L^\infty)^*$? I am having trouble in this. Any help would be appreciated.,This question already has an answer here : Show that $L^1$ is strictly contained in $(L^\infty)^*$ (1 answer) Closed 10 years ago . How does one show that $L^1\subsetneq (L^\infty)^*$? I am having trouble in this. Any help would be appreciated.,,"['analysis', 'functional-analysis']"
18,Exponents with the same power,Exponents with the same power,,"I've wanted to practice solving simple operations on exponents, so I've made a couple of equations to which I know the answers. $$5^x -4^x = 9$$ I feel really stupid, because I can't solve this one (obviously the answer is 2). I can't separate the x's after using logarithms. Any hints? Thanks.","I've wanted to practice solving simple operations on exponents, so I've made a couple of equations to which I know the answers. $$5^x -4^x = 9$$ I feel really stupid, because I can't solve this one (obviously the answer is 2). I can't separate the x's after using logarithms. Any hints? Thanks.",,"['calculus', 'analysis', 'logarithms', 'exponentiation']"
19,"If T is a bounded linear map, what does $(T^\ast T)^{1/2}$ mean?","If T is a bounded linear map, what does  mean?",(T^\ast T)^{1/2},"If $T: H_1 \rightarrow H_2$ is a continuous linear map between two Hilbert spaces, $H_1$ and $H_2$, what does the notation $(T^\ast T)^{1/2}$ mean?  The book I'm reading defines $|T|$ to mean $(T^\ast T)^{1/2}$. ($T^\ast$ is of course the adjoint)","If $T: H_1 \rightarrow H_2$ is a continuous linear map between two Hilbert spaces, $H_1$ and $H_2$, what does the notation $(T^\ast T)^{1/2}$ mean?  The book I'm reading defines $|T|$ to mean $(T^\ast T)^{1/2}$. ($T^\ast$ is of course the adjoint)",,"['analysis', 'functional-analysis']"
20,"Is $f(x)=\sup_{y\in K}g(x, y)$ a continuous function?",Is  a continuous function?,"f(x)=\sup_{y\in K}g(x, y)","Let $K\subset \mathbb R^n$ be a compact subset and consider a continuous function $g:K\times K\longrightarrow \mathbb R$. Define $f:K\longrightarrow \mathbb R$ by, $$f(x)=\sup_{y\in K}g(x, y).$$ Is $f$ a continuous function?","Let $K\subset \mathbb R^n$ be a compact subset and consider a continuous function $g:K\times K\longrightarrow \mathbb R$. Define $f:K\longrightarrow \mathbb R$ by, $$f(x)=\sup_{y\in K}g(x, y).$$ Is $f$ a continuous function?",,['analysis']
21,Does the series converge? [closed],Does the series converge? [closed],,"Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 10 years ago . Improve this question Let $a_n$ be a sequence of positive reals satisfying  $$ a_n\le a_{2n}+a_{2n+1} $$ What can we say about the convergence of $\sum_{n=1}^\infty a_n $ ?","Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 10 years ago . Improve this question Let $a_n$ be a sequence of positive reals satisfying  $$ a_n\le a_{2n}+a_{2n+1} $$ What can we say about the convergence of $\sum_{n=1}^\infty a_n $ ?",,"['real-analysis', 'sequences-and-series', 'analysis']"
22,Why should we use the Fourier Transform?,Why should we use the Fourier Transform?,,"I'm a CS/Math double major, and during my study (and reading sources out of my own interest) I've had some encounters with the Fourier Transform. I understand the theory behind Fourier series, and why the integral to calculate the coefficients works. (Nevertheless, my understanding is obviously far from complete; I feel the topics that I ask about are kind of basic). I basically understand/view the Fourier transform as an extension from the calculations from the discrete coefficients to the real numbers. Instead of a coefficient for each natural number, we have a coefficient for each positive real number. I have some questions about this approach. My first question is: why does the Fourier transform make sense? How do we know the Fourier transform spikes at the right spots and not at spots inbetween natural numbers? Further, the Fourier transform obviously seems have some nice properties, otherwise people wouldn't use it so much. What are the cons of using the Fourier transform as opposed to using the Fourier series?","I'm a CS/Math double major, and during my study (and reading sources out of my own interest) I've had some encounters with the Fourier Transform. I understand the theory behind Fourier series, and why the integral to calculate the coefficients works. (Nevertheless, my understanding is obviously far from complete; I feel the topics that I ask about are kind of basic). I basically understand/view the Fourier transform as an extension from the calculations from the discrete coefficients to the real numbers. Instead of a coefficient for each natural number, we have a coefficient for each positive real number. I have some questions about this approach. My first question is: why does the Fourier transform make sense? How do we know the Fourier transform spikes at the right spots and not at spots inbetween natural numbers? Further, the Fourier transform obviously seems have some nice properties, otherwise people wouldn't use it so much. What are the cons of using the Fourier transform as opposed to using the Fourier series?",,"['analysis', 'fourier-analysis', 'fourier-series']"
23,Prove Borel's Lemma (Pugh's book #35) [duplicate],Prove Borel's Lemma (Pugh's book #35) [duplicate],,"This question already has answers here : Every power series is the Taylor series of some $C^{\infty}$ function (3 answers) Closed 10 years ago . Given any sequence whatsoever of real numbers $(a_r:r\in\mathbb{Z}_+)$ , there is a smooth function $f: \mathbb{R} \to \mathbb{R}$ such that $f^{(r)}(0) = a_r$ .  Pugh's hint says to try $$f=\sum \beta_k(x)a_k\frac{x^k}{k!},$$ where $\beta_k$ is a well-chosen bump function. I'm working with a few other people and we're trying to define a bump function $\beta_k(x)$ = bump $*b_k*x$ where from 1/2 to 1, $(\exp(\frac{1}{|x|-1}))(1-\exp(\frac{-1}{|x|-1/2}) + C_k$ but have no idea how to define","This question already has answers here : Every power series is the Taylor series of some $C^{\infty}$ function (3 answers) Closed 10 years ago . Given any sequence whatsoever of real numbers , there is a smooth function such that .  Pugh's hint says to try where is a well-chosen bump function. I'm working with a few other people and we're trying to define a bump function = bump where from 1/2 to 1, but have no idea how to define","(a_r:r\in\mathbb{Z}_+) f: \mathbb{R} \to \mathbb{R} f^{(r)}(0) = a_r f=\sum \beta_k(x)a_k\frac{x^k}{k!}, \beta_k \beta_k(x) *b_k*x (\exp(\frac{1}{|x|-1}))(1-\exp(\frac{-1}{|x|-1/2}) + C_k","['calculus', 'analysis']"
24,Let $A\subset\mathbb{R}$ a measurable and bounded set. Show that exists for each $0<\alpha<1$ an interval $I$ such that $m(A\cap I)/m(I)>\alpha$.,Let  a measurable and bounded set. Show that exists for each  an interval  such that .,A\subset\mathbb{R} 0<\alpha<1 I m(A\cap I)/m(I)>\alpha,"Let $A\subset\mathbb{R}$ a measurable where $0<m(A)<\infty$. Show that exists for each $0<\alpha<1$ an interval $I$ such that  $$ \frac{m(A\cap I)}{m(I)}>\alpha. $$ MY ATTEMPT : Following a hint a give that: Let $\varepsilon>0$, exists $G$, a open set such that $m(A)\leq m(G)<m(A)+\varepsilon$. As $G$ is open, we can write as the disjoint sum of open intervals $\dot{\bigcup}_n I_n = G$.So, $$ m(A)\leq m(G)=m\left(\sum \dot{\bigcup_n} I_n \right)\leq \sum m\left(I_n \right)<m(A)+\varepsilon $$ Suppose that $\varepsilon=(\alpha^{-1}-1)m(A)$: $$ m(A)+\varepsilon=m(A)+m(A)(\alpha^{-1}-1)=m(A)(1+\alpha^{-1}-1)=m(A)\alpha^{-1} $$ and $$ \alpha\sum m(I_n)<m(A)=\sum m(A\cap I_n) $$ $$ \Rightarrow \alpha<\frac{\sum m(A\cap I_n)}{\sum m(I_n)}  $$ But I have a insecurity with that: (1) I can write that: $m(A)=\sum m(A\cap I_n)$? (2) How to get the result with this: $$ \alpha<\frac{\sum m(A\cap I_n)}{\sum m(I_n)}  $$","Let $A\subset\mathbb{R}$ a measurable where $0<m(A)<\infty$. Show that exists for each $0<\alpha<1$ an interval $I$ such that  $$ \frac{m(A\cap I)}{m(I)}>\alpha. $$ MY ATTEMPT : Following a hint a give that: Let $\varepsilon>0$, exists $G$, a open set such that $m(A)\leq m(G)<m(A)+\varepsilon$. As $G$ is open, we can write as the disjoint sum of open intervals $\dot{\bigcup}_n I_n = G$.So, $$ m(A)\leq m(G)=m\left(\sum \dot{\bigcup_n} I_n \right)\leq \sum m\left(I_n \right)<m(A)+\varepsilon $$ Suppose that $\varepsilon=(\alpha^{-1}-1)m(A)$: $$ m(A)+\varepsilon=m(A)+m(A)(\alpha^{-1}-1)=m(A)(1+\alpha^{-1}-1)=m(A)\alpha^{-1} $$ and $$ \alpha\sum m(I_n)<m(A)=\sum m(A\cap I_n) $$ $$ \Rightarrow \alpha<\frac{\sum m(A\cap I_n)}{\sum m(I_n)}  $$ But I have a insecurity with that: (1) I can write that: $m(A)=\sum m(A\cap I_n)$? (2) How to get the result with this: $$ \alpha<\frac{\sum m(A\cap I_n)}{\sum m(I_n)}  $$",,"['real-analysis', 'analysis', 'measure-theory', 'lebesgue-measure']"
25,Prove $y = x$ is continuous,Prove  is continuous,y = x,For every $\epsilon > 0$ there exists a $\delta > 0$ such that $|x - c| < \delta$ implies $|f(x) - f(c)| < \epsilon$. Start with $|f(x) - f(c)| < \epsilon$ which gives $|x - c| < \epsilon$. We also know $|x - c| < \delta$ but how can we connect $\epsilon$ and $\delta$?,For every $\epsilon > 0$ there exists a $\delta > 0$ such that $|x - c| < \delta$ implies $|f(x) - f(c)| < \epsilon$. Start with $|f(x) - f(c)| < \epsilon$ which gives $|x - c| < \epsilon$. We also know $|x - c| < \delta$ but how can we connect $\epsilon$ and $\delta$?,,['analysis']
26,Limit of an integral with changing domain,Limit of an integral with changing domain,,"I'm trying to figure out the limit : $$\lim_{n\rightarrow \infty} \int_{0}^{\sqrt{n}} \left(1-\frac{x^2}{n}\right)^{\!n}dx$$ Now the problem is that as $n$ rises so does the range of integration, otherwise I could use Dini's theorem to show uniform convergence to $\int_0^{\infty} \mathrm{e}^{-x^2}dx=\frac{\sqrt{\pi}}{2}$, but from what I gather the idea of taking the limit of a sequence of integrals into the integral only works if the domain remains static. My intuition however tells me that I would get $\frac{\sqrt{\pi}}{2}$ as a result, since obviously the expression above would behave almost exactly like  $\int_0^{\infty} \mathrm{e}^{-x^2}dx$ for large enough $n$'s - my problem is hereby with the formality. Thanks a million!","I'm trying to figure out the limit : $$\lim_{n\rightarrow \infty} \int_{0}^{\sqrt{n}} \left(1-\frac{x^2}{n}\right)^{\!n}dx$$ Now the problem is that as $n$ rises so does the range of integration, otherwise I could use Dini's theorem to show uniform convergence to $\int_0^{\infty} \mathrm{e}^{-x^2}dx=\frac{\sqrt{\pi}}{2}$, but from what I gather the idea of taking the limit of a sequence of integrals into the integral only works if the domain remains static. My intuition however tells me that I would get $\frac{\sqrt{\pi}}{2}$ as a result, since obviously the expression above would behave almost exactly like  $\int_0^{\infty} \mathrm{e}^{-x^2}dx$ for large enough $n$'s - my problem is hereby with the formality. Thanks a million!",,"['calculus', 'real-analysis', 'analysis']"
27,when is this statement is true?,when is this statement is true?,,Let $a_i>0$ be a sequence in $\mathbb{R}$. It's well known that: $\sum\limits_{i=0}^{n}a_i\to a \ $(as $n\to\infty)\Longrightarrow a_i\to0$ (as $n\to\infty$) My question is when is the following statement true: $\dfrac1n\sum\limits_{i=0}^{n}a_i\to 0\ $(as $n\to\infty$) $\Longrightarrow$ $a_i\to0$(as $n\to\infty$) ?,Let $a_i>0$ be a sequence in $\mathbb{R}$. It's well known that: $\sum\limits_{i=0}^{n}a_i\to a \ $(as $n\to\infty)\Longrightarrow a_i\to0$ (as $n\to\infty$) My question is when is the following statement true: $\dfrac1n\sum\limits_{i=0}^{n}a_i\to 0\ $(as $n\to\infty$) $\Longrightarrow$ $a_i\to0$(as $n\to\infty$) ?,,"['real-analysis', 'sequences-and-series', 'analysis']"
28,Does having a zero eigenvalue preclude a matrix from being indefinite?,Does having a zero eigenvalue preclude a matrix from being indefinite?,,"If a $3\times3$ matrix has a positive eigenvalue, a negative eigenvalue, and a zero eigenvalue, is it then, by definition, indefinite? I think so, since the matrix has both a positive and a negative eigenvalue. However, my optimisation lecture notes categorically claims that as a long as there is any zero eigenvalue, the stationary-point test fails. I am not including the details of the stationary-point test as it is tangential to the substance of my question.","If a $3\times3$ matrix has a positive eigenvalue, a negative eigenvalue, and a zero eigenvalue, is it then, by definition, indefinite? I think so, since the matrix has both a positive and a negative eigenvalue. However, my optimisation lecture notes categorically claims that as a long as there is any zero eigenvalue, the stationary-point test fails. I am not including the details of the stationary-point test as it is tangential to the substance of my question.",,"['linear-algebra', 'analysis', 'multivariable-calculus', 'optimization', 'nonlinear-optimization']"
29,How to construct a bump function ends at different value?,How to construct a bump function ends at different value?,,"May I ask how to construct a ''bump'' function ends at different value? For example: $\Psi\colon [0,1] \to [0,1]:$ $$ \Psi (x) =    \begin{cases}     0 & \quad \text{for $0 \leq x < 1/3$}\\     ??? & \quad  \text{for $1/3 \leq x < 1/2$}\\    1 & \quad \text{otherwise}   \end{cases}$$","May I ask how to construct a ''bump'' function ends at different value? For example: $\Psi\colon [0,1] \to [0,1]:$ $$ \Psi (x) =    \begin{cases}     0 & \quad \text{for $0 \leq x < 1/3$}\\     ??? & \quad  \text{for $1/3 \leq x < 1/2$}\\    1 & \quad \text{otherwise}   \end{cases}$$",,['analysis']
30,Doubt in the definition of the Fourier transform in $L^{2}(\mathbb R^n)$,Doubt in the definition of the Fourier transform in,L^{2}(\mathbb R^n),"I am trying to understand the definition of the Fourier transform in $L^{2}(\mathbb R^n)$ . I am understand of this manner : Let $f \in L^{2}(\mathbb R^n)$ and $n$ a natural number. Define $f_n = f {\chi}_{B(0,n)}$. The Fourier transform of $f$ is the function $\hat{f}$ given by $$ \hat{f} = \lim_n \hat{f_n}  \ \  \text{(the limit is in $L^2{(\mathbb R^n)}$)}$$ and the  Fourier transfom of $f$ in a point $\xi$ is given by $$ \hat{f}(\xi) = \lim_{n} \int_{B(0,n)} f_n(x)e^{-2 \pi i \langle x,\xi\rangle} \ dx$$ I am right? Thanks in advance","I am trying to understand the definition of the Fourier transform in $L^{2}(\mathbb R^n)$ . I am understand of this manner : Let $f \in L^{2}(\mathbb R^n)$ and $n$ a natural number. Define $f_n = f {\chi}_{B(0,n)}$. The Fourier transform of $f$ is the function $\hat{f}$ given by $$ \hat{f} = \lim_n \hat{f_n}  \ \  \text{(the limit is in $L^2{(\mathbb R^n)}$)}$$ and the  Fourier transfom of $f$ in a point $\xi$ is given by $$ \hat{f}(\xi) = \lim_{n} \int_{B(0,n)} f_n(x)e^{-2 \pi i \langle x,\xi\rangle} \ dx$$ I am right? Thanks in advance",,"['analysis', 'fourier-analysis', 'harmonic-analysis']"
31,Prove that $||x||= [(|\alpha_{1}|^2+|\alpha_{2}|^3)^{3/2} + |\alpha_{3}|^3]^{1/3}$ defines a norm,Prove that  defines a norm,||x||= [(|\alpha_{1}|^2+|\alpha_{2}|^3)^{3/2} + |\alpha_{3}|^3]^{1/3},"Let the vector space $X=K^3$. For $x=(\alpha_{1}, \alpha_{2}, \alpha_{3}) \in X$, we define $||x||= [(|\alpha_{1}|^2+|\alpha_{2}|^3)^\frac{3}{2} + |\alpha_{3}|^3]^\frac{1}{3}$ Proof that $||·||$ is a norm. To prove that it is a norm,  I have to proof 4 properties. I know how to prove 3 of them but I don´t know how to prove the triangle inequality, i.e: $||x+y||≤||x||+||y||$ If you can help me please. Thank you very much.","Let the vector space $X=K^3$. For $x=(\alpha_{1}, \alpha_{2}, \alpha_{3}) \in X$, we define $||x||= [(|\alpha_{1}|^2+|\alpha_{2}|^3)^\frac{3}{2} + |\alpha_{3}|^3]^\frac{1}{3}$ Proof that $||·||$ is a norm. To prove that it is a norm,  I have to proof 4 properties. I know how to prove 3 of them but I don´t know how to prove the triangle inequality, i.e: $||x+y||≤||x||+||y||$ If you can help me please. Thank you very much.",,"['real-analysis', 'analysis', 'normed-spaces']"
32,Continuity and uniform continuity of $1/x$,Continuity and uniform continuity of,1/x,"I was reviewing lecture notes from a term I just finished, and my lecturer leaves an example that illustrates the difference between continuity and uniform continuity but I'm not sure I understand it. We are told that on closed intervals uniform continuity = continuity and on open intervals uniform continuity is strong than continuity. i.e. Let $f : (0,1] \to [1,\infty) $ . $ f $ is continuous on $(0,1]$ but not uniformly continuous. Here is my proof, there are bits that are not correct and I'd like to clear up the confusion. So we know that f is continuous if $\exists c \in (0,1]$ s.t $\forall \epsilon > 0 \exists \delta>0$ s.t if $|x-c| < \delta \Rightarrow |f(x) - f(c)| < \epsilon $ . i.e. $|x-y| < \delta  \Rightarrow |1/x - 1/c| = |\frac{x-c}{xc}| < \epsilon$ $|\frac{x-c}{xc}| < |\frac{\delta}{xc}|$ but from here do I set epsilon to $|\frac{\delta}{xc}|$ ? But then epsilon depends on $x$ ? Anyway if I could clean this up then i would be able to show its continuous, but I cant quite manage the last step. After I do this, how do I show its not uniformly continuous?","I was reviewing lecture notes from a term I just finished, and my lecturer leaves an example that illustrates the difference between continuity and uniform continuity but I'm not sure I understand it. We are told that on closed intervals uniform continuity = continuity and on open intervals uniform continuity is strong than continuity. i.e. Let . is continuous on but not uniformly continuous. Here is my proof, there are bits that are not correct and I'd like to clear up the confusion. So we know that f is continuous if s.t s.t if . i.e. but from here do I set epsilon to ? But then epsilon depends on ? Anyway if I could clean this up then i would be able to show its continuous, but I cant quite manage the last step. After I do this, how do I show its not uniformly continuous?","f : (0,1] \to [1,\infty)   f  (0,1] \exists c \in (0,1] \forall \epsilon > 0 \exists \delta>0 |x-c| < \delta \Rightarrow |f(x) - f(c)| < \epsilon  |x-y| < \delta  \Rightarrow |1/x - 1/c| = |\frac{x-c}{xc}| < \epsilon |\frac{x-c}{xc}| < |\frac{\delta}{xc}| |\frac{\delta}{xc}| x","['real-analysis', 'analysis', 'continuity', 'uniform-continuity']"
33,Find the minimum value of the expression.,Find the minimum value of the expression.,,"Find the minimum value of the expression. $x,y,z \in R$ $\sqrt{x^2+1}+ \sqrt {4+(y-z)^2} + \sqrt{1+ (z-x)^2} + \sqrt{9+(10-y)^2}$","Find the minimum value of the expression. $x,y,z \in R$ $\sqrt{x^2+1}+ \sqrt {4+(y-z)^2} + \sqrt{1+ (z-x)^2} + \sqrt{9+(10-y)^2}$",,"['calculus', 'algebra-precalculus', 'analysis', 'polynomials']"
34,Question about topological spaces,Question about topological spaces,,"Let's mark the standard topological space on $\mathbb{R}$ with $\tau$. We'll define new topology on $\mathbb{R}$, $\tau_l$ with the following base: $B_l = \{[a,b)|a,b \in \mathbb{R},a<b\}$ I have to prove that a function $f:(\mathbb{R}_l,\tau_l)\to(\mathbb{R},\tau) $ is continuos iff $f$ is continuos from the right in the normal calculus meaning. So one direction is relatively easy (topological continuousness -> calculus continuousness) but the other one i'm having a hard time to prove.","Let's mark the standard topological space on $\mathbb{R}$ with $\tau$. We'll define new topology on $\mathbb{R}$, $\tau_l$ with the following base: $B_l = \{[a,b)|a,b \in \mathbb{R},a<b\}$ I have to prove that a function $f:(\mathbb{R}_l,\tau_l)\to(\mathbb{R},\tau) $ is continuos iff $f$ is continuos from the right in the normal calculus meaning. So one direction is relatively easy (topological continuousness -> calculus continuousness) but the other one i'm having a hard time to prove.",,"['general-topology', 'analysis']"
35,What are the benefits or losses of learning real analysis through a constructivist approach instead of a standard apporach?,What are the benefits or losses of learning real analysis through a constructivist approach instead of a standard apporach?,,Recently I've found some courses on real analysis that use the constructivist approach and I got curious on some aspects: What are the benefits of learning through this approach? Is it ok to learn through a constructivist approach instead of a standard approach? How different the teaching on these approaches would be? Here I'm presuming a student who haven't taken real analysis lectures. I'm presuming that constructivist approaches to real analysis are not a standard practice in analysis courses but I'm not sure about that.,Recently I've found some courses on real analysis that use the constructivist approach and I got curious on some aspects: What are the benefits of learning through this approach? Is it ok to learn through a constructivist approach instead of a standard approach? How different the teaching on these approaches would be? Here I'm presuming a student who haven't taken real analysis lectures. I'm presuming that constructivist approaches to real analysis are not a standard practice in analysis courses but I'm not sure about that.,,"['real-analysis', 'analysis', 'self-learning', 'education', 'constructive-mathematics']"
36,Strange mistake in this proof about normed separable vector spaces.,Strange mistake in this proof about normed separable vector spaces.,,"I am supposed to prove that an infinite-dimensionale separable normed space $X$ constains a countable subset $Y$ that is linearly independent and dense. There are three hints given:  First, prove that no subspace of X contains an open ball. Second, prove that $\overline{\{x_n;n \in \mathbb{N}\}} = X$ and $||x_n-y_n|| \le \frac{1}{n}$ for all n and therefore $\overline{\{y_n;n \in \mathbb{N} \} }=X$. Finally, construct the set inductively. My proof of the first hint: Assuming false, then some $B_{\epsilon}(x)\subset X' \subsetneq X$, where X' and X are both vector spaces. But in that case there is some vector $w \in X \backslash X'$ and therefore we have that $x + \frac{\epsilon}{2 |w|}w \in B_{\epsilon}(x)$, but since $x \in X'$ it would follow that since $B_{\epsilon}(x)\subset X'$ we have $w \in X'$ which is wrong. My second proof is: Assuming false, then $\exists x \in X \exists m \in \mathbb{N} \forall n \in \mathbb{N}: d(x,y_n) \ge \frac{1}{m} $ Since the $x_n$ are dense we can say that there are at least infinitely many $x_n$ such that $d(x_n,x)\le \frac{1}{2m}$. So we get that: $\forall n \in \mathbb{N}: d(x,x_n)+ d(x_n,y_n) \ge d(x,y_n) \ge \frac{1}{m}$, but this can't be true, since we know that for infinitely many $d(x,x_n)$ is small and for $n > m+1$ we have that $d(x_n,y_n) \leq \frac{1}{m+1}$. So contradiction. Now I wanted to construct the countable dense and linear independent set in the following way: Step 1 :  $ y_1:=x_1$ and therefore $Y=\{x_1\}$. This is of course linearly independent and we have $|x_1-y_1|=0$. Step n to n+1: $Y=\{y_1,...,y_n\}$ be constructed. Now if $x_{n+1} \in span(Y)$ then we choose some vector in $X$ that is not in $span(Y)$ and say $y_{n+1} = x_{n+1} + \frac{1}{n+2}\frac{w}{||w||}$. This is leaves Y linearly independet and $|y_{n+1}-x_{n+1}|\leq\frac{1}{n+1}$ If $x_{n+1} \not \in  span(Y)$, we set $y_{n+1} = x_{n+1}$. In that case Y is also linearly independent and $|y_{n+1}-x_{n+1}|=0\leq\frac{1}{n+1}$. In that case we get a set $Y$ that is linearly independent and countable and by the second hint that I proved it is also dense in $X$. But the problem is: I did not use hint 1, so my proof is probably wrong! So, where is my mistake?","I am supposed to prove that an infinite-dimensionale separable normed space $X$ constains a countable subset $Y$ that is linearly independent and dense. There are three hints given:  First, prove that no subspace of X contains an open ball. Second, prove that $\overline{\{x_n;n \in \mathbb{N}\}} = X$ and $||x_n-y_n|| \le \frac{1}{n}$ for all n and therefore $\overline{\{y_n;n \in \mathbb{N} \} }=X$. Finally, construct the set inductively. My proof of the first hint: Assuming false, then some $B_{\epsilon}(x)\subset X' \subsetneq X$, where X' and X are both vector spaces. But in that case there is some vector $w \in X \backslash X'$ and therefore we have that $x + \frac{\epsilon}{2 |w|}w \in B_{\epsilon}(x)$, but since $x \in X'$ it would follow that since $B_{\epsilon}(x)\subset X'$ we have $w \in X'$ which is wrong. My second proof is: Assuming false, then $\exists x \in X \exists m \in \mathbb{N} \forall n \in \mathbb{N}: d(x,y_n) \ge \frac{1}{m} $ Since the $x_n$ are dense we can say that there are at least infinitely many $x_n$ such that $d(x_n,x)\le \frac{1}{2m}$. So we get that: $\forall n \in \mathbb{N}: d(x,x_n)+ d(x_n,y_n) \ge d(x,y_n) \ge \frac{1}{m}$, but this can't be true, since we know that for infinitely many $d(x,x_n)$ is small and for $n > m+1$ we have that $d(x_n,y_n) \leq \frac{1}{m+1}$. So contradiction. Now I wanted to construct the countable dense and linear independent set in the following way: Step 1 :  $ y_1:=x_1$ and therefore $Y=\{x_1\}$. This is of course linearly independent and we have $|x_1-y_1|=0$. Step n to n+1: $Y=\{y_1,...,y_n\}$ be constructed. Now if $x_{n+1} \in span(Y)$ then we choose some vector in $X$ that is not in $span(Y)$ and say $y_{n+1} = x_{n+1} + \frac{1}{n+2}\frac{w}{||w||}$. This is leaves Y linearly independet and $|y_{n+1}-x_{n+1}|\leq\frac{1}{n+1}$ If $x_{n+1} \not \in  span(Y)$, we set $y_{n+1} = x_{n+1}$. In that case Y is also linearly independent and $|y_{n+1}-x_{n+1}|=0\leq\frac{1}{n+1}$. In that case we get a set $Y$ that is linearly independent and countable and by the second hint that I proved it is also dense in $X$. But the problem is: I did not use hint 1, so my proof is probably wrong! So, where is my mistake?",,"['real-analysis', 'analysis']"
37,"$Df(x)=0$ for a differentiable function $f$ from $A \subset R^n$ to $R^m$ with $A$ open and connected, then $f$ is constant","for a differentiable function  from  to  with  open and connected, then  is constant",Df(x)=0 f A \subset R^n R^m A f,"Let $A \subset R^n$ open and connected and let $f:A \to R^m$ a differentiable function. If $Df(x)=0$ for every $x \in A$, then f is constant in $A$. This is what I did up to now: Take $x_1, x_2 \in A$ such that the line segment that joins these two points is also in $A$. Define $\phi:[0,1] \to R^n$ as $\phi(t)=(1-t)x_1+x_2$. Then, $f(x_2)-f(x_1)= \int_0^1 \frac{d}{dt} f \circ \phi(t)dt=\int_0^1 D(\phi(t)).\phi'(t)dt$. But $D(\phi(t))=0$, which means $\int_0^1 D(\phi(t)).\phi'(t)dt=\int_0^1 0dt=0$, so $f(x_1)-f(x_2)=0 \implies f(x_1)=f(x_2)$. From here I don't know what else I can do. I mean, all that I know is that if I have $x_1,x_2 \in A$ such that $\overline {x_1,x_2}$ is in $A$, then the function takes the same value on $x_1$ and $x_2$. Here's what I could do with Eldredge's idea: Let $x_0 \in A$ and let $U$ be the set of all points in $A$ that can be joined to $x_0$ by a finite number of line segments all contained in $A$. Then, it's easy to see that $f$ is constant in $U$: take $u,z \in U$, we know there are $l_1,...,l_n$ line segments that join $x_0$ with $y$. The extreme points of $l_1$ are $x_0$ and $y_1$, and $l_1$ is a line segment contained in $A$, then $f(x_0)=f(y_1)$. By the same argument, $f(y_i)=f(y_{i+1})$ where $y_i$ and $y_{i+1}$ are the extreme points of the line segment $l_i$ for $2\leq i \leq n-1$ and with $y_n=y$. By induction one can prove $f(x_0)=f(y)$. Using the same argument, $f(x_0)=f(z) \implies f(y)=f(z)$. Now I want to show that $U$ is an open set. Let $x \in U$, as $A$ is open, there exists $\delta>0$ such that $B(x,\delta) \subset A$. If $y \in B(x,\delta)$, then the line segment $\phi$ which joins $x$ with $y$ is also in $A$. Then, the polygon obtained from the composition of $\phi$ with the polygon that joins $x_0$ with $x$ is also contained in $A$ and it joins $x_0$ with $y$, this means $y \in U$, so $B(x,\delta) \subset U \implies U$ is open. Let's prove that $U$ is a closed set in $A$. Take $u$ a limit point of $U$. There exists a sequence $\{u_n\}_{n \in \mathbb N}$ such that $u_n \to u$. As $A$ is open, there is $\epsilon>0$ such that $B(u,\epsilon) \subset A$ and as $u_n \to u$, there is $N \in \mathbb N$ such that for all $n: N \leq n$, $u_n \in B(u,\epsilon)$. We know there is a polygon $A$ which joins $x_0$ with $u_N$ that is completely contained in $A$ and the segment that joins $u$ with $u_N$ is also in $A$, then $u \in U$. So, we have $U \subset A$ which is open and closed, as A is connected, $U=\emptyset$ or $U=A$, but $x_0 \in U \implies U=A$.","Let $A \subset R^n$ open and connected and let $f:A \to R^m$ a differentiable function. If $Df(x)=0$ for every $x \in A$, then f is constant in $A$. This is what I did up to now: Take $x_1, x_2 \in A$ such that the line segment that joins these two points is also in $A$. Define $\phi:[0,1] \to R^n$ as $\phi(t)=(1-t)x_1+x_2$. Then, $f(x_2)-f(x_1)= \int_0^1 \frac{d}{dt} f \circ \phi(t)dt=\int_0^1 D(\phi(t)).\phi'(t)dt$. But $D(\phi(t))=0$, which means $\int_0^1 D(\phi(t)).\phi'(t)dt=\int_0^1 0dt=0$, so $f(x_1)-f(x_2)=0 \implies f(x_1)=f(x_2)$. From here I don't know what else I can do. I mean, all that I know is that if I have $x_1,x_2 \in A$ such that $\overline {x_1,x_2}$ is in $A$, then the function takes the same value on $x_1$ and $x_2$. Here's what I could do with Eldredge's idea: Let $x_0 \in A$ and let $U$ be the set of all points in $A$ that can be joined to $x_0$ by a finite number of line segments all contained in $A$. Then, it's easy to see that $f$ is constant in $U$: take $u,z \in U$, we know there are $l_1,...,l_n$ line segments that join $x_0$ with $y$. The extreme points of $l_1$ are $x_0$ and $y_1$, and $l_1$ is a line segment contained in $A$, then $f(x_0)=f(y_1)$. By the same argument, $f(y_i)=f(y_{i+1})$ where $y_i$ and $y_{i+1}$ are the extreme points of the line segment $l_i$ for $2\leq i \leq n-1$ and with $y_n=y$. By induction one can prove $f(x_0)=f(y)$. Using the same argument, $f(x_0)=f(z) \implies f(y)=f(z)$. Now I want to show that $U$ is an open set. Let $x \in U$, as $A$ is open, there exists $\delta>0$ such that $B(x,\delta) \subset A$. If $y \in B(x,\delta)$, then the line segment $\phi$ which joins $x$ with $y$ is also in $A$. Then, the polygon obtained from the composition of $\phi$ with the polygon that joins $x_0$ with $x$ is also contained in $A$ and it joins $x_0$ with $y$, this means $y \in U$, so $B(x,\delta) \subset U \implies U$ is open. Let's prove that $U$ is a closed set in $A$. Take $u$ a limit point of $U$. There exists a sequence $\{u_n\}_{n \in \mathbb N}$ such that $u_n \to u$. As $A$ is open, there is $\epsilon>0$ such that $B(u,\epsilon) \subset A$ and as $u_n \to u$, there is $N \in \mathbb N$ such that for all $n: N \leq n$, $u_n \in B(u,\epsilon)$. We know there is a polygon $A$ which joins $x_0$ with $u_N$ that is completely contained in $A$ and the segment that joins $u$ with $u_N$ is also in $A$, then $u \in U$. So, we have $U \subset A$ which is open and closed, as A is connected, $U=\emptyset$ or $U=A$, but $x_0 \in U \implies U=A$.",,"['analysis', 'derivatives']"
38,On the modes of convergence of the series $\sum\limits_{k=1}^{\infty} \frac{1}{k} \sin \left(\frac{x}{k+1} \right)$,On the modes of convergence of the series,\sum\limits_{k=1}^{\infty} \frac{1}{k} \sin \left(\frac{x}{k+1} \right),"Show that $$f(x) = \sum_{k=1}^{\infty} \dfrac{1}{k} \sin \left(\dfrac{x}{k+1}\right).$$ converges pointwise on $\mathbb{R}$ and uniformly on each bounded interval in $\mathbb{R}$ , to a differentiable function $f$ which satisifies $\vert f(x) \vert \leq \vert x \vert$ and $\vert f'(x) \vert \leq 1$ . This problem comes from a section that covers the Weierstrass M-Test and Dirichlet's Test. I am not sure where to begin with this. Analysis really isn't my thing at this point.","Show that converges pointwise on and uniformly on each bounded interval in , to a differentiable function which satisifies and . This problem comes from a section that covers the Weierstrass M-Test and Dirichlet's Test. I am not sure where to begin with this. Analysis really isn't my thing at this point.",f(x) = \sum_{k=1}^{\infty} \dfrac{1}{k} \sin \left(\dfrac{x}{k+1}\right). \mathbb{R} \mathbb{R} f \vert f(x) \vert \leq \vert x \vert \vert f'(x) \vert \leq 1,"['real-analysis', 'sequences-and-series', 'analysis', 'uniform-convergence', 'pointwise-convergence']"
39,"Proof of $|x^{\alpha} - y^{\alpha}| \le \alpha^{\alpha} |x-y|$ for $\alpha \ge 1, x,y\in [0,1]$",Proof of  for,"|x^{\alpha} - y^{\alpha}| \le \alpha^{\alpha} |x-y| \alpha \ge 1, x,y\in [0,1]","I want to prove $$  |x^{\alpha} - y^{\alpha}| \le \alpha^{\alpha} |x-y| $$ for $\alpha \ge 1$ and $x,y \in [0,1]$. For $\alpha \in \mathbb N$ I already got the proof by using the formulae $$  (x^n - y^n) = (x-y)(x^{n-1} + x^{n-2}y + \ldots + xy^{n-2} + y^{n-1}) $$ but I have no idea how to generalize to arbitrary $\alpha \ge 0$, any hints?","I want to prove $$  |x^{\alpha} - y^{\alpha}| \le \alpha^{\alpha} |x-y| $$ for $\alpha \ge 1$ and $x,y \in [0,1]$. For $\alpha \in \mathbb N$ I already got the proof by using the formulae $$  (x^n - y^n) = (x-y)(x^{n-1} + x^{n-2}y + \ldots + xy^{n-2} + y^{n-1}) $$ but I have no idea how to generalize to arbitrary $\alpha \ge 0$, any hints?",,"['analysis', 'inequality']"
40,Bochner integration and the associated notion of measurability,Bochner integration and the associated notion of measurability,,"In http://en.wikipedia.org/wiki/Bochner_integral a notion of measurability is discussed that depends on the measure $\mu$.  Usually measurability does not depend on having a measure anyway.  Is this really the right definition?  If so, how is it related to the more natural notion of measurability which would be the Borel $\sigma$ algebra from the norm topology of the Banach space? Since this notion of measurability must be not equivalent to the standard one if the Banach space is $\mathbb{C}$ or $\mathbb{R}$, what kind of theorems can we expect about the Bochner integral still being in some sense an extension of the Lebesgue?","In http://en.wikipedia.org/wiki/Bochner_integral a notion of measurability is discussed that depends on the measure $\mu$.  Usually measurability does not depend on having a measure anyway.  Is this really the right definition?  If so, how is it related to the more natural notion of measurability which would be the Borel $\sigma$ algebra from the norm topology of the Banach space? Since this notion of measurability must be not equivalent to the standard one if the Banach space is $\mathbb{C}$ or $\mathbb{R}$, what kind of theorems can we expect about the Bochner integral still being in some sense an extension of the Lebesgue?",,"['analysis', 'functional-analysis', 'lebesgue-integral']"
41,About the continuity of a convolution product,About the continuity of a convolution product,,"I need some help with this exercise: If $f\in L_p(\mathbb{R}^n)$ and $g\in L_q(\mathbb{R}^n)$ , where $\frac{1}{p}+\frac{1}{q}=1$ , Is the convolution $f\ast g(x)=\displaystyle\int_{\mathbb{R}^n}f(x-y)g(y)\;dy$ a continuous function? Thank you very much in advance.","I need some help with this exercise: If and , where , Is the convolution a continuous function? Thank you very much in advance.",f\in L_p(\mathbb{R}^n) g\in L_q(\mathbb{R}^n) \frac{1}{p}+\frac{1}{q}=1 f\ast g(x)=\displaystyle\int_{\mathbb{R}^n}f(x-y)g(y)\;dy,"['real-analysis', 'analysis', 'continuity', 'convolution']"
42,partial derivatives continuous $\implies$ differentiability in Euclidean space,partial derivatives continuous  differentiability in Euclidean space,\implies,"I am given this theorem: If $f \in C^1(A,\mathbb R^m)$, i.e. every partial derivative of $f$ is   continuous on $A$, and $A$ is open in $\mathbb R^n$, then $f$ is   differentiable on $A$. Is the following stronger assertion also true? If every partial derivative of $f:A\to\mathbb R^m$ is   continuous at $c\in A$, and $A$ is open in $\mathbb R^n$, then $f$ is   differentiable at $c$. Also, is the requirement that $A$ be an open set really necessary in the above statements? EDIT: After doing a little more research, here is the strongest version of the theorem I've come up with (there is an even stronger version, but it isn't quite as neat or succinct): If every partial derivative of $f:A\subset\mathbb R^n\to\mathbb R^m$ exists, and is continuous, at $c\in $int$(A)$, then $f$ is   differentiable at $c$.","I am given this theorem: If $f \in C^1(A,\mathbb R^m)$, i.e. every partial derivative of $f$ is   continuous on $A$, and $A$ is open in $\mathbb R^n$, then $f$ is   differentiable on $A$. Is the following stronger assertion also true? If every partial derivative of $f:A\to\mathbb R^m$ is   continuous at $c\in A$, and $A$ is open in $\mathbb R^n$, then $f$ is   differentiable at $c$. Also, is the requirement that $A$ be an open set really necessary in the above statements? EDIT: After doing a little more research, here is the strongest version of the theorem I've come up with (there is an even stronger version, but it isn't quite as neat or succinct): If every partial derivative of $f:A\subset\mathbb R^n\to\mathbb R^m$ exists, and is continuous, at $c\in $int$(A)$, then $f$ is   differentiable at $c$.",,"['real-analysis', 'analysis', 'multivariable-calculus', 'differential-geometry', 'normed-spaces']"
43,Proving $m\lambda(E_m)\le\sum_{k=1}^{\infty}\lambda(A_k)$,Proving,m\lambda(E_m)\le\sum_{k=1}^{\infty}\lambda(A_k),"Assume $A_1,A_2,...$ are measurable sets. Let $m\in\mathbb{N}$, and let $E_m$ be the set defined as follows: $x\in E_m\iff$ $x$ is a member of at least $m$ of the sets $A_k$. Prove that $E_m$ is measurable and that $$m\lambda(E_m)\le\sum_{k=1}^{\infty}\lambda(A_k)$$ Here everything is in Lebesgue measure. ($\lambda(E_m)$ denotes the Lebesgue measure of $E_m$) My thoughts: I think the inequality is fairly intuitive since $E_m$ contains the $x$'s that appear at least $m$ times in $A_1,A_2,...$, and so $m$ is a lower bound. But I'm not sure how to prove rigorously. To show $E_m$ is measurable, let $S$ be the collection of all subsets of $\mathbb{N}$ with at least $m$ elements. I think we can represent $E_m$ as $\cup_{T\in S}\cap_{i\in T}A_i$. Please correct me if I'm wrong. Thank you.","Assume $A_1,A_2,...$ are measurable sets. Let $m\in\mathbb{N}$, and let $E_m$ be the set defined as follows: $x\in E_m\iff$ $x$ is a member of at least $m$ of the sets $A_k$. Prove that $E_m$ is measurable and that $$m\lambda(E_m)\le\sum_{k=1}^{\infty}\lambda(A_k)$$ Here everything is in Lebesgue measure. ($\lambda(E_m)$ denotes the Lebesgue measure of $E_m$) My thoughts: I think the inequality is fairly intuitive since $E_m$ contains the $x$'s that appear at least $m$ times in $A_1,A_2,...$, and so $m$ is a lower bound. But I'm not sure how to prove rigorously. To show $E_m$ is measurable, let $S$ be the collection of all subsets of $\mathbb{N}$ with at least $m$ elements. I think we can represent $E_m$ as $\cup_{T\in S}\cap_{i\in T}A_i$. Please correct me if I'm wrong. Thank you.",,"['real-analysis', 'analysis', 'measure-theory', 'elementary-set-theory', 'lebesgue-integral']"
44,How to check whether a given inequality is correct for a large span of integers?,How to check whether a given inequality is correct for a large span of integers?,,The inequality $\sqrt{n+ 1}−\sqrt n < \frac{1}{\sqrt n}$ is false for all n such that $101 ≤ n ≤ 2000$. Is the statement true?,The inequality $\sqrt{n+ 1}−\sqrt n < \frac{1}{\sqrt n}$ is false for all n such that $101 ≤ n ≤ 2000$. Is the statement true?,,"['calculus', 'sequences-and-series', 'analysis', 'inequality']"
45,Clarification on this corollary of the Arzela-Ascoli Theorem,Clarification on this corollary of the Arzela-Ascoli Theorem,,"I am given the following corollary without proof: A family of continuous functions on a compact metric space into   $\mathbb R^m$ is compact iff it is closed, equicontinuous and bounded. Does the last word above mean pointwise bounded or uniformly bounded? (I'm pretty sure equicontinuous above means uniformly equicontinuous.)","I am given the following corollary without proof: A family of continuous functions on a compact metric space into   $\mathbb R^m$ is compact iff it is closed, equicontinuous and bounded. Does the last word above mean pointwise bounded or uniformly bounded? (I'm pretty sure equicontinuous above means uniformly equicontinuous.)",,"['real-analysis', 'general-topology', 'analysis', 'metric-spaces', 'compactness']"
46,"$F = \{f\in C^1([0,1])| \hspace{2mm} \|f\|\leq M, \|f'\|\leq N\}$. Showing it is precompact and not closed.",. Showing it is precompact and not closed.,"F = \{f\in C^1([0,1])| \hspace{2mm} \|f\|\leq M, \|f'\|\leq N\}","I have an example in my book: Let $C([0,1])$ denote the space of all continuous functions $f$ on $[0.1]$ with continuous derivative $f'$. For constants $M>0$ and $N>0$, we define the subset $F$ of $C([0,1])$ by $$F = \{f\in C^1([0,1])\mid \|f\|\leq M, \|f'\|\leq N\}.$$ Where $\|\cdot\|$ denotes the $\sup$-norm.The book claims that $F$ is precompact in $C(K)$ and it is not closed because the uniform limit of continuously differentiable functions need not be differentiable. My questions are: 1) How can I prove this set is precompact? I know I have to show it is bounded and equicontinuous. 2) Is there an example of such a function that can show this is not closed? This is not a homework exercise. I am trying to fill in the blanks that my text book has not filled in so I can have a better understanding. Thank you in advance for any help, advice and comments.","I have an example in my book: Let $C([0,1])$ denote the space of all continuous functions $f$ on $[0.1]$ with continuous derivative $f'$. For constants $M>0$ and $N>0$, we define the subset $F$ of $C([0,1])$ by $$F = \{f\in C^1([0,1])\mid \|f\|\leq M, \|f'\|\leq N\}.$$ Where $\|\cdot\|$ denotes the $\sup$-norm.The book claims that $F$ is precompact in $C(K)$ and it is not closed because the uniform limit of continuously differentiable functions need not be differentiable. My questions are: 1) How can I prove this set is precompact? I know I have to show it is bounded and equicontinuous. 2) Is there an example of such a function that can show this is not closed? This is not a homework exercise. I am trying to fill in the blanks that my text book has not filled in so I can have a better understanding. Thank you in advance for any help, advice and comments.",,"['analysis', 'functional-analysis', 'continuity', 'self-learning']"
47,Inequality remain true if we pass to the limit?,Inequality remain true if we pass to the limit?,,Say we have $$X \geq \sum_{k=1}^{n} F(k) $$ Does it follow that the inequality remain true if we pass to the limit $$X \geq \sum_{k=1}^{\infty} F(k) $$ Given that the $F$'s are all non negative?,Say we have $$X \geq \sum_{k=1}^{n} F(k) $$ Does it follow that the inequality remain true if we pass to the limit $$X \geq \sum_{k=1}^{\infty} F(k) $$ Given that the $F$'s are all non negative?,,"['calculus', 'analysis']"
48,A non-exponentially bounded analytic function?,A non-exponentially bounded analytic function?,,"A function $f:\mathbb R\to\mathbb R$ is said to be exponentially bounded if there is an $n$ such that for sufficiently large $x\in\mathbb R$, $\exp(\exp(\cdots \exp(x)))>f(x)$ (where the $\exp$ is repeated $n$ times). You know what analytic means.  Is there a ""classical"" (or easy to describe) non-exponentially bounded function? I ask because this is related to an open question in model theory, namely whether the real field, expanded with a non-exponentially-bounded function, can be o-minimal.  Most of what we know about o-minimal expansions are related to analytic functions, so I'm interested in what we could be looking for.","A function $f:\mathbb R\to\mathbb R$ is said to be exponentially bounded if there is an $n$ such that for sufficiently large $x\in\mathbb R$, $\exp(\exp(\cdots \exp(x)))>f(x)$ (where the $\exp$ is repeated $n$ times). You know what analytic means.  Is there a ""classical"" (or easy to describe) non-exponentially bounded function? I ask because this is related to an open question in model theory, namely whether the real field, expanded with a non-exponentially-bounded function, can be o-minimal.  Most of what we know about o-minimal expansions are related to analytic functions, so I'm interested in what we could be looking for.",,"['analysis', 'exponentiation', 'examples-counterexamples', 'analyticity']"
49,Inequality involving partial derivatives,Inequality involving partial derivatives,,"Suppose $f(x, y)$ is a twice continuously differentiable function with a unique minimum at $(0, 0)$. Why at $(0, 0)$ must we have $$\frac{\partial^{2}f}{\partial x^{2}}\frac{\partial^{2}f}{\partial y^{2}} \geq \left(\frac{\partial^{2}f}{\partial x \partial y}\right)^{2}?$$","Suppose $f(x, y)$ is a twice continuously differentiable function with a unique minimum at $(0, 0)$. Why at $(0, 0)$ must we have $$\frac{\partial^{2}f}{\partial x^{2}}\frac{\partial^{2}f}{\partial y^{2}} \geq \left(\frac{\partial^{2}f}{\partial x \partial y}\right)^{2}?$$",,"['real-analysis', 'analysis', 'multivariable-calculus']"
50,Interesting question in analysis,Interesting question in analysis,,"I am trying to prove this : Consider $\Omega \subset R^n$ ( $n \geq 2$) a bounded and open set and $u $ a smooth function defined in $\overline{\Omega}$. Suppose that $u(y) = 0$ for $y \in \partial \Omega$ and suppose that exists a $\alpha >0$ such that $|\nabla u (x)| = \sqrt{\displaystyle\sum_{i=1}^{n} (\frac{\partial u}{ \partial x_i }(x)} )^2\geq \alpha >0$ for all $x \in \Omega$, then $$ |u(x)| \geq \alpha |x-y|$$ for all $x \in \Omega$ and for all $y \in \partial \Omega$. drawing a picture is easy to see the affirmation..  i am trying to prove this. but nothing ... My professor said that this is true.... Someone can give me a hint ?","I am trying to prove this : Consider $\Omega \subset R^n$ ( $n \geq 2$) a bounded and open set and $u $ a smooth function defined in $\overline{\Omega}$. Suppose that $u(y) = 0$ for $y \in \partial \Omega$ and suppose that exists a $\alpha >0$ such that $|\nabla u (x)| = \sqrt{\displaystyle\sum_{i=1}^{n} (\frac{\partial u}{ \partial x_i }(x)} )^2\geq \alpha >0$ for all $x \in \Omega$, then $$ |u(x)| \geq \alpha |x-y|$$ for all $x \in \Omega$ and for all $y \in \partial \Omega$. drawing a picture is easy to see the affirmation..  i am trying to prove this. but nothing ... My professor said that this is true.... Someone can give me a hint ?",,"['real-analysis', 'analysis', 'multivariable-calculus']"
51,Differentiability at a point when you actually want in a neighborhood,Differentiability at a point when you actually want in a neighborhood,,"Let $f$ be a real-valued function on an open interval $I$ containing $c$. If $f$ is differentiable at $c$, and $(x_{n})$ and $(y_{n})$ are sequences in $I$ such that $x_{n}<c<y_{n}$ and $\lim_{n\rightarrow\infty}(y_{n}-x_{n})=0$, then prove that $ \displaystyle\lim_{n\rightarrow\infty} \dfrac{f(y_{n})-f(x_{n})}{y_{n}-x_{n}}=f'(c).$ If we were given that $f$ was differentiable in a neighborhood about $c$ the problem is trivial by the mean value theorem. So without that I don't know how to proceed with this problem. And I don't think there is any way to show that $f$ is analytic at $c$.","Let $f$ be a real-valued function on an open interval $I$ containing $c$. If $f$ is differentiable at $c$, and $(x_{n})$ and $(y_{n})$ are sequences in $I$ such that $x_{n}<c<y_{n}$ and $\lim_{n\rightarrow\infty}(y_{n}-x_{n})=0$, then prove that $ \displaystyle\lim_{n\rightarrow\infty} \dfrac{f(y_{n})-f(x_{n})}{y_{n}-x_{n}}=f'(c).$ If we were given that $f$ was differentiable in a neighborhood about $c$ the problem is trivial by the mean value theorem. So without that I don't know how to proceed with this problem. And I don't think there is any way to show that $f$ is analytic at $c$.",,"['real-analysis', 'analysis']"
52,Convergence in $L^P(E)$ implying convergence in measure requires $m(E) < \infty$?,Convergence in  implying convergence in measure requires ?,L^P(E) m(E) < \infty,"It's a common analysis problem to show that if $m(E) < \infty$ to show that convergence in $L^P(E)$ implies convergence in measure. However, I don't see where the necessity of $m(E) < \infty$ comes up in the proof. The proofs I have seen all look like: Let $A_n = \{x \in E: |f_n(x) - f(x)| > \epsilon \}$. Then, $\int_E |f_n(x) - f(x)|^p \ge \int_{A_n} |f_n(x) - f(x)|^p \ge \int_{A_n}  \epsilon^p = \epsilon^p m(A_n) \ge 0$ Thus $||f_n - f||_p \rightarrow 0$ implies that lim $m(A_n) = 0$ and hence it converges in measure. Where is the finite measure of $E$ required? EDIT* Based on the one answer so far, and the comment, I suspect that this was an incorrect question. It came up from a Qualifying exam I was going over. Which would make me want to believe that it is not mistake, but it looks like it must have been a typo of some sort (that would suck to get an impossible question on your exam...). So, the ""it's common"" was just me making that up. The original question was to prove this for a finite E, and I guess there are just so many similar type questions that specify a finite E I assumed I had always seen it like that for this particular problem. Should I delete this question?","It's a common analysis problem to show that if $m(E) < \infty$ to show that convergence in $L^P(E)$ implies convergence in measure. However, I don't see where the necessity of $m(E) < \infty$ comes up in the proof. The proofs I have seen all look like: Let $A_n = \{x \in E: |f_n(x) - f(x)| > \epsilon \}$. Then, $\int_E |f_n(x) - f(x)|^p \ge \int_{A_n} |f_n(x) - f(x)|^p \ge \int_{A_n}  \epsilon^p = \epsilon^p m(A_n) \ge 0$ Thus $||f_n - f||_p \rightarrow 0$ implies that lim $m(A_n) = 0$ and hence it converges in measure. Where is the finite measure of $E$ required? EDIT* Based on the one answer so far, and the comment, I suspect that this was an incorrect question. It came up from a Qualifying exam I was going over. Which would make me want to believe that it is not mistake, but it looks like it must have been a typo of some sort (that would suck to get an impossible question on your exam...). So, the ""it's common"" was just me making that up. The original question was to prove this for a finite E, and I guess there are just so many similar type questions that specify a finite E I assumed I had always seen it like that for this particular problem. Should I delete this question?",,"['analysis', 'functional-analysis']"
53,How can every $p$-adic integer be the limit of a sequence of non-negative integers?,How can every -adic integer be the limit of a sequence of non-negative integers?,p,"See Andrew Baker's P-adic Notes .  Every element of $\mathbb{Z}_p = \{a \in \mathbb{Q}_p : |a|_p \leq 1 \}$ is a limit of a sequence of non-negative integers, with respect to the $|\cdot|_p$ norm.  How is this possible?","See Andrew Baker's P-adic Notes .  Every element of $\mathbb{Z}_p = \{a \in \mathbb{Q}_p : |a|_p \leq 1 \}$ is a limit of a sequence of non-negative integers, with respect to the $|\cdot|_p$ norm.  How is this possible?",,"['abstract-algebra', 'analysis', 'algebraic-number-theory', 'p-adic-number-theory']"
54,Evulating $\int_I f$ by using Darboux Sum convergence Criterion,Evulating  by using Darboux Sum convergence Criterion,\int_I f,"I tried to solve the question. But, there may be some mistakes. I want to learn this properly. If there exist any notation mistake, typo, a general mistake in solution way or else, please correct this. Thank you:)","I tried to solve the question. But, there may be some mistakes. I want to learn this properly. If there exist any notation mistake, typo, a general mistake in solution way or else, please correct this. Thank you:)",,"['calculus', 'real-analysis', 'analysis', 'integration']"
55,"Prove that f is constant on $K$ that is, if $a \in K$ then $f(x)=f(a) \ \ \forall x\in K$","Prove that f is constant on  that is, if  then",K a \in K f(x)=f(a) \ \ \forall x\in K,"Suppose that $f: \Bbb R^n \to \Bbb R^m$ and that $a\in K$, where $K$ is a compact connected subset of $\Bbb R^n$  suppose for each $x\in$ $K$, $\exists$ $\delta_x >0$ such that $f(x)=f(y)$ $\forall$ $y\in$ $B_{\delta_x}(x)$ Prove that f is constant on $K$ that is, if $a \in K$ then $f(x)=f(a) \ \ \forall x\in K$","Suppose that $f: \Bbb R^n \to \Bbb R^m$ and that $a\in K$, where $K$ is a compact connected subset of $\Bbb R^n$  suppose for each $x\in$ $K$, $\exists$ $\delta_x >0$ such that $f(x)=f(y)$ $\forall$ $y\in$ $B_{\delta_x}(x)$ Prove that f is constant on $K$ that is, if $a \in K$ then $f(x)=f(a) \ \ \forall x\in K$",,"['calculus', 'real-analysis', 'general-topology', 'analysis']"
56,First order partial derivatives,First order partial derivatives,,"Suppose that $f:\Bbb R^2\to \Bbb R^2$ has $C^1$ partial derivatives in some ball $B_r(x_0,y_0)$ $r>0$. Prove that if $\Delta_f(x_0,y_0)\neq 0$, then $\displaystyle\frac{\partial f_1^{-1}}{\partial x}(f(x_0,y_0))= \displaystyle\frac{\partial f_2/\partial y(x_0,y_0)}{\Delta_f(x_0,y_0)}$ $\displaystyle\frac{\partial f_1^{-1}}{\partial y}(f(x_0,y_0))= \displaystyle\frac{\partial f_1/\partial y(x_0,y_0)}{\Delta_f(x_0,y_0)}$ $\displaystyle\frac{\partial f_2^{-1}}{\partial x}(f(x_0,y_0))= \displaystyle\frac{\partial f_2/\partial x(x_0,y_0)}{\Delta_f(x_0,y_0)}$ $\displaystyle\frac{\partial f_2^{-1}}{\partial xy}(f(x_0,y_0))= \displaystyle\frac{\partial f_1/\partial x(x_0,y_0)}{\Delta_f(x_0,y_0)}$ Please can someone show me only one equation? And then I Will beraber to solve second equation by myself. Thank you","Suppose that $f:\Bbb R^2\to \Bbb R^2$ has $C^1$ partial derivatives in some ball $B_r(x_0,y_0)$ $r>0$. Prove that if $\Delta_f(x_0,y_0)\neq 0$, then $\displaystyle\frac{\partial f_1^{-1}}{\partial x}(f(x_0,y_0))= \displaystyle\frac{\partial f_2/\partial y(x_0,y_0)}{\Delta_f(x_0,y_0)}$ $\displaystyle\frac{\partial f_1^{-1}}{\partial y}(f(x_0,y_0))= \displaystyle\frac{\partial f_1/\partial y(x_0,y_0)}{\Delta_f(x_0,y_0)}$ $\displaystyle\frac{\partial f_2^{-1}}{\partial x}(f(x_0,y_0))= \displaystyle\frac{\partial f_2/\partial x(x_0,y_0)}{\Delta_f(x_0,y_0)}$ $\displaystyle\frac{\partial f_2^{-1}}{\partial xy}(f(x_0,y_0))= \displaystyle\frac{\partial f_1/\partial x(x_0,y_0)}{\Delta_f(x_0,y_0)}$ Please can someone show me only one equation? And then I Will beraber to solve second equation by myself. Thank you",,"['calculus', 'real-analysis', 'analysis', 'derivatives']"
57,Two Definitions of Minkowski Dimension,Two Definitions of Minkowski Dimension,,"I'm currently reading a paper. Let $F\subset\mathbb R^n$ and $\epsilon\gt0$, the paper defined $m^s(F):=\liminf_{\epsilon \to 0}\epsilon^{s-n}\lambda(F_\epsilon)$ and $M^s(F):=\limsup_{\epsilon \to 0}\epsilon^{s-n}\lambda(F_\epsilon)$, where $\lambda$ is Lebesgue measure on $\mathbb R^n$ and $F_\epsilon$ is $\epsilon$-parallel set of $F$, i.e. $F_\epsilon:=\{x:d(x,F)\le\epsilon\}$. The author called $$\underline{D}(F):=\inf\{t\ge0:m^t(F)=0\}=\sup\{t\ge0:m^t(F)=\infty\},$$ $$\overline{D}(F):=\inf\{t\ge0:M^t(F)=0\}=\sup\{t\ge0:M^t(F)=\infty\}$$ as lower and upper Minkowski dimension of $F$ respectively. If $\underline{D}(F)=\overline{D}(F)$ then the common value $D=D(F)$ is called the Minkowski dimension of $F$. In the other hand, all textbooks i know defined lower and upper Minkowski dimension (or box-counting dimension) respectively as $$\underline{B}(F):=\liminf_{\delta \to 0}\frac{\ln N_\delta(F)}{-\ln\delta}$$ and $$\overline{B}(F):=\limsup_{\delta \to 0}\frac{\ln N_\delta(F)}{-\ln\delta}$$ where $N_\delta(F)$ is the smallest number of sets of diameter at most $\delta$ which can cover $F$. Likewise, if $\underline{B}(F)=\overline{B}(F)$ then the common value $B=B(F)$ is called the Minkowski dimension (or box-counting dimension) of $F$. My questions: Are these definitions equal or their names just coincide? If $D=B$, where can i find the proof? Thank you.","I'm currently reading a paper. Let $F\subset\mathbb R^n$ and $\epsilon\gt0$, the paper defined $m^s(F):=\liminf_{\epsilon \to 0}\epsilon^{s-n}\lambda(F_\epsilon)$ and $M^s(F):=\limsup_{\epsilon \to 0}\epsilon^{s-n}\lambda(F_\epsilon)$, where $\lambda$ is Lebesgue measure on $\mathbb R^n$ and $F_\epsilon$ is $\epsilon$-parallel set of $F$, i.e. $F_\epsilon:=\{x:d(x,F)\le\epsilon\}$. The author called $$\underline{D}(F):=\inf\{t\ge0:m^t(F)=0\}=\sup\{t\ge0:m^t(F)=\infty\},$$ $$\overline{D}(F):=\inf\{t\ge0:M^t(F)=0\}=\sup\{t\ge0:M^t(F)=\infty\}$$ as lower and upper Minkowski dimension of $F$ respectively. If $\underline{D}(F)=\overline{D}(F)$ then the common value $D=D(F)$ is called the Minkowski dimension of $F$. In the other hand, all textbooks i know defined lower and upper Minkowski dimension (or box-counting dimension) respectively as $$\underline{B}(F):=\liminf_{\delta \to 0}\frac{\ln N_\delta(F)}{-\ln\delta}$$ and $$\overline{B}(F):=\limsup_{\delta \to 0}\frac{\ln N_\delta(F)}{-\ln\delta}$$ where $N_\delta(F)$ is the smallest number of sets of diameter at most $\delta$ which can cover $F$. Likewise, if $\underline{B}(F)=\overline{B}(F)$ then the common value $B=B(F)$ is called the Minkowski dimension (or box-counting dimension) of $F$. My questions: Are these definitions equal or their names just coincide? If $D=B$, where can i find the proof? Thank you.",,"['analysis', 'measure-theory', 'geometric-measure-theory', 'dimension-theory-analysis']"
58,Differentiate a hypergeometric function expression,Differentiate a hypergeometric function expression,,"I have the following function $$f_\epsilon (p)=\frac{1}{2}(1-p)^\epsilon 2^\epsilon {_2}F_1(1-\epsilon,\epsilon;1+\epsilon;\frac{1-p}{2}),\qquad p\in(-1,1).$$ Here $F$ is the hypergeometric function ${_2}F_1(a,b;c;z)$, see e.g. here for a definition: http://en.wikipedia.org/wiki/Hypergeometric_function I need the first order Taylor expansion in $\epsilon$ given by $$f_0(p)+ \left[\frac{\partial f_\epsilon}{\partial \epsilon}(p)\right]_{\epsilon=0}\cdot\epsilon.$$ The zeroth order term $f_0(p)$ should be doable, but the first order term seems to be really hard to get. The problem is to carry out the differentiation w.r.t. $\epsilon$ in the Arguments of ${_2}F_1$. I would need to compute $$\frac{\partial }{\partial \epsilon}{_2}F_1(1-\epsilon,\epsilon;1+\epsilon;z).$$ Any ideas how this could be possible?","I have the following function $$f_\epsilon (p)=\frac{1}{2}(1-p)^\epsilon 2^\epsilon {_2}F_1(1-\epsilon,\epsilon;1+\epsilon;\frac{1-p}{2}),\qquad p\in(-1,1).$$ Here $F$ is the hypergeometric function ${_2}F_1(a,b;c;z)$, see e.g. here for a definition: http://en.wikipedia.org/wiki/Hypergeometric_function I need the first order Taylor expansion in $\epsilon$ given by $$f_0(p)+ \left[\frac{\partial f_\epsilon}{\partial \epsilon}(p)\right]_{\epsilon=0}\cdot\epsilon.$$ The zeroth order term $f_0(p)$ should be doable, but the first order term seems to be really hard to get. The problem is to carry out the differentiation w.r.t. $\epsilon$ in the Arguments of ${_2}F_1$. I would need to compute $$\frac{\partial }{\partial \epsilon}{_2}F_1(1-\epsilon,\epsilon;1+\epsilon;z).$$ Any ideas how this could be possible?",,"['analysis', 'integration', 'taylor-expansion', 'hypergeometric-function']"
59,exponential matrix,exponential matrix,,"Hi i am trying to understand the exponential matrix: When is exponential matrix function $e^{At}$ integrable where A is an $n \times n$ matrix and $t$ is an $n$-dimensional vector? By integrable i mean indefinite integral over $(0, \infty)$ Do we have to say something about the norm of the matrix? Can we say the following if integral of $e^{At}$ is infinity $e^{-At}$ is integrable? Could you give an example of a matrix $A$ where the integral $e^{At}$ does not converge to  any real number or plus or minus infinity? Could we say when $e^{At}$ is integrable then $e^{At} \dot e^{At}$ is integrable as well i.e. the dot product of these two same exponential matrices? What is the explicit representation for $e^{At} \dot e^{At}$? Similarly how is the derivative of this object defined why is the derivative equal to $Ae^{At}$ Thanks a lot for your answer!","Hi i am trying to understand the exponential matrix: When is exponential matrix function $e^{At}$ integrable where A is an $n \times n$ matrix and $t$ is an $n$-dimensional vector? By integrable i mean indefinite integral over $(0, \infty)$ Do we have to say something about the norm of the matrix? Can we say the following if integral of $e^{At}$ is infinity $e^{-At}$ is integrable? Could you give an example of a matrix $A$ where the integral $e^{At}$ does not converge to  any real number or plus or minus infinity? Could we say when $e^{At}$ is integrable then $e^{At} \dot e^{At}$ is integrable as well i.e. the dot product of these two same exponential matrices? What is the explicit representation for $e^{At} \dot e^{At}$? Similarly how is the derivative of this object defined why is the derivative equal to $Ae^{At}$ Thanks a lot for your answer!",,"['real-analysis', 'linear-algebra', 'analysis', 'multivariable-calculus']"
60,Property of the difference quotient in Evans(Partial Differential Equations),Property of the difference quotient in Evans(Partial Differential Equations),,"Why holds the property of the difference quotient in Evans(Partial Differential Equations) \begin{equation} \int_{U}v D_k^{-h}dx = -\int_U w D_k^hv dx \end{equation} for $v,w \in H^{1}_0(U)$ (16) in Evan's book, page 311. Where the difference quotient is given by \begin{equation} D_k^hu(x) = \dfrac{u(x+he_k) -u(x)}{h} \ (h \in \mathbb{R}, h\neq 0). \end{equation}","Why holds the property of the difference quotient in Evans(Partial Differential Equations) \begin{equation} \int_{U}v D_k^{-h}dx = -\int_U w D_k^hv dx \end{equation} for $v,w \in H^{1}_0(U)$ (16) in Evan's book, page 311. Where the difference quotient is given by \begin{equation} D_k^hu(x) = \dfrac{u(x+he_k) -u(x)}{h} \ (h \in \mathbb{R}, h\neq 0). \end{equation}",,"['analysis', 'partial-differential-equations', 'sobolev-spaces']"
61,How to deduce the uniform ellipticity condition from an integral condition,How to deduce the uniform ellipticity condition from an integral condition,,"Let $\Omega$ be a bounded open set, and let $A$ be a $N\times N$ symmetric matrix with entries in $L^\infty(\Omega, \mathbb{R})$, such that for some positive $\lambda$ and $\Lambda$ the following inequalities hold for every $u \in W^{1,2}_0(\Omega, \mathbb{R})$: $$  \lambda \int \limits_\Omega \! | \nabla u(x) |^2 \, \mathrm{d}x \leq \int \limits_\Omega \! A(x) \nabla u(x) \cdot \nabla u(x) \, \mathrm{d} x \leq \Lambda \int \limits_\Omega \! | \nabla u(x) |^2 \, \mathrm{d}x $$ Is there a way to deduce that $A$ satisfies the following pointwise inequalities $$ \lambda | \xi |^2 \leq A(x) \xi \cdot \xi \leq \Lambda |\xi|^2 $$ for almost every $x \in \Omega$ and for all $\xi \in \mathbb{R}^N$? The first strategy I tried to adopt was to choose a function in $W^{1,2}_0(\Omega)$ whose gradient is essentially a fixed vector $\xi \in \mathbb{R}^N$. However, to choose it in such a way that it belongs to $W^{1,2}_0(\Omega)$ it must vanish near the boundary. Hence I chose $$ u(x) := (\xi \cdot x ) \zeta(x) $$ where $\zeta$ is a cutoff function such that $\zeta \equiv 1$ on $\omega \subset \subset \Omega$ and with support in $\Omega$. Maybe, if it is convenient, we can also control $|\nabla \zeta|$ when $\partial{\omega}$ is ""near"" $\partial{\Omega}$. But I don't see how to obtain the pointwise estimates from the intagral ones! Second strategy . I tried to argue by contradiction. Assume, for example, that the inequality $$ \lambda |\xi|^2 \leq A(x) \xi \cdot \xi $$ doesn't hold for almost every $x \in \Omega$ for some $\xi \in \mathbb{R}^N$. This means that the set $$ \omega := \{ x \in \Omega \ | \ \lambda |\xi|^2 > A(x) \xi \cdot \xi \} $$ has positive measure. If I was able to construct a function in $W^{1,2}_0(\Omega)$ whose gradient is $\xi$ on $\omega$ and $0$ in $\Omega \setminus \omega$, I would conclude integrating, contradictiong the first intregral inequality. Though such function cannot be constructed in that precise way, maybe I can argue as in the first strategy using suitable cutoff functions, but my tries failed. Any suggestions?","Let $\Omega$ be a bounded open set, and let $A$ be a $N\times N$ symmetric matrix with entries in $L^\infty(\Omega, \mathbb{R})$, such that for some positive $\lambda$ and $\Lambda$ the following inequalities hold for every $u \in W^{1,2}_0(\Omega, \mathbb{R})$: $$  \lambda \int \limits_\Omega \! | \nabla u(x) |^2 \, \mathrm{d}x \leq \int \limits_\Omega \! A(x) \nabla u(x) \cdot \nabla u(x) \, \mathrm{d} x \leq \Lambda \int \limits_\Omega \! | \nabla u(x) |^2 \, \mathrm{d}x $$ Is there a way to deduce that $A$ satisfies the following pointwise inequalities $$ \lambda | \xi |^2 \leq A(x) \xi \cdot \xi \leq \Lambda |\xi|^2 $$ for almost every $x \in \Omega$ and for all $\xi \in \mathbb{R}^N$? The first strategy I tried to adopt was to choose a function in $W^{1,2}_0(\Omega)$ whose gradient is essentially a fixed vector $\xi \in \mathbb{R}^N$. However, to choose it in such a way that it belongs to $W^{1,2}_0(\Omega)$ it must vanish near the boundary. Hence I chose $$ u(x) := (\xi \cdot x ) \zeta(x) $$ where $\zeta$ is a cutoff function such that $\zeta \equiv 1$ on $\omega \subset \subset \Omega$ and with support in $\Omega$. Maybe, if it is convenient, we can also control $|\nabla \zeta|$ when $\partial{\omega}$ is ""near"" $\partial{\Omega}$. But I don't see how to obtain the pointwise estimates from the intagral ones! Second strategy . I tried to argue by contradiction. Assume, for example, that the inequality $$ \lambda |\xi|^2 \leq A(x) \xi \cdot \xi $$ doesn't hold for almost every $x \in \Omega$ for some $\xi \in \mathbb{R}^N$. This means that the set $$ \omega := \{ x \in \Omega \ | \ \lambda |\xi|^2 > A(x) \xi \cdot \xi \} $$ has positive measure. If I was able to construct a function in $W^{1,2}_0(\Omega)$ whose gradient is $\xi$ on $\omega$ and $0$ in $\Omega \setminus \omega$, I would conclude integrating, contradictiong the first intregral inequality. Though such function cannot be constructed in that precise way, maybe I can argue as in the first strategy using suitable cutoff functions, but my tries failed. Any suggestions?",,"['analysis', 'inequality', 'partial-differential-equations']"
62,Tietze–Urysohn's lemma in $\mathbb{R}^n$,Tietze–Urysohn's lemma in,\mathbb{R}^n,"Let $F_1$ and $F_0$ be closed subsets in $\mathbb{R}^n$, $F_0\cap F_1=\varnothing$. How to build a $C^{\infty}$- function $f:\mathbb{R}^n\to \mathbb{R}$, such that $f|_{F_1}=1$, $f|_{F_0}=0$ and $0<f(x)<1$ for $x\notin F_1\cup F_2$? The only useful fact I know is Tietze–Urysohn's lemma, which states that only continuous function exists. Could you give me any hints?","Let $F_1$ and $F_0$ be closed subsets in $\mathbb{R}^n$, $F_0\cap F_1=\varnothing$. How to build a $C^{\infty}$- function $f:\mathbb{R}^n\to \mathbb{R}$, such that $f|_{F_1}=1$, $f|_{F_0}=0$ and $0<f(x)<1$ for $x\notin F_1\cup F_2$? The only useful fact I know is Tietze–Urysohn's lemma, which states that only continuous function exists. Could you give me any hints?",,['general-topology']
63,Change of variables in a convolution..,Change of variables in a convolution..,,"I'm in trouble with change of variables in a convolution: Definition: The convolution of two $2\pi$-periodic functions $f$ and $g$ is defined as $$(f*g)(x)=\frac{1}{2\pi}\int_{-\pi}^{\pi} f(y)g(x-y)\ dy.$$ I wan't to show $f*(g*h)=(f*g)*h$. Well, $$[f*(g*h)](x)=\frac{1}{2\pi}\int_{-\pi}^{\pi}f(\tau)(g*h)(x-\tau)\ d\tau\\ =\frac{1}{2\pi}\int_{-\pi}^{\pi}f(\tau)\left(\frac{1}{2\pi}\int_{-\pi}^{\pi}g(\rho)h(x-\tau -\rho)\ d\rho\right)\ d\tau.$$ Now I wan't to make the change of variables $\sigma=\tau+\rho$ but I don't know how to proceed with $d\rho$ and with the extremals $-\pi$, $\pi$. Can anyone explain me that? How can I know what is fixed in $h(x-\tau-\rho)$ inside the integral?","I'm in trouble with change of variables in a convolution: Definition: The convolution of two $2\pi$-periodic functions $f$ and $g$ is defined as $$(f*g)(x)=\frac{1}{2\pi}\int_{-\pi}^{\pi} f(y)g(x-y)\ dy.$$ I wan't to show $f*(g*h)=(f*g)*h$. Well, $$[f*(g*h)](x)=\frac{1}{2\pi}\int_{-\pi}^{\pi}f(\tau)(g*h)(x-\tau)\ d\tau\\ =\frac{1}{2\pi}\int_{-\pi}^{\pi}f(\tau)\left(\frac{1}{2\pi}\int_{-\pi}^{\pi}g(\rho)h(x-\tau -\rho)\ d\rho\right)\ d\tau.$$ Now I wan't to make the change of variables $\sigma=\tau+\rho$ but I don't know how to proceed with $d\rho$ and with the extremals $-\pi$, $\pi$. Can anyone explain me that? How can I know what is fixed in $h(x-\tau-\rho)$ inside the integral?",,"['analysis', 'partial-differential-equations', 'fourier-analysis']"
64,Infinite series involving $\sqrt{n}$,Infinite series involving,\sqrt{n},"I am looking for examples of infinite series, whose sum is expressed as distributions or known functions, with a $\sqrt{n}$ in each term, such as: $$ \sum_{n=0}^{\infty} \sqrt{n} z^n, \quad \sum_{n=0}^{\infty} \frac{\sqrt{n}}{n!} z^n,$$ or more generally: $$ \sum_{n=0}^{\infty} \sqrt{n} f(n) z^n$$  where $f(n)$ is a combination (product, division) of integer factorials, constants, etc... Here, $z$ can  be real or complex.","I am looking for examples of infinite series, whose sum is expressed as distributions or known functions, with a $\sqrt{n}$ in each term, such as: $$ \sum_{n=0}^{\infty} \sqrt{n} z^n, \quad \sum_{n=0}^{\infty} \frac{\sqrt{n}}{n!} z^n,$$ or more generally: $$ \sum_{n=0}^{\infty} \sqrt{n} f(n) z^n$$  where $f(n)$ is a combination (product, division) of integer factorials, constants, etc... Here, $z$ can  be real or complex.",,"['sequences-and-series', 'analysis', 'examples-counterexamples']"
65,Calculating $d\omega$ for $\omega\in\Omega^{k}M$ explicitly for $k=2$,Calculating  for  explicitly for,d\omega \omega\in\Omega^{k}M k=2,"I am trying to explicitly calculate the exterior derivative $d\omega$ for $\omega\in\Omega^{2}M$ for a differentiable oriented manifold $M$. I know that we can express a differential $k$-form $\omega$ as follows. $$\omega=\sum_{\mu_1<\dots<\mu_k}\omega_{\mu_1,\dots,\mu_k} dx^{\mu_1}\wedge\dots\wedge dx^{\mu_k} $$ So for $k=2$ we can write $$ \begin{eqnarray*} d\omega &=& d ( \omega_{1,2} dx^{1}\wedge dx^{2} + \omega_{1,3} dx^{1}\wedge dx^{3} + \omega_{2,3} dx^{2}\wedge dx^{3} )\\ &=& d ( \omega_{1,2} \wedge dx^{1}\wedge dx^{2} + \omega_{1,3} \wedge dx^{1}\wedge dx^{3} + \omega_{2,3} \wedge dx^{2}\wedge dx^{3} )\\ &=& d ( \omega_{1,2} \wedge dx^{1}\wedge dx^{2} ) + d ( \omega_{1,3} \wedge dx^{1}\wedge dx^{3} )+ d ( \omega_{2,3} \wedge dx^{2}\wedge dx^{3} )\\ &=& d\omega_{1,2} \wedge dx^{1}\wedge dx^{2} + \omega_{1,2}\wedge (dx^2-dx^1) \\ &+& d ( \omega_{1,3} \wedge dx^{1}\wedge dx^{3}  + \omega_{1,3}\wedge (dx^3-dx^1)) \\ &+& d ( \omega_{2,3} \wedge dx^{2}\wedge dx^{3}  + \omega_{2,3}\wedge (dx^3-dx^2))\\ \end{eqnarray*} $$ In the first step I used the fact that $\omega_{i,j}$ for $i,j\in{1,2,3}$ and $i<j$ is a 0-form which allows us to exchange the corresponding $\cdot$ by a $\wedge$. The second step follows from the linearity of $d$. The third step follows by application of the properties of the exterior derivative. If I haven't made any errors, this is as far as I get. I know the solution must be $$d\omega = d\omega_{1,2} dx^{1}\wedge dx^{2} + \omega_{1,3} dx^{1}\wedge dx^{3} + \omega_{2,3} dx^{2}\wedge dx^{3} $$ But I don't see how the terms $\omega_{i,j}\wedge (dx^j-dx^i)$ with $i,j$ defined as above disappear. By which rule does this follow?","I am trying to explicitly calculate the exterior derivative $d\omega$ for $\omega\in\Omega^{2}M$ for a differentiable oriented manifold $M$. I know that we can express a differential $k$-form $\omega$ as follows. $$\omega=\sum_{\mu_1<\dots<\mu_k}\omega_{\mu_1,\dots,\mu_k} dx^{\mu_1}\wedge\dots\wedge dx^{\mu_k} $$ So for $k=2$ we can write $$ \begin{eqnarray*} d\omega &=& d ( \omega_{1,2} dx^{1}\wedge dx^{2} + \omega_{1,3} dx^{1}\wedge dx^{3} + \omega_{2,3} dx^{2}\wedge dx^{3} )\\ &=& d ( \omega_{1,2} \wedge dx^{1}\wedge dx^{2} + \omega_{1,3} \wedge dx^{1}\wedge dx^{3} + \omega_{2,3} \wedge dx^{2}\wedge dx^{3} )\\ &=& d ( \omega_{1,2} \wedge dx^{1}\wedge dx^{2} ) + d ( \omega_{1,3} \wedge dx^{1}\wedge dx^{3} )+ d ( \omega_{2,3} \wedge dx^{2}\wedge dx^{3} )\\ &=& d\omega_{1,2} \wedge dx^{1}\wedge dx^{2} + \omega_{1,2}\wedge (dx^2-dx^1) \\ &+& d ( \omega_{1,3} \wedge dx^{1}\wedge dx^{3}  + \omega_{1,3}\wedge (dx^3-dx^1)) \\ &+& d ( \omega_{2,3} \wedge dx^{2}\wedge dx^{3}  + \omega_{2,3}\wedge (dx^3-dx^2))\\ \end{eqnarray*} $$ In the first step I used the fact that $\omega_{i,j}$ for $i,j\in{1,2,3}$ and $i<j$ is a 0-form which allows us to exchange the corresponding $\cdot$ by a $\wedge$. The second step follows from the linearity of $d$. The third step follows by application of the properties of the exterior derivative. If I haven't made any errors, this is as far as I get. I know the solution must be $$d\omega = d\omega_{1,2} dx^{1}\wedge dx^{2} + \omega_{1,3} dx^{1}\wedge dx^{3} + \omega_{2,3} dx^{2}\wedge dx^{3} $$ But I don't see how the terms $\omega_{i,j}\wedge (dx^j-dx^i)$ with $i,j$ defined as above disappear. By which rule does this follow?",,"['analysis', 'differential-geometry']"
66,How to show that an open map $f $ implies the surjectivity of $f'$ in a dense set,How to show that an open map  implies the surjectivity of  in a dense set,f  f',"Let $f$ be a $C^1$ map from $U\to \mathbb{R}^m$, where $U$ is an open set in $\mathbb{R}^n$, $n\geq m$. Then we know that if $f'$ is surjective everywhere, then $f$ is open. My question is whether the converse is valid,  if the answer is no, can we prove the following weaker conclusion: If $f$ is open, $f'$ is surjective in a dense set of $U$","Let $f$ be a $C^1$ map from $U\to \mathbb{R}^m$, where $U$ is an open set in $\mathbb{R}^n$, $n\geq m$. Then we know that if $f'$ is surjective everywhere, then $f$ is open. My question is whether the converse is valid,  if the answer is no, can we prove the following weaker conclusion: If $f$ is open, $f'$ is surjective in a dense set of $U$",,"['analysis', 'differential-geometry']"
67,Does the sequence converges?,Does the sequence converges?,,"I am trying to prove if the sequence $a_n=(\root n\of e-1)\cdot n$ is convergent.  I know that the sequences $x_n=(1+1/n)^n$ and $y_n=(1+1/n)^{n+1}$ tends to the same limit which is $e$. Can anyone prove if the above sequence $a_n$ is convergent? and if so, find the limit. My trial was to write $a_n$ as $a_n=n(e^{1/n}-1)$ and taking $1/n=m$ so that $a_n=\frac{1}{m}(e^m-1)$ and taking the limit $\lim_{x\to 0^+}\frac{e^x-1}{x}$, but I don't know how to continue. Thanks to every one who solve this for me.","I am trying to prove if the sequence $a_n=(\root n\of e-1)\cdot n$ is convergent.  I know that the sequences $x_n=(1+1/n)^n$ and $y_n=(1+1/n)^{n+1}$ tends to the same limit which is $e$. Can anyone prove if the above sequence $a_n$ is convergent? and if so, find the limit. My trial was to write $a_n$ as $a_n=n(e^{1/n}-1)$ and taking $1/n=m$ so that $a_n=\frac{1}{m}(e^m-1)$ and taking the limit $\lim_{x\to 0^+}\frac{e^x-1}{x}$, but I don't know how to continue. Thanks to every one who solve this for me.",,['analysis']
68,Help understanding Dedekind cuts,Help understanding Dedekind cuts,,"I have an exam this tuesday and our prof gave us these problems to practice. Me and my friend were trying to do it. but I really never understood the concept or if its right. These are the questions: 1) Let $A_1$ be the set of rational numbers $x$ such that $x^2 < 3$, and $A_2$ be the set of rational numbers $x$ such that $x^2 > 3$. Is $(A_1, A_2)$ a dedekind cut? Prove your answer. 2) Let $(A_1, A_2)$ and $(B_1, B_2)$ be dedekind cuts representing real numbers $\alpha$ and $\beta$. Define what $\alpha < \beta$ means in terms of the Dedekind cuts. 3) Give an example of two Dedekind cuts $(A_1, A_2)$ and $(B_1, B_2)$ be dedekind cuts representing real numbers $\alpha$ and $\beta$ such that $A_1$ strictly contains $B_1$ but $\alpha$ is not strictly larger than $\beta$. 4) Give an example of two ""unessentially different"" Dedekind cuts. 5) Is it true that a rational number can be represented by two ""unesentially different"" Dedekind cuts? Explain Our answers: 1) for this one the teacher told us that is $A_1 = \{x \in \mathbb{Q} | x^2 <3\}$ and $A_2 = \{x \in \mathbb{Q} | x^2 >3\}$ she said is $0 \in A_1$ and $-100 \in A_2$ and that implies its not a dedekind cut. Am I missing something here? like where did she get 0 and -100 from. 2) and 3) we didnt get any help on this would be appreciated. 4) So if we let $(A_1 = (\mathbb{Q} \cap (-\infty , 2], A_2 = (2, \infty) \cap \mathbb{Q})$ and if $(B_1 = (\mathbb{Q} \cap (-\infty , 2), B_2 = [2, \infty) \cap \mathbb{Q})$. I dont know if that is right. please help out. and need help on 5) Any help on this would be greatly appreciated. Thank you very much","I have an exam this tuesday and our prof gave us these problems to practice. Me and my friend were trying to do it. but I really never understood the concept or if its right. These are the questions: 1) Let $A_1$ be the set of rational numbers $x$ such that $x^2 < 3$, and $A_2$ be the set of rational numbers $x$ such that $x^2 > 3$. Is $(A_1, A_2)$ a dedekind cut? Prove your answer. 2) Let $(A_1, A_2)$ and $(B_1, B_2)$ be dedekind cuts representing real numbers $\alpha$ and $\beta$. Define what $\alpha < \beta$ means in terms of the Dedekind cuts. 3) Give an example of two Dedekind cuts $(A_1, A_2)$ and $(B_1, B_2)$ be dedekind cuts representing real numbers $\alpha$ and $\beta$ such that $A_1$ strictly contains $B_1$ but $\alpha$ is not strictly larger than $\beta$. 4) Give an example of two ""unessentially different"" Dedekind cuts. 5) Is it true that a rational number can be represented by two ""unesentially different"" Dedekind cuts? Explain Our answers: 1) for this one the teacher told us that is $A_1 = \{x \in \mathbb{Q} | x^2 <3\}$ and $A_2 = \{x \in \mathbb{Q} | x^2 >3\}$ she said is $0 \in A_1$ and $-100 \in A_2$ and that implies its not a dedekind cut. Am I missing something here? like where did she get 0 and -100 from. 2) and 3) we didnt get any help on this would be appreciated. 4) So if we let $(A_1 = (\mathbb{Q} \cap (-\infty , 2], A_2 = (2, \infty) \cap \mathbb{Q})$ and if $(B_1 = (\mathbb{Q} \cap (-\infty , 2), B_2 = [2, \infty) \cap \mathbb{Q})$. I dont know if that is right. please help out. and need help on 5) Any help on this would be greatly appreciated. Thank you very much",,['analysis']
69,Uniform convergence for $x\arctan(nx)$,Uniform convergence for,x\arctan(nx),"I am to check the uniform convergence of this sequence of functions : $f_{n}(x) = x\arctan(nx)$ where $x \in \mathbb{R} $. I came to a conclusion that $f_{n}(x) \rightarrow \frac{\left|x\right|\pi}{2} $. So if $x\in [a,b]$ then $\sup_{x \in [a,b]}\left|f_n(x)- \frac{\left|x\right|\pi}{2}\right|\rightarrow 0$ as $n\to\infty.$ Now, how do I check the uniform convergence? $$\sup_{x\in\mathbb{R}}\left|f_n(x)-\frac{\left|x\right|\pi}{2}\right| = ?$$ Thanks in advance!","I am to check the uniform convergence of this sequence of functions : $f_{n}(x) = x\arctan(nx)$ where $x \in \mathbb{R} $. I came to a conclusion that $f_{n}(x) \rightarrow \frac{\left|x\right|\pi}{2} $. So if $x\in [a,b]$ then $\sup_{x \in [a,b]}\left|f_n(x)- \frac{\left|x\right|\pi}{2}\right|\rightarrow 0$ as $n\to\infty.$ Now, how do I check the uniform convergence? $$\sup_{x\in\mathbb{R}}\left|f_n(x)-\frac{\left|x\right|\pi}{2}\right| = ?$$ Thanks in advance!",,"['real-analysis', 'sequences-and-series', 'analysis', 'uniform-convergence']"
70,Inequality between norms in $\mathbb{R}^n$,Inequality between norms in,\mathbb{R}^n,"I am trying to prove that given $p>1$ there exists a constant $C=C(p,n)$ such that $\big||x|^px-|y|^py\big|\leq C\big(|x|^p+|y|^p\big)|x-y|$ for all $x,y\in\mathbb{R}^n$. It seems useful to consider the inequality $|x+y|^p\leq C\big(|x|^p+|y|^p\big)$ but I don´t know how to follow. Any help will be appreciated!","I am trying to prove that given $p>1$ there exists a constant $C=C(p,n)$ such that $\big||x|^px-|y|^py\big|\leq C\big(|x|^p+|y|^p\big)|x-y|$ for all $x,y\in\mathbb{R}^n$. It seems useful to consider the inequality $|x+y|^p\leq C\big(|x|^p+|y|^p\big)$ but I don´t know how to follow. Any help will be appreciated!",,"['analysis', 'inequality']"
71,"showing $W^{s,p}(\mathbb{R}^n) \subseteq W^{r,p}(\mathbb{R}^n)$ for $r < s$",showing  for,"W^{s,p}(\mathbb{R}^n) \subseteq W^{r,p}(\mathbb{R}^n) r < s","I'm aware of a way of doing this using pseudo-differential operator theory.  One can easily reduce to showing that $W^{t,p}(\mathbb{R}^n) \subseteq L^p$ for $t > 0$.  This in turn follows because the identity map is a symbol operator of order zero, and hence also of order $t$, and so will map $W^{t,p}(\mathbb{R}^n)$ to $L^p$ (and also isomorphically to itself), by a general boundedness result for symbol operators. I was wondering if anyone knows a more direct proof which doesn't rely on the general (and nontrivial) boundedness result for symbol operators.","I'm aware of a way of doing this using pseudo-differential operator theory.  One can easily reduce to showing that $W^{t,p}(\mathbb{R}^n) \subseteq L^p$ for $t > 0$.  This in turn follows because the identity map is a symbol operator of order zero, and hence also of order $t$, and so will map $W^{t,p}(\mathbb{R}^n)$ to $L^p$ (and also isomorphically to itself), by a general boundedness result for symbol operators. I was wondering if anyone knows a more direct proof which doesn't rely on the general (and nontrivial) boundedness result for symbol operators.",,"['analysis', 'sobolev-spaces']"
72,Finding x and y for continuous onto function,Finding x and y for continuous onto function,,"$f:[0,1]\rightarrow [0,1]$ continuous onto function such that $f(0)=0=f(1)$. Then show that there exists distinct values $x,y$ belongs to $[0,1]$ such that $f(x)=\frac{1}{2}=f(y)$. could you please tell some hint?","$f:[0,1]\rightarrow [0,1]$ continuous onto function such that $f(0)=0=f(1)$. Then show that there exists distinct values $x,y$ belongs to $[0,1]$ such that $f(x)=\frac{1}{2}=f(y)$. could you please tell some hint?",,['analysis']
73,"Is the function $f(x,y)=(x/y-a)^2(y/x-1/a)^2$ convex?",Is the function  convex?,"f(x,y)=(x/y-a)^2(y/x-1/a)^2","Is function $$f(x, y) = \left(\frac{x}{y} - a\right)^2 \left(\frac{y}{x} - \frac{1}{a}\right)^2$$ convex on the domain $$\{(x,y): x, y \in \mathbb{R}, x >0, y >0 \}\quad?$$ Now I think that it is not convex. I calculated $$f''_{xx}=\frac{2(x^2 - 3 a^2y^2 + 2xay)}{ayx^4}$$ which can be negative when $x$ is small and $y$ is large. It means that the hessian cannot be positive definite, so $f(x,y)$ is not convex. Is that right? EDIT: Correct calculations $$ f''_{xx} = \frac{3a^2y^2}{x^4} +\frac{1}{a^2y^2} - \frac{4ay}{x^3} $$","Is function $$f(x, y) = \left(\frac{x}{y} - a\right)^2 \left(\frac{y}{x} - \frac{1}{a}\right)^2$$ convex on the domain $$\{(x,y): x, y \in \mathbb{R}, x >0, y >0 \}\quad?$$ Now I think that it is not convex. I calculated $$f''_{xx}=\frac{2(x^2 - 3 a^2y^2 + 2xay)}{ayx^4}$$ which can be negative when $x$ is small and $y$ is large. It means that the hessian cannot be positive definite, so $f(x,y)$ is not convex. Is that right? EDIT: Correct calculations $$ f''_{xx} = \frac{3a^2y^2}{x^4} +\frac{1}{a^2y^2} - \frac{4ay}{x^3} $$",,"['analysis', 'convex-analysis']"
74,Banach-algebra homeomorphism.,Banach-algebra homeomorphism.,,"Let $ A $ be a commutative unital Banach algebra that is generated by a set $ Y \subseteq A $. I want to show that $ \Phi(A) $ is homeomorphic to a closed subset of the Cartesian product $ \displaystyle \prod_{y \in Y} \sigma(y) $. Moreover, if $ Y = \{ a \} $ for some $ a \in A $, I want to show that the map is onto. Notation: $ \Phi(A) $ is the set of characters on $ A $ and $ \sigma(y) $ is the spectrum of $ y $. I tried to do this with the map $$ f: \Phi(A) \longrightarrow \prod_{y \in Y} \sigma(y) $$ defined by $$ f(\phi) \stackrel{\text{def}}{=} (\phi(y))_{y \in Y}. $$ I don’t know if $ f $ makes sense, and I can’t show that it is open or continuous. Need your help. Thank you!","Let $ A $ be a commutative unital Banach algebra that is generated by a set $ Y \subseteq A $. I want to show that $ \Phi(A) $ is homeomorphic to a closed subset of the Cartesian product $ \displaystyle \prod_{y \in Y} \sigma(y) $. Moreover, if $ Y = \{ a \} $ for some $ a \in A $, I want to show that the map is onto. Notation: $ \Phi(A) $ is the set of characters on $ A $ and $ \sigma(y) $ is the spectrum of $ y $. I tried to do this with the map $$ f: \Phi(A) \longrightarrow \prod_{y \in Y} \sigma(y) $$ defined by $$ f(\phi) \stackrel{\text{def}}{=} (\phi(y))_{y \in Y}. $$ I don’t know if $ f $ makes sense, and I can’t show that it is open or continuous. Need your help. Thank you!",,"['analysis', 'functional-analysis', 'banach-algebras']"
75,Help with Dedekind cuts,Help with Dedekind cuts,,"I am reviewing what Dedekind cuts are for my quiz tomorrow. I had posted a question before about Dedekind cuts and I thought that was the only problem but there were these two problems as well for this unit I am having trouble understanding. I know what they are and could find it if it was in a real line. but i am having difficulty understanding these 2 problems For two subsets $X, Y$  of ${\mathbb{O}}$, define the subset $X + Y$ of ${\mathbb{O}}$ by $X + Y = \{x + y |x \in X$ and $y \in Y\}$. Let $(A_1, A_2)$ and $(B_1, B_2)$ be Dedekind cuts of ${\mathbb{O}}$. Let $C_1 = A_1 + B_1$  (in the above sense) and let $C_2  = {\mathbb{O}} \diagdown  C_1$. Prove that  $(C_1, C_2)$ is a Dedekind cut of ${\mathbb{O}}$ and Let $(A'_1, A'_2)$ be a Dedekind cut of ${\mathbb{O}}$ that represents the same real number as $(A_1, A_2)$. Let $C'_1 = A'_1 + B_1$ and $C'_2 = {\mathbb{O}} \diagdown  C'_1$. Prove that $(C'_1, C'_2)$ represents the same real number as $(C_1, C_2)$. thank you","I am reviewing what Dedekind cuts are for my quiz tomorrow. I had posted a question before about Dedekind cuts and I thought that was the only problem but there were these two problems as well for this unit I am having trouble understanding. I know what they are and could find it if it was in a real line. but i am having difficulty understanding these 2 problems For two subsets $X, Y$  of ${\mathbb{O}}$, define the subset $X + Y$ of ${\mathbb{O}}$ by $X + Y = \{x + y |x \in X$ and $y \in Y\}$. Let $(A_1, A_2)$ and $(B_1, B_2)$ be Dedekind cuts of ${\mathbb{O}}$. Let $C_1 = A_1 + B_1$  (in the above sense) and let $C_2  = {\mathbb{O}} \diagdown  C_1$. Prove that  $(C_1, C_2)$ is a Dedekind cut of ${\mathbb{O}}$ and Let $(A'_1, A'_2)$ be a Dedekind cut of ${\mathbb{O}}$ that represents the same real number as $(A_1, A_2)$. Let $C'_1 = A'_1 + B_1$ and $C'_2 = {\mathbb{O}} \diagdown  C'_1$. Prove that $(C'_1, C'_2)$ represents the same real number as $(C_1, C_2)$. thank you",,['analysis']
76,Does Riemann integrability on closed interval implies uniform boundedness?,Does Riemann integrability on closed interval implies uniform boundedness?,,"Does Riemann integrability on closed interval implies uniform boundedness? My thought process points to yes, because if f is Riemann integrable then it is bounded pointwise on [a,b]. I could be wrong, but I very vaguely remember from more elementary analysis that this implies uniform boundedness on that interval.","Does Riemann integrability on closed interval implies uniform boundedness? My thought process points to yes, because if f is Riemann integrable then it is bounded pointwise on [a,b]. I could be wrong, but I very vaguely remember from more elementary analysis that this implies uniform boundedness on that interval.",,"['real-analysis', 'analysis', 'integration']"
77,Versions of L'Hôpital's rule,Versions of L'Hôpital's rule,,"I am familiar with the following version of L'Hôpital's rule: Let $f,g:I\to\Bbb R$ be differentiable on the interval $I$, further assume that $g'(x)\ne0$. Let $a\in I$ and assume the limit $\lim_{x\to a}\frac{f'(x)}{g'(x)} $ exists. If it is the case that $\lim_{x\to a}f(x)=\lim_{x\to a}g(x)=0$ or $\lim_{x\to a}g(x)=\infty$ then the limit $\lim_{x\to a}\frac{f(x)}{g(x)}$ exists, and it coincides with $\lim_{x\to a}\frac{f'(x)}{g'(x)} $. Now my question is, why do such an overwhelming number of books state the theorem with the requirement that $\lim_{x\to a}f(x)=\lim_{x\to a}g(x)=0$ or $\lim_{x\to a}f(x)=\lim_{x\to a}g(x)=\infty$? In the latter case, $\lim_{x\to a}f(x)=\infty$ is really not needed. But I have seen the theorem stated like this even in non introductory books. What's the deal? thanks","I am familiar with the following version of L'Hôpital's rule: Let $f,g:I\to\Bbb R$ be differentiable on the interval $I$, further assume that $g'(x)\ne0$. Let $a\in I$ and assume the limit $\lim_{x\to a}\frac{f'(x)}{g'(x)} $ exists. If it is the case that $\lim_{x\to a}f(x)=\lim_{x\to a}g(x)=0$ or $\lim_{x\to a}g(x)=\infty$ then the limit $\lim_{x\to a}\frac{f(x)}{g(x)}$ exists, and it coincides with $\lim_{x\to a}\frac{f'(x)}{g'(x)} $. Now my question is, why do such an overwhelming number of books state the theorem with the requirement that $\lim_{x\to a}f(x)=\lim_{x\to a}g(x)=0$ or $\lim_{x\to a}f(x)=\lim_{x\to a}g(x)=\infty$? In the latter case, $\lim_{x\to a}f(x)=\infty$ is really not needed. But I have seen the theorem stated like this even in non introductory books. What's the deal? thanks",,"['real-analysis', 'analysis']"
78,nested compact set question,nested compact set question,,"Suppose $A \subset \mathbb R^n$ is not compact. Show that there exists a sequence $F_1 \supset F_2\supset F_3\supset\cdots$ of closed sets such that $F_k \cap A\ne\emptyset$ for all $k$ and $\left(\bigcap_{k=1}^\infty F_k\right)\cap A = \emptyset$. I remember a similar theorem that assume $A$ is compact and concludes the intersection is not empty (not if and only if), this is the opposite version of the questions, how to achieve this?","Suppose $A \subset \mathbb R^n$ is not compact. Show that there exists a sequence $F_1 \supset F_2\supset F_3\supset\cdots$ of closed sets such that $F_k \cap A\ne\emptyset$ for all $k$ and $\left(\bigcap_{k=1}^\infty F_k\right)\cap A = \emptyset$. I remember a similar theorem that assume $A$ is compact and concludes the intersection is not empty (not if and only if), this is the opposite version of the questions, how to achieve this?",,"['analysis', 'compactness']"
79,How to find the range and inverse of this linear operator?,How to find the range and inverse of this linear operator?,,"Given $T \colon C[0,1] \to C[0,1]$ defined by $$Tx(t):= \int_0^t x(r) dr$$ for each $t\in [0,1]$, where $C[0,1]$ is the normed space of continuous real-valued (or complex-valued) functions defined on the closed interval $[0,1]$ with the norm given by $||x|| \colon= \max_{t \in [0,1]} |x(t)|$. How to find the range of this operator? This operator is one-to-one; so an inverse exists. How to find this inverse?","Given $T \colon C[0,1] \to C[0,1]$ defined by $$Tx(t):= \int_0^t x(r) dr$$ for each $t\in [0,1]$, where $C[0,1]$ is the normed space of continuous real-valued (or complex-valued) functions defined on the closed interval $[0,1]$ with the norm given by $||x|| \colon= \max_{t \in [0,1]} |x(t)|$. How to find the range of this operator? This operator is one-to-one; so an inverse exists. How to find this inverse?",,"['analysis', 'functional-analysis', 'integration', 'operator-theory', 'normed-spaces']"
80,a question on $O$- notation,a question on - notation,O,"I'm trying to understand $O$-notationbetter. I've found a really helpful answer : Big O notation, $1/(1-x)$ series But I have trouble with quotient. Let me discuss about this example function: $$f(x)=a+bx+\frac{c+dx}{e+fx^2}e^{ax}$$ If I calculate for the zero order error $O(x^0)$, I'll get $$a+\frac{c}{e}=0$$ ...for the first order error $O(x^1)$, I'll get  $$b+\frac{d}{2fx}ae^{ax}|_{x=0}=0$$ which seems a bit or more then a bit silly:( Could you please explain me how to calculate for the first order error $O(x^1)$?","I'm trying to understand $O$-notationbetter. I've found a really helpful answer : Big O notation, $1/(1-x)$ series But I have trouble with quotient. Let me discuss about this example function: $$f(x)=a+bx+\frac{c+dx}{e+fx^2}e^{ax}$$ If I calculate for the zero order error $O(x^0)$, I'll get $$a+\frac{c}{e}=0$$ ...for the first order error $O(x^1)$, I'll get  $$b+\frac{d}{2fx}ae^{ax}|_{x=0}=0$$ which seems a bit or more then a bit silly:( Could you please explain me how to calculate for the first order error $O(x^1)$?",,"['analysis', 'numerical-methods']"
81,Question about subsequences of compact sets in metric spaces.,Question about subsequences of compact sets in metric spaces.,,"We are given a metric space $(M,d(x,y))$ and a sequence $\{x_j\}_{j=1}^\infty$ of elements of a compact set $L\subseteq M$. Also K is the set of $x\in M$  for which there is a subsequence, $\{x_{j_n}\}_{n=1}^\infty$ that converges to $x$. If $K$ has exactly one element, then we have to show that   $\{x_j\}_{j=1}^\infty$ converges to that element. Is this problem the same as proving that if all possible subsequences of a given sequence of elements of a compact set converge to an element, then the sequence also converges to that same element? And if not, how do we prove this result? By contradiction?","We are given a metric space $(M,d(x,y))$ and a sequence $\{x_j\}_{j=1}^\infty$ of elements of a compact set $L\subseteq M$. Also K is the set of $x\in M$  for which there is a subsequence, $\{x_{j_n}\}_{n=1}^\infty$ that converges to $x$. If $K$ has exactly one element, then we have to show that   $\{x_j\}_{j=1}^\infty$ converges to that element. Is this problem the same as proving that if all possible subsequences of a given sequence of elements of a compact set converge to an element, then the sequence also converges to that same element? And if not, how do we prove this result? By contradiction?",,"['real-analysis', 'analysis', 'convergence-divergence']"
82,Showing that $\lim_{x\rightarrow 0} \frac{1}{x}\int_0^x |\sin(1/y)| \mathrm{d} y \not=0$,Showing that,\lim_{x\rightarrow 0} \frac{1}{x}\int_0^x |\sin(1/y)| \mathrm{d} y \not=0,"How to show that: $$\lim_{x\rightarrow 0} \frac{1}{x}\int_0^x |\sin(1/y)| \mathrm{d} y \not=0$$ It seems like a easy example of illustrating 0 is not in the Lebesgue set of $g(x)$ where $g(x)=\sin(1/x)$ if $x\neq 0$ and $g(0)=0$. But I fail to see why the above integral is true. I tried looking at the intervals such that $\sin(1/y)$ is greater or equal to some constant (for example, $\left[\frac{1}{k\pi+\pi/6}, \frac{1}{k\pi+5\pi/6}\right]$ such that $\sin(1/y)\geq \frac{1}{2}$), however, $$\sum_{k \text{ large}} \left(\frac{1}{k\pi+\pi/6}-\frac{1}{k\pi+5\pi/6}\right)$$ converges, which is not strong enough to prove the claim. Any thoughts? Thanks in advance.","How to show that: $$\lim_{x\rightarrow 0} \frac{1}{x}\int_0^x |\sin(1/y)| \mathrm{d} y \not=0$$ It seems like a easy example of illustrating 0 is not in the Lebesgue set of $g(x)$ where $g(x)=\sin(1/x)$ if $x\neq 0$ and $g(0)=0$. But I fail to see why the above integral is true. I tried looking at the intervals such that $\sin(1/y)$ is greater or equal to some constant (for example, $\left[\frac{1}{k\pi+\pi/6}, \frac{1}{k\pi+5\pi/6}\right]$ such that $\sin(1/y)\geq \frac{1}{2}$), however, $$\sum_{k \text{ large}} \left(\frac{1}{k\pi+\pi/6}-\frac{1}{k\pi+5\pi/6}\right)$$ converges, which is not strong enough to prove the claim. Any thoughts? Thanks in advance.",,"['real-analysis', 'analysis', 'measure-theory', 'lebesgue-integral']"
83,Points where function is continuous,Points where function is continuous,,"I have a function that is defined as such, $f(x)=x$, if x is rational, ie $x=\frac{p}{q}$ and $f(x)=1-x$, if x is irrational. What are all the points of continuity? I would say that all the points of continuity are the points where $p\neq q$ since at any the limit of f(x) as x approaches 1 is 0, while the functional value of the limit of x, as x approaches 1 is 1. Since they do not agree, the function is discontinuous at any point $p=q$ Is that view correct?","I have a function that is defined as such, $f(x)=x$, if x is rational, ie $x=\frac{p}{q}$ and $f(x)=1-x$, if x is irrational. What are all the points of continuity? I would say that all the points of continuity are the points where $p\neq q$ since at any the limit of f(x) as x approaches 1 is 0, while the functional value of the limit of x, as x approaches 1 is 1. Since they do not agree, the function is discontinuous at any point $p=q$ Is that view correct?",,['analysis']
84,Monotonicity of function $f(x)=(1+1/x)^x$ [duplicate],Monotonicity of function  [duplicate],f(x)=(1+1/x)^x,"This question already has answers here : Closed 11 years ago . Possible Duplicate: How to prove $(1+1/x)^x$ is increasing when $x&amp;gt;0$? $$f(x)=(1+1/x)^x$$ Where $x>0$ I am in search to find a proof that the function $f(x)$ is always increasing in its any real number domain. As the above function always increasing a slight variation in the form of function will change the outcome in opposite way.That is when we change the exponent $x$ by ($1+x$) of the above function and letting all the expression on the right hand side intact, this new function will always be decreasing for real domain $x>0$.","This question already has answers here : Closed 11 years ago . Possible Duplicate: How to prove $(1+1/x)^x$ is increasing when $x&amp;gt;0$? $$f(x)=(1+1/x)^x$$ Where $x>0$ I am in search to find a proof that the function $f(x)$ is always increasing in its any real number domain. As the above function always increasing a slight variation in the form of function will change the outcome in opposite way.That is when we change the exponent $x$ by ($1+x$) of the above function and letting all the expression on the right hand side intact, this new function will always be decreasing for real domain $x>0$.",,['analysis']
85,Mean Power Inequality,Mean Power Inequality,,Most of the proofs of mean power inequality are based on jensen's inequality. Can the mean power inequality be prooved without use of one? Mean Power Inequality: http://www.artofproblemsolving.com/Wiki/index.php/Power_Mean_Inequality,Most of the proofs of mean power inequality are based on jensen's inequality. Can the mean power inequality be prooved without use of one? Mean Power Inequality: http://www.artofproblemsolving.com/Wiki/index.php/Power_Mean_Inequality,,"['analysis', 'inequality']"
86,Lebesgue Measure theory proof / question,Lebesgue Measure theory proof / question,,"An interesting question came up in our analysis class today. Let $S= \{f:\Bbb R \rightarrow \Bbb R \cup \{\infty\} \ | \ \{f>c\} \text{ is open for each } c \in \Bbb R\}.$ If $A$ is a nonempty indexing set and for each $a\in A$, let $f_a \in S$. Show that $\sup \{ f_a:a\in A \} \in S$. Some classmates and I are confused as to what the supremum is doing here. If the supremum is just one $f_a$, for instance say $f_k$ for some $k \in A$ then it is trivial. It must mean something else. I think we are just tripped up on the meaning of the supremum of a sequence of functions. Edit: After clarification it must mean supremum regarding $x$.","An interesting question came up in our analysis class today. Let $S= \{f:\Bbb R \rightarrow \Bbb R \cup \{\infty\} \ | \ \{f>c\} \text{ is open for each } c \in \Bbb R\}.$ If $A$ is a nonempty indexing set and for each $a\in A$, let $f_a \in S$. Show that $\sup \{ f_a:a\in A \} \in S$. Some classmates and I are confused as to what the supremum is doing here. If the supremum is just one $f_a$, for instance say $f_k$ for some $k \in A$ then it is trivial. It must mean something else. I think we are just tripped up on the meaning of the supremum of a sequence of functions. Edit: After clarification it must mean supremum regarding $x$.",,"['real-analysis', 'analysis', 'measure-theory']"
87,"Compactness in $C([0,1])$",Compactness in,"C([0,1])","I read in a paper (pp8) that the set $$A=\left\{f\in W^{1,1}(0,1):\sup |f|\leq C,\ \int_0^1|f'(t)|dt\leq M \right\}$$ is compact in $C([0,1])$, where $C$ and $M$ are fixed constants. I understand that $W^{1,1}(0,1)$ denotes in this situation the space of absolutely continuous functions. In order to apply the Arzela Ascoli Theorem, I would require uniform boundedness which is given, but to prove equicontinuity, a bound on $f'$ would have been nicer. I do not see how to use the second condition to prove equicontinuity. Could someone give a hint.","I read in a paper (pp8) that the set $$A=\left\{f\in W^{1,1}(0,1):\sup |f|\leq C,\ \int_0^1|f'(t)|dt\leq M \right\}$$ is compact in $C([0,1])$, where $C$ and $M$ are fixed constants. I understand that $W^{1,1}(0,1)$ denotes in this situation the space of absolutely continuous functions. In order to apply the Arzela Ascoli Theorem, I would require uniform boundedness which is given, but to prove equicontinuity, a bound on $f'$ would have been nicer. I do not see how to use the second condition to prove equicontinuity. Could someone give a hint.",,"['analysis', 'functional-analysis']"
88,rectifiable continuous function,rectifiable continuous function,,"could you please help me with this question: if $F:[a,b] \rightarrow \mathbb{R}^{m}$ is continuous and rectifiable, then $ F ([a, b]) $ has $ m $ - measure zero. Any suggestions are welcome.","could you please help me with this question: if $F:[a,b] \rightarrow \mathbb{R}^{m}$ is continuous and rectifiable, then $ F ([a, b]) $ has $ m $ - measure zero. Any suggestions are welcome.",,"['real-analysis', 'analysis', 'integration']"
89,characterisation of compactness in the space of all convergent sequences,characterisation of compactness in the space of all convergent sequences,,"I go through a proof of the following. Let $(\ell_1,d)$ be the metric space of all sequences $x = (\xi_i)_{i \in \mathbb{N}}$ with $\sum_{i=1}^{\infty} |\xi_i| < \infty$ and the metric $$   d(x,y) = \sum_{i=1}^{\infty} |\xi_i - \eta_i|, \qquad x = (\xi_i), y = (\eta_i). $$ Theorem : A subset $M$ of $l_1$ is totally bounded (pre-compact) iff (i) There is a $K > 0$ with $\sum_{i=1}^{\infty} |\xi_i| \le K$ for all $x = (\xi_i) \in M$; (ii) $\forall \varepsilon > 0 \exists n_0 \in \mathbb{N} \forall x = (\xi_i) \in M: \sum_{i=n_0}^{\infty} |\xi_i| < \varepsilon$. The proof goes like this, let $M$ be a subset such that (i) and (ii) hold and let $x = (x_i)$ be a sequences in $\ell_1$ (i.e. a sequence of sequences), we show that it has a sub-sequence $x' = (x'_i)$ with $x'_i = (\xi^{(i)}_j)$ which is a Cauchy-Sequence. So let $\varepsilon > 0$ be given, then select $n_0$ such as in (ii), then for $n,m > n_0$ \begin{align*}  d(x_n, x_m) &= \sum_{i=1}^{\infty} | \xi^{(n)}_i - \xi^{(m)}_j |                \le \sum_{i=1}^{\infty} | \xi^{(n)}_i | - | \xi^{(m)}_j |              \le 2\varepsilon \end{align*} (end of proof) I dont understand the last steps, why is this sum smaller than $2\varepsilon$?.","I go through a proof of the following. Let $(\ell_1,d)$ be the metric space of all sequences $x = (\xi_i)_{i \in \mathbb{N}}$ with $\sum_{i=1}^{\infty} |\xi_i| < \infty$ and the metric $$   d(x,y) = \sum_{i=1}^{\infty} |\xi_i - \eta_i|, \qquad x = (\xi_i), y = (\eta_i). $$ Theorem : A subset $M$ of $l_1$ is totally bounded (pre-compact) iff (i) There is a $K > 0$ with $\sum_{i=1}^{\infty} |\xi_i| \le K$ for all $x = (\xi_i) \in M$; (ii) $\forall \varepsilon > 0 \exists n_0 \in \mathbb{N} \forall x = (\xi_i) \in M: \sum_{i=n_0}^{\infty} |\xi_i| < \varepsilon$. The proof goes like this, let $M$ be a subset such that (i) and (ii) hold and let $x = (x_i)$ be a sequences in $\ell_1$ (i.e. a sequence of sequences), we show that it has a sub-sequence $x' = (x'_i)$ with $x'_i = (\xi^{(i)}_j)$ which is a Cauchy-Sequence. So let $\varepsilon > 0$ be given, then select $n_0$ such as in (ii), then for $n,m > n_0$ \begin{align*}  d(x_n, x_m) &= \sum_{i=1}^{\infty} | \xi^{(n)}_i - \xi^{(m)}_j |                \le \sum_{i=1}^{\infty} | \xi^{(n)}_i | - | \xi^{(m)}_j |              \le 2\varepsilon \end{align*} (end of proof) I dont understand the last steps, why is this sum smaller than $2\varepsilon$?.",,"['general-topology', 'sequences-and-series', 'analysis', 'functional-analysis', 'compactness']"
90,Direct proof of Fubini's Theorem for the Darboux Integral over a rectangle,Direct proof of Fubini's Theorem for the Darboux Integral over a rectangle,,"I am looking for a direct proof of the ""Fubini"" theorem for the Darboux Integral. The Theorem: Let $I_1\subseteq \mathbb{R}^n$, $I_2\subseteq \mathbb{R}^m$ be boxes and $f:I_1\times I_2\to \mathbb{R}$ be integrable. Then the iterated integrals \begin{equation}\int_{I_1}\left(\int_{I_2}f(x,y)\;dx\right)\;dy\text{ and }\int_{I_2}\left(\int_{I_1}f(x,y)\;dy\right)\;dx\end{equation} exist and  \begin{equation}\int_{I_2\times I_2}f=\int_{I_1}\left(\int_{I_2}f(x,y)\;dx\right)\;dy=\int_{I_2}\left(\int_{I_1}f(x,y)\;dy\right)\;dx\end{equation} A ($n$-th dimensional) box $I$ is a set $I= \left\{(x_1,...,x_n)\in \mathbb{R}^n:a_i\le x_i\le b_i,\ i=1,...,n\right\}$. The integral of a bounded function $f:I\to \mathbb{R}$ is defined as follows: If $\mathcal{P}=\left\{ \mathbf{x}\in \mathbb{R}^n :c_{i-1,j}\le x_j\le c_{i,j}\ , j=1,...,n, i=1,...,k \right\}$  is a partition of $I$ with subpartitions $\mathcal{P}_i=\left\{\mathbf{x}\in \mathbb{R}^n :c_{i-1,j}\le x_j\le c_{i,j}\ , j=1,...,n \right\}$ we define the upper and lower Riemann sums of $f$ as  \begin{equation} U_{f,\mathcal{P}}:=\sum\limits_{i=1}^k\sup_{\mathbf{x}\in \mathcal{P}_i}f(\mathbf{x})vol(\mathcal{P}_i) \text{ and } L_{f,\mathcal{P}}:=\sum\limits_{i=1}^k\inf_{\mathbf{x}\in \mathcal{P}_i}f(\mathbf{x})vol(\mathcal{P}_i) \end{equation}  where $vol(\mathcal{P}_i)=\prod_{j=1}^{n}(c_{i,j}-c_{i-1,j})$. If the numbers \begin{equation}\int\limits_{I}^{*}f:=\inf_{\mathcal{P}}U_{f,\mathcal{P}} \text{ and } \int\limits_{*I}f:=\sup_{\mathcal{Q}}L_{f,\mathcal{Q}}\end{equation} are equal we say that $f$ is Riemann Integrable and denote their common value  with the symbol $\int\limits_{I}f$. As I already mentioned I am looking for a somewhat direct proof from this definition. Other proofs utilising the definition with step functions can be seen here: http://www.tau.ac.il/~tsirel/Courses/Analysis3/lect9.pdf , http://www.cmc.edu/math/publications/aksoy/Mixed_Partials.pdf , http://www.owlnet.rice.edu/~fjones/chap9.pdf Based on http://math.berkeley.edu/~wodzicki/H104.F10/Integral.pdf pg 19 here is what I have done: Let $\mathcal{P}$ be a partition of $I_1\times I_2\subseteq \mathbb{R}^{n+m}$,  \begin{equation}\mathcal{P}=\left\{ \mathbf{z}\in \mathbb{R}^{n+m}:c_{i-1,j}\le x_j\le c_{i,j}\text{ and }\notag\\c_{i-1,j^{\prime}}\le y_{j^{\prime}-n}\le c_{i,j^{\prime}}\ , j=1,...,n, j^{\prime}=n+1,...,n+m, i=1,...,k \right\}\end{equation} where $\mathbf{z}=(x_1,...,x_n,y_1,...,y_m)$. Consider the partitions $\mathcal{P}_1$, $\mathcal{P}_2$ of $I_1$ and $I_2$ respectively, \begin{gather}\mathcal{P}_1=\left\{ \mathbf{x}\in \mathbb{R}^{n}:c_{i-1,j}\le x_j\le c_{i,j}\ , j=1,...,n, i=1,...,k \right\}\text{ and }\notag\\ \mathcal{P}_2=\left\{ \mathbf{y}\in \mathbb{R}^{m}:c_{i-1,j^{\prime}}\le y_{j^{\prime}-n}\le c_{i,j^{\prime}}\ j^{\prime}=n+1,...,n+m, i=1,...,k \right\}\end{gather} Obviously $\mathcal{P}_{1i}\times\mathcal{P}_{2i}= \mathcal{P}_i$ and $\mathcal{P}_{1}\times\mathcal{P}_{2}= \mathcal{P}$.  Then, for $i=1,...,k$ \begin{equation}\inf_{(x,y)\in \mathcal{P}_i}f(x,y)=\inf_{x\in \mathcal{P}_{1i}}\left(\inf_{y\in \mathcal{P}_{2i}}f(x,y)\right)\end{equation} Indeed, for arbitrary $\epsilon>0$, \begin{gather}\exists x\in \mathcal{P}_{1i}:\inf_{x\in \mathcal{P}_{1i}}\left(\inf_{y\in \mathcal{P}_{2i}}f(x,y)\right)+\frac{\epsilon}{2}>\inf_{y\in \mathcal{P}_{2i}}f(x,y)\text{ and }  \exists y\in \mathcal{P}_{2i}:\inf_{y\in \mathcal{P}_{2i}}f(x,y)+\frac{\epsilon}{2}>f(x,y)\Rightarrow \notag\\ \exists (x,y)\in \mathcal{P}{i}:\inf_{x\in \mathcal{P}_{1i}}\left(\inf_{y\in \mathcal{P}_{2i}}f(x,y)\right)+\epsilon>f(x,y) \end{gather} Therefore, \begin{equation} L_{f,\mathcal{P}}=\sum\limits_{i=1}^k\inf_{(x,y)\in \mathcal{P}_i}f(x,y)vol(\mathcal{P}_i)=\sum\limits_{i=1}^k\inf_{x\in \mathcal{P}_{1i}}\left(\inf_{y\in \mathcal{P}_{2i}}f(x,y)\right)vol(\mathcal{P}_{1i})vol(\mathcal{P}_{2i}) \end{equation}  How do I proceed from there?","I am looking for a direct proof of the ""Fubini"" theorem for the Darboux Integral. The Theorem: Let $I_1\subseteq \mathbb{R}^n$, $I_2\subseteq \mathbb{R}^m$ be boxes and $f:I_1\times I_2\to \mathbb{R}$ be integrable. Then the iterated integrals \begin{equation}\int_{I_1}\left(\int_{I_2}f(x,y)\;dx\right)\;dy\text{ and }\int_{I_2}\left(\int_{I_1}f(x,y)\;dy\right)\;dx\end{equation} exist and  \begin{equation}\int_{I_2\times I_2}f=\int_{I_1}\left(\int_{I_2}f(x,y)\;dx\right)\;dy=\int_{I_2}\left(\int_{I_1}f(x,y)\;dy\right)\;dx\end{equation} A ($n$-th dimensional) box $I$ is a set $I= \left\{(x_1,...,x_n)\in \mathbb{R}^n:a_i\le x_i\le b_i,\ i=1,...,n\right\}$. The integral of a bounded function $f:I\to \mathbb{R}$ is defined as follows: If $\mathcal{P}=\left\{ \mathbf{x}\in \mathbb{R}^n :c_{i-1,j}\le x_j\le c_{i,j}\ , j=1,...,n, i=1,...,k \right\}$  is a partition of $I$ with subpartitions $\mathcal{P}_i=\left\{\mathbf{x}\in \mathbb{R}^n :c_{i-1,j}\le x_j\le c_{i,j}\ , j=1,...,n \right\}$ we define the upper and lower Riemann sums of $f$ as  \begin{equation} U_{f,\mathcal{P}}:=\sum\limits_{i=1}^k\sup_{\mathbf{x}\in \mathcal{P}_i}f(\mathbf{x})vol(\mathcal{P}_i) \text{ and } L_{f,\mathcal{P}}:=\sum\limits_{i=1}^k\inf_{\mathbf{x}\in \mathcal{P}_i}f(\mathbf{x})vol(\mathcal{P}_i) \end{equation}  where $vol(\mathcal{P}_i)=\prod_{j=1}^{n}(c_{i,j}-c_{i-1,j})$. If the numbers \begin{equation}\int\limits_{I}^{*}f:=\inf_{\mathcal{P}}U_{f,\mathcal{P}} \text{ and } \int\limits_{*I}f:=\sup_{\mathcal{Q}}L_{f,\mathcal{Q}}\end{equation} are equal we say that $f$ is Riemann Integrable and denote their common value  with the symbol $\int\limits_{I}f$. As I already mentioned I am looking for a somewhat direct proof from this definition. Other proofs utilising the definition with step functions can be seen here: http://www.tau.ac.il/~tsirel/Courses/Analysis3/lect9.pdf , http://www.cmc.edu/math/publications/aksoy/Mixed_Partials.pdf , http://www.owlnet.rice.edu/~fjones/chap9.pdf Based on http://math.berkeley.edu/~wodzicki/H104.F10/Integral.pdf pg 19 here is what I have done: Let $\mathcal{P}$ be a partition of $I_1\times I_2\subseteq \mathbb{R}^{n+m}$,  \begin{equation}\mathcal{P}=\left\{ \mathbf{z}\in \mathbb{R}^{n+m}:c_{i-1,j}\le x_j\le c_{i,j}\text{ and }\notag\\c_{i-1,j^{\prime}}\le y_{j^{\prime}-n}\le c_{i,j^{\prime}}\ , j=1,...,n, j^{\prime}=n+1,...,n+m, i=1,...,k \right\}\end{equation} where $\mathbf{z}=(x_1,...,x_n,y_1,...,y_m)$. Consider the partitions $\mathcal{P}_1$, $\mathcal{P}_2$ of $I_1$ and $I_2$ respectively, \begin{gather}\mathcal{P}_1=\left\{ \mathbf{x}\in \mathbb{R}^{n}:c_{i-1,j}\le x_j\le c_{i,j}\ , j=1,...,n, i=1,...,k \right\}\text{ and }\notag\\ \mathcal{P}_2=\left\{ \mathbf{y}\in \mathbb{R}^{m}:c_{i-1,j^{\prime}}\le y_{j^{\prime}-n}\le c_{i,j^{\prime}}\ j^{\prime}=n+1,...,n+m, i=1,...,k \right\}\end{gather} Obviously $\mathcal{P}_{1i}\times\mathcal{P}_{2i}= \mathcal{P}_i$ and $\mathcal{P}_{1}\times\mathcal{P}_{2}= \mathcal{P}$.  Then, for $i=1,...,k$ \begin{equation}\inf_{(x,y)\in \mathcal{P}_i}f(x,y)=\inf_{x\in \mathcal{P}_{1i}}\left(\inf_{y\in \mathcal{P}_{2i}}f(x,y)\right)\end{equation} Indeed, for arbitrary $\epsilon>0$, \begin{gather}\exists x\in \mathcal{P}_{1i}:\inf_{x\in \mathcal{P}_{1i}}\left(\inf_{y\in \mathcal{P}_{2i}}f(x,y)\right)+\frac{\epsilon}{2}>\inf_{y\in \mathcal{P}_{2i}}f(x,y)\text{ and }  \exists y\in \mathcal{P}_{2i}:\inf_{y\in \mathcal{P}_{2i}}f(x,y)+\frac{\epsilon}{2}>f(x,y)\Rightarrow \notag\\ \exists (x,y)\in \mathcal{P}{i}:\inf_{x\in \mathcal{P}_{1i}}\left(\inf_{y\in \mathcal{P}_{2i}}f(x,y)\right)+\epsilon>f(x,y) \end{gather} Therefore, \begin{equation} L_{f,\mathcal{P}}=\sum\limits_{i=1}^k\inf_{(x,y)\in \mathcal{P}_i}f(x,y)vol(\mathcal{P}_i)=\sum\limits_{i=1}^k\inf_{x\in \mathcal{P}_{1i}}\left(\inf_{y\in \mathcal{P}_{2i}}f(x,y)\right)vol(\mathcal{P}_{1i})vol(\mathcal{P}_{2i}) \end{equation}  How do I proceed from there?",,"['analysis', 'multivariable-calculus', 'integration']"
91,Conditional sum and absolute sum,Conditional sum and absolute sum,,"My professor in a Measure theory class mentioned that there is a difference between $$\sum_{n\in\mathbb{N}}a_n$$ and $$\sum_{n=0}^\infty a_n,$$ in the sense that the latter considers the order (taking the definition of the bottom to be the usual limit of partial sums), while the former doesn't (defined as $\sup\{\sum_{a_n\in J}a_n : J \text{ finite} \}$). I believe they are called conditional sums and absolute sums (but I could be mistaken). I'm having trouble seeing the difference (i.e. I see the difference in the definiton, but I don't see how the two values could be different, or how a series $(a_n)$ could converge in one way, but not the other).  Examples would be appreciated, please.","My professor in a Measure theory class mentioned that there is a difference between $$\sum_{n\in\mathbb{N}}a_n$$ and $$\sum_{n=0}^\infty a_n,$$ in the sense that the latter considers the order (taking the definition of the bottom to be the usual limit of partial sums), while the former doesn't (defined as $\sup\{\sum_{a_n\in J}a_n : J \text{ finite} \}$). I believe they are called conditional sums and absolute sums (but I could be mistaken). I'm having trouble seeing the difference (i.e. I see the difference in the definiton, but I don't see how the two values could be different, or how a series $(a_n)$ could converge in one way, but not the other).  Examples would be appreciated, please.",,"['sequences-and-series', 'analysis']"
92,Can I get better approximation of $\sum_{k=1}^{n} k^k$,Can I get better approximation of,\sum_{k=1}^{n} k^k,Is it possible to get approximation$f(n)$ of $\sum_{k=1}^{n} k^k$ with \begin{align} \lim_{n\to +\infty }\left(f(n)-\sum_{k=1}^{n} k^k\right)=0 \end{align} Thanks for your attention!,Is it possible to get approximation$f(n)$ of $\sum_{k=1}^{n} k^k$ with \begin{align} \lim_{n\to +\infty }\left(f(n)-\sum_{k=1}^{n} k^k\right)=0 \end{align} Thanks for your attention!,,['analysis']
93,Metric Of A Graph,Metric Of A Graph,,"The following is question 6 from page 99 of Walter Rudin's Principles Of Mathematical Analysis .  I'm having trouble understanding what the metric of the graph might be (which, as far as I can tell, is not defined in the text or the problem)... If f is defined on E , the graph of f is the set of points   $(x,f(x))$, for $x \in E$.  In particular, if E is a set of real   numbers, and f is real-valued, the graph of f is a subset of the   plane. Suppose E is compact, and prove that f is continuous on E if and   only if its graph is compact. I think I've been able to prove the forward result.  Suppose that E is compact.  Rudin proves a theorem in the text that states the image of a compact metric space under a continuous function is also compact.  Therefore, we know that $f(E)$ is compact.  Now suppose that $\lbrace G_\alpha \rbrace, \alpha \in A$ is an open cover of the graph, where $G_i = B_i \times C_i$ and $B_i \in E, C_i \in f(E)$.  Then $\lbrace A_\alpha \rbrace$ and $\lbrace B_\alpha \rbrace$ are open covers for E and $f(E)$, respectively.  Because these sets are compact, their open covers contain finite subcovers, $\lbrace A^\prime_\beta \rbrace$ and $\lbrace B^\prime_\gamma \rbrace$, respectively.  Thus, the set of all combinations of $(A^\prime_\beta, B^\prime_\gamma)$ forms a finite open subcover of the graph, proving that the graph is compact. Actually, I'm really confused at this point, because it's just occured to me while typing the above that I cannot assume that each set $G_i$ can be represented as a set $\lbrace (x,y) \mid x \in A_i, y\in B_i \rbrace$ for open sets $A_i \subseteq E$ and $B_i \subseteq f(E)$. So at this point, I'm not sure what to do, since I am unable to figure out what the distance metric might be in the metric space containing the graph.  Is there a convention for this sort of problem?  Did Rudin want the reader to only consider real-valued functions for f ?","The following is question 6 from page 99 of Walter Rudin's Principles Of Mathematical Analysis .  I'm having trouble understanding what the metric of the graph might be (which, as far as I can tell, is not defined in the text or the problem)... If f is defined on E , the graph of f is the set of points   $(x,f(x))$, for $x \in E$.  In particular, if E is a set of real   numbers, and f is real-valued, the graph of f is a subset of the   plane. Suppose E is compact, and prove that f is continuous on E if and   only if its graph is compact. I think I've been able to prove the forward result.  Suppose that E is compact.  Rudin proves a theorem in the text that states the image of a compact metric space under a continuous function is also compact.  Therefore, we know that $f(E)$ is compact.  Now suppose that $\lbrace G_\alpha \rbrace, \alpha \in A$ is an open cover of the graph, where $G_i = B_i \times C_i$ and $B_i \in E, C_i \in f(E)$.  Then $\lbrace A_\alpha \rbrace$ and $\lbrace B_\alpha \rbrace$ are open covers for E and $f(E)$, respectively.  Because these sets are compact, their open covers contain finite subcovers, $\lbrace A^\prime_\beta \rbrace$ and $\lbrace B^\prime_\gamma \rbrace$, respectively.  Thus, the set of all combinations of $(A^\prime_\beta, B^\prime_\gamma)$ forms a finite open subcover of the graph, proving that the graph is compact. Actually, I'm really confused at this point, because it's just occured to me while typing the above that I cannot assume that each set $G_i$ can be represented as a set $\lbrace (x,y) \mid x \in A_i, y\in B_i \rbrace$ for open sets $A_i \subseteq E$ and $B_i \subseteq f(E)$. So at this point, I'm not sure what to do, since I am unable to figure out what the distance metric might be in the metric space containing the graph.  Is there a convention for this sort of problem?  Did Rudin want the reader to only consider real-valued functions for f ?",,"['analysis', 'metric-spaces', 'continuity']"
94,Distribution function and $L^p$ spaces.,Distribution function and  spaces.,L^p,"I saw the result below without a proof and I would like to see it. Result: Let $g$ a nonnegative and measurable function in $\Omega$ and $\mu_{g} $ its distribuction function, i.e.,   \begin{equation} \mu_{g}(t)= |\{x\in \Omega : g(x)>t\}|, t>0. \end{equation}   Let $\eta>0$ and $M>1$ be constants. Then, for $0<p<\infty,$   \begin{equation} g \in L^{p}(\Omega) \Leftrightarrow \sum_{k\ge 1} M^{pk} \mu_{g}(\eta M^k) = S < \infty.   \end{equation}","I saw the result below without a proof and I would like to see it. Result: Let $g$ a nonnegative and measurable function in $\Omega$ and $\mu_{g} $ its distribuction function, i.e.,   \begin{equation} \mu_{g}(t)= |\{x\in \Omega : g(x)>t\}|, t>0. \end{equation}   Let $\eta>0$ and $M>1$ be constants. Then, for $0<p<\infty,$   \begin{equation} g \in L^{p}(\Omega) \Leftrightarrow \sum_{k\ge 1} M^{pk} \mu_{g}(\eta M^k) = S < \infty.   \end{equation}",,"['analysis', 'measure-theory']"
95,Find a function that satisfies the following five conditions.,Find a function that satisfies the following five conditions.,,"My task is to find a function $h:[-1,1] \to \mathbb{R}$ so that (i) $h(-1) = h(1) = 0$ (ii) $h$ is continuously differentiable on $[-1,1]$ (iii) $h$ is twice differentiable on $(-1,0) \cup (0,1)$ (iv) $|h^{\prime\prime}(x)| < 1$ for all $x \in (-1,0)\cup(0,1)$ (v) $|h(x)| > \frac{1}{2}$ for some $x \in [-1,1]$ The source I have says to use the function $h(x) = \frac{3}{4}\left(1-x^{4/3}\right)$ which fails to satisfy condition (iv) so it is incorrect. I'm starting to doubt the validity of the problem statement because of this. So my question is does such a function exist? If not, why? Thanks!","My task is to find a function $h:[-1,1] \to \mathbb{R}$ so that (i) $h(-1) = h(1) = 0$ (ii) $h$ is continuously differentiable on $[-1,1]$ (iii) $h$ is twice differentiable on $(-1,0) \cup (0,1)$ (iv) $|h^{\prime\prime}(x)| < 1$ for all $x \in (-1,0)\cup(0,1)$ (v) $|h(x)| > \frac{1}{2}$ for some $x \in [-1,1]$ The source I have says to use the function $h(x) = \frac{3}{4}\left(1-x^{4/3}\right)$ which fails to satisfy condition (iv) so it is incorrect. I'm starting to doubt the validity of the problem statement because of this. So my question is does such a function exist? If not, why? Thanks!",,"['real-analysis', 'analysis']"
96,Using Khinchin's inequality,Using Khinchin's inequality,,"At the end of page 5 of the Tao's lectures notes , he sets $\psi$ a Schwartz function supported on the unit cube $[0,1]^n$ and choose $f(x)=\sum_{k=1}^N\epsilon_k\psi(x-ke_1)$, where $e_1$ is one of the basis vectors of $\mathbb{R}^n$, $N$ is a large integer and $\epsilon_k$ are a collection of independent identically distributed signs $\epsilon_k=\pm1$. We have $\|f\|_p\sim N^{1/p}$ (here $A\sim B\Leftrightarrow A\lesssim B$ and $B\lesssim A$, where $A\lesssim B$ means that there is C such that $A\leq C.B$.) Using the Khinchin inequality: If $f_1,\ldots,f_N$ are a collection of functions and $\epsilon_k$ are randomized signs, then for any $1<p<\infty$ we have $$E(\|\sum_{k=1}^N\epsilon_kf_k\|_p^p)\sim \|(\sum_{k=1}^N|f_k|^2 )^{1/2}\|_p^p,$$ where the constants in the $\sim$  symbol are independent of $N$ and $f_k$ and $E$ denotes the expectation, we see that $$E(\|\hat{f}\|_q^q)\sim \|(\sum_{k=1}^N|\hat{\psi}(\xi)e^{2\pi ik\xi_1}|^2)^{1/2}\|_q^q\sim N^{q/2}.$$ I don't understand the following statement: there must exist some choice of signs for which $\|\hat{f}\|_q\gtrsim N^{1/2}$. Help me, please?","At the end of page 5 of the Tao's lectures notes , he sets $\psi$ a Schwartz function supported on the unit cube $[0,1]^n$ and choose $f(x)=\sum_{k=1}^N\epsilon_k\psi(x-ke_1)$, where $e_1$ is one of the basis vectors of $\mathbb{R}^n$, $N$ is a large integer and $\epsilon_k$ are a collection of independent identically distributed signs $\epsilon_k=\pm1$. We have $\|f\|_p\sim N^{1/p}$ (here $A\sim B\Leftrightarrow A\lesssim B$ and $B\lesssim A$, where $A\lesssim B$ means that there is C such that $A\leq C.B$.) Using the Khinchin inequality: If $f_1,\ldots,f_N$ are a collection of functions and $\epsilon_k$ are randomized signs, then for any $1<p<\infty$ we have $$E(\|\sum_{k=1}^N\epsilon_kf_k\|_p^p)\sim \|(\sum_{k=1}^N|f_k|^2 )^{1/2}\|_p^p,$$ where the constants in the $\sim$  symbol are independent of $N$ and $f_k$ and $E$ denotes the expectation, we see that $$E(\|\hat{f}\|_q^q)\sim \|(\sum_{k=1}^N|\hat{\psi}(\xi)e^{2\pi ik\xi_1}|^2)^{1/2}\|_q^q\sim N^{q/2}.$$ I don't understand the following statement: there must exist some choice of signs for which $\|\hat{f}\|_q\gtrsim N^{1/2}$. Help me, please?",,"['probability', 'analysis', 'fourier-analysis']"
97,existence of a harmonic function,existence of a harmonic function,,"Let $\Omega\subset\mathbb R^n$ open, not bounded and $n\ge3$. Let $\partial\Omega$ bounded and regular concering the laplace operator. Given a continuous function $\phi:\partial\Omega\rightarrow\mathbb R$ and $\gamma\in\mathbb R$ there exists a  harmonic function $u\in C^2(\Omega)\cap C^0(\overline\Omega)$ with $u=g\space\space\text{on}\space\partial\Omega$ and $\lim\limits_{|x|\rightarrow\infty}u(x)=\gamma$ How can you prove the existence?","Let $\Omega\subset\mathbb R^n$ open, not bounded and $n\ge3$. Let $\partial\Omega$ bounded and regular concering the laplace operator. Given a continuous function $\phi:\partial\Omega\rightarrow\mathbb R$ and $\gamma\in\mathbb R$ there exists a  harmonic function $u\in C^2(\Omega)\cap C^0(\overline\Omega)$ with $u=g\space\space\text{on}\space\partial\Omega$ and $\lim\limits_{|x|\rightarrow\infty}u(x)=\gamma$ How can you prove the existence?",,['real-analysis']
98,Clarification on bounded convergence theorem.,Clarification on bounded convergence theorem.,,"For the proof of Bounded Convergence Theorem, I see how to get most all the information, but I don't see exactly why $f$ is measurable. I assume I am missing something completely obvious. Here is the theorem as I understand it so far. Theorem: Let $\{ f_n \}$ be a sequence of functions bounded by $M$, supported on a set $E$ of finite measure, and $f_n(x) \to f(x)$ a.e. $x$. Then $f$ is bounded measurable function supported on $E$ for a.e. $x$ and $\int |f_n(x) - f(x)|dx \to 0$ as $n \to 0$. Proof: Almost everywhere we have, $$|f(x)| - |f_n(x)| \le |f(x) - f_n(x)| < \epsilon.$$ Therefore, $|f(x)| \le M$ almost everywhere, and $f(x) = 0$ for $x \in E$ almost everywhere. Let $\epsilon > 0$. By Egorov's theorem we can find a measurable set $A \subset E$ such that $f_n \to f$ uniformly on $A$ and $m(E - A) < \epsilon$. Thus $|f_n(x) - f(x)| < \epsilon$ for large $n$ for all $x \in A$. $$\int |f_n(x) - f(x)|dx \le \int_A |f_n(x) - f(x)| dx + \int_{E - A} |f_n(x) - f(x)| dx \le \epsilon m(A) + 2M \epsilon.$$ As $\epsilon$ is arbitrary, this completes the proof. Is measurability something that I just get automatically?","For the proof of Bounded Convergence Theorem, I see how to get most all the information, but I don't see exactly why $f$ is measurable. I assume I am missing something completely obvious. Here is the theorem as I understand it so far. Theorem: Let $\{ f_n \}$ be a sequence of functions bounded by $M$, supported on a set $E$ of finite measure, and $f_n(x) \to f(x)$ a.e. $x$. Then $f$ is bounded measurable function supported on $E$ for a.e. $x$ and $\int |f_n(x) - f(x)|dx \to 0$ as $n \to 0$. Proof: Almost everywhere we have, $$|f(x)| - |f_n(x)| \le |f(x) - f_n(x)| < \epsilon.$$ Therefore, $|f(x)| \le M$ almost everywhere, and $f(x) = 0$ for $x \in E$ almost everywhere. Let $\epsilon > 0$. By Egorov's theorem we can find a measurable set $A \subset E$ such that $f_n \to f$ uniformly on $A$ and $m(E - A) < \epsilon$. Thus $|f_n(x) - f(x)| < \epsilon$ for large $n$ for all $x \in A$. $$\int |f_n(x) - f(x)|dx \le \int_A |f_n(x) - f(x)| dx + \int_{E - A} |f_n(x) - f(x)| dx \le \epsilon m(A) + 2M \epsilon.$$ As $\epsilon$ is arbitrary, this completes the proof. Is measurability something that I just get automatically?",,"['real-analysis', 'analysis']"
99,"To prove $f(x)\to\infty$ with an ""Oresme"" strategy","To prove  with an ""Oresme"" strategy",f(x)\to\infty,"My goal is to prove that: $\displaystyle\sum\limits_{n=1}^\infty \frac{1}{n^{x}} \rightarrow \infty$ when $ x\rightarrow 1^+$ My first approach (which failed) is here: To prove $f(x)\rightarrow \infty$ with a ""home made"" strategy I think the confusing part is the ""$x\rightarrow 1^+$"". I have now argued for the statement, but I haven't used the direction to $1$. I begin with $s_{N}(x) = \displaystyle\sum\limits_{n=1}^N \frac{1}{n^{x}}$ and write $s_{2^{n}}(x)$ as: $\begin{equation*} \begin{split} s_{2^{n}}(x) &= 1 \\  & + \frac{1}{2^{x}}\\ & + \frac{1}{3^{x}} + \frac{1}{4^{x}} \\ \\ \\ & \dots \\  & + \frac{1}{(2^{nx-x}+1)^x} + \frac{1}{(2^{xn-x}+2)^x}+ \ldots + \frac{1}{2^{nx}} \end{split} \end{equation*}$ From this I can deduce that there are $n+1$ lines and every line has a number of terms of the form $2^{k-1}$ and ends with a term on the form $\frac{1}{2^{kx}}$ (where all the other terms are greater than this, because $x \rightarrow 1^+$). The sum of a line (including the first line) is therefore greater than or equal $2^{k-1}\frac{1}{2^{kx}}$ which is equal to $\frac{1}{2}$ when $x\rightarrow 1^+$ (the first line is $1$) Finally I get: $\frac{1}{2} \cdot (n+1) \leq s_{2^{n}}(x)$ And when $n\rightarrow \infty$ then $s_{2^{n}}(x)\rightarrow \infty$ And I'm done. Question Well. My suspicion is that I have forgotten something, because I havn't used the direction $x\rightarrow 1^+$. Is this a valid proof?","My goal is to prove that: $\displaystyle\sum\limits_{n=1}^\infty \frac{1}{n^{x}} \rightarrow \infty$ when $ x\rightarrow 1^+$ My first approach (which failed) is here: To prove $f(x)\rightarrow \infty$ with a ""home made"" strategy I think the confusing part is the ""$x\rightarrow 1^+$"". I have now argued for the statement, but I haven't used the direction to $1$. I begin with $s_{N}(x) = \displaystyle\sum\limits_{n=1}^N \frac{1}{n^{x}}$ and write $s_{2^{n}}(x)$ as: $\begin{equation*} \begin{split} s_{2^{n}}(x) &= 1 \\  & + \frac{1}{2^{x}}\\ & + \frac{1}{3^{x}} + \frac{1}{4^{x}} \\ \\ \\ & \dots \\  & + \frac{1}{(2^{nx-x}+1)^x} + \frac{1}{(2^{xn-x}+2)^x}+ \ldots + \frac{1}{2^{nx}} \end{split} \end{equation*}$ From this I can deduce that there are $n+1$ lines and every line has a number of terms of the form $2^{k-1}$ and ends with a term on the form $\frac{1}{2^{kx}}$ (where all the other terms are greater than this, because $x \rightarrow 1^+$). The sum of a line (including the first line) is therefore greater than or equal $2^{k-1}\frac{1}{2^{kx}}$ which is equal to $\frac{1}{2}$ when $x\rightarrow 1^+$ (the first line is $1$) Finally I get: $\frac{1}{2} \cdot (n+1) \leq s_{2^{n}}(x)$ And when $n\rightarrow \infty$ then $s_{2^{n}}(x)\rightarrow \infty$ And I'm done. Question Well. My suspicion is that I have forgotten something, because I havn't used the direction $x\rightarrow 1^+$. Is this a valid proof?",,"['sequences-and-series', 'analysis']"
