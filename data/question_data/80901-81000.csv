,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,When is an upper triangular matrix invertible?,When is an upper triangular matrix invertible?,,"I did the following (the exercise assumes $2\times 2$ matrices): $$\begin{pmatrix} {a}&{b}\\  {0}&{d} \end{pmatrix} \begin{pmatrix} {A}&{B}\\  {C}&{D} \end{pmatrix} = \begin{pmatrix} {1}&{0}\\  {0}&{1} \end{pmatrix}$$ And then, for it to have an inverse would be the same of finding $A,B,C,D$ on: \begin{eqnarray*}   {aA+bC}&=&{1} \\    {aB+bD}&=&{0} \\    {dC}&=&{0} \\    {dD}&=&{1}  \end{eqnarray*} Here, If $dC=0$ then $d=0$ or $D=0$, $d\neq 0$ otherwise $dD=0$ then $C=0.$ With this, we found $C$ and we can proceed to: \begin{eqnarray*}   {aA}&=&{1} \\    {aB+bD}&=&{0} \\     {dD}&=&{1}  \end{eqnarray*} And then: $a=\frac{1}{A}$ ,now: $$\begin{eqnarray*}   {\frac{B}{A}+bD}&=&{0} \\     {dD}&=&{1}  \end{eqnarray*}\tag{1}$$ And then, It is invertible if $C=0$ and we can find $A,B,D$ such that $(1)$ is satisfied. Is this correct? I think it's still too complicated and perhaps wrong.","I did the following (the exercise assumes $2\times 2$ matrices): $$\begin{pmatrix} {a}&{b}\\  {0}&{d} \end{pmatrix} \begin{pmatrix} {A}&{B}\\  {C}&{D} \end{pmatrix} = \begin{pmatrix} {1}&{0}\\  {0}&{1} \end{pmatrix}$$ And then, for it to have an inverse would be the same of finding $A,B,C,D$ on: \begin{eqnarray*}   {aA+bC}&=&{1} \\    {aB+bD}&=&{0} \\    {dC}&=&{0} \\    {dD}&=&{1}  \end{eqnarray*} Here, If $dC=0$ then $d=0$ or $D=0$, $d\neq 0$ otherwise $dD=0$ then $C=0.$ With this, we found $C$ and we can proceed to: \begin{eqnarray*}   {aA}&=&{1} \\    {aB+bD}&=&{0} \\     {dD}&=&{1}  \end{eqnarray*} And then: $a=\frac{1}{A}$ ,now: $$\begin{eqnarray*}   {\frac{B}{A}+bD}&=&{0} \\     {dD}&=&{1}  \end{eqnarray*}\tag{1}$$ And then, It is invertible if $C=0$ and we can find $A,B,D$ such that $(1)$ is satisfied. Is this correct? I think it's still too complicated and perhaps wrong.",,"['linear-algebra', 'matrices']"
1,Find the eigenvalues of $A^{3}$,Find the eigenvalues of,A^{3},"Given the matrix  \begin{bmatrix}     4       & 0 & 1 \\     -2       & 1 & 0 \\     -2       & 0 & 1 \end{bmatrix} I have found the eigenvalues to be $\lambda_{1} = 1$, $\lambda_{2} = 2$, $\lambda_{3} = 3$. How do I find the eigenvalues of $A^{3}$?","Given the matrix  \begin{bmatrix}     4       & 0 & 1 \\     -2       & 1 & 0 \\     -2       & 0 & 1 \end{bmatrix} I have found the eigenvalues to be $\lambda_{1} = 1$, $\lambda_{2} = 2$, $\lambda_{3} = 3$. How do I find the eigenvalues of $A^{3}$?",,"['linear-algebra', 'matrices', 'eigenvalues-eigenvectors']"
2,Classify orbits of conjugating action on $\mathrm{GL}_2(\mathbb{C})$,Classify orbits of conjugating action on,\mathrm{GL}_2(\mathbb{C}),"We have the general linear group $\mathrm{GL}_{2}(\mathbb{C})$ that acts on the set $\mathrm{M}_2(\mathbb{C})$ of $(2 \times 2)$ -matrices by conjugation. I want to classify the orbits of this action. What I know: $$   \mathrm{Orb}_{G}(A) = \{ BAB^{-1} : B \in \mathrm{GL}_2(\mathbb{C})\} \,. $$",We have the general linear group that acts on the set of -matrices by conjugation. I want to classify the orbits of this action. What I know:,"\mathrm{GL}_{2}(\mathbb{C}) \mathrm{M}_2(\mathbb{C}) (2 \times 2) 
  \mathrm{Orb}_{G}(A) = \{ BAB^{-1} : B \in \mathrm{GL}_2(\mathbb{C})\} \,.
","['linear-algebra', 'matrices', 'group-theory', 'group-actions']"
3,How $a+bi$ becomes $\left(\matrix{a & -b\\b & a}\right)$? [duplicate],How  becomes ? [duplicate],a+bi \left(\matrix{a & -b\\b & a}\right),This question already has answers here : Why is the complex number $z=a+bi$ equivalent to the matrix form $\left(\begin{smallmatrix}a &-b\\b&a\end{smallmatrix}\right)$ [duplicate] (8 answers) Closed 8 years ago . How do you change something into a matrix? E.g) How does $2+3i$ become $\left(\matrix{2 & -3\\3 & 2}\right)$ Is  there a specific rule for changing an input into a matrix that has to be learned? Or how is it done? Thank you.,This question already has answers here : Why is the complex number $z=a+bi$ equivalent to the matrix form $\left(\begin{smallmatrix}a &-b\\b&a\end{smallmatrix}\right)$ [duplicate] (8 answers) Closed 8 years ago . How do you change something into a matrix? E.g) How does $2+3i$ become $\left(\matrix{2 & -3\\3 & 2}\right)$ Is  there a specific rule for changing an input into a matrix that has to be learned? Or how is it done? Thank you.,,"['matrices', 'complex-numbers']"
4,"For $n\times n$ matrices, is it true that $AB=CD\implies AEB=CED$?","For  matrices, is it true that ?",n\times n AB=CD\implies AEB=CED,"If $A,B,C,D,E$ are $n\times n$ matrices, does $AB=CD$ imply $AEB=CED$? I only know that $AB=CD \implies ABE=CDE$, but I don't see how you can sandwhich $E$ within it. Also, if $AB=CD=0$, does $\det(AB)=\det(CD)=0$? I think this should be true because $AB$ and $CD$ are the same matrices and $\det(0)=0$","If $A,B,C,D,E$ are $n\times n$ matrices, does $AB=CD$ imply $AEB=CED$? I only know that $AB=CD \implies ABE=CDE$, but I don't see how you can sandwhich $E$ within it. Also, if $AB=CD=0$, does $\det(AB)=\det(CD)=0$? I think this should be true because $AB$ and $CD$ are the same matrices and $\det(0)=0$",,"['matrices', 'determinant']"
5,Prove that $|GL_n(\mathbb{F})|< q^{n^2}$.,Prove that .,|GL_n(\mathbb{F})|< q^{n^2},"Let $\Bbb F$ be a finite field, say $|\Bbb F|=q$; then we know that $|GL_n(\Bbb F)| < \infty$. But how can we prove that $|GL_n(\mathbb{F})|< q^{n^2}$? I'm guessing because there $n^2$ entries in a $n\times n$ matrix. A little background would be helpful for me about the proof as I am inexperience with algebra proofs.","Let $\Bbb F$ be a finite field, say $|\Bbb F|=q$; then we know that $|GL_n(\Bbb F)| < \infty$. But how can we prove that $|GL_n(\mathbb{F})|< q^{n^2}$? I'm guessing because there $n^2$ entries in a $n\times n$ matrix. A little background would be helpful for me about the proof as I am inexperience with algebra proofs.",,"['linear-algebra', 'abstract-algebra', 'matrices', 'group-theory', 'finite-groups']"
6,Why does $\frac{1}{{\left\| {\left| {{A^{ - 1}}} \right|} \right\|}} \le \left\| {\left| B \right|} \right\|$?,Why does ?,\frac{1}{{\left\| {\left| {{A^{ - 1}}} \right|} \right\|}} \le \left\| {\left| B \right|} \right\|,"Let $A,B \in {M_n}$ suppose that the following statements are true: $A$ is nonsingular, $A+B$ is singular, $\left\| {\left| . \right|} \right\|$ is matrix norm. Why is it true that: $\frac{1}{{\left\| {\left| {{A^{ - 1}}} \right|} \right\|}} \le \left\| {\left| B \right|} \right\|$?","Let $A,B \in {M_n}$ suppose that the following statements are true: $A$ is nonsingular, $A+B$ is singular, $\left\| {\left| . \right|} \right\|$ is matrix norm. Why is it true that: $\frac{1}{{\left\| {\left| {{A^{ - 1}}} \right|} \right\|}} \le \left\| {\left| B \right|} \right\|$?",,"['linear-algebra', 'matrices']"
7,Proving that matrix in equation is invertible,Proving that matrix in equation is invertible,,"The $2 \times 2$ matrix ${A}$ satisfies ${A}^2 - 4 {A} - 7 {I} = {0}$ where ${I}$ is the $2 \times 2$ identity matrix. Prove that ${A}$ is invertible. I have tried to solve it like a quadratic, but that doesn't work.  Any help is appreciated!","The $2 \times 2$ matrix ${A}$ satisfies ${A}^2 - 4 {A} - 7 {I} = {0}$ where ${I}$ is the $2 \times 2$ identity matrix. Prove that ${A}$ is invertible. I have tried to solve it like a quadratic, but that doesn't work.  Any help is appreciated!",,['matrices']
8,Why if the columns of a matrix are not linearly independent the matrix is not invertible?,Why if the columns of a matrix are not linearly independent the matrix is not invertible?,,"Why if the columns of a matrix are not linearly independent the matrix is not invertible? I have watched this video about eigenvalues and eigenvectors by Sal from Khan Academy, where he says that for $\lambda$ to be an eigenvalue for the matrix $A$, the following must be true $$A \cdot \vec{v} = \lambda \cdot \vec{v} \\ \vec{0} = \lambda \cdot \vec{v} - A \cdot \vec{v} \\ \vec{0} = (\lambda - A )\cdot \vec{v} \\ \vec{0} = (\lambda \cdot I - A )\cdot \vec{v}$$ and the determinant of $(\lambda \cdot I - A )$ must be $0$, or in other words $(\lambda \cdot I - A )$ is not invertible, or in other words the columns of $(\lambda \cdot I - A )$ are linearly dependent, or the nullspace of $(\lambda \cdot I - A )$ is non trivial. Could someone explain me better these statements? What's the relation between a statement and the other? I understood some stuff, but some other clarifications might help too.","Why if the columns of a matrix are not linearly independent the matrix is not invertible? I have watched this video about eigenvalues and eigenvectors by Sal from Khan Academy, where he says that for $\lambda$ to be an eigenvalue for the matrix $A$, the following must be true $$A \cdot \vec{v} = \lambda \cdot \vec{v} \\ \vec{0} = \lambda \cdot \vec{v} - A \cdot \vec{v} \\ \vec{0} = (\lambda - A )\cdot \vec{v} \\ \vec{0} = (\lambda \cdot I - A )\cdot \vec{v}$$ and the determinant of $(\lambda \cdot I - A )$ must be $0$, or in other words $(\lambda \cdot I - A )$ is not invertible, or in other words the columns of $(\lambda \cdot I - A )$ are linearly dependent, or the nullspace of $(\lambda \cdot I - A )$ is non trivial. Could someone explain me better these statements? What's the relation between a statement and the other? I understood some stuff, but some other clarifications might help too.",,['matrices']
9,Proving $\mathrm{SL}_2(\mathbb{R})\trianglelefteq\mathrm{GL}_2(\mathbb{R})$,Proving,\mathrm{SL}_2(\mathbb{R})\trianglelefteq\mathrm{GL}_2(\mathbb{R}),"I've been struggling to show that $\mathrm{SL}_2(\mathbb{R})$ is a normal subgroup of $\mathrm{GL}_2(\mathbb{R})$. I already proved that $\mathrm{SL}_2(\mathbb{R})\leq\mathrm{GL}_2(\mathbb{R})$ (not shown). Now I want to show that $$ A\cdot \mathrm{SL}_2(\mathbb{R})=\mathrm{SL}_2(\mathbb{R})\cdot A  $$ for every $A\in \mathrm{GL}_2(\mathbb{R})$. I know that $\det(AB)=\det(A)\det(B)=\det(B)\det(A)=\det(BA)$. Thus, $$\det(A\cdot \mathrm{SL}_2(\mathbb{R}))=\det(\mathrm{SL}_2(\mathbb{R})\cdot A )$$ but this does not seem to help me prove normality. I thought that perhaps rearranging in the following form would help: $$ A\cdot \mathrm{SL}_2(\mathbb{R})\cdot A^{-1}=\mathrm{SL}_2(\mathbb{R}) $$ If I can show that $A\cdot \mathrm{SL}_2(\mathbb{R})\cdot A^{-1}$ has determinant 1, then I am done. How can I do this? I would like a hint (no full solutions, please) on how I can proceed. Thanks!","I've been struggling to show that $\mathrm{SL}_2(\mathbb{R})$ is a normal subgroup of $\mathrm{GL}_2(\mathbb{R})$. I already proved that $\mathrm{SL}_2(\mathbb{R})\leq\mathrm{GL}_2(\mathbb{R})$ (not shown). Now I want to show that $$ A\cdot \mathrm{SL}_2(\mathbb{R})=\mathrm{SL}_2(\mathbb{R})\cdot A  $$ for every $A\in \mathrm{GL}_2(\mathbb{R})$. I know that $\det(AB)=\det(A)\det(B)=\det(B)\det(A)=\det(BA)$. Thus, $$\det(A\cdot \mathrm{SL}_2(\mathbb{R}))=\det(\mathrm{SL}_2(\mathbb{R})\cdot A )$$ but this does not seem to help me prove normality. I thought that perhaps rearranging in the following form would help: $$ A\cdot \mathrm{SL}_2(\mathbb{R})\cdot A^{-1}=\mathrm{SL}_2(\mathbb{R}) $$ If I can show that $A\cdot \mathrm{SL}_2(\mathbb{R})\cdot A^{-1}$ has determinant 1, then I am done. How can I do this? I would like a hint (no full solutions, please) on how I can proceed. Thanks!",,"['abstract-algebra', 'group-theory', 'matrices']"
10,At least one diagonal element of any real symmetric matrix of rank $1$ is non-zero ?,At least one diagonal element of any real symmetric matrix of rank  is non-zero ?,1,If $A$ is a real symmetric matrix of rank $1$ then is it true that at least one diagonal element is  non-zero ?,If $A$ is a real symmetric matrix of rank $1$ then is it true that at least one diagonal element is  non-zero ?,,['matrices']
11,"If A is normal, then the nullspace of A is the nullspace of A*","If A is normal, then the nullspace of A is the nullspace of A*",,"Suppose $A$ is a normal matrix. Prove that $x$ is in the nullspace of $A$ if and only if $x$ is in the nullspace of $A^{*}$. This isn't a homework problem. It was on a test I took recently, and I'd like to know if I was on the right track in solving it.","Suppose $A$ is a normal matrix. Prove that $x$ is in the nullspace of $A$ if and only if $x$ is in the nullspace of $A^{*}$. This isn't a homework problem. It was on a test I took recently, and I'd like to know if I was on the right track in solving it.",,"['linear-algebra', 'matrices']"
12,Linear operator with same representation in any basis,Linear operator with same representation in any basis,,"I'm trying to recall a question from a past exam to review for an upcoming exam; I think it went like this: Suppose a finite-dimensional linear operator $T:V \to V$ has the same matrix representation in every basis. Show that $T$ must be a scalar multiple of the identity transformation. First, does it sound like my recollection of the problem is correct? Second, any suggestions on how to approach a proof?","I'm trying to recall a question from a past exam to review for an upcoming exam; I think it went like this: Suppose a finite-dimensional linear operator $T:V \to V$ has the same matrix representation in every basis. Show that $T$ must be a scalar multiple of the identity transformation. First, does it sound like my recollection of the problem is correct? Second, any suggestions on how to approach a proof?",,"['linear-algebra', 'matrices']"
13,Proof that a matrix is nonsingular,Proof that a matrix is nonsingular,,"Let $A$ be $n \times n$ matrix. Show that if $A^2 = 0$, then $I - A$ is non-singular and $(I-A)^{-1} = I+A$. The second part is easy for me, but how can I show that if $A^2 = 0$, then $I - A$ is non-singular. I found in Wolfram Alpha that ""A matrix is singular iff its determinant is 0."", but how I can relate this to the given $A^2 = 0$. or is there another easier way. Thank you for your help in advance.","Let $A$ be $n \times n$ matrix. Show that if $A^2 = 0$, then $I - A$ is non-singular and $(I-A)^{-1} = I+A$. The second part is easy for me, but how can I show that if $A^2 = 0$, then $I - A$ is non-singular. I found in Wolfram Alpha that ""A matrix is singular iff its determinant is 0."", but how I can relate this to the given $A^2 = 0$. or is there another easier way. Thank you for your help in advance.",,"['linear-algebra', 'matrices']"
14,Why do Matrices work the way they do? [duplicate],Why do Matrices work the way they do? [duplicate],,This question already has answers here : Matrix multiplication: interpreting and understanding the process (2 answers) Closed 7 years ago . You get taught about matrices and how they work but nobody ever tells you WHY they work in the way that they do. What was the idea that sparked the creation of matrices?,This question already has answers here : Matrix multiplication: interpreting and understanding the process (2 answers) Closed 7 years ago . You get taught about matrices and how they work but nobody ever tells you WHY they work in the way that they do. What was the idea that sparked the creation of matrices?,,"['matrices', 'soft-question']"
15,A $2011\times 2011$ real symmetric matrix,A  real symmetric matrix,2011\times 2011,"Prove that if a $2011\times 2011$ real symmetric matrix $A$ satisfies $A^{2011}=0$, then $A=0$ What I know is that a real symmetric matrix always has real eigenvalue. Can you help me?","Prove that if a $2011\times 2011$ real symmetric matrix $A$ satisfies $A^{2011}=0$, then $A=0$ What I know is that a real symmetric matrix always has real eigenvalue. Can you help me?",,"['linear-algebra', 'matrices', 'matrix-equations']"
16,Leibniz formula and determinants,Leibniz formula and determinants,,"Let $A \in M_n(\mathbb C)$ , then $$\det(A) = \sum_{\sigma \in S_n} \mbox{sign}(\sigma) a_{1\sigma(1)} a_{2\sigma(2)} \cdots a_{n\sigma(n)} = \sum_{\sigma \in S_n}\prod_{i=1}^n a_{1\sigma(i)}$$ I looked at different resources to try and understand how the formula works, but it hasn't really clicked with me yet. I even read through Understanding the Leibniz formula for determinants and it helped a bit, but I still need some clarification. What throws me off the most are the subscripts for the elements in $A$ , and I don't understand how you know which elements to multiply together. If anyone can break it down into simpler terms, that would be greatly appreciated. Thanks!","Let , then I looked at different resources to try and understand how the formula works, but it hasn't really clicked with me yet. I even read through Understanding the Leibniz formula for determinants and it helped a bit, but I still need some clarification. What throws me off the most are the subscripts for the elements in , and I don't understand how you know which elements to multiply together. If anyone can break it down into simpler terms, that would be greatly appreciated. Thanks!",A \in M_n(\mathbb C) \det(A) = \sum_{\sigma \in S_n} \mbox{sign}(\sigma) a_{1\sigma(1)} a_{2\sigma(2)} \cdots a_{n\sigma(n)} = \sum_{\sigma \in S_n}\prod_{i=1}^n a_{1\sigma(i)} A,"['linear-algebra', 'matrices', 'determinant']"
17,$A^3 = I$. Find the possible Jordan Forms???,. Find the possible Jordan Forms???,A^3 = I,"If $A^3 = I$, then I want to find the possible Jordan forms of the matrix.  Since the minimal polynomial has degree at most three, each block is at most 3, and the eigenvalues are third roots of unity. Is that the answer, or are there some other components I am missing?","If $A^3 = I$, then I want to find the possible Jordan forms of the matrix.  Since the minimal polynomial has degree at most three, each block is at most 3, and the eigenvalues are third roots of unity. Is that the answer, or are there some other components I am missing?",,['linear-algebra']
18,Find the inverse of a matrix with a variable,Find the inverse of a matrix with a variable,,"$$X= \begin{pmatrix} 2-n & 1 & 1 & 1 & \ldots & 1 & 1 \\ 1 & 2-n & 1 & 1 & \ldots & 1 & 1 \\ \vdots & \vdots & \vdots & \vdots & \ddots & \vdots & \vdots \\ 1 & 1 & 1 & 1 & \ldots & 2-n & 1 \\ 1 & 1 & 1 & 1 & \ldots & 1 & 2-n\end{pmatrix}_{n\times n} $$ Which means that the matrix with the size of $n\times n$ have $n-2$ along the diagonal and $1$ everywhere else.  Help please, I am stuck with this problem.","$$X= \begin{pmatrix} 2-n & 1 & 1 & 1 & \ldots & 1 & 1 \\ 1 & 2-n & 1 & 1 & \ldots & 1 & 1 \\ \vdots & \vdots & \vdots & \vdots & \ddots & \vdots & \vdots \\ 1 & 1 & 1 & 1 & \ldots & 2-n & 1 \\ 1 & 1 & 1 & 1 & \ldots & 1 & 2-n\end{pmatrix}_{n\times n} $$ Which means that the matrix with the size of $n\times n$ have $n-2$ along the diagonal and $1$ everywhere else.  Help please, I am stuck with this problem.",,"['matrices', 'inverse']"
19,Solve $x^2 = I_2$ where x is a 2 by 2 matrix,Solve  where x is a 2 by 2 matrix,x^2 = I_2,"I tried a basic approach and wrote x as a matrix of four unknown elements $\begin{pmatrix} a && b \\ c && d \end{pmatrix}$ and squared it when I obtained $\begin{pmatrix} a^2 + bc && ab + bd \\  ca + dc && cd + d^2\end{pmatrix}$ and by making it equal with $I_2$ I got the following system $a^2 + bc = 1$ $ab +bd = 0$ $ca + dc = 0$ $cd + d^2 = 1$ I don't know how to proceed. (Also, if anyone knows of a better or simpler way of solving this matrix equation I'd be more than happy to know).","I tried a basic approach and wrote x as a matrix of four unknown elements $\begin{pmatrix} a && b \\ c && d \end{pmatrix}$ and squared it when I obtained $\begin{pmatrix} a^2 + bc && ab + bd \\  ca + dc && cd + d^2\end{pmatrix}$ and by making it equal with $I_2$ I got the following system $a^2 + bc = 1$ $ab +bd = 0$ $ca + dc = 0$ $cd + d^2 = 1$ I don't know how to proceed. (Also, if anyone knows of a better or simpler way of solving this matrix equation I'd be more than happy to know).",,"['linear-algebra', 'matrices']"
20,Show that $A$ is similar to a diagonal matrix iff $b=c=d=e=f=g=0$,Show that  is similar to a diagonal matrix iff,A b=c=d=e=f=g=0,"Show that $A$ is similar to a diagonal matrix iff $$b=c=d=e=f=g=0$$  $$A= \left(\begin{array}{cccc}a & b & c & d \\ 0 & a & e & f\\ 0 & 0 & a & g\\ 0 & 0 & 0 & a \end{array}\right)$$ Attempt : If $$b=c=d=e=f=g=0$$, then clearly $A$ is similar to a diagonal matrix. $$A = I^{-1}AI$$ Conversely, if $A$ is similar to a diagonal matrix, then $$A=P^{-1}DP$$ where $P$ is non-singular. Suppose $$b,c,d,e,f,g \neq 0$$ We know that the only eigenvalue of $A$ is $a$. To be diagonalizable, we know that $A$ must have 4 eigenvectors. With the assumption of $$b,c,d,e,f,g \neq 0$$ we get that the only eigenvector w.r.t. the eigenvalue $$\lambda=a$$ is $$(1,0,0,0)$$ This contradicts the fact that $A$ is diagonalizable. Hence $$b=c=d=e=f=g=0$$ Is the above logic wrong? Is there any other way I can prove this?","Show that $A$ is similar to a diagonal matrix iff $$b=c=d=e=f=g=0$$  $$A= \left(\begin{array}{cccc}a & b & c & d \\ 0 & a & e & f\\ 0 & 0 & a & g\\ 0 & 0 & 0 & a \end{array}\right)$$ Attempt : If $$b=c=d=e=f=g=0$$, then clearly $A$ is similar to a diagonal matrix. $$A = I^{-1}AI$$ Conversely, if $A$ is similar to a diagonal matrix, then $$A=P^{-1}DP$$ where $P$ is non-singular. Suppose $$b,c,d,e,f,g \neq 0$$ We know that the only eigenvalue of $A$ is $a$. To be diagonalizable, we know that $A$ must have 4 eigenvectors. With the assumption of $$b,c,d,e,f,g \neq 0$$ we get that the only eigenvector w.r.t. the eigenvalue $$\lambda=a$$ is $$(1,0,0,0)$$ This contradicts the fact that $A$ is diagonalizable. Hence $$b=c=d=e=f=g=0$$ Is the above logic wrong? Is there any other way I can prove this?",,"['linear-algebra', 'matrices']"
21,Prove that $A^TD-C^TB=I$,Prove that,A^TD-C^TB=I,"Let A,B,C,D be complex matrices $n \times n$ such that $AB^T,CD^T$ are symmetric and $AD^T-BC^T=I$. Prove that $A^TD-C^TB=I$. Can anyone give me any idea? Thank you.","Let A,B,C,D be complex matrices $n \times n$ such that $AB^T,CD^T$ are symmetric and $AD^T-BC^T=I$. Prove that $A^TD-C^TB=I$. Can anyone give me any idea? Thank you.",,['matrices']
22,"Given a $4\times 4$ symmetric matrix, is there an efficient way to find its eigenvalues and diagonalize it?","Given a  symmetric matrix, is there an efficient way to find its eigenvalues and diagonalize it?",4\times 4,I have a $4\times 4$ matrix $$A=\left(\begin{array}{cccc}8 & 11 & 4 & 3\\11 & 12 & 4 & 7\\4 & 4 & 7 & 12\\3 & 7 & 12 & 17\end{array}\right).$$ I want to do the things I describe below. Find the eigenvalues. Find a  unitary matrix $P$ (if there is any) so that the matrix $(P^{-1})AP$ is diagonal. Find (if there are any) an identity matrix $Q$ and an upper triangular matrix $R$ so that $A=QR$ . Comments (item by  item) I want to know if there is a better way than calculating $\det(A-\lambda I)$ . Well for this I think I have the answer as the matrix A is symmetric that means that it has 4 distinct eigenvectors that are orthogonal with each other also P a matrix composed by using the eigenvectors as columns gives us that $(P^{-1})AP$ = with the diagonal form of A. And P is unitary as if we take the inner product of all the eigenvectors with each other we get 0 since they are orthogonal with each other. Is there a flaw to the way i am thinking? I tried to solve this using the Gram–Schmidt process I found the first column of Q but then the numbers get too big and gets hard to compute. I have been thinking maybe symmetric matrices have some better way for QR decomposition,I have a matrix I want to do the things I describe below. Find the eigenvalues. Find a  unitary matrix (if there is any) so that the matrix is diagonal. Find (if there are any) an identity matrix and an upper triangular matrix so that . Comments (item by  item) I want to know if there is a better way than calculating . Well for this I think I have the answer as the matrix A is symmetric that means that it has 4 distinct eigenvectors that are orthogonal with each other also P a matrix composed by using the eigenvectors as columns gives us that = with the diagonal form of A. And P is unitary as if we take the inner product of all the eigenvectors with each other we get 0 since they are orthogonal with each other. Is there a flaw to the way i am thinking? I tried to solve this using the Gram–Schmidt process I found the first column of Q but then the numbers get too big and gets hard to compute. I have been thinking maybe symmetric matrices have some better way for QR decomposition,4\times 4 A=\left(\begin{array}{cccc}8 & 11 & 4 & 3\\11 & 12 & 4 & 7\\4 & 4 & 7 & 12\\3 & 7 & 12 & 17\end{array}\right). P (P^{-1})AP Q R A=QR \det(A-\lambda I) (P^{-1})AP,"['linear-algebra', 'matrices', 'eigenvalues-eigenvectors']"
23,Properties matrix $A^2=I$,Properties matrix,A^2=I,"Let be $A$ a matrix such that $A^2=I$. According my book the $A$'eigenvalues are $-1$ and $+1$?. but, For me is only $+1$, this because $A = \sqrt{I} = I$.","Let be $A$ a matrix such that $A^2=I$. According my book the $A$'eigenvalues are $-1$ and $+1$?. but, For me is only $+1$, this because $A = \sqrt{I} = I$.",,"['linear-algebra', 'matrices']"
24,$2\times 2 $ matrices over $\mathbb{C}$ that satisfy $\mathrm A^3=\mathrm A$,matrices over  that satisfy,2\times 2  \mathbb{C} \mathrm A^3=\mathrm A,Let $\mathrm A \in \mathbb C^{2 \times 2}$. How many $2 \times 2$ matrices $\mathrm A$ satisfy $\mathrm A^{3} = \mathrm A$. Infinitely many? If it is $3 \times 3$ matrix then by applying Cayley-Hamilton theorem I could have said that given matrix is diagonalizable. Also zero is eigenvalue of $\mathrm A$. So it would be collection of all singular diagonalizable matrices. But how to count them? Here I have $2 \times 2$ matrices i feel I can't apply the Cayley-Hamilton Theorem here? I am stuck with these thoughts?,Let $\mathrm A \in \mathbb C^{2 \times 2}$. How many $2 \times 2$ matrices $\mathrm A$ satisfy $\mathrm A^{3} = \mathrm A$. Infinitely many? If it is $3 \times 3$ matrix then by applying Cayley-Hamilton theorem I could have said that given matrix is diagonalizable. Also zero is eigenvalue of $\mathrm A$. So it would be collection of all singular diagonalizable matrices. But how to count them? Here I have $2 \times 2$ matrices i feel I can't apply the Cayley-Hamilton Theorem here? I am stuck with these thoughts?,,"['linear-algebra', 'matrices', 'vector-spaces', 'eigenvalues-eigenvectors']"
25,Sufficient condition for a matrix to be positive definite,Sufficient condition for a matrix to be positive definite,,Is a sufficient condition for a $2\times 2$ matrix $$\left(\begin{array}{cc}a&b\\b&d\end{array}\right)$$ to be positive definite that $a >0$ and $ad > b^2$ ?,Is a sufficient condition for a $2\times 2$ matrix $$\left(\begin{array}{cc}a&b\\b&d\end{array}\right)$$ to be positive definite that $a >0$ and $ad > b^2$ ?,,"['linear-algebra', 'matrices']"
26,What does $M^{-1}RM$ represent?,What does  represent?,M^{-1}RM,"I'm a bit confused about the use of $M^{-1}RM$ where $R$ is a transformation matrix. Actually I was looking at the script here which reads and renders bvh files . But, I could not understand the meaning of line 483 which does similar job (where $M$ is transformation matrix of rest bone and $R$ is rotation matrix of pose bone). Or, can anyone tell me what does $A^{-1}BA$ represent in general? Thanks in advance, Mihir Gokani","I'm a bit confused about the use of $M^{-1}RM$ where $R$ is a transformation matrix. Actually I was looking at the script here which reads and renders bvh files . But, I could not understand the meaning of line 483 which does similar job (where $M$ is transformation matrix of rest bone and $R$ is rotation matrix of pose bone). Or, can anyone tell me what does $A^{-1}BA$ represent in general? Thanks in advance, Mihir Gokani",,"['matrices', 'rotations', 'inverse']"
27,"Is $\det$ of $M_{i,j} = \min(x_i, x_j)^2 \left( 3 \max(x_i, x_j) - \min(x_i, x_j) \right)$ non zero?",Is  of  non zero?,"\det M_{i,j} = \min(x_i, x_j)^2 \left( 3 \max(x_i, x_j) - \min(x_i, x_j) \right)","Let $0<x_1<x_2<...<x_n<1$. Let us consider the matrix M defined such that: $$M_{i,j} = \min(x_i, x_j)^2 \left( 3 \max(x_i, x_j) - \min(x_i, x_j) \right)$$ I believe the determinant of M is different from zero. Any idea on how to prove it ( if true )? Of course, any reason why M would be invertible would suit me as well, no real need to get the formula of the determinant.","Let $0<x_1<x_2<...<x_n<1$. Let us consider the matrix M defined such that: $$M_{i,j} = \min(x_i, x_j)^2 \left( 3 \max(x_i, x_j) - \min(x_i, x_j) \right)$$ I believe the determinant of M is different from zero. Any idea on how to prove it ( if true )? Of course, any reason why M would be invertible would suit me as well, no real need to get the formula of the determinant.",,"['linear-algebra', 'matrices', 'determinant']"
28,The relation between the eigenvalue of a Hermitian matrix and the block matrix that composed by it real and imaginary part,The relation between the eigenvalue of a Hermitian matrix and the block matrix that composed by it real and imaginary part,,"Recently I am reading a paper . In their ""Proof of Lemma 1"" on page 24, they have: $$\lambda_+(\mathbf{Q})=2\lambda_+(\tilde{\mathbf{Q}})$$ where $\mathbf{Q}$ is a Hermtian matrix, $\tilde{\mathbf{Q}}=\frac{1}{2}\left[\begin{array}{cc} \operatorname{Re}\{\mathbf{Q}\} & -\operatorname{Im}\{\mathbf{Q}\} \\ \operatorname{Im}\{\mathbf{Q}\} & \operatorname{Re}\{\mathbf{Q}\} \end{array}\right]$ is a real symmetric matrix, $\lambda_+ = \max\{\lambda_{\max}(\mathbf{-Q},0)\}$ , and $\lambda_{\max}$ denotes the maximum eigenvalue. I haven't figured out why it's that.","Recently I am reading a paper . In their ""Proof of Lemma 1"" on page 24, they have: where is a Hermtian matrix, is a real symmetric matrix, , and denotes the maximum eigenvalue. I haven't figured out why it's that.","\lambda_+(\mathbf{Q})=2\lambda_+(\tilde{\mathbf{Q}}) \mathbf{Q} \tilde{\mathbf{Q}}=\frac{1}{2}\left[\begin{array}{cc}
\operatorname{Re}\{\mathbf{Q}\} & -\operatorname{Im}\{\mathbf{Q}\} \\
\operatorname{Im}\{\mathbf{Q}\} & \operatorname{Re}\{\mathbf{Q}\}
\end{array}\right] \lambda_+ = \max\{\lambda_{\max}(\mathbf{-Q},0)\} \lambda_{\max}","['matrices', 'matrix-decomposition', 'hermitian-matrices', 'skew-symmetric-matrices']"
29,What are the quaternion algebras over $\mathbb{F}$ for a field $\mathbb{F}$?,What are the quaternion algebras over  for a field ?,\mathbb{F} \mathbb{F},I know that the only quaternion algebras over $\mathbb{R}$ are the quaternions and the split-quaternions. What is the characterization of the quaternion algebras over a particular field? Which of these be extended to octonion algebras?,I know that the only quaternion algebras over are the quaternions and the split-quaternions. What is the characterization of the quaternion algebras over a particular field? Which of these be extended to octonion algebras?,\mathbb{R},"['abstract-algebra', 'matrices', 'field-theory', 'quaternions']"
30,"Given $v\in\mathbb R^3$, what are the solutions $A\in\mathbb R^{3\times 3}$ of $A^2=I+vv^{T}$?","Given , what are the solutions  of ?",v\in\mathbb R^3 A\in\mathbb R^{3\times 3} A^2=I+vv^{T},"The following question arose while deriving the explicit form of symmetric Lorentz transformations: Given $v\in\mathbb R^3$ , what can we say about the set \begin{equation} \{A\in\mathbb R^{3\times 3}:A^2=I+vv^{T}\}\  \end{equation} (with $I\in\mathbb R^{3\times 3}$ being the identity matrix)? Are there any analogies to the case where $A$ and $v$ are real numbers, i.e. are there exactly two solutions which differ by a minus sign?","The following question arose while deriving the explicit form of symmetric Lorentz transformations: Given , what can we say about the set (with being the identity matrix)? Are there any analogies to the case where and are real numbers, i.e. are there exactly two solutions which differ by a minus sign?","v\in\mathbb R^3 \begin{equation}
\{A\in\mathbb R^{3\times 3}:A^2=I+vv^{T}\}\ 
\end{equation} I\in\mathbb R^{3\times 3} A v","['linear-algebra', 'matrices']"
31,Proving $(A+B)^{-1} = A^{-1} + B^{-1}$ when there exists $J$ so that $J^2 = -I$,Proving  when there exists  so that,(A+B)^{-1} = A^{-1} + B^{-1} J J^2 = -I,"I'm having trouble proving this biconditional statement: Prove that there exist $A, B \in \mathcal{M}_{n\times n}(\mathbb{R}^n)$ such that $(A+B)^{-1} = A^{-1} + B^{-1}$ if and only if there exists $J \in \mathcal{M}_{n \times n}(\mathbb{R}^n)$ such that $J^2 = -I_n$ . Here, $I_n$ is the $n \times n$ identity matrix. So far, none of my ideas have panned out and I don't feel like I have enough information to solve this problem. Any hints would be appreciated!","I'm having trouble proving this biconditional statement: Prove that there exist such that if and only if there exists such that . Here, is the identity matrix. So far, none of my ideas have panned out and I don't feel like I have enough information to solve this problem. Any hints would be appreciated!","A, B \in \mathcal{M}_{n\times n}(\mathbb{R}^n) (A+B)^{-1} = A^{-1} + B^{-1} J \in \mathcal{M}_{n \times n}(\mathbb{R}^n) J^2 = -I_n I_n n \times n","['linear-algebra', 'matrices', 'inverse']"
32,$\lVert A^{-1} \rVert \ge \frac{1}{\lVert A-B \rVert}$ for norm $\lVert•\rVert$ and nonsingular (matrix) $A$ and singular $B$?,for norm  and nonsingular (matrix)  and singular ?,\lVert A^{-1} \rVert \ge \frac{1}{\lVert A-B \rVert} \lVert•\rVert A B,"I have a question. For any norm $\lVert•\rVert$ and any nonsingular matrix $A$ and singular matrix $B$ , $$\lVert A^{-1} \rVert \ge \frac{1}{\lVert A-B \rVert}$$ How can we show that?","I have a question. For any norm and any nonsingular matrix and singular matrix , How can we show that?",\lVert•\rVert A B \lVert A^{-1} \rVert \ge \frac{1}{\lVert A-B \rVert},"['linear-algebra', 'matrices', 'numerical-methods', 'normed-spaces']"
33,What are some cool implications of the Perron-Fronenius Theorem?,What are some cool implications of the Perron-Fronenius Theorem?,,I’m doing a 15 minute presentation on The Perron-Frobenius Theorem. Does anyone know any nice results or examples I can include in it?,I’m doing a 15 minute presentation on The Perron-Frobenius Theorem. Does anyone know any nice results or examples I can include in it?,,"['linear-algebra', 'matrices', 'soft-question']"
34,what is Explicit And Implicit Qr Algorithms For Symmetric And Non-symmetric Matrices?,what is Explicit And Implicit Qr Algorithms For Symmetric And Non-symmetric Matrices?,,"I thought that QR algorithm decomposes a matrix into an orthogonal matrix Q and a upper triangular matrix R using GramSchmidth process for singular matrices but, what  is meant by Explicit and Implicit QR algorithms? and how will they help in decomposing a non-singular matrix?","I thought that QR algorithm decomposes a matrix into an orthogonal matrix Q and a upper triangular matrix R using GramSchmidth process for singular matrices but, what  is meant by Explicit and Implicit QR algorithms? and how will they help in decomposing a non-singular matrix?",,"['linear-algebra', 'matrices', 'linear-programming', 'symmetric-matrices', 'gram-schmidt']"
35,Reordering of matrix multiplication,Reordering of matrix multiplication,,"I want to compute the matrix multiplication $ABA$ , where $A$ and $B$ are real and orthogonal matrices. In fact, they are specifically $3\times3$ rotation matrices.  However, it is much easier if I can reverse the order of $BA$ somehow, because I can then perform the multiplication much easier. I know that matrix multiplication is not commutative, however, I am asking because both $A$ and $B$ are orthogonal matrices, and hopefully, may be there is some trick to utilize their orthogonality to reorder the product. I tried to solve this, but got stuck here: $$ ABA = A((BA)^{-1})^{-1}=A(A^{-1}B^{-1})^{-1} $$ Is there a way to proceed from here? Edit: I also know that matrix multiplication is associative; however, I am not after associativity here. I want to multiply the $A$ matrix by $A$ (or by its inverse/transpose), then multiply the result by $B$ . Edit: To put this into context, consider the following product of rotation matrices $$ R_x(\theta)R_z(\pi)R_x(\theta) $$ where, $R_x(\theta)$ is the rotation matrix about the $x$ -axis by $\theta$ , and $R_z(\pi)$ is the rotation matrix about the $z$ -axis by $\pi$ . This product simplifies to $R_z(\pi)$ . Is it possible to come to this conclusion without carrying out the matrix multiplication of the three rotation matrices? It looks that the $R_x(\theta)$ got cancelled somehow.","I want to compute the matrix multiplication , where and are real and orthogonal matrices. In fact, they are specifically rotation matrices.  However, it is much easier if I can reverse the order of somehow, because I can then perform the multiplication much easier. I know that matrix multiplication is not commutative, however, I am asking because both and are orthogonal matrices, and hopefully, may be there is some trick to utilize their orthogonality to reorder the product. I tried to solve this, but got stuck here: Is there a way to proceed from here? Edit: I also know that matrix multiplication is associative; however, I am not after associativity here. I want to multiply the matrix by (or by its inverse/transpose), then multiply the result by . Edit: To put this into context, consider the following product of rotation matrices where, is the rotation matrix about the -axis by , and is the rotation matrix about the -axis by . This product simplifies to . Is it possible to come to this conclusion without carrying out the matrix multiplication of the three rotation matrices? It looks that the got cancelled somehow.","ABA A B 3\times3 BA A B 
ABA = A((BA)^{-1})^{-1}=A(A^{-1}B^{-1})^{-1}
 A A B 
R_x(\theta)R_z(\pi)R_x(\theta)
 R_x(\theta) x \theta R_z(\pi) z \pi R_z(\pi) R_x(\theta)","['linear-algebra', 'matrices', 'orthogonal-matrices']"
36,Linear Independence for Vectors of Cosine Values,Linear Independence for Vectors of Cosine Values,,"For every integer $n\geq3$ define an $n\times n$ matrix $A$ using the following row vector as the first row: $(c_0,c_1,c_2,\ldots, c_{n-1})$ where $c_k=\cos (2\pi k/n)$ . The subsequent rows are obtained as cyclic shifts. This matrix seems to be always of rank 2. Actually I was working with some families of  representations of dihedral groups, arising in another context and was computing the degrees of the representations. That calculation implied this matrix should be of rank 2. But definitely there must be some direct way of doing it. Can anyone tell me how to accomplish it?","For every integer define an matrix using the following row vector as the first row: where . The subsequent rows are obtained as cyclic shifts. This matrix seems to be always of rank 2. Actually I was working with some families of  representations of dihedral groups, arising in another context and was computing the degrees of the representations. That calculation implied this matrix should be of rank 2. But definitely there must be some direct way of doing it. Can anyone tell me how to accomplish it?","n\geq3 n\times n A (c_0,c_1,c_2,\ldots, c_{n-1}) c_k=\cos (2\pi k/n)","['matrices', 'trigonometry']"
37,"Let $A$ be a $n\times n$ matrix such that $a_{ij}=ij$,find the eigenvalues of the matrix $A$.","Let  be a  matrix such that ,find the eigenvalues of the matrix .",A n\times n a_{ij}=ij A,"Let $A$ be a $n\times n$ matrix such that $a_{ij}=ij$ . I want to find the eigenvalues of the matrix $A$ . Efforts: $n=2$ , I found that eigenvalues are equal to $0,5$ . For $n=3$ , I found eigenvalues are equal to $0,0,14$ . For $n=4$ I found eigenvalues are equal to $0,0,0,30$ . For $n=5$ I found eigenvalues are equal to $0,0,0,0,0,55$ . It looks like there is a pattern here. Eigenvalues of $n\times n$ matrix seems to be $\sum_{i=1}^n i^2, \underbrace{0,0,0,..0}_{n-1\text{ times}}$ . But how can I prove it? I mean whether there are some beautiful arguments that makes things easy, and ideas clear. Thanks for reading and helping out.","Let be a matrix such that . I want to find the eigenvalues of the matrix . Efforts: , I found that eigenvalues are equal to . For , I found eigenvalues are equal to . For I found eigenvalues are equal to . For I found eigenvalues are equal to . It looks like there is a pattern here. Eigenvalues of matrix seems to be . But how can I prove it? I mean whether there are some beautiful arguments that makes things easy, and ideas clear. Thanks for reading and helping out.","A n\times n a_{ij}=ij A n=2 0,5 n=3 0,0,14 n=4 0,0,0,30 n=5 0,0,0,0,0,55 n\times n \sum_{i=1}^n i^2, \underbrace{0,0,0,..0}_{n-1\text{ times}}","['linear-algebra', 'matrices']"
38,"Let $A\in M_n(\mathbb{Q})$ with $A^k=I_n$. If $j$ is a positive integer with $\gcd(j,k)=1$, show that $ \operatorname{tr}(A)= \operatorname{tr}(A^j)$. [closed]","Let  with . If  is a positive integer with , show that . [closed]","A\in M_n(\mathbb{Q}) A^k=I_n j \gcd(j,k)=1  \operatorname{tr}(A)= \operatorname{tr}(A^j)","Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 5 years ago . Improve this question Let $A\in M_n(\mathbb{Q})$ with $A^k=I_n$ . If $j$ is a positive      integer with $\gcd(j,k)=1$ , show that $ \operatorname{tr}(A)=    \operatorname{tr}(A^j)$ . I don't know how to start to prove that. I tried to find the matrix $B$ similar with $A$ , but I am stuck... Thank you.","Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 5 years ago . Improve this question Let with . If is a positive      integer with , show that . I don't know how to start to prove that. I tried to find the matrix similar with , but I am stuck... Thank you.","A\in M_n(\mathbb{Q}) A^k=I_n j \gcd(j,k)=1  \operatorname{tr}(A)=
   \operatorname{tr}(A^j) B A",['linear-algebra']
39,Find the farthest rotation matrix in $\mathrm{SO}(3)$ from a given matrix.,Find the farthest rotation matrix in  from a given matrix.,\mathrm{SO}(3),"Consider the norm $\| A \| = \sqrt{\mathrm{tr}(AA^t)}$ . It's easy to see that $\mathrm{SO}(3)$ is a compact subspace of $3 \times 3$ matrices in the topology induced by this norm because $\mathrm{O}(3)$ is compact and $SO(3)$ being the inverse image of $\{1\}$ under the map $\mathrm{det}$ is a closed subset of $\mathrm{O}(n)$ . So, it makes sense to talk about the nearest rotation and the farthest rotation matrix from a given matrix. The former one, the nearest one, has been discussed online and I could find a lot of information about it by Googling. However, the farthest rotation matrix was not discussed. Out of curiosity, is it possible to find the farthest rotation matrix to a given matrix? I tried to solve the problem using Lagrange multipliers but I didn't know how to proceed because I'm not good at matrix calculus.","Consider the norm . It's easy to see that is a compact subspace of matrices in the topology induced by this norm because is compact and being the inverse image of under the map is a closed subset of . So, it makes sense to talk about the nearest rotation and the farthest rotation matrix from a given matrix. The former one, the nearest one, has been discussed online and I could find a lot of information about it by Googling. However, the farthest rotation matrix was not discussed. Out of curiosity, is it possible to find the farthest rotation matrix to a given matrix? I tried to solve the problem using Lagrange multipliers but I didn't know how to proceed because I'm not good at matrix calculus.",\| A \| = \sqrt{\mathrm{tr}(AA^t)} \mathrm{SO}(3) 3 \times 3 \mathrm{O}(3) SO(3) \{1\} \mathrm{det} \mathrm{O}(n),"['linear-algebra', 'matrices', 'optimization', 'numerical-linear-algebra', 'matrix-calculus']"
40,Can a non-invertible matrix be extended to an invertible one?,Can a non-invertible matrix be extended to an invertible one?,,"For a square matrix $M$ call any square matrix M' of the form  $$\left(\begin{array}{cc} M & A\\ B & C \end{array}\right)$$ an extension of $M$. Does it follow that if $M$ is not invertible that all extensions $M'$ are not invertible? I believe the answer is no. If not, is there an extension that is invertible? Can we prove that there always is?","For a square matrix $M$ call any square matrix M' of the form  $$\left(\begin{array}{cc} M & A\\ B & C \end{array}\right)$$ an extension of $M$. Does it follow that if $M$ is not invertible that all extensions $M'$ are not invertible? I believe the answer is no. If not, is there an extension that is invertible? Can we prove that there always is?",,"['linear-algebra', 'matrices', 'linear-transformations']"
41,Find a matrix A such that $\operatorname{rank}{A} = \operatorname{rank}{A^2} \neq \operatorname{rank}{A^3}$,Find a matrix A such that,\operatorname{rank}{A} = \operatorname{rank}{A^2} \neq \operatorname{rank}{A^3},"Let $A$ be a complex square matrix of order 2 ($A \in M_{2,2}$). Then, does there exist $A$ such that $\operatorname{rank}{A} = \operatorname{rank}{A^2} \neq \operatorname{rank}{A^3}$? If that doesn't exist, how can I prove it?","Let $A$ be a complex square matrix of order 2 ($A \in M_{2,2}$). Then, does there exist $A$ such that $\operatorname{rank}{A} = \operatorname{rank}{A^2} \neq \operatorname{rank}{A^3}$? If that doesn't exist, how can I prove it?",,"['matrices', 'matrix-rank']"
42,The isotropic cone of a positive semi-definite bilinear is a subspace of $\Bbb R^n$.,The isotropic cone of a positive semi-definite bilinear is a subspace of .,\Bbb R^n,"Let $A \in \Bbb R^n \times \Bbb R^n$ be a symmetric and positive semidefinite matrix. Let $q(x) =x^\top Ax$ and let the isotropic cone be defined as $$W= \{x ; q(x) = 0   \}$$ I want to show that $W$ is a subspace of $\Bbb R^n$. My closest attempt : Take $x$ and $y$ in $W$ (the zero vector always belongs to $W$ and is a subspace of $\Bbb R^n$). let $\alpha, \beta \in\Bbb R $ It is known that $$q(\alpha x- \beta y) = -2 \alpha \beta x^\top Ay + \alpha^2 q(x) + \beta^2 q(y) = -2 \alpha \beta  x^\top Ay$$ So ignoring constants we are done if $x^\top Ay = 0$ for $x,y \in W$. By the spectral theorem we have that $A = UDU^\top$ where $D$ is diagonal and $U$ is unitary. This means that  $$ x^\top Ay = (U^\top x)D (U^\top y) $$ but I don't see where I can go from here.","Let $A \in \Bbb R^n \times \Bbb R^n$ be a symmetric and positive semidefinite matrix. Let $q(x) =x^\top Ax$ and let the isotropic cone be defined as $$W= \{x ; q(x) = 0   \}$$ I want to show that $W$ is a subspace of $\Bbb R^n$. My closest attempt : Take $x$ and $y$ in $W$ (the zero vector always belongs to $W$ and is a subspace of $\Bbb R^n$). let $\alpha, \beta \in\Bbb R $ It is known that $$q(\alpha x- \beta y) = -2 \alpha \beta x^\top Ay + \alpha^2 q(x) + \beta^2 q(y) = -2 \alpha \beta  x^\top Ay$$ So ignoring constants we are done if $x^\top Ay = 0$ for $x,y \in W$. By the spectral theorem we have that $A = UDU^\top$ where $D$ is diagonal and $U$ is unitary. This means that  $$ x^\top Ay = (U^\top x)D (U^\top y) $$ but I don't see where I can go from here.",,"['linear-algebra', 'matrices', 'quadratic-forms']"
43,Exponentiation of a symmetric $2 \times 2$ matrix,Exponentiation of a symmetric  matrix,2 \times 2,Why does the following hold? $$\left[\begin{matrix}a & b\\b & a\end{matrix}\right]^k=\frac{1}{2}\left[\begin{matrix}\left(a - b\right)^{k} + \left(a + b\right)^{k} & - \left(a - b\right)^{k} + \left(a + b\right)^{k}\\- \left(a - b\right)^{k} + \left(a + b\right)^{k} & \left(a - b\right)^{k} + \left(a + b\right)^{k}\end{matrix}\right]$$,Why does the following hold? $$\left[\begin{matrix}a & b\\b & a\end{matrix}\right]^k=\frac{1}{2}\left[\begin{matrix}\left(a - b\right)^{k} + \left(a + b\right)^{k} & - \left(a - b\right)^{k} + \left(a + b\right)^{k}\\- \left(a - b\right)^{k} + \left(a + b\right)^{k} & \left(a - b\right)^{k} + \left(a + b\right)^{k}\end{matrix}\right]$$,,"['matrices', 'exponentiation', 'symmetric-matrices']"
44,Eigenvalues of $A=vv^T$,Eigenvalues of,A=vv^T,"Let $v\in\mathbb{R}^n$ a column vector and consider the matrix $A=v\cdot v^T$. I need find all of auto-values of $A$. For the case $n=2$ and $n=3$, I was obtain that the only auto-values are $\lambda=x_1^2+x_2^2$ and $\lambda=x_1^2+x_2^2+x_3^2$ respectively (Where $v:=(x_1,x_2,\dots,x_n)$). I think that this holds for any case, i.e., $\lambda=\sum_{i=1}^nx_i^2$  foe $n=1,2,...$ but I can't prove it. My conjecture is right? And if this is the case, can someone give me a hint to solution the problem? Thanks for advance","Let $v\in\mathbb{R}^n$ a column vector and consider the matrix $A=v\cdot v^T$. I need find all of auto-values of $A$. For the case $n=2$ and $n=3$, I was obtain that the only auto-values are $\lambda=x_1^2+x_2^2$ and $\lambda=x_1^2+x_2^2+x_3^2$ respectively (Where $v:=(x_1,x_2,\dots,x_n)$). I think that this holds for any case, i.e., $\lambda=\sum_{i=1}^nx_i^2$  foe $n=1,2,...$ but I can't prove it. My conjecture is right? And if this is the case, can someone give me a hint to solution the problem? Thanks for advance",,"['linear-algebra', 'matrices', 'eigenvalues-eigenvectors']"
45,finding the eigenvalues of a matrix without the polynomial,finding the eigenvalues of a matrix without the polynomial,,"I have a matrix $A = \begin{bmatrix} -1 && 1 && 1 \\ 1 && -1 && 1 \\ 1 && 1 && -1 \end{bmatrix}$ I already know that one eigenvector of that matrix is $(1, 1, 1)$ and one eigenvalue is $1$, based on the constant sum of rows. Now I am supposed to calculate the other eigenvalues, without using the polynomial of the matrix..  I know that there are supposed to be two more eigenvalues and that the product of the eigenvalues is equal to $detA=4$ and that the sum of eigenvalues is equal to $trA=-3$, but I just can't seem to get the right answer. Is this even the right approach? Any hints or ideas on how to proceed are appreciated.","I have a matrix $A = \begin{bmatrix} -1 && 1 && 1 \\ 1 && -1 && 1 \\ 1 && 1 && -1 \end{bmatrix}$ I already know that one eigenvector of that matrix is $(1, 1, 1)$ and one eigenvalue is $1$, based on the constant sum of rows. Now I am supposed to calculate the other eigenvalues, without using the polynomial of the matrix..  I know that there are supposed to be two more eigenvalues and that the product of the eigenvalues is equal to $detA=4$ and that the sum of eigenvalues is equal to $trA=-3$, but I just can't seem to get the right answer. Is this even the right approach? Any hints or ideas on how to proceed are appreciated.",,"['linear-algebra', 'matrices', 'eigenvalues-eigenvectors']"
46,Finding the $3 \times 3$ binary matrix with the least spectral radius greater than $1$,Finding the  binary matrix with the least spectral radius greater than,3 \times 3 1,"Let $A$ be a $3 \times 3$ matrix whose elements are only $0$ and $1$. Let $|\lambda_A^{\max}|$ be the spectral radius , i.e., the maximum absolute value of eigenvalues of $A$. How can I choose the matrix $A$ such that $\log |\lambda_A^{\max}|$ has the minimal positive value? Can someone give me a hint? Thank you!","Let $A$ be a $3 \times 3$ matrix whose elements are only $0$ and $1$. Let $|\lambda_A^{\max}|$ be the spectral radius , i.e., the maximum absolute value of eigenvalues of $A$. How can I choose the matrix $A$ such that $\log |\lambda_A^{\max}|$ has the minimal positive value? Can someone give me a hint? Thank you!",,"['linear-algebra', 'matrices', 'eigenvalues-eigenvectors', 'discrete-optimization', 'spectral-radius']"
47,"Formal expression for $A^{-1}$, given $A=1+B$","Formal expression for , given",A^{-1} A=1+B,"I have a 2x2 real matrix $A$, with $\det A\not=0$ and I define it as  $$A=1+B$$ where 1 is the identity and B another regular and real matrix. I need to express the inverse $A^{-1}$ in terms of B: I have used $A^{-1}A=1$ in order to derive a recurrence relation that yelds $$A^{-1}=1-B+B^{2}-B^{3}+B^{4}-...$$  which formally is a geometric series in B. My questions are: ""When this series is properly defined?"" ""Does exist a constraint on $B$ in order to deal with a convergent series?"" ""If for some $B$ the series diverge, what is the expression for $A^{-1}$ in terms of $B$?""","I have a 2x2 real matrix $A$, with $\det A\not=0$ and I define it as  $$A=1+B$$ where 1 is the identity and B another regular and real matrix. I need to express the inverse $A^{-1}$ in terms of B: I have used $A^{-1}A=1$ in order to derive a recurrence relation that yelds $$A^{-1}=1-B+B^{2}-B^{3}+B^{4}-...$$  which formally is a geometric series in B. My questions are: ""When this series is properly defined?"" ""Does exist a constraint on $B$ in order to deal with a convergent series?"" ""If for some $B$ the series diverge, what is the expression for $A^{-1}$ in terms of $B$?""",,"['sequences-and-series', 'matrices', 'convergence-divergence', 'inverse']"
48,Is there anything interesting about this matrix update?,Is there anything interesting about this matrix update?,,"I have a symmetric, real matrix $M$ (not necessarily full rank) and I am performing the following rank one update $$M - \frac{M\textbf{1}\textbf{1}^TM}{\textbf{1}^TM\textbf{1}}$$ where $\textbf{1}$ is the vector of all ones. Is there anything interesting I can say about this matrix (properties, theorems that use a similar update, perturbation effects) or is it just a boring old rank one update?","I have a symmetric, real matrix $M$ (not necessarily full rank) and I am performing the following rank one update $$M - \frac{M\textbf{1}\textbf{1}^TM}{\textbf{1}^TM\textbf{1}}$$ where $\textbf{1}$ is the vector of all ones. Is there anything interesting I can say about this matrix (properties, theorems that use a similar update, perturbation effects) or is it just a boring old rank one update?",,"['linear-algebra', 'matrices', 'numerical-linear-algebra']"
49,How to write $ uv^{\top} + vu^{\top} $ as $ xx^{\top} + yy^{\top} $?,How to write  as ?, uv^{\top} + vu^{\top}   xx^{\top} + yy^{\top} ,"I'm wondering, if it's possible to bring every $ uv^{\top} + vu^{\top} $, where $u,v \in \mathbb{R}^{n} $ in the form of $ xx^{\top} + yy^{\top} $, with  $x,y \in \mathbb{R}^{n} $, since $ rank(uv^{\top} + vu^{\top}) = 2.$ Is there a explicit formula for such a decomposition? I tried to calculate it directly on paper, but need to calculate a nonlinear equation system. Is there a trick or an idea, what I could do to find such a formula?","I'm wondering, if it's possible to bring every $ uv^{\top} + vu^{\top} $, where $u,v \in \mathbb{R}^{n} $ in the form of $ xx^{\top} + yy^{\top} $, with  $x,y \in \mathbb{R}^{n} $, since $ rank(uv^{\top} + vu^{\top}) = 2.$ Is there a explicit formula for such a decomposition? I tried to calculate it directly on paper, but need to calculate a nonlinear equation system. Is there a trick or an idea, what I could do to find such a formula?",,"['linear-algebra', 'matrices', 'matrix-calculus', 'matrix-decomposition', 'matrix-rank']"
50,Find a matrix $B$ such that $B^3 = A$,Find a matrix  such that,B B^3 = A,$$A=\begin{pmatrix} 1 & -1 \\ -2 & 1 \end{pmatrix}$$ Find a matrix $B$ such that $B^3$ = A My attempt: I found $\lambda_1= 1+{\sqrt 2}$ and  $\lambda_2= 1-{\sqrt 2}$ I also found their corresponding eigenvectors $\vec v_1 =\begin{pmatrix} \frac{-\sqrt 2}{2} \\ 1 \end{pmatrix}$ and $\vec v_2 = \begin{pmatrix} \frac{\sqrt 2}{2} \\ 1 \end{pmatrix}$ I know the Power function of a matrix formula $A=PDP^{-1}$ Because it's the cubed root I'm looking for I don't know how to get the cubed root of the eigenvaules and keep the maths neat. Is there another way to solve this problem or an I going the wrong way about doing it ?,$$A=\begin{pmatrix} 1 & -1 \\ -2 & 1 \end{pmatrix}$$ Find a matrix $B$ such that $B^3$ = A My attempt: I found $\lambda_1= 1+{\sqrt 2}$ and  $\lambda_2= 1-{\sqrt 2}$ I also found their corresponding eigenvectors $\vec v_1 =\begin{pmatrix} \frac{-\sqrt 2}{2} \\ 1 \end{pmatrix}$ and $\vec v_2 = \begin{pmatrix} \frac{\sqrt 2}{2} \\ 1 \end{pmatrix}$ I know the Power function of a matrix formula $A=PDP^{-1}$ Because it's the cubed root I'm looking for I don't know how to get the cubed root of the eigenvaules and keep the maths neat. Is there another way to solve this problem or an I going the wrong way about doing it ?,,"['linear-algebra', 'matrices']"
51,find two matrices A and B such that $A^2 -BA-AB+B^2 = O_{2\times 2}$,find two matrices A and B such that,A^2 -BA-AB+B^2 = O_{2\times 2},"Can someone please explain this question to me Question : Construct a $2\times 2$ matrix A and B with A different from B and neither A = $O_{2\times 2}$ nor $B = O_{2\times 2}$ such that $A^2 - BA - AB + B^2 = O_{2\times 2}$ Attempt: I found a matrix A such such that $a_{11} = 0$, $a_{12} = 0$, $a_{21} =1$, and $a_{22} = 0$ but I couldn't find a second matrix B that will respect the given condition Thanks.","Can someone please explain this question to me Question : Construct a $2\times 2$ matrix A and B with A different from B and neither A = $O_{2\times 2}$ nor $B = O_{2\times 2}$ such that $A^2 - BA - AB + B^2 = O_{2\times 2}$ Attempt: I found a matrix A such such that $a_{11} = 0$, $a_{12} = 0$, $a_{21} =1$, and $a_{22} = 0$ but I couldn't find a second matrix B that will respect the given condition Thanks.",,"['linear-algebra', 'matrices']"
52,Find a matrix $A^2$ if $A$ is known,Find a matrix  if  is known,A^2 A,Given $$\mathbf{A}^2=2\mathbf{A}-\mathbf{I}$$ where $\mathbf{A}$ is a $4\times4$ matrix and $\mathbf{I}$ is the $4\times 4$ identity matrix. Express $\mathbf{A}^3$ and $\mathbf{A}^4$ in the form $$k\mathbf{A}+l\mathbf{I}$$ where $k$ and $l$ are scalars. My attempt at answering this is for $\mathbf{A}^3$$$\begin{align}\mathbf{A}^3=\mathbf{A}^2\mathbf{A} &=(2\mathbf{A}-\mathbf{I})\mathbf{A}\\ &=2\mathbf{A}^2-\mathbf{I}\mathbf{A}\\ &=2(2\mathbf{A}-\mathbf{I})-\mathbf{I}\mathbf{A}\\ &=4\mathbf{A}-2\mathbf{I}-\mathbf{I}\mathbf{A} \end{align}$$ Which is where I get stuck. My question is how do I find $l$?,Given $$\mathbf{A}^2=2\mathbf{A}-\mathbf{I}$$ where $\mathbf{A}$ is a $4\times4$ matrix and $\mathbf{I}$ is the $4\times 4$ identity matrix. Express $\mathbf{A}^3$ and $\mathbf{A}^4$ in the form $$k\mathbf{A}+l\mathbf{I}$$ where $k$ and $l$ are scalars. My attempt at answering this is for $\mathbf{A}^3$$$\begin{align}\mathbf{A}^3=\mathbf{A}^2\mathbf{A} &=(2\mathbf{A}-\mathbf{I})\mathbf{A}\\ &=2\mathbf{A}^2-\mathbf{I}\mathbf{A}\\ &=2(2\mathbf{A}-\mathbf{I})-\mathbf{I}\mathbf{A}\\ &=4\mathbf{A}-2\mathbf{I}-\mathbf{I}\mathbf{A} \end{align}$$ Which is where I get stuck. My question is how do I find $l$?,,['matrices']
53,"If $B^2$= this matrix, find $B$","If = this matrix, find",B^2 B,"Find the real matrix $B$ such that $$ B^2 = \begin{pmatrix} 2 & 0 & 1 \\ 0 & 1 & 0 \\ 1 & 0 & 2 \end{pmatrix} $$ I think I am meant to use egenvalues to solve this, but would i find the egenvalues for $B^2$ or what? I am stuck on what to do.","Find the real matrix $B$ such that $$ B^2 = \begin{pmatrix} 2 & 0 & 1 \\ 0 & 1 & 0 \\ 1 & 0 & 2 \end{pmatrix} $$ I think I am meant to use egenvalues to solve this, but would i find the egenvalues for $B^2$ or what? I am stuck on what to do.",,"['linear-algebra', 'matrices', 'eigenvalues-eigenvectors']"
54,Fact regarding Kirchhoff's Theorem,Fact regarding Kirchhoff's Theorem,,Question regarding Kirchhoff's Theorem : If $ L(G)$ denotes the Laplacian of a graph $G$ then Kirchhoff's Theorem states that number of spanning trees in $G$ is equal to $(-1)^{i+j} \det L(i|j)$ where $L(i|j)$ is obtained by deleting the $i^{th}$ row and $j^{th}$ column. But how does that give that number of spanning trees is equal to $\frac{1}{n}\{\lambda_1 \lambda_2...\lambda_{n-1}\}$ where $\lambda_i$ are non-zero eigen values of $L(G)$ . Please help me to understand the second result stated in bold,Question regarding Kirchhoff's Theorem : If $ L(G)$ denotes the Laplacian of a graph $G$ then Kirchhoff's Theorem states that number of spanning trees in $G$ is equal to $(-1)^{i+j} \det L(i|j)$ where $L(i|j)$ is obtained by deleting the $i^{th}$ row and $j^{th}$ column. But how does that give that number of spanning trees is equal to $\frac{1}{n}\{\lambda_1 \lambda_2...\lambda_{n-1}\}$ where $\lambda_i$ are non-zero eigen values of $L(G)$ . Please help me to understand the second result stated in bold,,"['linear-algebra', 'matrices', 'spectral-theory', 'spectral-graph-theory']"
55,How to solve for the matrix $X$ in the following equation $AXB + X = CD$,How to solve for the matrix  in the following equation,X AXB + X = CD,"How to solve for the matrix $X$ in the following equation $AXB + X = CD$? $A$ and $B$ are full rank symmetric matrices, and there is no structure to $CD$. $CD$ just could be $C$.","How to solve for the matrix $X$ in the following equation $AXB + X = CD$? $A$ and $B$ are full rank symmetric matrices, and there is no structure to $CD$. $CD$ just could be $C$.",,"['linear-algebra', 'matrices']"
56,Show that $G/H\cong\mathbb{R}^*$.,Show that .,G/H\cong\mathbb{R}^*,"Let $G:= \bigg\{\left( \begin{array}{ccc} a & b  \\ 0 & a  \\ \end{array} \right)\Bigg| a,b \in \mathbb{R},a\ne 0\bigg\}$ .  Let $H:= \bigg\{\left( \begin{array}{ccc} 1 & b  \\ 0 & 1  \\ \end{array} \right) \Bigg| b \in \mathbb{R}\bigg\}$ . I know that $H$ is normal subgroup of $G$ .  I need to prove that $G/H\cong\mathbb{R}^*$ . My attempt: $$G/H=\{gH\mid g\in G\}$$ $$ \left( \begin{array}{ccc} a & b  \\ 0 & a  \\ \end{array} \right) $$ $$ \left( \begin{array}{ccc} 1 & b  \\ 0 & 1 \end{array} \right) = \left( \begin{array}{ccc} a & ab+b  \\ 0 & a \end{array} \right) $$ And there are $\infty$ solutions from the form : $ \left( \begin{array}{ccc} a & ab  \\ 0 & a \end{array} \right) $ . $$ \begin{vmatrix} \left( \begin{array}{ccc} a & ab  \\ 0 & a \end{array} \right)\end{vmatrix}=\mathfrak{c}=|\mathbb{R}^*| $$ $$\Rightarrow G/N\cong \mathbb{R}^*$$ Is it correct? is there another way to solve this?",Let .  Let . I know that is normal subgroup of .  I need to prove that . My attempt: And there are solutions from the form : . Is it correct? is there another way to solve this?,"G:=
\bigg\{\left( \begin{array}{ccc}
a & b  \\
0 & a  \\
\end{array} \right)\Bigg| a,b \in \mathbb{R},a\ne 0\bigg\} H:=
\bigg\{\left( \begin{array}{ccc}
1 & b  \\
0 & 1  \\
\end{array} \right) \Bigg| b \in \mathbb{R}\bigg\} H G G/H\cong\mathbb{R}^* G/H=\{gH\mid g\in G\} 
\left( \begin{array}{ccc}
a & b  \\
0 & a  \\
\end{array} \right)
 
\left( \begin{array}{ccc}
1 & b  \\
0 & 1
\end{array} \right)
=
\left( \begin{array}{ccc}
a & ab+b  \\
0 & a
\end{array} \right)
 \infty 
\left( \begin{array}{ccc}
a & ab  \\
0 & a
\end{array} \right)
 
\begin{vmatrix}
\left( \begin{array}{ccc}
a & ab  \\
0 & a
\end{array} \right)\end{vmatrix}=\mathfrak{c}=|\mathbb{R}^*|
 \Rightarrow G/N\cong \mathbb{R}^*","['abstract-algebra', 'matrices', 'group-theory', 'group-isomorphism', 'infinite-groups']"
57,Determinant of matrix with trigonometric functions,Determinant of matrix with trigonometric functions,,"Find the determinant of the following matrix:   $$\begin{pmatrix}\cos\left(a_{1}-b_{1}\right) & \cos\left(a_{1}-b_{2}\right) & \cos\left(a_{1}-b_{3}\right)\\ \cos\left(a_{2}-b_{1}\right) & \cos\left(a_{2}-b_{2}\right) & \cos\left(a_{2}-b_{3}\right)\\ \cos\left(a_{3}-b_{1}\right) & \cos\left(a_{3}-b_{2}\right) & \cos\left(a_{3}-b_{3}\right) \end{pmatrix}$$For $a_1,\dots ,a_3,b_1,\dots, b_3\in\mathbb{R}$ I'm completely stumped honestly. I tried using the cosine addition identity to open the cosine, but I wasn't able to find how it helps me, and even for a $2\times2$ version of the matrix I wasn't really sure what to do. Any help?","Find the determinant of the following matrix:   $$\begin{pmatrix}\cos\left(a_{1}-b_{1}\right) & \cos\left(a_{1}-b_{2}\right) & \cos\left(a_{1}-b_{3}\right)\\ \cos\left(a_{2}-b_{1}\right) & \cos\left(a_{2}-b_{2}\right) & \cos\left(a_{2}-b_{3}\right)\\ \cos\left(a_{3}-b_{1}\right) & \cos\left(a_{3}-b_{2}\right) & \cos\left(a_{3}-b_{3}\right) \end{pmatrix}$$For $a_1,\dots ,a_3,b_1,\dots, b_3\in\mathbb{R}$ I'm completely stumped honestly. I tried using the cosine addition identity to open the cosine, but I wasn't able to find how it helps me, and even for a $2\times2$ version of the matrix I wasn't really sure what to do. Any help?",,"['linear-algebra', 'matrices', 'determinant']"
58,"If $A$ is a matrix, and $A^2=I$, then can I say that $|A|= \pm1$?","If  is a matrix, and , then can I say that ?",A A^2=I |A|= \pm1,$A^2=I$ Take determinant on both sides: $$|A^2|= |I| $$ $$|A|^2= 1$$ $$|A| = +1 \text{  or  } -1$$ Is this proof correct?,$A^2=I$ Take determinant on both sides: $$|A^2|= |I| $$ $$|A|^2= 1$$ $$|A| = +1 \text{  or  } -1$$ Is this proof correct?,,"['matrices', 'determinant']"
59,Looking for a proof that the resultant is the product of the differences of roots,Looking for a proof that the resultant is the product of the differences of roots,,"I'm trying to find a general proof to an exercise given in Garrity et al's book, Algebraic Geometry: A problem-solving approach. The problem is this: Given two polynomials f and g, show that for each pair of roots, $f(r) = 0, g(s) = 0$ , that $(r - s)$ divides the resultant. There is a book of selected answers, but somewhat disappointingly, the solution is given as a brutal appeal to algebra. Moreover, the result is only given for quadratic polynomials. It seems cited in a few places that the resultant, defined as the determinant of the Sylvester matrix of two polynomials $f = \lambda_1\prod (x - r_i)$ and $g = \lambda_2 \prod (x - s_i)$ , is equal to the product $\prod r_i - s_i$ . But so far, I have been unable to find a general proof of this fact. Would anyone either mind sketching the proof, or else pointing me to a resource which does?","I'm trying to find a general proof to an exercise given in Garrity et al's book, Algebraic Geometry: A problem-solving approach. The problem is this: Given two polynomials f and g, show that for each pair of roots, , that divides the resultant. There is a book of selected answers, but somewhat disappointingly, the solution is given as a brutal appeal to algebra. Moreover, the result is only given for quadratic polynomials. It seems cited in a few places that the resultant, defined as the determinant of the Sylvester matrix of two polynomials and , is equal to the product . But so far, I have been unable to find a general proof of this fact. Would anyone either mind sketching the proof, or else pointing me to a resource which does?","f(r) = 0, g(s) = 0 (r - s) f = \lambda_1\prod (x - r_i) g = \lambda_2 \prod (x - s_i) \prod r_i - s_i","['matrices', 'algebraic-geometry', 'polynomials', 'determinant', 'resultant']"
60,How to compute the trace of an exponential and diagonal matrix?,How to compute the trace of an exponential and diagonal matrix?,,"I would like to understand how to compute the trace of an exponential and diagonal matrix. For instance, what is: $$ \mathrm{Tr}\left[ \exp \begin{pmatrix} 5 & 0 \\ 0 & 8 \end{pmatrix}  \right] = \; ? $$ I've tried to Google it, but couldn't find anything that answers this question.","I would like to understand how to compute the trace of an exponential and diagonal matrix. For instance, what is: $$ \mathrm{Tr}\left[ \exp \begin{pmatrix} 5 & 0 \\ 0 & 8 \end{pmatrix}  \right] = \; ? $$ I've tried to Google it, but couldn't find anything that answers this question.",,"['linear-algebra', 'matrices']"
61,"If $A$ is positive definite, $B$ is symmetric and $BAB = B$, how to show that $A^{1/2}BA^{1/2}$ is a diagonal matrix?","If  is positive definite,  is symmetric and , how to show that  is a diagonal matrix?",A B BAB = B A^{1/2}BA^{1/2},"If $A,B$ are symmetric $m$-by-$m$ matrices with $A$ positive definite, I want to show that $A^{1/2}BA^{1/2}$ is a diagonal matrix if it is given that $BAB=B$. Here $A^{1/2}$ is the positive definite square root of $A$. I have been stuck on this problem for a long time. I have tried the decomposition of $A,B$ into the products of orthogonal matrices and diagonal matrices, but maybe the correct path has not hit me. I have not yet arrived at any notable result in trying to prove this. All my attempts have failed. I would really appreciate some help.","If $A,B$ are symmetric $m$-by-$m$ matrices with $A$ positive definite, I want to show that $A^{1/2}BA^{1/2}$ is a diagonal matrix if it is given that $BAB=B$. Here $A^{1/2}$ is the positive definite square root of $A$. I have been stuck on this problem for a long time. I have tried the decomposition of $A,B$ into the products of orthogonal matrices and diagonal matrices, but maybe the correct path has not hit me. I have not yet arrived at any notable result in trying to prove this. All my attempts have failed. I would really appreciate some help.",,"['linear-algebra', 'matrices', 'orthogonality']"
62,How to show that $B^{-1}\cdot A^{-1}=B\cdot A$?,How to show that ?,B^{-1}\cdot A^{-1}=B\cdot A,"I can't find the solution of this problem: Given two $n\times n$ square matrices $A,B$ such that $A^2\cdot B^2=I_n$, show that  $B^{-1}\cdot A^{-1}=B\cdot A$. Thanks in advance.","I can't find the solution of this problem: Given two $n\times n$ square matrices $A,B$ such that $A^2\cdot B^2=I_n$, show that  $B^{-1}\cdot A^{-1}=B\cdot A$. Thanks in advance.",,"['linear-algebra', 'matrices']"
63,What is $\Bbb{R}^n$?,What is ?,\Bbb{R}^n,"I earlier asked this question The basis of a matrix representation . I now have a another question related to the same topic. The vector space $\Bbb{R}^n$ I have seen defined as all $n$-tuples of real numbers$^1$. But I think a more intuitive definition would be that $\Bbb{R}^n$ is simply the $n$-dimensional real space such as a plane for $n=2$ or a line for $n=1$. My problem with the first definition is that it seems like we have already specified a basis, namely $(i,j,k)$ for n=3, which themselves have no meaning as column vectors until we define them to be $(1,0,0),(0,1,0)$ and $ (0,0,1)$ respectively. Thus if we write vectors in $\Bbb{R}^n$ as column vectors we have already defined our basis and the tuples are therefore not basis independent objects. So my point is that if we change basis, the $n$-order tuples will change but the actual  'arrow' from one point to another in the $n$-dimensional space will not. So surely this 'arrow' is the actual element $\Bbb{R}^n$ and the $n$-order tuple is its coordinate map. Is this right or wrong? If it is right please could you give me a source where it states it explicitly, I have tried to look for one without success. from http://www.math.vt.edu/people/dlr/m2k_svb01_vecspc.pdf","I earlier asked this question The basis of a matrix representation . I now have a another question related to the same topic. The vector space $\Bbb{R}^n$ I have seen defined as all $n$-tuples of real numbers$^1$. But I think a more intuitive definition would be that $\Bbb{R}^n$ is simply the $n$-dimensional real space such as a plane for $n=2$ or a line for $n=1$. My problem with the first definition is that it seems like we have already specified a basis, namely $(i,j,k)$ for n=3, which themselves have no meaning as column vectors until we define them to be $(1,0,0),(0,1,0)$ and $ (0,0,1)$ respectively. Thus if we write vectors in $\Bbb{R}^n$ as column vectors we have already defined our basis and the tuples are therefore not basis independent objects. So my point is that if we change basis, the $n$-order tuples will change but the actual  'arrow' from one point to another in the $n$-dimensional space will not. So surely this 'arrow' is the actual element $\Bbb{R}^n$ and the $n$-order tuple is its coordinate map. Is this right or wrong? If it is right please could you give me a source where it states it explicitly, I have tried to look for one without success. from http://www.math.vt.edu/people/dlr/m2k_svb01_vecspc.pdf",,['linear-algebra']
64,To prove that the sum of the roots of the characteristic polynomial of a square matrix is equal to the trace of the matrix,To prove that the sum of the roots of the characteristic polynomial of a square matrix is equal to the trace of the matrix,,"How do we prove that the sum of the roots of the characteristic polynomial of a square matrix is equal to the trace of the matrix ? I want a proof which does not use much computation or determinants ; please help , Thanks in Advance .","How do we prove that the sum of the roots of the characteristic polynomial of a square matrix is equal to the trace of the matrix ? I want a proof which does not use much computation or determinants ; please help , Thanks in Advance .",,"['linear-algebra', 'matrices', 'eigenvalues-eigenvectors']"
65,How do you calculate the dimensions of the null space and column space of the following matrix?,How do you calculate the dimensions of the null space and column space of the following matrix?,,"I understand you are supposed to get the reduced row echelon form, which I did, and this is what I came up with: 1   -2  0   19  -6  0   -37 0   0   1   -6  2   0   6 0   0   0   0   0   1   3 0   0   0   0   0   0   0 From here, I know you're supposed to put it in equations, which I also did, and this is what I got: x1 – 2x2 + 19x4 – 6x5 – 37x7 = 0 x3 – 6x4 + 2x5 + 6x7 = 0 x6 + 3x7 = 0 x1 = 2x2 – 19x4 + 6x5 + 37x7 x3 = 6x4 – 2x5 – 6x7 x6 = -3x7 From here I know you make the columns, but what I don't know is if I'm supposed to also solve the equations for x2, x4, x5, and x7, and make columns for those as well, which would give me a different dimension for the column space.  Do I do that or do I stick with the current equations only and end up with a column space of 4? Thank you for your help.","I understand you are supposed to get the reduced row echelon form, which I did, and this is what I came up with: 1   -2  0   19  -6  0   -37 0   0   1   -6  2   0   6 0   0   0   0   0   1   3 0   0   0   0   0   0   0 From here, I know you're supposed to put it in equations, which I also did, and this is what I got: x1 – 2x2 + 19x4 – 6x5 – 37x7 = 0 x3 – 6x4 + 2x5 + 6x7 = 0 x6 + 3x7 = 0 x1 = 2x2 – 19x4 + 6x5 + 37x7 x3 = 6x4 – 2x5 – 6x7 x6 = -3x7 From here I know you make the columns, but what I don't know is if I'm supposed to also solve the equations for x2, x4, x5, and x7, and make columns for those as well, which would give me a different dimension for the column space.  Do I do that or do I stick with the current equations only and end up with a column space of 4? Thank you for your help.",,"['linear-algebra', 'matrices']"
66,what's the relationship of $A*A^T$ and $A^T*A$,what's the relationship of  and,A*A^T A^T*A,"For a $m \times n$ matrix $A$, what's the relationship of $A*A^T$ and $A^T*A$? The background of this question is that if we see the row of $A$ as observations and column as variables, $A*A^T$ is the covariance across variables, and $A^T*A$ are the covariance across observations. How is these two matrix related?","For a $m \times n$ matrix $A$, what's the relationship of $A*A^T$ and $A^T*A$? The background of this question is that if we see the row of $A$ as observations and column as variables, $A*A^T$ is the covariance across variables, and $A^T*A$ are the covariance across observations. How is these two matrix related?",,"['linear-algebra', 'matrices', 'statistics']"
67,Expressions for Permanent of a Matrix,Expressions for Permanent of a Matrix,,"Given that the permanent of a matrix can be written in a similar form as the determinant, as a sum of permutations of the elements of the matrix, is there also a relationship between the permanent and the eigenvalues of the matrix? Analogous to the determinant given by the product of eigenvalues? Also, does anyone know of a comprehensive reference on matrix permanents? It's hard to find a good one (maybe it's just because there isn't a lot written on them.)","Given that the permanent of a matrix can be written in a similar form as the determinant, as a sum of permutations of the elements of the matrix, is there also a relationship between the permanent and the eigenvalues of the matrix? Analogous to the determinant given by the product of eigenvalues? Also, does anyone know of a comprehensive reference on matrix permanents? It's hard to find a good one (maybe it's just because there isn't a lot written on them.)",,"['linear-algebra', 'matrices', 'reference-request']"
68,Matrix Equality,Matrix Equality,,"Can you help me to prove this equality ? Let $A,B$ be $n\times n$ matrices. Let $[A,B]$ denote the usual matrix commutator and $e^{A}$ the usual matrix exponential.  By hypothesis, let's say that $[A,[A,B]]=[B,[A,B]]=0$. We want to prove that, $\forall t\in \mathbb{R}$ we have that $e^{tA}e^{tB}=e^{t(A+B)}e^{\frac{t^{2}}{2}[A,B]}$. EDIT : Thank you for the suggestion of Zassenhaus Formula, but I think, since this is homework question, I should do a little bit more than just say this is a direct consequence of that result. Instead of proving Zassenhaus Formula, I think it's easier if I prove that $e^{-t(A+B)}e^{tA}e^{tB}$ is the solution of the following differential equation : $\frac{d}{dt}x = t[A,B]x$. Can somebody help me doing this ? Thank you very much :)","Can you help me to prove this equality ? Let $A,B$ be $n\times n$ matrices. Let $[A,B]$ denote the usual matrix commutator and $e^{A}$ the usual matrix exponential.  By hypothesis, let's say that $[A,[A,B]]=[B,[A,B]]=0$. We want to prove that, $\forall t\in \mathbb{R}$ we have that $e^{tA}e^{tB}=e^{t(A+B)}e^{\frac{t^{2}}{2}[A,B]}$. EDIT : Thank you for the suggestion of Zassenhaus Formula, but I think, since this is homework question, I should do a little bit more than just say this is a direct consequence of that result. Instead of proving Zassenhaus Formula, I think it's easier if I prove that $e^{-t(A+B)}e^{tA}e^{tB}$ is the solution of the following differential equation : $\frac{d}{dt}x = t[A,B]x$. Can somebody help me doing this ? Thank you very much :)",,"['linear-algebra', 'matrices']"
69,How to find the determinant of this matrix,How to find the determinant of this matrix,,I have the following matrix: $ \begin{bmatrix}  a & 1 & 1 & 1 \\  1 & a & 1 & 1 \\  1 & 1 & a & 1 \\  1 & 1 & 1 & a \\ \end{bmatrix} $ My approach is to rref the matrix so that i can find the determinant by multiplying along the diagonals. I attempted to do an rref and ended up with  $ \begin{bmatrix}  1 & a & 1 & 1 \\  1-a & a-1 & 0 & 0 \\  0 & 1-a & a-1 & 1 \\  0 & 0 & 1-a & a-1 \\ \end{bmatrix} $ I then factored out (1-a) to get this $ \begin{bmatrix}  1 & a & 1 & 1 \\  1 & -1 & 0 & 0 \\  0 & 1 & -1 & 1 \\  0 & 0 & 1 & -1 \\ \end{bmatrix} $ which will then make my determinant a multipile of $(1-a)^3$:  $\det(A) = (1-a)^3x$ But now here I don't know what to do. I have a feeling that my approach is wrong. Any help or guide please?,I have the following matrix: $ \begin{bmatrix}  a & 1 & 1 & 1 \\  1 & a & 1 & 1 \\  1 & 1 & a & 1 \\  1 & 1 & 1 & a \\ \end{bmatrix} $ My approach is to rref the matrix so that i can find the determinant by multiplying along the diagonals. I attempted to do an rref and ended up with  $ \begin{bmatrix}  1 & a & 1 & 1 \\  1-a & a-1 & 0 & 0 \\  0 & 1-a & a-1 & 1 \\  0 & 0 & 1-a & a-1 \\ \end{bmatrix} $ I then factored out (1-a) to get this $ \begin{bmatrix}  1 & a & 1 & 1 \\  1 & -1 & 0 & 0 \\  0 & 1 & -1 & 1 \\  0 & 0 & 1 & -1 \\ \end{bmatrix} $ which will then make my determinant a multipile of $(1-a)^3$:  $\det(A) = (1-a)^3x$ But now here I don't know what to do. I have a feeling that my approach is wrong. Any help or guide please?,,"['matrices', 'determinant']"
70,Rotation matrix in arbitrary dimension to align vector,Rotation matrix in arbitrary dimension to align vector,,"I was sure it's going to be trivial to do, but then got stuck. Problem - given a vector $u\in\mathbb{R}^d$ in $d$ dimensions, find a rotation matrix $M$, such that (Rotation) $M^T=M^{-1}$, (Rotation) $\det(M) = 1$, $Mu = \lambda\cdot(1,0,\dots, 0)^T$, for some scalar $\lambda > 0$. That is, $M$ aligns $u$ with the $\hat{x_1}$ vector.","I was sure it's going to be trivial to do, but then got stuck. Problem - given a vector $u\in\mathbb{R}^d$ in $d$ dimensions, find a rotation matrix $M$, such that (Rotation) $M^T=M^{-1}$, (Rotation) $\det(M) = 1$, $Mu = \lambda\cdot(1,0,\dots, 0)^T$, for some scalar $\lambda > 0$. That is, $M$ aligns $u$ with the $\hat{x_1}$ vector.",,"['linear-algebra', 'matrices', 'rotations']"
71,Looking for an elegant proof of $\det(A) = \det(A^t)$ without Schur decomposition,Looking for an elegant proof of  without Schur decomposition,\det(A) = \det(A^t),"Looking for an elegant proof of $\det(\textbf{A}) = \det(\textbf{A}^{t})$ without Schur decomposition. Proof 1 with Schur decomposition $$\textbf{A} = \textbf{P}^{t}\Delta\textbf{P} \implies\textbf{A}^{t} = (\textbf{P}^{t}\Delta\textbf{P})^{t} = \textbf{P}^{t}\Delta^{t}\textbf{P}$$ So, $\textbf{P}$  is unitary matrix, $\textbf{P}^{t}=\textbf{P}^{-1}$. $$\det(\textbf{P})=\det(\textbf{P}^{t})= \det(\textbf{P}^{-1})$$  $\Delta$ is upper triangular matrix. $$\det(\Delta)=\det(\Delta^{t}) \implies   \det(\textbf{A})=\det(\textbf{A}^{t})$$","Looking for an elegant proof of $\det(\textbf{A}) = \det(\textbf{A}^{t})$ without Schur decomposition. Proof 1 with Schur decomposition $$\textbf{A} = \textbf{P}^{t}\Delta\textbf{P} \implies\textbf{A}^{t} = (\textbf{P}^{t}\Delta\textbf{P})^{t} = \textbf{P}^{t}\Delta^{t}\textbf{P}$$ So, $\textbf{P}$  is unitary matrix, $\textbf{P}^{t}=\textbf{P}^{-1}$. $$\det(\textbf{P})=\det(\textbf{P}^{t})= \det(\textbf{P}^{-1})$$  $\Delta$ is upper triangular matrix. $$\det(\Delta)=\det(\Delta^{t}) \implies   \det(\textbf{A})=\det(\textbf{A}^{t})$$",,"['linear-algebra', 'matrices', 'determinant']"
72,Does $\exp(\ln(I+A))=I+A$ when $\|A\|<1$?,Does  when ?,\exp(\ln(I+A))=I+A \|A\|<1,"For matrices, I know certain equalities like $e^{A+B}=e^Ae^B$ aren't always true. I'm curious, do $\exp$ and $\ln$ serve as inverses? I saw earlier that if $\|A\|<1$, then $\ln(I+A)$ converges. My question is, does $$ \exp(\ln(I+A))=I+A $$ when $\|A\|<1$? Certainly $\exp(\ln(I+A))$ converge since $\ln(I+A)$ converges and $\exp(\cdot)$ converges for all matrices, but do we necessarily get the original matrix back?","For matrices, I know certain equalities like $e^{A+B}=e^Ae^B$ aren't always true. I'm curious, do $\exp$ and $\ln$ serve as inverses? I saw earlier that if $\|A\|<1$, then $\ln(I+A)$ converges. My question is, does $$ \exp(\ln(I+A))=I+A $$ when $\|A\|<1$? Certainly $\exp(\ln(I+A))$ converge since $\ln(I+A)$ converges and $\exp(\cdot)$ converges for all matrices, but do we necessarily get the original matrix back?",,"['matrices', 'convergence-divergence', 'inverse']"
73,Non-trivial solutions implies row of zeros?,Non-trivial solutions implies row of zeros?,,"If there exist non trivial solutions, the row echelon matrix of homogenous augmented matrix A has a row of zeros. True or False? I'm not sure where to begin as to see why this would be true or false. I know that if there are a row of zeros it means that there are infinitely many solutions, but not sure how I can tell if that means there are non-trivial solutions though. Any help would be appreciated. Thanks in advance.","If there exist non trivial solutions, the row echelon matrix of homogenous augmented matrix A has a row of zeros. True or False? I'm not sure where to begin as to see why this would be true or false. I know that if there are a row of zeros it means that there are infinitely many solutions, but not sure how I can tell if that means there are non-trivial solutions though. Any help would be appreciated. Thanks in advance.",,"['linear-algebra', 'matrices']"
74,$ e^{At}$ for $A = B^{-1} \lvert \cdots \rvert B $,for, e^{At} A = B^{-1} \lvert \cdots \rvert B ,"For a homework problem, I have to compute $ e^{At}$ for $$ A = B^{-1}  \begin{pmatrix} -1 & 0 & 0 \\ 0 & 2 & 0 \\ 0 & 0 & 3 \end{pmatrix} B$$ I know how to compute the result for $2 \times 2$ matrices where I can calculate the eigenvalues, but this is $3 \times 3$, and I cannot compute eigenvalues, so is there any identity or something which allows computing such exponentials? Thanks!","For a homework problem, I have to compute $ e^{At}$ for $$ A = B^{-1}  \begin{pmatrix} -1 & 0 & 0 \\ 0 & 2 & 0 \\ 0 & 0 & 3 \end{pmatrix} B$$ I know how to compute the result for $2 \times 2$ matrices where I can calculate the eigenvalues, but this is $3 \times 3$, and I cannot compute eigenvalues, so is there any identity or something which allows computing such exponentials? Thanks!",,"['matrices', 'exponential-function']"
75,Sylow $p$-subgroups of Finite Matrix Groups,Sylow -subgroups of Finite Matrix Groups,p,"Let $G$ be a subgroup of $GL_n(\mathbb{F}_p)$, with $n\le p$, and let $P$ be a Sylow $p$-subgroup of $G$.  Do all non-trivial elements of $P$ have order $p$? I believe the answer is yes, because I think the result holds for $G=GL_n(\mathbb{F}_p)$.  I'm hoping someone can show me a quick way to see this fact, perhaps using minimal/characteristic polynomials.","Let $G$ be a subgroup of $GL_n(\mathbb{F}_p)$, with $n\le p$, and let $P$ be a Sylow $p$-subgroup of $G$.  Do all non-trivial elements of $P$ have order $p$? I believe the answer is yes, because I think the result holds for $G=GL_n(\mathbb{F}_p)$.  I'm hoping someone can show me a quick way to see this fact, perhaps using minimal/characteristic polynomials.",,"['linear-algebra', 'matrices', 'finite-groups', 'finite-fields']"
76,Invert of Matrix I-BA [duplicate],Invert of Matrix I-BA [duplicate],,This question already has answers here : $I_m - AB$ is invertible if and only if $I_n - BA$ is invertible. (4 answers) Closed 11 years ago . Suppose $A$ and $B$ are two square Matrix. Let $I-AB$ be invertible. I would like to know why $I-BA$ is also invertible? Also what is invert of $I-BA$? Thanks.,This question already has answers here : $I_m - AB$ is invertible if and only if $I_n - BA$ is invertible. (4 answers) Closed 11 years ago . Suppose $A$ and $B$ are two square Matrix. Let $I-AB$ be invertible. I would like to know why $I-BA$ is also invertible? Also what is invert of $I-BA$? Thanks.,,['matrices']
77,set of all symmetric non-negative definite matrices are closed or not,set of all symmetric non-negative definite matrices are closed or not,,Can anyone tell me please that set of all symmetric non-negative definite matrices are closed or not in $\mathbb{M}_n(\mathbb{R})$ with usual topology,Can anyone tell me please that set of all symmetric non-negative definite matrices are closed or not in $\mathbb{M}_n(\mathbb{R})$ with usual topology,,"['general-topology', 'matrices']"
78,What are the dimensions of the singular vectors matrices in the singular value decomposition?,What are the dimensions of the singular vectors matrices in the singular value decomposition?,,"Consider a $p\times q$ matrix $Y$ with rank $r$ . I read in a paper that the SVD can be written as $$Y=U_0 \Sigma_0 V^*_0 + U_1 \Sigma_1 V^*_1,$$ where $U_0$ and $V_0$ are the singular vectors associated with the singular values greater than $\tau$ , and $U_1$ and $V_1$ are the singular vectors associated with singular values less than $\tau$ . Also, $*$ denotes the transpose of a matrix. Could someone tell me what the dimensions of the various submatrices are so that the decomposition is true? You can assume whatever you want about the number of eigenvalues greater than $\tau$ and the number of eigenvalues less than $\tau$ . I'm trying to understand how the original $Y$ can be written that way. I understand that it can be written as $Y = U \Sigma V^*$ which is the regular SVD but I don't understand how this regular SVD can be broken down into the form at the top with the sub-matrices. Thanks. Also, if there's a paper or book that shows it, that's fine also.","Consider a matrix with rank . I read in a paper that the SVD can be written as where and are the singular vectors associated with the singular values greater than , and and are the singular vectors associated with singular values less than . Also, denotes the transpose of a matrix. Could someone tell me what the dimensions of the various submatrices are so that the decomposition is true? You can assume whatever you want about the number of eigenvalues greater than and the number of eigenvalues less than . I'm trying to understand how the original can be written that way. I understand that it can be written as which is the regular SVD but I don't understand how this regular SVD can be broken down into the form at the top with the sub-matrices. Thanks. Also, if there's a paper or book that shows it, that's fine also.","p\times q Y r Y=U_0 \Sigma_0 V^*_0 + U_1 \Sigma_1 V^*_1, U_0 V_0 \tau U_1 V_1 \tau * \tau \tau Y Y = U \Sigma V^*","['linear-algebra', 'matrices', 'matrix-decomposition', 'svd', 'singular-values']"
79,Is a vector of coprime integers column of a regular matrix?,Is a vector of coprime integers column of a regular matrix?,,"Given a vector $a_1=(k_1,\ldots,k_n)^T$ of coprime integers. Are there $a_2,\ldots,a_n \in \mathbb{Z}^n$ such that that the matrix $A := (a_1,\ldots,a_n) \in \mathbb{Z}^{n \times n}$ is regular, i.e. $A \in GL_n(\mathbb{Z})$ ? In case $n=2$ this is true because there are integers $l_1,l_2$ s.t. $k_1l_1 + k_2l_2 = 1$. Then $a_2=(-l_2 , l_1)^T$ will do.","Given a vector $a_1=(k_1,\ldots,k_n)^T$ of coprime integers. Are there $a_2,\ldots,a_n \in \mathbb{Z}^n$ such that that the matrix $A := (a_1,\ldots,a_n) \in \mathbb{Z}^{n \times n}$ is regular, i.e. $A \in GL_n(\mathbb{Z})$ ? In case $n=2$ this is true because there are integers $l_1,l_2$ s.t. $k_1l_1 + k_2l_2 = 1$. Then $a_2=(-l_2 , l_1)^T$ will do.",,"['matrices', 'elementary-number-theory']"
80,eigenvalues by inspection,eigenvalues by inspection,,"Well, May be my question does not make any sense, But one of my junior asked me whether we can say eigenvalues of a matrix by inspection may be for $3\times 3$ matrix? He said for the following matrix, will $3$ be an eigenvalue with multiplicity $3$ with out calculation? $$\left(   \begin{array}{ccc}     3 & 2 & 2 \\     2 & 3 & 2 \\     2 & 2 & 3 \\   \end{array} \right).$$ So far I know that if it is real symmetric then its eigenvalues are purely real, if skew then purely imaginary, if diagonal then all the entries, and if unitary then with modulus 1, known facts at all.If any one knows more about this please write.","Well, May be my question does not make any sense, But one of my junior asked me whether we can say eigenvalues of a matrix by inspection may be for $3\times 3$ matrix? He said for the following matrix, will $3$ be an eigenvalue with multiplicity $3$ with out calculation? $$\left(   \begin{array}{ccc}     3 & 2 & 2 \\     2 & 3 & 2 \\     2 & 2 & 3 \\   \end{array} \right).$$ So far I know that if it is real symmetric then its eigenvalues are purely real, if skew then purely imaginary, if diagonal then all the entries, and if unitary then with modulus 1, known facts at all.If any one knows more about this please write.",,"['linear-algebra', 'matrices', 'eigenvalues-eigenvectors']"
81,"Given a matrix A, how can we find C if A = AC - CA?","Given a matrix A, how can we find C if A = AC - CA?",,"Give this matrix A: \begin{pmatrix}-25&2&3&-29\\2&7&7&11\\3&7&7&2\\-29&11&2&11\end{pmatrix} How can we calculate C matrix when A = AC - CA without extensive computations? Thought of doing these steps: Let C be {{a,b,c,d},{e,f,g,h},{i,j,k,l},{m,n,o,p}} Calculate (via wolfram) AC, CA Subtract AC, CA (cannot do it with wolfram, there's a limit on the characters of input Then assign the 1st row of the result with the respective row of matrix A Solve the system of 4 linear equations with 4 variables Is that correct? How can I find a workaround for step 3? Thank you for your time!","Give this matrix A: \begin{pmatrix}-25&2&3&-29\\2&7&7&11\\3&7&7&2\\-29&11&2&11\end{pmatrix} How can we calculate C matrix when A = AC - CA without extensive computations? Thought of doing these steps: Let C be {{a,b,c,d},{e,f,g,h},{i,j,k,l},{m,n,o,p}} Calculate (via wolfram) AC, CA Subtract AC, CA (cannot do it with wolfram, there's a limit on the characters of input Then assign the 1st row of the result with the respective row of matrix A Solve the system of 4 linear equations with 4 variables Is that correct? How can I find a workaround for step 3? Thank you for your time!",,"['linear-algebra', 'matrices']"
82,Let $\alpha$ and $\beta$ be two distinct eigenvalues of $A$ then $ A^3 = \frac{\alpha^3-\beta^3}{\alpha-\beta}A-\alpha\beta(\alpha+\beta)I$?,Let  and  be two distinct eigenvalues of  then ?,\alpha \beta A  A^3 = \frac{\alpha^3-\beta^3}{\alpha-\beta}A-\alpha\beta(\alpha+\beta)I,Let $\alpha$ and $\beta$ be two distinct eigenvalues of a $2\times2$ matrix $A$. Then which of the following statements must be true. 1 - $A^n$ is not a scalar multiple of identity matrix for any positive positive integer $n$. 2 - $ A^3 = \dfrac{\alpha^3-\beta^3}{\alpha-\beta}A-\alpha\beta(\alpha+\beta)I$ For the statement 1 I picked up a diagonal matrix with diagonal entries 1 and -1 whose square comes out to be identity matrix. Thus statement may be false. But for the second statement i am not able to figure out a way to start. This  probably may be easy but I am not able to get this. Please post a small hint so that I may proceed further.,Let $\alpha$ and $\beta$ be two distinct eigenvalues of a $2\times2$ matrix $A$. Then which of the following statements must be true. 1 - $A^n$ is not a scalar multiple of identity matrix for any positive positive integer $n$. 2 - $ A^3 = \dfrac{\alpha^3-\beta^3}{\alpha-\beta}A-\alpha\beta(\alpha+\beta)I$ For the statement 1 I picked up a diagonal matrix with diagonal entries 1 and -1 whose square comes out to be identity matrix. Thus statement may be false. But for the second statement i am not able to figure out a way to start. This  probably may be easy but I am not able to get this. Please post a small hint so that I may proceed further.,,"['linear-algebra', 'matrices', 'eigenvalues-eigenvectors']"
83,What is an example of a linear function that maps a matrix to a scalar? What makes it a 'function'?,What is an example of a linear function that maps a matrix to a scalar? What makes it a 'function'?,,"I suppose this is part terminology question and part math, but I am trying to untangle what we mean when we say ""The linear function $f$ maps $R_{m\times n}$ space to $R_{m}$ space"", and ""The linear function $f$ maps $R_{m\times n}$ space to $R_{1}$ space"". To wit: Let us say that there exists an $m\times n$ matrix $A$, and a function $f$ that maps $R_{m\times n}$ space to $R_{m}$ space. In this case, such a function $f$ can be an $n\times 1$ vector $v$. Thus, to apply the function, we have simply: $$ f(A_{m\times n}) = A_{m\times n}v_{n\times 1} =b_{m\times 1}$$ So here, the function $f$ is the vector $v$. My question is when we read the statement ""The linear function $f$ maps the $R_{m\times n}$ space to $R_{1}$ space"", (scalar), then what is an example of this function? It cant be just a vector or just a matrix, so what does it look like? I realize we can do $v^{T}Av$ if $A$ is a square, and this will give a scalar, but what is the function here? Thanks","I suppose this is part terminology question and part math, but I am trying to untangle what we mean when we say ""The linear function $f$ maps $R_{m\times n}$ space to $R_{m}$ space"", and ""The linear function $f$ maps $R_{m\times n}$ space to $R_{1}$ space"". To wit: Let us say that there exists an $m\times n$ matrix $A$, and a function $f$ that maps $R_{m\times n}$ space to $R_{m}$ space. In this case, such a function $f$ can be an $n\times 1$ vector $v$. Thus, to apply the function, we have simply: $$ f(A_{m\times n}) = A_{m\times n}v_{n\times 1} =b_{m\times 1}$$ So here, the function $f$ is the vector $v$. My question is when we read the statement ""The linear function $f$ maps the $R_{m\times n}$ space to $R_{1}$ space"", (scalar), then what is an example of this function? It cant be just a vector or just a matrix, so what does it look like? I realize we can do $v^{T}Av$ if $A$ is a square, and this will give a scalar, but what is the function here? Thanks",,"['matrices', 'functions']"
84,Proof skew-symmetric matrix $A$ exhibits $\vec{x}^T A \vec{x} = 0$,Proof skew-symmetric matrix  exhibits,A \vec{x}^T A \vec{x} = 0,"The condition that $P'^TFP$ is skew-symmetric is equivalent to $\vec{X}^TP'^TFP\vec{X} = 0$ for all $\vec{X}$. [1] The authors say that if A is skew-symmetric than $\vec{x}^T A \vec{x} = 0$. Sadly, there is no further proof of that. Question: Other than expanding the equation (which I did) is there a different proof that holds for any sized skew-symmetric matrix? If so, what is the proof? My thoughts: I can only assume that because (as Wikipedia tells me) a skew-symmetric matrix of odd-size has one eigenvalue equal to zero, there always exists a kernel such that $A\vec{x} = \vec{0}$ but that would be specific to odd-sized matrices and would not hold for all $\vec{x}$. Source: [1] Multiple View Geometry, p.255, Second Edition 2004, Hartley & Zisserman, CUP (Section canonical cameras given F)","The condition that $P'^TFP$ is skew-symmetric is equivalent to $\vec{X}^TP'^TFP\vec{X} = 0$ for all $\vec{X}$. [1] The authors say that if A is skew-symmetric than $\vec{x}^T A \vec{x} = 0$. Sadly, there is no further proof of that. Question: Other than expanding the equation (which I did) is there a different proof that holds for any sized skew-symmetric matrix? If so, what is the proof? My thoughts: I can only assume that because (as Wikipedia tells me) a skew-symmetric matrix of odd-size has one eigenvalue equal to zero, there always exists a kernel such that $A\vec{x} = \vec{0}$ but that would be specific to odd-sized matrices and would not hold for all $\vec{x}$. Source: [1] Multiple View Geometry, p.255, Second Edition 2004, Hartley & Zisserman, CUP (Section canonical cameras given F)",,"['linear-algebra', 'matrices']"
85,How to create 2x2 matrix to rotate vector to right side?,How to create 2x2 matrix to rotate vector to right side?,,"I have vector u=(x,y) and i need to create matrix M: M*u=(1,0) . But that matrix has to rotate vector, instead of keep and scale the x unit. So when i apply it on different vectors, the angle between them won't change. Btw, this isn't homework! We haven't learned any matrices at school yet. ;)","I have vector u=(x,y) and i need to create matrix M: M*u=(1,0) . But that matrix has to rotate vector, instead of keep and scale the x unit. So when i apply it on different vectors, the angle between them won't change. Btw, this isn't homework! We haven't learned any matrices at school yet. ;)",,"['matrices', 'trigonometry', 'rotations']"
86,Equivalent of logarithms for products of matrices,Equivalent of logarithms for products of matrices,,"Let $a, b \in \mathbb{R}$ . We know that $\log(ab) = \log(a) + \log(b)$ . My question is very straightforward: Is there an equivalent function for one of the two most used matrix products? I.e., does there exist a function $f: M_n(\mathbb{R}) \to M_k(\mathbb{R})$ for some $k$ such that either: $f(MN) = f(M) + f(N)$ ; or $f(M \otimes N) = f(M) + f(N)$ , where $\otimes$ denotes the Kronecker product; I've never heard of any such function, though I believe the matrix exponential does the job when going from Kronecker sums to products, and the same for commuting matrices in the regular product. Perhaps this means at least one of the sums above has to be a Kronecker sum, but it would be even better to do it for the regular sum. Note also that I accept any $k$ , including $k = 1$ , in the definition of $f$ . Does anyone know if a function like that exists? Thanks in advance!","Let . We know that . My question is very straightforward: Is there an equivalent function for one of the two most used matrix products? I.e., does there exist a function for some such that either: ; or , where denotes the Kronecker product; I've never heard of any such function, though I believe the matrix exponential does the job when going from Kronecker sums to products, and the same for commuting matrices in the regular product. Perhaps this means at least one of the sums above has to be a Kronecker sum, but it would be even better to do it for the regular sum. Note also that I accept any , including , in the definition of . Does anyone know if a function like that exists? Thanks in advance!","a, b \in \mathbb{R} \log(ab) = \log(a) + \log(b) f: M_n(\mathbb{R}) \to M_k(\mathbb{R}) k f(MN) = f(M) + f(N) f(M \otimes N) = f(M) + f(N) \otimes k k = 1 f","['matrices', 'functional-equations', 'kronecker-product']"
87,Inverse of a special triangular matrix,Inverse of a special triangular matrix,,"Let $A$ be an invertible upper triangular matrix with $A_{i,j}=A_{i+1,j+1}$ for all $i,j$ . How can I show that $A^{-1}$ has the same property? That, it is (an upper triangular) matrix with $A^{-1}_{i,j}=A^{-1}_{i+1,j+1}$ for all $i,j$ . I have verified this using computations, but is there a simple proof? Thanks.","Let be an invertible upper triangular matrix with for all . How can I show that has the same property? That, it is (an upper triangular) matrix with for all . I have verified this using computations, but is there a simple proof? Thanks.","A A_{i,j}=A_{i+1,j+1} i,j A^{-1} A^{-1}_{i,j}=A^{-1}_{i+1,j+1} i,j","['linear-algebra', 'matrices']"
88,Two matrices $A$ and $B$ with $AB+BA=0$,Two matrices  and  with,A B AB+BA=0,"Question: There are two matrices $A$ and $B$ of order $3\times3$ with $AB+BA=0$ , then is it necessary that $AB=0$ ? I tried this problem but couldn't reach any conclusive answer. I didn't see any reason for $AB=0$ , then I tried to get some counterexample. I couldn't find it. Please give me some counterexample if it exists. Also, please tell me how you reached there? I suppose it is not just a matter to attempt with two matrices by trial only. There should be some thought process to find such matrices. In case if you find $AB=0$ , please prove it. Thank you.","Question: There are two matrices and of order with , then is it necessary that ? I tried this problem but couldn't reach any conclusive answer. I didn't see any reason for , then I tried to get some counterexample. I couldn't find it. Please give me some counterexample if it exists. Also, please tell me how you reached there? I suppose it is not just a matter to attempt with two matrices by trial only. There should be some thought process to find such matrices. In case if you find , please prove it. Thank you.",A B 3\times3 AB+BA=0 AB=0 AB=0 AB=0,"['linear-algebra', 'matrices']"
89,Determine the image of the unit circle $S^1$ by the action of the matrix $e^A$.,Determine the image of the unit circle  by the action of the matrix .,S^1 e^A,"We have: $$e^{ \begin{pmatrix} -5 & 9\\ -4 & 7 \end{pmatrix} }$$ I need to determine the image of the unit circle $S^1$ by the action of the matrix $e^A$ . I think that I know how to calculate $e^A$ : I get the Jordan decomposition: $$A = \begin{pmatrix} 		-5 & 9\\ 		-4 & 7 	\end{pmatrix}  = 	\begin{pmatrix} 		-6 & 1\\ 		-4 & 0 	\end{pmatrix} 	\cdot 	\begin{pmatrix} 		1 & 1\\ 		0 & 1 	\end{pmatrix} 	\cdot 	\frac{1}{4} 	\begin{pmatrix} 		0 & -1\\ 		1 & -6 	\end{pmatrix} 	$$ With eigenvalues: $\lambda$ = 1, algebraic multiplicity = 2, eigenvecotrs: $\left\{ \begin{pmatrix} 		1\\ 		0 	\end{pmatrix}, \begin{pmatrix} 	0\\ 	1 	\end{pmatrix} \right\}$ $$ \displaystyle e^A = \sum^{\infty}_{i = 0} \frac{A^i}{i!}$$ $$e^A = 	\begin{pmatrix} 		-6 & 1\\ 		-4 & 0 	\end{pmatrix} 	\cdot 	\left( 	\begin{pmatrix} 		1 & 0\\ 		0 & 1 	\end{pmatrix} 	+ 	\displaystyle \sum^{\infty}_{i = 1} \frac{ \begin{pmatrix} 		1 & 1\\ 		0 & 1 	\end{pmatrix}}{i!} 	\right) 	\cdot 	\frac{1}{4} 	\begin{pmatrix} 		0 & -1\\ 		1 & -6 	\end{pmatrix}$$ $$e^A = \begin{pmatrix} 		-5 & 9\\ 		-4 & 7 	\end{pmatrix}  = 	\begin{pmatrix} 		-6 & 1\\ 		-4 & 0 	\end{pmatrix} 	\cdot 	\begin{pmatrix} 		\displaystyle \sum^{\infty}_{i = 1} \frac{1}{i!}& \displaystyle \sum^{\infty}_{i = 1} \frac{2^{i-1}}{i!}\\ 		0 & \displaystyle \sum^{\infty}_{i = 1} \frac{1}{i!} 	\end{pmatrix} 	\cdot 	\frac{1}{4} 	\begin{pmatrix} 		0 & -1\\ 		1 & -6 	\end{pmatrix}$$ Where: $$\displaystyle \sum^{\infty}_{i = 1} \frac{2^{i-1}}{i!} = \frac{1}{2} \sum^{\infty}_{i = 1} \frac{2^{i}}{i!} = \frac{1}{2}(e^2 - 1) 	$$ So: $$e^A = 	\begin{pmatrix} 		-6 & 1\\ 		-4 & 0 	\end{pmatrix} 	\cdot 	\begin{pmatrix} 		e & \displaystyle \frac{e^2}{2} - \displaystyle \frac{1}{2}\\ 		0 & e 	\end{pmatrix} 	\cdot 	\frac{1}{4} 	\begin{pmatrix} 		0 & -1\\ 		1 & -6 	\end{pmatrix} = 	\begin{pmatrix} 		\displaystyle \frac{-3e^2 + e + 3}{4} & \displaystyle \frac{9e^2 - 9}{2}\\ 		\displaystyle \frac{-e^2 + 1}{2} & 3e^2 + e - 3 	\end{pmatrix} 	$$ Now, I don't know if I did it correctly up to this point and what I should do next - to operate on my unit circle. Solution: Because of @Oscar Lanzi we know that: $$e^{\begin{pmatrix}-5 & 9\\-4 & 7\end{pmatrix}}=e\begin{pmatrix}-5 & 9\\-4 & 7\end{pmatrix}$$ Then because of that: Equation of unit circle under linear transformation - can't understand role of inverse matrix (answer by @Prototank) We know that the image of unit circle in action of the matrix $A$ is given by: $$65x^{2}-166xy+106y^{2}=1$$ Now we need to scale by $e$ and we get the image of unit circle in action of the matrix $e^A$ : $$65x^{2}-166xy+106y^{2}=e^2$$","We have: I need to determine the image of the unit circle by the action of the matrix . I think that I know how to calculate : I get the Jordan decomposition: With eigenvalues: = 1, algebraic multiplicity = 2, eigenvecotrs: Where: So: Now, I don't know if I did it correctly up to this point and what I should do next - to operate on my unit circle. Solution: Because of @Oscar Lanzi we know that: Then because of that: Equation of unit circle under linear transformation - can't understand role of inverse matrix (answer by @Prototank) We know that the image of unit circle in action of the matrix is given by: Now we need to scale by and we get the image of unit circle in action of the matrix :","e^{ \begin{pmatrix}
-5 & 9\\
-4 & 7
\end{pmatrix} } S^1 e^A e^A A = \begin{pmatrix}
		-5 & 9\\
		-4 & 7
	\end{pmatrix}  =
	\begin{pmatrix}
		-6 & 1\\
		-4 & 0
	\end{pmatrix}
	\cdot
	\begin{pmatrix}
		1 & 1\\
		0 & 1
	\end{pmatrix}
	\cdot
	\frac{1}{4}
	\begin{pmatrix}
		0 & -1\\
		1 & -6
	\end{pmatrix}
	 \lambda \left\{ \begin{pmatrix}
		1\\
		0
	\end{pmatrix}, \begin{pmatrix}
	0\\
	1
	\end{pmatrix} \right\}  \displaystyle e^A = \sum^{\infty}_{i = 0} \frac{A^i}{i!} e^A =
	\begin{pmatrix}
		-6 & 1\\
		-4 & 0
	\end{pmatrix}
	\cdot
	\left(
	\begin{pmatrix}
		1 & 0\\
		0 & 1
	\end{pmatrix}
	+
	\displaystyle \sum^{\infty}_{i = 1} \frac{ \begin{pmatrix}
		1 & 1\\
		0 & 1
	\end{pmatrix}}{i!}
	\right)
	\cdot
	\frac{1}{4}
	\begin{pmatrix}
		0 & -1\\
		1 & -6
	\end{pmatrix} e^A = \begin{pmatrix}
		-5 & 9\\
		-4 & 7
	\end{pmatrix}  =
	\begin{pmatrix}
		-6 & 1\\
		-4 & 0
	\end{pmatrix}
	\cdot
	\begin{pmatrix}
		\displaystyle \sum^{\infty}_{i = 1} \frac{1}{i!}& \displaystyle \sum^{\infty}_{i = 1} \frac{2^{i-1}}{i!}\\
		0 & \displaystyle \sum^{\infty}_{i = 1} \frac{1}{i!}
	\end{pmatrix}
	\cdot
	\frac{1}{4}
	\begin{pmatrix}
		0 & -1\\
		1 & -6
	\end{pmatrix} \displaystyle \sum^{\infty}_{i = 1} \frac{2^{i-1}}{i!} = \frac{1}{2} \sum^{\infty}_{i = 1} \frac{2^{i}}{i!} = \frac{1}{2}(e^2 - 1)
	 e^A =
	\begin{pmatrix}
		-6 & 1\\
		-4 & 0
	\end{pmatrix}
	\cdot
	\begin{pmatrix}
		e & \displaystyle \frac{e^2}{2} - \displaystyle \frac{1}{2}\\
		0 & e
	\end{pmatrix}
	\cdot
	\frac{1}{4}
	\begin{pmatrix}
		0 & -1\\
		1 & -6
	\end{pmatrix} =
	\begin{pmatrix}
		\displaystyle \frac{-3e^2 + e + 3}{4} & \displaystyle \frac{9e^2 - 9}{2}\\
		\displaystyle \frac{-e^2 + 1}{2} & 3e^2 + e - 3
	\end{pmatrix}
	 e^{\begin{pmatrix}-5 & 9\\-4 & 7\end{pmatrix}}=e\begin{pmatrix}-5 & 9\\-4 & 7\end{pmatrix} A 65x^{2}-166xy+106y^{2}=1 e e^A 65x^{2}-166xy+106y^{2}=e^2","['linear-algebra', 'matrices', 'matrix-exponential']"
90,derivative of matrix with respect to vector,derivative of matrix with respect to vector,,"I need to calculate the derivative of matrix w.r.t. vector. < Given Equation > 1) $\mathbb Y = \mathbb A \mathbb X$ ,where $\mathbb A$ : (n $\times$ n) matrix $\mathbb X$ : (n $\times$ 1) vector. 2) all elements in $\mathbb A$ and $\mathbb X$ are the function of $z_i$ , where $\mathbb Z = [z_1\ z_2\ \cdots\ z_m]^\top$ In other words, $\mathbb Y(z)=\mathbb A(z) \mathbb X(z)$ < Problem definition > I want to calculate the following partial derivative: $\frac{\partial \mathbb Y}{\partial \mathbb Z}$ , which yields a (n $\times$ m) matrix From the general derivation rule for multiplication, it looks like the rule can be expanded (with some modifications) to the matrix/vector version, $\frac{\partial \mathbb Y}{\partial \mathbb Z} =  \frac{\partial (\mathbb A \mathbb X)}{\partial \mathbb Z} = \frac{\partial \mathbb A}{\partial \mathbb Z}\mathbb X + \mathbb A \frac{\partial \mathbb X}{\partial \mathbb Z}$ However, the above rule is wrong, as you can easily see that the first term's dimension doesn't coincide with (n $\times$ m). I want to calculate the derivation without explicitly calculating all elements in the output $\mathbb Y$ . How can I solve this problem?","I need to calculate the derivative of matrix w.r.t. vector. < Given Equation > 1) ,where : (n n) matrix : (n 1) vector. 2) all elements in and are the function of , where In other words, < Problem definition > I want to calculate the following partial derivative: , which yields a (n m) matrix From the general derivation rule for multiplication, it looks like the rule can be expanded (with some modifications) to the matrix/vector version, However, the above rule is wrong, as you can easily see that the first term's dimension doesn't coincide with (n m). I want to calculate the derivation without explicitly calculating all elements in the output . How can I solve this problem?","\mathbb Y = \mathbb A \mathbb X \mathbb A \times \mathbb X \times \mathbb A \mathbb X z_i \mathbb Z = [z_1\ z_2\ \cdots\ z_m]^\top \mathbb Y(z)=\mathbb A(z) \mathbb X(z) \frac{\partial \mathbb Y}{\partial \mathbb Z} \times \frac{\partial \mathbb Y}{\partial \mathbb Z}
= 
\frac{\partial (\mathbb A \mathbb X)}{\partial \mathbb Z}
=
\frac{\partial \mathbb A}{\partial \mathbb Z}\mathbb X
+
\mathbb A \frac{\partial \mathbb X}{\partial \mathbb Z} \times \mathbb Y","['linear-algebra', 'matrices', 'derivatives', 'partial-derivative', 'matrix-calculus']"
91,Matrix Group under multiplication,Matrix Group under multiplication,,"Suppose the collection $\{A_1, A_2,...,A_k\}$ forms a Group under matrix multiplication, where each $A_i$ is an $n \times n$ real matrix. Let $A = \sum_{i=1}^{k} A_i$ Show that $A^2 = kA$ If the trace of $A$ is zero, then show that $A$ is the zero matrix. Context I am new to Group Theory and while browsing the web, I recently stumbled upon this question. I have tried to prove the first part but have no idea how to proceed with the next. Attempt: Part 1 Let us construct a Cayley Table, T $\times$ $A_1$ $A_2$ $...$ $A_k$ $A_1$ $A_1 A_1$ $A_1 A_2$ $...$ $A_1 A_k$ $A_2$ $A_2 A_1$ $A_2 A_2$ $...$ $A_2 A_k$ $...$ $...$ $...$ $...$ $...$ $A_k$ $A_k A_1$ $A_k A_2$ $...$ $A_k A_k$ $$ A^2 = \{A_1 + A_2 + ... + A_k \}\{A_1 + A_2 + ... + A_k \}\\ A^2 = \{A_1 A_1 + A_1 A_2 + ... + A_1 A_k + A_2 A_1 + A_2 A_2 + ... + A_2 A_k +...A_k A_1 + A_k A_2 + ... + A_k A_k\}\\ A^2 = \sum_{i=0}^k \sum_{j=0}^k a_{ij},   \forall a_{ij} \in T\\  $$ Since, each row of a Cayley table consists of unique elements of the Group, hence the sum of all elements in each row should be equal to $A$ . As there are k such rows, thus, the sum of all the elements of all the rows must be $kA$ . Therefore, $$ A^2 = kA $$ Part 2 If, $$ A^2 = kA \\ \implies A^2 - kA = 0 \\ $$ As eigenvalues of $A$ must satisfy the characteristic equation, hence $$ \lambda^2 - k\lambda = 0 $$ Comparing this with the general format of a quadratic characteristic equation $$ \lambda^2 - (trace (A))\lambda + det(A)=0 $$ We get, $k=trace(A)=0$ , as per the given data. This gives as $\lambda=0,0$ . Hence, all the eigen values are 0. Question: Is there any more elegant proof of the first part than this? How to proceed in the second part? I am completely lost.","Suppose the collection forms a Group under matrix multiplication, where each is an real matrix. Let Show that If the trace of is zero, then show that is the zero matrix. Context I am new to Group Theory and while browsing the web, I recently stumbled upon this question. I have tried to prove the first part but have no idea how to proceed with the next. Attempt: Part 1 Let us construct a Cayley Table, T Since, each row of a Cayley table consists of unique elements of the Group, hence the sum of all elements in each row should be equal to . As there are k such rows, thus, the sum of all the elements of all the rows must be . Therefore, Part 2 If, As eigenvalues of must satisfy the characteristic equation, hence Comparing this with the general format of a quadratic characteristic equation We get, , as per the given data. This gives as . Hence, all the eigen values are 0. Question: Is there any more elegant proof of the first part than this? How to proceed in the second part? I am completely lost.","\{A_1, A_2,...,A_k\} A_i n \times n A = \sum_{i=1}^{k} A_i A^2 = kA A A \times A_1 A_2 ... A_k A_1 A_1 A_1 A_1 A_2 ... A_1 A_k A_2 A_2 A_1 A_2 A_2 ... A_2 A_k ... ... ... ... ... A_k A_k A_1 A_k A_2 ... A_k A_k 
A^2 = \{A_1 + A_2 + ... + A_k \}\{A_1 + A_2 + ... + A_k \}\\
A^2 = \{A_1 A_1 + A_1 A_2 + ... + A_1 A_k + A_2 A_1 + A_2 A_2 + ... + A_2 A_k +...A_k A_1 + A_k A_2 + ... + A_k A_k\}\\
A^2 = \sum_{i=0}^k \sum_{j=0}^k a_{ij},   \forall a_{ij} \in T\\ 
 A kA 
A^2 = kA
 
A^2 = kA \\
\implies A^2 - kA = 0 \\
 A 
\lambda^2 - k\lambda = 0
 
\lambda^2 - (trace (A))\lambda + det(A)=0
 k=trace(A)=0 \lambda=0,0","['matrices', 'group-theory', 'solution-verification', 'cayley-table']"
92,Does dropping off-diagonal elements from a positive semidefinite matrix preserve positive semidefinite property?,Does dropping off-diagonal elements from a positive semidefinite matrix preserve positive semidefinite property?,,"Let $M$ be a symmetric positive semidefinite matrix of size $n \times n$ . Randomly drop any number of off-diagonal elements by setting them to zero on both the upper and lower triangle (keeping symmetry). Is the resulting matrix still positive semidefinite? For example: $$ \begin{pmatrix} 2.7732 & 0.670677 & 0.141774 \\ 0.670677 & 2.50917 & 0.393801 \\ 0.141774 & 0.393801 & 2.3329 \end{pmatrix} \rightarrow \begin{pmatrix} 2.7732 & 0.670677 & 0 \\ 0.670677 & 2.50917 & 0 \\ 0 & 0 & 2.3329  \end{pmatrix} $$ is still positive semidefinite. I observed this numerically by generating matrices following this algorithm , but it certainly may just be due to the distribution that those matrices came from. The number of elements to drop on one of the sides is $r \in \{ 1, 2, \dots, n(n-1)/2 \}$ . Thanks","Let be a symmetric positive semidefinite matrix of size . Randomly drop any number of off-diagonal elements by setting them to zero on both the upper and lower triangle (keeping symmetry). Is the resulting matrix still positive semidefinite? For example: is still positive semidefinite. I observed this numerically by generating matrices following this algorithm , but it certainly may just be due to the distribution that those matrices came from. The number of elements to drop on one of the sides is . Thanks","M n \times n 
\begin{pmatrix}
2.7732 & 0.670677 & 0.141774 \\
0.670677 & 2.50917 & 0.393801 \\
0.141774 & 0.393801 & 2.3329
\end{pmatrix}
\rightarrow
\begin{pmatrix}
2.7732 & 0.670677 & 0 \\
0.670677 & 2.50917 & 0 \\
0 & 0 & 2.3329 
\end{pmatrix}
 r \in \{ 1, 2, \dots, n(n-1)/2 \}","['linear-algebra', 'matrices', 'positive-semidefinite']"
93,Prove identities for matrices.,Prove identities for matrices.,,"Let $A,B,C,D$ be $n \times n$ real valued matrices such that $$AC-BD=Id$$ and $$AD+BC=O$$ Prove $$CA-DB=Id$$ and $$DA+CB=0$$ One idea that I had is to consider the matrices $A,B,C,D$ as blocks of a larger matrix. Consider the matrix $$\begin{pmatrix} A & -B \\ B & A \end{pmatrix} $$ and $$ \begin{pmatrix} C & D \\ D & -C \end{pmatrix} $$ Then $$\begin{pmatrix} A & -B \\ B & A \end{pmatrix} \begin{pmatrix} C & D \\ D & -C \end{pmatrix} =\begin{pmatrix} Id & O \\ O & - Id \end{pmatrix} $$ So I want to prove $$\begin{pmatrix} C & D \\ D & -C \end{pmatrix} \begin{pmatrix} A & B \\ -B & A \end{pmatrix} = \begin{pmatrix} Id & O \\ O & - Id \end{pmatrix} $$ but I haven't had any luck yet, can you help?","Let be real valued matrices such that and Prove and One idea that I had is to consider the matrices as blocks of a larger matrix. Consider the matrix and Then So I want to prove but I haven't had any luck yet, can you help?","A,B,C,D n \times n AC-BD=Id AD+BC=O CA-DB=Id DA+CB=0 A,B,C,D \begin{pmatrix} A & -B \\ B & A \end{pmatrix}   \begin{pmatrix} C & D \\ D & -C \end{pmatrix}  \begin{pmatrix} A & -B \\ B & A \end{pmatrix} \begin{pmatrix} C & D \\ D & -C \end{pmatrix} =\begin{pmatrix} Id & O \\ O & - Id \end{pmatrix}  \begin{pmatrix} C & D \\ D & -C \end{pmatrix} \begin{pmatrix} A & B \\ -B & A \end{pmatrix} = \begin{pmatrix} Id & O \\ O & - Id \end{pmatrix} ","['linear-algebra', 'matrices']"
94,Eigenvalues of a particular block circulant matrix,Eigenvalues of a particular block circulant matrix,,"I need to compute all the eigenvalues of the following block-circulant matrix for a research. Can anyone help me compute the eigenvalues of the following matrix? $$\left[\begin{array}{l}2I&-I&0&0&0&...&0&0&-I\\-I&2I&-I&0&0&...&0&0&0\\0&-I&2I&-I&0&..&0&0&0\\.\\.\\.\\0&0&0&0&0&...&-I&2I&-I\\-I&0&0&0&0&...&0&-I&2I\end{array}\right]$$ In above, $I$ denotes the identity matrix of dimension $a$ and there are $r$ block-rows and block-columns, making the entire matrix have dimension $ar \times ar$ . Any help will be greatly appreciated!","I need to compute all the eigenvalues of the following block-circulant matrix for a research. Can anyone help me compute the eigenvalues of the following matrix? In above, denotes the identity matrix of dimension and there are block-rows and block-columns, making the entire matrix have dimension . Any help will be greatly appreciated!",\left[\begin{array}{l}2I&-I&0&0&0&...&0&0&-I\\-I&2I&-I&0&0&...&0&0&0\\0&-I&2I&-I&0&..&0&0&0\\.\\.\\.\\0&0&0&0&0&...&-I&2I&-I\\-I&0&0&0&0&...&0&-I&2I\end{array}\right] I a r ar \times ar,"['linear-algebra', 'matrices', 'eigenvalues-eigenvectors', 'block-matrices', 'circulant-matrices']"
95,"Let $A_1,A_2,\dots,A_d$ be $d\times d$ matrices that are strictly upper triangular. Then, the product of $A_1,A_2,\dots,A_d$ is the zero matrix.","Let  be  matrices that are strictly upper triangular. Then, the product of  is the zero matrix.","A_1,A_2,\dots,A_d d\times d A_1,A_2,\dots,A_d","SOLUTION : Let $B_i$ be given by $A_1A_2\dots A_i$ . It can be shown inductively that $B_i$ is also strictly triangular, but with at least $i$ zero rows and columns. Therefore, $B_d$ will be the zero matrix. I can't quite wrap my head around this. I would expect that in fact, the product of any two strictly triangular matrices would be the $0$ matrix. And therefore any other matrix (can be non-zero) that is multiplied with it will be the $0$ matrix. But the answer seems to say that it only has at least $i$ zero rows and columns after multiplying $i$ strictly trangiangular matrices. It might be that I am misunderstanding the entire answer in general but any simplification explanation is welcomed. SOURCE : Linear Algebra and Optimization for Machine Learning: A Textbook (Problem 1.23)","SOLUTION : Let be given by . It can be shown inductively that is also strictly triangular, but with at least zero rows and columns. Therefore, will be the zero matrix. I can't quite wrap my head around this. I would expect that in fact, the product of any two strictly triangular matrices would be the matrix. And therefore any other matrix (can be non-zero) that is multiplied with it will be the matrix. But the answer seems to say that it only has at least zero rows and columns after multiplying strictly trangiangular matrices. It might be that I am misunderstanding the entire answer in general but any simplification explanation is welcomed. SOURCE : Linear Algebra and Optimization for Machine Learning: A Textbook (Problem 1.23)",B_i A_1A_2\dots A_i B_i i B_d 0 0 i i,"['linear-algebra', 'matrices', 'solution-verification', 'intuition']"
96,What is the best way to explain why Matrix Multiplication is not commutative?,What is the best way to explain why Matrix Multiplication is not commutative?,,"I have a question asking me why matrix multiplication isn't commutative. I'm not exactly sure what's the best way to explain this without simply saying ""it's obvious"". $AB \not= BA$ because the steps to multiply the values are different going one way and the other way ways. Only in rare circumstances is it commutative.","I have a question asking me why matrix multiplication isn't commutative. I'm not exactly sure what's the best way to explain this without simply saying ""it's obvious"". because the steps to multiply the values are different going one way and the other way ways. Only in rare circumstances is it commutative.",AB \not= BA,['matrices']
97,How to solve for unknown variables in a matrix?,How to solve for unknown variables in a matrix?,,"How do I solve this problem? Ideally, I think $$a^2-4=1$$ is the best option to solve for a I set the entire expression equal to 1. Then to solve for b I just put $b=1$ . Is this right? I feel like that's really wrong. So update I got this I suppose my answer is just b does not equal 0 and a does not equal +- sqrt 4","How do I solve this problem? Ideally, I think is the best option to solve for a I set the entire expression equal to 1. Then to solve for b I just put . Is this right? I feel like that's really wrong. So update I got this I suppose my answer is just b does not equal 0 and a does not equal +- sqrt 4",a^2-4=1 b=1,['matrices']
98,How to show that $ C := A^3-3A^2+2A = 0$?,How to show that ?, C := A^3-3A^2+2A = 0,"Let $$A = \begin{pmatrix} -1 & 1 & 2 \\ 0 & 2 & 0 \\ -1 & 1 & 2  \end{pmatrix}$$ Let $C:= A^3-3A^2+2A $ . Show that $C=0$ . I know that $A$ is diagonalisable with $\operatorname{spec}(A)=\{0,1,2\}$ . I have no clue how to approach that problem. Any advice?",Let Let . Show that . I know that is diagonalisable with . I have no clue how to approach that problem. Any advice?,"A = \begin{pmatrix} -1 & 1 & 2 \\ 0 & 2 & 0 \\ -1 & 1 & 2  \end{pmatrix} C:= A^3-3A^2+2A  C=0 A \operatorname{spec}(A)=\{0,1,2\}","['matrices', 'spectral-theory', 'diagonalization']"
99,"What are some intuitive ways to find a $3 \times 3$ permutation matrix with $P^3 = I$, $P \ne I $?","What are some intuitive ways to find a  permutation matrix with , ?",3 \times 3 P^3 = I P \ne I ,"Find a $3\times3$ permutation matrix with $P^3 = I$ , $P \ne I$ ? I reduced the above problem to $P^T = P^2$ and tried solving for all $6$ $3 \times 3$ permutation matrices which yielded $$P = \begin{pmatrix} 0&1&0\\0 & 0 & 1\\1&0 & 0\end{pmatrix}$$","Find a permutation matrix with , ? I reduced the above problem to and tried solving for all permutation matrices which yielded",3\times3 P^3 = I P \ne I P^T = P^2 6 3 \times 3 P = \begin{pmatrix} 0&1&0\\0 & 0 & 1\\1&0 & 0\end{pmatrix},"['linear-algebra', 'matrices', 'permutations', 'permutation-matrices']"
