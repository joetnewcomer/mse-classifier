,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,Spherical Bessel Functions,Spherical Bessel Functions,,"So I have been given a formula for the spherical Bessel functions in the form of $$ j_\ell(x)=(-x)^\ell \left(\frac{1}{x}\frac{d}{dx}\right)^\ell\frac{\sin(x)}{x} $$ which is Rayleigh's formula. I've been asked to show this satisfies Helmholtz's equation, however, I don't know how to diffentiate the middle part (containing the differential operator). To differentiate this, do I use the product rule as I normally would taking the $-x $, $\frac{1}{x}\frac{d}{dx}$ and $\frac{\sin(x)}{x}$ separately and pulling down the powers? Many thanks in advance.","So I have been given a formula for the spherical Bessel functions in the form of $$ j_\ell(x)=(-x)^\ell \left(\frac{1}{x}\frac{d}{dx}\right)^\ell\frac{\sin(x)}{x} $$ which is Rayleigh's formula. I've been asked to show this satisfies Helmholtz's equation, however, I don't know how to diffentiate the middle part (containing the differential operator). To differentiate this, do I use the product rule as I normally would taking the $-x $, $\frac{1}{x}\frac{d}{dx}$ and $\frac{\sin(x)}{x}$ separately and pulling down the powers? Many thanks in advance.",,"['derivatives', 'bessel-functions']"
1,Find a the value of a point on the tangent line,Find a the value of a point on the tangent line,,"Suppose that the line tangent to the graph of $y = h(x)$ at $x = 3$ passes through the points  $(-2, 3)$ and $(4, -1)$ with a slope of $-2/3$. Find $h(3)$. Hey guys, here's a question from my test review and I know how to do it, I know the answer, but I'm confused as to the ""why"" not the ""how"". Specifically, why should my mind jump straight to ""Use slope-intercept form to find the value at $h(3$)""?","Suppose that the line tangent to the graph of $y = h(x)$ at $x = 3$ passes through the points  $(-2, 3)$ and $(4, -1)$ with a slope of $-2/3$. Find $h(3)$. Hey guys, here's a question from my test review and I know how to do it, I know the answer, but I'm confused as to the ""why"" not the ""how"". Specifically, why should my mind jump straight to ""Use slope-intercept form to find the value at $h(3$)""?",,"['calculus', 'derivatives']"
2,Discrete Time Fourier Transform of the signal represented by $x[n] = n^2 a^n u[n]$,Discrete Time Fourier Transform of the signal represented by,x[n] = n^2 a^n u[n],"I have a homework problem that I am just not sure where to start with.  I have to take the Discrete Time Fourier Transform of a signal represented by: $$x[n] = n^2 a^n u[n]$$ given that $|a| < 1$,  $\Omega_0 < \pi$, and u[n] being the unit step function. There is a hint saying that ""Calculus and derivatives will help!"" but that actually confuses me more than it helps.  However, regardless, I'm just not sure how to even get started evaluating that.  I know how to take the DTFT of a signal, but just can't figure out how to get it into a usable form.  I also am allowed to use the following conversion: $$ x[n] = a^nu[n] \iff X(\Omega) = \frac{e^{j\Omega}}{e^{j\Omega} - a}$$ I am not looking for the answer, but rather just some pointers as to how to start manipulating the original signal.","I have a homework problem that I am just not sure where to start with.  I have to take the Discrete Time Fourier Transform of a signal represented by: $$x[n] = n^2 a^n u[n]$$ given that $|a| < 1$,  $\Omega_0 < \pi$, and u[n] being the unit step function. There is a hint saying that ""Calculus and derivatives will help!"" but that actually confuses me more than it helps.  However, regardless, I'm just not sure how to even get started evaluating that.  I know how to take the DTFT of a signal, but just can't figure out how to get it into a usable form.  I also am allowed to use the following conversion: $$ x[n] = a^nu[n] \iff X(\Omega) = \frac{e^{j\Omega}}{e^{j\Omega} - a}$$ I am not looking for the answer, but rather just some pointers as to how to start manipulating the original signal.",,"['calculus', 'derivatives', 'fourier-analysis', 'signal-processing']"
3,Plotting a function (by hand) if the second derivative is hard to find,Plotting a function (by hand) if the second derivative is hard to find,,"In plotting graphics we use the first derivative to find critical points and in which intervals the function grows and becomes smaller. We can insert the critical points in the second derivative to see if it is convex or concave at these points. But sometimes the second derivative is really hard to find. It is important because it gives us additional information, like inflex points, which help us make our graph more precise. Is there any approach if the second derivative is hard to find?","In plotting graphics we use the first derivative to find critical points and in which intervals the function grows and becomes smaller. We can insert the critical points in the second derivative to see if it is convex or concave at these points. But sometimes the second derivative is really hard to find. It is important because it gives us additional information, like inflex points, which help us make our graph more precise. Is there any approach if the second derivative is hard to find?",,"['calculus', 'derivatives', 'graphing-functions']"
4,Showing Differentiability of Function,Showing Differentiability of Function,,"Prove that $$f(t) = \int_{1}^{\infty} \frac{\sin(tx)}{1+x^{2}} dx $$ is differentiable on $(0, \infty)$. I tried to use dominated convergence theorem but have trouble finding the dominating function.... If we take the derivative $\frac{d}{dt}$ inside the integral, we get $$f_{t}(x) = \frac{x \cos(tx)}{1+x^{2}}$$ inside the integral, but I don't see how I can use the theorem here...","Prove that $$f(t) = \int_{1}^{\infty} \frac{\sin(tx)}{1+x^{2}} dx $$ is differentiable on $(0, \infty)$. I tried to use dominated convergence theorem but have trouble finding the dominating function.... If we take the derivative $\frac{d}{dt}$ inside the integral, we get $$f_{t}(x) = \frac{x \cos(tx)}{1+x^{2}}$$ inside the integral, but I don't see how I can use the theorem here...",,"['real-analysis', 'derivatives']"
5,I need help with a partial derivative,I need help with a partial derivative,,"I was given a function and I need to find a partial derivative of it. The result I got is different from the answer, and I don't know why. Here's the function: $$sin(\theta_{a}) = \frac{\sqrt{[R_Y\sin(a) - R_Z\cos(a)\cos(A)]^2 + [R_X\sin(a) + R_Z\sin(A)\cos(a)]^2 + [R_Y\sin(A)\cos(a) + R_X\cos(A)\cos(a)]^2}}{\mid R \mid}$$ where $$ R = \sqrt{R_X^2 + R_Y^2 + R_Z^2} $$ Here's what I need to do: $$ \frac{\partial \sin(\theta_{a})}{\partial R_Z} $$ The correct answer is: $$ \frac{\partial \sin(\theta_{a})}{\partial R_Z} = \frac{1}{2} \frac{1}{\mid R \mid} \frac{1}{\sqrt{...}}\big\{-2\cos(a)\sin(A)[R_Y\sin(a)-R_Z\cos(a)\cos(A)] + 2\sin(A)\cos(a)[R_X\sin(a)+R_Z\sin(A)\cos(a)]\big\}$$ Where $ \sqrt{...} $ is the numerator of the function. My result fallows this derivation principle: $$ \frac{f'(x) \cdot g(x) - f(x) \cdot g'(x)}{g(x)^2}$$ However the only way to get the result shown above is by not deriving the denominator $\mid R \mid $ by $ R_Z $ but considering it instead as constant.  Is there a reason why I shouldn't do it or am I mistaken somewhere?","I was given a function and I need to find a partial derivative of it. The result I got is different from the answer, and I don't know why. Here's the function: $$sin(\theta_{a}) = \frac{\sqrt{[R_Y\sin(a) - R_Z\cos(a)\cos(A)]^2 + [R_X\sin(a) + R_Z\sin(A)\cos(a)]^2 + [R_Y\sin(A)\cos(a) + R_X\cos(A)\cos(a)]^2}}{\mid R \mid}$$ where $$ R = \sqrt{R_X^2 + R_Y^2 + R_Z^2} $$ Here's what I need to do: $$ \frac{\partial \sin(\theta_{a})}{\partial R_Z} $$ The correct answer is: $$ \frac{\partial \sin(\theta_{a})}{\partial R_Z} = \frac{1}{2} \frac{1}{\mid R \mid} \frac{1}{\sqrt{...}}\big\{-2\cos(a)\sin(A)[R_Y\sin(a)-R_Z\cos(a)\cos(A)] + 2\sin(A)\cos(a)[R_X\sin(a)+R_Z\sin(A)\cos(a)]\big\}$$ Where $ \sqrt{...} $ is the numerator of the function. My result fallows this derivation principle: $$ \frac{f'(x) \cdot g(x) - f(x) \cdot g'(x)}{g(x)^2}$$ However the only way to get the result shown above is by not deriving the denominator $\mid R \mid $ by $ R_Z $ but considering it instead as constant.  Is there a reason why I shouldn't do it or am I mistaken somewhere?",,"['derivatives', 'partial-derivative']"
6,How does this derivative notation work?,How does this derivative notation work?,,"Elasticity of substitution = $\dfrac{\mathrm d \ln(k/l)\;\;\;}{\mathrm d \ln(f_l/f_k)}$ This is type of notation I haven't really worked with before. Is this read as ""The change in the natural log of k over l with respect to the change in the natural log of the partial derivative of f with respect to l over the partial derivative of f with respect to k""? The production function I am working with specifically is $f(k, l) = k^{1/3}l^{2/3}$ and I know it should equal 1 (via another elasticity of substitution equation). But what and how does this equation I posted work? Thanks :)","Elasticity of substitution = $\dfrac{\mathrm d \ln(k/l)\;\;\;}{\mathrm d \ln(f_l/f_k)}$ This is type of notation I haven't really worked with before. Is this read as ""The change in the natural log of k over l with respect to the change in the natural log of the partial derivative of f with respect to l over the partial derivative of f with respect to k""? The production function I am working with specifically is $f(k, l) = k^{1/3}l^{2/3}$ and I know it should equal 1 (via another elasticity of substitution equation). But what and how does this equation I posted work? Thanks :)",,"['calculus', 'derivatives', 'economics']"
7,"$f(x)$ is a differentiable function for $x\in[a,b]$, and $f'(a)=f'(b)$, prove: there is a $\theta$ such that...","is a differentiable function for , and , prove: there is a  such that...","f(x) x\in[a,b] f'(a)=f'(b) \theta","$f(x)$ is a differentiable function for $x\in[a,b]$  ($f'(x)$ may not continuously), and $f'(a)=f'(b)$, prove: there is a $\theta$ such that  $$f'(\theta)=\frac{f(a)-f(\theta)}{a-\theta}$$ I think we should be using the Intermediate Value Theorem to solve, but i hava not idea how to get the key point, please help me.","$f(x)$ is a differentiable function for $x\in[a,b]$  ($f'(x)$ may not continuously), and $f'(a)=f'(b)$, prove: there is a $\theta$ such that  $$f'(\theta)=\frac{f(a)-f(\theta)}{a-\theta}$$ I think we should be using the Intermediate Value Theorem to solve, but i hava not idea how to get the key point, please help me.",,"['calculus', 'integration', 'derivatives']"
8,Determining $\frac{d^2\arcsin(2x)}{dx^2}$,Determining,\frac{d^2\arcsin(2x)}{dx^2},I am hitting my head against a wall trying to understand how to differentiate this. $$f(x) = \frac{d^2\arcsin(2x)}{dx^2}$$ Can someone please hold my hand through this?  I understand that $\arcsin(2x) = \sin^{-1}(2x)$ . Is this implicit differentiation?,I am hitting my head against a wall trying to understand how to differentiate this. Can someone please hold my hand through this?  I understand that . Is this implicit differentiation?,f(x) = \frac{d^2\arcsin(2x)}{dx^2} \arcsin(2x) = \sin^{-1}(2x),"['calculus', 'trigonometry', 'derivatives']"
9,Prove that $\frac{d}{dx}(\tan^{-1}(x))=\frac{1}{1+x^2}dx$,Prove that,\frac{d}{dx}(\tan^{-1}(x))=\frac{1}{1+x^2}dx,Prove that $$\frac{d}{dx}(\tan^{-1}(x))=\frac{1}{1+x^2}$$,Prove that $$\frac{d}{dx}(\tan^{-1}(x))=\frac{1}{1+x^2}$$,,['calculus']
10,Find derivative of tricky logarithmic functions,Find derivative of tricky logarithmic functions,,"Find the derivative of $y=(x^{x+1})(x+1)^x$ So this is what I have, $$\ln y=\ln[(x^{x+1})(x+1)^x]$$ $$= \ln x^{x+1} + \ln(x+1)^x$$ $$\frac{1}{y}y' = (1)(\ln x) + (x+1)\frac{1}{x} + (1)(\ln(x+1)) + (x)\frac{1}{x+1}$$ $$= \ln x + \frac{x+1}{x} + \ln(x+1) + \frac{x}{x+1}$$ $$y' = (x^{x+1})(x+1)^x \left[\ln x + \ln(x+1) + \frac{x+1}{x} + \frac{x}{x+1}\right]$$ Is that the right answer?","Find the derivative of $y=(x^{x+1})(x+1)^x$ So this is what I have, $$\ln y=\ln[(x^{x+1})(x+1)^x]$$ $$= \ln x^{x+1} + \ln(x+1)^x$$ $$\frac{1}{y}y' = (1)(\ln x) + (x+1)\frac{1}{x} + (1)(\ln(x+1)) + (x)\frac{1}{x+1}$$ $$= \ln x + \frac{x+1}{x} + \ln(x+1) + \frac{x}{x+1}$$ $$y' = (x^{x+1})(x+1)^x \left[\ln x + \ln(x+1) + \frac{x+1}{x} + \frac{x}{x+1}\right]$$ Is that the right answer?",,"['derivatives', 'logarithms']"
11,How to get tangent of inverse of curve??,How to get tangent of inverse of curve??,,"Ok so my question is. Let $ f(x)=(1/7)x^3+21x-1.$ and let y=g(x) be the inverse function of f. Determine all points on the graph of the inverse function g so that the tangent line is perpendicular to the straight line $ y=-42x+4. $There are two point $P(x1,y1)$ and $Q(x2,y2)$ where y1 So i put the equation into wolfram and got a very long inverse. $-(7 2^(1/3) 7^(2/3))/(1+x+sqrt(9605+2 x+x^2))^(1/3)+(7/2)^(1/3) (1+x+sqrt(9605+2 x+x^2))^(1/3) $ now i took the derivative of this  $(14 2^(1/3) 7^(2/3) + 2^(2/3) 7^(1/3) (1 + x + sqrt[9605 + 2 x + x^2])^(2/3))/(6 Sqrt[9605 + 2 x + x^2] (1 + x + Sqrt[9605 + 2 x + x^2])^(1/3))$  i equated the derivative to $1/42$  and solved for x and got $x=195$ and $x=-197$ is this correct so far? and how do i solve for the rest?","Ok so my question is. Let $ f(x)=(1/7)x^3+21x-1.$ and let y=g(x) be the inverse function of f. Determine all points on the graph of the inverse function g so that the tangent line is perpendicular to the straight line $ y=-42x+4. $There are two point $P(x1,y1)$ and $Q(x2,y2)$ where y1 So i put the equation into wolfram and got a very long inverse. $-(7 2^(1/3) 7^(2/3))/(1+x+sqrt(9605+2 x+x^2))^(1/3)+(7/2)^(1/3) (1+x+sqrt(9605+2 x+x^2))^(1/3) $ now i took the derivative of this  $(14 2^(1/3) 7^(2/3) + 2^(2/3) 7^(1/3) (1 + x + sqrt[9605 + 2 x + x^2])^(2/3))/(6 Sqrt[9605 + 2 x + x^2] (1 + x + Sqrt[9605 + 2 x + x^2])^(1/3))$  i equated the derivative to $1/42$  and solved for x and got $x=195$ and $x=-197$ is this correct so far? and how do i solve for the rest?",,"['calculus', 'derivatives', 'inverse']"
12,differentiation of the following equation 3,differentiation of the following equation 3,,"i already done the differentiation, just wanna confirm either i got it right or wrong. Can someone verify this for me. 1) f(x) = $ -3\over x^{5/2}$ f '(x) = $ 3({ 5\over 2}x^{3/2})$ . $\frac{1}{x^5}$ = $ { 15\over 2}x^{(3/2)-5}$ = ${ 15\over 2}x^{-7/2}$ 2) f(x) = $\frac{2x^2 + 3}{(x^3 - 4)^3}$ f ' (x) $= \frac{(4x) (x^3 - 4)^3  - (2x^2 + 3)[3(x^3 -4)^2 (3x)}{[(x^3 - 4)^3]^2}$ $= \frac{(4x) (x^3 - 4)^3  - 9x^2 (2x^2 + 3)(x^3 -4)^2}{(x^3 - 4)^6}$ $= \frac{(4x) (x^3 - 4)^3  - 9x^2 (2x^2 + 3)}{(x^3 - 4)^4}$  ---quotient rule 3) f(x) = sin(x cos x) $f ' (x) = \cos [x \cos(x)] (\cos x - x \sin x)$ 4) f(x) = $x^2$ tan 2x $f ' (x) =  2x \tan (2x)$ + $2x^2$ $\sec^2 2x$ 5) f(x) = $ 3 \ln (\cos 3x) $ $f ' (x) =  -9 \tan 3x$","i already done the differentiation, just wanna confirm either i got it right or wrong. Can someone verify this for me. 1) f(x) = $ -3\over x^{5/2}$ f '(x) = $ 3({ 5\over 2}x^{3/2})$ . $\frac{1}{x^5}$ = $ { 15\over 2}x^{(3/2)-5}$ = ${ 15\over 2}x^{-7/2}$ 2) f(x) = $\frac{2x^2 + 3}{(x^3 - 4)^3}$ f ' (x) $= \frac{(4x) (x^3 - 4)^3  - (2x^2 + 3)[3(x^3 -4)^2 (3x)}{[(x^3 - 4)^3]^2}$ $= \frac{(4x) (x^3 - 4)^3  - 9x^2 (2x^2 + 3)(x^3 -4)^2}{(x^3 - 4)^6}$ $= \frac{(4x) (x^3 - 4)^3  - 9x^2 (2x^2 + 3)}{(x^3 - 4)^4}$  ---quotient rule 3) f(x) = sin(x cos x) $f ' (x) = \cos [x \cos(x)] (\cos x - x \sin x)$ 4) f(x) = $x^2$ tan 2x $f ' (x) =  2x \tan (2x)$ + $2x^2$ $\sec^2 2x$ 5) f(x) = $ 3 \ln (\cos 3x) $ $f ' (x) =  -9 \tan 3x$",,"['calculus', 'derivatives']"
13,How to find the derivative of $(3x-1)^2(2x+3)^2$,How to find the derivative of,(3x-1)^2(2x+3)^2,"I used the power rule and the chain rule and ended up with this: $$y'= (3x-1)^2 \times 2(2x+3) \times 2 + (2x+3)^2 \times 2(3x-1)\times 3$$ The next step, which I do not understand how it is combined or created is this: $$y'= 2(3x-1)(2x+3)\left[2\cdot(3x-1)+3(2x+3)\right]$$ Where did the exponents go? What is combined? How is it combined? The Final Answer should be this, according to my teacher:  $$y'= 2(3x-1)(2x+3)(12x+18)$$","I used the power rule and the chain rule and ended up with this: $$y'= (3x-1)^2 \times 2(2x+3) \times 2 + (2x+3)^2 \times 2(3x-1)\times 3$$ The next step, which I do not understand how it is combined or created is this: $$y'= 2(3x-1)(2x+3)\left[2\cdot(3x-1)+3(2x+3)\right]$$ Where did the exponents go? What is combined? How is it combined? The Final Answer should be this, according to my teacher:  $$y'= 2(3x-1)(2x+3)(12x+18)$$",,"['calculus', 'derivatives']"
14,"If all directional derivatives are $0$, the function is constant.","If all directional derivatives are , the function is constant.",0,"Let $f:\mathbb{R}^{n} \rightarrow \mathbb{R}$ be differentiable in every point of the disc $ B_{r}(\vec{a})$. If $D_{\vec{y}}f(\vec{x})=0$ for $n$ linearly independent vectors $\vec{y}_{1}, \vec{y}_{2},...,\vec{y}_{n}$ and $\forall \vec{x} \in B_{r}(\vec{a})$, show that $f$ is constant. I've proven the following statement: Let $f$ be a function of $n$ variables such that the directional derivatives $D_{\vec{y}}f(\vec{a}+t\vec{y})$ exist $\forall t \in [0,1]$. Then, there exists a real number $\theta \in ]0,1[$ such that: \begin{equation} f(\vec{a}+\vec{y})-f(\vec{a}) =  D_{\vec{y}}f(\vec{a}+\theta \vec{y})  \end{equation} I believe that this question may be answered by means of a corollary of this last statement, or by using a similar version of the Mean Value Theorem . I would appreciate any help.","Let $f:\mathbb{R}^{n} \rightarrow \mathbb{R}$ be differentiable in every point of the disc $ B_{r}(\vec{a})$. If $D_{\vec{y}}f(\vec{x})=0$ for $n$ linearly independent vectors $\vec{y}_{1}, \vec{y}_{2},...,\vec{y}_{n}$ and $\forall \vec{x} \in B_{r}(\vec{a})$, show that $f$ is constant. I've proven the following statement: Let $f$ be a function of $n$ variables such that the directional derivatives $D_{\vec{y}}f(\vec{a}+t\vec{y})$ exist $\forall t \in [0,1]$. Then, there exists a real number $\theta \in ]0,1[$ such that: \begin{equation} f(\vec{a}+\vec{y})-f(\vec{a}) =  D_{\vec{y}}f(\vec{a}+\theta \vec{y})  \end{equation} I believe that this question may be answered by means of a corollary of this last statement, or by using a similar version of the Mean Value Theorem . I would appreciate any help.",,"['real-analysis', 'derivatives']"
15,There is a unique polynomial interpolating $f$ and its derivatives,There is a unique polynomial interpolating  and its derivatives,f,"I have questions on a similar topic here , here , and here , but this is a different question. It is well-known that a Hermite interpolation polynomial (where we sample the function and its derivatives at certain points) exists uniquely. That means that the sort of ""modified Vandermonde matrix"" such as  $$ \left[ \begin{matrix} 1&\alpha_1&\alpha_1^2&\alpha_1^3&\alpha_1^4 \\ 0&1&2\alpha_1 & 2\alpha_1^2 & 4\alpha_1^3 \\ 0&0&2&6\alpha_1 &12 \alpha_1^2 \\ 1 & \alpha_2 & \alpha_2^2 & \alpha_2^3 & \alpha_2^4\\ 0&1&2\alpha_2 &3\alpha_2^2 &4\alpha_2^3 \end{matrix} \right] $$ is invertible for $\alpha_1 \neq \alpha_2$, because $$ \left[ \begin{matrix} 1&\alpha_1&\alpha_1^2&\alpha_1^3&\alpha_1^4 \\ 0&1&2\alpha_1 & 2\alpha_1^2 & 4\alpha_1^3 \\ 0&0&2&6\alpha_1 &12 \alpha_1^2 \\ 1 & \alpha_2 & \alpha_2^2 & \alpha_2^3 & \alpha_2^4\\ 0&1&2\alpha_2 &3\alpha_2^2 &4\alpha_2^3 \end{matrix} \right] \left[ \begin{matrix} c_0 \\ c_1 \\ c_2 \\ c_3 \\ c_4 \end{matrix} \right] = \left[ \begin{matrix} f(\alpha_1) \\ f'(\alpha_1)\\ f''(\alpha_1) \\ f(\alpha_2) \\ f'(\alpha_2) \end{matrix} \right] $$ is the equation for the coefficients of the Hermite polynomial $c_4 x^4 + c_3 x^3 + \dotsb + c_0$ that agrees with $f$ at $\alpha_1$ up to order 2 and at $\alpha_2$ up to order 1. This will be the unique polynomial of degree $\leq 4$ with this property. This matrix has another interesting application, which I'll place below my question. I'm wondering: how can we show this matrix, and others of the same form, are invertible? The normal way to show uniqueness of the Hermite interpolation is through ""divided differences"", and I'd like a proof that doesn't rely on them. Things I've tried: Playing with column reduction, similar to the way that we show that the Vandermonde matrix has determinant $\prod_{i<j}(x_i - x_j)$. It got messy. Note that the determinant of the matrix is  $$ \frac{\partial^4}{\partial x_2 \partial x_3^2 \partial x_5}\left((x_1, \dotsc, x_5)\mapsto \prod_{1\leq i < j \leq 5}(x_i - x_j)\right) \left. \right|_{\substack{x_1=x_2=x_3=\alpha_1\\ x_4=x_5=\alpha_2}}. $$ Through computation I could show this is nonzero in this case, but I haven't found a way to extend this result more generally. I claim also that the invertibility of a matrix like this one (or, more precisely, its transpose) is exactly what we need in order to show that there is a unique solution to an initial value problem $f^{(n)}=\sum a_i f^{(i)}$ when we have initial data at zero, and when $x^n - \sum a_i x^i$ has repeated roots. The above case would be if $\alpha_1$ were a thrice-repeated root, and $\alpha_2$ were a twice-repeated root. Then the system would be $$ \left[ \begin{matrix} 1&\alpha_1&\alpha_1^2&\alpha_1^3&\alpha_1^4 \\ 0&1&2\alpha_1 & 2\alpha_1^2 & 4\alpha_1^3 \\ 0&0&2&6\alpha_1 &12 \alpha_1^2 \\ 1 & \alpha_2 & \alpha_2^2 & \alpha_2^3 & \alpha_2^4\\ 0&1&2\alpha_2 &3\alpha_2^2 &4\alpha_2^3 \end{matrix} \right]^T \left[ \begin{matrix} c_1 \\ c_2 \\ c_3 \\ c_4 \\ c_5 \end{matrix} \right] = \left[ \begin{matrix} f(0) \\ f'(0)\\ f''(0) \\ f^{(3)}(0) \\ f^{(4)}(0) \end{matrix} \right], $$ where we want a solution of the form $c_1 e^{\alpha_1 x} + c_2 x e^{\alpha_1 x} + c_3 x^2 e^{\alpha_1 x} + c_4 e^{\alpha_2 x} + c_5 x e^{\alpha_2 x}$.","I have questions on a similar topic here , here , and here , but this is a different question. It is well-known that a Hermite interpolation polynomial (where we sample the function and its derivatives at certain points) exists uniquely. That means that the sort of ""modified Vandermonde matrix"" such as  $$ \left[ \begin{matrix} 1&\alpha_1&\alpha_1^2&\alpha_1^3&\alpha_1^4 \\ 0&1&2\alpha_1 & 2\alpha_1^2 & 4\alpha_1^3 \\ 0&0&2&6\alpha_1 &12 \alpha_1^2 \\ 1 & \alpha_2 & \alpha_2^2 & \alpha_2^3 & \alpha_2^4\\ 0&1&2\alpha_2 &3\alpha_2^2 &4\alpha_2^3 \end{matrix} \right] $$ is invertible for $\alpha_1 \neq \alpha_2$, because $$ \left[ \begin{matrix} 1&\alpha_1&\alpha_1^2&\alpha_1^3&\alpha_1^4 \\ 0&1&2\alpha_1 & 2\alpha_1^2 & 4\alpha_1^3 \\ 0&0&2&6\alpha_1 &12 \alpha_1^2 \\ 1 & \alpha_2 & \alpha_2^2 & \alpha_2^3 & \alpha_2^4\\ 0&1&2\alpha_2 &3\alpha_2^2 &4\alpha_2^3 \end{matrix} \right] \left[ \begin{matrix} c_0 \\ c_1 \\ c_2 \\ c_3 \\ c_4 \end{matrix} \right] = \left[ \begin{matrix} f(\alpha_1) \\ f'(\alpha_1)\\ f''(\alpha_1) \\ f(\alpha_2) \\ f'(\alpha_2) \end{matrix} \right] $$ is the equation for the coefficients of the Hermite polynomial $c_4 x^4 + c_3 x^3 + \dotsb + c_0$ that agrees with $f$ at $\alpha_1$ up to order 2 and at $\alpha_2$ up to order 1. This will be the unique polynomial of degree $\leq 4$ with this property. This matrix has another interesting application, which I'll place below my question. I'm wondering: how can we show this matrix, and others of the same form, are invertible? The normal way to show uniqueness of the Hermite interpolation is through ""divided differences"", and I'd like a proof that doesn't rely on them. Things I've tried: Playing with column reduction, similar to the way that we show that the Vandermonde matrix has determinant $\prod_{i<j}(x_i - x_j)$. It got messy. Note that the determinant of the matrix is  $$ \frac{\partial^4}{\partial x_2 \partial x_3^2 \partial x_5}\left((x_1, \dotsc, x_5)\mapsto \prod_{1\leq i < j \leq 5}(x_i - x_j)\right) \left. \right|_{\substack{x_1=x_2=x_3=\alpha_1\\ x_4=x_5=\alpha_2}}. $$ Through computation I could show this is nonzero in this case, but I haven't found a way to extend this result more generally. I claim also that the invertibility of a matrix like this one (or, more precisely, its transpose) is exactly what we need in order to show that there is a unique solution to an initial value problem $f^{(n)}=\sum a_i f^{(i)}$ when we have initial data at zero, and when $x^n - \sum a_i x^i$ has repeated roots. The above case would be if $\alpha_1$ were a thrice-repeated root, and $\alpha_2$ were a twice-repeated root. Then the system would be $$ \left[ \begin{matrix} 1&\alpha_1&\alpha_1^2&\alpha_1^3&\alpha_1^4 \\ 0&1&2\alpha_1 & 2\alpha_1^2 & 4\alpha_1^3 \\ 0&0&2&6\alpha_1 &12 \alpha_1^2 \\ 1 & \alpha_2 & \alpha_2^2 & \alpha_2^3 & \alpha_2^4\\ 0&1&2\alpha_2 &3\alpha_2^2 &4\alpha_2^3 \end{matrix} \right]^T \left[ \begin{matrix} c_1 \\ c_2 \\ c_3 \\ c_4 \\ c_5 \end{matrix} \right] = \left[ \begin{matrix} f(0) \\ f'(0)\\ f''(0) \\ f^{(3)}(0) \\ f^{(4)}(0) \end{matrix} \right], $$ where we want a solution of the form $c_1 e^{\alpha_1 x} + c_2 x e^{\alpha_1 x} + c_3 x^2 e^{\alpha_1 x} + c_4 e^{\alpha_2 x} + c_5 x e^{\alpha_2 x}$.",,"['calculus', 'polynomials', 'derivatives', 'interpolation']"
16,Problem with notation in a thesis,Problem with notation in a thesis,,"I am struggling with section 3.3 of the following thesis https://smartech.gatech.edu/xmlui/bitstream/handle/1853/29610/grigo_alexander_200908_phd.pdf .  Page 21 is fine, then the problems occur in pages 22 and 23 which I believe are mainly down to notation. ""For $\epsilon_0 > 0$ consider a family of $C^5$ function $L_{\epsilon}: U \times U \rightarrow \mathbb{R^2}$ for $|\epsilon|< \epsilon_0$ which satisfy $ \displaystyle \partial_{\epsilon}\Big|_{\epsilon=0}L_{\epsilon}(s,s_1)=C \frac{s^4+s_1^4}{24}+O_5(s,s_1)$ and $\partial_s \partial_{s_1}L_0(0,0) \neq 0$ for some $C \neq 0$."" What do the $\partial_{\epsilon}$ and $\partial_s \partial_{s_1}$ mean? What do they act on? Then $L_{ij} := \partial^i_s \partial^j_{s_1}L_0(s,s_1)$. What are $\partial^i_s$ and $\partial^j_{s_1}$, what do $i$ and $j$ correspond to? In particular on page 23 I cannot see how $\partial_s L_\epsilon(s, S_{\epsilon}(s,y))=y$ and $\partial_{s_1}L_{\epsilon}(s,S_{\epsilon}(s,y))=Y_{\epsilon}(s,y)$ are derived? I seem to be missing some identities used. Finally, on page 24, I cannot see how $\partial_{\epsilon}\Big|_{\epsilon=0}A_{\epsilon}$ only the third order term $Im c_{21}$? Why is it third order, why arent the other terms third order?","I am struggling with section 3.3 of the following thesis https://smartech.gatech.edu/xmlui/bitstream/handle/1853/29610/grigo_alexander_200908_phd.pdf .  Page 21 is fine, then the problems occur in pages 22 and 23 which I believe are mainly down to notation. ""For $\epsilon_0 > 0$ consider a family of $C^5$ function $L_{\epsilon}: U \times U \rightarrow \mathbb{R^2}$ for $|\epsilon|< \epsilon_0$ which satisfy $ \displaystyle \partial_{\epsilon}\Big|_{\epsilon=0}L_{\epsilon}(s,s_1)=C \frac{s^4+s_1^4}{24}+O_5(s,s_1)$ and $\partial_s \partial_{s_1}L_0(0,0) \neq 0$ for some $C \neq 0$."" What do the $\partial_{\epsilon}$ and $\partial_s \partial_{s_1}$ mean? What do they act on? Then $L_{ij} := \partial^i_s \partial^j_{s_1}L_0(s,s_1)$. What are $\partial^i_s$ and $\partial^j_{s_1}$, what do $i$ and $j$ correspond to? In particular on page 23 I cannot see how $\partial_s L_\epsilon(s, S_{\epsilon}(s,y))=y$ and $\partial_{s_1}L_{\epsilon}(s,S_{\epsilon}(s,y))=Y_{\epsilon}(s,y)$ are derived? I seem to be missing some identities used. Finally, on page 24, I cannot see how $\partial_{\epsilon}\Big|_{\epsilon=0}A_{\epsilon}$ only the third order term $Im c_{21}$? Why is it third order, why arent the other terms third order?",,"['derivatives', 'notation']"
17,$n$th derivative of $f(x)^{n+1}$,th derivative of,n f(x)^{n+1},"Hello, I'm trying to find the $n$th derivative of a function, where $n\in\mathbb N$   $$\frac{d^{n-1}}{dx^{n-1}} \!\!\left[f(x)^n\right]$$   I'm looking for some kind of sum or product (or nesting of these) which gives the value above. So I tried evaluating the function at some values of $n$ $$ \begin{array}{lr} f(x) & \text{for  } n=1\\ 2f(x)f^{(1)}(x) & n=2\\ 3f(x)^2f^{(2)}(x)+6f(x)f^{(1)}(x)^2&n=3\\ 4f(x)f^{(4)}+24f(x)f^{(1)}(x)+\cdots\\ \qquad36f(x)^2f^{(1)}(x)f^{(2)}(x) & n=4 \end{array} $$ Some things I noticed was 1 . In expanded form, it always had a term of the form $$nf(x)^{n-1}f^{(n-1)}(x)$$ 2 . In expanded form you could always factor out $nf(x)$ from all terms 3 . $f$ is always multiplied together with itself (or a derivative of itself) $n$ times in each term. 4 . There's always $n-1$ terms, in the case of $n=1$ it seems to be the always being there factor of $nf(x)$ multiplied with the empty product. Tip: I have found the wolfram alpha input D[f[x]^n, {x,n-1}], n -> (insert n here) very useful for calculating the $n$'th version of this formula. Note: The notation below means the $m$th derivative of $f$ $$f^{(m)}(x)$$","Hello, I'm trying to find the $n$th derivative of a function, where $n\in\mathbb N$   $$\frac{d^{n-1}}{dx^{n-1}} \!\!\left[f(x)^n\right]$$   I'm looking for some kind of sum or product (or nesting of these) which gives the value above. So I tried evaluating the function at some values of $n$ $$ \begin{array}{lr} f(x) & \text{for  } n=1\\ 2f(x)f^{(1)}(x) & n=2\\ 3f(x)^2f^{(2)}(x)+6f(x)f^{(1)}(x)^2&n=3\\ 4f(x)f^{(4)}+24f(x)f^{(1)}(x)+\cdots\\ \qquad36f(x)^2f^{(1)}(x)f^{(2)}(x) & n=4 \end{array} $$ Some things I noticed was 1 . In expanded form, it always had a term of the form $$nf(x)^{n-1}f^{(n-1)}(x)$$ 2 . In expanded form you could always factor out $nf(x)$ from all terms 3 . $f$ is always multiplied together with itself (or a derivative of itself) $n$ times in each term. 4 . There's always $n-1$ terms, in the case of $n=1$ it seems to be the always being there factor of $nf(x)$ multiplied with the empty product. Tip: I have found the wolfram alpha input D[f[x]^n, {x,n-1}], n -> (insert n here) very useful for calculating the $n$'th version of this formula. Note: The notation below means the $m$th derivative of $f$ $$f^{(m)}(x)$$",,"['derivatives', 'summation']"
18,Calc II - Definite integral of sqrt(t^2 + t) from 2x to 1?,Calc II - Definite integral of sqrt(t^2 + t) from 2x to 1?,,"How do I find $$\int_1^{2x}\sqrt{t^2 + t}$$ with only knowledge from a Calculus I course? I've tried plugging this puppy into Wolfram Alpha and other integral solvers, which report it as solvable (looks really long and nasty), but I think this is outside the scope of my just-entered-Calc-II knowledge and that I need to solve it in a tricky way. The problem is part of the linked green sheet. Since I don't know how to integrate this I've tried to solve the sheet without doing so, though on part D) it seems like my luck is about to run out. I doubt what I am trying to do is even legal. Please advise.","How do I find $$\int_1^{2x}\sqrt{t^2 + t}$$ with only knowledge from a Calculus I course? I've tried plugging this puppy into Wolfram Alpha and other integral solvers, which report it as solvable (looks really long and nasty), but I think this is outside the scope of my just-entered-Calc-II knowledge and that I need to solve it in a tricky way. The problem is part of the linked green sheet. Since I don't know how to integrate this I've tried to solve the sheet without doing so, though on part D) it seems like my luck is about to run out. I doubt what I am trying to do is even legal. Please advise.",,"['calculus', 'integration', 'derivatives', 'definite-integrals']"
19,Computing the derivative of a transformation matrix,Computing the derivative of a transformation matrix,,"I am trying to find a geometric transformation between two images, where the transformation is a simple scaling matrix. So, if I denote the two image functions as $r$ and $f$ and the scaling matrix as $S$, I am trying to minimize the following cost function (CF) with respect to the scaling parameters: $$ CF = 0.5 * (r(x) - f(S(x)))^{2} $$ So, I can take the derivative as follows: $$ CF' = (r(x) -f(S(x))) * f'(S(x)) * S'(x)  $$ where $f'(S(x))$ is the gradient of the transformed $f$ image according to the scaling matrix and $S'(x)$ is the derivative of the scaling matrix with respect to the scaling parameters, which is simple to compute. Now, I have the following question: The image is sampled at various pixel locations and I can use this formula to compute the gradient of the cost function at every pixel location. However, if I want to get the global gradient. Should I sum up these individual gradients to get the global gradient to use in the optimisation?","I am trying to find a geometric transformation between two images, where the transformation is a simple scaling matrix. So, if I denote the two image functions as $r$ and $f$ and the scaling matrix as $S$, I am trying to minimize the following cost function (CF) with respect to the scaling parameters: $$ CF = 0.5 * (r(x) - f(S(x)))^{2} $$ So, I can take the derivative as follows: $$ CF' = (r(x) -f(S(x))) * f'(S(x)) * S'(x)  $$ where $f'(S(x))$ is the gradient of the transformed $f$ image according to the scaling matrix and $S'(x)$ is the derivative of the scaling matrix with respect to the scaling parameters, which is simple to compute. Now, I have the following question: The image is sampled at various pixel locations and I can use this formula to compute the gradient of the cost function at every pixel location. However, if I want to get the global gradient. Should I sum up these individual gradients to get the global gradient to use in the optimisation?",,"['derivatives', 'optimization', 'transformation']"
20,Prove that $f(a) \leq f(x) \leq f(b) $,Prove that,f(a) \leq f(x) \leq f(b) ,"If the following data are given, prove that $f(a) \leq f(x) \leq f(b) $ f is differentiable on [a,b] and f'(x) $ \geq  0 \forall x \in (a,b) $ Is the following argument correct? $f'(x) \geq 0 \implies f $ is increasing on (a,b) $ \implies f(a) \leq f(b) $ Let $x_0 \in (a,b) $ Since f is increasing $f(a) \leq f(x_0) \leq f(b) $  $$ \therefore \forall x \in (a,b)  f(a) \leq f(x) \leq f(b) $$","If the following data are given, prove that $f(a) \leq f(x) \leq f(b) $ f is differentiable on [a,b] and f'(x) $ \geq  0 \forall x \in (a,b) $ Is the following argument correct? $f'(x) \geq 0 \implies f $ is increasing on (a,b) $ \implies f(a) \leq f(b) $ Let $x_0 \in (a,b) $ Since f is increasing $f(a) \leq f(x_0) \leq f(b) $  $$ \therefore \forall x \in (a,b)  f(a) \leq f(x) \leq f(b) $$",,['derivatives']
21,Finding derivative form the definition,Finding derivative form the definition,,"I want to find the derivative of the function $f:\mathbb R^n\to \mathbb R^m$ at a point $x_0\in \mathbb R^n$, where $f(x)=c\in \mathbb R^m$, is a constant function. What I did is as follows: If $f$ is differentiable at $x_0$, then there exists a linear function $L_{x_{0}}:\mathbb R^n\to \mathbb R^m$ such that $\lim\limits_{\parallel h\parallel \to 0}\frac{\parallel f(x_0+h)-f(x_0)-L_{x_0}(h)\parallel }{\parallel h\parallel }=0$. This gives $\lim\limits_{\parallel h\parallel \to 0}\frac{\parallel L_{x_0}(h)\parallel }{\parallel h\parallel }=0$.  Now how to show that $L_{x_0}(h)=0?$ Please help!","I want to find the derivative of the function $f:\mathbb R^n\to \mathbb R^m$ at a point $x_0\in \mathbb R^n$, where $f(x)=c\in \mathbb R^m$, is a constant function. What I did is as follows: If $f$ is differentiable at $x_0$, then there exists a linear function $L_{x_{0}}:\mathbb R^n\to \mathbb R^m$ such that $\lim\limits_{\parallel h\parallel \to 0}\frac{\parallel f(x_0+h)-f(x_0)-L_{x_0}(h)\parallel }{\parallel h\parallel }=0$. This gives $\lim\limits_{\parallel h\parallel \to 0}\frac{\parallel L_{x_0}(h)\parallel }{\parallel h\parallel }=0$.  Now how to show that $L_{x_0}(h)=0?$ Please help!",,"['real-analysis', 'derivatives']"
22,Is this an immediate consequence of the Straddle Lemma?,Is this an immediate consequence of the Straddle Lemma?,,"As main book, I'm using Bartle, Introduction to Real Analysis (2011 4 ed) . Exercise 17 Section 6.1, p 171 asks you to prove the Straddle Lemma: Let $f:I\rightarrow\mathbb R$ be differentiable at $c\in I$ . Establish the Straddle Lemma : Given $\varepsilon >0 $ there exists $\delta >0$ such that if $u, v\in I$ satisfy $c-\delta \lt u\leq c\leq v\lt c+\delta$ then we have $|f(v)-f(u)-(v-u)f'(c)|\leq \varepsilon (v-u).$ [Hint: The $\delta$ is given by Definition 6.1.1. Subtract and add the term $f(c) - cf'(c)$ on the left side and use the Triangle Inequality.] It is sufficient to know the formal definition of derivative to prove this result. Later, in exercise 18 of section 6.2, p 180 (that is based on the Mean Value Theorem and some of its consequences) asks Let $I:=[a,b]$ and let $f:I\rightarrow\mathbb R$ be differentiable at $c\in I$ . Show that for every $\varepsilon >0 $ there exists $\delta >0$ such that if $\;0<|x-y|\lt \delta$ and $a \leq x \leq c \leq y \leq b$ , then $$\left|\frac {f(x)-f(y)}{x-y}-f'(c)\right|\lt \varepsilon$$ Is this a direct consequence of the Straddle Lemma? I didn't get to anything keeping distance from the Straddle Lemma and trying to use the Mean Value Theorem. I'm unsure about using the Straddle Lemma firstly because it gives me a not strict inequality and the second exercise has a strict inequality, and secondly because I'm not using any ""tools"" provided from the respective section. Is the Straddle Lemma useful? If so, how can I end up with a strict inequality? If not, any hints would be appreciated. Thank you in advance.","As main book, I'm using Bartle, Introduction to Real Analysis (2011 4 ed) . Exercise 17 Section 6.1, p 171 asks you to prove the Straddle Lemma: Let be differentiable at . Establish the Straddle Lemma : Given there exists such that if satisfy then we have [Hint: The is given by Definition 6.1.1. Subtract and add the term on the left side and use the Triangle Inequality.] It is sufficient to know the formal definition of derivative to prove this result. Later, in exercise 18 of section 6.2, p 180 (that is based on the Mean Value Theorem and some of its consequences) asks Let and let be differentiable at . Show that for every there exists such that if and , then Is this a direct consequence of the Straddle Lemma? I didn't get to anything keeping distance from the Straddle Lemma and trying to use the Mean Value Theorem. I'm unsure about using the Straddle Lemma firstly because it gives me a not strict inequality and the second exercise has a strict inequality, and secondly because I'm not using any ""tools"" provided from the respective section. Is the Straddle Lemma useful? If so, how can I end up with a strict inequality? If not, any hints would be appreciated. Thank you in advance.","f:I\rightarrow\mathbb R c\in I \varepsilon >0  \delta >0 u, v\in I c-\delta \lt u\leq c\leq v\lt c+\delta |f(v)-f(u)-(v-u)f'(c)|\leq \varepsilon (v-u). \delta f(c) - cf'(c) I:=[a,b] f:I\rightarrow\mathbb R c\in I \varepsilon >0  \delta >0 \;0<|x-y|\lt \delta a \leq x \leq c \leq y \leq b \left|\frac {f(x)-f(y)}{x-y}-f'(c)\right|\lt \varepsilon","['real-analysis', 'derivatives']"
23,Optimal way to find derivative - numerically,Optimal way to find derivative - numerically,,"Suppose we are given points $x_0,x_1,x_2$ evenly spaced points $(x_0-x_1=x_1-x_2)$, and $u(x_1),u(x_2),u(x_3)$ Where $u$ is some function. Find the best way to approximate $u''(x)$ using only the above data, and approximate the error. Normally, I would use the method of undetermined coefficients, but that would require that $x$ is one of the given points. This case is different. In this case, we are given 3 points and we are asked to find an approximation for the derivative at any point. Interpolation comes to mind, but who says it is the best way? Would appreciate any help, I'm quite new to this material.","Suppose we are given points $x_0,x_1,x_2$ evenly spaced points $(x_0-x_1=x_1-x_2)$, and $u(x_1),u(x_2),u(x_3)$ Where $u$ is some function. Find the best way to approximate $u''(x)$ using only the above data, and approximate the error. Normally, I would use the method of undetermined coefficients, but that would require that $x$ is one of the given points. This case is different. In this case, we are given 3 points and we are asked to find an approximation for the derivative at any point. Interpolation comes to mind, but who says it is the best way? Would appreciate any help, I'm quite new to this material.",,"['calculus', 'derivatives', 'numerical-methods', 'interpolation']"
24,differentiability check,differentiability check,,"$$f(x)=\frac{1}{x-2}$$ number of points where $f$ is not differentiable? I know that the domain of the function is $\mathbb{R}\setminus\{2\}$ and differentiability is checked only in the domain of the function so according to me the answer should be 0, But my teacher is saying that as the function is not continuous at $x=2$, it must be non-differentiable also. please help by solving this confusion.","$$f(x)=\frac{1}{x-2}$$ number of points where $f$ is not differentiable? I know that the domain of the function is $\mathbb{R}\setminus\{2\}$ and differentiability is checked only in the domain of the function so according to me the answer should be 0, But my teacher is saying that as the function is not continuous at $x=2$, it must be non-differentiable also. please help by solving this confusion.",,"['calculus', 'derivatives']"
25,supremum of derivatives [closed],supremum of derivatives [closed],,"Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 5 years ago . Improve this question Let $f$ twice continuously differentiable on $(a, \infty)$. Let $M_{0} = \sup f$, $M_{1} = \sup f'$, $M_{2} = \sup f''$. Show that $ (M_{1})^{2} \leq M_{0}M_{2}$. Also, How can this be modified for a finite interval?","Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 5 years ago . Improve this question Let $f$ twice continuously differentiable on $(a, \infty)$. Let $M_{0} = \sup f$, $M_{1} = \sup f'$, $M_{2} = \sup f''$. Show that $ (M_{1})^{2} \leq M_{0}M_{2}$. Also, How can this be modified for a finite interval?",,"['real-analysis', 'inequality', 'derivatives']"
26,What is the largest class of measurable functions $f$ s.t. $f'$ a.e.?,What is the largest class of measurable functions  s.t.  a.e.?,f f',"We know by Lebesgue Theorem that monotone functions on interval [a,b] has finite derivate almost everywhere and different of two monotone functions have finite derivative a.e. $\textbf{My Question}$ : what other functions have a.e. finite  derivative ? What is the largest class ? We know continuous functions are Riemann integrable and we can take derivative. and continuous functions with countably many discontinuous is Riemann integrable and again we can take derivative . Can I push more for example does any Riemann Integrable function has derivative a.e. ? Thanks in advance for your interest and help.","We know by Lebesgue Theorem that monotone functions on interval [a,b] has finite derivate almost everywhere and different of two monotone functions have finite derivative a.e. $\textbf{My Question}$ : what other functions have a.e. finite  derivative ? What is the largest class ? We know continuous functions are Riemann integrable and we can take derivative. and continuous functions with countably many discontinuous is Riemann integrable and again we can take derivative . Can I push more for example does any Riemann Integrable function has derivative a.e. ? Thanks in advance for your interest and help.",,"['real-analysis', 'integration', 'derivatives', 'lebesgue-integral']"
27,Weierstrass function,Weierstrass function,,"I got stuck on this exercise from Prof. Tao's real analysis notes. Let $f:\mathbb{R}\rightarrow \mathbb{R}$ be the function $$f:= \sum_{n=1}^\infty 4^{-n} \sin(8^n\pi x)$$ Show that for every 8-dyadic interval $[\frac{j}{8^k},\frac{j+1}{8^k}]$ with $k\geq 1$, one has $|f(\frac{j+1}{8^k})-f(\frac{j}{8^k})| \geq C4^{-k}$ for some absolute constant $C > 0$. I can see that all terms $n\geq k$ in the summation are $0$; the problem can be deduced to  $$\left|\sum_{n=1}^{k-1} 4^{-n} \sin(8^{n-k}(j+1)\pi ) - \sum_{n=1}^{k-1} 4^{-n} \sin(8^{n-k}(j)\pi )\right|\geq C4^{-k}.$$ I also tried to use sum of angles formula since we know that $0 < 8^{n-k}\pi\leq \frac{\pi}{8}$.","I got stuck on this exercise from Prof. Tao's real analysis notes. Let $f:\mathbb{R}\rightarrow \mathbb{R}$ be the function $$f:= \sum_{n=1}^\infty 4^{-n} \sin(8^n\pi x)$$ Show that for every 8-dyadic interval $[\frac{j}{8^k},\frac{j+1}{8^k}]$ with $k\geq 1$, one has $|f(\frac{j+1}{8^k})-f(\frac{j}{8^k})| \geq C4^{-k}$ for some absolute constant $C > 0$. I can see that all terms $n\geq k$ in the summation are $0$; the problem can be deduced to  $$\left|\sum_{n=1}^{k-1} 4^{-n} \sin(8^{n-k}(j+1)\pi ) - \sum_{n=1}^{k-1} 4^{-n} \sin(8^{n-k}(j)\pi )\right|\geq C4^{-k}.$$ I also tried to use sum of angles formula since we know that $0 < 8^{n-k}\pi\leq \frac{\pi}{8}$.",,"['calculus', 'real-analysis', 'trigonometry', 'derivatives']"
28,Books explaining differentiation under the integral sign,Books explaining differentiation under the integral sign,,"I've heard that this is a great tool to have in you math toolkit, but I cannot comprehend this method just from the wiki entry and 2 page pdf files. I'm looking for a book which has problems (preferably solutions). I'm not well versed in mathematical notation, but I'm currently doing a course on multi variable calculus. Is this method an alternative to the Jacobian, or am I mistaken? Is it really that useful?","I've heard that this is a great tool to have in you math toolkit, but I cannot comprehend this method just from the wiki entry and 2 page pdf files. I'm looking for a book which has problems (preferably solutions). I'm not well versed in mathematical notation, but I'm currently doing a course on multi variable calculus. Is this method an alternative to the Jacobian, or am I mistaken? Is it really that useful?",,"['integration', 'reference-request', 'derivatives']"
29,Why is that a risk averse consumer buys the optimum insurance when there is actuarially fair insurance?,Why is that a risk averse consumer buys the optimum insurance when there is actuarially fair insurance?,,"I've asked the same question at the Quantitative Finance StackExchange. Consider the following example: ""As a risk-averse consumer, you would want to choose a value of x so as to maximize expected utility, i.e. Given actuarially fair insurance, where p = r, you would solve: max pu(w - px - L + x) + (1-p)u(w - px), since in case of an accident, you total wealth would be w, less the loss suffered due to the accident, less the premium paid, and adding the amount received from the insurance company. Differentiating with respect to x, and setting the result equal to zero, we get the first-order necessary condition as: (1-p)pu'(w - px - L + x) - p(1-p)u'(w - px) = 0, which gives us: u'(w - px - L + x) = u'(w - px) Risk-aversion implies u"" < 0, so that equality of the marginal utilities of wealth implies equality of the wealth levels, i.e. w - px - L + x = w - px, so we must have x = L. So, given actuarially fair insurance, you would choose to fully insure your car. Since you're risk-averse, you'd aim to equalize your wealth across all circumstances - whether or not you have an accident. However, if p and r are not equal, we will have x < L; you would under-insure. How much you'd underinsure would depend on the how much greater r was than p."" Now, how the condition u""<0 changes anything to reach the result expressed above?","I've asked the same question at the Quantitative Finance StackExchange. Consider the following example: ""As a risk-averse consumer, you would want to choose a value of x so as to maximize expected utility, i.e. Given actuarially fair insurance, where p = r, you would solve: max pu(w - px - L + x) + (1-p)u(w - px), since in case of an accident, you total wealth would be w, less the loss suffered due to the accident, less the premium paid, and adding the amount received from the insurance company. Differentiating with respect to x, and setting the result equal to zero, we get the first-order necessary condition as: (1-p)pu'(w - px - L + x) - p(1-p)u'(w - px) = 0, which gives us: u'(w - px - L + x) = u'(w - px) Risk-aversion implies u"" < 0, so that equality of the marginal utilities of wealth implies equality of the wealth levels, i.e. w - px - L + x = w - px, so we must have x = L. So, given actuarially fair insurance, you would choose to fully insure your car. Since you're risk-averse, you'd aim to equalize your wealth across all circumstances - whether or not you have an accident. However, if p and r are not equal, we will have x < L; you would under-insure. How much you'd underinsure would depend on the how much greater r was than p."" Now, how the condition u""<0 changes anything to reach the result expressed above?",,"['derivatives', 'optimization', 'finance', 'economics', 'risk-assessment']"
30,Use implicit differentiation to find derivative,Use implicit differentiation to find derivative,,$$x\sin(4x+5y)=y\cos(x)$$ I am trying to use implicit differentiation to find dx/dy for this problem but the answer i keep getting is $$4x\cos(4x+5y)=-y\sin(x)$$ and I am stuck.,$$x\sin(4x+5y)=y\cos(x)$$ I am trying to use implicit differentiation to find dx/dy for this problem but the answer i keep getting is $$4x\cos(4x+5y)=-y\sin(x)$$ and I am stuck.,,"['calculus', 'derivatives', 'implicit-differentiation']"
31,"Numerically calculate the second ""left hand"" derivative","Numerically calculate the second ""left hand"" derivative",,"The Problem I have a series of measurements for which I have to calculate the first and second derivative numerically in a ""live"" fashion, i.e. using only previous data. This is easy for the first derivative: $f'(x) \approx \frac{f(x) - f(x-h)}{h}$ For the second derivative I use this formula: $f''(x) \approx \frac{f(x) - 2f(x-h) + f(x-2h) }{h^2}$ The problem I have is that the results are a good estimate only for ""previous"" values of $x$. I.e. $f'(x)$ yields a good estimate for $x-\frac12 h$ and   $f''(x)$ yields a good estimate for $x-h$. This is because the formulae I use are indeed the central formulae at these points,  e.g. $f'(x + \frac12 h) = \frac{f(x+\frac h2)-f(x-\frac h2)}{2 \frac h2}$ The question Is there a way to create a better estimate for the second derivation but still stick to using only ""previous"" points? Comparison of approaches so far I visualized the three approaches so far, using $f(x) = \operatorname{sin}(2x)$ as an example and $h = 0.16$: orange diamond: based on $f''(x) \approx \frac{f(x) - 2f(x-h) + f(x-2h) }{h^2}$ pink cross: The points from orange diamond translated by $(-h,0)$ green circle: Using 4 points for approximation yielding $f''(x)\approx \frac{2 f(x) - 5f(x-h) + 4 f(x-2h)- f(x-3h)}{h^2}$  as suggested by gammatester Note: If you do a linear extrapolation of $f''(x-2h)$ and $f''(x-h)$ (using a central differential approach) like $f''(x) = 2 f''(x-h) - f''(x-2h)$ as suggested by Hagen von Eitzen , you'll end up with the same formula as proposed by gammatester. It seems to me that adding more points to the formulae not necessarily decreases the error, especially if the ""scanning distance"" $h$ is quite large. The same function with $h = 0.06$: The solution I did take me some time but after I tested the higher accuracy formulae the wikipedia page gammatester mentioned, I found a formula that is quite accurate and only uses previously gathered data. $f''(x) \approx \frac{ \frac{469}{90} f(x-0h) −\frac{223}{10} f(x-1h) + \frac{879}{20} f(x-2h) −\frac{949}{18} f(x-3h) + 41 f(x-4h) −\frac{201}{10} f(x-5h) +\frac{1019}{180} f(x-6h)− \frac{7}{10} f(x-7h)}{h^2} $","The Problem I have a series of measurements for which I have to calculate the first and second derivative numerically in a ""live"" fashion, i.e. using only previous data. This is easy for the first derivative: $f'(x) \approx \frac{f(x) - f(x-h)}{h}$ For the second derivative I use this formula: $f''(x) \approx \frac{f(x) - 2f(x-h) + f(x-2h) }{h^2}$ The problem I have is that the results are a good estimate only for ""previous"" values of $x$. I.e. $f'(x)$ yields a good estimate for $x-\frac12 h$ and   $f''(x)$ yields a good estimate for $x-h$. This is because the formulae I use are indeed the central formulae at these points,  e.g. $f'(x + \frac12 h) = \frac{f(x+\frac h2)-f(x-\frac h2)}{2 \frac h2}$ The question Is there a way to create a better estimate for the second derivation but still stick to using only ""previous"" points? Comparison of approaches so far I visualized the three approaches so far, using $f(x) = \operatorname{sin}(2x)$ as an example and $h = 0.16$: orange diamond: based on $f''(x) \approx \frac{f(x) - 2f(x-h) + f(x-2h) }{h^2}$ pink cross: The points from orange diamond translated by $(-h,0)$ green circle: Using 4 points for approximation yielding $f''(x)\approx \frac{2 f(x) - 5f(x-h) + 4 f(x-2h)- f(x-3h)}{h^2}$  as suggested by gammatester Note: If you do a linear extrapolation of $f''(x-2h)$ and $f''(x-h)$ (using a central differential approach) like $f''(x) = 2 f''(x-h) - f''(x-2h)$ as suggested by Hagen von Eitzen , you'll end up with the same formula as proposed by gammatester. It seems to me that adding more points to the formulae not necessarily decreases the error, especially if the ""scanning distance"" $h$ is quite large. The same function with $h = 0.06$: The solution I did take me some time but after I tested the higher accuracy formulae the wikipedia page gammatester mentioned, I found a formula that is quite accurate and only uses previously gathered data. $f''(x) \approx \frac{ \frac{469}{90} f(x-0h) −\frac{223}{10} f(x-1h) + \frac{879}{20} f(x-2h) −\frac{949}{18} f(x-3h) + 41 f(x-4h) −\frac{201}{10} f(x-5h) +\frac{1019}{180} f(x-6h)− \frac{7}{10} f(x-7h)}{h^2} $",,"['derivatives', 'numerical-methods']"
32,How to show the monotonicity of $\frac{1+ny}{1-y^{n+1}}-\frac{1}{1-y}$?,How to show the monotonicity of ?,\frac{1+ny}{1-y^{n+1}}-\frac{1}{1-y},"The question is to prove that $\dfrac{1+ny}{1-y^{n+1}}-\dfrac{1}{1-y}$ is a decreasing function in $y$ for $y>1$, where $n$ is a positive integer. My first thought is to take the derivative and show the sign is negative. But it seems the derivative is a bit complicate to easily draw the conclusion.  Any other thoughts/hints/comments?   Thanks a lot.","The question is to prove that $\dfrac{1+ny}{1-y^{n+1}}-\dfrac{1}{1-y}$ is a decreasing function in $y$ for $y>1$, where $n$ is a positive integer. My first thought is to take the derivative and show the sign is negative. But it seems the derivative is a bit complicate to easily draw the conclusion.  Any other thoughts/hints/comments?   Thanks a lot.",,['derivatives']
33,Characterization of differentiability via Lie derivatives,Characterization of differentiability via Lie derivatives,,"Yesterday I asked this question in MathOverflow but did not receive an answer yet. I want to try my chance here too, since I am in kind of a hurry. Answers will be much appreciated. I intend to propose as a project the proof of the statement below, but I want to make sure that it is not already proved somewhere else before. Let $G$ be a Lie group, and $f$ a real-valued function on $G$. The expression $f \in \mathcal{C}^k(G)$ makes sense, and this would be the case even if $G$ were merely a smooth manifold. On the other hand, the Lie group structure on $G$ enables one to speak of directional derivatives of $f$. Indeed, the Lie algebra $\mathfrak{g}$ of $G$ is canonically isomorphic to the space of left-invariant derivations of $\mathcal{C}^\infty(G)$; under this isomorphism, each $X \in \mathfrak{g}$ is associated with the (left) Lie derivative operator $\mathcal{L}_X$ given by $$(\mathcal{L}_X f)(y) := \left.\frac{\mathrm{d}}{\mathrm{d} t}\right|_{t=0} f(y e^{tX})$$ where $f \in \mathcal{C}^\infty(G)$ and $y \in G$. It is natural to call $\mathcal{L}_X f$ as the (left) directional derivative of $f$ along $X$ . Taking the above equality as a definition, one may expect as in elementary analysis that being in $\mathcal{C}^k$ is equivalent to having continuous directional derivatives of order $k$. This is what our statement says: Statement. Let $G$ be a Lie group, and $f$ a real-valued function on $G$. For each $k \in \mathbb{N}$, $f \in \mathcal{C}^k(G)$ if and only if $(\mathcal{L}_{X_1} \cdots \mathcal{L}_{X_k})f$ exists and is continuous for all $X_1,\ldots,X_k \in \mathfrak{g}$. I searched quite a while for this statement in the literature but could not find anything. (The proof is not so trivial as you might think at a first glance. Please have a look at this question and p. 15 of this essay also.) Have you ever seen it somewhere? If so, could you please give a reference?","Yesterday I asked this question in MathOverflow but did not receive an answer yet. I want to try my chance here too, since I am in kind of a hurry. Answers will be much appreciated. I intend to propose as a project the proof of the statement below, but I want to make sure that it is not already proved somewhere else before. Let $G$ be a Lie group, and $f$ a real-valued function on $G$. The expression $f \in \mathcal{C}^k(G)$ makes sense, and this would be the case even if $G$ were merely a smooth manifold. On the other hand, the Lie group structure on $G$ enables one to speak of directional derivatives of $f$. Indeed, the Lie algebra $\mathfrak{g}$ of $G$ is canonically isomorphic to the space of left-invariant derivations of $\mathcal{C}^\infty(G)$; under this isomorphism, each $X \in \mathfrak{g}$ is associated with the (left) Lie derivative operator $\mathcal{L}_X$ given by $$(\mathcal{L}_X f)(y) := \left.\frac{\mathrm{d}}{\mathrm{d} t}\right|_{t=0} f(y e^{tX})$$ where $f \in \mathcal{C}^\infty(G)$ and $y \in G$. It is natural to call $\mathcal{L}_X f$ as the (left) directional derivative of $f$ along $X$ . Taking the above equality as a definition, one may expect as in elementary analysis that being in $\mathcal{C}^k$ is equivalent to having continuous directional derivatives of order $k$. This is what our statement says: Statement. Let $G$ be a Lie group, and $f$ a real-valued function on $G$. For each $k \in \mathbb{N}$, $f \in \mathcal{C}^k(G)$ if and only if $(\mathcal{L}_{X_1} \cdots \mathcal{L}_{X_k})f$ exists and is continuous for all $X_1,\ldots,X_k \in \mathfrak{g}$. I searched quite a while for this statement in the literature but could not find anything. (The proof is not so trivial as you might think at a first glance. Please have a look at this question and p. 15 of this essay also.) Have you ever seen it somewhere? If so, could you please give a reference?",,"['reference-request', 'differential-geometry', 'derivatives', 'lie-groups']"
34,Derivative of sum of Schwartz functions,Derivative of sum of Schwartz functions,,"Let $f:\mathbb{R}\rightarrow\mathbb{R}$ be a Schwartz-class function. Let $F(x)=\sum_{n\in\mathbb{Z}}f(x-2\pi n)$. I understand that because Schwartz-class functions are nice, this series converges absolutely and uniformly for all $x\in\mathbb{R}$. However, why is $F$ differentiable, and why is its derivative equal to $\sum_{n\in\mathbb{Z}}f'(x-2\pi n)$?","Let $f:\mathbb{R}\rightarrow\mathbb{R}$ be a Schwartz-class function. Let $F(x)=\sum_{n\in\mathbb{Z}}f(x-2\pi n)$. I understand that because Schwartz-class functions are nice, this series converges absolutely and uniformly for all $x\in\mathbb{R}$. However, why is $F$ differentiable, and why is its derivative equal to $\sum_{n\in\mathbb{Z}}f'(x-2\pi n)$?",,"['real-analysis', 'derivatives', 'schwartz-space']"
35,Definition of derivative,Definition of derivative,,"Well, I know that the derivative of a function $f(x)$is defined this way: $$\frac{df(x)}{dx} = \lim_{\Delta x\to 0}\frac{f(x+\Delta x) - f(x)}{\Delta x}$$ And it's pretty clear that the expression inside the limit will approach the tangent line at a given point. I know that this is the definition of derivative. However, we can't define this to be equals the tangent line at a given point. So how do we know that this limit will in fact be equal the slope of the function?","Well, I know that the derivative of a function $f(x)$is defined this way: $$\frac{df(x)}{dx} = \lim_{\Delta x\to 0}\frac{f(x+\Delta x) - f(x)}{\Delta x}$$ And it's pretty clear that the expression inside the limit will approach the tangent line at a given point. I know that this is the definition of derivative. However, we can't define this to be equals the tangent line at a given point. So how do we know that this limit will in fact be equal the slope of the function?",,"['calculus', 'derivatives']"
36,Differentiation under integral sign for exponential,Differentiation under integral sign for exponential,,"This question arises from this question : Suppose $P(x)$ is a polynomial. Why is it the case that $$\dfrac{d}{dy}\int_\mathbb{R}iP(x)e^{-x^2/2}e^{-ixy}dx=\int_\mathbb{R}xP(x)e^{-x^2/2}e^{-ixy}dx?$$ I'm thinking about using dominated convergence thm, but not sure how to apply it here.","This question arises from this question : Suppose $P(x)$ is a polynomial. Why is it the case that $$\dfrac{d}{dy}\int_\mathbb{R}iP(x)e^{-x^2/2}e^{-ixy}dx=\int_\mathbb{R}xP(x)e^{-x^2/2}e^{-ixy}dx?$$ I'm thinking about using dominated convergence thm, but not sure how to apply it here.",,"['integration', 'derivatives']"
37,"Related rates: Find dA/dt of triangle, given d(theta)/dt -- Can't come to textbook answer","Related rates: Find dA/dt of triangle, given d(theta)/dt -- Can't come to textbook answer",,"The question: ABC is a triangle in which the lines $\overline {AB} = 20cm$, $\overline {AC} = 32cm$ and $\angle BAC = \theta$. If $\theta$ is increasing at the rate of 2° per minute, determine the rate at which the triangle's area is changing when $\theta = 120°$. Here's my attempt: Let $t$ be time in minutes. We're given $\frac{d\theta}{dt} = 2°$, Area of triangle with two sides and included angle: $A = \frac12\overline {AB}$ $\overline {AC}$ $sin\theta$,  i.e. $A = \frac12(20)(32)sin\theta$ $= 320sin\theta$ $\therefore \frac{dA}{dt} = \frac{dA}{d\theta}\frac{d\theta}{dt} = 320cos\theta\frac{d\theta}{dt}$ i.e. at $\theta = 120°$: $\frac{dA}{dt} = 320cos(120) * (2)$ $= -320 cm^2/min$ The textbook answer: Decreasing at $5.59cm^2/min$ The question is exactly as above, and I've double checked the units. Can anyone please point out where I've gone wrong, or if I haven't? Thank you!","The question: ABC is a triangle in which the lines $\overline {AB} = 20cm$, $\overline {AC} = 32cm$ and $\angle BAC = \theta$. If $\theta$ is increasing at the rate of 2° per minute, determine the rate at which the triangle's area is changing when $\theta = 120°$. Here's my attempt: Let $t$ be time in minutes. We're given $\frac{d\theta}{dt} = 2°$, Area of triangle with two sides and included angle: $A = \frac12\overline {AB}$ $\overline {AC}$ $sin\theta$,  i.e. $A = \frac12(20)(32)sin\theta$ $= 320sin\theta$ $\therefore \frac{dA}{dt} = \frac{dA}{d\theta}\frac{d\theta}{dt} = 320cos\theta\frac{d\theta}{dt}$ i.e. at $\theta = 120°$: $\frac{dA}{dt} = 320cos(120) * (2)$ $= -320 cm^2/min$ The textbook answer: Decreasing at $5.59cm^2/min$ The question is exactly as above, and I've double checked the units. Can anyone please point out where I've gone wrong, or if I haven't? Thank you!",,"['calculus', 'derivatives', 'triangles']"
38,Substituting total derivative d for partial derivative \partial,Substituting total derivative d for partial derivative \partial,,"In economic models it seems to be commonplace to substitute a total derivative derived from one equation, say $\frac{d k}{d \tau}$ , for the partial derivative derived from another equation, say $\frac{\partial k}{\partial \tau}$ . Why is it allowed to perform this substitution? After all, $d$ and $\partial$ are different concepts, are they not? Here is a (simplified) example from a model on tax competition by Köthenbürger (2002) : A firm maximizes profits from the production and sale of a single good. One of the resulting first-order conditions is $f_k(k) = r(\tau) + \tau$ , where $f_k$ is the derivative of the production function with respect to the input capital $k$ , $r$ is the interest rate, and $\tau$ is the tax rate of a tax on capital. Differentiating with respect to $k$ and $\tau$ on both sides, we can rearrange to arrive at $$ \frac{d k}{d \tau} = \frac{\frac{\partial r}{\partial \tau} + 1}{f_{kk}} $$ The model also contains the maximization problem of a government which yields the first-order condition $$ \frac{d u}{d \tau} = u_c \left( - f_{kk} \frac{\partial k}{\partial \tau} k + \dots \right) + \dots \overset{!}{=} 0 $$ The analysis of the model continues with a substitution of $\frac{d k}{d \tau}$ from the first equation for $\frac{\partial k}{\partial \tau}$ in the second equation.","In economic models it seems to be commonplace to substitute a total derivative derived from one equation, say , for the partial derivative derived from another equation, say . Why is it allowed to perform this substitution? After all, and are different concepts, are they not? Here is a (simplified) example from a model on tax competition by Köthenbürger (2002) : A firm maximizes profits from the production and sale of a single good. One of the resulting first-order conditions is , where is the derivative of the production function with respect to the input capital , is the interest rate, and is the tax rate of a tax on capital. Differentiating with respect to and on both sides, we can rearrange to arrive at The model also contains the maximization problem of a government which yields the first-order condition The analysis of the model continues with a substitution of from the first equation for in the second equation.","\frac{d k}{d \tau} \frac{\partial k}{\partial \tau} d \partial f_k(k) = r(\tau) + \tau f_k k r \tau k \tau 
\frac{d k}{d \tau} = \frac{\frac{\partial r}{\partial \tau} + 1}{f_{kk}}
 
\frac{d u}{d \tau} = u_c \left( - f_{kk} \frac{\partial k}{\partial \tau} k + \dots \right) + \dots \overset{!}{=} 0
 \frac{d k}{d \tau} \frac{\partial k}{\partial \tau}","['derivatives', 'economics', 'partial-derivative']"
39,Smoothing continuous functions,Smoothing continuous functions,,"Asked the wrong question.  Try again. Let $f$ be continuous on [a,b] and suppose f is differentiable at all but a finite number of points.  Given an $\epsilon$ can we find a g differentiable on all of [a, b] such that f(x) = g(x) except on a set of measure < $\epsilon$ would it be true with countable number of points not differentiable","Asked the wrong question.  Try again. Let $f$ be continuous on [a,b] and suppose f is differentiable at all but a finite number of points.  Given an $\epsilon$ can we find a g differentiable on all of [a, b] such that f(x) = g(x) except on a set of measure < $\epsilon$ would it be true with countable number of points not differentiable",,"['real-analysis', 'derivatives']"
40,Matrix math syntax in wolfram alpha,Matrix math syntax in wolfram alpha,,"I'm having trouble getting Wolfram Alpha to do my bidding with regard to matrix manipulations.  I am trying to take the derivative of the following matrix expression with respect to m , and was hoping WolframAlpha could be used: -(x-m)^T * E^-1 * (x-m) ...However, I cannot discover how to input matrices as variables, and do not know what to use for the transpose operator.  ( ^T was used in this case. x and m are vectors, E is a symmetric matrix.) Give a man a fish : How can this derivative be solved? Teach a man to fish: How can I input such expressions into Wolfram Alpha? If it helps, I know the solution should look something like (but may be off by some scalar constant): E^-1(x-m)","I'm having trouble getting Wolfram Alpha to do my bidding with regard to matrix manipulations.  I am trying to take the derivative of the following matrix expression with respect to m , and was hoping WolframAlpha could be used: -(x-m)^T * E^-1 * (x-m) ...However, I cannot discover how to input matrices as variables, and do not know what to use for the transpose operator.  ( ^T was used in this case. x and m are vectors, E is a symmetric matrix.) Give a man a fish : How can this derivative be solved? Teach a man to fish: How can I input such expressions into Wolfram Alpha? If it helps, I know the solution should look something like (but may be off by some scalar constant): E^-1(x-m)",,"['matrices', 'derivatives', 'wolfram-alpha']"
41,Number of real roots of the equation $2^x = 1+x^2$,Number of real roots of the equation,2^x = 1+x^2,"Find the number   of real roots of the equation $2^x = 1+x^2$ My try :  Let we take $f(x) = 2^x-1-x^2$. Now for Drawing Graph of given function, we use Derivative Test. $f'(x) = 2^x \cdot \ln (2)-2x$ and $f''(x) = 2^x \ln (2)-2$  and $f'''(x) = 2^x   \ln(2)>0\;\forall x\in \mathbb{R}$ Means $f''(x)$ is an Strictly Increasing function. Or $f''(x)$ is an Concave upward function. Now I did not understand how can I calculate nature of $f'(x)$ using higher derivatives. Please explain, thanks.","Find the number   of real roots of the equation $2^x = 1+x^2$ My try :  Let we take $f(x) = 2^x-1-x^2$. Now for Drawing Graph of given function, we use Derivative Test. $f'(x) = 2^x \cdot \ln (2)-2x$ and $f''(x) = 2^x \ln (2)-2$  and $f'''(x) = 2^x   \ln(2)>0\;\forall x\in \mathbb{R}$ Means $f''(x)$ is an Strictly Increasing function. Or $f''(x)$ is an Concave upward function. Now I did not understand how can I calculate nature of $f'(x)$ using higher derivatives. Please explain, thanks.",,"['calculus', 'derivatives', 'roots']"
42,"Derivatives, when to use the chain rule, and when to use the formula.","Derivatives, when to use the chain rule, and when to use the formula.",,"When should I use the formula below, and when should I use the chain rule? Or does it not matter? I find using chain rule to be much faster and easier to solve. $$\lim_{x \to 0} \frac{f(x+h)-f(x)}{h}$$","When should I use the formula below, and when should I use the chain rule? Or does it not matter? I find using chain rule to be much faster and easier to solve. $$\lim_{x \to 0} \frac{f(x+h)-f(x)}{h}$$",,['derivatives']
43,A question on functional equations.,A question on functional equations.,,"Question: If it is given that $$ e^xf(x) = 2 + \int_0^x\sqrt{1+x^4}\,dx $$ then what is the value of $ \dfrac {d} {dx} \Big(f^{-1}(x)\Big)\Bigg|_{x=2} $ Where I am stuck: Now, since we are to evaluate $ \dfrac {d} {dx} \Big(f^{-1}(x)\Big)\Bigg|_{x=2} $, all we need to evaluate is $ f'(2) $. Our answer will be the reciprocal of this. So I differentiated the given equation: $$ e^x(f(x) + f'(x)) = \sqrt{1+x^4}\ $$ But to find $f'(2)$ we also need to evaluate $f(2)$ which seems to be an insane thing to do. So what to do?","Question: If it is given that $$ e^xf(x) = 2 + \int_0^x\sqrt{1+x^4}\,dx $$ then what is the value of $ \dfrac {d} {dx} \Big(f^{-1}(x)\Big)\Bigg|_{x=2} $ Where I am stuck: Now, since we are to evaluate $ \dfrac {d} {dx} \Big(f^{-1}(x)\Big)\Bigg|_{x=2} $, all we need to evaluate is $ f'(2) $. Our answer will be the reciprocal of this. So I differentiated the given equation: $$ e^x(f(x) + f'(x)) = \sqrt{1+x^4}\ $$ But to find $f'(2)$ we also need to evaluate $f(2)$ which seems to be an insane thing to do. So what to do?",,"['calculus', 'integration', 'derivatives', 'functional-equations']"
44,Help with differential problem,Help with differential problem,,The equatorial radius of the earth is approximately 6370 kilometers. Suppose that a wire is wrapped tightly around the earth's equator. Use differentials to determine approximately how much this wire must be lengthened if it is to be strung all the way around the earth on poles 1 meter about the ground. I'm also not sure of what it looks like so it would be helpful if you could describe / show a diagram and if there were other ways of solving this without using differentials. Thanks in advance.,The equatorial radius of the earth is approximately 6370 kilometers. Suppose that a wire is wrapped tightly around the earth's equator. Use differentials to determine approximately how much this wire must be lengthened if it is to be strung all the way around the earth on poles 1 meter about the ground. I'm also not sure of what it looks like so it would be helpful if you could describe / show a diagram and if there were other ways of solving this without using differentials. Thanks in advance.,,"['calculus', 'derivatives']"
45,lipschitz function and its properties,lipschitz function and its properties,,"$f\colon\mathbb{R}\to\mathbb{R}$ satisfies $|f(x)-f(y)|\le |x-y|^{\beta}$. Which of the following are correct statements? $\beta=1$ $f$ is differentiable; $\beta>0$ $f$ is uniformly continuous; $\beta>1$ $f$ is constant function; $f$ must be a polynomial. $1\rightarrow |x|$ is an counter example, $2\rightarrow$ is true we can choose $\delta=\epsilon^{1\over\beta}$, $3\rightarrow$ true as$-(x-y) \le {f(x)-f(y)\over x-y}\le (x-y)$, taking limit $x\to y$ and then by sandwich theorem we see $f'(x)=0$ so $f$ is constant, $4$ is not true in general $f(x)=\sin x$ satsfies same relation.","$f\colon\mathbb{R}\to\mathbb{R}$ satisfies $|f(x)-f(y)|\le |x-y|^{\beta}$. Which of the following are correct statements? $\beta=1$ $f$ is differentiable; $\beta>0$ $f$ is uniformly continuous; $\beta>1$ $f$ is constant function; $f$ must be a polynomial. $1\rightarrow |x|$ is an counter example, $2\rightarrow$ is true we can choose $\delta=\epsilon^{1\over\beta}$, $3\rightarrow$ true as$-(x-y) \le {f(x)-f(y)\over x-y}\le (x-y)$, taking limit $x\to y$ and then by sandwich theorem we see $f'(x)=0$ so $f$ is constant, $4$ is not true in general $f(x)=\sin x$ satsfies same relation.",,"['real-analysis', 'derivatives', 'holder-spaces']"
46,complicated derivative with nested summations,complicated derivative with nested summations,,How would I solve for this derivative? $$s=\frac{1}{N} \sum_i^N \left[t_i - \left(\sum_j \left[c_j e^{-\frac{(r_i-r_j)^2}{2w^2}} + b\right]\right)\right]^2$$ I want to solve for $\dfrac{ds}{dw_j}$. I don't have the first clue on how to solve this. It has been a while since I have done calculus.,How would I solve for this derivative? $$s=\frac{1}{N} \sum_i^N \left[t_i - \left(\sum_j \left[c_j e^{-\frac{(r_i-r_j)^2}{2w^2}} + b\right]\right)\right]^2$$ I want to solve for $\dfrac{ds}{dw_j}$. I don't have the first clue on how to solve this. It has been a while since I have done calculus.,,"['derivatives', 'summation']"
47,optimizing a logdet function with respect to a scalar and the Hessian matrix,optimizing a logdet function with respect to a scalar and the Hessian matrix,,"Given a logdet function $\mathcal{L}(\gamma)$,  $$ \mathcal{L}(\gamma) = \log\vert \mathbf{I} + \gamma\mathbf{S} \vert - \mathbf{q}^T(\gamma^{-1}\mathbf{I} + \mathbf{S})^{-1} \mathbf{q}, $$ where $\mathbf{S}$ is a symmetric positive semi-definite matrix, $\mathbf{q}$ is a column vector, and $\mathbf{I}$ is the identity matrix. Can I solve $\frac{\partial \mathcal{L}(\gamma)}{\partial \gamma}=0$ in a closed form ? and how could I derive the Hessian $H = \frac{\partial^2 \mathcal{L}(\gamma)}{\partial \gamma^2}$ ?","Given a logdet function $\mathcal{L}(\gamma)$,  $$ \mathcal{L}(\gamma) = \log\vert \mathbf{I} + \gamma\mathbf{S} \vert - \mathbf{q}^T(\gamma^{-1}\mathbf{I} + \mathbf{S})^{-1} \mathbf{q}, $$ where $\mathbf{S}$ is a symmetric positive semi-definite matrix, $\mathbf{q}$ is a column vector, and $\mathbf{I}$ is the identity matrix. Can I solve $\frac{\partial \mathcal{L}(\gamma)}{\partial \gamma}=0$ in a closed form ? and how could I derive the Hessian $H = \frac{\partial^2 \mathcal{L}(\gamma)}{\partial \gamma^2}$ ?",,"['linear-algebra', 'matrices', 'derivatives', 'convex-analysis', 'convex-optimization']"
48,How I can explain this result geometrically for the single real $a$?,How I can explain this result geometrically for the single real ?,a,"The mean values theorem says that there exists a $c∈(u,v)$ such that $$f(v)−f(u)=f'(c)(v−u)$$ My question is: Assume that $f$ is a real analytic function. Fix the value of $v$ as $v=a$. Then we conclude the existence of a $c∈(u,a)$ for all $u≠a$ such that $$f'(c)=(f(a)-f(u))/(a-u)$$ How I can explain this result geometrically for the single real $a$? I know about the general setting: the slope of the line joining two points equal to the slope of the tangent to the curve at the point $(c,f(c))$.","The mean values theorem says that there exists a $c∈(u,v)$ such that $$f(v)−f(u)=f'(c)(v−u)$$ My question is: Assume that $f$ is a real analytic function. Fix the value of $v$ as $v=a$. Then we conclude the existence of a $c∈(u,a)$ for all $u≠a$ such that $$f'(c)=(f(a)-f(u))/(a-u)$$ How I can explain this result geometrically for the single real $a$? I know about the general setting: the slope of the line joining two points equal to the slope of the tangent to the curve at the point $(c,f(c))$.",,"['real-analysis', 'derivatives']"
49,Derivative of Function with Exponentials,Derivative of Function with Exponentials,,I would like to know the derivatives of the following function: $ y = x^e*e^x$ At first sight it looks like the product rule should be used and so one would get $e*x^{e-1}*e^x+x^e*e^x$. Is this correct?,I would like to know the derivatives of the following function: $ y = x^e*e^x$ At first sight it looks like the product rule should be used and so one would get $e*x^{e-1}*e^x+x^e*e^x$. Is this correct?,,[]
50,maximising the angle $\theta$,maximising the angle,\theta,"OK, suppose I have two points in cartesian coordinate system, say $P(x_1,y_1)$ and $Q(x_2,y_2)$. I have a line as well, that is, for simplicity $$y=mx$$ Assuming that  $$y_1\neq mx_1,y_2\neq mx_2$$ I need to find the point $A$ on the line so that the angle $PAQ$ is maximum. So I assume that point is $A(h,k)$. So the slope of $PA$ is ${k-y_1 \over h-x_1}$, and that of $QA$ is ${k-y_2 \over h-x_2}$. Then I find $\theta$ (the angle $PAQ$) by the inverse tangent way $$\theta=arctan({m_1-m_2 \over 1+m_1m_2 })$$ For the maxima, I differentiate this angle with respect to either $h$ or $k$ using the fact that $k=mh$ and put it to zero. Then all of that and the answer comes out. I plug that back and get a value. But the range of $arctan$ is from $[{-\pi \over 2},{\pi \over 2}]$. I do not think my way will work for obtuse angles. Is there any other way? P.S.Just give me a hint. I apologize if this is too ""elementary"". Diagram-","OK, suppose I have two points in cartesian coordinate system, say $P(x_1,y_1)$ and $Q(x_2,y_2)$. I have a line as well, that is, for simplicity $$y=mx$$ Assuming that  $$y_1\neq mx_1,y_2\neq mx_2$$ I need to find the point $A$ on the line so that the angle $PAQ$ is maximum. So I assume that point is $A(h,k)$. So the slope of $PA$ is ${k-y_1 \over h-x_1}$, and that of $QA$ is ${k-y_2 \over h-x_2}$. Then I find $\theta$ (the angle $PAQ$) by the inverse tangent way $$\theta=arctan({m_1-m_2 \over 1+m_1m_2 })$$ For the maxima, I differentiate this angle with respect to either $h$ or $k$ using the fact that $k=mh$ and put it to zero. Then all of that and the answer comes out. I plug that back and get a value. But the range of $arctan$ is from $[{-\pi \over 2},{\pi \over 2}]$. I do not think my way will work for obtuse angles. Is there any other way? P.S.Just give me a hint. I apologize if this is too ""elementary"". Diagram-",,"['calculus', 'derivatives', 'euclidean-geometry', 'coordinate-systems']"
51,Finding the derivative of a function with a Natural Log.,Finding the derivative of a function with a Natural Log.,,"I am trying to differentiate the function: $${\rm ln} \left(\frac{3x \ {\rm tan}(x)}{x^2 + 2}\right)$$ I think step one is to use the quotient rule of natural log expanding the expression. However doing this would still leave $\ln(3x \tan(x)) - \ln(x^2+2) $. Therefore, I'm confused if I should expand this expression again giving me..... $$ \ln(3x) + \ln(\tan(x)) - \ln(x^2+2)$$ Now, I would assume I should take the derivative of each term and apply the differentiation rule of natural log which is $u'/u$. However, if I do this I end up with a long answer of fractions that I cannot reduce much. Where am I going wrong here. The fact that there are two terms in the numerator, ""$3x$"" and ""$\tan(x)$"", is confusing me on what to do. I'm lost as to what procedures I should take in approaching problems like these. Can someone help me out or at the very least push me in the right direction?","I am trying to differentiate the function: $${\rm ln} \left(\frac{3x \ {\rm tan}(x)}{x^2 + 2}\right)$$ I think step one is to use the quotient rule of natural log expanding the expression. However doing this would still leave $\ln(3x \tan(x)) - \ln(x^2+2) $. Therefore, I'm confused if I should expand this expression again giving me..... $$ \ln(3x) + \ln(\tan(x)) - \ln(x^2+2)$$ Now, I would assume I should take the derivative of each term and apply the differentiation rule of natural log which is $u'/u$. However, if I do this I end up with a long answer of fractions that I cannot reduce much. Where am I going wrong here. The fact that there are two terms in the numerator, ""$3x$"" and ""$\tan(x)$"", is confusing me on what to do. I'm lost as to what procedures I should take in approaching problems like these. Can someone help me out or at the very least push me in the right direction?",,"['derivatives', 'logarithms']"
52,Question regarding Gâteaux Derivative,Question regarding Gâteaux Derivative,,"This is a question on an assignment for a grad engineering class that I cannot seem to figure out. The statement is as follows: Consider $X$ the space of continuous functions on the interval [0,1]. Let $F:X\to\mathbb{R}$ be defined by $F(x)=\max\limits_{0\leq t\leq 1} f(x)$ for any $f\in X$. We are supposed to show that the Gâteaux derivative of $F$ exists at $f\in C[0,1]$ if $f$ has a maximum at the unique point $x_0\in[0,1]$. I used the definition of the Gâteaux derivative $\left[(\lim\limits_{t\to0}\frac{1}{t}(F(f+th)-F(f))\text{ for each increment $h\in X$}\right]$ to see that $\lim\limits_{t\to0}\frac{1}{t}\left(F(f+th)-F(f)\right) \\=\lim\limits_{t\to0}\frac{1}{t}\left(\max\limits_{x}(f+th)(x)-\max\limits_{x}f(x)\right) \\=\lim\limits_{t\to0}\frac{1}{t}\left(\max\limits_{x}f(x)+\max\limits_{x}th(x)-\max\limits_{x}f(x)\right) \\=\lim\limits_{t\to0}t\frac{\max\limits_{x}h(x)}{t} \\=\max\limits_{x}h(x).$ I do not understand why $f$ having a maximum at $x_1\neq x_2\in[0,1]$ ruins the above argument. If anyone could shed some light on that, I would be most appreciative.","This is a question on an assignment for a grad engineering class that I cannot seem to figure out. The statement is as follows: Consider $X$ the space of continuous functions on the interval [0,1]. Let $F:X\to\mathbb{R}$ be defined by $F(x)=\max\limits_{0\leq t\leq 1} f(x)$ for any $f\in X$. We are supposed to show that the Gâteaux derivative of $F$ exists at $f\in C[0,1]$ if $f$ has a maximum at the unique point $x_0\in[0,1]$. I used the definition of the Gâteaux derivative $\left[(\lim\limits_{t\to0}\frac{1}{t}(F(f+th)-F(f))\text{ for each increment $h\in X$}\right]$ to see that $\lim\limits_{t\to0}\frac{1}{t}\left(F(f+th)-F(f)\right) \\=\lim\limits_{t\to0}\frac{1}{t}\left(\max\limits_{x}(f+th)(x)-\max\limits_{x}f(x)\right) \\=\lim\limits_{t\to0}\frac{1}{t}\left(\max\limits_{x}f(x)+\max\limits_{x}th(x)-\max\limits_{x}f(x)\right) \\=\lim\limits_{t\to0}t\frac{\max\limits_{x}h(x)}{t} \\=\max\limits_{x}h(x).$ I do not understand why $f$ having a maximum at $x_1\neq x_2\in[0,1]$ ruins the above argument. If anyone could shed some light on that, I would be most appreciative.",,"['functional-analysis', 'derivatives']"
53,"Analysis: Show that $f(x)$ converges, pointwise and uniformly on $\mathbb{R}$ to a differentiable function $f$ that satisfies:","Analysis: Show that  converges, pointwise and uniformly on  to a differentiable function  that satisfies:",f(x) \mathbb{R} f,"Show that $$f(x) = \sum_{k=1}^\infty \frac{\sin kx}{k^3} \tag1$$ converges, pointwise and uniformly on $\mathbb R$, to a differentiable function that satisfies $$\int_0^{\pi/2} f(x)\,dx = \sum_{k=1}^\infty \left(\frac{(-1)^{k-1}}{(2k)^4}+\frac{1}{k^4}\right)\tag{a}$$ and  $$ |f'(x)|\le \pi/8 \tag{b} $$ I would be able to do it if it wasn't for the part ""$f$ that satisfies"" (a) and (b).  If someone could explain it to me, that would be helpful.","Show that $$f(x) = \sum_{k=1}^\infty \frac{\sin kx}{k^3} \tag1$$ converges, pointwise and uniformly on $\mathbb R$, to a differentiable function that satisfies $$\int_0^{\pi/2} f(x)\,dx = \sum_{k=1}^\infty \left(\frac{(-1)^{k-1}}{(2k)^4}+\frac{1}{k^4}\right)\tag{a}$$ and  $$ |f'(x)|\le \pi/8 \tag{b} $$ I would be able to do it if it wasn't for the part ""$f$ that satisfies"" (a) and (b).  If someone could explain it to me, that would be helpful.",,"['sequences-and-series', 'derivatives', 'fourier-series']"
54,Derivative chainrule on khanacadamy ignoring some terms,Derivative chainrule on khanacadamy ignoring some terms,,"I watched the chain rule series on khanacademy.org and decided to do the ""questions"". One of the questions is: Let $y = \sin(6x^2−4x−1+3x^{−1}−5x^{−2})$ $dy/dx=?$ The answer is $dy/dx=(\cos(6x^2−4x−1+3x^{−1}−5x^{−2}))(12x−4)$ Since $dy/dx [f(g(x))] = {\rm D}f(g(x))*{\rm D}g(x)$ I figure that it should be $dy/dx = \cos(6x^2−4x−1+3x^{−1}−5x^{−2})(12x-4-3x^{-2}+10x^{-3})$ Why aren't they deriving the negative exponent terms in the inner function ""$g(x)$""? Thank you in advance for your help!","I watched the chain rule series on khanacademy.org and decided to do the ""questions"". One of the questions is: Let $y = \sin(6x^2−4x−1+3x^{−1}−5x^{−2})$ $dy/dx=?$ The answer is $dy/dx=(\cos(6x^2−4x−1+3x^{−1}−5x^{−2}))(12x−4)$ Since $dy/dx [f(g(x))] = {\rm D}f(g(x))*{\rm D}g(x)$ I figure that it should be $dy/dx = \cos(6x^2−4x−1+3x^{−1}−5x^{−2})(12x-4-3x^{-2}+10x^{-3})$ Why aren't they deriving the negative exponent terms in the inner function ""$g(x)$""? Thank you in advance for your help!",,"['calculus', 'derivatives']"
55,Doubt about temporal derivative of a partial derivative,Doubt about temporal derivative of a partial derivative,,I have $\theta (t)$ and $\phi (t)$ and I have to find: $$\frac {d}{dt}\left(\frac{\partial \cos(\theta - \phi)\dot \theta \dot\phi}{\partial \dot\theta}\right) $$ Why the correct result is $$\cos(\theta-\phi)\ddot \phi+\sin(\theta-\phi)\dot \phi^2$$ instead of $$-\sin(\theta-\phi)(\dot \theta-\dot\phi)\dot\phi+\cos(\theta-\phi)\ddot\phi$$ Thanks for any help!,I have $\theta (t)$ and $\phi (t)$ and I have to find: $$\frac {d}{dt}\left(\frac{\partial \cos(\theta - \phi)\dot \theta \dot\phi}{\partial \dot\theta}\right) $$ Why the correct result is $$\cos(\theta-\phi)\ddot \phi+\sin(\theta-\phi)\dot \phi^2$$ instead of $$-\sin(\theta-\phi)(\dot \theta-\dot\phi)\dot\phi+\cos(\theta-\phi)\ddot\phi$$ Thanks for any help!,,['derivatives']
56,The monotonicity of function $y=x/2 + x^{2}\sin(1/x)$ near $x=0$,The monotonicity of function  near,y=x/2 + x^{2}\sin(1/x) x=0,"Is the function $y=x/2 + x^{2}\sin(1/x)$ monotonic near $0$? The derivative $f'$ obviously goes positive and negative near $0$, because $$f'(x)= \frac12 + 2x\sin(1/x) - \cos(1/x))$$ Does that mean that $f$ is not monotonic near $0$?","Is the function $y=x/2 + x^{2}\sin(1/x)$ monotonic near $0$? The derivative $f'$ obviously goes positive and negative near $0$, because $$f'(x)= \frac12 + 2x\sin(1/x) - \cos(1/x))$$ Does that mean that $f$ is not monotonic near $0$?",,"['real-analysis', 'derivatives']"
57,Derivative of a parallel translation inside a metric,Derivative of a parallel translation inside a metric,,"Let $M$ be a riemannian manifold with metric $g$ and a connection    $\nabla$ on $M$.  Let $X,Y$ two vector fields along a curve $\gamma$    on $M$. Let $$\tau_{t,s}:T_{\gamma(s)}M\to T_{\gamma(t)}M$$ the    parallel translation along $\gamma$. We consider the fonction    $$F(t)=g(\gamma(t))(\tau_{t,s} X(\gamma(s)), \tau_{t,s}  Y(\gamma(s))).$$ Then    $$F'(s)=g(\gamma(t))\left(\frac{d}{dt}\bigg|_{t=s}\tau_{t,s}  X(\gamma(s)),  \tau_{t,s}Y(\gamma(s))\right)+g(\gamma(t))\left(\frac{d}{dt}\bigg|_{t=s}\tau_{t,s}  Y(\gamma(s)), \tau_{t,s}X(\gamma(s))\right)+\dot\gamma\left[g(X,Y)\right](\gamma(s)) \ (\star).$$ I do not understand why this identity holds. Here is what I have tried so far: Let $$a(t)=\tau_{t,s} X(\gamma(s))=a_i(t)\frac{\partial}{\partial x_i}\bigg|_{\gamma(t)}\text{ and }b(t)=\tau_{t,s} Y(\gamma(s))=b_j(t)\frac{\partial}{\partial x_i}\bigg|_{\gamma(t)},$$ so $F(t)=g(\gamma(t))(a(t),b(t))$. If we let $$g_{ij}(t)=g(\gamma(t))\left(\frac{\partial}{\partial x_i}\bigg|_{\gamma(t)}, \frac{\partial}{\partial x_j}\bigg|_{\gamma(t)}\right),$$ we find that $F(t)=a_i(t)b_j(t)g_{ij}(t)$, thus the derivative of $F$ is $$F'(s)=a_i'(s)b_j(s)g_{ij}(s)+a_i(s)b_j'(s)g_{ij}(s)+a_i(s)b_j(s)g'_{ij}(s).$$ The first two terms give the first two terms of $(\star)$, but what about the last one ? We have that $$\dot\gamma\left[g(X,Y)\right](\gamma(s))=\frac{d}{dt}\bigg|_{t=s} g(\gamma(t))(X(\gamma(t)),Y(\gamma(t))),$$ right ? This does not seem equal to $a_i(s)b_j(s)g'_{ij}(s)$... ?","Let $M$ be a riemannian manifold with metric $g$ and a connection    $\nabla$ on $M$.  Let $X,Y$ two vector fields along a curve $\gamma$    on $M$. Let $$\tau_{t,s}:T_{\gamma(s)}M\to T_{\gamma(t)}M$$ the    parallel translation along $\gamma$. We consider the fonction    $$F(t)=g(\gamma(t))(\tau_{t,s} X(\gamma(s)), \tau_{t,s}  Y(\gamma(s))).$$ Then    $$F'(s)=g(\gamma(t))\left(\frac{d}{dt}\bigg|_{t=s}\tau_{t,s}  X(\gamma(s)),  \tau_{t,s}Y(\gamma(s))\right)+g(\gamma(t))\left(\frac{d}{dt}\bigg|_{t=s}\tau_{t,s}  Y(\gamma(s)), \tau_{t,s}X(\gamma(s))\right)+\dot\gamma\left[g(X,Y)\right](\gamma(s)) \ (\star).$$ I do not understand why this identity holds. Here is what I have tried so far: Let $$a(t)=\tau_{t,s} X(\gamma(s))=a_i(t)\frac{\partial}{\partial x_i}\bigg|_{\gamma(t)}\text{ and }b(t)=\tau_{t,s} Y(\gamma(s))=b_j(t)\frac{\partial}{\partial x_i}\bigg|_{\gamma(t)},$$ so $F(t)=g(\gamma(t))(a(t),b(t))$. If we let $$g_{ij}(t)=g(\gamma(t))\left(\frac{\partial}{\partial x_i}\bigg|_{\gamma(t)}, \frac{\partial}{\partial x_j}\bigg|_{\gamma(t)}\right),$$ we find that $F(t)=a_i(t)b_j(t)g_{ij}(t)$, thus the derivative of $F$ is $$F'(s)=a_i'(s)b_j(s)g_{ij}(s)+a_i(s)b_j'(s)g_{ij}(s)+a_i(s)b_j(s)g'_{ij}(s).$$ The first two terms give the first two terms of $(\star)$, but what about the last one ? We have that $$\dot\gamma\left[g(X,Y)\right](\gamma(s))=\frac{d}{dt}\bigg|_{t=s} g(\gamma(t))(X(\gamma(t)),Y(\gamma(t))),$$ right ? This does not seem equal to $a_i(s)b_j(s)g'_{ij}(s)$... ?",,"['derivatives', 'riemannian-geometry']"
58,Chain rule and gradients,Chain rule and gradients,,"Suppose $f:\mathbb{R}^n \to \mathbb{R}^n$ and $v:\mathbb{R}^n \to \mathbb{R}$. I want to find $\nabla (v \circ f).$ Am I right: $\nabla (v \circ f) = \nabla v|_f \cdot Df$ where $Df$ is the matrix of partial derivatives of $f$. This is what Wikipedia tells me. But we are doing a column vector dotted with a matrix, so I guess it should be $Df \cdot \nabla v|_f$? Am I correct?","Suppose $f:\mathbb{R}^n \to \mathbb{R}^n$ and $v:\mathbb{R}^n \to \mathbb{R}$. I want to find $\nabla (v \circ f).$ Am I right: $\nabla (v \circ f) = \nabla v|_f \cdot Df$ where $Df$ is the matrix of partial derivatives of $f$. This is what Wikipedia tells me. But we are doing a column vector dotted with a matrix, so I guess it should be $Df \cdot \nabla v|_f$? Am I correct?",,"['calculus', 'derivatives']"
59,Elementary Analysis,Elementary Analysis,,"I wish to prove the following: If $f: \mathbb{R} \to \mathbb{R}$ is a differentiable function, $x_0$ is a point in $\mathbb{R}$ such that $f'(x_0) = 0$, and $f''(x_0) > 0$ (so in particular $f''$ exists at $x_0$), then there is a $d > 0$ so that $f(x_0) < f(x)$ for all $x \in (x_0 - d, x_0 + d)$ that aren't equal to $x_0$.","I wish to prove the following: If $f: \mathbb{R} \to \mathbb{R}$ is a differentiable function, $x_0$ is a point in $\mathbb{R}$ such that $f'(x_0) = 0$, and $f''(x_0) > 0$ (so in particular $f''$ exists at $x_0$), then there is a $d > 0$ so that $f(x_0) < f(x)$ for all $x \in (x_0 - d, x_0 + d)$ that aren't equal to $x_0$.",,['real-analysis']
60,What is it called when a function is not continuous but still can have a derivative?,What is it called when a function is not continuous but still can have a derivative?,,"Consider the following function (I think it has a name, but I don't remember it): $$ f(x) = \cases{-1 & $x < 0$ \\ 0 & $x = 0$ \\ 1 & $x > 0$} $$ $f'(x)$ is zero everywhere except at $x=0$, where $f$ is not continuous. But suppose we ignore the right half of the real line and define $f(0)$ to be $-1$. Then $f$ has a left derivative at $x=0$, and it is zero. We can do the same thing from the right, so in a way it could make a little bit of sense to say that $f'(0) =0$. Of course, I understand that going by the definition $f$ isn't differentiable at $x=0$. But one could imagine an alternative definition of derivative for discontinous functions, in which one calculates lateral derivatives by redefining the function to be continuous, and then we see if the lateral derivatives match. This doesn't always work; for example it's hard to meaningfully assign a derivative to $x \mapsto |x|$ at $x=0$. Are there other functions with this property? Does it have a name?","Consider the following function (I think it has a name, but I don't remember it): $$ f(x) = \cases{-1 & $x < 0$ \\ 0 & $x = 0$ \\ 1 & $x > 0$} $$ $f'(x)$ is zero everywhere except at $x=0$, where $f$ is not continuous. But suppose we ignore the right half of the real line and define $f(0)$ to be $-1$. Then $f$ has a left derivative at $x=0$, and it is zero. We can do the same thing from the right, so in a way it could make a little bit of sense to say that $f'(0) =0$. Of course, I understand that going by the definition $f$ isn't differentiable at $x=0$. But one could imagine an alternative definition of derivative for discontinous functions, in which one calculates lateral derivatives by redefining the function to be continuous, and then we see if the lateral derivatives match. This doesn't always work; for example it's hard to meaningfully assign a derivative to $x \mapsto |x|$ at $x=0$. Are there other functions with this property? Does it have a name?",,"['calculus', 'derivatives']"
61,Derivative of the exponential of a function,Derivative of the exponential of a function,,"What's the general expression of $\dfrac{d ^n e^{f(x)}}{d  x^n}$ in term of derivative $\dfrac{d f(x)}{d x}$, $\dfrac{d ^2e^{f(x)}}{d x^2}$,$\dfrac{d ^3e^{f(x)}}{d x^3},\dots$? Actually I wonder whether there is a special expression for the series of coefficients for different integers $n$? Some of the series: $(1),(1,1),(1,3,1),(1,6,4,3,1),(1,10,10,5,15,10,1)\dots$","What's the general expression of $\dfrac{d ^n e^{f(x)}}{d  x^n}$ in term of derivative $\dfrac{d f(x)}{d x}$, $\dfrac{d ^2e^{f(x)}}{d x^2}$,$\dfrac{d ^3e^{f(x)}}{d x^3},\dots$? Actually I wonder whether there is a special expression for the series of coefficients for different integers $n$? Some of the series: $(1),(1,1),(1,3,1),(1,6,4,3,1),(1,10,10,5,15,10,1)\dots$",,"['calculus', 'derivatives']"
62,Tricky derivative with chain rule.,Tricky derivative with chain rule.,,"Let $z = 1/x$ and $y = f(z)$, find $\dfrac{d^2y}{dx^2}$ So the answer was $$\frac{\mathrm{d} y}{\mathrm{d} x} = \frac{\mathrm{d} y}{\mathrm{d} z}\frac{\mathrm{d}z }{\mathrm{d}x }$$ Where $$\frac{\mathrm{d} y}{\mathrm{d} x} = \frac{\mathrm{d} y}{\mathrm{d} z}\frac{\mathrm{d}z }{\mathrm{d}x } =-\frac{\mathrm{d} y}{\mathrm{d} z}\frac{1}{x^2} = -\frac{\mathrm{d} y}{\mathrm{d} z}z^2$$ $$\dfrac{d^2y}{dx^2} =  \frac{\mathrm{d} }{\mathrm{d} x}\left ( \frac{\mathrm{d} y}{\mathrm{d} x} \right ) = \frac{\mathrm{d} }{\mathrm{d} x}\left ( \frac{\mathrm{d} y}{\mathrm{d} z}\frac{\mathrm{d}z }{\mathrm{d}x }\right ) = \frac{\mathrm{d} ^2y}{\mathrm{d} z^2}\left (\frac{\mathrm{d} z}{\mathrm{d} x}  \right )^2 + \frac{\mathrm{d} y}{\mathrm{d} z}\frac{\mathrm{d} ^2z}{\mathrm{d} x^2} $$ Could someone explain to me how on earth did $\frac{\mathrm{d} ^2y}{\mathrm{d} z^2}\left (\frac{\mathrm{d} z}{\mathrm{d} x}  \right )^2$ appear? EDIT: I am going to show what I did. $$\frac{\mathrm{d} }{\mathrm{d} x}\left ( \frac{\mathrm{d} y}{\mathrm{d} z}\frac{\mathrm{d}z }{\mathrm{d}x }\right ) = \frac{d}{dx}\left(\frac{dy}{dz}\right) \frac{dz}{dx}+\frac{d ^2z}{dx^2}\frac{dy}{dz} = \frac{d}{dx}\left(\frac{-1}{z^2} \frac{dy}{dx}\right) \frac{dz}{dx}+\frac{d ^2z}{dx^2}\frac{dy}{dz}$$ Basically I don't understand how $$\frac{d}{dx}\left(\frac{-1}{z^2} \frac{dy}{dx}\right)$$ could turn into $$\frac{\mathrm{d} ^2y}{\mathrm{d} z^2}\left (\frac{\mathrm{d} z}{\mathrm{d} x}  \right )^2$$ In fact I got  $$\frac{d}{dx}\left(\frac{-1}{z^2} \frac{dy}{dx}\right) = 2z^{-3}\frac{\mathrm{d} z}{\mathrm{d} x}\frac{\mathrm{d} y}{\mathrm{d} x} - \frac{1}{z^2}\frac{\mathrm{d} ^2 y}{\mathrm{d} x^2}$$","Let $z = 1/x$ and $y = f(z)$, find $\dfrac{d^2y}{dx^2}$ So the answer was $$\frac{\mathrm{d} y}{\mathrm{d} x} = \frac{\mathrm{d} y}{\mathrm{d} z}\frac{\mathrm{d}z }{\mathrm{d}x }$$ Where $$\frac{\mathrm{d} y}{\mathrm{d} x} = \frac{\mathrm{d} y}{\mathrm{d} z}\frac{\mathrm{d}z }{\mathrm{d}x } =-\frac{\mathrm{d} y}{\mathrm{d} z}\frac{1}{x^2} = -\frac{\mathrm{d} y}{\mathrm{d} z}z^2$$ $$\dfrac{d^2y}{dx^2} =  \frac{\mathrm{d} }{\mathrm{d} x}\left ( \frac{\mathrm{d} y}{\mathrm{d} x} \right ) = \frac{\mathrm{d} }{\mathrm{d} x}\left ( \frac{\mathrm{d} y}{\mathrm{d} z}\frac{\mathrm{d}z }{\mathrm{d}x }\right ) = \frac{\mathrm{d} ^2y}{\mathrm{d} z^2}\left (\frac{\mathrm{d} z}{\mathrm{d} x}  \right )^2 + \frac{\mathrm{d} y}{\mathrm{d} z}\frac{\mathrm{d} ^2z}{\mathrm{d} x^2} $$ Could someone explain to me how on earth did $\frac{\mathrm{d} ^2y}{\mathrm{d} z^2}\left (\frac{\mathrm{d} z}{\mathrm{d} x}  \right )^2$ appear? EDIT: I am going to show what I did. $$\frac{\mathrm{d} }{\mathrm{d} x}\left ( \frac{\mathrm{d} y}{\mathrm{d} z}\frac{\mathrm{d}z }{\mathrm{d}x }\right ) = \frac{d}{dx}\left(\frac{dy}{dz}\right) \frac{dz}{dx}+\frac{d ^2z}{dx^2}\frac{dy}{dz} = \frac{d}{dx}\left(\frac{-1}{z^2} \frac{dy}{dx}\right) \frac{dz}{dx}+\frac{d ^2z}{dx^2}\frac{dy}{dz}$$ Basically I don't understand how $$\frac{d}{dx}\left(\frac{-1}{z^2} \frac{dy}{dx}\right)$$ could turn into $$\frac{\mathrm{d} ^2y}{\mathrm{d} z^2}\left (\frac{\mathrm{d} z}{\mathrm{d} x}  \right )^2$$ In fact I got  $$\frac{d}{dx}\left(\frac{-1}{z^2} \frac{dy}{dx}\right) = 2z^{-3}\frac{\mathrm{d} z}{\mathrm{d} x}\frac{\mathrm{d} y}{\mathrm{d} x} - \frac{1}{z^2}\frac{\mathrm{d} ^2 y}{\mathrm{d} x^2}$$",,"['calculus', 'derivatives']"
63,finding the derivative of an arcsin,finding the derivative of an arcsin,,I'm not sure if I did the problem right. Any help verifying would be great. finding the derivative $$y= \arcsin(e^x)$$ $$\frac{dy}{dx}= \frac{1}{\sqrt{1-(e^x)^2}} \cdot e^x \cdot 1$$,I'm not sure if I did the problem right. Any help verifying would be great. finding the derivative $$y= \arcsin(e^x)$$ $$\frac{dy}{dx}= \frac{1}{\sqrt{1-(e^x)^2}} \cdot e^x \cdot 1$$,,"['calculus', 'derivatives']"
64,Is this a valid formula for the Wronskian?,Is this a valid formula for the Wronskian?,,"I was messing around with the Wronskian of two functions $y_1(x)$ and $y_2(x)$ , which is defined by: $$ W(y_1,y_2) = \begin{vmatrix} y_1 & y_2 \\ y_1' & y_2' \end{vmatrix} = y_1y_2'-y_2y_1^{\prime} $$ And came across a supposedly new way of calculating the Wronskian, and I would like to know if it is correct: \begin{align} \mbox{We know that}\ \frac{d}{dx}\left(\frac{y_2}{y_1}\right) & = \frac{y_1y_2'-y_2y_1'}{y_1^2} \\[3mm] \mbox{So then:}\ \frac{W(y_1,y_2)}{y_1^2} & = \frac{d}{dx}\left(\frac{y_2}{y_1}\right) \\[3mm] \mbox{Which gives us:}\ W(y_1,y_2) & = y_1^2\left(\frac{d}{dx}\left(\frac{y_2}{y_1}\right)\right) \end{align} Will this always be true, assuming $y_1 \neq 0$ ?.","I was messing around with the Wronskian of two functions and , which is defined by: And came across a supposedly new way of calculating the Wronskian, and I would like to know if it is correct: Will this always be true, assuming ?.","y_1(x) y_2(x) 
W(y_1,y_2) = \begin{vmatrix}
y_1 & y_2 \\
y_1' & y_2'
\end{vmatrix} = y_1y_2'-y_2y_1^{\prime}
 \begin{align}
\mbox{We know that}\ \frac{d}{dx}\left(\frac{y_2}{y_1}\right) & = \frac{y_1y_2'-y_2y_1'}{y_1^2}
\\[3mm] \mbox{So then:}\
\frac{W(y_1,y_2)}{y_1^2} & = \frac{d}{dx}\left(\frac{y_2}{y_1}\right)
\\[3mm]
\mbox{Which gives us:}\
W(y_1,y_2) & = y_1^2\left(\frac{d}{dx}\left(\frac{y_2}{y_1}\right)\right)
\end{align} y_1 \neq 0","['calculus', 'derivatives', 'wronskian']"
65,What is the Euclidean norm of the vector containing all $k$-order partial derivatives of $|x|$?,What is the Euclidean norm of the vector containing all -order partial derivatives of ?,k |x|,"Denote $|x|$ the Euclidean norm of a vector $x\in\mathbb R^N$ . Also denote $D^kf$ as the vector in $\mathbb R^{N^k}$ containing all $k$ -order partial derivatives of the function $f\colon\mathbb R^N\to\mathbb R$ . Consider the following norm for $D^kf(x)\in\mathbb R^{N^k}$ : $$\|D^kf(x)\|=\left[\sum_{i_1,\ldots,i_k=1}^N\left(\dfrac{\partial^kf}{\partial x_{i_1}\cdots\partial x_{i_k}}(x)\right)^2\right]^{\frac12}. $$ Now let $f(x)=|x|$ . My question is if the following equation holds for some $C_{N,k}\in\mathbb R$ : $$\|D^kf(x)\|=\dfrac{C_{N,k}}{|x|^{k-1}},\quad\forall x\in\mathbb R^N\backslash\{0\}\mbox{ and }k\in\mathbb N=\{1,2,\ldots\}. $$ The case $k=1,2,3$ : I was able to prove for those cases because $$\dfrac{\partial f}{\partial x_i}(x)=\dfrac{x_i}{|x|},\ \dfrac{\partial^2f}{\partial x_i\partial x_j}(x)=\dfrac{\delta_{ij}}{|x|}-\dfrac{x_ix_j}{|x|^3},$$ and $$\dfrac{\partial^3f}{\partial x_i\partial x_j\partial x_k}(x)=\dfrac{3x_ix_jx_k}{|x|^5}-\dfrac{\delta_{jk}x_i}{|x|^3}-\dfrac{\delta_{ij}x_k}{|x|^3}-\dfrac{\delta_{ik}x_j}{|x|^3}.$$ After doing the calculations, we have $$\|D^1f(x)\|=1,\ \|D^2f(x)\|=\dfrac{\sqrt{N-1}}{|x|},\mbox{ and }\|D^3f(x)\|=\dfrac{\sqrt{3N-3}}{|x|^2}.$$ Therefore, the question is true for $k=1,2,3$ with $C_{N,1}=1$ , $C_{N,2}=\sqrt{N-1}$ , and $C_{N,3}=\sqrt{3N-3}$ . My try: I only have two types of ideas that seem promising. First idea: proving by induction on $k$ . However, I was not able to use the induction hypothesis properly. Second idea: for $k\geq3$ , we have $$\dfrac{\partial^k}{\partial x_{i_1}\cdots\partial x_{i_k}}\left(|x|^2\right)=0.$$ Unfortunately, I was not able to develop the left-term (using chain rule) to isolate the term $\frac{\partial^kf}{\partial x_{i_1}\cdots\partial x_{i_k}}(x)$ to calculate $\|D^kf(x)\|$ . A last attempt (less promising) is to develop by ""brute force"" the term $\frac{\partial^kf}{\partial x_{i_1}\cdots\partial x_{i_k}}(x)$ to obtain an expression for it which should be formally proved by induction on $k$ .","Denote the Euclidean norm of a vector . Also denote as the vector in containing all -order partial derivatives of the function . Consider the following norm for : Now let . My question is if the following equation holds for some : The case : I was able to prove for those cases because and After doing the calculations, we have Therefore, the question is true for with , , and . My try: I only have two types of ideas that seem promising. First idea: proving by induction on . However, I was not able to use the induction hypothesis properly. Second idea: for , we have Unfortunately, I was not able to develop the left-term (using chain rule) to isolate the term to calculate . A last attempt (less promising) is to develop by ""brute force"" the term to obtain an expression for it which should be formally proved by induction on .","|x| x\in\mathbb R^N D^kf \mathbb R^{N^k} k f\colon\mathbb R^N\to\mathbb R D^kf(x)\in\mathbb R^{N^k} \|D^kf(x)\|=\left[\sum_{i_1,\ldots,i_k=1}^N\left(\dfrac{\partial^kf}{\partial x_{i_1}\cdots\partial x_{i_k}}(x)\right)^2\right]^{\frac12}.  f(x)=|x| C_{N,k}\in\mathbb R \|D^kf(x)\|=\dfrac{C_{N,k}}{|x|^{k-1}},\quad\forall x\in\mathbb R^N\backslash\{0\}\mbox{ and }k\in\mathbb N=\{1,2,\ldots\}.  k=1,2,3 \dfrac{\partial f}{\partial x_i}(x)=\dfrac{x_i}{|x|},\ \dfrac{\partial^2f}{\partial x_i\partial x_j}(x)=\dfrac{\delta_{ij}}{|x|}-\dfrac{x_ix_j}{|x|^3}, \dfrac{\partial^3f}{\partial x_i\partial x_j\partial x_k}(x)=\dfrac{3x_ix_jx_k}{|x|^5}-\dfrac{\delta_{jk}x_i}{|x|^3}-\dfrac{\delta_{ij}x_k}{|x|^3}-\dfrac{\delta_{ik}x_j}{|x|^3}. \|D^1f(x)\|=1,\ \|D^2f(x)\|=\dfrac{\sqrt{N-1}}{|x|},\mbox{ and }\|D^3f(x)\|=\dfrac{\sqrt{3N-3}}{|x|^2}. k=1,2,3 C_{N,1}=1 C_{N,2}=\sqrt{N-1} C_{N,3}=\sqrt{3N-3} k k\geq3 \dfrac{\partial^k}{\partial x_{i_1}\cdots\partial x_{i_k}}\left(|x|^2\right)=0. \frac{\partial^kf}{\partial x_{i_1}\cdots\partial x_{i_k}}(x) \|D^kf(x)\| \frac{\partial^kf}{\partial x_{i_1}\cdots\partial x_{i_k}}(x) k","['real-analysis', 'derivatives', 'normed-spaces', 'induction', 'partial-derivative']"
66,Justification for differentiation under integral,Justification for differentiation under integral,,"Given a function I seek to find its derivative $$f(x) = \int_{\frac{1}{x}}^{\frac{e^x}{x}} \frac{\cos(xt)}{t} \, dt, \quad (x>0)$$ My question is regarding the justification of the differentiation under the integral sign rather than how to do it. The theorems that I have at hand are the following. $\textbf{Theorem 1}$ Let $[a,b]$ compact interval, $J$ open interval, suppose that $f(x,t), \frac{\partial f}{\partial t}(x, t)$ continuous for $(x,t)\in [a,b]\times J$ . Then $F(t) = \int_{a}^{b}\frac{\partial f}{\partial t}(x, t) \,dx\quad t\in J$ . $\textbf{Theorem 2 (For generalised domains)}$ Suppose there are majorants $g(x), h(x)$ such that $\left| f(x,t) \right| \le g(x), \left| \frac{\partial f}{\partial t}(x, t) \right| \le h(x)$ and $\int_Ig(x)\,dx < \infty,\quad \int_Ih(x)\,dx < \infty $ . We form $F(t) = \int_If(x,t)\, dx$ then $F(t)$ is differentiable and $F'(t) = \int_I\frac{\partial f}{\partial t}(x, t) \,dx$ . Now if we come back to the function I gave as an example if $x<\infty$ then the interval is compact it is also clear that the integrand and its derivative are continuous on that interval hence differentiation is justified as I understand. Now consider $x \to \infty$ if we use the second theorem a possible majorant is $\frac{1}{t}$ but leads to $x<\infty$ . As I understand this is not possible as $x$ here is a variable. A similar issue occurs with bounding its derivative. Could anyone clarify what I am misunderstanding, or perhaps I am choosing the wrong majorants? Perhaps I have to assume that the interval is not generalised. Thanks for any ideas and clarifications!","Given a function I seek to find its derivative My question is regarding the justification of the differentiation under the integral sign rather than how to do it. The theorems that I have at hand are the following. Let compact interval, open interval, suppose that continuous for . Then . Suppose there are majorants such that and . We form then is differentiable and . Now if we come back to the function I gave as an example if then the interval is compact it is also clear that the integrand and its derivative are continuous on that interval hence differentiation is justified as I understand. Now consider if we use the second theorem a possible majorant is but leads to . As I understand this is not possible as here is a variable. A similar issue occurs with bounding its derivative. Could anyone clarify what I am misunderstanding, or perhaps I am choosing the wrong majorants? Perhaps I have to assume that the interval is not generalised. Thanks for any ideas and clarifications!","f(x) = \int_{\frac{1}{x}}^{\frac{e^x}{x}} \frac{\cos(xt)}{t} \, dt, \quad (x>0) \textbf{Theorem 1} [a,b] J f(x,t), \frac{\partial f}{\partial t}(x, t) (x,t)\in [a,b]\times J F(t) = \int_{a}^{b}\frac{\partial f}{\partial t}(x, t) \,dx\quad t\in J \textbf{Theorem 2 (For generalised domains)} g(x), h(x) \left| f(x,t) \right| \le g(x), \left| \frac{\partial f}{\partial t}(x, t) \right| \le h(x) \int_Ig(x)\,dx < \infty,\quad \int_Ih(x)\,dx < \infty  F(t) = \int_If(x,t)\, dx F(t) F'(t) = \int_I\frac{\partial f}{\partial t}(x, t) \,dx x<\infty x \to \infty \frac{1}{t} x<\infty x","['real-analysis', 'calculus', 'integration', 'derivatives', 'vector-analysis']"
67,Condition on derivatives,Condition on derivatives,,"I am working with a ""well-behaved"" optimization problem of the form: \begin{equation*} \max_{x} f( g_{1}( x) ,g_{2}( x) ,g_{3}( x) ,\mathbf{y}) \end{equation*} where $\displaystyle f:\mathbb{R}^{n+3}\rightarrow \mathbb{R}$ is continously differentiable, $\displaystyle x\in \mathbb{R}$ , $\displaystyle \mathbf{y} \in \mathbb{R}^{n}$ are parameters, and $\displaystyle g_{k} :\mathbb{R}\rightarrow \mathbb{R}$ for $\displaystyle k=1,2,3\ $ are continuosly differentiable. I am interested in studying the assumptions I could make on $\displaystyle f$ and $\displaystyle g_{k}$ to study the ""comparative statics"" of the solution around an optimum. Say that there is a function $\displaystyle x\left(\mathbf{y}^{*}\right) :\mathbb{R}^{n+1}\rightarrow \mathbb{R}$ such that: \begin{equation*} \sum _{k=1}^{3}\frac{\partial f}{\partial g_{k}}\frac{\partial g_{k}}{\partial x}\left( x\left(\mathbf{y}^{*}\right) ,\mathbf{y}^{*}\right) =0 \end{equation*} I am taking functional assumptions over the $\displaystyle g_{k}$ functions and study their implications, such as: \begin{gather*} g_{1}( x) =cy_{1} x-cy_{2} ,\ c\in [ 0,1]\\ g_{2}( x) =( 1-y_{1}) x+y_{2}\\ g_{3}( x) =x \end{gather*} The standard problem I am building on takes $\displaystyle c=0$ , so I am interested in studying what happens when I increase $\displaystyle c$ around the optimal solution. I am interested in elasticity notions, for instance: \begin{equation*} {\textstyle \epsilon =\frac{dx}{dy_{1}}\left(\mathbf{y}^{*}\right)\frac{1-y_{1}}{x\left(\mathbf{y}^{*}\right)}} \end{equation*} Using the Implicit function theorem (I include my derivations below), I find that $\displaystyle \epsilon $ is given by: \begin{equation*} {\textstyle \epsilon =-\frac{\frac{1}{x} c\left(\frac{\partial f}{\partial g_{1}} -\frac{\partial f}{\partial g_{2}}\right) +cy_{1}\left( c\frac{\partial ^{2} f}{\partial g_{1} \partial g_{1}} -\frac{\partial ^{2} f}{\partial g_{1} \partial g_{2}}\right) +( 1-y_{1})\left( c\frac{\partial ^{2} f}{\partial g_{2} \partial g_{1}} -\frac{\partial ^{2} f}{\partial g_{2} \partial g_{2}}\right) +\left( c\frac{\partial ^{2} f}{\partial g_{3} \partial g_{1}} -\frac{\partial ^{2} f}{\partial g_{3} \partial g_{2}}\right)}{cy_{1}\left[ c\frac{y_{1}}{1-y_{1}}\frac{\partial ^{2} f}{\partial g_{1} \partial g_{1}} +\frac{\partial ^{2} f}{\partial g_{1} \partial g_{2}} +\frac{\partial ^{2} f}{\partial g_{1} \partial g_{3}}\right] +\left[ c\frac{y_{1}}{1-y_{1}}\frac{\partial ^{2} f}{\partial g_{2} \partial g_{1}} +\frac{\partial ^{2} f}{\partial g_{2} \partial g_{2}} +\frac{\partial ^{2} f}{\partial g_{2} \partial g_{3}}\right] +c\frac{y_{1}}{1-y_{1}}\frac{\partial ^{2} f}{\partial g_{3} \partial g_{1}} +\frac{\partial ^{2} f}{\partial g_{3} \partial g_{2}} +\frac{\partial ^{2} f}{\partial g_{3} \partial g_{3}}}} \end{equation*} I am looking for conditions that may guarantee an easy characterization of the expression above when we increase $c$ . Clearly, differentiating the expression above with respect to $\displaystyle c$ yields a messy expression, especially due to the emergence of third order derivatives. Here are ny thoughts: The first-order effect given by $\frac{1}{x}\frac{\partial f}{\partial g_{1}}$ should dominate in most solutions, meaning that the derivative should have the sign of - $\frac{1}{x}\frac{\partial f}{\partial g_{1}}$ Second-order effects may matter for high values of $\displaystyle c$ (this is clear for $\displaystyle c\rightarrow \infty $ ), but this should not be \ a concern since I assume $\displaystyle c\leq 1$ . This means that in order for the first-order effect to be dominant I need a conition on the derivatives of my $\displaystyle f$ function. My intuition is that I need something like $\displaystyle f^{( n)}$ to be decrasing in $\displaystyle n$ . Clearly, there are some ""nice"" functions that do the trick, especially those in which the second-order derivative of $\displaystyle g_{1}$ is zero, but I was wondering if there could be a more general alternative. Derivations I can approach this by applying the implicit function theorem, I know that I can study the function: \begin{equation*} F( x,\mathbf{y}) =\sum _{k=1}^{3}\frac{\partial f}{\partial g_{k}}\frac{\partial g_{k}}{\partial x}( g_{1}( x) ,g_{2}( x) ,g_{3}( x) ,\mathbf{y}) =cy_{1}\frac{\partial f}{\partial g_{1}} +( 1-y_{1})\frac{\partial f}{\partial g_{2}} +\frac{\partial f}{\partial g_{3}} =0 \end{equation*} And that if, for $\displaystyle \mathbf{y}^{*}$ such that $\displaystyle x\left(\mathbf{y}^{*}\right) =x^{*}$ , and if $\frac{\partial F}{\partial x}\left( x^{*} ,\mathbf{y}^{*}\right) \neq 0$ , then \begin{equation*} \frac{dx}{dy_{1}}\left(\mathbf{y}^{*}\right) =-\frac{\frac{\partial F}{\partial y_{1}} [x(\mathbf{y}^{*} );\mathbf{y}^{*} ]}{\frac{\partial F}{\partial x} [x(\mathbf{y}^{*} );\mathbf{y}^{*} ]} =-\frac{\sum _{k=1}^{3}\left[\frac{\partial ^{2} f}{\partial g_{k} \partial y_{1}}\frac{\partial g_{k}}{\partial x} +\frac{\partial f}{\partial g_{k}}\frac{\partial ^{2} g_{k}}{\partial x\partial y_{1}}\right]\left( x^{*} ,\mathbf{y}^{*}\right)}{\sum _{k=1}^{3}\left[\frac{\partial ^{2} f}{\partial g_{k} \partial x}\frac{\partial g_{k}}{\partial x} +\frac{\partial f}{\partial g_{k}}\frac{\partial ^{2} g_{k}}{\partial x^{2}}\right]\left( x^{*} ,\mathbf{y}^{*}\right)} \end{equation*} In this case: \begin{gather*} {\textstyle \frac{d\ F}{d\ y_{1}} =c\frac{\partial f}{\partial g_{1}} -\frac{\partial f}{\partial g_{2}} +cy_{1}\left( cx\frac{\partial ^{2} f}{\partial g_{1} \partial g_{1}} -x\frac{\partial ^{2} f}{\partial g_{1} \partial g_{2}}\right) +( 1-y_{1})\left( cx\frac{\partial ^{2} f}{\partial g_{2} \partial g_{1}} -x\frac{\partial ^{2} f}{\partial g_{2} \partial g_{2}}\right) +\left( cx\frac{\partial ^{2} f}{\partial g_{3} \partial g_{1}} -x\frac{\partial ^{2} f}{\partial g_{3} \partial g_{2}}\right)}\\ \end{gather*} \begin{gather} {\textstyle \frac{d\ F}{d\ x} =cy_{1}\left[ cy_{1}\frac{\partial ^{2} f}{\partial g_{1} \partial g_{1}} +( 1-y_{1})\frac{\partial ^{2} f}{\partial g_{1} \partial g_{2}} +\frac{\partial ^{2} f}{\partial g_{1} \partial g_{3}}\right]}\\ {\textstyle +( 1-y_{1})\left[ cy_{1}\frac{\partial ^{2} f}{\partial g_{2} \partial g_{1}} +( 1-y_{1})\frac{\partial ^{2} f}{\partial g_{2} \partial g_{2}} +\frac{\partial ^{2} f}{\partial g_{2} \partial g_{3}}\right] +cy_{1}\frac{\partial ^{2} f}{\partial g_{3} \partial g_{1}} +( 1-y_{1})\frac{\partial ^{2} f}{\partial g_{3} \partial g_{2}} +\frac{\partial ^{2} f}{\partial g_{3} \partial g_{3}}} \notag \end{gather} And: \begin{equation*} {\textstyle \frac{dx}{dy_{1}}\left(\mathbf{y}^{*}\right) =-\frac{c\frac{\partial f}{\partial g_{1}} -\frac{\partial f}{\partial g_{2}} +cy_{1}\left( cx\frac{\partial ^{2} f}{\partial g_{1} \partial g_{1}} -x\frac{\partial ^{2} f}{\partial g_{1} \partial g_{2}}\right) +( 1-y_{1})\left( cx\frac{\partial ^{2} f}{\partial g_{2} \partial g_{1}} -x\frac{\partial ^{2} f}{\partial g_{2} \partial g_{2}}\right) +\left( cx\frac{\partial ^{2} f}{\partial g_{3} \partial g_{1}} -x\frac{\partial ^{2} f}{\partial g_{3} \partial g_{2}}\right)}{{\textstyle cy_{1}\left[ cy_{1}\frac{\partial ^{2} f}{\partial g_{1} \partial g_{1}} +( 1-y_{1})\frac{\partial ^{2} f}{\partial g_{1} \partial g_{2}} +\frac{\partial ^{2} f}{\partial g_{1} \partial g_{3}}\right] +( 1-y_{1})\left[ cy_{1}\frac{\partial ^{2} f}{\partial g_{2} \partial g_{1}} +( 1-y_{1})\frac{\partial ^{2} f}{\partial g_{2} \partial g_{2}} +\frac{\partial ^{2} f}{\partial g_{2} \partial g_{3}}\right] +cy_{1}\frac{\partial ^{2} f}{\partial g_{3} \partial g_{1}} +( 1-y_{1})\frac{\partial ^{2} f}{\partial g_{3} \partial g_{2}} +\frac{\partial ^{2} f}{\partial g_{3} \partial g_{3}}}}} \end{equation*}","I am working with a ""well-behaved"" optimization problem of the form: where is continously differentiable, , are parameters, and for are continuosly differentiable. I am interested in studying the assumptions I could make on and to study the ""comparative statics"" of the solution around an optimum. Say that there is a function such that: I am taking functional assumptions over the functions and study their implications, such as: The standard problem I am building on takes , so I am interested in studying what happens when I increase around the optimal solution. I am interested in elasticity notions, for instance: Using the Implicit function theorem (I include my derivations below), I find that is given by: I am looking for conditions that may guarantee an easy characterization of the expression above when we increase . Clearly, differentiating the expression above with respect to yields a messy expression, especially due to the emergence of third order derivatives. Here are ny thoughts: The first-order effect given by should dominate in most solutions, meaning that the derivative should have the sign of - Second-order effects may matter for high values of (this is clear for ), but this should not be \ a concern since I assume . This means that in order for the first-order effect to be dominant I need a conition on the derivatives of my function. My intuition is that I need something like to be decrasing in . Clearly, there are some ""nice"" functions that do the trick, especially those in which the second-order derivative of is zero, but I was wondering if there could be a more general alternative. Derivations I can approach this by applying the implicit function theorem, I know that I can study the function: And that if, for such that , and if , then In this case: And:","\begin{equation*}
\max_{x} f( g_{1}( x) ,g_{2}( x) ,g_{3}( x) ,\mathbf{y})
\end{equation*} \displaystyle f:\mathbb{R}^{n+3}\rightarrow \mathbb{R} \displaystyle x\in \mathbb{R} \displaystyle \mathbf{y} \in \mathbb{R}^{n} \displaystyle g_{k} :\mathbb{R}\rightarrow \mathbb{R} \displaystyle k=1,2,3\  \displaystyle f \displaystyle g_{k} \displaystyle x\left(\mathbf{y}^{*}\right) :\mathbb{R}^{n+1}\rightarrow \mathbb{R} \begin{equation*}
\sum _{k=1}^{3}\frac{\partial f}{\partial g_{k}}\frac{\partial g_{k}}{\partial x}\left( x\left(\mathbf{y}^{*}\right) ,\mathbf{y}^{*}\right) =0
\end{equation*} \displaystyle g_{k} \begin{gather*}
g_{1}( x) =cy_{1} x-cy_{2} ,\ c\in [ 0,1]\\
g_{2}( x) =( 1-y_{1}) x+y_{2}\\
g_{3}( x) =x
\end{gather*} \displaystyle c=0 \displaystyle c \begin{equation*}
{\textstyle \epsilon =\frac{dx}{dy_{1}}\left(\mathbf{y}^{*}\right)\frac{1-y_{1}}{x\left(\mathbf{y}^{*}\right)}}
\end{equation*} \displaystyle \epsilon  \begin{equation*}
{\textstyle \epsilon =-\frac{\frac{1}{x} c\left(\frac{\partial f}{\partial g_{1}} -\frac{\partial f}{\partial g_{2}}\right) +cy_{1}\left( c\frac{\partial ^{2} f}{\partial g_{1} \partial g_{1}} -\frac{\partial ^{2} f}{\partial g_{1} \partial g_{2}}\right) +( 1-y_{1})\left( c\frac{\partial ^{2} f}{\partial g_{2} \partial g_{1}} -\frac{\partial ^{2} f}{\partial g_{2} \partial g_{2}}\right) +\left( c\frac{\partial ^{2} f}{\partial g_{3} \partial g_{1}} -\frac{\partial ^{2} f}{\partial g_{3} \partial g_{2}}\right)}{cy_{1}\left[ c\frac{y_{1}}{1-y_{1}}\frac{\partial ^{2} f}{\partial g_{1} \partial g_{1}} +\frac{\partial ^{2} f}{\partial g_{1} \partial g_{2}} +\frac{\partial ^{2} f}{\partial g_{1} \partial g_{3}}\right] +\left[ c\frac{y_{1}}{1-y_{1}}\frac{\partial ^{2} f}{\partial g_{2} \partial g_{1}} +\frac{\partial ^{2} f}{\partial g_{2} \partial g_{2}} +\frac{\partial ^{2} f}{\partial g_{2} \partial g_{3}}\right] +c\frac{y_{1}}{1-y_{1}}\frac{\partial ^{2} f}{\partial g_{3} \partial g_{1}} +\frac{\partial ^{2} f}{\partial g_{3} \partial g_{2}} +\frac{\partial ^{2} f}{\partial g_{3} \partial g_{3}}}}
\end{equation*} c \displaystyle c \frac{1}{x}\frac{\partial f}{\partial g_{1}} \frac{1}{x}\frac{\partial f}{\partial g_{1}} \displaystyle c \displaystyle c\rightarrow \infty  \displaystyle c\leq 1 \displaystyle f \displaystyle f^{( n)} \displaystyle n \displaystyle g_{1} \begin{equation*}
F( x,\mathbf{y}) =\sum _{k=1}^{3}\frac{\partial f}{\partial g_{k}}\frac{\partial g_{k}}{\partial x}( g_{1}( x) ,g_{2}( x) ,g_{3}( x) ,\mathbf{y}) =cy_{1}\frac{\partial f}{\partial g_{1}} +( 1-y_{1})\frac{\partial f}{\partial g_{2}} +\frac{\partial f}{\partial g_{3}} =0
\end{equation*} \displaystyle \mathbf{y}^{*} \displaystyle x\left(\mathbf{y}^{*}\right) =x^{*} \frac{\partial F}{\partial x}\left( x^{*} ,\mathbf{y}^{*}\right) \neq 0 \begin{equation*}
\frac{dx}{dy_{1}}\left(\mathbf{y}^{*}\right) =-\frac{\frac{\partial F}{\partial y_{1}} [x(\mathbf{y}^{*} );\mathbf{y}^{*} ]}{\frac{\partial F}{\partial x} [x(\mathbf{y}^{*} );\mathbf{y}^{*} ]} =-\frac{\sum _{k=1}^{3}\left[\frac{\partial ^{2} f}{\partial g_{k} \partial y_{1}}\frac{\partial g_{k}}{\partial x} +\frac{\partial f}{\partial g_{k}}\frac{\partial ^{2} g_{k}}{\partial x\partial y_{1}}\right]\left( x^{*} ,\mathbf{y}^{*}\right)}{\sum _{k=1}^{3}\left[\frac{\partial ^{2} f}{\partial g_{k} \partial x}\frac{\partial g_{k}}{\partial x} +\frac{\partial f}{\partial g_{k}}\frac{\partial ^{2} g_{k}}{\partial x^{2}}\right]\left( x^{*} ,\mathbf{y}^{*}\right)}
\end{equation*} \begin{gather*}
{\textstyle \frac{d\ F}{d\ y_{1}} =c\frac{\partial f}{\partial g_{1}} -\frac{\partial f}{\partial g_{2}} +cy_{1}\left( cx\frac{\partial ^{2} f}{\partial g_{1} \partial g_{1}} -x\frac{\partial ^{2} f}{\partial g_{1} \partial g_{2}}\right) +( 1-y_{1})\left( cx\frac{\partial ^{2} f}{\partial g_{2} \partial g_{1}} -x\frac{\partial ^{2} f}{\partial g_{2} \partial g_{2}}\right) +\left( cx\frac{\partial ^{2} f}{\partial g_{3} \partial g_{1}} -x\frac{\partial ^{2} f}{\partial g_{3} \partial g_{2}}\right)}\\
\end{gather*} \begin{gather}
{\textstyle \frac{d\ F}{d\ x} =cy_{1}\left[ cy_{1}\frac{\partial ^{2} f}{\partial g_{1} \partial g_{1}} +( 1-y_{1})\frac{\partial ^{2} f}{\partial g_{1} \partial g_{2}} +\frac{\partial ^{2} f}{\partial g_{1} \partial g_{3}}\right]}\\
{\textstyle +( 1-y_{1})\left[ cy_{1}\frac{\partial ^{2} f}{\partial g_{2} \partial g_{1}} +( 1-y_{1})\frac{\partial ^{2} f}{\partial g_{2} \partial g_{2}} +\frac{\partial ^{2} f}{\partial g_{2} \partial g_{3}}\right] +cy_{1}\frac{\partial ^{2} f}{\partial g_{3} \partial g_{1}} +( 1-y_{1})\frac{\partial ^{2} f}{\partial g_{3} \partial g_{2}} +\frac{\partial ^{2} f}{\partial g_{3} \partial g_{3}}} \notag
\end{gather} \begin{equation*}
{\textstyle \frac{dx}{dy_{1}}\left(\mathbf{y}^{*}\right) =-\frac{c\frac{\partial f}{\partial g_{1}} -\frac{\partial f}{\partial g_{2}} +cy_{1}\left( cx\frac{\partial ^{2} f}{\partial g_{1} \partial g_{1}} -x\frac{\partial ^{2} f}{\partial g_{1} \partial g_{2}}\right) +( 1-y_{1})\left( cx\frac{\partial ^{2} f}{\partial g_{2} \partial g_{1}} -x\frac{\partial ^{2} f}{\partial g_{2} \partial g_{2}}\right) +\left( cx\frac{\partial ^{2} f}{\partial g_{3} \partial g_{1}} -x\frac{\partial ^{2} f}{\partial g_{3} \partial g_{2}}\right)}{{\textstyle cy_{1}\left[ cy_{1}\frac{\partial ^{2} f}{\partial g_{1} \partial g_{1}} +( 1-y_{1})\frac{\partial ^{2} f}{\partial g_{1} \partial g_{2}} +\frac{\partial ^{2} f}{\partial g_{1} \partial g_{3}}\right] +( 1-y_{1})\left[ cy_{1}\frac{\partial ^{2} f}{\partial g_{2} \partial g_{1}} +( 1-y_{1})\frac{\partial ^{2} f}{\partial g_{2} \partial g_{2}} +\frac{\partial ^{2} f}{\partial g_{2} \partial g_{3}}\right] +cy_{1}\frac{\partial ^{2} f}{\partial g_{3} \partial g_{1}} +( 1-y_{1})\frac{\partial ^{2} f}{\partial g_{3} \partial g_{2}} +\frac{\partial ^{2} f}{\partial g_{3} \partial g_{3}}}}}
\end{equation*}","['real-analysis', 'derivatives', 'implicit-function-theorem', 'implicit-function']"
68,Can you remove the poles from $\frac{f'(x)}{f(x) - y}$ with $L^\infty_y L^1_x$ error?,Can you remove the poles from  with  error?,\frac{f'(x)}{f(x) - y} L^\infty_y L^1_x,"I believe that the following below holds, which I need for some estimates. I have been struggling to prove it though! Let $f \in C^1(\mathbb{R}; \mathbb{R})$ have finitely many critical points and be generally a rather nice function (for example a real power of a polynomial). I might need more assumptions but I could not nail it down yet. Let $y \in \mathbb{R}$ and $x_1 < \dots < x_n$ be all the solutions to $f(x) = y$ . I claim that there exist $a_j(y), b \in \mathbb{R}$ such that $$ \frac{f'(x)}{f(x) - y} = \frac{b}{1 + |x|} + \sum_{j=1}^n \frac{a_j(y)}{x - x_j} + R(y,x) $$ where $\sup_{y \in \mathbb{R}, 1 \leq j \leq n} |a_j(y)| < \infty$ (actually trivial) and $$ \sup_{y \in \mathbb{R}} \int_{\mathbb{R}} |R(y, x)| dx < \infty \,. $$ The idea is simple and I can illustrate what is supposed to happend for the case where $y = 0$ and $\phi(x) = y$ has only one solution $x_1 = 0$ . Let's say we assume that both $\frac{x f'(x)}{f(x)}$ as well as $\frac{f'(\frac{1}{x})}{f(\frac{1}{x})}$ are differentiable at $0$ with value $0$ . This means we can make the following Taylor expansions: $$ \frac{x f'(x)}{f(x)} = a_1 + a_2 x + R_0(x) x \\ \frac{f'(\frac{1}{x})}{f(\frac{1}{x})} = 0 + b_1 x + b_2 x^2 + R_\infty\left(\frac{1}{x}\right) x^2 $$ which implies $$ \frac{f'(x)}{f(x)} - \frac{a_1}{x} = a_2 + R_0(x) \\ \frac{f'(x)}{f(x)} - \frac{b_1}{x} = \frac{b_2}{x^2} + R_\infty(x) \frac{1}{x^2} $$ where due to Taylor's theorem the remainder terms $R_0(x)$ and $R_\infty\left(\frac{1}{x}\right)$ are continuous at $0$ and $\infty$ respectively, and hence $R_0 \in C(B_1(0))$ and $R_\infty \in C(\mathbb{R} \setminus B_1(0))$ . Basically, we can understand what happens near the pole and near infinity, and in both regimes cancel out the $\frac{1}{x}$ order. I have of course tried to apply this technique to the general case but struggled with the following issues: I need to estimate my remainder terms uniformly in $y$ , and I have not been able to get a grip on that with the various forms. I always have to deal with the function $\frac{(x - x_j) f'(x)}{f(x) - y}$ . If $f$ is differentiable then the limit $x \rightarrow x_j$ exists, and it is $1$ if $f'(x_j) = 0$ . More generally it seems to be the ""order"" of vanishing of the equation $f(x_j) - y = 0$ . Remark: I have posted another question about this function and I am about to answer it myself. We can understand its derivatives around $y$ , and how exactly they blow up near a critical point. When $x_0$ is near a critical point of $f$ the behavior of the above function becomes bad, related to the fact that two poles are merging into one. I have a feeling that there should be some known theory regarding this. Removing the poles from your function should yield you something continuous, and removing the decay at infinity something integrable. Furthermore, why should this not work uniformly in $y$ ? Lastly I would like to remark that I am trying to not only find the proof but also the lemma here. That is, if you find a counterexample to what I have exactly formulated, it indicates that further assumptions are needed but it might not fully answer my question.","I believe that the following below holds, which I need for some estimates. I have been struggling to prove it though! Let have finitely many critical points and be generally a rather nice function (for example a real power of a polynomial). I might need more assumptions but I could not nail it down yet. Let and be all the solutions to . I claim that there exist such that where (actually trivial) and The idea is simple and I can illustrate what is supposed to happend for the case where and has only one solution . Let's say we assume that both as well as are differentiable at with value . This means we can make the following Taylor expansions: which implies where due to Taylor's theorem the remainder terms and are continuous at and respectively, and hence and . Basically, we can understand what happens near the pole and near infinity, and in both regimes cancel out the order. I have of course tried to apply this technique to the general case but struggled with the following issues: I need to estimate my remainder terms uniformly in , and I have not been able to get a grip on that with the various forms. I always have to deal with the function . If is differentiable then the limit exists, and it is if . More generally it seems to be the ""order"" of vanishing of the equation . Remark: I have posted another question about this function and I am about to answer it myself. We can understand its derivatives around , and how exactly they blow up near a critical point. When is near a critical point of the behavior of the above function becomes bad, related to the fact that two poles are merging into one. I have a feeling that there should be some known theory regarding this. Removing the poles from your function should yield you something continuous, and removing the decay at infinity something integrable. Furthermore, why should this not work uniformly in ? Lastly I would like to remark that I am trying to not only find the proof but also the lemma here. That is, if you find a counterexample to what I have exactly formulated, it indicates that further assumptions are needed but it might not fully answer my question.","f \in C^1(\mathbb{R}; \mathbb{R}) y \in \mathbb{R} x_1 < \dots < x_n f(x) = y a_j(y), b \in \mathbb{R} 
\frac{f'(x)}{f(x) - y} = \frac{b}{1 + |x|} + \sum_{j=1}^n \frac{a_j(y)}{x - x_j} + R(y,x)
 \sup_{y \in \mathbb{R}, 1 \leq j \leq n} |a_j(y)| < \infty 
\sup_{y \in \mathbb{R}} \int_{\mathbb{R}} |R(y, x)| dx < \infty \,.
 y = 0 \phi(x) = y x_1 = 0 \frac{x f'(x)}{f(x)} \frac{f'(\frac{1}{x})}{f(\frac{1}{x})} 0 0 
\frac{x f'(x)}{f(x)} = a_1 + a_2 x + R_0(x) x \\
\frac{f'(\frac{1}{x})}{f(\frac{1}{x})} = 0 + b_1 x + b_2 x^2 + R_\infty\left(\frac{1}{x}\right) x^2
 
\frac{f'(x)}{f(x)} - \frac{a_1}{x} = a_2 + R_0(x) \\
\frac{f'(x)}{f(x)} - \frac{b_1}{x} = \frac{b_2}{x^2} + R_\infty(x) \frac{1}{x^2}
 R_0(x) R_\infty\left(\frac{1}{x}\right) 0 \infty R_0 \in C(B_1(0)) R_\infty \in C(\mathbb{R} \setminus B_1(0)) \frac{1}{x} y \frac{(x - x_j) f'(x)}{f(x) - y} f x \rightarrow x_j 1 f'(x_j) = 0 f(x_j) - y = 0 y x_0 f y","['real-analysis', 'complex-analysis', 'derivatives', 'taylor-expansion', 'laurent-series']"
69,Integer partitions appearing in the derivatives of $f(x) \log\left(f(x)\right)$,Integer partitions appearing in the derivatives of,f(x) \log\left(f(x)\right),"This topic has probably been already investigated before, if not, it suggests an interesting connection between calculus and number theory , in particular integer partitions . Consider the function defined as $F(x) = f(x)\log\left(f(x)\right)$ , where $\log$ is the natural logarithm. Now, take a look at its derivatives: $$ \begin{align}  F^{(1)}&=f'\log f+f' \\  F^{(2)}&=f''\log f+f''+\frac{\left(f'\right)^2}{f} \\   F^{(3)}&=f^{(3)}\log f+f^{(3)}-\frac{\left(f'\right)^3}{f^2}+3\frac{f'f''}{f} \\  F^{(4)}&=f^{(4)}\log f+f^{(4)}+2\frac{\left(f'\right)^4}            {f^3}-6\frac{\left(f'\right)^2f''}{f^2}+3\frac{\left(f''\right)^2}            {f}+4\frac{f^{(3)}f'}{f} \\  F^{(5)}&=f^{(5)}\log f+f^{(5)}-6\frac{\left(f'\right)^5}{f^4}+5\frac{f^{(4)}f'}{f}+10\frac{f^{(3)}f''}{f}-10\frac{f^{(3)}\left(f'\right)^2}{f^2}+20\frac{\left(f'\right)^3f''}{f^3}-15\frac{f'\left(f''\right)^2}{f^2} \\ \vdots \end{align} $$ As you can see, if we ignore the first term, where the logarithm appears, and the denominators of the fractions, for example by supposing there is an $x_0$ s.t. $f(x_0)=1$ , then it seems that $F^{(n)}(x_0)$ has terms corresponding exactly to the partitions of $n$ . To see this, just sum up the orders of the derivatives that appear in each term with multiplicity given by their exponent, it will always result in exactly the order of the derivative you are looking at. Moreover, this ""correspondence"" appears to be a ""bijection"", in the sense that $$\text{If $(\lambda_1,\lambda_2,\dots,\lambda_k)$ is a partition of $n$, then it appears in the expansion of $F^{(n)}(x_0)$} $$ All of these are still conjectures. I was only able to check this up to $n=10$ since Wolfram won't go any further in calculating the derivatives, but it seems true up to that point. If the result holds, then, by referring with $p(n)=$ to the partition function, in other words the number of distinct partitions of $n$ $\left| F(x) \right|$ to the ""number of terms"" in the expansion of $F$ (not very clear as a definition but it's the best I have) Then the following should hold: $$p(n) = \left|F^{(n)}(x_0)\right|$$ Now, to make it more rigorous, is the following conjecture true? If $F(x)=f(x)\log\left(f(x)\right)$ , and $f(x_0)=1$ , then $$ F^{(n)}(x_0) = \sum_{(\lambda_1,\dots,\lambda_k) \text{partition of $n$}} C(\lambda_1,\dots,\lambda_k)\cdot f^{(\lambda_1)}(x_0)\cdot\dots\cdot f^{(\lambda_k)}(x_0) $$ where the coefficients $C(\lambda_1,\dots,\lambda_k)$ are non-zero integers. I believe it is true, but can't see a way to prove it. Either there is something very obvious about how the terms in every derivative generate from the terms in the previous one and I'm missing it, or there is something more interesting going on. Now, let us focus on the coefficients $C(\lambda_1,\dots,\lambda_k)$ , since they follow many non-trivial patterns. In a way, $C(\lambda_1,\dots,\lambda_k)$ can be interpreted as some kind of ""weight"" of the partition $(\lambda_1,\dots,\lambda_k)$ . Unfortunately their value seems to be far from obvious, altough in some cases it seems to follow some patterns that I will show, but first, some notation: I will denote the partition $n=\underbrace{a+\dots+a}_{c_a}+\underbrace{b+\dots+b}_{c_b}+\dots+\underbrace{m+\dots+m}_{c_m}$ as $(a\cdot c_a,b\cdot c_b,\dots,m\cdot c_m)$ Now, for a fixed $n\in\mathbb{N}$ , some of the regularities that I spotted in the weights are the following: $C(n)=1$ $C(1\cdot(n-k),k)=\frac{(-1)^{n-k-1}n!}{(n-k)k!}$ for $k$ up to $n-1$ $C(n-k,k)=\binom{n}{k}$ EXCEPT when $n-k=n/2$ , in that case this coefficient fails if $n=2k$ , then $C(k\cdot2)=\frac12 \frac{(2k)!}{(k!)^2}$ if $n=3k$ , then $C(k\cdot3)=-\frac16 \frac{(3k)!}{(k!)^3}$ if $n=\alpha\cdot k$ , $\alpha\ge 4$ , then $C(k\cdot\alpha)$ appears to be of the form $\frac{1}{c_{\alpha}}\frac{(\alpha k)!}{(k!)^{\alpha}}$ for some $c_{\alpha}\in\mathbb{N}$ As you can see, this are all pretty ""easy"" partitions, in the sense that they either have only two components or most of the components are the same. I couldn't find any regularity for more complicated partitions, and part of the reason was the number of partitions of $n$ grows exponentially as $\sqrt n$ , so there are many cases to check. However, I am interested in knowing if some kind of closed expression for $C(\lambda_1,\dots,\lambda_k)$ exists, so the second question is: Given a partition of $n$ of the form $(\lambda_1,\dots,\lambda_k)$ , is there a way to determine the value of its ""weight"" $C(\lambda_1,\dots,\lambda_k)$ , defined as the coefficient of the partition in the expansion of $F^{(n)}(x_0)$ ? As an interesting final remark, notice that, if we set $f(x)=e^x$ , and $x_0=0$ , then, expanding $xe^x$ as a Taylor series, and assuming the conjecture holds true, we have that, for all $n\in\mathbb{N}$ , $$\sum_{(\lambda_1,\dots,\lambda_k) \text{partition of $n$}} C(\lambda_1,\dots,\lambda_k) = n $$ which I think is quite nice.","This topic has probably been already investigated before, if not, it suggests an interesting connection between calculus and number theory , in particular integer partitions . Consider the function defined as , where is the natural logarithm. Now, take a look at its derivatives: As you can see, if we ignore the first term, where the logarithm appears, and the denominators of the fractions, for example by supposing there is an s.t. , then it seems that has terms corresponding exactly to the partitions of . To see this, just sum up the orders of the derivatives that appear in each term with multiplicity given by their exponent, it will always result in exactly the order of the derivative you are looking at. Moreover, this ""correspondence"" appears to be a ""bijection"", in the sense that All of these are still conjectures. I was only able to check this up to since Wolfram won't go any further in calculating the derivatives, but it seems true up to that point. If the result holds, then, by referring with to the partition function, in other words the number of distinct partitions of to the ""number of terms"" in the expansion of (not very clear as a definition but it's the best I have) Then the following should hold: Now, to make it more rigorous, is the following conjecture true? If , and , then where the coefficients are non-zero integers. I believe it is true, but can't see a way to prove it. Either there is something very obvious about how the terms in every derivative generate from the terms in the previous one and I'm missing it, or there is something more interesting going on. Now, let us focus on the coefficients , since they follow many non-trivial patterns. In a way, can be interpreted as some kind of ""weight"" of the partition . Unfortunately their value seems to be far from obvious, altough in some cases it seems to follow some patterns that I will show, but first, some notation: I will denote the partition as Now, for a fixed , some of the regularities that I spotted in the weights are the following: for up to EXCEPT when , in that case this coefficient fails if , then if , then if , , then appears to be of the form for some As you can see, this are all pretty ""easy"" partitions, in the sense that they either have only two components or most of the components are the same. I couldn't find any regularity for more complicated partitions, and part of the reason was the number of partitions of grows exponentially as , so there are many cases to check. However, I am interested in knowing if some kind of closed expression for exists, so the second question is: Given a partition of of the form , is there a way to determine the value of its ""weight"" , defined as the coefficient of the partition in the expansion of ? As an interesting final remark, notice that, if we set , and , then, expanding as a Taylor series, and assuming the conjecture holds true, we have that, for all , which I think is quite nice.","F(x) = f(x)\log\left(f(x)\right) \log 
\begin{align}
 F^{(1)}&=f'\log f+f' \\
 F^{(2)}&=f''\log f+f''+\frac{\left(f'\right)^2}{f} \\ 
 F^{(3)}&=f^{(3)}\log f+f^{(3)}-\frac{\left(f'\right)^3}{f^2}+3\frac{f'f''}{f} \\
 F^{(4)}&=f^{(4)}\log f+f^{(4)}+2\frac{\left(f'\right)^4} 
          {f^3}-6\frac{\left(f'\right)^2f''}{f^2}+3\frac{\left(f''\right)^2} 
          {f}+4\frac{f^{(3)}f'}{f} \\
 F^{(5)}&=f^{(5)}\log f+f^{(5)}-6\frac{\left(f'\right)^5}{f^4}+5\frac{f^{(4)}f'}{f}+10\frac{f^{(3)}f''}{f}-10\frac{f^{(3)}\left(f'\right)^2}{f^2}+20\frac{\left(f'\right)^3f''}{f^3}-15\frac{f'\left(f''\right)^2}{f^2} \\
\vdots
\end{align}
 x_0 f(x_0)=1 F^{(n)}(x_0) n \text{If (\lambda_1,\lambda_2,\dots,\lambda_k) is a partition of n, then it appears in the expansion of F^{(n)}(x_0)}  n=10 p(n)= n \left| F(x) \right| F p(n) = \left|F^{(n)}(x_0)\right| F(x)=f(x)\log\left(f(x)\right) f(x_0)=1  F^{(n)}(x_0) = \sum_{(\lambda_1,\dots,\lambda_k) \text{partition of n}} C(\lambda_1,\dots,\lambda_k)\cdot f^{(\lambda_1)}(x_0)\cdot\dots\cdot f^{(\lambda_k)}(x_0)  C(\lambda_1,\dots,\lambda_k) C(\lambda_1,\dots,\lambda_k) C(\lambda_1,\dots,\lambda_k) (\lambda_1,\dots,\lambda_k) n=\underbrace{a+\dots+a}_{c_a}+\underbrace{b+\dots+b}_{c_b}+\dots+\underbrace{m+\dots+m}_{c_m} (a\cdot c_a,b\cdot c_b,\dots,m\cdot c_m) n\in\mathbb{N} C(n)=1 C(1\cdot(n-k),k)=\frac{(-1)^{n-k-1}n!}{(n-k)k!} k n-1 C(n-k,k)=\binom{n}{k} n-k=n/2 n=2k C(k\cdot2)=\frac12 \frac{(2k)!}{(k!)^2} n=3k C(k\cdot3)=-\frac16 \frac{(3k)!}{(k!)^3} n=\alpha\cdot k \alpha\ge 4 C(k\cdot\alpha) \frac{1}{c_{\alpha}}\frac{(\alpha k)!}{(k!)^{\alpha}} c_{\alpha}\in\mathbb{N} n \sqrt n C(\lambda_1,\dots,\lambda_k) n (\lambda_1,\dots,\lambda_k) C(\lambda_1,\dots,\lambda_k) F^{(n)}(x_0) f(x)=e^x x_0=0 xe^x n\in\mathbb{N} \sum_{(\lambda_1,\dots,\lambda_k) \text{partition of n}} C(\lambda_1,\dots,\lambda_k) = n ","['real-analysis', 'calculus', 'sequences-and-series', 'number-theory', 'derivatives']"
70,"""Paradox"" on the equations of envelope of a family of curves","""Paradox"" on the equations of envelope of a family of curves",,"So I was computing the envelope of the family of lines $x \cos{C} + y \sin{C} = p$ , where $p$ is a constant and $C$ is the parameter that defines the family. According to the normal procedure, we would differentiate the equation with respect to $C$ and get $-x \sin{C} + y \cos{C} =0$ , and we would have a system of equations that when solved would give us the circumference of radius $p$ centered at the origin. My problem is this: suppose we tried to differentiate with respect to $C$ again; we would have $-x \cos{C} - y\sin{C}=0$ . But this is just the symmetric of the first equation, so combining them we would have $p=0$ . Now this conclusion obviously makes no sense, so what is the problem with differentiating the equation twice?","So I was computing the envelope of the family of lines , where is a constant and is the parameter that defines the family. According to the normal procedure, we would differentiate the equation with respect to and get , and we would have a system of equations that when solved would give us the circumference of radius centered at the origin. My problem is this: suppose we tried to differentiate with respect to again; we would have . But this is just the symmetric of the first equation, so combining them we would have . Now this conclusion obviously makes no sense, so what is the problem with differentiating the equation twice?",x \cos{C} + y \sin{C} = p p C C -x \sin{C} + y \cos{C} =0 p C -x \cos{C} - y\sin{C}=0 p=0,"['derivatives', 'envelope']"
71,Calculating the functional derivative of an implicit functional,Calculating the functional derivative of an implicit functional,,"We want to calculate the functional derivative of the following wrt $\rho$ : $$F=\int X dx$$ where $X$ is implicitly defined as: $$X=\frac{1}{1+\overline \rho(x) \int \rho(x) X dx}$$ and $\overline \rho$ is the result of the following convolution: $$\overline \rho(x) = \int \rho(x^\prime)H(\sigma^2 - (x-x^\prime)^2)dx^\prime$$ where $H$ is the Heaviside step function: $$\begin{cases}  1,& x\geq 0\\  0,& x < 0\end{cases}$$ How can I calculate the functional derivative $\frac{\delta F}{\delta \rho}$ analytically? Note that I can solve this numerically by solving iteratively for X and then computing the functional derivative numerically. But I'm looking for the analytical expression.",We want to calculate the functional derivative of the following wrt : where is implicitly defined as: and is the result of the following convolution: where is the Heaviside step function: How can I calculate the functional derivative analytically? Note that I can solve this numerically by solving iteratively for X and then computing the functional derivative numerically. But I'm looking for the analytical expression.,"\rho F=\int X dx X X=\frac{1}{1+\overline \rho(x) \int \rho(x) X dx} \overline \rho \overline \rho(x) = \int \rho(x^\prime)H(\sigma^2 - (x-x^\prime)^2)dx^\prime H \begin{cases}
 1,& x\geq 0\\
 0,& x < 0\end{cases} \frac{\delta F}{\delta \rho}","['functional-analysis', 'derivatives', 'functional-equations', 'calculus-of-variations', 'implicit-differentiation']"
72,Verifying a combinatorics identity with rth derivative of this function,Verifying a combinatorics identity with rth derivative of this function,,"Problem 14 of the chapter IV of the Feller's book ""An introduction to probability"" reads as follows: From the result of problem 12 conclude that $$\sum_{k=0}^n (-1)^k{n \choose k}{(ns-ks)}_r = 0\qquad\qquad(1)$$ if r<n and for r=n $$\sum_{k=0}^n (-1)^k{n \choose k}{(ns-ks)}_n ={s^n}{n!}. \qquad\qquad(2)$$ Verify this by evaluating the r th derivative, at x=0, of $$\frac{1}{{(1-x)}^{ns-r+1}}{[1-{(1-x)}^s]}^n\qquad\qquad(3)$$ The problem 12 that was mentioned in the problem description, is a particular version of coupon collector's problem: A pack of cards consisting of s identical series, each containing n cards numbered 1,2,...,n . A random sample of $r≥n$ cards is drawn from the pack without replacement. Calculate the probability $u_r$ that each number is represented in the sample. The answer to this latter problem is $$u_r=\sum_{k=0}^n (-1)^k{n \choose k}\frac{{(ns-ks)}_r}{(ns)_r}\qquad\qquad(4)  $$ which is not so hard to arrive at. Also with this answer we can show that (1) and (2) hold. But my problem is with the verification with the r th derivative of (3). With direct differentiation, I calculated the below as the r th derivative at x=0: $$\sum_{k=0}^r {r \choose k}{\frac{(ns-k)!}{(ns-r)!}}\sum_{\sum_{}{i_j}=k}{(-1)}^{k+n}{k \choose {}i_1,i_2,...,i_n} [\prod_{j=1}^n \frac{s!}{({s-{i_j}})!}]A(I) \qquad\qquad(5) $$ where A(I) is a function of the n-tuple $I=(i_1,i_2,....,i_n)$ and is equal to zero if at least one of the elements of I is zero, otherwise A(I) is one (I used multinomial theorem ) Now it is easy to verify that (5) become $s^n{n!}$ if r=n and zero for r<n , but still I can't see how the cumbersome summation (5) can be simplified to be something as straightforward as (1). I expected to be able to reproduce the summation (1) by differentiating (3); so how one can arrive at (1) by (5) or by differentiating (3)? Thank you all.","Problem 14 of the chapter IV of the Feller's book ""An introduction to probability"" reads as follows: From the result of problem 12 conclude that if r<n and for r=n Verify this by evaluating the r th derivative, at x=0, of The problem 12 that was mentioned in the problem description, is a particular version of coupon collector's problem: A pack of cards consisting of s identical series, each containing n cards numbered 1,2,...,n . A random sample of cards is drawn from the pack without replacement. Calculate the probability that each number is represented in the sample. The answer to this latter problem is which is not so hard to arrive at. Also with this answer we can show that (1) and (2) hold. But my problem is with the verification with the r th derivative of (3). With direct differentiation, I calculated the below as the r th derivative at x=0: where A(I) is a function of the n-tuple and is equal to zero if at least one of the elements of I is zero, otherwise A(I) is one (I used multinomial theorem ) Now it is easy to verify that (5) become if r=n and zero for r<n , but still I can't see how the cumbersome summation (5) can be simplified to be something as straightforward as (1). I expected to be able to reproduce the summation (1) by differentiating (3); so how one can arrive at (1) by (5) or by differentiating (3)? Thank you all.","\sum_{k=0}^n (-1)^k{n \choose k}{(ns-ks)}_r = 0\qquad\qquad(1) \sum_{k=0}^n (-1)^k{n \choose k}{(ns-ks)}_n ={s^n}{n!}. \qquad\qquad(2) \frac{1}{{(1-x)}^{ns-r+1}}{[1-{(1-x)}^s]}^n\qquad\qquad(3) r≥n u_r u_r=\sum_{k=0}^n (-1)^k{n \choose k}\frac{{(ns-ks)}_r}{(ns)_r}\qquad\qquad(4)   \sum_{k=0}^r {r \choose k}{\frac{(ns-k)!}{(ns-r)!}}\sum_{\sum_{}{i_j}=k}{(-1)}^{k+n}{k \choose {}i_1,i_2,...,i_n} [\prod_{j=1}^n \frac{s!}{({s-{i_j}})!}]A(I) \qquad\qquad(5)  I=(i_1,i_2,....,i_n) s^n{n!}","['combinatorics', 'derivatives']"
73,I can't solve this derivative problem which contains $\cos x$ and $\sin x$,I can't solve this derivative problem which contains  and,\cos x \sin x,"I want to calculate the derivative of the following function $$ y = {(\cos x - 1)}^{\sec x - 1}$$ I searched and found a video on YouTube which started solving this by applying natural logarithm in both sides. $$ \ln (y) = \ln {(\cos x - 1)^{\sec x - 1}}$$ $$\Rightarrow \frac{y'}{y} = (\sec x - 1).[\ln(\cos x - 1)]$$ Here is the part I do not understand. Since we used natural logarithm we must have $$\cos x - 1> 0 \Rightarrow \cos x > 1$$ For logarithm to be well defined, however it is impossible for any x. So, I don't know whether the video from YouTube made a mistake or whether my reasoning has some flaw I can't find. Please help me understand the mistake and calculate the derivative properly! Thanks in advance!","I want to calculate the derivative of the following function I searched and found a video on YouTube which started solving this by applying natural logarithm in both sides. Here is the part I do not understand. Since we used natural logarithm we must have For logarithm to be well defined, however it is impossible for any x. So, I don't know whether the video from YouTube made a mistake or whether my reasoning has some flaw I can't find. Please help me understand the mistake and calculate the derivative properly! Thanks in advance!", y = {(\cos x - 1)}^{\sec x - 1}  \ln (y) = \ln {(\cos x - 1)^{\sec x - 1}} \Rightarrow \frac{y'}{y} = (\sec x - 1).[\ln(\cos x - 1)] \cos x - 1> 0 \Rightarrow \cos x > 1,['derivatives']
74,Let $f$ be a differentiable function with no point $x$ such that $f(x)=0=f'(x)$ show that $f$ has finitely many zeros.,Let  be a differentiable function with no point  such that  show that  has finitely many zeros.,f x f(x)=0=f'(x) f,"Let $f: [0, 1] \rightarrow R$ be a differentiable function. Assume there is no point $x$ in $[0,1]$ such that $f(x) = 0 = f'(x)$ . Show that $f$ has only a finite number of zeros in $[0, 1]$ . My proof. Assume otherwise. Keep bisecting the interval choosing the subinterval with infinitely many zeros. (This is fairly. standard so I won't go into it). We obtain $(x_n)$ such that $f(x_n)=0$ for all $n$ . Moreover, $x_n\rightarrow x$ as $n\rightarrow \infty$ . We see immediately that $f(x)=0$ . Our goal is to show $f'(x)=0$ as well. We know, $$ \lim_{h\rightarrow 0}\dfrac{f(x+h)}{h}=L $$ There's a subsequence $x_{n_k}$ , $h_k = x_{n_k}-x\ge 0$ , (if not we will use $x-x_{n_k}$ and the proof will be similar) and we observe for every $h$ , there's $N$ such that if $k\ge N$ , $h_k=x_{n_k}-x\le h$ . Thus we observe, $$ \lim_{k\rightarrow \infty }\dfrac{f(x+h_k)}{h_k}=L=0 $$ The last part is due to the fact $f(x+h_k)=0$ . Contradiction! I am only looking for proof verification. $\textbf{Please only provide hints if my proof is wrong. Complete solutions won't benefit me at all!}$","Let be a differentiable function. Assume there is no point in such that . Show that has only a finite number of zeros in . My proof. Assume otherwise. Keep bisecting the interval choosing the subinterval with infinitely many zeros. (This is fairly. standard so I won't go into it). We obtain such that for all . Moreover, as . We see immediately that . Our goal is to show as well. We know, There's a subsequence , , (if not we will use and the proof will be similar) and we observe for every , there's such that if , . Thus we observe, The last part is due to the fact . Contradiction! I am only looking for proof verification.","f: [0, 1] \rightarrow R x [0,1] f(x) = 0 = f'(x) f [0, 1] (x_n) f(x_n)=0 n x_n\rightarrow x n\rightarrow \infty f(x)=0 f'(x)=0 
\lim_{h\rightarrow 0}\dfrac{f(x+h)}{h}=L
 x_{n_k} h_k = x_{n_k}-x\ge 0 x-x_{n_k} h N k\ge N h_k=x_{n_k}-x\le h 
\lim_{k\rightarrow \infty }\dfrac{f(x+h_k)}{h_k}=L=0
 f(x+h_k)=0 \textbf{Please only provide hints if my proof is wrong. Complete solutions won't benefit me at all!}",['derivatives']
75,"If $\sum_ix^i\partial_if=0$, then $f$ is constant?","If , then  is constant?",\sum_ix^i\partial_if=0 f,"Let $f$ be a real-valued function defined on a neighborhood $D$ of $0\in\mathbb R^n$ - is it true that $\sum_ix^i\partial_if=0$ implies that $f$ is constant? Intuitively, I'd say yes: Consider the radial vector field $R(x):=x$ , then $\sum_i x^i\partial_if=\mathrm df(R)$ and we get that $f$ is constant on each ray emanating from the origin. Because of continuity, $f$ should be constant everywhere. Is my reasoning correct? What properties does $D$ need to have such that everything works out? (My guess would be that $D$ needs to be star-shaped w.r.t. the origin.) Motivation: As far as I understand, this is used in the proof of Lemma $4.13$ of Heat Kernels and Dirac Operators .","Let be a real-valued function defined on a neighborhood of - is it true that implies that is constant? Intuitively, I'd say yes: Consider the radial vector field , then and we get that is constant on each ray emanating from the origin. Because of continuity, should be constant everywhere. Is my reasoning correct? What properties does need to have such that everything works out? (My guess would be that needs to be star-shaped w.r.t. the origin.) Motivation: As far as I understand, this is used in the proof of Lemma of Heat Kernels and Dirac Operators .",f D 0\in\mathbb R^n \sum_ix^i\partial_if=0 f R(x):=x \sum_i x^i\partial_if=\mathrm df(R) f f D D 4.13,"['derivatives', 'partial-derivative', 'differential']"
76,Series representation of $n$th derivative of $x^n/(1+x^2)$,Series representation of th derivative of,n x^n/(1+x^2),"Find the nth derivative of $\frac{x^n}{1+x^2}$ . Please I need help in this. They are further asking to show that when $x=\cot y$ the nth derivative can be expressed as $$n!\sin y\sum_{r=0}^{n}(-1)^r {n\choose r}\cos^r y\sin[(r+1)y]$$ My attempt Since the function inside can be expressed as $P(x)-\frac{x}{1+x^2}$ or $P(x)-\frac{1}{1+x^2}$ with $\deg(P)=n-2$ as per $n$ is odd or even, I tried to write the nth derivative of the function as $(-1)^n n! \cos(n+1)y \sin^{n+1} y$ or $(-1)^n n! \sin(n+1)y \sin^{n+1} y$ as and when n is odd or even and then tried induction to prove this equal to the R.H.S , obviously cancelling out the $n!$ . But at that induction step I am getting entangled. Though i showed this holds for n=1,2.","Find the nth derivative of . Please I need help in this. They are further asking to show that when the nth derivative can be expressed as My attempt Since the function inside can be expressed as or with as per is odd or even, I tried to write the nth derivative of the function as or as and when n is odd or even and then tried induction to prove this equal to the R.H.S , obviously cancelling out the . But at that induction step I am getting entangled. Though i showed this holds for n=1,2.",\frac{x^n}{1+x^2} x=\cot y n!\sin y\sum_{r=0}^{n}(-1)^r {n\choose r}\cos^r y\sin[(r+1)y] P(x)-\frac{x}{1+x^2} P(x)-\frac{1}{1+x^2} \deg(P)=n-2 n (-1)^n n! \cos(n+1)y \sin^{n+1} y (-1)^n n! \sin(n+1)y \sin^{n+1} y n!,"['real-analysis', 'calculus', 'derivatives', 'trigonometry', 'summation']"
77,"Let $f(x)\in C^{1}(\mathbb{R}),\ f^{'}(x)>f(f(x)),\ \forall x\in\mathbb{R}. $ Prove: $f(f(f(x))))\le 0,\ \forall x\ge0.$",Let  Prove:,"f(x)\in C^{1}(\mathbb{R}),\ f^{'}(x)>f(f(x)),\ \forall x\in\mathbb{R}.  f(f(f(x))))\le 0,\ \forall x\ge0.","Let $f(x)\in C^{1}(\mathbb{R}),\ f^{'}(x)>f(f(x)),\ \forall x\in\mathbb{R}. $ Prove: $f(f(f(x))))\le 0,\ \forall x\ge0.$ I have just proved $f$ is bonuded. In fact, if $\displaystyle\lim_{x\to +\infty}f(x)=+\infty,\ $ when $x$ is sufficiently big, $$f(x)>1+\epsilon>1,\ f^{'}(x)>f(f(x))>1+\epsilon$$ so that when x is more sufficiently big, we have $$f(x)>x,\ f^{'}(x)>f(f(x))>f(x),\ e^{-x}f(x)>c>0,\ f^{'}>ce^{f(x)}$$ than $$+\infty>\int_{x_0}^{+\infty}\frac{f^{'}(x)}{e^{cf(x)}}=+\infty$$ that's a contradiction! But then I ran out of ideas.","Let Prove: I have just proved is bonuded. In fact, if when is sufficiently big, so that when x is more sufficiently big, we have than that's a contradiction! But then I ran out of ideas.","f(x)\in C^{1}(\mathbb{R}),\ f^{'}(x)>f(f(x)),\ \forall x\in\mathbb{R}.  f(f(f(x))))\le 0,\ \forall x\ge0. f \displaystyle\lim_{x\to +\infty}f(x)=+\infty,\  x f(x)>1+\epsilon>1,\ f^{'}(x)>f(f(x))>1+\epsilon f(x)>x,\ f^{'}(x)>f(f(x))>f(x),\ e^{-x}f(x)>c>0,\ f^{'}>ce^{f(x)} +\infty>\int_{x_0}^{+\infty}\frac{f^{'}(x)}{e^{cf(x)}}=+\infty","['derivatives', 'inequality']"
78,Is the symmetric definition of the derivative equivalent?,Is the symmetric definition of the derivative equivalent?,,"Is the symmetric definition of the derivative (below) equivalent to the usual one? \begin{equation} \lim_{h\to0}\frac{f(x+h)-f(x-h)}{2h} \end{equation} I've seen it used before in my computational physics class. I assumed it was equivalent but it seems like it wouldn't matter if there were a hole at $x=h$ in the symmetric derivative, whereas with the usual one it wouldn't be defined. Which is kinda interesting... If they're not equivalent - is there a good reason as to why we should use the common one? Or is the symmetric one actually more useful in some sense because it ""doesn't care"" about holes?","Is the symmetric definition of the derivative (below) equivalent to the usual one? \begin{equation} \lim_{h\to0}\frac{f(x+h)-f(x-h)}{2h} \end{equation} I've seen it used before in my computational physics class. I assumed it was equivalent but it seems like it wouldn't matter if there were a hole at $x=h$ in the symmetric derivative, whereas with the usual one it wouldn't be defined. Which is kinda interesting... If they're not equivalent - is there a good reason as to why we should use the common one? Or is the symmetric one actually more useful in some sense because it ""doesn't care"" about holes?",,"['real-analysis', 'calculus', 'derivatives', 'definition']"
79,"How $ψ(x,y)$ can be equal to $C$ when $ψ'$ equals $0$ [closed]",How  can be equal to  when  equals  [closed],"ψ(x,y) C ψ' 0","Closed . This question needs details or clarity . It is not currently accepting answers. Want to improve this question? Add details and clarify the problem by editing this post . Closed last year . The community reviewed whether to reopen this question last year and left it closed: Original close reason(s) were not resolved Improve this question I watched a lesson about exact equations in Khan Academy. Link to lesson In the video, there is a function called $ψ(x,y)$ . At the end, he found that : $$ \frac{d}{dx} ψ(x,y) = 0 $$ Then Sal khan said that we can integrate both sides of this equation and reach this: $$ ψ(x,y)=C $$ but I can't understand how we can deduce that. We only know that it's derivative with respect to $x$ is zero. How we can say that the function equals to a constant value? Can't we have it as a function of y ?: $$ ψ(x,y)=f(y) $$","Closed . This question needs details or clarity . It is not currently accepting answers. Want to improve this question? Add details and clarify the problem by editing this post . Closed last year . The community reviewed whether to reopen this question last year and left it closed: Original close reason(s) were not resolved Improve this question I watched a lesson about exact equations in Khan Academy. Link to lesson In the video, there is a function called . At the end, he found that : Then Sal khan said that we can integrate both sides of this equation and reach this: but I can't understand how we can deduce that. We only know that it's derivative with respect to is zero. How we can say that the function equals to a constant value? Can't we have it as a function of y ?:","ψ(x,y) 
\frac{d}{dx} ψ(x,y) = 0
 
ψ(x,y)=C
 x 
ψ(x,y)=f(y)
","['integration', 'derivatives', 'partial-derivative']"
80,Slope of normal,Slope of normal,,"Let a curve given by $y=x+\cos(xy) $ . Find the slope of normal line at $(0,1) $ point. My solution: $$f(x,y) =y-x-\cos(xy)=0$$ $$f'=-f_x/f_y=\dfrac{1-y\sin(xy)}{1+x\sin(xy)}$$ so $f'(0,1)=1$ and slope of normal line must be $-1$ . Is it true? Any mistake? Thanks.",Let a curve given by . Find the slope of normal line at point. My solution: so and slope of normal line must be . Is it true? Any mistake? Thanks.,"y=x+\cos(xy)  (0,1)  f(x,y) =y-x-\cos(xy)=0 f'=-f_x/f_y=\dfrac{1-y\sin(xy)}{1+x\sin(xy)} f'(0,1)=1 -1","['derivatives', 'solution-verification']"
81,Why are we solving this integral in this specific fashion?,Why are we solving this integral in this specific fashion?,,I am solving the following problem: $\int \frac{\text{d}x}{(x^2*\sqrt{1+x^2})}$ I set $x = \tan(\theta)$ $\frac{\text{d}x}{\text{d}\theta} = \sec^2(\theta)$ $\int \frac{\sec^2(\theta)}{\tan^2(\theta)*\sqrt{1+\tan^2(\theta)}}*\text{d}x$ $\int \frac{\sec^2(\theta)}{\tan^2(\theta)*\sec(\theta)} *\text{d}\theta$ After some simplifying I get: $\int \frac{\cos(\theta)}{\sin^2(\theta)} * \text{d}\theta$ $u = \sin(\theta)$ $\frac{\text{d}u}{\text{d}\theta} = \cos(\theta)$ $\int \frac{\text{d}u}{u^2}$ $\frac{-1}{\sin(\theta)} + c$ It is here where I get confused If you use the first triangle: $\sin(x) = \frac{x}{\sqrt{x^2+1}}$ If you use the second triangle: $\sin(x) = \frac{1}{\sqrt{x^2+1}}$ Could I use either one to find my answer in terms of x? My theory is that we use the first triangle because $x = \tan(\theta)$ and using the first triangle we get $x = \frac{x}{1}$ compared to $x = \frac{1}{x}$ This makes $x$ true for all real numbers in the first one while $x$ can only be equal to 1 in the second equation. Is this right or no? Sorry for making this question so long.,I am solving the following problem: I set After some simplifying I get: It is here where I get confused If you use the first triangle: If you use the second triangle: Could I use either one to find my answer in terms of x? My theory is that we use the first triangle because and using the first triangle we get compared to This makes true for all real numbers in the first one while can only be equal to 1 in the second equation. Is this right or no? Sorry for making this question so long.,\int \frac{\text{d}x}{(x^2*\sqrt{1+x^2})} x = \tan(\theta) \frac{\text{d}x}{\text{d}\theta} = \sec^2(\theta) \int \frac{\sec^2(\theta)}{\tan^2(\theta)*\sqrt{1+\tan^2(\theta)}}*\text{d}x \int \frac{\sec^2(\theta)}{\tan^2(\theta)*\sec(\theta)} *\text{d}\theta \int \frac{\cos(\theta)}{\sin^2(\theta)} * \text{d}\theta u = \sin(\theta) \frac{\text{d}u}{\text{d}\theta} = \cos(\theta) \int \frac{\text{d}u}{u^2} \frac{-1}{\sin(\theta)} + c \sin(x) = \frac{x}{\sqrt{x^2+1}} \sin(x) = \frac{1}{\sqrt{x^2+1}} x = \tan(\theta) x = \frac{x}{1} x = \frac{1}{x} x x,"['calculus', 'integration', 'derivatives', 'trigonometry']"
82,Holomorphic function has infinite many derivatives (proof without using Cauchy theorem),Holomorphic function has infinite many derivatives (proof without using Cauchy theorem),,"Let $f$ be an holomorphic function. It is possible to prove that it has infinitely many derivatives but without using Cauchy theorem? It is not an homework from my math class, it is just a curiosity. Hope someone could help or give a reference. Thank you in advance.","Let be an holomorphic function. It is possible to prove that it has infinitely many derivatives but without using Cauchy theorem? It is not an homework from my math class, it is just a curiosity. Hope someone could help or give a reference. Thank you in advance.",f,"['calculus', 'complex-analysis', 'derivatives']"
83,Flaw in proof of Jacobi's formula on Wikipedia?,Flaw in proof of Jacobi's formula on Wikipedia?,,"The Wikipedia article on Jacobi's formula (which gives the differential of the determinant function) contains two proofs, the second of which begins with a lemma claiming $\det'(I)=\operatorname{tr}$ , where $\det'$ is the differential of $\operatorname{det}$ . The article does not explicitly say "" total differential ,"" but I think it is reasonable to assume that this is intended, especially because it is true and one version of Jacobi's formula (the cleanest, in my opinion) gives the total differential of $\det$ as $\det'(T)(H)=\operatorname{tr}[\operatorname{adj}(T)H]$ . But the proof proceeds to show only that the directional derivative (or "" Gateaux derivative "") of $\det$ is as claimed. Even though the resulting operator $\operatorname{tr}$ is linear and continuous, this is not sufficient to guarantee that the total derivative exists, as I learned here . Am I correct that Wikipedia's proof is flawed, and if so, how can it be fixed? What I'd ideally like is a general theorem saying that if the directional derivative (in all directions) is given by a continuous linear operator $A$ , and some condition $X$ is satisfied, then the total derivative exists and is equal to $A$ (and then a proof that $X$ is satisfied in the given situation). Edit: I should add that I'm hoping to avoid something that explicitly references coordinates/components. For me, the appeal of the cited proof in the above article is its coordinate-free style. As PhoemueX points out in the comments, we could argue that, because $\det$ in coordinates is a polynomial, it has continuous partial derivatives, and hence its total derivative exists. But I'd prefer something more abstract that doesn't rely on coordinates, i.e., in the style of the original article if possible.","The Wikipedia article on Jacobi's formula (which gives the differential of the determinant function) contains two proofs, the second of which begins with a lemma claiming , where is the differential of . The article does not explicitly say "" total differential ,"" but I think it is reasonable to assume that this is intended, especially because it is true and one version of Jacobi's formula (the cleanest, in my opinion) gives the total differential of as . But the proof proceeds to show only that the directional derivative (or "" Gateaux derivative "") of is as claimed. Even though the resulting operator is linear and continuous, this is not sufficient to guarantee that the total derivative exists, as I learned here . Am I correct that Wikipedia's proof is flawed, and if so, how can it be fixed? What I'd ideally like is a general theorem saying that if the directional derivative (in all directions) is given by a continuous linear operator , and some condition is satisfied, then the total derivative exists and is equal to (and then a proof that is satisfied in the given situation). Edit: I should add that I'm hoping to avoid something that explicitly references coordinates/components. For me, the appeal of the cited proof in the above article is its coordinate-free style. As PhoemueX points out in the comments, we could argue that, because in coordinates is a polynomial, it has continuous partial derivatives, and hence its total derivative exists. But I'd prefer something more abstract that doesn't rely on coordinates, i.e., in the style of the original article if possible.",\det'(I)=\operatorname{tr} \det' \operatorname{det} \det \det'(T)(H)=\operatorname{tr}[\operatorname{adj}(T)H] \det \operatorname{tr} A X A X \det,"['real-analysis', 'derivatives', 'solution-verification', 'frechet-derivative', 'gateaux-derivative']"
84,"$f(x)=\begin{cases}x^{n+1}, & x \text{ irrational}\\ 0, & x \text{ rational}\end{cases}$ is differentiable at $0$ and discontinuous at $x\neq 0$.",is differentiable at  and discontinuous at .,"f(x)=\begin{cases}x^{n+1}, & x \text{ irrational}\\ 0, & x \text{ rational}\end{cases} 0 x\neq 0","I have proved that For every $n\in\mathbb{N}$ , the function $f(x)=\begin{cases}x^{n+1}, & x \text{ irrational}\\                     0, & x \text{ rational}\end{cases}$ is differentiable at $0$ and discontinuous at every $x\neq 0$ . but I am not sure about the correctness of my proof, especially the last part, where I show that it is discontinuous at every non-zero rational number, so I would appreciate if someone would check it out. I welcome comments. Thanks Proof. Let $n\in\mathbb{N}$ and $h\neq 0$ . $|\frac{f(0+h)-f(0)}{h}|\leq|\frac{h^{n+1}}{h}|=|h|^{n}\xrightarrow{h\to 0}0$ so $\lim\limits_{h\to 0}\frac{f(0+h)-f(0)}{h}=0$ thus $f$ is differentiable at $0$ and $f'(0)=0.$ Suppose $a$ is irrational and take $\varepsilon:=\frac{|a|^{n+1}}{2}$ . Then, since the rationals are dense in the reals, we have that however we choose $\delta>0$ there exists $x_q\in (a-\delta,a+\delta)\setminus\{a\}\cap\mathbb{Q}$ such that $|f(x_q)-f(a)|=|0-a^{n+1}|=|a|^{n+1}>\varepsilon$ so $\lim\limits_{x\to a}f(x)\neq f(a)$ i.e. $f$ is not continuous at $a$ . Suppose now that $a$ is a non-zero rational number. Pick $|a|>d>0$ and set $\ell:=\inf\{|f(x)|:x\in (a-d,a+d)\setminus\{a\}\cap\mathbb{R}\setminus\mathbb{Q}\}$ . Then if we take $\varepsilon:=\frac{\ell}{2}$ we have that for every $\delta>0$ there exists some $x_p\in (a-d,a+d)\setminus\{a\}\cap (a-\delta,a+\delta)\setminus\{a\}$ such that $|f(x_p)-f(a)|=|f(x_p)|>\varepsilon$ , so $\lim\limits_{x\to a}f(x)\neq f(a)$ i.e. $f$ is not continuous at $a$ . $\square$","I have proved that For every , the function is differentiable at and discontinuous at every . but I am not sure about the correctness of my proof, especially the last part, where I show that it is discontinuous at every non-zero rational number, so I would appreciate if someone would check it out. I welcome comments. Thanks Proof. Let and . so thus is differentiable at and Suppose is irrational and take . Then, since the rationals are dense in the reals, we have that however we choose there exists such that so i.e. is not continuous at . Suppose now that is a non-zero rational number. Pick and set . Then if we take we have that for every there exists some such that , so i.e. is not continuous at .","n\in\mathbb{N} f(x)=\begin{cases}x^{n+1}, & x \text{ irrational}\\ 
                   0, & x \text{ rational}\end{cases} 0 x\neq 0 n\in\mathbb{N} h\neq 0 |\frac{f(0+h)-f(0)}{h}|\leq|\frac{h^{n+1}}{h}|=|h|^{n}\xrightarrow{h\to 0}0 \lim\limits_{h\to 0}\frac{f(0+h)-f(0)}{h}=0 f 0 f'(0)=0. a \varepsilon:=\frac{|a|^{n+1}}{2} \delta>0 x_q\in (a-\delta,a+\delta)\setminus\{a\}\cap\mathbb{Q} |f(x_q)-f(a)|=|0-a^{n+1}|=|a|^{n+1}>\varepsilon \lim\limits_{x\to a}f(x)\neq f(a) f a a |a|>d>0 \ell:=\inf\{|f(x)|:x\in (a-d,a+d)\setminus\{a\}\cap\mathbb{R}\setminus\mathbb{Q}\} \varepsilon:=\frac{\ell}{2} \delta>0 x_p\in (a-d,a+d)\setminus\{a\}\cap (a-\delta,a+\delta)\setminus\{a\} |f(x_p)-f(a)|=|f(x_p)|>\varepsilon \lim\limits_{x\to a}f(x)\neq f(a) f a \square","['real-analysis', 'calculus', 'derivatives', 'solution-verification', 'continuity']"
85,"If for $x\in(\frac{1}{2},\infty)$ we have $f'(x)=(e^x-1)(x-2)(x-3)$. Show that there exist exactly two roots of $f''(x)=0$ in the given domain",If for  we have . Show that there exist exactly two roots of  in the given domain,"x\in(\frac{1}{2},\infty) f'(x)=(e^x-1)(x-2)(x-3) f''(x)=0","Let $f:\left(\frac{1}{2},\infty \right)\to \mathbb{R}$ be a function such that $f'(x)=(e^x-1)(x-2)(x-3)$ . Show that there exist exactly two roots of $f''(x)=0$ in the given domain. My Attempt I evaluated $f''(x)=e^x(x-2)(x-3)+(e^x-1)(x-2)+(e^x-1)(x-3)$ and then plugged in the values $1,2$ and $3$ and obtained $f''(1)=3-e>0$ $f''(2)=(e^2-1)(-1)<0$ $f''(3)=e^3-1>0$ So one can observe that there is at least one root on both of the intervals $(1,2)$ and $(2,3)$ . But how does one claim that there are exactly two roots in the given domain",Let be a function such that . Show that there exist exactly two roots of in the given domain. My Attempt I evaluated and then plugged in the values and and obtained So one can observe that there is at least one root on both of the intervals and . But how does one claim that there are exactly two roots in the given domain,"f:\left(\frac{1}{2},\infty \right)\to \mathbb{R} f'(x)=(e^x-1)(x-2)(x-3) f''(x)=0 f''(x)=e^x(x-2)(x-3)+(e^x-1)(x-2)+(e^x-1)(x-3) 1,2 3 f''(1)=3-e>0 f''(2)=(e^2-1)(-1)<0 f''(3)=e^3-1>0 (1,2) (2,3)","['real-analysis', 'calculus', 'derivatives', 'rolles-theorem']"
86,"Spivak, Ch. 15, Problem 31: Prove that $\sin$ isn't defined implicitly by an algebraic equation. Understanding a step in the proof.","Spivak, Ch. 15, Problem 31: Prove that  isn't defined implicitly by an algebraic equation. Understanding a step in the proof.",\sin,"The following is a problem from Ch. 15 of Spivak's Calculus (a) After all the work involved in the definition of $\sin$ , it would be disconcerting to find that $\sin$ is actually a rational function. Prove that it isn't. (There is a simple property of $\sin$ which a rational function cannot possibly have.). (b) Prove that $\sin$ isn't even defined implicitly by an algebraic equation. That is, there do not exist rational functions $f_0,...,f_{n-1}$ such that $$(\sin x)^n+f_{n-1}(x)(\sin x)^{n-1}+...+f_0(x)=0,\text{ for all } > x\tag{1}$$ Hint: Prove that $f_0=0$ , so that $\sin{x}$ can be factored out. The remaining factor is $0$ except perhaps at multiples of $\pi$ . But this implies that it is $0$ for all $x$ ( Why? ) You are now set up for a proof by induction. My question is precisely the one in bold above. It has been asked before here , but the answer there is but a hint that is already contained in the solution manual. Here is how the question arises in the context of a proof of part $(b)$ . $(a)$ A rational function has a finite number of roots (unless it is zero everywhere). $\sin$ has infinite roots $$\sin{k\pi}=0,k\in\mathbb{Z}$$ and $\sin$ is not $0$ everywhere. Thus $\sin$ is not a rational function. $(b)$ $$x=k\pi\implies\sin(k\pi)=0$$ So the entire expression $(1)$ becomes just $f_0(k\pi)=0$ for all $k\in\mathbb{Z}$ . But $f_0$ is a rational function so it must be $0$ , otherwise it'd have infinite roots. Then $(1)$ becomes $$\sin{x}[ (\sin{x})^{n-1}+f_{n-1}(x)(\sin{x})^{n-2}+...+f_1(x) ]=0\tag{2}$$ Now $x\neq k\pi, k\in\mathbb{Z}\implies\sin{x}\neq 0$ , so the expression in brackets must equal zero. So here is the part that my question is about. We have $$(\sin{x})^{n-1}+f_{n-1}(x)(\sin{x})^{n-2}+...+f_1(x)=0\tag{3}$$ Here is what the solution manual says at this point The term in brackets is continuous and $0$ except perhaps at multiples of $2\pi$ , so it is $0$ everywhere. This doesn't explain why. $(3)$ is true for all $x\neq k\pi$ , but possibly not at $x=k\pi$ . Rational functions can also be undefined at a finite number of points, but I believe this case is ruled out for any points because $(1)$ is true for all $x$ , correct? Ie, even at points $x=k\pi$ , the lefthand expression in $(3)$ must at least be defined for $(2)$ and thus $(1)$ to be true, correct? Therefore, the rational functions in question are continuous everywhere, $\sin$ and powers of $\sin$ are continuous everywhere, and so the expression in $(3)$ is continuous everywhere. But if we assume that the lefthand expression in $(3)$ is anything other than $0$ at $k\pi$ , then it is discontinuous at such points, which is a contradiction. I think the general idea is on the right track, but I don't feel very comfortable with the details of this last part. So, going back to original the problem statement: Why?","The following is a problem from Ch. 15 of Spivak's Calculus (a) After all the work involved in the definition of , it would be disconcerting to find that is actually a rational function. Prove that it isn't. (There is a simple property of which a rational function cannot possibly have.). (b) Prove that isn't even defined implicitly by an algebraic equation. That is, there do not exist rational functions such that Hint: Prove that , so that can be factored out. The remaining factor is except perhaps at multiples of . But this implies that it is for all ( Why? ) You are now set up for a proof by induction. My question is precisely the one in bold above. It has been asked before here , but the answer there is but a hint that is already contained in the solution manual. Here is how the question arises in the context of a proof of part . A rational function has a finite number of roots (unless it is zero everywhere). has infinite roots and is not everywhere. Thus is not a rational function. So the entire expression becomes just for all . But is a rational function so it must be , otherwise it'd have infinite roots. Then becomes Now , so the expression in brackets must equal zero. So here is the part that my question is about. We have Here is what the solution manual says at this point The term in brackets is continuous and except perhaps at multiples of , so it is everywhere. This doesn't explain why. is true for all , but possibly not at . Rational functions can also be undefined at a finite number of points, but I believe this case is ruled out for any points because is true for all , correct? Ie, even at points , the lefthand expression in must at least be defined for and thus to be true, correct? Therefore, the rational functions in question are continuous everywhere, and powers of are continuous everywhere, and so the expression in is continuous everywhere. But if we assume that the lefthand expression in is anything other than at , then it is discontinuous at such points, which is a contradiction. I think the general idea is on the right track, but I don't feel very comfortable with the details of this last part. So, going back to original the problem statement: Why?","\sin \sin \sin \sin f_0,...,f_{n-1} (\sin x)^n+f_{n-1}(x)(\sin x)^{n-1}+...+f_0(x)=0,\text{ for all }
> x\tag{1} f_0=0 \sin{x} 0 \pi 0 x (b) (a) \sin \sin{k\pi}=0,k\in\mathbb{Z} \sin 0 \sin (b) x=k\pi\implies\sin(k\pi)=0 (1) f_0(k\pi)=0 k\in\mathbb{Z} f_0 0 (1) \sin{x}[ (\sin{x})^{n-1}+f_{n-1}(x)(\sin{x})^{n-2}+...+f_1(x) ]=0\tag{2} x\neq k\pi, k\in\mathbb{Z}\implies\sin{x}\neq 0 (\sin{x})^{n-1}+f_{n-1}(x)(\sin{x})^{n-2}+...+f_1(x)=0\tag{3} 0 2\pi 0 (3) x\neq k\pi x=k\pi (1) x x=k\pi (3) (2) (1) \sin \sin (3) (3) 0 k\pi","['calculus', 'derivatives', 'proof-explanation', 'rational-functions', 'trigonometric-integrals']"
87,"Suppose $f:(a,b)\to\mathbb{R},c\in(a,b),$ and $f'(c)>0.$ Prove that there exists $\delta>0$ such that $f(x)>f(c)$ if $c<x<c+\delta.$",Suppose  and  Prove that there exists  such that  if,"f:(a,b)\to\mathbb{R},c\in(a,b), f'(c)>0. \delta>0 f(x)>f(c) c<x<c+\delta.","This question was on a practice final exam for my Elementary Real Analysis course. It says: Suppose $f:(a,b)\to\mathbb{R},c\in(a,b),$ and $f'(c)>0.$ Prove that there exists $\delta>0$ such that $f(x)>f(c)$ if $c<x<c+\delta.$ My solution was slightly different than their solution, so I'm wondering if this solution is correct? Here is my attempt: Proof: Since $f'(c)$ exists, $\forall\epsilon>0,\exists\delta>0$ such that if $x\in(a,b)$ and $0<|x-c|<\delta,$ then $$\left|\frac{f(x)-f(c)}{x-c}-f'(c)\right|<\epsilon. $$ Now choose $\epsilon=\frac{f'(c)}{2}.$ Then there exists some $\delta>0$ such that if $c<x<c+\delta,$ then $\begin{gather} \left|\frac{f(x)-f(c)}{x-c}-f'(c)\right|<\frac{f'(c)}{2}\\ -\frac{f'(c)}{2}<\frac{f(x)-f(c)}{x-c}-f'(c)<\frac{f'(c)}{2}\\ \frac{f'(c)}{2}<\frac{f(x)-f(c)}{x-c}<\frac{3f'(c)}{2}\\ 0<\frac{f'(c)}{2}(x-c)<f(x)-f(c)<\frac{3f'(c)}{2}(x-c), \end{gather}$ So then $f(x)>f(c).\blacksquare$","This question was on a practice final exam for my Elementary Real Analysis course. It says: Suppose and Prove that there exists such that if My solution was slightly different than their solution, so I'm wondering if this solution is correct? Here is my attempt: Proof: Since exists, such that if and then Now choose Then there exists some such that if then So then","f:(a,b)\to\mathbb{R},c\in(a,b), f'(c)>0. \delta>0 f(x)>f(c) c<x<c+\delta. f'(c) \forall\epsilon>0,\exists\delta>0 x\in(a,b) 0<|x-c|<\delta, \left|\frac{f(x)-f(c)}{x-c}-f'(c)\right|<\epsilon.  \epsilon=\frac{f'(c)}{2}. \delta>0 c<x<c+\delta, \begin{gather} \left|\frac{f(x)-f(c)}{x-c}-f'(c)\right|<\frac{f'(c)}{2}\\
-\frac{f'(c)}{2}<\frac{f(x)-f(c)}{x-c}-f'(c)<\frac{f'(c)}{2}\\
\frac{f'(c)}{2}<\frac{f(x)-f(c)}{x-c}<\frac{3f'(c)}{2}\\
0<\frac{f'(c)}{2}(x-c)<f(x)-f(c)<\frac{3f'(c)}{2}(x-c),
\end{gather} f(x)>f(c).\blacksquare","['real-analysis', 'derivatives', 'solution-verification']"
88,About extreme values of $\{f(x)-x\}^2$ when $f(x)$ is a cubic function.,About extreme values of  when  is a cubic function.,\{f(x)-x\}^2 f(x),"$t \ge 6$ , $t \in \mathbb{R}$ $f(x) = \frac{1}{t}\left( \frac{1}{8}x^3 + \frac{t^2}{8}x+2\right)$ $\{f(x)-x\}^2$ has an extreme value on $x = k$ Sum of such $k = g(t)$ $g(p) = -1$ for some $p \in \mathbb{R}$ $$\int_{6}^{p} g'(t)(8t-t^2)dt = \,?$$ My approach: I. For condition 3: Let $h(x) = \{f(x)-x\}^2$ . Since $h(x)$ has an extreme value on $x=k$ , $h'(k) = 0$ and $h''(k) \ne 0$ . For $h'(k) = 0$ , $h'(k) = 2\{f(k)-k\}(f'(k)-1) =0$ , so $f(k)=k$ or $f'(k)=1$ . For $h''(k) \ne 0$ , $h''(k)=2\left[\{(f'(k)-1)^2 + (f(k)-k)f''(k)\}\right] \ne 0$ Combining both 3 and 4, If $f(k) = k$ , $f'(k) \ne 1$ . If $f'(k) = 1$ , $f''(k)\ne0 \rightarrow k \ne 0$ and $f(k) \ne k$ . II. Applying $f(x)$ : Equation $f(k)=k$ becomes $8t-t^2=k^2 + \frac{16}{k}$ . Equation $f'(k)=1$ becomes $8t-t^2=3k^2$ . So, step I-5 becomes like: If $8t-t^2=k^2 + \frac{16}{k}$ , $8t-t^2\ne3k^2$ . If $8t-t^2=3k^2$ , $8t-t^2\ne k^2 + \frac{16}{k}$ and $k \ne 0$ . We can draw graphs of $y=3k^2$ , $y=k^2 + \frac{16}{k}$ , and $y = 8t-t^2$ which will be a constant graph. And we observe points which satisfies step 3. Constant graph( $y=8t-t^2$ ) has a value of less or equal than $12$ since $t \ge 6$ from condition. But the intersect $(2, 12)$ (where $t=6$ ) is the only point where it does not meet condition of step 3, since it is on both functions. So, we can say that $g(6) =(-4) + (-2) = -6$ . Below that, all intersects are correct points, so the sum $g(t)$ becomes the $x$ -coordinate which satisfies $x^2 + \frac{16}{x} = 8t-t^2$ , since sum of $x$ -coordinates on $y=3x^2$ becomes $0$ due to symmetry. So we can say that $\left(g(t) \right)^2 + \frac{16}{g(t)} = 8t - t^2$ . III. Getting the answer. $$\int_{6}^{p} g'(t)(8t-t^2)dt = \int_{6}^{p} g'(t)\left(\left(g(t) \right)^2 + \frac{16}{g(t)}\right)dt  $$ Let $g(t) = s$ , $$\int_{g(6)}^{g(p)} \left(s^2 + \frac{16}{s}\right)ds$$ Since $g(p) = -1$ from condition and $g(6) = -6$ from what we've got, $$\int_{-6}^{-1} \left(s^2 + \frac{16}{s}\right)ds = \left[ \frac{1}{3}s^3 + 16\ln{\left| s\right|} \right]_{-6}^{-1} = \frac{215}{8}-\ln{6}$$ And it was wrong. The correct answer was $21 - 32\ln2$ , and it seems like the value $g(6)$ was slightly off. I tried to explain my process as specific as possible, because I want to know if I am making a inefficient approach or I use too vague logic. And, the reason of my wrong answer. Any comment would be so helpful to me right now. +edit: By If a function $f(x)$ has an extreme value on $x=k$, $f''(k) \ne 0$? , step I-2 is a completely wrong logic. But $h(x)$ is a polynomial function, so checking $h'(x)=0$ is still a valid approach(not that it approves it is an extrema). By manually checking if the point $k=2$ is also an extrema, $k^2 + \frac{16}{k}$ is a $(+)$ , and $3k^2$ goes from $(-)$ to $(+)$ , it becomes an extrema. Thus, $g(6)=-4$ , and we can get the correct answer.",", has an extreme value on Sum of such for some My approach: I. For condition 3: Let . Since has an extreme value on , and . For , , so or . For , Combining both 3 and 4, If , . If , and . II. Applying : Equation becomes . Equation becomes . So, step I-5 becomes like: If , . If , and . We can draw graphs of , , and which will be a constant graph. And we observe points which satisfies step 3. Constant graph( ) has a value of less or equal than since from condition. But the intersect (where ) is the only point where it does not meet condition of step 3, since it is on both functions. So, we can say that . Below that, all intersects are correct points, so the sum becomes the -coordinate which satisfies , since sum of -coordinates on becomes due to symmetry. So we can say that . III. Getting the answer. Let , Since from condition and from what we've got, And it was wrong. The correct answer was , and it seems like the value was slightly off. I tried to explain my process as specific as possible, because I want to know if I am making a inefficient approach or I use too vague logic. And, the reason of my wrong answer. Any comment would be so helpful to me right now. +edit: By If a function $f(x)$ has an extreme value on $x=k$, $f''(k) \ne 0$? , step I-2 is a completely wrong logic. But is a polynomial function, so checking is still a valid approach(not that it approves it is an extrema). By manually checking if the point is also an extrema, is a , and goes from to , it becomes an extrema. Thus, , and we can get the correct answer.","t \ge 6 t \in \mathbb{R} f(x) = \frac{1}{t}\left( \frac{1}{8}x^3 + \frac{t^2}{8}x+2\right) \{f(x)-x\}^2 x = k k = g(t) g(p) = -1 p \in \mathbb{R} \int_{6}^{p} g'(t)(8t-t^2)dt = \,? h(x) = \{f(x)-x\}^2 h(x) x=k h'(k) = 0 h''(k) \ne 0 h'(k) = 0 h'(k) = 2\{f(k)-k\}(f'(k)-1) =0 f(k)=k f'(k)=1 h''(k) \ne 0 h''(k)=2\left[\{(f'(k)-1)^2 + (f(k)-k)f''(k)\}\right] \ne 0 f(k) = k f'(k) \ne 1 f'(k) = 1 f''(k)\ne0 \rightarrow k \ne 0 f(k) \ne k f(x) f(k)=k 8t-t^2=k^2 + \frac{16}{k} f'(k)=1 8t-t^2=3k^2 8t-t^2=k^2 + \frac{16}{k} 8t-t^2\ne3k^2 8t-t^2=3k^2 8t-t^2\ne k^2 + \frac{16}{k} k \ne 0 y=3k^2 y=k^2 + \frac{16}{k} y = 8t-t^2 y=8t-t^2 12 t \ge 6 (2, 12) t=6 g(6) =(-4) + (-2) = -6 g(t) x x^2 + \frac{16}{x} = 8t-t^2 x y=3x^2 0 \left(g(t) \right)^2 + \frac{16}{g(t)} = 8t - t^2 \int_{6}^{p} g'(t)(8t-t^2)dt = \int_{6}^{p} g'(t)\left(\left(g(t) \right)^2 + \frac{16}{g(t)}\right)dt   g(t) = s \int_{g(6)}^{g(p)} \left(s^2 + \frac{16}{s}\right)ds g(p) = -1 g(6) = -6 \int_{-6}^{-1} \left(s^2 + \frac{16}{s}\right)ds = \left[ \frac{1}{3}s^3 + 16\ln{\left| s\right|} \right]_{-6}^{-1} = \frac{215}{8}-\ln{6} 21 - 32\ln2 g(6) h(x) h'(x)=0 k=2 k^2 + \frac{16}{k} (+) 3k^2 (-) (+) g(6)=-4","['calculus', 'integration', 'derivatives', 'cubics']"
89,Radically different answers for $\frac{\mathrm d}{\mathrm dx}\left(\arccos\frac{\sqrt{1 - x^3} - \sqrt{1 + x^3}}{2}\right)$,Radically different answers for,\frac{\mathrm d}{\mathrm dx}\left(\arccos\frac{\sqrt{1 - x^3} - \sqrt{1 + x^3}}{2}\right),"Find the derivative with respect to $x$ of $$\cos^{-1}\left(\frac{\sqrt{1 - x^3}  - \sqrt{1 + x^3}}{2}\right).$$ Here's my work: Substituting $x^3 = \cos(2\theta):$ $$\begin{aligned}\cos^{-1}\left(\frac{\sqrt{1 - x^3}  - \sqrt{1 + x^3}}{2}\right) &= \cos^{-1}\left(\frac{\sqrt{1 - \cos(2\theta)}  - \sqrt{1 + \cos(2\theta)}}{2}\right)\\& = \cos^{-1}\left(\frac{\sqrt{2\sin^2\theta}  - \sqrt{2\cos^2\theta}}{2}\right)\\& = \cos^{-1}\left(\frac{1}{\sqrt{2}}\sin\theta - \frac{1}{\sqrt2}\cos\theta\right) \\& = \cos^{-1}\left(\sin\theta\cos(\pi/4) - \sin(\pi/4)\cos\theta\right) \\& = \cos^{-1}\sin\left(\theta - \frac{\pi}{4} \right)\\& = \cos^{-1}\cos\left(\frac{\pi}2 - \theta + \frac{\pi}{4} \right)\\& = \frac{3\pi}4 - \theta \\& = \frac{3\pi}4 - \frac12\cos^{-1}\left(x^3\right).\end{aligned}$$ Differentiating this gives $$\boxed{\frac{3x^2}{2\sqrt{1 - x^6}}}.$$ So far so good... My textbook also shows this answer. But the problem is that this , this , this , and the symPy package of Python all give this clunky result $$\boxed{-\frac{-\frac{3x^{2}}{2\sqrt{1-x^{3}}}-\frac{3x^{2}}{2\sqrt{1+x^{3}}}}{2\sqrt{1-\frac{1}{4}\left(\sqrt{1-x^{3}}-\sqrt{1+x^{3}}\right)^{2}}}}$$ (not even neutralising the three negative signs) instead of the above simpler one. WolframAlpha and Desmos shows that these two expressions have the same domain $(-1,1)$ and are identically equal. Why do these software not give the answer in simplified form?","Find the derivative with respect to of Here's my work: Substituting Differentiating this gives So far so good... My textbook also shows this answer. But the problem is that this , this , this , and the symPy package of Python all give this clunky result (not even neutralising the three negative signs) instead of the above simpler one. WolframAlpha and Desmos shows that these two expressions have the same domain and are identically equal. Why do these software not give the answer in simplified form?","x \cos^{-1}\left(\frac{\sqrt{1 - x^3}  - \sqrt{1 + x^3}}{2}\right). x^3 = \cos(2\theta): \begin{aligned}\cos^{-1}\left(\frac{\sqrt{1 - x^3}  - \sqrt{1 + x^3}}{2}\right) &= \cos^{-1}\left(\frac{\sqrt{1 - \cos(2\theta)}  - \sqrt{1 + \cos(2\theta)}}{2}\right)\\& = \cos^{-1}\left(\frac{\sqrt{2\sin^2\theta}  - \sqrt{2\cos^2\theta}}{2}\right)\\& = \cos^{-1}\left(\frac{1}{\sqrt{2}}\sin\theta - \frac{1}{\sqrt2}\cos\theta\right) \\& = \cos^{-1}\left(\sin\theta\cos(\pi/4) - \sin(\pi/4)\cos\theta\right) \\& = \cos^{-1}\sin\left(\theta - \frac{\pi}{4} \right)\\& = \cos^{-1}\cos\left(\frac{\pi}2 - \theta + \frac{\pi}{4} \right)\\& = \frac{3\pi}4 - \theta \\& = \frac{3\pi}4 - \frac12\cos^{-1}\left(x^3\right).\end{aligned} \boxed{\frac{3x^2}{2\sqrt{1 - x^6}}}. \boxed{-\frac{-\frac{3x^{2}}{2\sqrt{1-x^{3}}}-\frac{3x^{2}}{2\sqrt{1+x^{3}}}}{2\sqrt{1-\frac{1}{4}\left(\sqrt{1-x^{3}}-\sqrt{1+x^{3}}\right)^{2}}}} (-1,1)","['calculus', 'derivatives']"
90,Limit does not preserve strict inequality,Limit does not preserve strict inequality,,"When I was reading Rudin's proof of L'Hospital's Rule, I was confused with the following two lines. Since my question is not directly related to the proof itself but more on the limiting behavior of functions, I will only mention the part I don't understand. The set up is that functions $f,g$ are continuous on $[a,b]$ and differentiable on $(a,b)$ , the limit of $\frac{f'(t)}{g'(t)}$ exists and quals to $A$ , which could be anything on the extended real line. Given that $$\frac{f(x) - f(y)}{g(x) - g(y)} =\frac{f'(t)}{g'(t)} < r,$$ where we can treat $x$ as the variable, $y$ and $t$ as fixed, then as $x\to a$ , Rudin wrote $$\frac{f(x)}{f(y)}\le r. $$ Given that $$\frac{f(x)}{g(x)}< r - \frac{g(y)}{g(x)}+\frac{f(y)}{f(x)},$$ if we let $x\to a$ , then $$\frac{f(x)}{g(x)} < r .$$ Notice the first one changes to $\le$ after taking the limit while the second one remains $<$ . May I ask if there is a general rule on this? Could it be related to limsup or liminf? New update: I was actually misquoted the second part - Rudin wrote $\frac{f(x)}{g(x)}< q$ directly.","When I was reading Rudin's proof of L'Hospital's Rule, I was confused with the following two lines. Since my question is not directly related to the proof itself but more on the limiting behavior of functions, I will only mention the part I don't understand. The set up is that functions are continuous on and differentiable on , the limit of exists and quals to , which could be anything on the extended real line. Given that where we can treat as the variable, and as fixed, then as , Rudin wrote Given that if we let , then Notice the first one changes to after taking the limit while the second one remains . May I ask if there is a general rule on this? Could it be related to limsup or liminf? New update: I was actually misquoted the second part - Rudin wrote directly.","f,g [a,b] (a,b) \frac{f'(t)}{g'(t)} A \frac{f(x) - f(y)}{g(x) - g(y)} =\frac{f'(t)}{g'(t)} < r, x y t x\to a \frac{f(x)}{f(y)}\le r.  \frac{f(x)}{g(x)}< r - \frac{g(y)}{g(x)}+\frac{f(y)}{f(x)}, x\to a \frac{f(x)}{g(x)} < r . \le < \frac{f(x)}{g(x)}< q","['real-analysis', 'limits', 'derivatives', 'continuity']"
91,Relation between second derivatives,Relation between second derivatives,,"In attempting to find a relation between second derivatives of $\frac{dx}{dy}$ and $\frac{dy}{dx}$ , I used the relation: $\frac{dx}{dy} \times \frac{dy}{dx}$ = $1$ Then I differentiated both sides with respect to $x$ giving, using product rule and chain rule: $\left(\frac{dy}{dx} \times \left(\frac{d^2x}{dy^2}\times\frac{dy}{dx}\right)\right)$ + $\left(\frac{dx}{dy} \times \frac{d^2y}{dx^2}\right)$ = $0$ Thus $(\frac{dy}{dx})^2 \times \frac{d^2x}{dy^2} =  $ - $\left(\frac{dx}{dy} \times \frac{d^2y}{dx^2}\right)$ Which gives: - $\left(\frac{d^2y}{dx^2}\right)$ = $(\frac{dy}{dx})^3 \times \frac{d^2x}{dy^2}$ But apparently the answer is: - $\left(\frac{d^2y}{dx^2}\right)$ = $(\frac{dx}{dy})^3 \times \frac{d^2x}{dy^2}$ Am I wrong in applying the chain rule there? Or is there another mistake?","In attempting to find a relation between second derivatives of and , I used the relation: = Then I differentiated both sides with respect to giving, using product rule and chain rule: + = Thus - Which gives: - = But apparently the answer is: - = Am I wrong in applying the chain rule there? Or is there another mistake?",\frac{dx}{dy} \frac{dy}{dx} \frac{dx}{dy} \times \frac{dy}{dx} 1 x \left(\frac{dy}{dx} \times \left(\frac{d^2x}{dy^2}\times\frac{dy}{dx}\right)\right) \left(\frac{dx}{dy} \times \frac{d^2y}{dx^2}\right) 0 (\frac{dy}{dx})^2 \times \frac{d^2x}{dy^2} =   \left(\frac{dx}{dy} \times \frac{d^2y}{dx^2}\right) \left(\frac{d^2y}{dx^2}\right) (\frac{dy}{dx})^3 \times \frac{d^2x}{dy^2} \left(\frac{d^2y}{dx^2}\right) (\frac{dx}{dy})^3 \times \frac{d^2x}{dy^2},"['calculus', 'derivatives']"
92,Exchange of $\frac{d}{dx}$ and $\lim_{n\to\infty}$ in terms of continuity of the differential operator,Exchange of  and  in terms of continuity of the differential operator,\frac{d}{dx} \lim_{n\to\infty},"Under certain conditions, the derivative and the limit of a real function can be exchanged. $$\lim_{n\to\infty}\frac{df_n}{dx}=\frac{d}{dx}\lim_{n\to\infty}f_n$$ Can this be understood as the continuity of the differential operator on some metric space $\frac{d}{dx}:X\to Y$ ?","Under certain conditions, the derivative and the limit of a real function can be exchanged. Can this be understood as the continuity of the differential operator on some metric space ?",\lim_{n\to\infty}\frac{df_n}{dx}=\frac{d}{dx}\lim_{n\to\infty}f_n \frac{d}{dx}:X\to Y,"['calculus', 'functional-analysis']"
93,Formula for $k$th order derivative of the determinant of a matrix?,Formula for th order derivative of the determinant of a matrix?,k,Background Jacobi's formula tells us that $$\frac{d}{dt}\det A(t) = \operatorname{tr} \left( \operatorname{adj}(A(t)) \frac{dA(t)}{dt} \right) = (\det A(t)) \cdot \operatorname{tr} \left( A(t)^{-1} \cdot \frac{dA(t)}{dt} \right)$$ for the first derivative of a matrix $A(t)$ with respect to parameter $t$ . A previous question showed that there exists such a formula $$\frac{\partial^2}{\partial \alpha^2}\det A= \det(A) \left[\text{tr}^2\left( A^{-1} A_{\alpha} \right) +          \text{tr} \left( A^{-1} A_{\alpha^2} \right)+N\right]$$ for the second partial derivatives. Question Is there such a formula for the $k$ th order derivative $$\frac{d^k}{dt^k}\det A(t) = ?$$ of the determinant of such a matrix? Note that the question nth derivative of determinant wrt matrix is only a special case since here I am assuming each entry is function of a parameter $t$ that I am taking the derivative with respect to.,Background Jacobi's formula tells us that for the first derivative of a matrix with respect to parameter . A previous question showed that there exists such a formula for the second partial derivatives. Question Is there such a formula for the th order derivative of the determinant of such a matrix? Note that the question nth derivative of determinant wrt matrix is only a special case since here I am assuming each entry is function of a parameter that I am taking the derivative with respect to.,"\frac{d}{dt}\det A(t) = \operatorname{tr} \left( \operatorname{adj}(A(t)) \frac{dA(t)}{dt} \right) = (\det A(t)) \cdot \operatorname{tr} \left( A(t)^{-1} \cdot \frac{dA(t)}{dt} \right) A(t) t \frac{\partial^2}{\partial \alpha^2}\det A= \det(A) \left[\text{tr}^2\left( A^{-1} A_{\alpha} \right) +
         \text{tr} \left( A^{-1} A_{\alpha^2} \right)+N\right] k \frac{d^k}{dt^k}\det A(t) = ? t","['derivatives', 'determinant', 'matrix-calculus']"
94,"Advantage of ""integrating by differentiating"" ie.$\def\e{\varepsilon}\int_a^b f(x)dx=\lim_{\e\to0}f(d/d\e)\frac{e^{\e b}-e^{\e a}}\e$","Advantage of ""integrating by differentiating"" ie.",\def\e{\varepsilon}\int_a^b f(x)dx=\lim_{\e\to0}f(d/d\e)\frac{e^{\e b}-e^{\e a}}\e,"I just stumbled upon a (german) article that features the following formula to compute integrals by differentiating: $$\def\e{\varepsilon}\int_a^b\!\! f(x)dx\ =\ \lim_{\e\to0}f\left(\frac d{d\e}\right)\frac{e^{\e b}-e^{\e a}}{\e} \tag{*}$$ As the article states, the method was initially developed to calculate / approximate path integrals in Quantum Field Theory.  The new (from 2014) approach may have it's merits in QFT, but the article also states that it can make computation of integrals over $\Bbb R$ more efficient is some cases. It took me some seconds to decode the meaning of the right hand side, though.  It means that $d/d \e$ is expanded by $f$ which has to be analytic, and then apply that operator to $g_{a,b}(\e) = (e^{\e b}-e^{\e a})/\e$ and finally let $\e\to 0$ . My question: If $f$ cannot be integrated easily or symbolically, why is the right hand side of $(*)$ easier to compute or to approximate than the left hand side?  As $f$ is analytic, it's expansion has to be known in order to apply $f$ to $d/d\e$ . Hence computing $n$ term of the RHS looks way more complicated than integrating $n$ terms of the power-series expansion of $f$ on the LHS.  Does the RHS converge faster?  That would basically mean that the RHS can ""guess"" expansion coefficients of $f$ . The artical also states that: The computer algebra system Maple imlpemented the approach in 2019 and could achieve considerable speed-ups of some calculations. So it appears useful in practice, even if the title ""Revolution in Calculus"" might be 90% click-bait. It there some rule-of-thumb when that approach might be advantageous?","I just stumbled upon a (german) article that features the following formula to compute integrals by differentiating: As the article states, the method was initially developed to calculate / approximate path integrals in Quantum Field Theory.  The new (from 2014) approach may have it's merits in QFT, but the article also states that it can make computation of integrals over more efficient is some cases. It took me some seconds to decode the meaning of the right hand side, though.  It means that is expanded by which has to be analytic, and then apply that operator to and finally let . My question: If cannot be integrated easily or symbolically, why is the right hand side of easier to compute or to approximate than the left hand side?  As is analytic, it's expansion has to be known in order to apply to . Hence computing term of the RHS looks way more complicated than integrating terms of the power-series expansion of on the LHS.  Does the RHS converge faster?  That would basically mean that the RHS can ""guess"" expansion coefficients of . The artical also states that: The computer algebra system Maple imlpemented the approach in 2019 and could achieve considerable speed-ups of some calculations. So it appears useful in practice, even if the title ""Revolution in Calculus"" might be 90% click-bait. It there some rule-of-thumb when that approach might be advantageous?","\def\e{\varepsilon}\int_a^b\!\! f(x)dx\ =\ \lim_{\e\to0}f\left(\frac d{d\e}\right)\frac{e^{\e b}-e^{\e a}}{\e} \tag{*} \Bbb R d/d \e f g_{a,b}(\e) = (e^{\e b}-e^{\e a})/\e \e\to 0 f (*) f f d/d\e n n f f","['complex-analysis', 'derivatives', 'definite-integrals', 'numerical-methods', 'approximate-integration']"
95,Equivalence of the definition of differentials,Equivalence of the definition of differentials,,"I have two definitions of a differential of a map $f: \mathbb{R}^n \rightarrow \mathbb{R}$ at $x$ denoted as $df_x$ . $df_x(h) = \sum_{i = 1}^n \frac{\partial f (x)}{\partial x_i}h_i$ $df_x(h) = \lim_{t \rightarrow 0} \frac{f(x + ht) - f(x)}{t}$ How do I go about showing this equivalence? Intuitively the equivalence makes sense. If we take $n = 2$ , the partials in each direction give a sense of how much $f$ changes in each respective dimension, so (1) makes sense. I can also see how (2) gives us the differential if we think of the change of a surface in the direction of $h$ . I started with (2) and began by saying $$df_x(h) + E(t) = \frac{f(x + ht) - f(x)}{t}$$ such that $\lim_{t \rightarrow 0} E(t) = 0$ . This gives us $f(x + h) = f(x) + df_x(h)t + o(t)$ . I then tried to show that $f(x + h) = f(x) + (\sum_{i = 1}^n \frac{\partial f(x)} + {\partial x_i}h_i) + o(t)$ but I'm not sure if this helps and I am stuck here.","I have two definitions of a differential of a map at denoted as . How do I go about showing this equivalence? Intuitively the equivalence makes sense. If we take , the partials in each direction give a sense of how much changes in each respective dimension, so (1) makes sense. I can also see how (2) gives us the differential if we think of the change of a surface in the direction of . I started with (2) and began by saying such that . This gives us . I then tried to show that but I'm not sure if this helps and I am stuck here.",f: \mathbb{R}^n \rightarrow \mathbb{R} x df_x df_x(h) = \sum_{i = 1}^n \frac{\partial f (x)}{\partial x_i}h_i df_x(h) = \lim_{t \rightarrow 0} \frac{f(x + ht) - f(x)}{t} n = 2 f h df_x(h) + E(t) = \frac{f(x + ht) - f(x)}{t} \lim_{t \rightarrow 0} E(t) = 0 f(x + h) = f(x) + df_x(h)t + o(t) f(x + h) = f(x) + (\sum_{i = 1}^n \frac{\partial f(x)} + {\partial x_i}h_i) + o(t),"['derivatives', 'differential-geometry']"
96,What's the problem with the evaluation map not being continuous?,What's the problem with the evaluation map not being continuous?,,"When introducing differentiable functions between locally convex spaces, many authors (e.g. Bastiani, Keller, Kriegl-Michor) notice that the evaluation map $$ E\times E^*\to\mathbb R,\qquad (x,L)\mapsto L(x) $$ is not continuous with respect to any locally convex topology on $E^*$ , where $E$ is a non normable locally convex space and $E^*$ is the set of all continuous linear functionals $E\to\mathbb R$ . Then they argue that, for this reason, it is not good to define $f\colon E\to\mathbb R$ to be continuously differentiable by requiring $$ x\mapsto Df(x)\in E^*$$ to be continuous with respect to some locally convex topology on $E^*$ (say, for example, the finest locally convex topology, in order to make the strongest assumption). My question is: what's the problem with this definition? More precisely, what is an example of missing properties of $f\in C^1$ defined as above? I was thinking about continuity of $f$ being not implied by $f\in C^1$ , or maybe the failure of the chain rule, but I didn't find an explicit issue. For example, the definition is strong enough to have a mean value theorem $$ f(x+h)-f(x)=\int_0^1 Df(x+th)h\,dt$$ Moreover, which of these classical properties actually require the evaluation map to be continuous? I am particularly interested in the case of $E$ Fréchet space.","When introducing differentiable functions between locally convex spaces, many authors (e.g. Bastiani, Keller, Kriegl-Michor) notice that the evaluation map is not continuous with respect to any locally convex topology on , where is a non normable locally convex space and is the set of all continuous linear functionals . Then they argue that, for this reason, it is not good to define to be continuously differentiable by requiring to be continuous with respect to some locally convex topology on (say, for example, the finest locally convex topology, in order to make the strongest assumption). My question is: what's the problem with this definition? More precisely, what is an example of missing properties of defined as above? I was thinking about continuity of being not implied by , or maybe the failure of the chain rule, but I didn't find an explicit issue. For example, the definition is strong enough to have a mean value theorem Moreover, which of these classical properties actually require the evaluation map to be continuous? I am particularly interested in the case of Fréchet space."," E\times E^*\to\mathbb R,\qquad (x,L)\mapsto L(x)
 E^* E E^* E\to\mathbb R f\colon E\to\mathbb R  x\mapsto Df(x)\in E^* E^* f\in C^1 f f\in C^1  f(x+h)-f(x)=\int_0^1 Df(x+th)h\,dt E","['derivatives', 'dual-spaces', 'locally-convex-spaces']"
97,Is there a connection between exact differentials and law of total probability?,Is there a connection between exact differentials and law of total probability?,,"The law of total probability is in general: $$P(A)=\sum_n P(A\mid B_n)P(B_n)$$ You can express a differential of a function in the following sum: $$\text{d}f = \sum_{i=1}^n \frac{df(x_1,x_2,...,x_i)}{dx_i} \,dx_i$$ These two expressions look quite similar, and is about summing terms that are in some sense 'disjoint'. Like a decomposition of of an 'object' (I don't know how I would call it) in its disjoint terms. It's almost like $x_i$ is like a partition similar to $B_n$ . Is there a direct relation? Or is there actual no similarity at all? I hope there is some insight to be gained.","The law of total probability is in general: You can express a differential of a function in the following sum: These two expressions look quite similar, and is about summing terms that are in some sense 'disjoint'. Like a decomposition of of an 'object' (I don't know how I would call it) in its disjoint terms. It's almost like is like a partition similar to . Is there a direct relation? Or is there actual no similarity at all? I hope there is some insight to be gained.","P(A)=\sum_n P(A\mid B_n)P(B_n) \text{d}f = \sum_{i=1}^n \frac{df(x_1,x_2,...,x_i)}{dx_i} \,dx_i x_i B_n","['probability', 'derivatives']"
98,Check my example: does a function satisfying properties $1-4$ exist?,Check my example: does a function satisfying properties  exist?,1-4,"As an exercise, I need to find a $C^2$ function $f:\mathbb{R}^*\to\mathbb{R}$ such that $\eta>0$ exists such that $$ 1.\quad |f^{\prime}(x)|\le \eta\quad \mbox{ as } x\to +\infty;$$ $$ 2.\quad f^{\prime}(x)\ge \frac{1}{x^2}\quad \mbox{ as } x\to 0;$$ $$3. \quad a\in\mathbb{R}^* \mbox{ exists such that } f^{\prime}(a)=a \mbox{ (i.e. $f^{\prime}(a)$ has a fixed point)};$$ $$4.\quad f^{\prime\prime} (a)<-1. $$ I am thinking about the function $f(x) =-\frac{1}{x}$ ; in fact $\lim_{x\to+\infty} f(x)=0$ and $2.$ is satisfied with the equality. Moreover, $f^{\prime}$ has the fixed point $a=1$ and $f^{\prime\prime}(x)=\frac{-2}{x^3}$ so that $4.$ holds true. Could someone please tell me if my computations hold true? If possible, could someone please give me another (or more than another one) example of function satisfying these assumptions. Thank you in advance!","As an exercise, I need to find a function such that exists such that I am thinking about the function ; in fact and is satisfied with the equality. Moreover, has the fixed point and so that holds true. Could someone please tell me if my computations hold true? If possible, could someone please give me another (or more than another one) example of function satisfying these assumptions. Thank you in advance!",C^2 f:\mathbb{R}^*\to\mathbb{R} \eta>0  1.\quad |f^{\prime}(x)|\le \eta\quad \mbox{ as } x\to +\infty;  2.\quad f^{\prime}(x)\ge \frac{1}{x^2}\quad \mbox{ as } x\to 0; 3. \quad a\in\mathbb{R}^* \mbox{ exists such that } f^{\prime}(a)=a \mbox{ (i.e. f^{\prime}(a) has a fixed point)}; 4.\quad f^{\prime\prime} (a)<-1.  f(x) =-\frac{1}{x} \lim_{x\to+\infty} f(x)=0 2. f^{\prime} a=1 f^{\prime\prime}(x)=\frac{-2}{x^3} 4.,"['real-analysis', 'calculus', 'derivatives', 'fixed-points']"
99,Prove integral is convex,Prove integral is convex,,"$X$ are an iid draw from $(-\infty, \infty)$ according to $F$ with mean $\mu$ .  Further let $A = a(x, \theta)/\cos (\alpha)$ and $B = ((1- \cos(\alpha) - \sin (\alpha))\mu + \sin (\alpha)x)/\cos(\alpha)$ , where $a(x,\theta)$ is determined by \begin{equation}\tag{1} 1 - F(A-B) = f(A-B)A. \end{equation} It is assumed that that $F$ has an increasing hazard rate, so there is a unique solution.  Note that both $A$ and $B$ are functions of $x$ and $\alpha \in [0,\pi/2]$ . Show that the following integral is convex: \begin{equation} \int_{-\infty}^{\infty}(1-F(A-B))A \cos(\alpha)dF(x) \end{equation} Attempt : The regular method of differenting the function twice does not work.  I thought it may have something to do with Brunn-Minkowski theorem","are an iid draw from according to with mean .  Further let and , where is determined by It is assumed that that has an increasing hazard rate, so there is a unique solution.  Note that both and are functions of and . Show that the following integral is convex: Attempt : The regular method of differenting the function twice does not work.  I thought it may have something to do with Brunn-Minkowski theorem","X (-\infty, \infty) F \mu A = a(x, \theta)/\cos (\alpha) B = ((1- \cos(\alpha) - \sin (\alpha))\mu + \sin (\alpha)x)/\cos(\alpha) a(x,\theta) \begin{equation}\tag{1}
1 - F(A-B) = f(A-B)A.
\end{equation} F A B x \alpha \in [0,\pi/2] \begin{equation}
\int_{-\infty}^{\infty}(1-F(A-B))A \cos(\alpha)dF(x)
\end{equation}","['derivatives', 'probability-distributions', 'definite-integrals', 'convex-analysis', 'integral-inequality']"
