,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,Derivative with multiplication and division,Derivative with multiplication and division,,"So I have the following homework. I don't want the answer, only point me in the right direction please. Thanks. I'm stuck in the product rule. Do I apply the product rule twice or just one time after applying the quotient rule? $$ \frac{(2x^7-x^2)(x-1)}{(x+1)} $$ I get this. $$ \frac{(x+1)\frac{d}{dx}(2x^7-x^2)(x-1)-(2x^7-x^2)(x-1)\frac{d}{dx}(x+1)}{(x+1)^2} $$ By writing this down here I think I answered my own question. I should apply the product rule to the first $$(2x^7-x^2)(x-1)$$ but not to the second one, since the second one (after the '-' sign) doesn't have the $$\frac{d}{dx}$$ am I right? Sorry for any confusion.","So I have the following homework. I don't want the answer, only point me in the right direction please. Thanks. I'm stuck in the product rule. Do I apply the product rule twice or just one time after applying the quotient rule? $$ \frac{(2x^7-x^2)(x-1)}{(x+1)} $$ I get this. $$ \frac{(x+1)\frac{d}{dx}(2x^7-x^2)(x-1)-(2x^7-x^2)(x-1)\frac{d}{dx}(x+1)}{(x+1)^2} $$ By writing this down here I think I answered my own question. I should apply the product rule to the first $$(2x^7-x^2)(x-1)$$ but not to the second one, since the second one (after the '-' sign) doesn't have the $$\frac{d}{dx}$$ am I right? Sorry for any confusion.",,['derivatives']
1,Derivative of a ratio of geometric series,Derivative of a ratio of geometric series,,"I am trying to prove a theorem in my paper and am stuck at this irritating thing. Please help me. Show that $$\frac{d}{dk}\left(\frac{\sum_{x=1}^{n} x*k^x}{\sum_{x=1}^{n} k^x}\right) > 0$$ where $n > 1, k >1$ When I just calculate the ratio, I get $$\frac{n k^{(n+1)}-(n+1) k^n+1}{(k-1) (k^n-1)}$$. A simplified version of the derivative is $$\frac{1}{(k-1)^2}-\frac{n^2 k^{(n-1)}}{(k^n-1)^2}$$. There must be a simple way to show that the derivative is positive. Another way, I have tried to do this is by induction. Checked that it is true when $n=2$. Assuming, it holds for $N$, and show it for $N+1$. Again, I get a complicated expression there.","I am trying to prove a theorem in my paper and am stuck at this irritating thing. Please help me. Show that $$\frac{d}{dk}\left(\frac{\sum_{x=1}^{n} x*k^x}{\sum_{x=1}^{n} k^x}\right) > 0$$ where $n > 1, k >1$ When I just calculate the ratio, I get $$\frac{n k^{(n+1)}-(n+1) k^n+1}{(k-1) (k^n-1)}$$. A simplified version of the derivative is $$\frac{1}{(k-1)^2}-\frac{n^2 k^{(n-1)}}{(k^n-1)^2}$$. There must be a simple way to show that the derivative is positive. Another way, I have tried to do this is by induction. Checked that it is true when $n=2$. Assuming, it holds for $N$, and show it for $N+1$. Again, I get a complicated expression there.",,"['sequences-and-series', 'derivatives']"
2,Maximum of $3x^2e^{-x^3}$,Maximum of,3x^2e^{-x^3},"I have a PDF which looks like: $f(x) = 3x^2e^{-x^3}, \quad x \geq 0 $ I need to find it's maximum (to sample from it using the rejection method), so I differentiate and set the result to $0$: $(3x^2e^{-x^3})(-3x^2) + 6xe^{-x^3} = 0$ $\qquad$ (chain rule) $6xe^{-x^3} - 9x^4e^{-x^3} = 0$ Diving by $3xe^{-x3}$ to get: $2 - 3x^3 = 0$ Now I'm stuck and I'm not sure that I'm even doing the right thing. Plotting a graph of $f(x)$ looks like this . So the value $\approx 0.9$, but I don't know how to get there.  Thanks for your help.","I have a PDF which looks like: $f(x) = 3x^2e^{-x^3}, \quad x \geq 0 $ I need to find it's maximum (to sample from it using the rejection method), so I differentiate and set the result to $0$: $(3x^2e^{-x^3})(-3x^2) + 6xe^{-x^3} = 0$ $\qquad$ (chain rule) $6xe^{-x^3} - 9x^4e^{-x^3} = 0$ Diving by $3xe^{-x3}$ to get: $2 - 3x^3 = 0$ Now I'm stuck and I'm not sure that I'm even doing the right thing. Plotting a graph of $f(x)$ looks like this . So the value $\approx 0.9$, but I don't know how to get there.  Thanks for your help.",,"['optimization', 'derivatives', 'monte-carlo']"
3,Find values of C that satisfy the statement of theorem (Rolles/MVT),Find values of C that satisfy the statement of theorem (Rolles/MVT),,"I have the following function: $x^3+x-1$    $[0, 2]$ And determined the following: $f(0)=0^3+0-1=-1$ $f(2)=2^3+2-1=9$ And then this: $f'(c)=(9-(-1))/(2-0)$ $f'(c)=3c^2+c-1=5$ And I'm stuck on the above.  I'm not sure what to do from here.","I have the following function: $x^3+x-1$    $[0, 2]$ And determined the following: $f(0)=0^3+0-1=-1$ $f(2)=2^3+2-1=9$ And then this: $f'(c)=(9-(-1))/(2-0)$ $f'(c)=3c^2+c-1=5$ And I'm stuck on the above.  I'm not sure what to do from here.",,"['calculus', 'derivatives']"
4,"Chain rule definition for $f=f(x,g(x,y))$",Chain rule definition for,"f=f(x,g(x,y))","Here I have a question related to chain rule: Apply the chain rule to calculate: $\frac{\partial^2 f}{\partial x^2} \quad$ here $\quad\ f=f(x,g(x,y)) $ I have a trouble in the the first member of F because I don't know how to write correctly. if x was another fuction like h(x,y), then $\ f=f(h(x,y),g(x,y)) $ i write: $$\frac{\partial f}{\partial x} = \frac{\partial f}{\partial h}\frac{\partial h}{\partial x} + \frac{\partial f}{\partial g}\frac{\partial g}{\partial x} $$ but no. how to write correctly $\frac{\partial^2 f}{\partial x^2} $ where $\ f=f(x,g(x,y)) $ Sorry for my bad English.","Here I have a question related to chain rule: Apply the chain rule to calculate: $\frac{\partial^2 f}{\partial x^2} \quad$ here $\quad\ f=f(x,g(x,y)) $ I have a trouble in the the first member of F because I don't know how to write correctly. if x was another fuction like h(x,y), then $\ f=f(h(x,y),g(x,y)) $ i write: $$\frac{\partial f}{\partial x} = \frac{\partial f}{\partial h}\frac{\partial h}{\partial x} + \frac{\partial f}{\partial g}\frac{\partial g}{\partial x} $$ but no. how to write correctly $\frac{\partial^2 f}{\partial x^2} $ where $\ f=f(x,g(x,y)) $ Sorry for my bad English.",,"['ordinary-differential-equations', 'derivatives', 'partial-derivative']"
5,Derivative of $(Y-HX)^\top C(Y-HX)$ by $X$,Derivative of  by,(Y-HX)^\top C(Y-HX) X,"I'm trying to derive an expression for $$\nabla_X(Y-HX)^\top C(Y-HX).$$ $Y$ and $X$ are column vectors of size $N \! \times 1$. $H$ and $C$ are matrices of size $N \! \times N$. I have checked this Wikipedia page , but there wasn't an exactly matching identity there.","I'm trying to derive an expression for $$\nabla_X(Y-HX)^\top C(Y-HX).$$ $Y$ and $X$ are column vectors of size $N \! \times 1$. $H$ and $C$ are matrices of size $N \! \times N$. I have checked this Wikipedia page , but there wasn't an exactly matching identity there.",,"['matrices', 'derivatives']"
6,"Is this correct? $\sin'(z) = \cos(z),~\cos'(z) = -\sin(z)$",Is this correct?,"\sin'(z) = \cos(z),~\cos'(z) = -\sin(z)","I just want you guys to double check if I'm on the right track. so to prove $\sin '(z)$ = $ \cos (z)$ I did this: $\sin (z)$ = $\frac {e^{iz} - e^{-iz}}{2i}$ $\sin '(z)$ = $\frac {i(e^{iz} + e^{-iz})}{2i}$ = $\frac {e^{iz} + e^{-iz}}{2}$ = $\cos (z)$ is that correct? and for $\cos '(z)$ = $-\sin (z)$ I did something similar. $\cos (z)$ = $\frac {e^{iz} + e^{-iz}}{2}$ $\cos '(z)$ = $\frac {i(e^{iz} - e^{-iz})}{2}$ = $\frac {i(e^{iz} - e^{-iz})}{2}* \frac{i}{i}$ = $\frac {i^2(e^{iz} - e^{-iz})}{2i}$ = $\frac {-e^{iz} + e^{-iz}}{2i}$ = $-\sin (z)$ or using the Cauchy-Riemann formula/equation: $f '(z_0)= \frac{du}{dx} (x_0, y_0) - i \frac{du}{dy} (x_0,y_0)$ where $\frac{du}{dx} (x_0,y_0) = \frac{dv}{dy} (x_0,y_0)$ and $\frac{du}{dy} (x_0,y_0) = -\frac{dv}{dx} (x_0,y_0)$ so $\cos z = \cos x\cosh y - i\sin x\sinh y$ using the formula/equations we get: $\cos '(z) = -\sin x\cosh y - i\cos x\sinh y$. I believe this is the correct way. but please correct me if I'm wrong. Thank you!","I just want you guys to double check if I'm on the right track. so to prove $\sin '(z)$ = $ \cos (z)$ I did this: $\sin (z)$ = $\frac {e^{iz} - e^{-iz}}{2i}$ $\sin '(z)$ = $\frac {i(e^{iz} + e^{-iz})}{2i}$ = $\frac {e^{iz} + e^{-iz}}{2}$ = $\cos (z)$ is that correct? and for $\cos '(z)$ = $-\sin (z)$ I did something similar. $\cos (z)$ = $\frac {e^{iz} + e^{-iz}}{2}$ $\cos '(z)$ = $\frac {i(e^{iz} - e^{-iz})}{2}$ = $\frac {i(e^{iz} - e^{-iz})}{2}* \frac{i}{i}$ = $\frac {i^2(e^{iz} - e^{-iz})}{2i}$ = $\frac {-e^{iz} + e^{-iz}}{2i}$ = $-\sin (z)$ or using the Cauchy-Riemann formula/equation: $f '(z_0)= \frac{du}{dx} (x_0, y_0) - i \frac{du}{dy} (x_0,y_0)$ where $\frac{du}{dx} (x_0,y_0) = \frac{dv}{dy} (x_0,y_0)$ and $\frac{du}{dy} (x_0,y_0) = -\frac{dv}{dx} (x_0,y_0)$ so $\cos z = \cos x\cosh y - i\sin x\sinh y$ using the formula/equations we get: $\cos '(z) = -\sin x\cosh y - i\cos x\sinh y$. I believe this is the correct way. but please correct me if I'm wrong. Thank you!",,"['complex-analysis', 'derivatives', 'solution-verification']"
7,The rate of increase of the Gamma Function over real numbers,The rate of increase of the Gamma Function over real numbers,,"If $$ x_1 > x_2 > 0$$  and $$\Delta{x}>0$$ does it follow that: $$\ln\Gamma(x_1 + \Delta{x}) - \ln\Gamma(x_1) \ge \ln\Gamma(x_2 + \Delta{x}) - \ln\Gamma(x_2)$$ Would it be enough to show that the Gamma Function is a strictly increasing function for $x > 0$? Thanks, -Larry","If $$ x_1 > x_2 > 0$$  and $$\Delta{x}>0$$ does it follow that: $$\ln\Gamma(x_1 + \Delta{x}) - \ln\Gamma(x_1) \ge \ln\Gamma(x_2 + \Delta{x}) - \ln\Gamma(x_2)$$ Would it be enough to show that the Gamma Function is a strictly increasing function for $x > 0$? Thanks, -Larry",,"['calculus', 'derivatives', 'logarithms', 'gamma-function']"
8,Convergence of $\sum\limits^\infty _{k=0} a_k \sin(kx)+b_k \cos(kx)$,Convergence of,\sum\limits^\infty _{k=0} a_k \sin(kx)+b_k \cos(kx),"Ok, for the infinite series: $$\sum^\infty _{k=0} a_k \sin(kx)+b_k \cos(kx)$$ How do I show that this converges on any finite interval if $\sum^\infty _{k=0} k(|a_k|+|b_k|)<\infty$? Also, do the hack do I show that this function differentiable, and also finding its derivative? Thanks!","Ok, for the infinite series: $$\sum^\infty _{k=0} a_k \sin(kx)+b_k \cos(kx)$$ How do I show that this converges on any finite interval if $\sum^\infty _{k=0} k(|a_k|+|b_k|)<\infty$? Also, do the hack do I show that this function differentiable, and also finding its derivative? Thanks!",,"['sequences-and-series', 'analysis']"
9,Why is this function smooth?,Why is this function smooth?,,"Let $f: \mathbb{R}^n\rightarrow \mathbb{R}$ be the following function, $$f(x)=\begin{cases} \operatorname{e}^{-\tfrac{1}{1-\|x\|^2}} & \text{if }\|x\|<1,\\\\ 0 & \text{otherwise}. \end{cases}$$ How can I show that $f$ is smooth?","Let $f: \mathbb{R}^n\rightarrow \mathbb{R}$ be the following function, $$f(x)=\begin{cases} \operatorname{e}^{-\tfrac{1}{1-\|x\|^2}} & \text{if }\|x\|<1,\\\\ 0 & \text{otherwise}. \end{cases}$$ How can I show that $f$ is smooth?",,"['real-analysis', 'analysis', 'derivatives']"
10,derivatives and constants,derivatives and constants,,"I am confused on finding the derivative when constants are involved: with $3\ln(x^4 + \sec x)$ the derivative is $(3(4x^3 + \sec x \tan x))/(x^4+\sec x)$ notice the three stayed, but with $x^2+2x+7$ the derivative of seven turns to zero in the answer $2(x+1)$ my question is what is the difference.","I am confused on finding the derivative when constants are involved: with $3\ln(x^4 + \sec x)$ the derivative is $(3(4x^3 + \sec x \tan x))/(x^4+\sec x)$ notice the three stayed, but with $x^2+2x+7$ the derivative of seven turns to zero in the answer $2(x+1)$ my question is what is the difference.",,"['calculus', 'derivatives']"
11,help in a continuity problem of piecewise function,help in a continuity problem of piecewise function,,"If $f(x)=x^2-2|x|$ , then we have to test the differentiability of $g(x)$ in the interval $[-2,3] $ ,where $$g(x) =        \begin{cases}        \min\{f(t); -2≤t≤x\}&: x \in [-2,0)\\        \max\{f(t);0≤t≤x\},&: x \in [0,3]        \end{cases}         $$ My solution We can write $f(x) $ as $x^2+2x$ when $-2≤x≤0$ and $f(x)=x^2-2x$ for $ 0<x≤3$ . Now by drawing graph of $f(x)$ we see that it is decreasing in $[-2,-1]$ and $[0,1]$ and is increasing in $[-1,0]$ and $[1,3]$ , so $$g(x) =  \begin{cases}    x^2+2x,&:x \in [-2,-1]\\        -1,&:x \in (-1,0]\\ 0,&:x \in (0,1]\\ x^2-2x,&:x \in (1,3] \end{cases} $$ We now draw the graph of obtained $g(x)$ and find the points off discontinuity and indifferentiability as $0,1$ and $0,1$ , respectively. But the answer on back of book says the $g(x)$ is discontinuous at $x=0$ only and is indifferentiable at $x=0,2$ . I explained my approach to the problem and its solution as best I could. Can anyone tell where I'm wrong? I know the mistake is somewhere in deciding the function $g(x)$ in the interval $(0,1]$ , but I'm not able to figure that out . Please help.","If , then we have to test the differentiability of in the interval ,where My solution We can write as when and for . Now by drawing graph of we see that it is decreasing in and and is increasing in and , so We now draw the graph of obtained and find the points off discontinuity and indifferentiability as and , respectively. But the answer on back of book says the is discontinuous at only and is indifferentiable at . I explained my approach to the problem and its solution as best I could. Can anyone tell where I'm wrong? I know the mistake is somewhere in deciding the function in the interval , but I'm not able to figure that out . Please help.","f(x)=x^2-2|x| g(x) [-2,3]  g(x) =
       \begin{cases}
       \min\{f(t); -2≤t≤x\}&: x \in [-2,0)\\
       \max\{f(t);0≤t≤x\},&: x \in [0,3]
       \end{cases}
         f(x)  x^2+2x -2≤x≤0 f(x)=x^2-2x  0<x≤3 f(x) [-2,-1] [0,1] [-1,0] [1,3] g(x) = 
\begin{cases}
   x^2+2x,&:x \in [-2,-1]\\      
 -1,&:x \in (-1,0]\\
0,&:x \in (0,1]\\
x^2-2x,&:x \in (1,3]
\end{cases}
 g(x) 0,1 0,1 g(x) x=0 x=0,2 g(x) (0,1]","['calculus', 'derivatives', 'solution-verification', 'piecewise-continuity']"
12,Derivatives of Logarithmic Functions.,Derivatives of Logarithmic Functions.,,"I've been working through my practice problems and came across one that has stumped me. $$y = (3x^{2}+2)^{ln x}$$ The answer to this is: $$\frac{dy}{dx} = (3x^{2}+2)^{lnx} (\frac{1}{x}ln(3x^{2}+2)+\frac{6xlnx}{3x^{2}+2})$$ What I'm coming up with is: $$\frac{dy}{dx} = (3x^{2}+2)^{lnx} (\frac{1}{x}ln(3x^{2}+2)+\frac{6x}{3x^{2}+2})$$ What I'm not understanding is where the $\frac{6xlnx}{3x^{2}+2}$ comes from, if anyone could explain this I'd really appreciate it.","I've been working through my practice problems and came across one that has stumped me. $$y = (3x^{2}+2)^{ln x}$$ The answer to this is: $$\frac{dy}{dx} = (3x^{2}+2)^{lnx} (\frac{1}{x}ln(3x^{2}+2)+\frac{6xlnx}{3x^{2}+2})$$ What I'm coming up with is: $$\frac{dy}{dx} = (3x^{2}+2)^{lnx} (\frac{1}{x}ln(3x^{2}+2)+\frac{6x}{3x^{2}+2})$$ What I'm not understanding is where the $\frac{6xlnx}{3x^{2}+2}$ comes from, if anyone could explain this I'd really appreciate it.",,"['calculus', 'derivatives']"
13,Partial derivative with respect to a vector x for $F(x) = x^TA(x)x$,Partial derivative with respect to a vector x for,F(x) = x^TA(x)x,"I have the next function  $F(x) = x^TA(x)x$, where $x$ is a real vector with dimension $n$, and $A$ is a square real matrix $n \times n$ depending on the components of $x$. How can I compute the partial derivative of $F(x)$ with respect to $x$? I know when $A$ is constant that $\frac{\partial F}{\partial x} = x^T(A+A^T)$. What I do not know how to deal when $A$ depends on elements of $x$. Thanks in advance.","I have the next function  $F(x) = x^TA(x)x$, where $x$ is a real vector with dimension $n$, and $A$ is a square real matrix $n \times n$ depending on the components of $x$. How can I compute the partial derivative of $F(x)$ with respect to $x$? I know when $A$ is constant that $\frac{\partial F}{\partial x} = x^T(A+A^T)$. What I do not know how to deal when $A$ depends on elements of $x$. Thanks in advance.",,['linear-algebra']
14,"Partial differentiation of integrals from u(x, y) to v(x, y)","Partial differentiation of integrals from u(x, y) to v(x, y)",,"I'm having trouble with this question. If: $$ w = \displaystyle\int_{xy}^{2x-3y}du/ln(u)\,du $$ Find $$ \frac{\partial y}{\partial x} $$ at x = 3, y= 1. I know that the general rule for differentiation of integrals of this type goes: $$\frac{d}{dx}\displaystyle\int_{u(x)}^{v(x)}f(t)dt = f(v)\frac{dv}{dx} - f(u)\frac{du}{dx}$$ Using the general rule and substituting partial derivatives for singular derivatives, I found: $$\frac{\partial w}{\partial x} = 1/ln(3)$$ and  $$\frac{\partial w}{\partial y} = -6/ln(3)$$ However, I am at a loss as to how to find the solution to this question as the partial derivative is of the terms that define the integral's interval. Would I be able to consider the equation: $$ \frac{\partial y}{\partial x} =  \frac{\partial y}{\partial w}\frac{\partial w}{\partial x} $$ and then say  $$ \frac{\partial y}{\partial w} = 1/\frac{\partial w}{\partial y} $$ Any help would be greatly appreciated.","I'm having trouble with this question. If: $$ w = \displaystyle\int_{xy}^{2x-3y}du/ln(u)\,du $$ Find $$ \frac{\partial y}{\partial x} $$ at x = 3, y= 1. I know that the general rule for differentiation of integrals of this type goes: $$\frac{d}{dx}\displaystyle\int_{u(x)}^{v(x)}f(t)dt = f(v)\frac{dv}{dx} - f(u)\frac{du}{dx}$$ Using the general rule and substituting partial derivatives for singular derivatives, I found: $$\frac{\partial w}{\partial x} = 1/ln(3)$$ and  $$\frac{\partial w}{\partial y} = -6/ln(3)$$ However, I am at a loss as to how to find the solution to this question as the partial derivative is of the terms that define the integral's interval. Would I be able to consider the equation: $$ \frac{\partial y}{\partial x} =  \frac{\partial y}{\partial w}\frac{\partial w}{\partial x} $$ and then say  $$ \frac{\partial y}{\partial w} = 1/\frac{\partial w}{\partial y} $$ Any help would be greatly appreciated.",,"['calculus', 'integration', 'derivatives', 'partial-derivative']"
15,Proving differentiability (rigorously).,Proving differentiability (rigorously).,,"How do I prove them to be differentiable in a ""rigorous"" way? Or do i just simply compute their differentials?","How do I prove them to be differentiable in a ""rigorous"" way? Or do i just simply compute their differentials?",,"['real-analysis', 'ordinary-differential-equations', 'derivatives']"
16,Mean Value Theorem inequality,Mean Value Theorem inequality,,"Suppose $\mathbb{R}^n$ space with norm $\|\cdot\|$ and $I$ open interval and $f: I \rightarrow \mathbb{R}^n$ derivable function. let $k$ be a constant and $x_0$ element in $I$. we have  $\|f\,'(x)\|\leq k\|f(x)\|$ for every $x \in I$ and we have $ f(x_0)=0$ show that (by applying the mean value theorem on $f(x)$ in the interval $[x_0-h, x_0+h]$ for $h$ very small )there exist $h>0$ such that  $f$ is equal  zero  on the interval $[x_0-h, x_0+h]$. Am not sure how to make it, hope to get a hint. Thanks alot Sahar","Suppose $\mathbb{R}^n$ space with norm $\|\cdot\|$ and $I$ open interval and $f: I \rightarrow \mathbb{R}^n$ derivable function. let $k$ be a constant and $x_0$ element in $I$. we have  $\|f\,'(x)\|\leq k\|f(x)\|$ for every $x \in I$ and we have $ f(x_0)=0$ show that (by applying the mean value theorem on $f(x)$ in the interval $[x_0-h, x_0+h]$ for $h$ very small )there exist $h>0$ such that  $f$ is equal  zero  on the interval $[x_0-h, x_0+h]$. Am not sure how to make it, hope to get a hint. Thanks alot Sahar",,"['calculus', 'derivatives']"
17,2nd order implicit derivative,2nd order implicit derivative,,What would the 2nd order implicit derivative of $y^2=12x$ be? I get the first derivative is $2yy'=12$ but Wolfram gives the second as $y''=\frac{-3}{xy}$ and I don't understand how they get that.,What would the 2nd order implicit derivative of $y^2=12x$ be? I get the first derivative is $2yy'=12$ but Wolfram gives the second as $y''=\frac{-3}{xy}$ and I don't understand how they get that.,,['derivatives']
18,Is there any way to calculate change in derivatives along a vector?,Is there any way to calculate change in derivatives along a vector?,,"Suppose I have a function F($\vec{x})$ $x\subset R^{n}$, and a vector $\vec{g}$. I want to perturb $\vec{x}$ along the $\vec{g}$ vector, and see how the gradient changes as I move further along the $\vec{g}$ vector. So I'm trying to solve $\lim_{h\to 0}(\nabla F(\vec{x}+h*\vec{g})-\nabla F(\vec{x}))/h$. Note, I don't want to know the change in the function as h approaches 0, but the change in each gradient (so there should be as many terms in the answer to the above formula as there are dimensions in $\vec{x}$. I'm pretty sure there should be an efficient way to do this (linear in the number of parameters), since at worst case I can just use a small value for h and use finite differences. Likewise, I think we should be able to get higher derivatives along the vector also linear in the number of parameters, since we can also accomplish that linear in the number of parameters with finite differences. My first thought was to set $\vec{x}(h)=\vec{x}_{o}+h*\vec{g}$, and then try to use the chain rule, so I end up with $F'(\vec{x}(h))*\vec{x}'(h)*dh$, but that quantity seems like it would be one dimensional ($\frac{dF}{dh}$), and not the change in the each gradient itself. Plus, I'm not sure that would even work for higher derivatives, since $F''(\vec{x})$ has $n^{2}$ terms, so we're no longer linear in the number of parameters. Now I'm trying to see if I can derive some transformation of F that will change it to a new function that has as its derivatives the quantities I'm trying to calculate, but I'm not having much luck doing it that way either. In general, I'd like to be able to calculate the change in the gradients along h even if $\vec{x}(h)$ is a non-linear function of h (so we're following a curve instead of a line), but for the time being I'm keeping it simple and saying that $\vec{x}(h)$ is linear. Is there some general way to find these quantities.","Suppose I have a function F($\vec{x})$ $x\subset R^{n}$, and a vector $\vec{g}$. I want to perturb $\vec{x}$ along the $\vec{g}$ vector, and see how the gradient changes as I move further along the $\vec{g}$ vector. So I'm trying to solve $\lim_{h\to 0}(\nabla F(\vec{x}+h*\vec{g})-\nabla F(\vec{x}))/h$. Note, I don't want to know the change in the function as h approaches 0, but the change in each gradient (so there should be as many terms in the answer to the above formula as there are dimensions in $\vec{x}$. I'm pretty sure there should be an efficient way to do this (linear in the number of parameters), since at worst case I can just use a small value for h and use finite differences. Likewise, I think we should be able to get higher derivatives along the vector also linear in the number of parameters, since we can also accomplish that linear in the number of parameters with finite differences. My first thought was to set $\vec{x}(h)=\vec{x}_{o}+h*\vec{g}$, and then try to use the chain rule, so I end up with $F'(\vec{x}(h))*\vec{x}'(h)*dh$, but that quantity seems like it would be one dimensional ($\frac{dF}{dh}$), and not the change in the each gradient itself. Plus, I'm not sure that would even work for higher derivatives, since $F''(\vec{x})$ has $n^{2}$ terms, so we're no longer linear in the number of parameters. Now I'm trying to see if I can derive some transformation of F that will change it to a new function that has as its derivatives the quantities I'm trying to calculate, but I'm not having much luck doing it that way either. In general, I'd like to be able to calculate the change in the gradients along h even if $\vec{x}(h)$ is a non-linear function of h (so we're following a curve instead of a line), but for the time being I'm keeping it simple and saying that $\vec{x}(h)$ is linear. Is there some general way to find these quantities.",,"['calculus', 'derivatives']"
19,"Switching Variables in L'Hopital, Series, Limits","Switching Variables in L'Hopital, Series, Limits",,My Question is from lines 2 to 3. How did you get from lim x to infinity to  limit t approaching zero from the positive?,My Question is from lines 2 to 3. How did you get from lim x to infinity to  limit t approaching zero from the positive?,,"['limits', 'derivatives']"
20,Identify the equation of the normal line?,Identify the equation of the normal line?,,"Identify the equation of the normal line to the curve $y=g(p)=2.5+3.5(4^p)$ where it crosses the $y$-axis. So I am guessing the normal line would be the inverse of the derivative function, since it is perpendicular to the tangent line. Is this correct? If so, I got the derivative to be $g'(p)=3.5\cdot 4^{p}\ln(4)$. Is this the correct derivative form? If so, how do I take the inverse of this? Thank you!","Identify the equation of the normal line to the curve $y=g(p)=2.5+3.5(4^p)$ where it crosses the $y$-axis. So I am guessing the normal line would be the inverse of the derivative function, since it is perpendicular to the tangent line. Is this correct? If so, I got the derivative to be $g'(p)=3.5\cdot 4^{p}\ln(4)$. Is this the correct derivative form? If so, how do I take the inverse of this? Thank you!",,"['calculus', 'derivatives', 'inverse']"
21,Gâteaux derivate of the Tikhonov functional,Gâteaux derivate of the Tikhonov functional,,"Let $X,Y$ be Hilbert spaces, and let $A\colon X\to Y$ be a compact operator. The Tikhonov functional is given by   $$ F(x)=\lVert Ax-y\rVert_X^2+\alpha\lVert x\rVert_X^2. $$   Calculate the Gâteaux derivative of the Tikhonov functional. Tip: Use $\lVert a+b\rVert=\lVert a\rVert^2+2(a,b)+\lVert b\rVert^2$. First question: Is it really $\lVert Ax-y\rVert_X^2$ and not the Y-norm? Second question: What I have to do here is to my opinion calculate $$ \lim\limits_{t\to 0}\frac{F(x+th)-F(x)}{t}. $$ So I would start with calculating $F(x+th)=\lVert A(x+th)-y)\rVert_X^2+\alpha\lVert x+th\rVert_X^2$. I start with calculating $\lVert A(x+th)-y)_X^2$. I can not read from the text if the operator $A$ is linear, too. Is it right to calculate with the given tip $$\lVert A(x+th)-y\rVert_X^2=\lVert A(th)+Ax-y\rVert_X^2=\lVert A(th)\rVert_X^2+2(A(th),Ax-y)+\lVert Ax-y\rVert_X^2?$$","Let $X,Y$ be Hilbert spaces, and let $A\colon X\to Y$ be a compact operator. The Tikhonov functional is given by   $$ F(x)=\lVert Ax-y\rVert_X^2+\alpha\lVert x\rVert_X^2. $$   Calculate the Gâteaux derivative of the Tikhonov functional. Tip: Use $\lVert a+b\rVert=\lVert a\rVert^2+2(a,b)+\lVert b\rVert^2$. First question: Is it really $\lVert Ax-y\rVert_X^2$ and not the Y-norm? Second question: What I have to do here is to my opinion calculate $$ \lim\limits_{t\to 0}\frac{F(x+th)-F(x)}{t}. $$ So I would start with calculating $F(x+th)=\lVert A(x+th)-y)\rVert_X^2+\alpha\lVert x+th\rVert_X^2$. I start with calculating $\lVert A(x+th)-y)_X^2$. I can not read from the text if the operator $A$ is linear, too. Is it right to calculate with the given tip $$\lVert A(x+th)-y\rVert_X^2=\lVert A(th)+Ax-y\rVert_X^2=\lVert A(th)\rVert_X^2+2(A(th),Ax-y)+\lVert Ax-y\rVert_X^2?$$",,"['functional-analysis', 'derivatives']"
22,3D Numerical differentiation with spline approximation,3D Numerical differentiation with spline approximation,,"I have three 3D matrices X, Y, and Z that define a matrix V of the same size over some region. The matrices are regularly spaced. I'm trying to compute the gradient of V. I have read that interpolating and computing derivatives with splines leads to better results than using central differences. For instance, I have worked before with splinefit and ppdiff ( http://www.mathworks.com/matlabcentral/fileexchange/13812-splinefit ). The problem is that I can't find code to do this in 3D. Assuming I only want the derivatives at the sampled locations define by the X, Y, and Z matrices, could I do 1D spline approximations for each dimension and compute the partial derivatives that way? I've tried it with a simple 2D Matlab example, and this idea works; however, I wanted to see if it made mathematical sense Thanks for your help!","I have three 3D matrices X, Y, and Z that define a matrix V of the same size over some region. The matrices are regularly spaced. I'm trying to compute the gradient of V. I have read that interpolating and computing derivatives with splines leads to better results than using central differences. For instance, I have worked before with splinefit and ppdiff ( http://www.mathworks.com/matlabcentral/fileexchange/13812-splinefit ). The problem is that I can't find code to do this in 3D. Assuming I only want the derivatives at the sampled locations define by the X, Y, and Z matrices, could I do 1D spline approximations for each dimension and compute the partial derivatives that way? I've tried it with a simple 2D Matlab example, and this idea works; however, I wanted to see if it made mathematical sense Thanks for your help!",,"['derivatives', 'interpolation', 'spline']"
23,What is the result of deriving a polygon?,What is the result of deriving a polygon?,,"If you define a polygon - say, for simplicity, a triangle - as a list of functions, defined piecewise, for example: $a(x)=2x$ defined on $[0,1]$ $b(x)=-2x+2$ defined on $[1,2]$ $c(x)=0$ defined on $[0,2]$ What do you get by deriving the functions on their intervals and plotting $a'(x)$, $b'(x)$ and $c'(x)$? (In this specific case, the result is made of three segments parallel to the x axis, but what is the result of any given polygon?)","If you define a polygon - say, for simplicity, a triangle - as a list of functions, defined piecewise, for example: $a(x)=2x$ defined on $[0,1]$ $b(x)=-2x+2$ defined on $[1,2]$ $c(x)=0$ defined on $[0,2]$ What do you get by deriving the functions on their intervals and plotting $a'(x)$, $b'(x)$ and $c'(x)$? (In this specific case, the result is made of three segments parallel to the x axis, but what is the result of any given polygon?)",,"['geometry', 'derivatives']"
24,Question regarding Cauchy's theorem?,Question regarding Cauchy's theorem?,,I want to  prove the inequality $3x\cdot \mathrm{arccot}{x}\geq\ln(1+x^{2})$... using Cauchy's theorem..I saw this in a book which I bought in a library and ive never seen this type of exercise..can you solve it ?,I want to  prove the inequality $3x\cdot \mathrm{arccot}{x}\geq\ln(1+x^{2})$... using Cauchy's theorem..I saw this in a book which I bought in a library and ive never seen this type of exercise..can you solve it ?,,"['analysis', 'derivatives']"
25,What is the derivative of a power series composed with a sum of iterations on x?,What is the derivative of a power series composed with a sum of iterations on x?,,"Assume the following situation. I want to evaluate the derivative of a function for which I have a power series. In principle this is well known: just insert the derivatives at each coefficient: $$ S(x) = \sum_{k=0}^\infty a_k \cdot x^k \to S(x)' = \sum_{k=0}^\infty (k+1)\cdot a_{k+1} \cdot x^k $$ and evaluate. So far, so good. The convergence-radius of the power series is small,  but fortunately I can reexpress it as  $$ S(x) = x_0-x_1+x_2-\ldots - x_{m-1}+\sum_{k=0}^\infty a_k \cdot x_m^k $$ and I do not know, how I reflect the leading $x_k$ into the derivative. It is with a transfer-function $f(x)=b^x-1$ that  $$x_1=b^x-1,x_2=b^{x_1}-1,\ldots x_m=b^{x_{m-1}}-1$$ such that $x_m$ is in the radius of the power series for $S(x)$. So my question is now how to include that leading terms in the formula for the derivative? Is it simply to write the derivative  $$ S(x)' = f'(t)_{|t=x} - f'(t)_{|t=x_1} + \ldots - f'(t)_{|t=x_{m-1}} + \sum_{k=0}^\infty (k+1)\cdot a_{k+1} \cdot x_m^k \qquad \text{???}$$ but this is just a guess...","Assume the following situation. I want to evaluate the derivative of a function for which I have a power series. In principle this is well known: just insert the derivatives at each coefficient: $$ S(x) = \sum_{k=0}^\infty a_k \cdot x^k \to S(x)' = \sum_{k=0}^\infty (k+1)\cdot a_{k+1} \cdot x^k $$ and evaluate. So far, so good. The convergence-radius of the power series is small,  but fortunately I can reexpress it as  $$ S(x) = x_0-x_1+x_2-\ldots - x_{m-1}+\sum_{k=0}^\infty a_k \cdot x_m^k $$ and I do not know, how I reflect the leading $x_k$ into the derivative. It is with a transfer-function $f(x)=b^x-1$ that  $$x_1=b^x-1,x_2=b^{x_1}-1,\ldots x_m=b^{x_{m-1}}-1$$ such that $x_m$ is in the radius of the power series for $S(x)$. So my question is now how to include that leading terms in the formula for the derivative? Is it simply to write the derivative  $$ S(x)' = f'(t)_{|t=x} - f'(t)_{|t=x_1} + \ldots - f'(t)_{|t=x_{m-1}} + \sum_{k=0}^\infty (k+1)\cdot a_{k+1} \cdot x_m^k \qquad \text{???}$$ but this is just a guess...",,"['derivatives', 'power-series']"
26,Can someone check my work on this differential problem?,Can someone check my work on this differential problem?,,"Use differentials to estimate the amount of ice in cubic inches that covers a 3 ft cube if the ice is $\frac{1}{2}$ inch thick. Since it is a cube, I believe the equation should be: $$ y = x^3 $$ Take the derivative: $$ y' = 3x^2 $$ Plug in 3: $$ y = 3*3^2 $$ Then plug in 3 + .5 (1/2 inch) $$ y = 3 * (3.5)^2 $$ Resulting in: 36.75 - 27 = 9.75 inches cubed Is this correct, or did I make a mistake somewhere along the way? EDIT $ y = 3 * (3)^2 * \frac{1}{12} $ Resulting in: 2.25 inches cubed. (seems more reasonable).","Use differentials to estimate the amount of ice in cubic inches that covers a 3 ft cube if the ice is $\frac{1}{2}$ inch thick. Since it is a cube, I believe the equation should be: $$ y = x^3 $$ Take the derivative: $$ y' = 3x^2 $$ Plug in 3: $$ y = 3*3^2 $$ Then plug in 3 + .5 (1/2 inch) $$ y = 3 * (3.5)^2 $$ Resulting in: 36.75 - 27 = 9.75 inches cubed Is this correct, or did I make a mistake somewhere along the way? EDIT $ y = 3 * (3)^2 * \frac{1}{12} $ Resulting in: 2.25 inches cubed. (seems more reasonable).",,"['calculus', 'derivatives']"
27,Math question help here? Tangents,Math question help here? Tangents,,"Find $a$, $b$ and $c$ so the line $y=x$ can be a tangent of the parabola $y=ax^2+ bx+c$ at the point $x=1$. The parabola passes from the point $M(-1;0)$. So I formed the system $$2a+b=1$$ $$a-b+c=0$$ How do I solve this system? Details : From $y=x$ we see that $k=1$ (we also have that $x=1$) so $2\cdot a\cdot 1+b\cdot 1=1$","Find $a$, $b$ and $c$ so the line $y=x$ can be a tangent of the parabola $y=ax^2+ bx+c$ at the point $x=1$. The parabola passes from the point $M(-1;0)$. So I formed the system $$2a+b=1$$ $$a-b+c=0$$ How do I solve this system? Details : From $y=x$ we see that $k=1$ (we also have that $x=1$) so $2\cdot a\cdot 1+b\cdot 1=1$",,['derivatives']
28,"Prove $\sum_{n=1}^{\infty}f_n'(x)<\infty$ on $(0,1)$, when non-negative and increasing function $\lim_{x\to \infty}\sum_{n=1}^{\infty}f_n(x)<\infty$","Prove  on , when non-negative and increasing function","\sum_{n=1}^{\infty}f_n'(x)<\infty (0,1) \lim_{x\to \infty}\sum_{n=1}^{\infty}f_n(x)<\infty","When $f_n$ if non-negative and increasing on $(0,\ \infty)$ $$\lim_{x\to \infty}\sum_{n=1}^{\infty}f_n(x)<\infty$$ Prove that $$\sum_{n=1}^{\infty}f_n'(x)<\infty$$  on $(0,\ 1)$ a.e $[m]$. Is there the question means $f$ is differentiable? If so I will try mean value theorem. If not, I am totally stuck at the beginning, since $f$ is not mentioned absolutely continuous or f' belong to $L^1(m)$, I have no idea how to connect $f'$ and $f$ here.","When $f_n$ if non-negative and increasing on $(0,\ \infty)$ $$\lim_{x\to \infty}\sum_{n=1}^{\infty}f_n(x)<\infty$$ Prove that $$\sum_{n=1}^{\infty}f_n'(x)<\infty$$  on $(0,\ 1)$ a.e $[m]$. Is there the question means $f$ is differentiable? If so I will try mean value theorem. If not, I am totally stuck at the beginning, since $f$ is not mentioned absolutely continuous or f' belong to $L^1(m)$, I have no idea how to connect $f'$ and $f$ here.",,"['real-analysis', 'derivatives']"
29,The existence of second derivative,The existence of second derivative,,"Let $f(x)=|x|x$ then $f''(0)$ does not exist. Why? If $x>0$, $f'(x)=2x$ and if $x<0$, $f'(x)=-2x$. Then when $x=0$, does $f'(x)$ also not exist?","Let $f(x)=|x|x$ then $f''(0)$ does not exist. Why? If $x>0$, $f'(x)=2x$ and if $x<0$, $f'(x)=-2x$. Then when $x=0$, does $f'(x)$ also not exist?",,['derivatives']
30,Derivatives question involving tangent,Derivatives question involving tangent,,"Find the derivative of $2^{\tan(1/x)}$. I know that I should replace $\frac1x$ with $u$ and such, but then I can't continue it...","Find the derivative of $2^{\tan(1/x)}$. I know that I should replace $\frac1x$ with $u$ and such, but then I can't continue it...",,['derivatives']
31,What is the derivative of $f(x)=e^{f^{\prime \prime}}$,What is the derivative of,f(x)=e^{f^{\prime \prime}},"I made this problem: $f(x)=e^{f^{\prime \prime}}$ I have just been taught the first derivative, and was thinking about what if the derivative depended upon it own derivative.  I understand that $e^x$ is its ""own"" derivative, but the problem I made I was thinking that the first derviative is not logical, because to know the first derivative you then must know the 2nd or 3rd derviative, it seems self-referenecing. Is the problem I made, a real problem or just some abstract idea?","I made this problem: $f(x)=e^{f^{\prime \prime}}$ I have just been taught the first derivative, and was thinking about what if the derivative depended upon it own derivative.  I understand that $e^x$ is its ""own"" derivative, but the problem I made I was thinking that the first derviative is not logical, because to know the first derivative you then must know the 2nd or 3rd derviative, it seems self-referenecing. Is the problem I made, a real problem or just some abstract idea?",,"['calculus', 'derivatives']"
32,How to reduce the limit one gets when deriving the derivative of the general exponential function?,How to reduce the limit one gets when deriving the derivative of the general exponential function?,,"When applying the definition of a derivative to $\frac{d}{dx}b^x$ and a little algebra one arrives to $$b^x\times\lim\limits_{h \to 0}\frac{b^h - 1}{h}$$ where of course that limit equals $\ln(b)$. I know this limit can be evaluated with L'Hospitals rule, but that involves using the derivative what I am just about to prove, so that would be a circular proof. I suspect one has to reduce this limit to something that relates to the definition of e as $$\lim\limits_{n \to \infty}(1+1/n)^n$$ but I can not do it. How to show that that limit equals $\ln(b)$, so the proof of the derivative of $b^x$ is complete ? Note: I found that by substituting for $s = b^h-1$, etc, one can separate $\ln(b)$, but than the another indeterminate limit remains: $\lim\limits_{s \to 0}(s/\ln(1+s))$, which must be $1$, and again I do not want to solve it using L'Hospital's rule, but I cannot otherwise.","When applying the definition of a derivative to $\frac{d}{dx}b^x$ and a little algebra one arrives to $$b^x\times\lim\limits_{h \to 0}\frac{b^h - 1}{h}$$ where of course that limit equals $\ln(b)$. I know this limit can be evaluated with L'Hospitals rule, but that involves using the derivative what I am just about to prove, so that would be a circular proof. I suspect one has to reduce this limit to something that relates to the definition of e as $$\lim\limits_{n \to \infty}(1+1/n)^n$$ but I can not do it. How to show that that limit equals $\ln(b)$, so the proof of the derivative of $b^x$ is complete ? Note: I found that by substituting for $s = b^h-1$, etc, one can separate $\ln(b)$, but than the another indeterminate limit remains: $\lim\limits_{s \to 0}(s/\ln(1+s))$, which must be $1$, and again I do not want to solve it using L'Hospital's rule, but I cannot otherwise.",,"['calculus', 'limits', 'logarithms', 'derivatives']"
33,How to find the min and max of $y = x \sin(\ln|x|)$?,How to find the min and max of ?,y = x \sin(\ln|x|),"I'm trying to find the minimum and maximum points of the following equation: $y = x \sin(\ln|x|)$ where $x > 0$ which, when graphed, looks something like this: I tried deriving the equation using the chain rule: $$\begin{align}\frac{dy}{dx} &= \frac{d}{dx}\left(x \cdot \sin(\ln|x|)) \right) \\ & = \sin(\ln|x|) + x \cos(\ln|x|) \frac{1}{x} \\ & = \sin(\ln|x|) + \cos(\ln|x|) \end{align}$$ ...and setting it equal to zero: $$0 = \sin(\ln|x|) + \cos(\ln|x|)$$ I set $\ln|x|$ equal to $\frac{3\pi}{4}$ or $\frac{7\pi}{4}$ (because using the unit circle, using those two should cause the sin and cos to cancel out), so that $x = e^{{3\pi}/{4}}$ or $x = e^{{7\pi}/{4}}$, but both of those are clearly far more higher then the x-coordinates of the min and max points in the graph. How can I find the min and max x-coordinates of this equation? This is a homework problem I've been puzzling over for some time, so either hints or full answers would be appreciated.","I'm trying to find the minimum and maximum points of the following equation: $y = x \sin(\ln|x|)$ where $x > 0$ which, when graphed, looks something like this: I tried deriving the equation using the chain rule: $$\begin{align}\frac{dy}{dx} &= \frac{d}{dx}\left(x \cdot \sin(\ln|x|)) \right) \\ & = \sin(\ln|x|) + x \cos(\ln|x|) \frac{1}{x} \\ & = \sin(\ln|x|) + \cos(\ln|x|) \end{align}$$ ...and setting it equal to zero: $$0 = \sin(\ln|x|) + \cos(\ln|x|)$$ I set $\ln|x|$ equal to $\frac{3\pi}{4}$ or $\frac{7\pi}{4}$ (because using the unit circle, using those two should cause the sin and cos to cancel out), so that $x = e^{{3\pi}/{4}}$ or $x = e^{{7\pi}/{4}}$, but both of those are clearly far more higher then the x-coordinates of the min and max points in the graph. How can I find the min and max x-coordinates of this equation? This is a homework problem I've been puzzling over for some time, so either hints or full answers would be appreciated.",,"['calculus', 'derivatives']"
34,Vector derivative,Vector derivative,,"What's the derivative of $f(w)$ with respect to the vector $w$? $$f(w)=\mathrm{tr}(ww'A) + x^{\prime}ww'x$$ Note: $x,w$ are vectors and $A$ is a square matrix. ${}'$ indicates transpose Thanks.","What's the derivative of $f(w)$ with respect to the vector $w$? $$f(w)=\mathrm{tr}(ww'A) + x^{\prime}ww'x$$ Note: $x,w$ are vectors and $A$ is a square matrix. ${}'$ indicates transpose Thanks.",,"['calculus', 'matrices', 'derivatives']"
35,Finding Partial Derivatives,Finding Partial Derivatives,,"For the equation below, of Van der Waal form:   $$\left(P+\frac{n^{2}a}{V^{2}}\right)(V-nb)=nRT$$ Determine the partial derivatives; $\Bigl(\frac{\partial V}{\partial T}\Bigr)_{P,n}    \text{and }  \Bigl(\frac{\partial P}{\partial V}\Bigr)_{T,n}$ Where $a,b, n, R$ are constants. This is what I've done so far for one of the partial derivatives. I dont know what else to do from here, sorry. $$P=\frac{nRT}{V-nb}-\frac{n^{2}a}{V^{2}}$$ $$\frac{\partial P}{\partial V}=-\frac{nRT}{(V-nb)^2}+\frac{2n^{2}a}{V^{3}}$$","For the equation below, of Van der Waal form:   $$\left(P+\frac{n^{2}a}{V^{2}}\right)(V-nb)=nRT$$ Determine the partial derivatives; $\Bigl(\frac{\partial V}{\partial T}\Bigr)_{P,n}    \text{and }  \Bigl(\frac{\partial P}{\partial V}\Bigr)_{T,n}$ Where $a,b, n, R$ are constants. This is what I've done so far for one of the partial derivatives. I dont know what else to do from here, sorry. $$P=\frac{nRT}{V-nb}-\frac{n^{2}a}{V^{2}}$$ $$\frac{\partial P}{\partial V}=-\frac{nRT}{(V-nb)^2}+\frac{2n^{2}a}{V^{3}}$$",,"['calculus', 'derivatives']"
36,Multiple differentiable functions,Multiple differentiable functions,,"Check for what $n\in\mathbb{N}\cup \left\{+\infty\right\}$ $f\in C^n(\mathbb{R})$ (that is $f$ is $n$ times differentiable, I wasn't sure that this designation is common), if: a) $f(x)=|x|^m, \ m\in\mathbb{R}$ b) $f(x)= \begin{cases} e^{-1/x}, \ x>0 \\ 0, \ x\le 0 \end{cases} $ I wasn't given in school any convenient theorem to check such things but I really want to learn it. How can I check it?","Check for what (that is is times differentiable, I wasn't sure that this designation is common), if: a) b) I wasn't given in school any convenient theorem to check such things but I really want to learn it. How can I check it?","n\in\mathbb{N}\cup \left\{+\infty\right\} f\in C^n(\mathbb{R}) f n f(x)=|x|^m, \ m\in\mathbb{R} f(x)= \begin{cases} e^{-1/x}, \ x>0 \\ 0, \ x\le 0 \end{cases} ","['real-analysis', 'derivatives']"
37,Chain Rule for Multivariable Functions,Chain Rule for Multivariable Functions,,"I know this is probably quite basic but having trouble with the following question: Let f be a function of x and y , where x = cos(uv) and y = sin(uv) . Use the chain rule to show that: $\displaystyle \frac{\partial f}{\partial u} = v(x\frac{\partial f}{\partial y}-y\frac{\partial f}{\partial x})$ The problem I have is I get the following: $\displaystyle \frac{\partial f}{\partial x}=\frac{\partial f}{\partial u}\frac{\partial u}{\partial x} + \frac{\partial f}{\partial v}\frac{\partial v}{\partial x}$          (1) $\displaystyle \frac{\partial f}{\partial y}=\frac{\partial f}{\partial u}\frac{\partial u}{\partial y} + \frac{\partial f}{\partial v}\frac{\partial v}{\partial y}$          (2) $\therefore$ from (1): $\displaystyle \frac{\partial f}{\partial u}=\frac{\partial x}{\partial u}(\frac{\partial f}{\partial x} - \frac{\partial f}{\partial v}\frac{\partial v}{\partial x})$      (3) & $\displaystyle \frac{\partial f}{\partial v}=\frac{\partial y}{\partial v}(\frac{\partial f}{\partial y} - \frac{\partial f}{\partial u}\frac{\partial u}{\partial y})$      (4) Then: $\displaystyle \frac{\partial x}{\partial u} = -vsin(uv), \frac{\partial x}{\partial v} = -usin(uv), \frac{\partial y}{\partial u} = vcos(uv), \frac{\partial y}{\partial v} = ucos(uv)$ Plugging in the partial derivatives for x and y in to (3) I get: $\displaystyle \frac{\partial f}{\partial u}=-vy(\frac{\partial f}{\partial x} + \frac{1}{uy}\frac{\partial f}{\partial v})$ Doing the same with (4) I get: $\displaystyle \frac{\partial f}{\partial v}=ux(\frac{\partial f}{\partial y} - \frac{1}{vx}\frac{\partial f}{\partial u})$ After substituting for $\displaystyle \frac{\partial f}{\partial v}$ and some algebra, I get the following: $\displaystyle \frac{\partial f}{\partial u}=-vy\frac{\partial f}{\partial x} - vx\frac{\partial f}{\partial y} + \frac{\partial f}{\partial u}$ So I'm close but not quite there. Any thoughts? What silly mistake have I made? Thanks in advance.","I know this is probably quite basic but having trouble with the following question: Let f be a function of x and y , where x = cos(uv) and y = sin(uv) . Use the chain rule to show that: $\displaystyle \frac{\partial f}{\partial u} = v(x\frac{\partial f}{\partial y}-y\frac{\partial f}{\partial x})$ The problem I have is I get the following: $\displaystyle \frac{\partial f}{\partial x}=\frac{\partial f}{\partial u}\frac{\partial u}{\partial x} + \frac{\partial f}{\partial v}\frac{\partial v}{\partial x}$          (1) $\displaystyle \frac{\partial f}{\partial y}=\frac{\partial f}{\partial u}\frac{\partial u}{\partial y} + \frac{\partial f}{\partial v}\frac{\partial v}{\partial y}$          (2) $\therefore$ from (1): $\displaystyle \frac{\partial f}{\partial u}=\frac{\partial x}{\partial u}(\frac{\partial f}{\partial x} - \frac{\partial f}{\partial v}\frac{\partial v}{\partial x})$      (3) & $\displaystyle \frac{\partial f}{\partial v}=\frac{\partial y}{\partial v}(\frac{\partial f}{\partial y} - \frac{\partial f}{\partial u}\frac{\partial u}{\partial y})$      (4) Then: $\displaystyle \frac{\partial x}{\partial u} = -vsin(uv), \frac{\partial x}{\partial v} = -usin(uv), \frac{\partial y}{\partial u} = vcos(uv), \frac{\partial y}{\partial v} = ucos(uv)$ Plugging in the partial derivatives for x and y in to (3) I get: $\displaystyle \frac{\partial f}{\partial u}=-vy(\frac{\partial f}{\partial x} + \frac{1}{uy}\frac{\partial f}{\partial v})$ Doing the same with (4) I get: $\displaystyle \frac{\partial f}{\partial v}=ux(\frac{\partial f}{\partial y} - \frac{1}{vx}\frac{\partial f}{\partial u})$ After substituting for $\displaystyle \frac{\partial f}{\partial v}$ and some algebra, I get the following: $\displaystyle \frac{\partial f}{\partial u}=-vy\frac{\partial f}{\partial x} - vx\frac{\partial f}{\partial y} + \frac{\partial f}{\partial u}$ So I'm close but not quite there. Any thoughts? What silly mistake have I made? Thanks in advance.",,['derivatives']
38,Smoothing of absolute value and sign functions for numerical integration,Smoothing of absolute value and sign functions for numerical integration,,"I'm doing Numerical integration of ODEs. for a special system that has an always positive coordinate s and a conjugated momentum ps . At some point the equations become so stiff that s becomes negative and if I improve tolerances of the numerical integrator, it crashes. To solve this problem, I'd like to approximate the ODEs with s_new=abs(s) and ps_new=-ps around the critical region (for example if(s<1.0e-8) ). The problem is that these are not smooth. Numerical integrators (I use BDF) break down. One solution that I came across is to use arctan for abs and erf for sign . It was hard for me to find anything in literature on it. If you know of a better approach, please let me know.  Note, there is conservation of energy, so there exist f(s,ps) such that df/dt=0","I'm doing Numerical integration of ODEs. for a special system that has an always positive coordinate s and a conjugated momentum ps . At some point the equations become so stiff that s becomes negative and if I improve tolerances of the numerical integrator, it crashes. To solve this problem, I'd like to approximate the ODEs with s_new=abs(s) and ps_new=-ps around the critical region (for example if(s<1.0e-8) ). The problem is that these are not smooth. Numerical integrators (I use BDF) break down. One solution that I came across is to use arctan for abs and erf for sign . It was hard for me to find anything in literature on it. If you know of a better approach, please let me know.  Note, there is conservation of energy, so there exist f(s,ps) such that df/dt=0",,"['numerical-methods', 'integration', 'approximation', 'derivatives', 'absolute-value']"
39,Is this true about integrating composite functions?,Is this true about integrating composite functions?,,"Let's say that I'm integrating a composite function, say $f(g(x))$, that is in a form to which I can apply the substitution rule.  Is it true to say that both $f$ and $g$ must be differentiable? I understand that the substitution rule requires $g$ to be differentiable and that the substitution rule relies on the chain rule, and the chain rule requires both f and g to be differentiable.","Let's say that I'm integrating a composite function, say $f(g(x))$, that is in a form to which I can apply the substitution rule.  Is it true to say that both $f$ and $g$ must be differentiable? I understand that the substitution rule requires $g$ to be differentiable and that the substitution rule relies on the chain rule, and the chain rule requires both f and g to be differentiable.",,"['calculus', 'integration', 'limits', 'derivatives']"
40,Rewriting the second derivative of a function by substitution,Rewriting the second derivative of a function by substitution,,"I would like to know if the equation $$ \frac{d^2T(x)}{dx^2} = \frac{1}{2}\cdot\frac{d}{dT}\left(\frac{d}{dx}T(x)\right)^2\quad(1) $$ is true for a general function T(x). The function T(x) describes the temperature along a rod, so the function is smooth and its derivatives exist. Working through examples such as $$T(x) = \sin(x)\quad \text{or}\quad T(x) =\sqrt{a+x^2},$$ where it is easy to derive x(T), I can check that (1) is at least true in these cases. I tried to prove (1) by making the following substitution: $$ \frac{dT(x)}{dx} = u(x(T)). $$ Applying the chain-rule to the right hand side of (1) we get: $$ \frac{1}{2}\cdot\frac{d}{dT}\left(u(x(T))\right)^2 = \frac{1}{2}\cdot2\cdot u(x(T))\cdot\frac{d}{dx}u(x(T))\cdot\frac{d}{dT}x(T). $$ Canceling the 2's and substituting the original definition back in we get: $$ u(x(T))\cdot\frac{d}{dx}u(x(T))\cdot\frac{d}{dT}x(T) = \frac{dT(x)}{dx}\cdot\frac{d^2T(x)}{dx^2}\cdot\frac{dx(T)}{dT} =\frac{d^2T(x)}{dx^2}, $$ since $$ \frac{dT(x)}{dx}= \left(\frac{dx(T)}{dT}\right)^{-1}. $$ So evidently $$ \frac{d^2T(x)}{dx^2} = \frac{1}{2}\cdot\frac{d}{dT}\left(\frac{dT(x)}{dx}\right)^2. $$ Physicists (me) have a habit of bending mathematical definitions to suit their needs and to me, defining a function like $$ \frac{dT(x)}{dx} = u(x(T)) $$ feels wrong, because in a certain sense it is a recursive definition. So, is my proof valid? Or, is (1) in general false?","I would like to know if the equation is true for a general function T(x). The function T(x) describes the temperature along a rod, so the function is smooth and its derivatives exist. Working through examples such as where it is easy to derive x(T), I can check that (1) is at least true in these cases. I tried to prove (1) by making the following substitution: Applying the chain-rule to the right hand side of (1) we get: Canceling the 2's and substituting the original definition back in we get: since So evidently Physicists (me) have a habit of bending mathematical definitions to suit their needs and to me, defining a function like feels wrong, because in a certain sense it is a recursive definition. So, is my proof valid? Or, is (1) in general false?","
\frac{d^2T(x)}{dx^2} = \frac{1}{2}\cdot\frac{d}{dT}\left(\frac{d}{dx}T(x)\right)^2\quad(1)
 T(x) = \sin(x)\quad \text{or}\quad T(x) =\sqrt{a+x^2}, 
\frac{dT(x)}{dx} = u(x(T)).
 
\frac{1}{2}\cdot\frac{d}{dT}\left(u(x(T))\right)^2 = \frac{1}{2}\cdot2\cdot u(x(T))\cdot\frac{d}{dx}u(x(T))\cdot\frac{d}{dT}x(T).
 
u(x(T))\cdot\frac{d}{dx}u(x(T))\cdot\frac{d}{dT}x(T) = \frac{dT(x)}{dx}\cdot\frac{d^2T(x)}{dx^2}\cdot\frac{dx(T)}{dT} =\frac{d^2T(x)}{dx^2},
 
\frac{dT(x)}{dx}= \left(\frac{dx(T)}{dT}\right)^{-1}.
 
\frac{d^2T(x)}{dx^2} = \frac{1}{2}\cdot\frac{d}{dT}\left(\frac{dT(x)}{dx}\right)^2.
 
\frac{dT(x)}{dx} = u(x(T))
","['derivatives', 'substitution', 'chain-rule']"
41,Uniform Fréchet differentiability,Uniform Fréchet differentiability,,"Right now, I'm studying concepts of differentiation in Banach spaces, but I'm pretty new. In several references, I've found the following property: ""Let $U\subset X$ be an open convex subset of a Banach space $X$ and $f:U\rightarrow\mathbb{R}$ . Then $f$ is uniformly Fréchet differentiable if and only if $f$ is Fréchet differentiable and the map $x\mapsto f'(x)$ is uniformly continuous"". However, none of them proved it. They all say that it's trivial, but I can't figure this out. I've tried to remove the dependence of $x$ from $\delta$ in the $\impliedby$ direction and tried to bound by $\varepsilon$ the quantity $|f'(x)(h)-f'(y)(h)|$ when $\lVert x-y\rVert<\delta$ for the $\implies$ direction, but haven't reached anything. Could you guys help me? I'd be really thankful! See you!! :) Edit: My definition of Fréchet differentiability is Let $f$ be a real-valued function on an open subset $U$ of a Banach space $X$ and let $S$ be a subset of $U$ . We say that $f$ is uniformly Fréchet differentiable (UF) if it is Fr'echet differentiable at each $x \in S$ and the limit $\lim_{t\to 0}\frac{f(x+th)-f(x)}{t}$ is uniform in $x \in S$ and also in $h \in S_X$ ; more precisely, given any $\varepsilon > 0$ , there exists $\delta := \delta(\varepsilon) > 0$ , with the following property: for every $x \in S$ , $h \in S_X$ , and $0 < |t| < \delta$ with $x + th \in U$ , we have $\left| \frac{1}{t} (f(x + th) - f(x)) - f'(x)h \right| < \varepsilon$ . Definition of Fréchet differentiability","Right now, I'm studying concepts of differentiation in Banach spaces, but I'm pretty new. In several references, I've found the following property: ""Let be an open convex subset of a Banach space and . Then is uniformly Fréchet differentiable if and only if is Fréchet differentiable and the map is uniformly continuous"". However, none of them proved it. They all say that it's trivial, but I can't figure this out. I've tried to remove the dependence of from in the direction and tried to bound by the quantity when for the direction, but haven't reached anything. Could you guys help me? I'd be really thankful! See you!! :) Edit: My definition of Fréchet differentiability is Let be a real-valued function on an open subset of a Banach space and let be a subset of . We say that is uniformly Fréchet differentiable (UF) if it is Fr'echet differentiable at each and the limit is uniform in and also in ; more precisely, given any , there exists , with the following property: for every , , and with , we have . Definition of Fréchet differentiability",U\subset X X f:U\rightarrow\mathbb{R} f f x\mapsto f'(x) x \delta \impliedby \varepsilon |f'(x)(h)-f'(y)(h)| \lVert x-y\rVert<\delta \implies f U X S U f x \in S \lim_{t\to 0}\frac{f(x+th)-f(x)}{t} x \in S h \in S_X \varepsilon > 0 \delta := \delta(\varepsilon) > 0 x \in S h \in S_X 0 < |t| < \delta x + th \in U \left| \frac{1}{t} (f(x + th) - f(x)) - f'(x)h \right| < \varepsilon,"['functional-analysis', 'derivatives', 'banach-spaces', 'frechet-derivative']"
42,Hadamard product: derivatives,Hadamard product: derivatives,,"How to compute the partial derivatives of the following expression $$ \phi(x,y)=Ax\circ By, $$ with the Hadamard product $\circ$ , where $A(n-2,n)$ , $B(n-2,n)$ are the matrices $$ A=\left[\begin{array}{ccccccc} -0.5 & 0 & 0.5 & \cdots & 0 & 0 & 0\\ \vdots & \vdots & \vdots & \ddots & \vdots & \vdots & \vdots\\ 0 & 0 & 0 & \cdots & -0.5 & 0 & 0.5 \end{array}\right],\quad B=\left[\begin{array}{ccccccc} 1 & -2 & 1 & \cdots & 0 & 0 & 0\\ \vdots & \vdots & \vdots & \ddots & \vdots & \vdots & \vdots\\ 0 & 0 & 0 & \cdots & 1 & -2 & 1 \end{array}\right], $$ and $x(n,1)$ , $y(n,1)$ are the column vectors. Since $$ Ax\circ By=By\circ Ax, $$ the expression can be rewritten to the matrix form $$ \phi(x,y)=diag(By)Ax=diag(Ax)By, $$ with the partial derivatives \begin{align*} \frac{\partial\phi(x,y)}{\partial x} & =diag(By)A,\\ \frac{\partial\phi(x,y)}{\partial y} & =diag(Ax)B. \end{align*} I am not sure, whether the solution is correct. Thanks for your help. Updated computation: Consider $$ \phi_{i}(x,y)=0.5(-x_{i}+x_{i+2})(y_{i}-2y_{i+1}+y_{i+2}), $$ the partial derivatives have the form \begin{align*} \frac{\partial\phi_{i}(x,y)}{\partial x_{j}} & =-0.5(y_{j}-2y_{j+1}+y_{j+2}).\\ \frac{\partial\phi_{i}(x,y)}{\partial x_{j+1}} & =0,\\ \frac{\partial\phi_{i}(x,y)}{\partial x_{j+2}} & =0.5(y_{j}-2y_{j+1}+y_{j+2}),\\ \frac{\partial\phi_{i}(x,y)}{\partial x_{i+3}} & =0,\\ \frac{\partial\phi_{i}(x,y)}{\partial x_{i+3}} & =0,\\ \cdots & \cdots \end{align*} and represent elements of $A$ . Hence, the above-mentioned partial derivatives should be OK...","How to compute the partial derivatives of the following expression with the Hadamard product , where , are the matrices and , are the column vectors. Since the expression can be rewritten to the matrix form with the partial derivatives I am not sure, whether the solution is correct. Thanks for your help. Updated computation: Consider the partial derivatives have the form and represent elements of . Hence, the above-mentioned partial derivatives should be OK...","
\phi(x,y)=Ax\circ By,
 \circ A(n-2,n) B(n-2,n) 
A=\left[\begin{array}{ccccccc}
-0.5 & 0 & 0.5 & \cdots & 0 & 0 & 0\\
\vdots & \vdots & \vdots & \ddots & \vdots & \vdots & \vdots\\
0 & 0 & 0 & \cdots & -0.5 & 0 & 0.5
\end{array}\right],\quad B=\left[\begin{array}{ccccccc}
1 & -2 & 1 & \cdots & 0 & 0 & 0\\
\vdots & \vdots & \vdots & \ddots & \vdots & \vdots & \vdots\\
0 & 0 & 0 & \cdots & 1 & -2 & 1
\end{array}\right],
 x(n,1) y(n,1) 
Ax\circ By=By\circ Ax,
 
\phi(x,y)=diag(By)Ax=diag(Ax)By,
 \begin{align*}
\frac{\partial\phi(x,y)}{\partial x} & =diag(By)A,\\
\frac{\partial\phi(x,y)}{\partial y} & =diag(Ax)B.
\end{align*} 
\phi_{i}(x,y)=0.5(-x_{i}+x_{i+2})(y_{i}-2y_{i+1}+y_{i+2}),
 \begin{align*}
\frac{\partial\phi_{i}(x,y)}{\partial x_{j}} & =-0.5(y_{j}-2y_{j+1}+y_{j+2}).\\
\frac{\partial\phi_{i}(x,y)}{\partial x_{j+1}} & =0,\\
\frac{\partial\phi_{i}(x,y)}{\partial x_{j+2}} & =0.5(y_{j}-2y_{j+1}+y_{j+2}),\\
\frac{\partial\phi_{i}(x,y)}{\partial x_{i+3}} & =0,\\
\frac{\partial\phi_{i}(x,y)}{\partial x_{i+3}} & =0,\\
\cdots & \cdots
\end{align*} A","['calculus', 'derivatives', 'hadamard-product']"
43,Solve $\frac{dy}{dx}=\frac{2x^2-xy+y^2}{y-x}$.,Solve .,\frac{dy}{dx}=\frac{2x^2-xy+y^2}{y-x},"This problem gives me a lot of trouble. I am asked to solve the differential equation $$\frac{dy}{dx}=\frac{2x^2-xy+y^2}{y-x}.$$ Note that this is non-linear, non-homogeneous, non-exact, and inseparable for variables $x$ and $y$ . This is a problem on my final exam. I gave attempts to test for known form of equations and substitutions but they yield nothing.","This problem gives me a lot of trouble. I am asked to solve the differential equation Note that this is non-linear, non-homogeneous, non-exact, and inseparable for variables and . This is a problem on my final exam. I gave attempts to test for known form of equations and substitutions but they yield nothing.",\frac{dy}{dx}=\frac{2x^2-xy+y^2}{y-x}. x y,"['calculus', 'ordinary-differential-equations', 'derivatives']"
44,Wrongly counting the number of discontinuities of a function with an inbuilt limit,Wrongly counting the number of discontinuities of a function with an inbuilt limit,,"Plot the given function for $x>0$ : $$f(x)=\lim_{n \to ∞} \cos ^n \left(\dfrac{1}{n^x} \right)$$ My attempt: I substituted $n=1/h$ and took the logarithm on both sides; $$\ln f(x)=\lim_{h \to 0} \dfrac{\ln \cos (h^x)}{h}$$ Applying L'hopital's rule (the above limit is a $0/0$ form for all $x>0$ ), I obtained; $$\ln f(x)=\lim_{h \to 0}\dfrac{\sin(h^x).x.h^{x-1}}{\cos(h^x)}$$ Here, two cases arise; (I) $x>1$ and (II) $x<1$ (and $f(1)=1$ of course) In the first case, $f(x)=1$ . In the second case, we again obtain a $0/0$ form thus upon applying L'hopital's rule I got; $$\ln f(x)=\lim_{h \to 0} \dfrac{\cos(h^x).x^2h^{x-1}}{(x-1)h^{-x}}=0 \implies f(x)=1$$ So it appears as though the function is simply the constant function $f(x)=1$ . However, the answer reported is that $f(x)=0$ when $x<1/2$ , $f(x)=1$ when $x>1/2$ and $f(1/2)=e^{-1/2}$ . So, I made a second attempt using series expansions and recognizing that the original limit is of the form $1^∞$ . Then, I got the answer as reported. However, what is wrong with the method I used above? Why does L'hopital's rule not account for the discontinuity at $x=1/2$ ?","Plot the given function for : My attempt: I substituted and took the logarithm on both sides; Applying L'hopital's rule (the above limit is a form for all ), I obtained; Here, two cases arise; (I) and (II) (and of course) In the first case, . In the second case, we again obtain a form thus upon applying L'hopital's rule I got; So it appears as though the function is simply the constant function . However, the answer reported is that when , when and . So, I made a second attempt using series expansions and recognizing that the original limit is of the form . Then, I got the answer as reported. However, what is wrong with the method I used above? Why does L'hopital's rule not account for the discontinuity at ?",x>0 f(x)=\lim_{n \to ∞} \cos ^n \left(\dfrac{1}{n^x} \right) n=1/h \ln f(x)=\lim_{h \to 0} \dfrac{\ln \cos (h^x)}{h} 0/0 x>0 \ln f(x)=\lim_{h \to 0}\dfrac{\sin(h^x).x.h^{x-1}}{\cos(h^x)} x>1 x<1 f(1)=1 f(x)=1 0/0 \ln f(x)=\lim_{h \to 0} \dfrac{\cos(h^x).x^2h^{x-1}}{(x-1)h^{-x}}=0 \implies f(x)=1 f(x)=1 f(x)=0 x<1/2 f(x)=1 x>1/2 f(1/2)=e^{-1/2} 1^∞ x=1/2,"['calculus', 'limits', 'derivatives', 'piecewise-continuity']"
45,Second (and higher) derivatives of the adjugate operator for singular matrix,Second (and higher) derivatives of the adjugate operator for singular matrix,,"Consider the operator $$ f(A): A \rightarrow adj(A) $$ The derivative $Df_A (H)$ was given here and depends on the rank. I am interested in the second (and higher) derivatives in the $rank(A) = n-1$ case. Simply differentiating the resulting formula given in the linked answer does not work because the pseudo inverse is not differentiable at the singularity. Instead, I tried to derive the second derivative analogously as in the given answer by differentiating Jacobi's formula a second time and then taking the limit of $S_t$ for $t \rightarrow 0$ . However, I am getting stuck because I do not really understand how the linked answer went from equations $3$ , $4$ and $5$ to $6$ or if this approach really leads to the goal. In the end, I am interested in a procedure that can be implemented numerically efficiently, so directly working with Laplace's formula does not work.","Consider the operator The derivative was given here and depends on the rank. I am interested in the second (and higher) derivatives in the case. Simply differentiating the resulting formula given in the linked answer does not work because the pseudo inverse is not differentiable at the singularity. Instead, I tried to derive the second derivative analogously as in the given answer by differentiating Jacobi's formula a second time and then taking the limit of for . However, I am getting stuck because I do not really understand how the linked answer went from equations , and to or if this approach really leads to the goal. In the end, I am interested in a procedure that can be implemented numerically efficiently, so directly working with Laplace's formula does not work.","
f(A): A \rightarrow adj(A)
 Df_A (H) rank(A) = n-1 S_t t \rightarrow 0 3 4 5 6","['matrices', 'derivatives', 'determinant', 'matrix-calculus']"
46,A property of $C^1(\mathbb{R})$ functions,A property of  functions,C^1(\mathbb{R}),"Let $f\in C^1(\mathbb{R})$ and let $x_0\in P$ where $P$ is a perfect set. I want proof that $\forall \varepsilon>0$ $\exists\delta>0$ with $\lvert f(x)-f(y)-f'(y)(x-y) \rvert<\varepsilon\lvert  x-y\rvert$ $\forall x,y\in P$ , $\lvert x-x_0\rvert<\delta$ , $\lvert y-x_0\rvert<\delta$ . My attempt: let $\varepsilon>0$ . Because $F'$ is continous in $x_0$ , exists $\delta>0$ with $\lvert f'(x)-f'(x_0)\rvert<\dfrac{\varepsilon}{2}$ for all $x\in\mathbb{R}$ , $\lvert x-x_0\rvert<\delta$ . Let now $x,y\in P$ , $x\neq y$ with $\lvert x-x_0\rvert<\delta $ and $\lvert y-x_0\rvert<\delta$ . For MVT theorem exists $x'\in (x,y)\cup (y,x)$ with $f'(x')=\dfrac{f(y)-f(x)}{y-x}$ . Then $\lvert f(x)-f(y)-f'(y)(x-y)\rvert=\biggl\lvert \dfrac{f(x)-f(y)}{x-y}-f'(y)\biggr\rvert\lvert x-y\rvert=\lvert f'(x')-f'(y)\rvert\lvert x-y\rvert<[ \lvert f'(x')-f'(x_0)\rvert+\lvert f'(x_0)-f'(y)\rvert] \lvert x-y\rvert$ . I control $\lvert f'(x_0)-f'(y)\rvert<\dfrac{\varepsilon}{2}$ because $\lvert y-x_0\rvert<\delta$ . How i can control instead $\lvert f'(x')-f'(x_0)\rvert$ ? I have to distinguish position of $x,y,x_0,x'$ to evaluate $\lvert x'-x_0\rvert$ or the hypothesis that $P$ is perfect is relevant? Thanks in advance.","Let and let where is a perfect set. I want proof that with , , . My attempt: let . Because is continous in , exists with for all , . Let now , with and . For MVT theorem exists with . Then . I control because . How i can control instead ? I have to distinguish position of to evaluate or the hypothesis that is perfect is relevant? Thanks in advance.","f\in C^1(\mathbb{R}) x_0\in P P \forall \varepsilon>0 \exists\delta>0 \lvert f(x)-f(y)-f'(y)(x-y) \rvert<\varepsilon\lvert  x-y\rvert \forall x,y\in P \lvert x-x_0\rvert<\delta \lvert y-x_0\rvert<\delta \varepsilon>0 F' x_0 \delta>0 \lvert f'(x)-f'(x_0)\rvert<\dfrac{\varepsilon}{2} x\in\mathbb{R} \lvert x-x_0\rvert<\delta x,y\in P x\neq y \lvert x-x_0\rvert<\delta  \lvert y-x_0\rvert<\delta x'\in (x,y)\cup (y,x) f'(x')=\dfrac{f(y)-f(x)}{y-x} \lvert f(x)-f(y)-f'(y)(x-y)\rvert=\biggl\lvert \dfrac{f(x)-f(y)}{x-y}-f'(y)\biggr\rvert\lvert x-y\rvert=\lvert f'(x')-f'(y)\rvert\lvert x-y\rvert<[ \lvert f'(x')-f'(x_0)\rvert+\lvert f'(x_0)-f'(y)\rvert] \lvert x-y\rvert \lvert f'(x_0)-f'(y)\rvert<\dfrac{\varepsilon}{2} \lvert y-x_0\rvert<\delta \lvert f'(x')-f'(x_0)\rvert x,y,x_0,x' \lvert x'-x_0\rvert P","['real-analysis', 'derivatives', 'solution-verification', 'continuity']"
47,Extension of C1 functions,Extension of C1 functions,,"I studying the extension problem for classes of function $C^0,C^1,D^1$ ( $D^1=$ class of derivable function). For $C^0$ function there is Tietze theorem and for $D^1$ function there is Jarnik theorem (rediscovered by G. Petruska and M.Laczkovich-https://math.wvu.edu/~kciesiel/prepF/129.DifferentiableExtensionThm/129.DifferentiableExtensionThm.pdf). I pose attention on $C^1$ functions. I know that: exists a function $f\in C^1(X)$ on a perfect set $X ⊂ \mathbb{R}$ with no has extension $F \in C^1(\mathbb{R})$ . I'm looking for a necessary and sufficient condition for existence of a extension of $f\in C^1(X)$ where $X\subseteq\mathbb{R}$ is perfect. I have found a very general Whitney's theorem that i want adapte to monodimensional case. I ask: it's true that $f\in C^1(X)$ is extendible to $F\in C^1(\mathbb{R})$ iff $\forall \varepsilon>0$ $\exists\delta>0$ t.c. $\lvert  f(x)-f(y)-f'(y)(x-y)\rvert<\varepsilon\lvert  x-y\rvert$ $\forall x,y\in P$ , $\lvert x-y\rvert<\delta$ ? Is this correct? Can someone kindly show me a reference or an idea of ​​the proof (aslike Jarnik?) of this property?","I studying the extension problem for classes of function ( class of derivable function). For function there is Tietze theorem and for function there is Jarnik theorem (rediscovered by G. Petruska and M.Laczkovich-https://math.wvu.edu/~kciesiel/prepF/129.DifferentiableExtensionThm/129.DifferentiableExtensionThm.pdf). I pose attention on functions. I know that: exists a function on a perfect set with no has extension . I'm looking for a necessary and sufficient condition for existence of a extension of where is perfect. I have found a very general Whitney's theorem that i want adapte to monodimensional case. I ask: it's true that is extendible to iff t.c. , ? Is this correct? Can someone kindly show me a reference or an idea of ​​the proof (aslike Jarnik?) of this property?","C^0,C^1,D^1 D^1= C^0 D^1 C^1 f\in C^1(X) X ⊂ \mathbb{R} F \in C^1(\mathbb{R}) f\in C^1(X) X\subseteq\mathbb{R} f\in C^1(X) F\in C^1(\mathbb{R}) \forall \varepsilon>0 \exists\delta>0 \lvert  f(x)-f(y)-f'(y)(x-y)\rvert<\varepsilon\lvert  x-y\rvert \forall x,y\in P \lvert x-y\rvert<\delta","['real-analysis', 'derivatives', 'continuity', 'question-verification']"
48,What is $\sup\{a\}$ such that for continuous non differentiable function $f$ $\lim\limits_{h\to0}\frac{f(x+h)-f(x)}{h^a}$ exist?,What is  such that for continuous non differentiable function   exist?,\sup\{a\} f \lim\limits_{h\to0}\frac{f(x+h)-f(x)}{h^a},"Now asked on MO here The definition of the derivative of a function $f$ at a point $x$ is : $\lim\limits_{h \to 0 }\frac{f(x+h)  -f(x)}{h}$ but what if we change that $h$ to be any continuous function $g(h)$ that goes to $0$ as $h$ goes to $0$ ? Lets focus our attention at powers of $h$ so $g(h)=h^a$ for some $a$ . If $f$ is a differentiable function at $x$ any function other than $h$ the absolute value of the derivative will go to either $0$ or $\infty$ . I will refer to $\lim\limits_{h \to 0 }\frac{f(x+h) -f(x)}{g(h)}$ as $a-$ derivative. A question that came to my mind Given any continuous function nowhere differentiable on $\mathbb{R}$ is there a $g$ such that the absolute value of the $a-$ derivative"" exist almost everywhere and how to determine such functions ? The case where $g(x)$ is big enough to make this $a-$ derivative $0$ is annoying because one can say a very big number like $a=1/TREE(3) $ and say, for example, at this number the $a-$ derivative exist and it is $0$ for the Weierstrass function, so let me clear that : Given any continuous function nowhere differentiable what is the $\sup g(x)$ such that $\lim\limits_{h \to 0 }\frac{f(x+h)  -f(x)}{g(h)}= 0$ for all $x$ where the $\sup $ is taken of all real $a$ does the absolute value of the $a-$ derivative exist at that value ? Lets take L. van der Waerden's function for example Define $g(x)= |x|$ for $|x|\in [-1,1]$ , $g(x+2)=g(x)$ $$f(x)= \sum_{n \ge 1} \frac{3^n g\left(4^n x\right) }{4^n}$$ Is an example of continuous function nowhere differentiable, There is a proof that the absolute value secant line's slope goes to infinity as it approaches the point so this made me wonder will the   absolute value of the $a-$ derivative exist if we decrease the power of $h$ ?","Now asked on MO here The definition of the derivative of a function at a point is : but what if we change that to be any continuous function that goes to as goes to ? Lets focus our attention at powers of so for some . If is a differentiable function at any function other than the absolute value of the derivative will go to either or . I will refer to as derivative. A question that came to my mind Given any continuous function nowhere differentiable on is there a such that the absolute value of the derivative"" exist almost everywhere and how to determine such functions ? The case where is big enough to make this derivative is annoying because one can say a very big number like and say, for example, at this number the derivative exist and it is for the Weierstrass function, so let me clear that : Given any continuous function nowhere differentiable what is the such that for all where the is taken of all real does the absolute value of the derivative exist at that value ? Lets take L. van der Waerden's function for example Define for , Is an example of continuous function nowhere differentiable, There is a proof that the absolute value secant line's slope goes to infinity as it approaches the point so this made me wonder will the   absolute value of the derivative exist if we decrease the power of ?","f x \lim\limits_{h \to 0 }\frac{f(x+h)  -f(x)}{h} h g(h) 0 h 0 h g(h)=h^a a f x h 0 \infty \lim\limits_{h \to 0 }\frac{f(x+h) -f(x)}{g(h)} a- \mathbb{R} g a- g(x) a- 0 a=1/TREE(3)  a- 0 \sup g(x) \lim\limits_{h \to 0 }\frac{f(x+h)  -f(x)}{g(h)}= 0 x \sup  a a- g(x)= |x| |x|\in [-1,1] g(x+2)=g(x) f(x)= \sum_{n \ge 1} \frac{3^n g\left(4^n x\right) }{4^n} a- h","['real-analysis', 'limits', 'derivatives', 'continuity', 'conjectures']"
49,"If $f'' + f = 0$ and $f(0) = f'(0) = 0$, then $f = 0$.","If  and , then .",f'' + f = 0 f(0) = f'(0) = 0 f = 0,"Let $f \in D^{2}(\mathbb{R})$ verifying $f''+f = 0$ . Prove that if $f(0) = f'(0) = 0$ , then $f(x) = 0$ for all $x \in \mathbb{R}$ . Also, show that in any case $f(x) = f(0)\cos x + f'(0)\sin x$ . My attempt: Let $g:\mathbb{R} \to \mathbb{R}$ be defined by $g(x) = (f(x))^{2}+(f'(x))^{2}$ . Then $g \in D(\mathbb{R})$ and $$ g'(x) = 2f(x)f'(x) + 2f'(x)f''(x)\\       = 2f'(x)(f(x)+f''(x))\\       = 0, $$ for all $x \in \mathbb{R}$ . Therefore $g$ is constant, so there exists some $c\in\mathbb{R}$ such that $g(x) = c$ for all $x \in \mathbb{R}$ . In particular, $$ c = g(0) = (f(0))^{2}+(f'(0))^{2} = 0. $$ Thus, $$ (f(x))^{2}+(f'(x))^{2} = 0,\ \text{for all}\ x \in \mathbb{R}. $$ And this implies that $$ f(x) = 0 $$ for every $x \in \mathbb{R}$ . Now suppose that $f(0) = b$ and $f'(0) = a$ for some $a,\ b \in \mathbb{R}$ and $f'' + f = 0$ . Consider $h:\mathbb{R} \to \mathbb{R}$ defined by $h(x) = f(x) - a \sin x - b \cos x$ . Then $$ h'(x) = f'(x) - a \cos x + b \sin x,\\ h''(x) = f''(x) + a \sin x + b \cos x. $$ Hence, $$ h(x) + h''(x) = 0 $$ for every $x$ . Moreover, $$ h(0) = f(0) - a \sin 0 - b \cos 0 = b-b = 0,\ h'(0) = f'(0) - a \cos 0 + b \sin 0 = a-a = 0. $$ So, by the previous result $h = 0$ , which says that $$ f(x) = a \sin x + b \cos x = f'(0) \sin x + f(0) \cos x $$ for all $x \in \mathbb{R}.$ I´m not 100% sure if I can make the assumption that $f(0) =b,\ f'(0) = a$","Let verifying . Prove that if , then for all . Also, show that in any case . My attempt: Let be defined by . Then and for all . Therefore is constant, so there exists some such that for all . In particular, Thus, And this implies that for every . Now suppose that and for some and . Consider defined by . Then Hence, for every . Moreover, So, by the previous result , which says that for all I´m not 100% sure if I can make the assumption that","f \in D^{2}(\mathbb{R}) f''+f = 0 f(0) = f'(0) = 0 f(x) = 0 x \in \mathbb{R} f(x) = f(0)\cos x + f'(0)\sin x g:\mathbb{R} \to \mathbb{R} g(x) = (f(x))^{2}+(f'(x))^{2} g \in D(\mathbb{R}) 
g'(x) = 2f(x)f'(x) + 2f'(x)f''(x)\\
      = 2f'(x)(f(x)+f''(x))\\
      = 0,
 x \in \mathbb{R} g c\in\mathbb{R} g(x) = c x \in \mathbb{R} 
c = g(0) = (f(0))^{2}+(f'(0))^{2} = 0.
 
(f(x))^{2}+(f'(x))^{2} = 0,\ \text{for all}\ x \in \mathbb{R}.
 
f(x) = 0
 x \in \mathbb{R} f(0) = b f'(0) = a a,\ b \in \mathbb{R} f'' + f = 0 h:\mathbb{R} \to \mathbb{R} h(x) = f(x) - a \sin x - b \cos x 
h'(x) = f'(x) - a \cos x + b \sin x,\\
h''(x) = f''(x) + a \sin x + b \cos x.
 
h(x) + h''(x) = 0
 x 
h(0) = f(0) - a \sin 0 - b \cos 0 = b-b = 0,\ h'(0) = f'(0) - a \cos 0 + b \sin 0 = a-a = 0.
 h = 0 
f(x) = a \sin x + b \cos x = f'(0) \sin x + f(0) \cos x
 x \in \mathbb{R}. f(0) =b,\ f'(0) = a","['real-analysis', 'calculus', 'derivatives']"
50,"Criteria for the maximum of $f(x) = axe^{bx}$ at $(2,1)$",Criteria for the maximum of  at,"f(x) = axe^{bx} (2,1)","I have this question and have been trying for a long time to fully solve it, hopefully you guys can help me. Here is the question: Consider the function $f(x) = axe^{bx}$ , where $a$ and $b$ are constants and real numbers. Find the possible value(s) of $a$ and $b$ such that $f(x)$ has an absolute maximum of $(2, 1)$ on the interval $[0, 50]$ . Give your answer in exact form. I got one answer, but can't seem to prove or find out other possible answers. Here is my solution for one possible answer: $f(x) = axe^{bx}$ $1=f(2) =2ae^{2b}$ $ae^{2b}=\frac12$ $f'(x) = ae^{bx}(1+xb)$ $0=f'(2) = ae^{2b}(1+2b)=\frac12(1+2b)$ $b = -\frac12$ $ae^{2(-1/2)}=\frac12$ $a =\frac e2$ . So one of my possible answers is $a=\frac e2$ and $b=-\frac12$ . Now my only problem is the question asked for the possible value(s), so I don't know if there is more than one possible answer. Then question did specify on the interval from $[0,50]$ . So I don't know if there is a function that is greater in the negative, but has a maximum on the interval $[0,50]$ on the positive end, or sort of a U-shaped function where it has a maximum at $(2,1)$ goes below, but then possible goes back up after $x=50$ , which would still satisfy the criteria. Just looking for some help if I got the only answer, or if there are more answers. *Note: the question did say that $a$ and $b$ are constants and real numbers.","I have this question and have been trying for a long time to fully solve it, hopefully you guys can help me. Here is the question: Consider the function , where and are constants and real numbers. Find the possible value(s) of and such that has an absolute maximum of on the interval . Give your answer in exact form. I got one answer, but can't seem to prove or find out other possible answers. Here is my solution for one possible answer: . So one of my possible answers is and . Now my only problem is the question asked for the possible value(s), so I don't know if there is more than one possible answer. Then question did specify on the interval from . So I don't know if there is a function that is greater in the negative, but has a maximum on the interval on the positive end, or sort of a U-shaped function where it has a maximum at goes below, but then possible goes back up after , which would still satisfy the criteria. Just looking for some help if I got the only answer, or if there are more answers. *Note: the question did say that and are constants and real numbers.","f(x) = axe^{bx} a b a b f(x) (2, 1) [0, 50] f(x) = axe^{bx} 1=f(2) =2ae^{2b} ae^{2b}=\frac12 f'(x) = ae^{bx}(1+xb) 0=f'(2) = ae^{2b}(1+2b)=\frac12(1+2b) b = -\frac12 ae^{2(-1/2)}=\frac12 a =\frac e2 a=\frac e2 b=-\frac12 [0,50] [0,50] (2,1) x=50 a b","['calculus', 'derivatives', 'maxima-minima']"
51,On the definition of a differential equation with an initial value ON the boundary of an open intervall,On the definition of a differential equation with an initial value ON the boundary of an open intervall,,"In a textbook that I am reading the author defines a differential equation for values of $t>0$ and an initial condition for $t=0$ . As an example consider $$ y'(t) = 0 \enspace\text{for all } t\in\left]0,\infty\right[, \quad y(0)=1. $$ I don't fully understand how solutions to differential equations are even defined in this case. Clearly, the solution that one thinks of is $y(t)\equiv 1$ . But wouldn't the function $$ y(t)=\begin{cases}1, & \text{if }t=0, \\ 0, & \text{if }t>0 \end{cases} $$ satisfy this initial value problem, too? After all, it clearly has both required properties, right? In my eyes this kind of solution function should be excluded. Shouldn't the function and its derivative be defined on the same set? The way I see it the correct way to phrase the initial value problem  (i.e., the way that the author actually had in mind) should be $$ y'(t) = 0 \enspace\text{for all } t\in\left[0,\infty\right[, \quad y(0)=1. $$ This would force the solution to be continuous in $t=0$ because $y$ is then necessarily differentiable in $t=0$ . The derivative $y'(0)$ is simply, by definition*, the one-sided derivative. And if $y$ isn't continuous in $t=0$ , then this one-sided derivative does not exist. Am I correct? I have not really seen any textbooks that use this notation comment on this issue. Does this mean that for any differential equation on an open intervall one should demand that its initial value is given at a point inside of this domain? Any input is appreciated. * Many textbooks define differentiablity only for open sets to begin with. Some others (e.g. Rudin) choose the way that I have described above where the one-sided derivative is implemented ad hoc whenever a boundary point is part of the function's domain.","In a textbook that I am reading the author defines a differential equation for values of and an initial condition for . As an example consider I don't fully understand how solutions to differential equations are even defined in this case. Clearly, the solution that one thinks of is . But wouldn't the function satisfy this initial value problem, too? After all, it clearly has both required properties, right? In my eyes this kind of solution function should be excluded. Shouldn't the function and its derivative be defined on the same set? The way I see it the correct way to phrase the initial value problem  (i.e., the way that the author actually had in mind) should be This would force the solution to be continuous in because is then necessarily differentiable in . The derivative is simply, by definition*, the one-sided derivative. And if isn't continuous in , then this one-sided derivative does not exist. Am I correct? I have not really seen any textbooks that use this notation comment on this issue. Does this mean that for any differential equation on an open intervall one should demand that its initial value is given at a point inside of this domain? Any input is appreciated. * Many textbooks define differentiablity only for open sets to begin with. Some others (e.g. Rudin) choose the way that I have described above where the one-sided derivative is implemented ad hoc whenever a boundary point is part of the function's domain.","t>0 t=0 
y'(t) = 0 \enspace\text{for all } t\in\left]0,\infty\right[, \quad y(0)=1.
 y(t)\equiv 1 
y(t)=\begin{cases}1, & \text{if }t=0, \\ 0, & \text{if }t>0 \end{cases}
 
y'(t) = 0 \enspace\text{for all } t\in\left[0,\infty\right[, \quad y(0)=1.
 t=0 y t=0 y'(0) y t=0","['ordinary-differential-equations', 'derivatives', 'partial-differential-equations']"
52,Problem about inverse function and derivative,Problem about inverse function and derivative,,"Let $k$ be a real number. $$f(x)=x^3-3x^2+6x+k$$ and let $g(x)$ be the inverse function of $f(x)$ .  The equation $$4f'(x)+12x-18=(f'\circ g)(x)$$ has a real root on $[0, 1]$ . Find $m^2+M^2$ where $m$ is the minimum value of $k$ and $M$ is the maximum value of $k$ . My work: $$f'(x)=3x^2-6x+6\\\implies 4f'(x)+12x-18=12x^2-12x+6$$ Since $(g\circ f)(x)=x$ , plug $x=f(x)$ to the equation: $$12(f(x))^2-12f(x)+6=f'(x)$$ Now let $$h(x)=12(f(x))^2-12f(x)+6-f'(x)$$ then $h(0)h(1)\leq 0$ given that $h$ is continuous. $f(0)=k$ , $f(1)=k+4$ implies $h(0)=12k(k-1)$ , $h(1)=12k^2+84k+147=3(2k+7)^2$ . Thus I have $m=-7/2$ and $M=1$ . But the correct answer is $m=-8$ and $M=1$ .","Let be a real number. and let be the inverse function of .  The equation has a real root on . Find where is the minimum value of and is the maximum value of . My work: Since , plug to the equation: Now let then given that is continuous. , implies , . Thus I have and . But the correct answer is and .","k f(x)=x^3-3x^2+6x+k g(x) f(x) 4f'(x)+12x-18=(f'\circ g)(x) [0, 1] m^2+M^2 m k M k f'(x)=3x^2-6x+6\\\implies 4f'(x)+12x-18=12x^2-12x+6 (g\circ f)(x)=x x=f(x) 12(f(x))^2-12f(x)+6=f'(x) h(x)=12(f(x))^2-12f(x)+6-f'(x) h(0)h(1)\leq 0 h f(0)=k f(1)=k+4 h(0)=12k(k-1) h(1)=12k^2+84k+147=3(2k+7)^2 m=-7/2 M=1 m=-8 M=1","['calculus', 'derivatives', 'inverse-function']"
53,"The Area bounded by $y=e^x, 2y+2k-2kx-1-e^2=0$ is minimum for $ k=k_1$",The Area bounded by  is minimum for,"y=e^x, 2y+2k-2kx-1-e^2=0  k=k_1","The Area bounded by $y=e^x, 2y+2k-2kx-1-e^2=0$ is minimum for $ k=k_1$ ; then (where [.] denotes greatest integer function) A. $[2k_1]\gt5$ $\qquad$ B. $|2k_1-2\pi|\lt1$ $\qquad$ C. $k_1 \in \mathbb I$ $\qquad$ D. $k_1=\frac pq; p, q$ are integers $q\not=0,1, {-1}$ my attempt I had drawn the graphs of the exponential function and the the straightline and found that for $k\lt0$ we have $\infty$ area enclosed so $k$ must be $\gt0$ .  and also the line $$\frac y{\frac{e^2+1}2-k}+ {x\over1-\frac{e^2+1}{2k}}=1$$ passes through a fixed point $P\equiv\left(1,\frac{e^2+1}2\right)$ so as $k$ varies from $0$ to $\infty$ the line rotates about this fixed point. and this point P is above the graph of $y=e^x$ . I reached upto this I could not make any further conclusion from here. I had a thought to find the points of intersection in terms of $k$ then integrate but to find the points of intersection seems to be impossible. Graph of the question",The Area bounded by is minimum for ; then (where [.] denotes greatest integer function) A. B. C. D. are integers my attempt I had drawn the graphs of the exponential function and the the straightline and found that for we have area enclosed so must be .  and also the line passes through a fixed point so as varies from to the line rotates about this fixed point. and this point P is above the graph of . I reached upto this I could not make any further conclusion from here. I had a thought to find the points of intersection in terms of then integrate but to find the points of intersection seems to be impossible. Graph of the question,"y=e^x, 2y+2k-2kx-1-e^2=0  k=k_1 [2k_1]\gt5 \qquad |2k_1-2\pi|\lt1 \qquad k_1 \in \mathbb I \qquad k_1=\frac pq; p, q q\not=0,1, {-1} k\lt0 \infty k \gt0 \frac y{\frac{e^2+1}2-k}+ {x\over1-\frac{e^2+1}{2k}}=1 P\equiv\left(1,\frac{e^2+1}2\right) k 0 \infty y=e^x k","['calculus', 'integration', 'algebra-precalculus', 'derivatives', 'area']"
54,Frechet differential and implicit theorem,Frechet differential and implicit theorem,,"Given: $ \Omega \subseteq \mathbb{R}^n $ . $ f: \Omega \rightarrow \mathbb{R}^n $ and $ g: \Omega \rightarrow \mathcal{L}(\mathbb{R}^n, \mathbb{R}^n) $ are two $ C^1 $ -class functions on $ \Omega $ . Define $ F: \Omega \times \Omega \rightarrow \mathbb{R}^n $ as $ F(x, y) = g(x)(f(y)) + f(x) $ . (a) Show that $ F $ is of class $ C^1 $ on $ \Omega \times \Omega $ and find $ DF $ , $ D_1F $ , and $ D_2F $ . Now, let's proceed with the solution: To show that $ F $ is of class $ C^1 $ on $ \Omega \times \Omega $ , we need to demonstrate that ( F ) is continuously differentiable with respect to both variables ( x ) and ( y ). The function $ F(x, y) $ is composed of two parts: $ g(x)(f(y)) $ and $ f(x) $ . Since $ f $ and $ g $ are $ C^1 $ -class functions on $ \Omega $ , their compositions are also $ C^1 $ functions. Therefore, $ g(x)(f(y)) $ and $ f(x) $ are $ C^1 $ functions on $ \Omega \times \Omega $ . Now, let's compute the partial derivatives: $ D_1F = g'(x)f(y) + f'(x) $ $ D_2F = g(x)f'(y) $ I use this: $$ \Omega \rightarrow \mathcal{L}(\mathbb{R}^n, \mathbb{R}^n) \times \Omega \rightarrow \mathbb{R}^n $$ Where $$ x \mapsto (g(x), y) \mapsto g(x)y $$ Finally, the total derivative $ DF $ can be expressed as the matrix: $$ DF = \begin{pmatrix} D_1F \\ D_2F \end{pmatrix} = \begin{pmatrix} g'(x)f(y) + f'(x) \\ g(x)f'(y) \end{pmatrix} $$ Suppose $ 0 \in \Omega $ and $ f(0) = 0 $ . We want to find conditions on $ f $ and $ g $ for there to exist open neighborhoods $ U $ and $ V $ of $ 0 $ in $ \mathbb{R}^n $ and a function $ \Psi: U \rightarrow V $ of class $ C^1 $ on $ U $ such that $ F(x,y) = 0 $ if and only if $ y = \Psi(x) $ . Additionally,find $ D\Psi(0) $ . For this part, I know I need to examine the matrix $ D_2F $ and check if it's invertible at the point $ (0,0) $ to apply the implicit theorem. For this, I need both the matrix $ g(0) $ and the derivative $ f'(0) $ to be bijective. Can someone confirm this? I need help to find $ D\Psi(0) $ . Thank you.","Given: . and are two -class functions on . Define as . (a) Show that is of class on and find , , and . Now, let's proceed with the solution: To show that is of class on , we need to demonstrate that ( F ) is continuously differentiable with respect to both variables ( x ) and ( y ). The function is composed of two parts: and . Since and are -class functions on , their compositions are also functions. Therefore, and are functions on . Now, let's compute the partial derivatives: I use this: Where Finally, the total derivative can be expressed as the matrix: Suppose and . We want to find conditions on and for there to exist open neighborhoods and of in and a function of class on such that if and only if . Additionally,find . For this part, I know I need to examine the matrix and check if it's invertible at the point to apply the implicit theorem. For this, I need both the matrix and the derivative to be bijective. Can someone confirm this? I need help to find . Thank you."," \Omega \subseteq \mathbb{R}^n   f: \Omega \rightarrow \mathbb{R}^n   g: \Omega \rightarrow \mathcal{L}(\mathbb{R}^n, \mathbb{R}^n)   C^1   \Omega   F: \Omega \times \Omega \rightarrow \mathbb{R}^n   F(x, y) = g(x)(f(y)) + f(x)   F   C^1   \Omega \times \Omega   DF   D_1F   D_2F   F   C^1   \Omega \times \Omega   F(x, y)   g(x)(f(y))   f(x)   f   g   C^1   \Omega   C^1   g(x)(f(y))   f(x)   C^1   \Omega \times \Omega   D_1F = g'(x)f(y) + f'(x)   D_2F = g(x)f'(y)  
\Omega \rightarrow \mathcal{L}(\mathbb{R}^n, \mathbb{R}^n) \times \Omega \rightarrow \mathbb{R}^n
  x \mapsto (g(x), y) \mapsto g(x)y   DF   DF = \begin{pmatrix} D_1F \\ D_2F \end{pmatrix} = \begin{pmatrix} g'(x)f(y) + f'(x) \\ g(x)f'(y) \end{pmatrix}   0 \in \Omega   f(0) = 0   f   g   U   V   0   \mathbb{R}^n   \Psi: U \rightarrow V   C^1   U   F(x,y) = 0   y = \Psi(x)   D\Psi(0)   D_2F   (0,0)   g(0)   f'(0)   D\Psi(0) ","['derivatives', 'partial-derivative', 'banach-spaces', 'frechet-derivative', 'gateaux-derivative']"
55,Showing that a level set is not a submanifold,Showing that a level set is not a submanifold,,"Is there a criterion to show that a level set of some map is not an (embedded) submanifold? In particular, an exercise in Lee's smooth manifolds book asks to show that the sets defined by $x^3 - y^2 = 0$ and $x^2 - y^2 = 0$ are not embedded submanifolds. In general, is it possible that a level set of a map which does not has constant rank on the set still defines a embedded submanifold?","Is there a criterion to show that a level set of some map is not an (embedded) submanifold? In particular, an exercise in Lee's smooth manifolds book asks to show that the sets defined by $x^3 - y^2 = 0$ and $x^2 - y^2 = 0$ are not embedded submanifolds. In general, is it possible that a level set of a map which does not has constant rank on the set still defines a embedded submanifold?",,['differential-topology']
56,$nt$ derivative of $\displaystyle \frac{1}{A-\cos x}$,derivative of,nt \displaystyle \frac{1}{A-\cos x},"Assuming $A>1$ , I'm triying to obtain the $nt-$ derivative of $\displaystyle f(x)=\frac{1}{A-\cos x}$ at $x=0$ , that is, $f^{(n}(0)$ . First, $$f(x)=\frac{1}{A-\cos x}=\frac{1/A}{1-\frac{\cos x}{A}}=\frac{1}{A}\sum_{k=0}^{\infty}\frac{1}{A^k}\cos^k x$$ and then $$f^{(n}(0)=\sum_{k=0}^{\infty}\frac{1}{A^{k+1}}\,\left.\frac{d^n(\cos^k x)}{d\,x^n}\right|_{x=0}.$$ We have taht odd derivatives of $f(x)$ vanishes at $x=0$ and the even derivatives see $$\left.\frac{d^{2n}(\cos^k x)}{d\,x^{2n}}\right|_{x=0}=\frac{1}{2^k}\sum_{j=0}^k\binom{k}{j}(-1)^{n}(k-2j)^{2n}$$ Thus $$\boxed{f^{(2n}(0)=\frac{(-1)^{n}}{A}\sum_{k=0}^{\infty}\,\frac{1}{(2\,A)^k}\sum_{j=0}^k\binom{k}{j}(k-2j)^{2n}}$$ I wonder if there is some way to simplify this last expression or another procedure to obtain a simplest formula.","Assuming , I'm triying to obtain the derivative of at , that is, . First, and then We have taht odd derivatives of vanishes at and the even derivatives see Thus I wonder if there is some way to simplify this last expression or another procedure to obtain a simplest formula.","A>1 nt- \displaystyle f(x)=\frac{1}{A-\cos x} x=0 f^{(n}(0) f(x)=\frac{1}{A-\cos x}=\frac{1/A}{1-\frac{\cos x}{A}}=\frac{1}{A}\sum_{k=0}^{\infty}\frac{1}{A^k}\cos^k x f^{(n}(0)=\sum_{k=0}^{\infty}\frac{1}{A^{k+1}}\,\left.\frac{d^n(\cos^k x)}{d\,x^n}\right|_{x=0}. f(x) x=0 \left.\frac{d^{2n}(\cos^k x)}{d\,x^{2n}}\right|_{x=0}=\frac{1}{2^k}\sum_{j=0}^k\binom{k}{j}(-1)^{n}(k-2j)^{2n} \boxed{f^{(2n}(0)=\frac{(-1)^{n}}{A}\sum_{k=0}^{\infty}\,\frac{1}{(2\,A)^k}\sum_{j=0}^k\binom{k}{j}(k-2j)^{2n}}","['sequences-and-series', 'derivatives', 'taylor-expansion']"
57,Calculate the differentiate of $ Q: \mathbb{R}^n \rightarrow \mathbb{R}$ defined by $Q(x)=B(x;x)$,Calculate the differentiate of  defined by, Q: \mathbb{R}^n \rightarrow \mathbb{R} Q(x)=B(x;x),"I have a question on the differentiable of a function and I want to be sure that my understanding of this concept is correct. Question: Calculate the differentiate of $ Q: \mathbb{R}^n \rightarrow \mathbb{R}$ defined by $Q(x)=B(x;x)$ where $B: \mathbb{R}^n \times \mathbb{R}^n \rightarrow \mathbb{R} $ is bilinear. Answer: 1- It is given that $B$ is a bilinear mapping thus $B(x;x)$ is continuous and $ \forall x \in \mathbb{R}^n, \exists c > 0  $ s.t. $ |B(x;x) - B(0;0)| \leq c \| x -0 \| $ . And as $B(0;0)=0$ because $B$ is linear mapping the last inequality can be re write $ |B(x;x)| \leq c \| x\| $ 2- Now according to the definition; for any given point $x \in \mathbb{R}^n$ the differential in this point will be the linear mapping $L(h)$ verifying that $ \forall  h  \in \mathbb{R}^n $ , $Q(x+h)$ can be writen as follow: $ Q(x+h)= Q(x) + L(h) + O(h)$ with $O(h)$ verifying $ \lim_{h \to 0} || \frac{O(h)}{h}|| = 0 $ Rem: $h$ can not appear in the exepression $Q(x)$ but $x$ can appear in the expression of $L(h)$ and $O(h)$ . More over $h$ can appear in the expression of $L(h)$ only as a power of $1$ that means $h$ but not $h^2 , h^3,...$ 3- First by definition we can write $Q(x+h) = B(x+h;x+h)$ . After as $B(.;.)$ is a bilinear mapping (given) we can keep on and continue to write $ Q(x+h) = B(x+h;x+h) = B(x;x+h) + B(h;x+h)=B(x;x)+B(x;h)+ B(h;x+h) = B(x;x)+B(x;h)+ B(h;x) + B(h;h) $ Rem: We used the following property of bilinear mapping: $\phi(a+v;b)= \phi(a;b) + \phi(v;b) $ 4- First lets note that according to ""1-"" we have that if we take $O(h) = B(h;h)$ that $ \lim_{h \to 0} || \frac{O(h)}{h}|| = 0 $ . Now $L(h) = B(x;h)+ B(h;x) = D[Q(x)] h  $ is the differentiate of $Q(x)$ as it is a the sum of two linear mapping in $h$ by assumption. Is it correct? I have mostly a question on my part ""2-"" concerning my understanding of the differentiate definition. Thank you.","I have a question on the differentiable of a function and I want to be sure that my understanding of this concept is correct. Question: Calculate the differentiate of defined by where is bilinear. Answer: 1- It is given that is a bilinear mapping thus is continuous and s.t. . And as because is linear mapping the last inequality can be re write 2- Now according to the definition; for any given point the differential in this point will be the linear mapping verifying that , can be writen as follow: with verifying Rem: can not appear in the exepression but can appear in the expression of and . More over can appear in the expression of only as a power of that means but not 3- First by definition we can write . After as is a bilinear mapping (given) we can keep on and continue to write Rem: We used the following property of bilinear mapping: 4- First lets note that according to ""1-"" we have that if we take that . Now is the differentiate of as it is a the sum of two linear mapping in by assumption. Is it correct? I have mostly a question on my part ""2-"" concerning my understanding of the differentiate definition. Thank you."," Q: \mathbb{R}^n \rightarrow \mathbb{R} Q(x)=B(x;x) B: \mathbb{R}^n \times \mathbb{R}^n \rightarrow \mathbb{R}  B B(x;x)  \forall x \in \mathbb{R}^n, \exists c > 0    |B(x;x) - B(0;0)| \leq c \| x -0 \|  B(0;0)=0 B  |B(x;x)| \leq c \| x\|  x \in \mathbb{R}^n L(h)  \forall  h  \in \mathbb{R}^n  Q(x+h)  Q(x+h)= Q(x) + L(h) + O(h) O(h)  \lim_{h \to 0} || \frac{O(h)}{h}|| = 0  h Q(x) x L(h) O(h) h L(h) 1 h h^2 , h^3,... Q(x+h) = B(x+h;x+h) B(.;.)  Q(x+h) = B(x+h;x+h) = B(x;x+h) + B(h;x+h)=B(x;x)+B(x;h)+ B(h;x+h) = B(x;x)+B(x;h)+ B(h;x) + B(h;h)  \phi(a+v;b)= \phi(a;b) + \phi(v;b)  O(h) = B(h;h)  \lim_{h \to 0} || \frac{O(h)}{h}|| = 0  L(h) = B(x;h)+ B(h;x) = D[Q(x)] h   Q(x) h","['derivatives', 'solution-verification', 'differential-topology', 'differential']"
58,Are there any other functions which satisfy $\frac d{dx}f^2(x)=f(2x)$ other than $f(x)=\sin(x)$ and $f(x)=x$?,Are there any other functions which satisfy  other than  and ?,\frac d{dx}f^2(x)=f(2x) f(x)=\sin(x) f(x)=x,"I have been practicing functional equations recently and decided to do a functional equation that involves derivatives: $$\text{Find all functions }\mathbb R\to\mathbb R\text{ such that }\dfrac d{dx}f^2(x)=f(2x)$$ Now a solution that can be found right away is the identity function $I(x)=x$ , and the other solution I was able to find was $\sin(x)$ (which is done through the sine addition formula): $$\dfrac d{dx}\sin^2(x)=\dfrac d{dx}\sin(x)\sin(x)\\=\sin(x)\cos(x)+\sin(x)\cos(x)\\=2\sin(x)\cos(x)=\sin(2x)$$ however I have been unable to find other solutions, so my question is: Are there other solutions to the functional equation $\dfrac d{dx}f^2(x)$ , or are $f(x)=x$ and $f(x)=\sin(x)$ the only solutions?","I have been practicing functional equations recently and decided to do a functional equation that involves derivatives: Now a solution that can be found right away is the identity function , and the other solution I was able to find was (which is done through the sine addition formula): however I have been unable to find other solutions, so my question is: Are there other solutions to the functional equation , or are and the only solutions?",\text{Find all functions }\mathbb R\to\mathbb R\text{ such that }\dfrac d{dx}f^2(x)=f(2x) I(x)=x \sin(x) \dfrac d{dx}\sin^2(x)=\dfrac d{dx}\sin(x)\sin(x)\\=\sin(x)\cos(x)+\sin(x)\cos(x)\\=2\sin(x)\cos(x)=\sin(2x) \dfrac d{dx}f^2(x) f(x)=x f(x)=\sin(x),"['calculus', 'derivatives', 'functional-equations']"
59,The proof for every even degree derivative being divisible by the factorial of the degree,The proof for every even degree derivative being divisible by the factorial of the degree,,"So in the process of proving the Maclaurin series for $$\frac{1}{x^2+1}$$ I tried to do prove that for all odd degrees of the derivative, the numerator contains x in every number so when 0 is input into the function, the result is 0, so the summation results in only x's that contain even integer powers. How would I go about proving that for those given derivatives of an even degree, they always contain the factorial of the degree? eg. $$f^{6}(x)=(6!)\frac{(7x^6-35x^4+21x^2-1)}{(x^2+1)^7}$$ I've tried doing induction as below By assuming that $$\frac{d^{2k}}{dx^{2k}}(\frac{1}{x^2+1})=(2k)!f(x)$$ and assuming true for n=k+1 by saying $$\frac{d^{2k+2}}{dx^{2k+2}}(\frac{1}{x^2+1})=(2k+2)!g(x)$$ I've tried substituting $$(2k+2)!g(x)=(2k)!(2k+1)(2k+2)g(x)$$ with $$f''(x)$$ and ended with $$g(x)(2k+1)(2k+2)=f''(x)$$ but was not really able to do anything with it. Am I doing something wrong? Thanks.","So in the process of proving the Maclaurin series for I tried to do prove that for all odd degrees of the derivative, the numerator contains x in every number so when 0 is input into the function, the result is 0, so the summation results in only x's that contain even integer powers. How would I go about proving that for those given derivatives of an even degree, they always contain the factorial of the degree? eg. I've tried doing induction as below By assuming that and assuming true for n=k+1 by saying I've tried substituting with and ended with but was not really able to do anything with it. Am I doing something wrong? Thanks.",\frac{1}{x^2+1} f^{6}(x)=(6!)\frac{(7x^6-35x^4+21x^2-1)}{(x^2+1)^7} \frac{d^{2k}}{dx^{2k}}(\frac{1}{x^2+1})=(2k)!f(x) \frac{d^{2k+2}}{dx^{2k+2}}(\frac{1}{x^2+1})=(2k+2)!g(x) (2k+2)!g(x)=(2k)!(2k+1)(2k+2)g(x) f''(x) g(x)(2k+1)(2k+2)=f''(x),"['derivatives', 'taylor-expansion']"
60,Is this equivalent to the definition of directional derivative?,Is this equivalent to the definition of directional derivative?,,"Let $f=f(x_1,x_2)$ be a $C^1$ scalar valued function of two variables at the point $\vec{x}_0$ . We know the directional derivative of $f$ at $\vec{x}_0$ in the direction of $\vec{v}$ (unit vector) is given by $$ \lim_{h \to 0} \frac{f(\vec{x}_0+h\vec{v})-f(\vec{x}_0)}{h} = D_{\vec{v}}f(\vec{x}_0).$$ What can we say about the following limit: $$\lim_{h \to 0} \frac{f(\vec{x}_0+h\vec{v} + h^2 \vec{w})-f(\vec{x}_0)}{||h\vec{v} + h^2 \vec{w}||}$$ where $\vec{w}$ (unit vector, linearly independent from $\vec{v}$ ) is another direction ? Is this the same as $D_{\vec{v}}f(\vec{x}_0)$ ?","Let be a scalar valued function of two variables at the point . We know the directional derivative of at in the direction of (unit vector) is given by What can we say about the following limit: where (unit vector, linearly independent from ) is another direction ? Is this the same as ?","f=f(x_1,x_2) C^1 \vec{x}_0 f \vec{x}_0 \vec{v}  \lim_{h \to 0} \frac{f(\vec{x}_0+h\vec{v})-f(\vec{x}_0)}{h} = D_{\vec{v}}f(\vec{x}_0). \lim_{h \to 0} \frac{f(\vec{x}_0+h\vec{v} + h^2 \vec{w})-f(\vec{x}_0)}{||h\vec{v} + h^2 \vec{w}||} \vec{w} \vec{v} D_{\vec{v}}f(\vec{x}_0)","['derivatives', 'gateaux-derivative']"
61,A question regarding bounded variation and differentialibility as well as integration,A question regarding bounded variation and differentialibility as well as integration,,"Let $a, b \in \mathbb{R}$ with $a < b$ , and $f: [a, b] \to \mathbb{R}$ be a monotonically increasing, right-continuous function. Show that there exists a monotonically increasing function $g: [a, b] \to \mathbb{R}$ and a Lebesgue-null set $N$ such that $f$ and $g$ are differentiable on $[a, b] \setminus N$ , the derivative of $g$ is zero everywhere on $[a, b] \setminus N$ , and the derivative $f'$ satisfies $$f(x) - f(a) = \int_a^x f'(t) \,d\lambda(t) + g(x)$$ for all $x \in [a, b] \setminus N$ . My first approach: Let $N$ be the set of points of discontinuity of $f$ . Define $g: [a, b] \to \mathbb{R}$ as follows: $$g(x) = \lim_{{t \to x^+}} f'(t)$$ Note that $g(x)$ is well-defined for each $x$ because $f$ is right-continuous. Furthermore, since $f$ is monotonically increasing, $g(x)$ is monotonically increasing as well. Both $f$ and $g$ are differentiable on the set $[a, b] \setminus N$ , since we have basically cut out the ""bad"" points for $f$ and since $f$ is monotonically increasing on $[a,b]/N$ , it is also differentiable. The differentialibility of $g$ follows from its definition as a limit. Which is the same reasion the derivative of $g$ is zero almost everywhere. However, I am a little bit lost regarding this integral identity. How can I derive that one? I know the fundamental theorem of calculus gives me $$f(x) = f(a) + \int_{a}^{x} f'(t) d\lambda(t)$$ if $f$ is continuous. But how can I argue that adding the function $g$ on the right side ""compensates"" the missing condition of $f$ being continuous? And how does my first approach look like in general? Are there any mistakes which I made? Could anybody give me feedback on that?","Let with , and be a monotonically increasing, right-continuous function. Show that there exists a monotonically increasing function and a Lebesgue-null set such that and are differentiable on , the derivative of is zero everywhere on , and the derivative satisfies for all . My first approach: Let be the set of points of discontinuity of . Define as follows: Note that is well-defined for each because is right-continuous. Furthermore, since is monotonically increasing, is monotonically increasing as well. Both and are differentiable on the set , since we have basically cut out the ""bad"" points for and since is monotonically increasing on , it is also differentiable. The differentialibility of follows from its definition as a limit. Which is the same reasion the derivative of is zero almost everywhere. However, I am a little bit lost regarding this integral identity. How can I derive that one? I know the fundamental theorem of calculus gives me if is continuous. But how can I argue that adding the function on the right side ""compensates"" the missing condition of being continuous? And how does my first approach look like in general? Are there any mistakes which I made? Could anybody give me feedback on that?","a, b \in \mathbb{R} a < b f: [a, b] \to \mathbb{R} g: [a, b] \to \mathbb{R} N f g [a, b] \setminus N g [a, b] \setminus N f' f(x) - f(a) = \int_a^x f'(t) \,d\lambda(t) + g(x) x \in [a, b] \setminus N N f g: [a, b] \to \mathbb{R} g(x) = \lim_{{t \to x^+}} f'(t) g(x) x f f g(x) f g [a, b] \setminus N f f [a,b]/N g g f(x) = f(a) + \int_{a}^{x} f'(t) d\lambda(t) f g f","['integration', 'measure-theory', 'derivatives', 'bounded-variation']"
62,Is open set and interiority a necessary condition for differentiability?,Is open set and interiority a necessary condition for differentiability?,,"I am going by what I am seeing on Wikipedia:  In 1D, the standard definition for differentiability is, A function $f:U\to\mathbb{R}$ , defined on an open set $U\subset\mathbb{R}$ , is said to be ''differentiable'' at $a\in U$ if the derivative $$f'(a)=\lim_{h\to0}\frac{f(a+h)-f(a)}{h}$$ exists. Other times, $a$ is defined to be a point which is in the interior of a not necessarily open domain. However, when I read some other references such as Terence Tao's Analysis I, this assumption does not seem to exist and I don't think it is a mistake or an omission. Can someone help me understand whether openness and interiority are necessary in order to define differentiability?","I am going by what I am seeing on Wikipedia:  In 1D, the standard definition for differentiability is, A function , defined on an open set , is said to be ''differentiable'' at if the derivative exists. Other times, is defined to be a point which is in the interior of a not necessarily open domain. However, when I read some other references such as Terence Tao's Analysis I, this assumption does not seem to exist and I don't think it is a mistake or an omission. Can someone help me understand whether openness and interiority are necessary in order to define differentiability?",f:U\to\mathbb{R} U\subset\mathbb{R} a\in U f'(a)=\lim_{h\to0}\frac{f(a+h)-f(a)}{h} a,"['real-analysis', 'derivatives', 'soft-question', 'definition', 'jacobian']"
63,Integrate of diferential of modulus of a complex function.,Integrate of diferential of modulus of a complex function.,,"Let $u(x,t):\mathbb{R}^2\rightarrow\mathbb{C}$ be a diferentiable function square integrable. So I want to calculate $$\int_{-\infty}^\infty \frac{d}{dt}|u|^2dx.$$ I know that the diferential of the modulus is not well defined in that case, but if we define $|u|^2=u\bar{u}$ we have: $$\int_{-\infty}^\infty \frac{d}{dt}(u\bar{u})=\int_{-\infty}^{\infty}u_t\bar{u}+u\bar{u_t}dx,$$ my questions is: why is this well defined (many books use that)? If $u$ in infinity goes to zero I can use integration by parts to conclude that: $$\int_{-\infty}^{\infty}u_t\bar{u}+u\bar{u_t}dx=\int_{-\infty}^{\infty}u_t\bar{u}-u_t\bar{u}dx=0?$$","Let be a diferentiable function square integrable. So I want to calculate I know that the diferential of the modulus is not well defined in that case, but if we define we have: my questions is: why is this well defined (many books use that)? If in infinity goes to zero I can use integration by parts to conclude that:","u(x,t):\mathbb{R}^2\rightarrow\mathbb{C} \int_{-\infty}^\infty \frac{d}{dt}|u|^2dx. |u|^2=u\bar{u} \int_{-\infty}^\infty \frac{d}{dt}(u\bar{u})=\int_{-\infty}^{\infty}u_t\bar{u}+u\bar{u_t}dx, u \int_{-\infty}^{\infty}u_t\bar{u}+u\bar{u_t}dx=\int_{-\infty}^{\infty}u_t\bar{u}-u_t\bar{u}dx=0?","['integration', 'derivatives']"
64,Understanding Gateaux derivatives,Understanding Gateaux derivatives,,"Background I'm studying Gateaux derivatives and I find some difficulties to underestand the general case where the order of the derivative is $n>1$ . First, I'm considering the following definition for the first-order derivative at $h(x):\mathbb{R}^d\mapsto \mathbb{R}$ and in direction $g(x):\mathbb{R}^d\mapsto\mathbb{R}$ of the generic functional $F[h]$ , \begin{equation*} F'[h;g]\triangleq \left[\frac{\text{d}}{\text{d}\varepsilon}F[h+\varepsilon g]\right]_{\varepsilon=0} \end{equation*} we can simplfy this expression by removing the variable $\varepsilon$ (since in the end it will be fixed to the value $\varepsilon=0$ ). Since the internal derivative is a ""conventional"" derivative in $\varepsilon$ , we have \begin{equation*} F'[h;g]=\left[\lim_{\tau\to 0} \frac{F[h+(\varepsilon+\tau)g]-F[h+\varepsilon g]}{\tau}\right]_{\varepsilon=0} \end{equation*} then setting $\varepsilon=0$ gives \begin{equation*} F'[h;g]=\lim_{\tau\to 0} \frac{F[h+\tau g]-F[h]}{\tau} \end{equation*} Questions The $n$ -th order derivative is defined as \begin{equation*} F^{(n)}[h;g]\triangleq \left[\frac{\text{d}^n}{\text{d}\varepsilon^n}F[h+\varepsilon g]\right]_{\varepsilon=0} \end{equation*} Currently I have two questions about this expression. I'm wondering if we can do, like in the previous case, a simplification where we eliminate $\varepsilon$ I'm wondering if, as it happens with conventional derivatives, we can write a recursion of the type \begin{equation*} F^{(k+1)}[h;g]=F^{(k)}[h;g] \end{equation*} If the answer of the previous questions is yes, how do you prove it? It will be enough the simple case with $n=2$ .","Background I'm studying Gateaux derivatives and I find some difficulties to underestand the general case where the order of the derivative is . First, I'm considering the following definition for the first-order derivative at and in direction of the generic functional , we can simplfy this expression by removing the variable (since in the end it will be fixed to the value ). Since the internal derivative is a ""conventional"" derivative in , we have then setting gives Questions The -th order derivative is defined as Currently I have two questions about this expression. I'm wondering if we can do, like in the previous case, a simplification where we eliminate I'm wondering if, as it happens with conventional derivatives, we can write a recursion of the type If the answer of the previous questions is yes, how do you prove it? It will be enough the simple case with .","n>1 h(x):\mathbb{R}^d\mapsto \mathbb{R} g(x):\mathbb{R}^d\mapsto\mathbb{R} F[h] \begin{equation*}
F'[h;g]\triangleq \left[\frac{\text{d}}{\text{d}\varepsilon}F[h+\varepsilon g]\right]_{\varepsilon=0}
\end{equation*} \varepsilon \varepsilon=0 \varepsilon \begin{equation*}
F'[h;g]=\left[\lim_{\tau\to 0} \frac{F[h+(\varepsilon+\tau)g]-F[h+\varepsilon g]}{\tau}\right]_{\varepsilon=0}
\end{equation*} \varepsilon=0 \begin{equation*}
F'[h;g]=\lim_{\tau\to 0} \frac{F[h+\tau g]-F[h]}{\tau}
\end{equation*} n \begin{equation*}
F^{(n)}[h;g]\triangleq \left[\frac{\text{d}^n}{\text{d}\varepsilon^n}F[h+\varepsilon g]\right]_{\varepsilon=0}
\end{equation*} \varepsilon \begin{equation*}
F^{(k+1)}[h;g]=F^{(k)}[h;g]
\end{equation*} n=2","['calculus', 'derivatives', 'gateaux-derivative']"
65,Question regarding Mean value theorem and a monotonically increasing function $f'$,Question regarding Mean value theorem and a monotonically increasing function,f',"I was given the following information about a function: Let $f$ : [0, $\infty$ ) be a function so that $f(0)=0$ $f$ is continuous on [0, $\infty$ ) $f$ is differentiable on (0, $\infty$ ) the function $f'$ : [0, $\infty$ ) is monotonically increasing Let $x, y \in (0, \infty)$ where $x < y$ . We know that f is differentiable on (0,x) and (0,y) as well. By Mean Value Theorem there exists a $c \in (0, x)$ such that $f'(c) = \frac{f(x) - f(0)}{x}$ = $\frac{f(x)}{x}$ . And, there exists a $d \in (0, y)$ such that $f'(d) = \frac{f(y) - f(0)}{y}$ = $\frac{f(y)}{y}$ . I was confused with the values $c,d$ I defined, Since $x<y$ does this mean that $c<d$ ? I have no idea if this is true but I want to know if theres any sort of relationship between $c$ and $d$ using this function $f$ ?","I was given the following information about a function: Let : [0, ) be a function so that is continuous on [0, ) is differentiable on (0, ) the function : [0, ) is monotonically increasing Let where . We know that f is differentiable on (0,x) and (0,y) as well. By Mean Value Theorem there exists a such that = . And, there exists a such that = . I was confused with the values I defined, Since does this mean that ? I have no idea if this is true but I want to know if theres any sort of relationship between and using this function ?","f \infty f(0)=0 f \infty f \infty f' \infty x, y \in (0, \infty) x < y c \in (0, x) f'(c) = \frac{f(x) - f(0)}{x} \frac{f(x)}{x} d \in (0, y) f'(d) = \frac{f(y) - f(0)}{y} \frac{f(y)}{y} c,d x<y c<d c d f","['real-analysis', 'derivatives', 'mean-value-theorem']"
66,Prove that $f'(a)=\lim_{x\rightarrow a}f'(x)$.,Prove that .,f'(a)=\lim_{x\rightarrow a}f'(x),"Let $f$ be a real-valued function continuous on $[a,b]$ and differentiable on $(a,b)$. Suppose that $\lim_{x\rightarrow a}f'(x)$ exists. Then, prove that $f$ is differentiable at $a$ and $f'(a)=\lim_{x\rightarrow a}f'(x)$. It seems like an easy example, but a little bit tricky. I'm not sure which theorems should be used in here. ============================================================== Using @David Mitra's advice and @Pete L. Clark's notes I tried to solve this proof. I want to know my proof is correct or not. By MVT, for $h>0$ and $c_h \in (a,a+h)$ $$\frac{f(a+h)-f(a)}{h}=f'(c_h)$$ and $\lim_{h \rightarrow 0^+}c_h=a$. Then $$\lim_{h \rightarrow 0^+}\frac{f(a+h)-f(a)}{h}=\lim_{h \rightarrow 0^+}f'(c_h)=\lim_{h \rightarrow 0^+}f'(a)$$ But that's enough? I think I should show something more, but don't know what it is.","Let $f$ be a real-valued function continuous on $[a,b]$ and differentiable on $(a,b)$. Suppose that $\lim_{x\rightarrow a}f'(x)$ exists. Then, prove that $f$ is differentiable at $a$ and $f'(a)=\lim_{x\rightarrow a}f'(x)$. It seems like an easy example, but a little bit tricky. I'm not sure which theorems should be used in here. ============================================================== Using @David Mitra's advice and @Pete L. Clark's notes I tried to solve this proof. I want to know my proof is correct or not. By MVT, for $h>0$ and $c_h \in (a,a+h)$ $$\frac{f(a+h)-f(a)}{h}=f'(c_h)$$ and $\lim_{h \rightarrow 0^+}c_h=a$. Then $$\lim_{h \rightarrow 0^+}\frac{f(a+h)-f(a)}{h}=\lim_{h \rightarrow 0^+}f'(c_h)=\lim_{h \rightarrow 0^+}f'(a)$$ But that's enough? I think I should show something more, but don't know what it is.",,"['real-analysis', 'limits', 'derivatives', 'continuity']"
67,Can we say that $x=0$ is a double root of $f(x) = (e^x-1)(\ln( x+1))$?,Can we say that  is a double root of ?,x=0 f(x) = (e^x-1)(\ln( x+1)),"Let $$f(x) = (e^x-1)(\ln( x+1))$$ So far , I've only seen examples of textbooks referencing repeated roots if we can write the function with linear factors raised to some natural exponent ( like if $a$ is a repeated root , then we can write the function as $(x-a)^{m+1}g(x), m\in\mathbb N$ ). But in above example, the factors are not linear polynomials. So , Is it correct to say that $x=0$ is a repeated root of $f$ or not? Context: I'm confirming the terminology because currently I'm studying derivatives. My teacher said that functions having repeated roots are always differentiable at that point with derivative equal to $0$ . That's why I asked question whether above type of functions come into that category so that the property of differentiability could be extended.","Let So far , I've only seen examples of textbooks referencing repeated roots if we can write the function with linear factors raised to some natural exponent ( like if is a repeated root , then we can write the function as ). But in above example, the factors are not linear polynomials. So , Is it correct to say that is a repeated root of or not? Context: I'm confirming the terminology because currently I'm studying derivatives. My teacher said that functions having repeated roots are always differentiable at that point with derivative equal to . That's why I asked question whether above type of functions come into that category so that the property of differentiability could be extended.","f(x) = (e^x-1)(\ln( x+1)) a (x-a)^{m+1}g(x), m\in\mathbb N x=0 f 0","['calculus', 'derivatives', 'soft-question', 'terminology', 'roots']"
68,"Geometric interpretation of $\frac {\partial^2} {\partial x \partial y} f(x,y)$",Geometric interpretation of,"\frac {\partial^2} {\partial x \partial y} f(x,y)","Is there any geometric interpretation for the following second partial derivative? $$f_{xy} = \frac {\partial^2 f} {\partial x \partial y}$$ In particular, I'm trying to understand the determinant from second partial derivative test for determining whether a critical point is a minima/maxima/saddle points: $$D(a, b) = f_{xx}(a,b) f_{yy}(a,b) - f_{xy}(a,b)^2$$ I have no trouble understanding $f_{xx}(x,y)$ and $f_{yy}(x,y)$ as the of measure of concavity/convexity of $f$ in the direction of $x$ and $y$ axis. But what does $f_{xy}(x,y)$ mean?","Is there any geometric interpretation for the following second partial derivative? In particular, I'm trying to understand the determinant from second partial derivative test for determining whether a critical point is a minima/maxima/saddle points: I have no trouble understanding and as the of measure of concavity/convexity of in the direction of and axis. But what does mean?","f_{xy} = \frac {\partial^2 f} {\partial x \partial y} D(a, b) = f_{xx}(a,b) f_{yy}(a,b) - f_{xy}(a,b)^2 f_{xx}(x,y) f_{yy}(x,y) f x y f_{xy}(x,y)","['multivariable-calculus', 'partial-derivative', 'hessian-matrix', 'scalar-fields', 'geometric-interpretation']"
69,Need Help Understanding a Differential Equation Problem,Need Help Understanding a Differential Equation Problem,,"I hope you're doing well. I'm a first-year mathematics student in a French preparatory program, and I'm currently working on a challenging exercise involving differential equations. I've made some progress, but I'm stuck. The exercise is as follows: Consider finding all functions $f: (1, +\infty) \rightarrow (1, +\infty)$ , differentiable, such that for every $x$ belonging to $(1, +\infty)$ , the following equation holds: $ (x-1)f'(x) = (x-1)f(x)^2 + (2-x^2)f(x) + x^2 - x - 1 $ Next, find a constant solution $K$ . Then, define $h(x) = f(x) - K$ . Determine the differential equation satisfied by $h$ . Let $g(x) = \frac{1}{h(x)}$ . Find the differential equation satisfied by $g$ . Finally, draw conclusions based on the analysis. I managed to find one constant solution $ K = 1 $ , but I'm struggling to understand how to proceed further. The next steps involve defining a new function $ h(x) = f(x) - K $ and determining the differential equation satisfied by $ h $ . After that, I'm supposed to define $ g(x) = \frac{1}{h(x)} $ and find the differential equation satisfied by $ g $ . If anyone could offer some guidance or insight into how to approach this problem, I would greatly appreciate it. I'm eager to learn and understand the steps involved in solving this type of problem. Thank you in advance for your help!","I hope you're doing well. I'm a first-year mathematics student in a French preparatory program, and I'm currently working on a challenging exercise involving differential equations. I've made some progress, but I'm stuck. The exercise is as follows: Consider finding all functions , differentiable, such that for every belonging to , the following equation holds: Next, find a constant solution . Then, define . Determine the differential equation satisfied by . Let . Find the differential equation satisfied by . Finally, draw conclusions based on the analysis. I managed to find one constant solution , but I'm struggling to understand how to proceed further. The next steps involve defining a new function and determining the differential equation satisfied by . After that, I'm supposed to define and find the differential equation satisfied by . If anyone could offer some guidance or insight into how to approach this problem, I would greatly appreciate it. I'm eager to learn and understand the steps involved in solving this type of problem. Thank you in advance for your help!","f: (1, +\infty) \rightarrow (1, +\infty) x (1, +\infty) 
(x-1)f'(x) = (x-1)f(x)^2 + (2-x^2)f(x) + x^2 - x - 1
 K h(x) = f(x) - K h g(x) = \frac{1}{h(x)} g  K = 1   h(x) = f(x) - K   h   g(x) = \frac{1}{h(x)}   g ","['calculus', 'ordinary-differential-equations', 'derivatives']"
70,Derivative of $X^{-1}AX^{-1}$ w.r.t. X,Derivative of  w.r.t. X,X^{-1}AX^{-1},"I want to calculate the derivative of $X^{-1}AX^{-1}$ w.r.t. X in a simple form Let \begin{align} AX^{-1}&=Y\\ X^{-1}A&=Z\\ dX^{-1} &= -X^{-1}dXX^{-1}\\ \text{vec}(dX^{-1}) &= -(X^{-T} \otimes X^{-1})\text{vec}(dX), \end{align} were $\text{vec}$ is the vectorization operator. We have \begin{align} F &= X^{-1}AX^{-1}\\ \implies dF & = d(X^{-1})AX^{-1} + X^{-1}Ad(X^{-1})\\ \implies dF & = d(X^{-1})Y + Zd(X^{-1})\\ \implies \text{vec}(dF) & = (Y^T \otimes I)\text{vec}(dX^{-1}) + (I \otimes Z) \text{vec} (dX^{-1})\\ \implies \text{vec}(dF) & = (Y^T \otimes I)\text{vec}(dX^{-1}) + (I \otimes Z) \text{vec}(dX^{-1})\\ & = -[(Y^T \otimes I) + (I \otimes Z)](X^{-T} \otimes X^{-1}) \text{vec}(dX)\\ & = -[((AX^{-1})^T \otimes I) + (I \otimes X^{-1}A)](X^{-T} \otimes X^{-1}) \text{vec}(dX)\\ \end{align} Can we obtain a simple formula of this result? Thank you.",I want to calculate the derivative of w.r.t. X in a simple form Let were is the vectorization operator. We have Can we obtain a simple formula of this result? Thank you.,"X^{-1}AX^{-1} \begin{align}
AX^{-1}&=Y\\
X^{-1}A&=Z\\
dX^{-1} &= -X^{-1}dXX^{-1}\\
\text{vec}(dX^{-1}) &= -(X^{-T} \otimes X^{-1})\text{vec}(dX),
\end{align} \text{vec} \begin{align}
F &= X^{-1}AX^{-1}\\
\implies dF & = d(X^{-1})AX^{-1} + X^{-1}Ad(X^{-1})\\
\implies dF & = d(X^{-1})Y + Zd(X^{-1})\\
\implies \text{vec}(dF) & = (Y^T \otimes I)\text{vec}(dX^{-1}) + (I \otimes Z) \text{vec} (dX^{-1})\\
\implies \text{vec}(dF) & = (Y^T \otimes I)\text{vec}(dX^{-1}) + (I \otimes Z) \text{vec}(dX^{-1})\\
& = -[(Y^T \otimes I) + (I \otimes Z)](X^{-T} \otimes X^{-1}) \text{vec}(dX)\\
& = -[((AX^{-1})^T \otimes I) + (I \otimes X^{-1}A)](X^{-T} \otimes X^{-1}) \text{vec}(dX)\\
\end{align}","['calculus', 'matrices', 'algebra-precalculus', 'derivatives']"
71,Exercise on a fixed end Lagrange's MVT,Exercise on a fixed end Lagrange's MVT,,"Given a function f with derivative on all $[a;b]$ with $f'(a) = f'(b)$, show that there exists $c \in (a;b)$ such that $f'(c) = \frac{f(c)-f(a)}{c-a}$. This is some kind of MVT with constraint. I have a proof but it uses Darboux's theorem. Can you prove it without using it? Sketch of the proof I have using Darboux I suppose first that $f'(a) = 0$ (it's easy to get back to this case with an affine transform) and I set $g(x) = \frac{f(x)-f(a)}{x-a}$. $g'(b) = - \frac{g(b)-g(a)}{b-a} = - g'(d)$ for some $d \in (a;b)$ by Lagrange's MVT. Then $g'(b)$ and $g'(d)$ are either null or have different signs in which case we can find $e \in (d;b)$ such that $g'(e) = 0$. In all cases $g'$ has a zero which is the $c$ we need to find.","Given a function f with derivative on all $[a;b]$ with $f'(a) = f'(b)$, show that there exists $c \in (a;b)$ such that $f'(c) = \frac{f(c)-f(a)}{c-a}$. This is some kind of MVT with constraint. I have a proof but it uses Darboux's theorem. Can you prove it without using it? Sketch of the proof I have using Darboux I suppose first that $f'(a) = 0$ (it's easy to get back to this case with an affine transform) and I set $g(x) = \frac{f(x)-f(a)}{x-a}$. $g'(b) = - \frac{g(b)-g(a)}{b-a} = - g'(d)$ for some $d \in (a;b)$ by Lagrange's MVT. Then $g'(b)$ and $g'(d)$ are either null or have different signs in which case we can find $e \in (d;b)$ such that $g'(e) = 0$. In all cases $g'$ has a zero which is the $c$ we need to find.",,['derivatives']
72,Non Linear Differential Equation: Switching Exponentiation with Differentiation,Non Linear Differential Equation: Switching Exponentiation with Differentiation,,"For which functions $y(x)$ can one swap the order of their derivative with their exponent? $$\left(\frac{d^my}{dx^m}\right)^n\overset{!}{=}\frac{d^m}{dx^m}\left(y^n\right), \quad\quad m,n\in\mathbb{N}^*$$ Of course there are some trivial solutions with constant functions or with $n=1$ . I also found the general solution for $m=1$ : $$\bigg(\frac{dy}{dx}\bigg)^n\overset{!}{=}\frac{d}{dx}\bigg(y^n\bigg) = n \frac{dy}{dx}y^{n-1}$$ $$\bigg(\frac{dy}{dx}\bigg)^{n-1}=ny^{n-1}$$ $$\frac{dy}{dx}= n^{\frac{1}{n-1}}y \quad\quad\Longleftrightarrow\quad\quad y = Ce^{x n^\frac{1}{n-1}} $$ But I'm already stuck with $m=2$ : $$\bigg(\frac{d^2y}{dx^2}\bigg)^n\overset{!}{=}\frac{d^2}{dx^2}\bigg(y^n\bigg) = n \frac{d}{dx}\bigg(y^{n-1}\frac{dy}{dx}\bigg) = n\bigg((n-1)y^{n-2}\bigg(\frac{dy}{dx}\bigg)^2+y^{n-1}\frac{d^2y}{dx^2}\bigg)$$ Is there a way to find a general solution for the triples $y$ , $m$ and $n$ . Or even just a few other non trivial solutions?","For which functions can one swap the order of their derivative with their exponent? Of course there are some trivial solutions with constant functions or with . I also found the general solution for : But I'm already stuck with : Is there a way to find a general solution for the triples , and . Or even just a few other non trivial solutions?","y(x) \left(\frac{d^my}{dx^m}\right)^n\overset{!}{=}\frac{d^m}{dx^m}\left(y^n\right), \quad\quad m,n\in\mathbb{N}^* n=1 m=1 \bigg(\frac{dy}{dx}\bigg)^n\overset{!}{=}\frac{d}{dx}\bigg(y^n\bigg) = n \frac{dy}{dx}y^{n-1} \bigg(\frac{dy}{dx}\bigg)^{n-1}=ny^{n-1} \frac{dy}{dx}= n^{\frac{1}{n-1}}y \quad\quad\Longleftrightarrow\quad\quad y = Ce^{x n^\frac{1}{n-1}}  m=2 \bigg(\frac{d^2y}{dx^2}\bigg)^n\overset{!}{=}\frac{d^2}{dx^2}\bigg(y^n\bigg) = n \frac{d}{dx}\bigg(y^{n-1}\frac{dy}{dx}\bigg) = n\bigg((n-1)y^{n-2}\bigg(\frac{dy}{dx}\bigg)^2+y^{n-1}\frac{d^2y}{dx^2}\bigg) y m n","['ordinary-differential-equations', 'derivatives']"
73,Second-order Taylor expansion for Operators,Second-order Taylor expansion for Operators,,Let $u(t)$ and $v(t)$ be functions in $C^{\infty}$ . Then let $A(u)$ be an operator. A valid reference mentioned that the second-order Taylor expansion of the operator $A$ is: $$A(u+v) = A(u) + dA(u)[v] + \int_{0}^{1}(1-\alpha)d^{(2)}A(u + \alpha v)[v]^{2} d\alpha$$ What does the notation $dA(u)[v]$ and $d^{(2)}A(u + \alpha v)[v]^{2}$ mean?,Let and be functions in . Then let be an operator. A valid reference mentioned that the second-order Taylor expansion of the operator is: What does the notation and mean?,u(t) v(t) C^{\infty} A(u) A A(u+v) = A(u) + dA(u)[v] + \int_{0}^{1}(1-\alpha)d^{(2)}A(u + \alpha v)[v]^{2} d\alpha dA(u)[v] d^{(2)}A(u + \alpha v)[v]^{2},"['ordinary-differential-equations', 'derivatives', 'operator-theory', 'approximation', 'nonlinear-analysis']"
74,Closest rigid-body pose and velocity along a screw motion to a given pose,Closest rigid-body pose and velocity along a screw motion to a given pose,,"Given two rigid-body poses given by dual quaternions ${\bf q}_1$ and ${\bf q}_2$ , where ${\bf q}_1 = e^{{\bf v} t}$ is some pose along a screw motion. The screw is given by dual vector (Pluecker coordinates) ${\bf v} = {\bf u} (\theta + \epsilon d)$ . Here, $|{\bf u}| = 1$ , $\theta$ is the half-angle about the screw axis, $d$ is the half-distance along the screw axis, and $\epsilon$ is the dual unit ( $\epsilon^2 = 0$ ). I'm looking for an analytical solution for finding the real parameter $t$ such that the magnitude of the relative pose ${\bf q}_1^*{\bf q}_2$ is minimized. Since our definition of magnitude is a real number and includes both rotation and translation, we define the magnitude of a dual quaternion ${\bf a} + \epsilon {\bf b}$ as: ${\rm mag}({\bf a} + \epsilon {\bf b}) = c_1| \log {\bf a}|^2 + c_2 | {\bf b} {\bf a}^*|^2$ . The constants $c_1$ and $c_2$ are positive values that are chosen arbitrarily to balance angular and linear differences. Let ${\bf r} = {\bf q}_1^*{\bf q}_2 = e^{-{\bf v} t}{\bf q}_2$ . We need to minimize $f(t) = {\rm mag}({\bf r})$ . This would boil down to finding $t$ for which $\frac{d f(t)}{d t} = 0$ . I'm well aware that there may be multiple, if not an infinite number of, solutions. I made an attempt at solving this, but stranded in the trig equations. Not sure if any computer algebra software is capable of solving this. Secondly, I'm also trying to find the closest screw velocity matching a given velocity (dual vector) ${\bf w}$ . Find a real parameter $t$ that minimizes ${\rm mag}({\bf v} t - {\bf w})$ , where ${\rm mag}({\bf a} + \epsilon {\bf b}) = c_1 |{\bf a}|^2 + c_2 \frac{({\bf a} \cdot {\bf b})^2}{|{\bf a}|^2}$ . This should be easier, since there are no trig functions involved, but still I struggle to find a solution. Any help is highly appreciated. PS: Notice that both magnitude definitons are invariant under rigid body transformation of their arguments. This might help in finding a solution.","Given two rigid-body poses given by dual quaternions and , where is some pose along a screw motion. The screw is given by dual vector (Pluecker coordinates) . Here, , is the half-angle about the screw axis, is the half-distance along the screw axis, and is the dual unit ( ). I'm looking for an analytical solution for finding the real parameter such that the magnitude of the relative pose is minimized. Since our definition of magnitude is a real number and includes both rotation and translation, we define the magnitude of a dual quaternion as: . The constants and are positive values that are chosen arbitrarily to balance angular and linear differences. Let . We need to minimize . This would boil down to finding for which . I'm well aware that there may be multiple, if not an infinite number of, solutions. I made an attempt at solving this, but stranded in the trig equations. Not sure if any computer algebra software is capable of solving this. Secondly, I'm also trying to find the closest screw velocity matching a given velocity (dual vector) . Find a real parameter that minimizes , where . This should be easier, since there are no trig functions involved, but still I struggle to find a solution. Any help is highly appreciated. PS: Notice that both magnitude definitons are invariant under rigid body transformation of their arguments. This might help in finding a solution.",{\bf q}_1 {\bf q}_2 {\bf q}_1 = e^{{\bf v} t} {\bf v} = {\bf u} (\theta + \epsilon d) |{\bf u}| = 1 \theta d \epsilon \epsilon^2 = 0 t {\bf q}_1^*{\bf q}_2 {\bf a} + \epsilon {\bf b} {\rm mag}({\bf a} + \epsilon {\bf b}) = c_1| \log {\bf a}|^2 + c_2 | {\bf b} {\bf a}^*|^2 c_1 c_2 {\bf r} = {\bf q}_1^*{\bf q}_2 = e^{-{\bf v} t}{\bf q}_2 f(t) = {\rm mag}({\bf r}) t \frac{d f(t)}{d t} = 0 {\bf w} t {\rm mag}({\bf v} t - {\bf w}) {\rm mag}({\bf a} + \epsilon {\bf b}) = c_1 |{\bf a}|^2 + c_2 \frac{({\bf a} \cdot {\bf b})^2}{|{\bf a}|^2},"['derivatives', 'optimization', 'quaternions', 'rigid-transformation', 'dual-numbers']"
75,"Show that there is $c \in (-1,2)$ such that $f'(c)=0$.",Show that there is  such that .,"c \in (-1,2) f'(c)=0","$f:\mathbb{R} \rightarrow \mathbb{R}$ differentible function and $F:\mathbb{R} \rightarrow \mathbb{R}$ a primitive of $f$ . We know $5F(xy^{2}-y)+F(x^{2}y-x)-F(x^{3}-y)F(2xy-2)\geq 9$ for every $x,y \in \mathbb{R}$ . Show that there is $c \in (-1,2)$ such that $f'(c)=0$ . I think that inequality somehow is related to a function that is positive and somehow we need to use Rolle's Thoerem with that function.",differentible function and a primitive of . We know for every . Show that there is such that . I think that inequality somehow is related to a function that is positive and somehow we need to use Rolle's Thoerem with that function.,"f:\mathbb{R} \rightarrow \mathbb{R} F:\mathbb{R} \rightarrow \mathbb{R} f 5F(xy^{2}-y)+F(x^{2}y-x)-F(x^{3}-y)F(2xy-2)\geq 9 x,y \in \mathbb{R} c \in (-1,2) f'(c)=0","['derivatives', 'functional-equations']"
76,"Finite roots of $f : [0,1] \longrightarrow \mathbb{R}$ derivable such that not exists $x \in [0,1]$, $f(x) = f'(x) = 0$.","Finite roots of  derivable such that not exists , .","f : [0,1] \longrightarrow \mathbb{R} x \in [0,1] f(x) = f'(x) = 0","My doubt is about the problem who has already asked here several times: Let $f : [0,1] \longrightarrow \mathbb{R}$ be a differentiable function. If there do not exist any $x \in [0,1]$ such that $f(x) = f′(x) = 0$ , prove that f  has only finite number of zeros in [0,1]. In this question: Prove that $f$ has finite number of roots , the second answer suggests that: First use the Taylor formula $f(x)=f(x_0)+f′(x_0)(x−x_0)+o(x−x_0)$ to show that zeros with nonvanishing derivative must be isolated. This was not clear to me. I would like to check my demonstration and point out possible errors. My proof: Suppose that the set $$Z = \{x \in [0,1] ; f(x) = 0\}$$ is infinite. Let us now suppose that $Z$ is not discrete. Then there exists $x_0 \in Z \cap Z'$ . Then, for every $\varepsilon > 0$ we have that $$[(x_0 - \varepsilon, x_0 + \varepsilon) - x_0] \cap Z \ne \emptyset.$$ Let $h \ne 0$ such that $x_0 + h \in [0,1]$ . Consider then formula of taylor \begin{align*} f(x_0 + h) & \ = \ f(x_0) + f'(x_0)h + r(h)\\ & \ = \ f'(x_0) + r(h)\\ & \ = \ \bigg[f'(x_0) + \dfrac{r(h)}{h}\bigg]h \end{align*} Since $x_0$ is an accumulation point, if we take $\varepsilon$ small enough, we will find $h'$ small enough such that $f(x_0 + h') \in Z$ . But this must be absurd because for $h'$ sufficiently small we will have to \begin{align*} f(x_0 + h') & \ = \ \bigg[\underbrace{f'(x_0)}_{\ne 0} + \underbrace{\dfrac{r(h)}{h}}_{= 0}\bigg]\underbrace{h}_{\ne 0}\\ & \ \ne \ 0. \end{align*}","My doubt is about the problem who has already asked here several times: Let be a differentiable function. If there do not exist any such that , prove that f  has only finite number of zeros in [0,1]. In this question: Prove that $f$ has finite number of roots , the second answer suggests that: First use the Taylor formula to show that zeros with nonvanishing derivative must be isolated. This was not clear to me. I would like to check my demonstration and point out possible errors. My proof: Suppose that the set is infinite. Let us now suppose that is not discrete. Then there exists . Then, for every we have that Let such that . Consider then formula of taylor Since is an accumulation point, if we take small enough, we will find small enough such that . But this must be absurd because for sufficiently small we will have to","f : [0,1] \longrightarrow \mathbb{R} x \in [0,1] f(x) = f′(x) = 0 f(x)=f(x_0)+f′(x_0)(x−x_0)+o(x−x_0) Z = \{x \in [0,1] ; f(x) = 0\} Z x_0 \in Z \cap Z' \varepsilon > 0 [(x_0 - \varepsilon, x_0 + \varepsilon) - x_0] \cap Z \ne \emptyset. h \ne 0 x_0 + h \in [0,1] \begin{align*}
f(x_0 + h) & \ = \ f(x_0) + f'(x_0)h + r(h)\\
& \ = \ f'(x_0) + r(h)\\
& \ = \ \bigg[f'(x_0) + \dfrac{r(h)}{h}\bigg]h
\end{align*} x_0 \varepsilon h' f(x_0 + h') \in Z h' \begin{align*}
f(x_0 + h') & \ = \ \bigg[\underbrace{f'(x_0)}_{\ne 0} + \underbrace{\dfrac{r(h)}{h}}_{= 0}\bigg]\underbrace{h}_{\ne 0}\\
& \ \ne \ 0.
\end{align*}","['real-analysis', 'derivatives', 'solution-verification']"
77,"Differentiation through integrals, tower of expectations - Proof validation and clearing up doubts","Differentiation through integrals, tower of expectations - Proof validation and clearing up doubts",,"I am interested in minimizing a loss function involving probability densities of kernel density estimation functions (KDEs). One of the terms I obtain is this one: $$\int _{X}\left(\int _{Z} p( z) K( x-g_{\theta }( z)) dz\right)^{2} dx$$ Eventually, I would like to get an expectation that I can turn into an estimator of the gradient to train a neural network model $g$ parametrized by $\theta$ . So I try to differentiate with respect to $\theta$ : $$ \begin{align} \nabla _{\theta }\int _{X}\left(\int _{Z} p( z) K( x-g_{\theta }( z)) dz\right)^{2} dx &= \int _{X} \nabla _{\theta }\left(\int _{Z} p( z) K( x-g_{\theta }( z)) dz\right)^{2} dx \\ &= 2\int _{X}\left(\int _{Z} p( z) K( x-g_{\theta }( z)) dz\right) \nabla _{\theta }\left(\int _{Z} p( z) K( x-g_{\theta }( z)) dz\right) dx\\ &=2\int _{X}\int _{Z} p( z) K( x-g_{\theta }( z)) dz\int _{Z} p( z) \nabla _{\theta } K( x-g_{\theta }( z)) dzdx\\ &=2\int _{X} E_{Z}[ K( x-g_{\theta }( Z))] E_{Z}[ \nabla _{\theta } K( x-g_{\theta }( Z))] dx\\ &=2E_{X\sim E_{Z}[ K( x-g_{\theta }( Z))]}[ E_{Z}[ \nabla _{\theta } K( X-g_{\theta }( Z))]]\\ &= 2E_{Z,X\sim E_{Z}[ K( x-g_{\theta }( Z))]}[ \nabla _{\theta } K( X-g_{\theta }( Z))] \end{align} $$ I am not completely certain I am correctly applying the chain rule between line $1$ and $2$ , that's the first issue. Then I am not sure if I am correct in going to the last line from the previous one. $E_{Z}[ K( x-g_{\theta }( Z))]$ should be a correct probability density function on $X$ so the previous line should be correct. However, I am not sure if I can collapse the two expectations into one like that. I don't know if it helps nor if it is correct but looking at the integral representation of the expectations, it seem to suggest that the two variables $Z$ and $E_{Z}[ K( x-g_{\theta }( Z))]$ should be independent in this context, although $Z$ participates in the computation of $E_{Z}[ K( x-g_{\theta }( Z))]$ . My intuition is that they should be independent because in this case $Z$ is ""defined"" by the inner expectation, it has no actual relation with the outer $Z$ . So if someone can clear these doubts and confirm that the proof seems correct, I would appreciate it. There is still one additional question I would like to ask: assuming the two variables are indeed independent, can I gain something by taking the same $Z$ for the computation of both variables, deliberately making them dependent?","I am interested in minimizing a loss function involving probability densities of kernel density estimation functions (KDEs). One of the terms I obtain is this one: Eventually, I would like to get an expectation that I can turn into an estimator of the gradient to train a neural network model parametrized by . So I try to differentiate with respect to : I am not completely certain I am correctly applying the chain rule between line and , that's the first issue. Then I am not sure if I am correct in going to the last line from the previous one. should be a correct probability density function on so the previous line should be correct. However, I am not sure if I can collapse the two expectations into one like that. I don't know if it helps nor if it is correct but looking at the integral representation of the expectations, it seem to suggest that the two variables and should be independent in this context, although participates in the computation of . My intuition is that they should be independent because in this case is ""defined"" by the inner expectation, it has no actual relation with the outer . So if someone can clear these doubts and confirm that the proof seems correct, I would appreciate it. There is still one additional question I would like to ask: assuming the two variables are indeed independent, can I gain something by taking the same for the computation of both variables, deliberately making them dependent?","\int _{X}\left(\int _{Z} p( z) K( x-g_{\theta }( z)) dz\right)^{2} dx g \theta \theta 
\begin{align}
\nabla _{\theta }\int _{X}\left(\int _{Z} p( z) K( x-g_{\theta }( z)) dz\right)^{2} dx
&= \int _{X} \nabla _{\theta }\left(\int _{Z} p( z) K( x-g_{\theta }( z)) dz\right)^{2} dx \\
&= 2\int _{X}\left(\int _{Z} p( z) K( x-g_{\theta }( z)) dz\right) \nabla _{\theta }\left(\int _{Z} p( z) K( x-g_{\theta }( z)) dz\right) dx\\
&=2\int _{X}\int _{Z} p( z) K( x-g_{\theta }( z)) dz\int _{Z} p( z) \nabla _{\theta } K( x-g_{\theta }( z)) dzdx\\
&=2\int _{X} E_{Z}[ K( x-g_{\theta }( Z))] E_{Z}[ \nabla _{\theta } K( x-g_{\theta }( Z))] dx\\
&=2E_{X\sim E_{Z}[ K( x-g_{\theta }( Z))]}[ E_{Z}[ \nabla _{\theta } K( X-g_{\theta }( Z))]]\\
&= 2E_{Z,X\sim E_{Z}[ K( x-g_{\theta }( Z))]}[ \nabla _{\theta } K( X-g_{\theta }( Z))]
\end{align}
 1 2 E_{Z}[ K( x-g_{\theta }( Z))] X Z E_{Z}[ K( x-g_{\theta }( Z))] Z E_{Z}[ K( x-g_{\theta }( Z))] Z Z Z","['derivatives', 'solution-verification', 'expected-value', 'density-function']"
78,How can I find the geometric median of n points in 2D Euclidean space using high school level calculus and optimization of total distance?,How can I find the geometric median of n points in 2D Euclidean space using high school level calculus and optimization of total distance?,,"I am a high school student trying to work on a math project. I have plotted the coordinates of all households in a Kenyan village and am trying to locate the geometric median of the set of points to find the ideal point to built a well. I have tried using Fermat's problem and the smallest enclosing circle problem, but these solutions are too complex for my high school math project. Is there a way to solve this using high school calculus? It could go a little beyond but I can't do things like second-order cone optimization. The idea is to find an equation for the total distance to the well (unweighted). Take the points (1,2), (1,-1), (-2,-1) so that there are 3 households in the village. I found the distance equation, but realized that there are 2 variables in the equation so I can't do optimization and implicit differentiation. I don't know what to do! I am already halfway through this project and can't scrap it!","I am a high school student trying to work on a math project. I have plotted the coordinates of all households in a Kenyan village and am trying to locate the geometric median of the set of points to find the ideal point to built a well. I have tried using Fermat's problem and the smallest enclosing circle problem, but these solutions are too complex for my high school math project. Is there a way to solve this using high school calculus? It could go a little beyond but I can't do things like second-order cone optimization. The idea is to find an equation for the total distance to the well (unweighted). Take the points (1,2), (1,-1), (-2,-1) so that there are 3 households in the village. I found the distance equation, but realized that there are 2 variables in the equation so I can't do optimization and implicit differentiation. I don't know what to do! I am already halfway through this project and can't scrap it!",,"['calculus', 'derivatives', 'optimization', 'computational-geometry', 'second-order-cone-programming']"
79,"The space of all square matrices with the same distinct real eigenvalues is a smooth submanifold of $\mathrm{Mat}(n,\mathbb{R})$",The space of all square matrices with the same distinct real eigenvalues is a smooth submanifold of,"\mathrm{Mat}(n,\mathbb{R})","Problem: Let $\lambda_1,\dots,\lambda_n$ be distinct real numbers; set $\Lambda$ to be the matrix with $\Lambda_{ii}=\lambda_i$ and $$H = \{Q\in\mathrm{Mat}(n,\mathbb{R}):Q\text{ has eigenvalues }\lambda_1,\dots,\lambda_n\}\\=\{Q\in\mathrm{Mat}(n,\mathbb{R}):Q\text{ is similar to }\Lambda\}$$ I want to show that $H$ is a smooth submanifold of $\mathrm{Mat}(n,\mathbb{R})$ . Previous Work: Since the $\lambda_i$ are distinct, then any two matrices are similar to $\Lambda$ if and only if they have the same distinct eigenvalues $\lambda_1,\dots,\lambda_n$ . Let $G=\mathrm{GL}(n,\mathbb{R})$ and $M=\mathrm{Mat}(n,\mathbb{R})$ . Consider the smooth left action $\alpha:G\times M\to M$ given by conjugation, i.e., $\alpha(g,m)=gmg^{-1}$ . Considering the orbit of $\Lambda$ , we note that $H=G\cdot\Lambda$ . I am trying to show that $H$ is an embedded submanifold via a more direct approach: First, I want to show that $\alpha_{\Lambda}:G\to H\subset M$ is an immersion. Here is my attempt: Fix $g\in G$ and $X\in \mathrm{T}_gG=M$ . Let $\gamma_g:\mathbb{R}\to G$ be the curve given by $\gamma_g(t)=ge^{g^{-1}X}$ so that $\gamma_g(0)=g$ and $\gamma_g'(0)=X$ . We may use this curve to compute the differential of $\alpha_{\Lambda}$ at $X$ . Working with $\alpha_{\Lambda}\circ\gamma_g$ first, we have \begin{align*} (\alpha_{\Lambda}\circ\gamma_g)(t) &=\gamma_g(t)\Lambda\gamma_g(t)^{-1}\\ &=\gamma_g(t)\Lambda\gamma_g(-t)\\ &=ge^{g^{-1}Xt}\Lambda ge^{-g^{-1}Xt}\\ &=e^{Xg^{-1}t}\alpha_{\Lambda}(g)e^{-Xg^{-1}t} \end{align*} so that \begin{align*} \mathrm{D}_g\alpha_{\Lambda}(X)=\frac{\mathrm{d}}{\mathrm{d}t}e^{Xg^{-1}t}\alpha_{\Lambda}(g)e^{-Xg^{-1}t}\bigg|_{t=0}=Xg^{-1}\alpha_{\Lambda}(g)-\alpha_{\Lambda}(g)Xg^{-1} \end{align*} Now, for this differential to be injective everywhere, i.e., $\mathrm{D}_g\alpha_{\Lambda}(X)=0\implies X=0$ , we have the condition $$Xg^{-1}\alpha_{\Lambda}(g)=\alpha_{\Lambda}(g)Xg^{-1}$$ $$\text{ or equivalently}$$ $$g^{-1}X\Lambda = \Lambda g^{-1}X$$ that is, $g^{-1}X$ commutes with $\Lambda$ . Since $\Lambda$ is a diagonal matrix with distinct entries, then $g^{-1}X$ has to be a diagonal matrix. This is where my progress stops. Alternative: I have found this math stack exchange post outlining a proof of a more general statement, that if Lie group acts smoothly on a manifold, then the orbits are immersed submanifolds. Moreover, if the action is proper, then the orbits are embedded submanifolds. The question I would have in this case is: how do I show that conjugation is a proper action? Question: I would be willing to give up on my previous work above, but not without asking someone else first. I am asking for a second look by someone with some more insight. Am I making an error anywhere in my previous work? Is there some connection I am not making to show that $X$ is zero? Any help is appreciated. Thank you.","Problem: Let be distinct real numbers; set to be the matrix with and I want to show that is a smooth submanifold of . Previous Work: Since the are distinct, then any two matrices are similar to if and only if they have the same distinct eigenvalues . Let and . Consider the smooth left action given by conjugation, i.e., . Considering the orbit of , we note that . I am trying to show that is an embedded submanifold via a more direct approach: First, I want to show that is an immersion. Here is my attempt: Fix and . Let be the curve given by so that and . We may use this curve to compute the differential of at . Working with first, we have so that Now, for this differential to be injective everywhere, i.e., , we have the condition that is, commutes with . Since is a diagonal matrix with distinct entries, then has to be a diagonal matrix. This is where my progress stops. Alternative: I have found this math stack exchange post outlining a proof of a more general statement, that if Lie group acts smoothly on a manifold, then the orbits are immersed submanifolds. Moreover, if the action is proper, then the orbits are embedded submanifolds. The question I would have in this case is: how do I show that conjugation is a proper action? Question: I would be willing to give up on my previous work above, but not without asking someone else first. I am asking for a second look by someone with some more insight. Am I making an error anywhere in my previous work? Is there some connection I am not making to show that is zero? Any help is appreciated. Thank you.","\lambda_1,\dots,\lambda_n \Lambda \Lambda_{ii}=\lambda_i H = \{Q\in\mathrm{Mat}(n,\mathbb{R}):Q\text{ has eigenvalues }\lambda_1,\dots,\lambda_n\}\\=\{Q\in\mathrm{Mat}(n,\mathbb{R}):Q\text{ is similar to }\Lambda\} H \mathrm{Mat}(n,\mathbb{R}) \lambda_i \Lambda \lambda_1,\dots,\lambda_n G=\mathrm{GL}(n,\mathbb{R}) M=\mathrm{Mat}(n,\mathbb{R}) \alpha:G\times M\to M \alpha(g,m)=gmg^{-1} \Lambda H=G\cdot\Lambda H \alpha_{\Lambda}:G\to H\subset M g\in G X\in \mathrm{T}_gG=M \gamma_g:\mathbb{R}\to G \gamma_g(t)=ge^{g^{-1}X} \gamma_g(0)=g \gamma_g'(0)=X \alpha_{\Lambda} X \alpha_{\Lambda}\circ\gamma_g \begin{align*}
(\alpha_{\Lambda}\circ\gamma_g)(t)
&=\gamma_g(t)\Lambda\gamma_g(t)^{-1}\\
&=\gamma_g(t)\Lambda\gamma_g(-t)\\
&=ge^{g^{-1}Xt}\Lambda ge^{-g^{-1}Xt}\\
&=e^{Xg^{-1}t}\alpha_{\Lambda}(g)e^{-Xg^{-1}t}
\end{align*} \begin{align*}
\mathrm{D}_g\alpha_{\Lambda}(X)=\frac{\mathrm{d}}{\mathrm{d}t}e^{Xg^{-1}t}\alpha_{\Lambda}(g)e^{-Xg^{-1}t}\bigg|_{t=0}=Xg^{-1}\alpha_{\Lambda}(g)-\alpha_{\Lambda}(g)Xg^{-1}
\end{align*} \mathrm{D}_g\alpha_{\Lambda}(X)=0\implies X=0 Xg^{-1}\alpha_{\Lambda}(g)=\alpha_{\Lambda}(g)Xg^{-1} \text{ or equivalently} g^{-1}X\Lambda = \Lambda g^{-1}X g^{-1}X \Lambda \Lambda g^{-1}X X","['linear-algebra', 'derivatives', 'lie-groups', 'differential-topology', 'smooth-manifolds']"
80,Derivative with respect to vectorization,Derivative with respect to vectorization,,"I refer to the vectorization $\mathrm{vec}$ of a matrix $A\in\mathbb{R}^{n\times n}$ as the vector in $\mathbb{R}^{n^2}$ with the columns of $A$ stacked one on top of each other. I have a function $g(x,A) = (A^T\otimes CA)x$ , for a fixed matrix $C\in\mathbb{R}^{n\times n}$ , and with $x\in\mathbb{R}^{n^2}$ . I want to compute the derivative of $g(x,A)$ with respect to $\mathrm{vec}(A)$ . I expect this to be a matrix of size $n^2\times n^2$ of course. If instead of one of the two $A$ s I had a $B\in\mathbb{R}^{n\times n}$ , I would know what to do since, for example for $h(x,A)=(A^T\otimes  CB)x$ I know $$ g(x,A)=\mathrm{vec}(CB\mathrm{Mat}(x)A)=(I_n\otimes CB\mathrm{Mat}(x))\mathrm{vec}(A) $$ and hence the derivative is simply given by $(I_n\otimes CB\mathrm{Mat}(x))$ . A similar reasoning applies when I fix the first $A$ . Here by $\mathrm{Mat}$ I intend the opposite of $\mathrm{vec}$ , i.e. $\mathrm{vec}(\mathrm{Mat}(x))=x$ . To differentiate the whole of $g(x,A)$ I would be tempted to sum these two contributions obtained rewriting $g(x,A)$ in two different and convenient ways. I have however the worry this is not correct. How can I think about solving this problem?","I refer to the vectorization of a matrix as the vector in with the columns of stacked one on top of each other. I have a function , for a fixed matrix , and with . I want to compute the derivative of with respect to . I expect this to be a matrix of size of course. If instead of one of the two s I had a , I would know what to do since, for example for I know and hence the derivative is simply given by . A similar reasoning applies when I fix the first . Here by I intend the opposite of , i.e. . To differentiate the whole of I would be tempted to sum these two contributions obtained rewriting in two different and convenient ways. I have however the worry this is not correct. How can I think about solving this problem?","\mathrm{vec} A\in\mathbb{R}^{n\times n} \mathbb{R}^{n^2} A g(x,A) = (A^T\otimes CA)x C\in\mathbb{R}^{n\times n} x\in\mathbb{R}^{n^2} g(x,A) \mathrm{vec}(A) n^2\times n^2 A B\in\mathbb{R}^{n\times n} h(x,A)=(A^T\otimes  CB)x 
g(x,A)=\mathrm{vec}(CB\mathrm{Mat}(x)A)=(I_n\otimes CB\mathrm{Mat}(x))\mathrm{vec}(A)
 (I_n\otimes CB\mathrm{Mat}(x)) A \mathrm{Mat} \mathrm{vec} \mathrm{vec}(\mathrm{Mat}(x))=x g(x,A) g(x,A)","['matrices', 'derivatives', 'vector-analysis', 'matrix-calculus']"
81,Are these 2 different definitions of a derivative?,Are these 2 different definitions of a derivative?,,"I've recently started learning calculus, and I'm trying to grasp the concept of derivatives. In my textbook, two seemingly different definitions of the derivative are presented: The first definition states: $$ \frac{d\ f(x)}{dx} = \lim_{h \to 0} \frac {f(x+h) - f(x)}{h}, \text{ provided limit exists} \implies f'(c) = \lim_{h \to 0} \frac {f(c+h) - f(c)}{h}, \text{ provided limit exists} $$ Later in the book, another expression is used: $$ f'(c) = \lim_{x \to c} \frac {f(x) - f(c)}{x-c}, \text{ provided limit exists} $$ I'm struggling to see how these two definitions represent the same concept. Could someone help clarify this for me? Thank you in advance, and I apologize if this question has been asked before or if there are any formatting errors.","I've recently started learning calculus, and I'm trying to grasp the concept of derivatives. In my textbook, two seemingly different definitions of the derivative are presented: The first definition states: Later in the book, another expression is used: I'm struggling to see how these two definitions represent the same concept. Could someone help clarify this for me? Thank you in advance, and I apologize if this question has been asked before or if there are any formatting errors.","
\frac{d\ f(x)}{dx} = \lim_{h \to 0} \frac {f(x+h) - f(x)}{h}, \text{ provided limit exists}
\implies
f'(c) = \lim_{h \to 0} \frac {f(c+h) - f(c)}{h}, \text{ provided limit exists}
 
f'(c) = \lim_{x \to c} \frac {f(x) - f(c)}{x-c}, \text{ provided limit exists}
",['calculus']
82,Taylor series up to first order,Taylor series up to first order,,"I have an expression of the form: $$\frac{1}{1+f(x)}$$ which I want to simplify by employing Taylor series up to linear order. I know that in general: $$\frac{1}{1+x}=1-x+x^2-x^3+....$$ if $|x|<1$ or $|x|<<1$ ? I want to know whether it is also applicable to the case above? i.e., $$\frac{1}{1+f(x)}=[1+f(x)]^{-1}=1-f(x)+f^2(x)-f^3(x)+....$$ with the condition that $|f(x)|<1$ or $f(x)<<1$ ? for any $f(x)$ . A linear order approximation would yield $$\frac{1}{1+f(x)}=1-f(x)$$ which is a reasonable approximation if $f(x)<<1$ .","I have an expression of the form: which I want to simplify by employing Taylor series up to linear order. I know that in general: if or ? I want to know whether it is also applicable to the case above? i.e., with the condition that or ? for any . A linear order approximation would yield which is a reasonable approximation if .",\frac{1}{1+f(x)} \frac{1}{1+x}=1-x+x^2-x^3+.... |x|<1 |x|<<1 \frac{1}{1+f(x)}=[1+f(x)]^{-1}=1-f(x)+f^2(x)-f^3(x)+.... |f(x)|<1 f(x)<<1 f(x) \frac{1}{1+f(x)}=1-f(x) f(x)<<1,"['derivatives', 'taylor-expansion']"
83,Different value of derivative after applying Leibniz Rule,Different value of derivative after applying Leibniz Rule,,"I was trying to find the general differential question of $\arcsin(x)$ for Taylor's series and noticed this: $$y=\arcsin^2(x)\\y_1=2(\arcsin(x))\dfrac1{\sqrt{1-x^2}}$$ Squaring both sides $$y_1^2=4y\left(\dfrac1{1-x^2}\right)\\y_1^2(1-x^2)=4y$$ Taking derivative wrt $x$ $$2y_1y_2(1-x^2)+y_1^2(-2x)-4y_1=0$$ $$(1-x^2)y_2-xy_1-2=0\tag1\label{eq1}$$ Taking $n$ th derivative by applying Leibniz Rule: $$y_{n+2}(1-x^2)-2nxy_{n+1}+\dfrac{n(n-1)}{2}(-2)y_n-y_{n+1}x-ny_n=0\\y_{n+2}(1-x^2)-(2n+1)xy_{n+1}-n^2y_n=0$$ when $x=0$ , $$y_{n+2}=n^2y_n\tag2\label{eq2}$$ So when I calculate $y_2$ by $\eqref{eq1}$ , I get $y_2=2$ But when I do the same by $\eqref{eq2}$ , $$y_2=(0)^2y_0=0$$ I get $y_2=0$ Why this inconsistency? I know I am making a silly mistake somewhere.","I was trying to find the general differential question of for Taylor's series and noticed this: Squaring both sides Taking derivative wrt Taking th derivative by applying Leibniz Rule: when , So when I calculate by , I get But when I do the same by , I get Why this inconsistency? I know I am making a silly mistake somewhere.",\arcsin(x) y=\arcsin^2(x)\\y_1=2(\arcsin(x))\dfrac1{\sqrt{1-x^2}} y_1^2=4y\left(\dfrac1{1-x^2}\right)\\y_1^2(1-x^2)=4y x 2y_1y_2(1-x^2)+y_1^2(-2x)-4y_1=0 (1-x^2)y_2-xy_1-2=0\tag1\label{eq1} n y_{n+2}(1-x^2)-2nxy_{n+1}+\dfrac{n(n-1)}{2}(-2)y_n-y_{n+1}x-ny_n=0\\y_{n+2}(1-x^2)-(2n+1)xy_{n+1}-n^2y_n=0 x=0 y_{n+2}=n^2y_n\tag2\label{eq2} y_2 \eqref{eq1} y_2=2 \eqref{eq2} y_2=(0)^2y_0=0 y_2=0,"['calculus', 'ordinary-differential-equations', 'derivatives']"
84,Do 'Anti-Fréchet Derivatives' work similar to typical anti-derivatives? Are there two ways different ways to define them?,Do 'Anti-Fréchet Derivatives' work similar to typical anti-derivatives? Are there two ways different ways to define them?,,"Assume a function $f:L_2(R^{+}):R$ is frechet differnetiable in $x\in L_2(R^{+})$ in that there exists a unique function $D(x_i,x)$ (where $x_i\in R^{++}$ is an element of x) such that: $f(x+h)=f(x)+\int_i D(x_i,x)h_idi + o (||h||_2)$ Further assume we can define the second cross partial, $DD(x_i,x_j,x)$ in a similar way. Do we then have that $\int_{-\infty}^{x_j}  DD(x_i,y,x) dy= D(x_i,x)+C?$ It seems like there would be 2 ways to interpret $\int_{-\infty}^{x_j}  DD(x_i,y,x) dy$ . The first being what feels like the ""typical"" sense, where the value of the second item ( $y$ or $x_j$ ) increases along the integral, but also the $x_j$ that lives inside of $x$ increases too, making it seem analogous to the typical partial derivative leading to my feeling the fundamental theorem of calculus holds. The second interpretation is to take the integral while ignoring the fact that $x_j$ is an element of $x$ , which seems to suggest FTC would not necessarily hold. Both interpretations seem like they could arise in certain contexts, but I am not aware of an explicit discussion of the second, though it arises in an application I have due to the way a limit is constructed. Is there a name or way of identifying the different ways of taking the integral, and does anyone know of any resources to help with understanding the second?","Assume a function is frechet differnetiable in in that there exists a unique function (where is an element of x) such that: Further assume we can define the second cross partial, in a similar way. Do we then have that It seems like there would be 2 ways to interpret . The first being what feels like the ""typical"" sense, where the value of the second item ( or ) increases along the integral, but also the that lives inside of increases too, making it seem analogous to the typical partial derivative leading to my feeling the fundamental theorem of calculus holds. The second interpretation is to take the integral while ignoring the fact that is an element of , which seems to suggest FTC would not necessarily hold. Both interpretations seem like they could arise in certain contexts, but I am not aware of an explicit discussion of the second, though it arises in an application I have due to the way a limit is constructed. Is there a name or way of identifying the different ways of taking the integral, and does anyone know of any resources to help with understanding the second?","f:L_2(R^{+}):R x\in L_2(R^{+}) D(x_i,x) x_i\in R^{++} f(x+h)=f(x)+\int_i D(x_i,x)h_idi + o (||h||_2) DD(x_i,x_j,x) \int_{-\infty}^{x_j}  DD(x_i,y,x) dy= D(x_i,x)+C? \int_{-\infty}^{x_j}  DD(x_i,y,x) dy y x_j x_j x x_j x","['calculus', 'integration', 'derivatives', 'partial-derivative', 'frechet-derivative']"
85,Infinitely differentiable function with given zero set?,Infinitely differentiable function with given zero set?,,"For each closed set $A\subseteq\mathbb{R}$, is it possible to construct a real continuous function $f$ such that the zero set, $f^{-1}(0)$, of $f$ is precisely $A$, and $f$ is infinitely differentiable on all of $\mathbb{R}$? Thanks!","For each closed set $A\subseteq\mathbb{R}$, is it possible to construct a real continuous function $f$ such that the zero set, $f^{-1}(0)$, of $f$ is precisely $A$, and $f$ is infinitely differentiable on all of $\mathbb{R}$? Thanks!",,"['real-analysis', 'functions']"
86,"For a Wiener process, when can one exchange “for all $t$” and “almost surely”?","For a Wiener process, when can one exchange “for all ” and “almost surely”?",t,"Certain local properties of the Wiener process $W_t$ are quick to prove at $t = 0$ , for instance: almost surely $W_t$ is monotonous on no interval beginning at $t = 0$ ; almost surely $W_t$ is not right-differentiable at $t = 0$ . Then, because $W_{t_0+t} - W_t$ is a Wiener process, we can conclude that for any $t \ge 0$ , almost surely $W_t$ is monotonous on no interval beginning at $t$ , for any $t \ge 0$ , almost surely $W_t$ is not right-differentiable at $t$ . Thus at any countable set of times, in particular a dense one, both hold almost surely. Now, if the first holds at a certain $t = t_0$ then it holds for all $t$ in a nonempty open interval to the right of $t_0$ , so almost surely it must hold for all $t$ . We have successfully exchanged “for all $t$ ” and “almost surely” for the first property. For the second property we are less fortunate: there are continuous functions not differentiable on a countable dense subset but differentiable on its complement. Yet this “exchange of quantifiers” is still valid here: almost surely $W_t$ is nowhere differentiable. However the proofs I have seen are considerably more involved than that for only $t = 0$ . Hence my question: is there a clean condition on the events $A_t$ under which $$\inf_{t\ge0} \Pr(W \in A_t) = 1 \implies \Pr\Bigl( W \in \bigcap_{t\ge0} A_t \Bigr) = 1 \text?$$ I am interested in “local” events $A_t$ ; let's suppose $$A_t \in \bigcap_{\varepsilon>0} \sigma(\, W_{t+\delta} \colon 0 \le \delta < \varepsilon \,) \text, \quad \text{$t \ge 0$.}$$ An obvious starting point would be $$A_t \subseteq \bigcup_{\varepsilon>0} \bigcap_{0\le\delta<\varepsilon} A_{t+\delta} \text,$$ as with the first property concerning monotonicity above. It would be extra nice if the condition applied to the second property about differentiability.","Certain local properties of the Wiener process are quick to prove at , for instance: almost surely is monotonous on no interval beginning at ; almost surely is not right-differentiable at . Then, because is a Wiener process, we can conclude that for any , almost surely is monotonous on no interval beginning at , for any , almost surely is not right-differentiable at . Thus at any countable set of times, in particular a dense one, both hold almost surely. Now, if the first holds at a certain then it holds for all in a nonempty open interval to the right of , so almost surely it must hold for all . We have successfully exchanged “for all ” and “almost surely” for the first property. For the second property we are less fortunate: there are continuous functions not differentiable on a countable dense subset but differentiable on its complement. Yet this “exchange of quantifiers” is still valid here: almost surely is nowhere differentiable. However the proofs I have seen are considerably more involved than that for only . Hence my question: is there a clean condition on the events under which I am interested in “local” events ; let's suppose An obvious starting point would be as with the first property concerning monotonicity above. It would be extra nice if the condition applied to the second property about differentiability.","W_t t = 0 W_t t = 0 W_t t = 0 W_{t_0+t} - W_t t \ge 0 W_t t t \ge 0 W_t t t = t_0 t t_0 t t W_t t = 0 A_t \inf_{t\ge0} \Pr(W \in A_t) = 1 \implies \Pr\Bigl( W \in \bigcap_{t\ge0} A_t \Bigr) = 1 \text? A_t A_t \in \bigcap_{\varepsilon>0} \sigma(\, W_{t+\delta} \colon 0 \le \delta < \varepsilon \,) \text, \quad \text{t \ge 0.} A_t \subseteq \bigcup_{\varepsilon>0} \bigcap_{0\le\delta<\varepsilon} A_{t+\delta} \text,","['derivatives', 'brownian-motion', 'quantifiers', 'almost-everywhere', 'wiener-measure']"
87,Which of these properties is not true of the directional derivative?,Which of these properties is not true of the directional derivative?,,"In Arnold's Ordinary Linear Equations , 1st edition, chapter 2, section 10, subsection 3, the author pulls what is just about the dirtiest, cruelest, most evil trick which could be imagined from an expositor of mathematics. Here is the setup: ""We denote by $F$ the set of all infinitely differentiable functions $f : U \rightarrow \textbf{R}$ . Let $\textbf{v}$ be an infinitely differentiable vector field in $U$ . The derivative of a function of $F$ in the direction of the field $\textbf{v}$ again belongs to $F$ . Thus differentiation in the direction of the field $\textbf{v}$ is a mapping $L_v : F \rightarrow F$ of the algebra of infinitely differentiable functions into itself. Let us consider several properties of this mapping: $L_v(f + g) = L_vf + L_vg$ $L_v(fg) = fL_vg + gL_vf$ $L_{u+v} = L_v + L_u$ $L_{fu} = fL_u$ $L_uL_v = L_vL_u$ ( $f$ and $g$ are smooth functions and $\textbf{u}$ and $\textbf{v}$ are smooth vector fields)."" And then comes the punch to the gut, in tiny text: "" Problem 1. Prove properties 1-5, except for the one which is not true."" I believe the falsehood is number 4, $L_{fu} = fL_u$ . The reason is because $f$ must be a function from a vector field to another vector field to be applied to $u$ , but $L_u$ transforms a real-valued function to another real-valued function, so it doesn't make sense to apply the same $f$ to both $u$ and $L_u$ . Is that correct?","In Arnold's Ordinary Linear Equations , 1st edition, chapter 2, section 10, subsection 3, the author pulls what is just about the dirtiest, cruelest, most evil trick which could be imagined from an expositor of mathematics. Here is the setup: ""We denote by the set of all infinitely differentiable functions . Let be an infinitely differentiable vector field in . The derivative of a function of in the direction of the field again belongs to . Thus differentiation in the direction of the field is a mapping of the algebra of infinitely differentiable functions into itself. Let us consider several properties of this mapping: ( and are smooth functions and and are smooth vector fields)."" And then comes the punch to the gut, in tiny text: "" Problem 1. Prove properties 1-5, except for the one which is not true."" I believe the falsehood is number 4, . The reason is because must be a function from a vector field to another vector field to be applied to , but transforms a real-valued function to another real-valued function, so it doesn't make sense to apply the same to both and . Is that correct?",F f : U \rightarrow \textbf{R} \textbf{v} U F \textbf{v} F \textbf{v} L_v : F \rightarrow F L_v(f + g) = L_vf + L_vg L_v(fg) = fL_vg + gL_vf L_{u+v} = L_v + L_u L_{fu} = fL_u L_uL_v = L_vL_u f g \textbf{u} \textbf{v} L_{fu} = fL_u f u L_u f u L_u,"['derivatives', 'vector-analysis']"
88,Differentiation on an arbitrary set,Differentiation on an arbitrary set,,"The subject of this question is differentiability of a function on an arbitrary set. It doesn't matter if we exclude isolated points from the domain of a function. So, let $X$ be a subset of $\mathbb{R}$ whose every point is an accumulation  point of $X$ . For a function $f:X\rightarrow\mathbb{R}$ and a point $p\in X$ , we can define $f’(p)=\lim\limits_{x\to p}\frac{f(x)-f(p)}{x-p}$ . If this limit exists at all points $p\in X$ , $f$ is differentiable . Two other definitions are known for the differentiability of a function on an arbitrary subset. The question is that if $f$ is differentiable in the above sense, is it differentiable in each of the following senses? (1) $f$ is differentiable if there is an open set $U$ including $X$ and a differentiable function $\tilde{f}:U\rightarrow \mathbb{R}$ such that $f=\tilde{f}|X$ . (2) $f$ is differentiable is for all $x \in X$ , there is an open set $U$ containing $x$ and a differentiable function $\tilde{f}:U\rightarrow \mathbb{R}$ such that $f|U\cap X=\tilde{f}|U\cap X$ . This is my original problem, not a homework.","The subject of this question is differentiability of a function on an arbitrary set. It doesn't matter if we exclude isolated points from the domain of a function. So, let be a subset of whose every point is an accumulation  point of . For a function and a point , we can define . If this limit exists at all points , is differentiable . Two other definitions are known for the differentiability of a function on an arbitrary subset. The question is that if is differentiable in the above sense, is it differentiable in each of the following senses? (1) is differentiable if there is an open set including and a differentiable function such that . (2) is differentiable is for all , there is an open set containing and a differentiable function such that . This is my original problem, not a homework.",X \mathbb{R} X f:X\rightarrow\mathbb{R} p\in X f’(p)=\lim\limits_{x\to p}\frac{f(x)-f(p)}{x-p} p\in X f f f U X \tilde{f}:U\rightarrow \mathbb{R} f=\tilde{f}|X f x \in X U x \tilde{f}:U\rightarrow \mathbb{R} f|U\cap X=\tilde{f}|U\cap X,"['real-analysis', 'calculus', 'derivatives']"
89,growth rate of the sides of a triangle,growth rate of the sides of a triangle,,"I got into this problem: In an isosceles triangle $ABC$ the vertex $C$ moves perpendicular to the base $AB$ so that the area of the triangle grows at a speed of $4 cm^2/s$ . The base $AB$ is $3 cm$ long. At what rate does the height $CH$ grow? What about the $CB$ side? I solved the first question using derivatives, knowing that: the area of a triangle measures $A =\frac{1}{2} \cdot AB \cdot CH$ the base stays constant in time So we get that: $\frac{dA}{dt} = \frac{3 \ cm}{2} \cdot \frac{dCH}{dt}$ $\frac{dCH}{dt} = 4 \frac{cm^2}{s} \cdot \frac{2}{3 \ cm}$ $\frac{dCH}{dt} = \frac{8 \ cm}{3 \ s}$ For the second question i thought i could use pythogorean theorem on the triangle $CHB$ , getting: $CB = \sqrt{\frac{1}{4} {AB}^2 + {CH}^2}$ but im not sure on how to get $\frac{dCB}{dt}$ .","I got into this problem: In an isosceles triangle the vertex moves perpendicular to the base so that the area of the triangle grows at a speed of . The base is long. At what rate does the height grow? What about the side? I solved the first question using derivatives, knowing that: the area of a triangle measures the base stays constant in time So we get that: For the second question i thought i could use pythogorean theorem on the triangle , getting: but im not sure on how to get .",ABC C AB 4 cm^2/s AB 3 cm CH CB A =\frac{1}{2} \cdot AB \cdot CH \frac{dA}{dt} = \frac{3 \ cm}{2} \cdot \frac{dCH}{dt} \frac{dCH}{dt} = 4 \frac{cm^2}{s} \cdot \frac{2}{3 \ cm} \frac{dCH}{dt} = \frac{8 \ cm}{3 \ s} CHB CB = \sqrt{\frac{1}{4} {AB}^2 + {CH}^2} \frac{dCB}{dt},"['calculus', 'derivatives']"
90,Differentiation and Chain Rule on the Hilbert Space $L^2$. (Reisz Representation).,Differentiation and Chain Rule on the Hilbert Space . (Reisz Representation).,L^2,"Let $F:L^2(\mathbb{R}^d)\to \mathbb{R}$ be a functional on the Hilbert space $L^2(\mathbb{R}^d)$ and $\rho:\mathbb{R}\to L^2(\mathbb{R}^d)$ a curve in the space $L^2(\mathbb{R}^d)$ . I want to calculate $\frac{d}{dt}F(\rho(t))$ . Let $DF(\rho(t))$ be the Frechet derivative at $\rho(t)$ . What `chain rule' do I apply to get $\frac{d}{dt}F(\rho(t))=DF(\rho(t))\dot{\rho}(t)$ ? (since this is not the usual chain rule in Euclidean space). Since $DF(\rho(t))$ is a linear functional on $L^2(\mathbb{R}^d)$ it can be associated to an element $f$ of $L^2(\mathbb{R}^d)$ so that $DF(\rho(t))\dot{\rho}(t)=\langle f , \dot{\rho}(t) \rangle_{L^2(\mathbb{R}^d)}$ . Is $f$ known in this case, does it relate to this Wiki article ? (note I have denoted the derivative of $\rho$ with respect to $t$ as $\dot{\rho}(t)$ )","Let be a functional on the Hilbert space and a curve in the space . I want to calculate . Let be the Frechet derivative at . What `chain rule' do I apply to get ? (since this is not the usual chain rule in Euclidean space). Since is a linear functional on it can be associated to an element of so that . Is known in this case, does it relate to this Wiki article ? (note I have denoted the derivative of with respect to as )","F:L^2(\mathbb{R}^d)\to \mathbb{R} L^2(\mathbb{R}^d) \rho:\mathbb{R}\to L^2(\mathbb{R}^d) L^2(\mathbb{R}^d) \frac{d}{dt}F(\rho(t)) DF(\rho(t)) \rho(t) \frac{d}{dt}F(\rho(t))=DF(\rho(t))\dot{\rho}(t) DF(\rho(t)) L^2(\mathbb{R}^d) f L^2(\mathbb{R}^d) DF(\rho(t))\dot{\rho}(t)=\langle f , \dot{\rho}(t) \rangle_{L^2(\mathbb{R}^d)} f \rho t \dot{\rho}(t)","['real-analysis', 'derivatives', 'hilbert-spaces', 'chain-rule', 'frechet-derivative']"
91,Differentiability without limits,Differentiability without limits,,"There are four concepts which are studied in Calculus and Analysis: Convergence, Continuity, Differentiability and Integrability. In Calculus, you can define the latter three in terms of the first, but also you can define integrability without convergence using Darboux approach. In topology, you can define convergence and continuity in terms of neighborhoods, and in measure theory you can define integrability in terms of measurable functions. I wonder if there is a definition of differentiability without making any reference to the concept of limit or convergence, thank you so much by your help.","There are four concepts which are studied in Calculus and Analysis: Convergence, Continuity, Differentiability and Integrability. In Calculus, you can define the latter three in terms of the first, but also you can define integrability without convergence using Darboux approach. In topology, you can define convergence and continuity in terms of neighborhoods, and in measure theory you can define integrability in terms of measurable functions. I wonder if there is a definition of differentiability without making any reference to the concept of limit or convergence, thank you so much by your help.",,"['limits', 'analysis', 'derivatives']"
92,How did this gradient get derived?,How did this gradient get derived?,,I am reading Pattern Recognition and Machine Learning by Bishop and equation 6.2 for gradient of regularized least squares is $$J(w) = \frac{1}{2}\sum^{N}_{n=1}\{\mathbf{w^T}\phi(\mathbf{x_{n}})-t_{n}\}^2+\frac{\lambda}{2}\mathbf{w^Tw}$$ then in the next step it says taking the gradient with respect to w leads to: $$w = -\frac{1}{\lambda}\sum_{n=1}^{N}\{\mathbf{w^T}\phi(\mathbf{x_{n}})\}\phi(\mathbf{x_{n}})$$ but I don't understand where the $-\frac{1}{\lambda}$ is coming from? wouldn't the differential be: $$\lambda\mathbf{w} + \sum_{n=1}^{N}\{\mathbf{w^T}\phi(\mathbf{x_{n}})\}\phi(\mathbf{x_{n}})$$,I am reading Pattern Recognition and Machine Learning by Bishop and equation 6.2 for gradient of regularized least squares is then in the next step it says taking the gradient with respect to w leads to: but I don't understand where the is coming from? wouldn't the differential be:,J(w) = \frac{1}{2}\sum^{N}_{n=1}\{\mathbf{w^T}\phi(\mathbf{x_{n}})-t_{n}\}^2+\frac{\lambda}{2}\mathbf{w^Tw} w = -\frac{1}{\lambda}\sum_{n=1}^{N}\{\mathbf{w^T}\phi(\mathbf{x_{n}})\}\phi(\mathbf{x_{n}}) -\frac{1}{\lambda} \lambda\mathbf{w} + \sum_{n=1}^{N}\{\mathbf{w^T}\phi(\mathbf{x_{n}})\}\phi(\mathbf{x_{n}}),['derivatives']
93,Differentiation of a function $f(x)$.,Differentiation of a function .,f(x),"Given, $f(x) = \sqrt{\sin x + \sqrt{\cos x +\sqrt{\sin x + \sqrt{\cos x.....}}}}$ I need to find $\frac{d}{dx}f(x)$ . I have tried by squaring both sides but was not able to make any headway. If anyone helps me, I shall be highly obliged.","Given, I need to find . I have tried by squaring both sides but was not able to make any headway. If anyone helps me, I shall be highly obliged.",f(x) = \sqrt{\sin x + \sqrt{\cos x +\sqrt{\sin x + \sqrt{\cos x.....}}}} \frac{d}{dx}f(x),['calculus']
94,Evaluating $\frac{d^n}{dt^n}e^{a(t-e^t)}$ as a single series to extend region of convergence for super root function,Evaluating  as a single series to extend region of convergence for super root function,\frac{d^n}{dt^n}e^{a(t-e^t)},"$\def\srt{\operatorname{srt}}$ Introduction: There is a multiple series expansion for the super root $\srt_n(z)$ valid near $0.7<|z|<1.4$ . However, for around $0<|z|<1.3$ , there is this expansion: $$\sqrt[k]z_s=\srt_k(z)=z-\sum_{n=1}^\infty\frac1{n!}\frac{d^{n-1}}{dt^{n-1}}e^{(n+1)t-e^t-\overbrace{ne^{t-e^{\dots}}}^{k-2\ “e^t”\text s}}\bigg|_{\ln(-\ln(z))}$$ Unfortunately, there is no obvious way to expand it as $k-1$ sums preserving convergence. A starting point is a method for finding a single series expansion for $\frac{d^{n-1}}{dt^{n-1}}e^{(n+1)(t-e^t)}$ , when $k=3$ , or the derivatives in: $$\ln(-\ln(\srt_3(e^z)))=\ln(-z)+\sum_{n=1}^\infty\frac1{n!}\left.\frac{d^{n-1}}{dt^{n-1}}e^{n(t-e^t)}\right|_{\ln(-z)}$$ converging around $0<|z|<1.3$ Attempt 1: Trying $\srt_3(z)$ via $e^y$ Maclaurin expansion and changing summation order gives: $$\srt_3(z)=1-\sum_{n=1}^\infty\sum_{m=0}^n\frac{(-1)^m m^{n-m} n^{m-2}}{(m-1)!(n-m)!}\ln^n(z)$$ converges for about $0.4<|z|<1.3$ . If $n\gg m$ , the region of convergence is a bit larger , but not the original $0<|z|<1.3$ : Attempt 2: Using Stirling S2 , general Leibniz rule , and factorial power $a^{(b)}$ : $$\frac{d^{n-1}}{dt^{n-1}}e^{n(t-e^t)}=\sum_{k=0}^{n-1}\sum_{m=0}^n s_{n-1}^{(k)}e^{-ne^t}e^{(k-m+n)t}\binom k nn^{(m)} (-n)^{k-m}$$ but this is not a single series expansion for: $$\frac{d^n}{dt^n}e^{a(t-e^t)} =ae^{a(t- e^t)}\sum_{m=0}^{n-1} (-1)^m a_m e^{am}$$ Pattern : Creating a table and using the OEIS shows that: $$\frac{d^{n-1}}{dt^{n-1}}e^{(n+1)(t-e^t)}=(-1)^{n+1} ne^{(n+1)(t-e^t)}\left((-1)^{n+1} (n+1)^{n-2}+(-1)^n ((n+2)^{n-1}-(n+1)^{n-1})e^t+?e^{2t}+\dots+(n+1)^{n-2} e^{n-1}\right)$$ and $$\frac{d^n}{dt^n}e^{t-e^t}=(-1)^n\left((-1)^n+(-1)^{n+1}S_{n+1}^{(2)}e^t+(-1)^nS_{n+1}^{(3)}e^{2t}+(-1)^{n+1}S_{n+1}^{(4)}e^{3t}+\dots+e^{nt}\right)$$ Question: What are methods for finding $\frac{d^n}{dt^n}e^{a(t-e^t)}$ as a single series so that a series expansion of $\srt_3(z)$ converges around $0<|z|<1.3$ ?","Introduction: There is a multiple series expansion for the super root valid near . However, for around , there is this expansion: Unfortunately, there is no obvious way to expand it as sums preserving convergence. A starting point is a method for finding a single series expansion for , when , or the derivatives in: converging around Attempt 1: Trying via Maclaurin expansion and changing summation order gives: converges for about . If , the region of convergence is a bit larger , but not the original : Attempt 2: Using Stirling S2 , general Leibniz rule , and factorial power : but this is not a single series expansion for: Pattern : Creating a table and using the OEIS shows that: and Question: What are methods for finding as a single series so that a series expansion of converges around ?",\def\srt{\operatorname{srt}} \srt_n(z) 0.7<|z|<1.4 0<|z|<1.3 \sqrt[k]z_s=\srt_k(z)=z-\sum_{n=1}^\infty\frac1{n!}\frac{d^{n-1}}{dt^{n-1}}e^{(n+1)t-e^t-\overbrace{ne^{t-e^{\dots}}}^{k-2\ “e^t”\text s}}\bigg|_{\ln(-\ln(z))} k-1 \frac{d^{n-1}}{dt^{n-1}}e^{(n+1)(t-e^t)} k=3 \ln(-\ln(\srt_3(e^z)))=\ln(-z)+\sum_{n=1}^\infty\frac1{n!}\left.\frac{d^{n-1}}{dt^{n-1}}e^{n(t-e^t)}\right|_{\ln(-z)} 0<|z|<1.3 \srt_3(z) e^y \srt_3(z)=1-\sum_{n=1}^\infty\sum_{m=0}^n\frac{(-1)^m m^{n-m} n^{m-2}}{(m-1)!(n-m)!}\ln^n(z) 0.4<|z|<1.3 n\gg m 0<|z|<1.3 a^{(b)} \frac{d^{n-1}}{dt^{n-1}}e^{n(t-e^t)}=\sum_{k=0}^{n-1}\sum_{m=0}^n s_{n-1}^{(k)}e^{-ne^t}e^{(k-m+n)t}\binom k nn^{(m)} (-n)^{k-m} \frac{d^n}{dt^n}e^{a(t-e^t)} =ae^{a(t- e^t)}\sum_{m=0}^{n-1} (-1)^m a_m e^{am} \frac{d^{n-1}}{dt^{n-1}}e^{(n+1)(t-e^t)}=(-1)^{n+1} ne^{(n+1)(t-e^t)}\left((-1)^{n+1} (n+1)^{n-2}+(-1)^n ((n+2)^{n-1}-(n+1)^{n-1})e^t+?e^{2t}+\dots+(n+1)^{n-2} e^{n-1}\right) \frac{d^n}{dt^n}e^{t-e^t}=(-1)^n\left((-1)^n+(-1)^{n+1}S_{n+1}^{(2)}e^t+(-1)^nS_{n+1}^{(3)}e^{2t}+(-1)^{n+1}S_{n+1}^{(4)}e^{3t}+\dots+e^{nt}\right) \frac{d^n}{dt^n}e^{a(t-e^t)} \srt_3(z) 0<|z|<1.3,"['derivatives', 'roots', 'closed-form', 'tetration', 'lagrange-inversion']"
95,Doubt about Rolle's theorem,Doubt about Rolle's theorem,,"When we use Rolle's theorem successfuly, it is because the function in the analysed interval $[a, b]$ is continuous, differentiable in $(a, b)$ and $f(a)$ = $f(b)$ . I am asked to prove that $x^3-3x+b=0$ has exactly and only a single root in $[-1, 1]$ using only Rolle's theorem. I beg you to excuse me, because I know the answer is duplicated, but I am (maybe) too stupid to understand it. It is impossible to use Rolle's theorem because $f(-1)\neq f(1)$ for every $b\in\mathbb{R}$ , so we must use intermediate value theorem. However, I can't see any relation between the fact that there is a $f'(c)=\frac{f(b) - f(a)}{b - a}$ in $(a, b)$ and the roots of $f(x)$ . For the second time, please, please please, you are welcome to close my question for being duplicated (better say cuatriplicated) and excuse me for annoying purposely, but at least have mercy and explain it to me.","When we use Rolle's theorem successfuly, it is because the function in the analysed interval is continuous, differentiable in and = . I am asked to prove that has exactly and only a single root in using only Rolle's theorem. I beg you to excuse me, because I know the answer is duplicated, but I am (maybe) too stupid to understand it. It is impossible to use Rolle's theorem because for every , so we must use intermediate value theorem. However, I can't see any relation between the fact that there is a in and the roots of . For the second time, please, please please, you are welcome to close my question for being duplicated (better say cuatriplicated) and excuse me for annoying purposely, but at least have mercy and explain it to me.","[a, b] (a, b) f(a) f(b) x^3-3x+b=0 [-1, 1] f(-1)\neq f(1) b\in\mathbb{R} f'(c)=\frac{f(b) - f(a)}{b - a} (a, b) f(x)","['calculus', 'derivatives']"
96,Distributional differential equations (delta) [closed],Distributional differential equations (delta) [closed],,"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed last year . Improve this question I'm learning the basics of distributional theory. Reading a book, I've found this exercises: Find the general solution of $$(x-1)T= \delta$$ $$(x-1)T =\delta'$$ I've tried different starting points but no solutions. Please, can you help me?","Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed last year . Improve this question I'm learning the basics of distributional theory. Reading a book, I've found this exercises: Find the general solution of I've tried different starting points but no solutions. Please, can you help me?",(x-1)T= \delta (x-1)T =\delta',"['ordinary-differential-equations', 'derivatives', 'partial-differential-equations', 'distribution-theory', 'dirac-delta']"
97,Derivative of a continuous bilinear form,Derivative of a continuous bilinear form,,"I'm trying to solve below exercise Let $(H, \langle \cdot, \cdot \rangle)$ be a real Hilbert space. Let $a: H \times H \rightarrow \mathbb{R}$ be a continuous bilinear form. Determine the derivative of $F: H \to \mathbb R, v \mapsto a(v, v)$ . Could you confirm if I correctly apply below Lemma and the chain rule? Lemma Let $E_1, \ldots, E_m, F$ be Banach spaces over the field $\mathbb{K} \in \{\mathbb{R}, \mathbb{C}\}$ . Let $\varphi: E_1 \times \cdots \times E_m \to F$ be a continuous multilinear map. Then $\varphi$ is continuously differentiable with $\partial \varphi (x_1, \ldots, x_m) : E_1 \times \cdots \times E_m \to F$ such that $$ \partial \varphi (x_1, \ldots, x_m) [h_1, \ldots, h_m] = \sum_{j=1}^m \varphi\left(x_1, \ldots, x_{j-1}, h_j, x_{j+1}, \ldots, x_m\right) $$ for every $(h_1, \ldots, h_m) \in E_1 \times \cdots \times E_m$ . My attempt Let $G: H \to H \times H, v\mapsto (v, v)$ . Then $G$ is linear continuous. Then $\partial G (v) = G$ for all $v \in H$ . We have $F = a \circ G$ . By chain rule, $$ \partial F (v) = \partial a(G(v)) \circ \partial G (v) = \partial a(v, v) \circ G. $$ By Lemma , $$ \partial F (v) [u] = a(u, v) + a(v, u). $$","I'm trying to solve below exercise Let be a real Hilbert space. Let be a continuous bilinear form. Determine the derivative of . Could you confirm if I correctly apply below Lemma and the chain rule? Lemma Let be Banach spaces over the field . Let be a continuous multilinear map. Then is continuously differentiable with such that for every . My attempt Let . Then is linear continuous. Then for all . We have . By chain rule, By Lemma ,","(H, \langle \cdot, \cdot \rangle) a: H \times H \rightarrow \mathbb{R} F: H \to \mathbb R, v \mapsto a(v, v) E_1, \ldots, E_m, F \mathbb{K} \in \{\mathbb{R}, \mathbb{C}\} \varphi: E_1 \times \cdots \times E_m \to F \varphi \partial \varphi (x_1, \ldots, x_m) : E_1 \times \cdots \times E_m \to F 
\partial \varphi (x_1, \ldots, x_m) [h_1, \ldots, h_m] = \sum_{j=1}^m \varphi\left(x_1, \ldots, x_{j-1}, h_j, x_{j+1}, \ldots, x_m\right)
 (h_1, \ldots, h_m) \in E_1 \times \cdots \times E_m G: H \to H \times H, v\mapsto (v, v) G \partial G (v) = G v \in H F = a \circ G 
\partial F (v) = \partial a(G(v)) \circ \partial G (v) = \partial a(v, v) \circ G.
 
\partial F (v) [u] = a(u, v) + a(v, u).
","['functional-analysis', 'derivatives', 'hilbert-spaces', 'banach-spaces', 'frechet-derivative']"
98,Brezis' exercise 5.14,Brezis' exercise 5.14,,"I'm trying to solve below exercise in Brezis' Functional Analysis Let $(H, \langle \cdot, \cdot \rangle)$ be a real Hilbert space and $\Vert \cdot\Vert $ its induced norm. Let $a: H \times H \rightarrow \mathbb{R}$ be a continuous bilinear form such that $$ a(v, v) \geq 0 \quad \forall v \in H $$ Prove that the function $F: H \to \mathbb R, v \mapsto a(v, v)$ is convex, of class $C^1$ , and determine its differential. The solution by the author is The convexity inequality $a(t u+(1-t) v, t u+(1-t) v) \leq t a(u, u)+$ $(1-t) a(v, v)$ is equivalent to $t(1-t) a(u-v, u-v) \geq 0$ . Consider the operator $A \in \mathcal{L}(H)$ defined by $a(u, v)=(A u, v)$ for all $u, v \in H$ . Then $F^{\prime}(u)=A u+A^{\star} u$ , since we have $$ F(u+h)-F(u)=\left(A u+A^{\star} u, h\right)+a(h, h) . $$ I would like to verify that the explicit form of $F'(u):H \to \mathbb R$ is indeed $F'(u)[v] = a(u, v)+ a(v, u)$ for all $v\in H$ . Could you have a check on my attempt? Because $a$ is continuous, there is $C>0$ such that $\Vert a(u, v)\Vert  \le C\Vert u\Vert \Vert v\Vert $ for all $u, v\in H$ . Then $$ \begin{align} & \lim_{v \to u} \frac{\Vert F(v)- F(u) - F'(u)[v-u]\Vert }{\Vert v-u\Vert } \\ = & \lim_{v \to u} \frac{\Vert a(v, v)- a(u, u) - (a(u, v-u)+ a(v-u, u))\Vert }{\Vert v-u\Vert } \\ = & \lim_{v \to u} \frac{\Vert a(v-u, v-u)\Vert }{\Vert v-u\Vert } \\ \le & \lim_{v \to u} C \Vert v-u\Vert =0. \end{align} $$ This completes the proof.","I'm trying to solve below exercise in Brezis' Functional Analysis Let be a real Hilbert space and its induced norm. Let be a continuous bilinear form such that Prove that the function is convex, of class , and determine its differential. The solution by the author is The convexity inequality is equivalent to . Consider the operator defined by for all . Then , since we have I would like to verify that the explicit form of is indeed for all . Could you have a check on my attempt? Because is continuous, there is such that for all . Then This completes the proof.","(H, \langle \cdot, \cdot \rangle) \Vert \cdot\Vert  a: H \times H \rightarrow \mathbb{R} 
a(v, v) \geq 0 \quad \forall v \in H
 F: H \to \mathbb R, v \mapsto a(v, v) C^1 a(t u+(1-t) v, t u+(1-t) v) \leq t a(u, u)+ (1-t) a(v, v) t(1-t) a(u-v, u-v) \geq 0 A \in \mathcal{L}(H) a(u, v)=(A u, v) u, v \in H F^{\prime}(u)=A u+A^{\star} u 
F(u+h)-F(u)=\left(A u+A^{\star} u, h\right)+a(h, h) .
 F'(u):H \to \mathbb R F'(u)[v] = a(u, v)+ a(v, u) v\in H a C>0 \Vert a(u, v)\Vert  \le C\Vert u\Vert \Vert v\Vert  u, v\in H 
\begin{align}
& \lim_{v \to u} \frac{\Vert F(v)- F(u) - F'(u)[v-u]\Vert }{\Vert v-u\Vert } \\
= & \lim_{v \to u} \frac{\Vert a(v, v)- a(u, u) - (a(u, v-u)+ a(v-u, u))\Vert }{\Vert v-u\Vert } \\
= & \lim_{v \to u} \frac{\Vert a(v-u, v-u)\Vert }{\Vert v-u\Vert } \\
\le & \lim_{v \to u} C \Vert v-u\Vert =0.
\end{align}
","['functional-analysis', 'derivatives', 'hilbert-spaces', 'frechet-derivative']"
99,Finding matrix least square problem,Finding matrix least square problem,,"If you have a function of the form $$ f(x) = \frac{1}{2}\left\lVert Ax - y \right\rVert_2^2 $$ We know that if $A \in \mathbb{R}^{n \times m}, x \in \mathbb{R}^m$ and $y \in \mathbb{R}^n$ we can find the minimizer by differentiating $$ \nabla_x f = A^TAx - A^Ty $$ setting this to $0$ leads to the linear system $$ A^TAx = A^Ty. $$ Suppose now instead of $x$ being unknown the unknown is $A$ . In this case we have $$ f(A) = \frac{1}{2} \left\lVert Ax - y \right\rVert_2^2 $$ To calculate the gradient w.r.t. $A$ I proceed as follows (assuming as a norm for $A$ I am using the 2 norm). $$ \lim_{E \to 0} \frac{\left| f(A + E) - f(A) - T(A)E \right|}{\left\lVert E \right\rVert_2} $$ With a little bit of calculation we can show that $$ f(A + E) - f(A) =  \left(x^TA^T - y^T\right)Ex +\left\lVert Ex \right\rVert_2^2 $$ Substituing this into the limit and using the squeeze theorem I get $$ 0 \leq \lim_{E \to 0} \frac{\left| \left(x^TA^T - y^T\right)Ex +\left\lVert Ex \right\rVert_2^2 - T(A)E \right|}{\left\lVert E \right\rVert_2} \leq \lim_{E \to 0} \frac{\left| \left(x^TA^T - y^T\right)Ex  - T(A)E \right| +\left\lVert Ex \right\rVert_2^2 }{\left\lVert E \right\rVert_2} \leq \lim_{E \to 0} \frac{\left\lVert \left(x^TA^T - y^T\right)(\cdot)x  - T(A) \right\rVert_{{\mathbb{R}^{n \times m}}^*} \left\lVert E \right\rVert_2 +\left\lVert Ex \right\rVert_2^2 }{\left\lVert E \right\rVert_2} = \lim_{E \to 0} \left\lVert \left(x^TA^T - y^T\right)(\cdot)x  - T(A) \right\rVert_{{\mathbb{R}^{n \times m}}^*} $$ The last limit is equal to 0 iff $$ T(A) = \left(x^TA^T - y^T\right)(\cdot)x $$ Question 1: Is my calculation of the differential correct? Assuming it is I was trying to characterize $T(A)$ using a basis $E_{ij} = \delta_{ij}$ by doing this I get $$ T(A)E_{ij} = \left(x^TA^T - y^T\right)E_{ij}x = \left(x^TA^T - y^T\right)x_j e_i = x_j \left(x^TA^T - y^T\right) e_i = x_j \left(x^TA^T e_i  - y^T e_i \right) = x_j \left(x^TA^T  e_i  - y_i \right) $$ Question 2 : Is this correct? This should give me a set of equations that I should be able to solve for $A$ ?",If you have a function of the form We know that if and we can find the minimizer by differentiating setting this to leads to the linear system Suppose now instead of being unknown the unknown is . In this case we have To calculate the gradient w.r.t. I proceed as follows (assuming as a norm for I am using the 2 norm). With a little bit of calculation we can show that Substituing this into the limit and using the squeeze theorem I get The last limit is equal to 0 iff Question 1: Is my calculation of the differential correct? Assuming it is I was trying to characterize using a basis by doing this I get Question 2 : Is this correct? This should give me a set of equations that I should be able to solve for ?,"
f(x) = \frac{1}{2}\left\lVert Ax - y \right\rVert_2^2
 A \in \mathbb{R}^{n \times m}, x \in \mathbb{R}^m y \in \mathbb{R}^n 
\nabla_x f = A^TAx - A^Ty
 0 
A^TAx = A^Ty.
 x A 
f(A) = \frac{1}{2} \left\lVert Ax - y \right\rVert_2^2
 A A 
\lim_{E \to 0} \frac{\left| f(A + E) - f(A) - T(A)E \right|}{\left\lVert E \right\rVert_2}
 
f(A + E) - f(A) =  \left(x^TA^T - y^T\right)Ex +\left\lVert Ex \right\rVert_2^2
 
0 \leq \lim_{E \to 0} \frac{\left| \left(x^TA^T - y^T\right)Ex +\left\lVert Ex \right\rVert_2^2 - T(A)E \right|}{\left\lVert E \right\rVert_2} \leq \lim_{E \to 0} \frac{\left| \left(x^TA^T - y^T\right)Ex  - T(A)E \right| +\left\lVert Ex \right\rVert_2^2 }{\left\lVert E \right\rVert_2} \leq \lim_{E \to 0} \frac{\left\lVert \left(x^TA^T - y^T\right)(\cdot)x  - T(A) \right\rVert_{{\mathbb{R}^{n \times m}}^*} \left\lVert E \right\rVert_2 +\left\lVert Ex \right\rVert_2^2 }{\left\lVert E \right\rVert_2} = \lim_{E \to 0} \left\lVert \left(x^TA^T - y^T\right)(\cdot)x  - T(A) \right\rVert_{{\mathbb{R}^{n \times m}}^*}
 
T(A) = \left(x^TA^T - y^T\right)(\cdot)x
 T(A) E_{ij} = \delta_{ij} 
T(A)E_{ij} = \left(x^TA^T - y^T\right)E_{ij}x = \left(x^TA^T - y^T\right)x_j e_i = x_j \left(x^TA^T - y^T\right) e_i = x_j \left(x^TA^T e_i  - y^T e_i \right) = x_j \left(x^TA^T  e_i  - y_i \right)
 A","['linear-algebra', 'derivatives', 'solution-verification']"
