,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,Integral with trigonometric and dilogarithm,Integral with trigonometric and dilogarithm,,"The following integral can be characterised as an exotic one! Evaluate the integral: $$\int_{0}^{\pi/2} \left ( \frac{\log  \left ( \tan x +1 \right )}{\log \tan x} - \frac{{\rm Li}_2 \left ( -\cot x \right )}{\log^2 \cot x} - \frac{\zeta(2)}{2 \log^2 \tan x} \right ) \, {\rm d}x$$ It was proposed by Cornel Ioan Valean, Romania. I don't have any clue on how to attack this. Also, I don't think I can come up with any strategy that could work. So, I leave it entirely up to the community to come up with a clever approach. Edit: The name of the proposer is Cornel not Corel that I had written.","The following integral can be characterised as an exotic one! Evaluate the integral: $$\int_{0}^{\pi/2} \left ( \frac{\log  \left ( \tan x +1 \right )}{\log \tan x} - \frac{{\rm Li}_2 \left ( -\cot x \right )}{\log^2 \cot x} - \frac{\zeta(2)}{2 \log^2 \tan x} \right ) \, {\rm d}x$$ It was proposed by Cornel Ioan Valean, Romania. I don't have any clue on how to attack this. Also, I don't think I can come up with any strategy that could work. So, I leave it entirely up to the community to come up with a clever approach. Edit: The name of the proposer is Cornel not Corel that I had written.",,"['real-analysis', 'integration', 'special-functions']"
1,Proving that Cauchy condensation test is true for $a_n$ any monotone sequence.,Proving that Cauchy condensation test is true for  any monotone sequence.,a_n,"I'm trying to show that then $\Sigma a_n$ converges iff $\Sigma 2^n a_{2^n}$ converges if $a_n$ is any monotone sequence. I am assuming I have to prove the cases: $a_n$ is positive and increasing $a_n$ is negative and increasing $a_n$ is positive and decreasing $a_n$ is negative and decreasing Here is what I have worked out from some of the comments below: Assume $a_n$ is an increasing sequence. If $a_n$ is positive, then $\lim a_n \neq 0$ and we have divergence. I don't really understand why this is. Is it because $a_n$ is unbounded and so it diverges to $\infty$? I don't know a rigorous way to prove this. I'm assuming I have to make use of the theorem that monotonic $s_n$ converges iff it is bounded. If $a_n$ is negative, then we have that $\lim a_n = 0$ Again, I don't know how to rigorously prove this. How do I find a bound for $a_n$? Let $s_n = a_1 + a_2 + ... + a_n$ and $t_n = a_1 + 2 a_2 + ... + 2^k a_{2^k}$.   For $n<2^k$, we have $s_n \geq a_1 + (a_2 + a_3) + ... _ (a_{2^k}+...+a_{2^{k+1}-1} \geq a_1 + a_2 + ... + 2^{k}a_{2k} = t_k $ And so $s_n \geq t_k$. Similar proof for $2s_{n} \leq t_k$ Someone below has already proved nonnegative decreasing case. I'm assuming that: If $a_n$ was decreasing and negative, then $\lim a_n \neq 0$ and we have divergence. Again, I'm not sure how to prove this.","I'm trying to show that then $\Sigma a_n$ converges iff $\Sigma 2^n a_{2^n}$ converges if $a_n$ is any monotone sequence. I am assuming I have to prove the cases: $a_n$ is positive and increasing $a_n$ is negative and increasing $a_n$ is positive and decreasing $a_n$ is negative and decreasing Here is what I have worked out from some of the comments below: Assume $a_n$ is an increasing sequence. If $a_n$ is positive, then $\lim a_n \neq 0$ and we have divergence. I don't really understand why this is. Is it because $a_n$ is unbounded and so it diverges to $\infty$? I don't know a rigorous way to prove this. I'm assuming I have to make use of the theorem that monotonic $s_n$ converges iff it is bounded. If $a_n$ is negative, then we have that $\lim a_n = 0$ Again, I don't know how to rigorously prove this. How do I find a bound for $a_n$? Let $s_n = a_1 + a_2 + ... + a_n$ and $t_n = a_1 + 2 a_2 + ... + 2^k a_{2^k}$.   For $n<2^k$, we have $s_n \geq a_1 + (a_2 + a_3) + ... _ (a_{2^k}+...+a_{2^{k+1}-1} \geq a_1 + a_2 + ... + 2^{k}a_{2k} = t_k $ And so $s_n \geq t_k$. Similar proof for $2s_{n} \leq t_k$ Someone below has already proved nonnegative decreasing case. I'm assuming that: If $a_n$ was decreasing and negative, then $\lim a_n \neq 0$ and we have divergence. Again, I'm not sure how to prove this.",,"['real-analysis', 'sequences-and-series', 'limits', 'proof-verification', 'convergence-divergence']"
2,"Lipschitz function $f\colon A \to \mathbb{R}$, $A \subseteq \mathbb{R}$ measurable.","Lipschitz function ,  measurable.",f\colon A \to \mathbb{R} A \subseteq \mathbb{R},"Suppose $A \subseteq \mathbb{R}$ is measurable and $f\colon A \to \mathbb{R}$ is Lipschitz on the set $A$, i.e there is some $K\ge 0$ such that $\lvert f(x)-f(y)\rvert \le K \lvert x-y\rvert$ for $x,y \in A$. I'm trying to prove that $$ m^\ast(f(E)) \le K\,m^\ast(E)\textrm{ for every set }E \subseteq A. $$ I've tried covering the set $E$ by intervals, yet the function $f$ may not have been defined outside the set $A$. Also, approaching the set $E$ from inside can encounter problem when $E$ is non-measurable.","Suppose $A \subseteq \mathbb{R}$ is measurable and $f\colon A \to \mathbb{R}$ is Lipschitz on the set $A$, i.e there is some $K\ge 0$ such that $\lvert f(x)-f(y)\rvert \le K \lvert x-y\rvert$ for $x,y \in A$. I'm trying to prove that $$ m^\ast(f(E)) \le K\,m^\ast(E)\textrm{ for every set }E \subseteq A. $$ I've tried covering the set $E$ by intervals, yet the function $f$ may not have been defined outside the set $A$. Also, approaching the set $E$ from inside can encounter problem when $E$ is non-measurable.",,"['real-analysis', 'measure-theory', 'lebesgue-measure', 'lipschitz-functions', 'bounded-variation']"
3,What is so stunning or elegant about this theorem?,What is so stunning or elegant about this theorem?,,"I read an essay by a mathematician (whose name I forgot) who visited TIFR in India and he was writing about his mathematical experiences during the visit. In his essay the mathematician had written that he was stunned by the elegance and beauty of the following theorem. Let $a_i$ be integers and $$ f(x) = x^n + a_{n-1}x^{n-1} + a_{n-2}x^{n-2} + \cdots + a_2 x^2 + a_1 x + a_0, $$ then $$\displaystyle\max_{x\in(-2,2)} |f(x)| \ge 2.$$ Questions : What is so stunning or elegant about it? I am looking for a proof Any reference to this theorem in literature? Note : I am trying to recall and search the essay. I shall update my post with its link if I am able to find it.","I read an essay by a mathematician (whose name I forgot) who visited TIFR in India and he was writing about his mathematical experiences during the visit. In his essay the mathematician had written that he was stunned by the elegance and beauty of the following theorem. Let $a_i$ be integers and $$ f(x) = x^n + a_{n-1}x^{n-1} + a_{n-2}x^{n-2} + \cdots + a_2 x^2 + a_1 x + a_0, $$ then $$\displaystyle\max_{x\in(-2,2)} |f(x)| \ge 2.$$ Questions : What is so stunning or elegant about it? I am looking for a proof Any reference to this theorem in literature? Note : I am trying to recall and search the essay. I shall update my post with its link if I am able to find it.",,"['real-analysis', 'analysis', 'polynomials']"
4,Least upper bound property implies Cauchy completeness,Least upper bound property implies Cauchy completeness,,"Given an ordered field $F$, the following two statements are equivalent: $F$ has the Least-Upper-Bound property. $F$ is Archimedean and $F$ is ""sequentially complete""/""Cauchy complete"" (all Cauchy sequences in $F$ converge). For some reason I have been unable to find a proof of this result on the web.","Given an ordered field $F$, the following two statements are equivalent: $F$ has the Least-Upper-Bound property. $F$ is Archimedean and $F$ is ""sequentially complete""/""Cauchy complete"" (all Cauchy sequences in $F$ converge). For some reason I have been unable to find a proof of this result on the web.",,"['real-analysis', 'real-numbers', 'ordered-fields']"
5,Proof of $(K_1+K_2)^* = K_1^*\cap K_2^*$: the dual of sum of convex cones is same to the intersection of duals of convex cones,Proof of : the dual of sum of convex cones is same to the intersection of duals of convex cones,(K_1+K_2)^* = K_1^*\cap K_2^*,"Let $K\subset R^n$ be a cone: $\lambda x \in K$ if $x \in K$ and $\lambda>0$. Let $K^*$ be the conjugate cone of $K$, defined as $K^* = \{x^* | \langle x, x^*\rangle \ge 0, \forall x \in K\}$. I am trying to prove the next theorem and do not understand why convexity is needed for $K_1$ and $K_2$. Let $K_1$ and $K_2$ be convex cones in $R^n$. Then   \begin{equation} (K_1+K_2)^* = K_1^*\cap K_2^*. \end{equation} My trial is as follows: If $x^* \in K_1^*\cap K_2^*$, then $\langle x_1+x_2, x^*\rangle = \langle x_1,x^*\rangle + \langle x_2,x^*\rangle \ge 0$ for every $x_1 \in K_1$ and $x_2 \in K_2$, so $x^* \in (K_1+K_2)^*$ and $K_1^* \cap K_2^* \subset (K_1 + K_2)^*$. Suppose $x^* \in (K_1+K_2)^*$ but $x^* \notin K_1^*$. Then, there exists $x_1 \in K_1$ such that $\langle x_1, x^*\rangle < 0$. Since $K_2$ is a cone, $\lambda x_2 \in K_2$ for every $x_2 \in K_2$ and $\lambda>0$. Thus, $\langle x_2,x^*\rangle$ can be arbitrarily small and there exists $x_2 \in K_2$ such that $\langle x_1,x^*\rangle + \langle x_2,x^*\rangle < 0$. It contradicts the assumption that $x^* \in (K_1+K_2)^*$. Therefore, if $x^* \in (K_1+K_2)^*$, then $x^*\in K_1^*\cap K_2^*$ and $(K_1+K_2)^* \subset K_1^*\cap K_2^*$. It completes the proof. Where should I use convexity of $K_1$ and $K_2$ or can I remove convexity from the Theorem?","Let $K\subset R^n$ be a cone: $\lambda x \in K$ if $x \in K$ and $\lambda>0$. Let $K^*$ be the conjugate cone of $K$, defined as $K^* = \{x^* | \langle x, x^*\rangle \ge 0, \forall x \in K\}$. I am trying to prove the next theorem and do not understand why convexity is needed for $K_1$ and $K_2$. Let $K_1$ and $K_2$ be convex cones in $R^n$. Then   \begin{equation} (K_1+K_2)^* = K_1^*\cap K_2^*. \end{equation} My trial is as follows: If $x^* \in K_1^*\cap K_2^*$, then $\langle x_1+x_2, x^*\rangle = \langle x_1,x^*\rangle + \langle x_2,x^*\rangle \ge 0$ for every $x_1 \in K_1$ and $x_2 \in K_2$, so $x^* \in (K_1+K_2)^*$ and $K_1^* \cap K_2^* \subset (K_1 + K_2)^*$. Suppose $x^* \in (K_1+K_2)^*$ but $x^* \notin K_1^*$. Then, there exists $x_1 \in K_1$ such that $\langle x_1, x^*\rangle < 0$. Since $K_2$ is a cone, $\lambda x_2 \in K_2$ for every $x_2 \in K_2$ and $\lambda>0$. Thus, $\langle x_2,x^*\rangle$ can be arbitrarily small and there exists $x_2 \in K_2$ such that $\langle x_1,x^*\rangle + \langle x_2,x^*\rangle < 0$. It contradicts the assumption that $x^* \in (K_1+K_2)^*$. Therefore, if $x^* \in (K_1+K_2)^*$, then $x^*\in K_1^*\cap K_2^*$ and $(K_1+K_2)^* \subset K_1^*\cap K_2^*$. It completes the proof. Where should I use convexity of $K_1$ and $K_2$ or can I remove convexity from the Theorem?",,"['real-analysis', 'analysis', 'convex-analysis', 'convex-optimization']"
6,"Can the ""radius of analyticity"" of a smooth real function be smaller than the radius of convergence of its Taylor series without being zero?","Can the ""radius of analyticity"" of a smooth real function be smaller than the radius of convergence of its Taylor series without being zero?",,"Does there exist an infinitely differentiable function $f:U\to\mathbb{R}$, where $U$ is open subset of $\mathbb{R}$, such that the Taylor series of $f$ at $x=x_0\in U$ has radius of convergence $R>0$ $f$ equals its Taylor series only on the subinterval $(x_0-r,x_0+r)$, where $\color{red}{0<}r<R$ The customary examples of smooth real functions that fail to be analytic, e.g. $e^{-1/x}$ or $e^{-1/x^2}$ at $x=0$, have $R=\infty$ but $r=0$. The substance of the question is whether we can find a less extreme example for which analyticity at $x=x_0$ gives out only at a nonzero radius smaller than the radius of convergence of the Taylor series. Note: I don't really know complex analysis, but I know that the easiest path to whatever the truth is here is probably through the complex domain.","Does there exist an infinitely differentiable function $f:U\to\mathbb{R}$, where $U$ is open subset of $\mathbb{R}$, such that the Taylor series of $f$ at $x=x_0\in U$ has radius of convergence $R>0$ $f$ equals its Taylor series only on the subinterval $(x_0-r,x_0+r)$, where $\color{red}{0<}r<R$ The customary examples of smooth real functions that fail to be analytic, e.g. $e^{-1/x}$ or $e^{-1/x^2}$ at $x=0$, have $R=\infty$ but $r=0$. The substance of the question is whether we can find a less extreme example for which analyticity at $x=x_0$ gives out only at a nonzero radius smaller than the radius of convergence of the Taylor series. Note: I don't really know complex analysis, but I know that the easiest path to whatever the truth is here is probably through the complex domain.",,"['real-analysis', 'complex-analysis', 'taylor-expansion', 'analyticity']"
7,A property of continuous bijective maps in the plane.,A property of continuous bijective maps in the plane.,,"Take a non-empty open connected bounded set $D$ of $\mathbb{R}^2$ and let $f:D\to D$ be a continuous bijective map. It is well known that $f$ could have no fixed points i. e. $f(p)\not=p$ for all $p\in D$. I wonder if the following weaker property holds: there is a point $p\in D$ such that $d(f(p),\partial D)=d(p,\partial D)$ where $d(z,\partial D)$ is the euclidean distance from a point $z\in \mathbb{R}^2$ to the boundary of $D$. Any existing reference in the literature will be appreciated.","Take a non-empty open connected bounded set $D$ of $\mathbb{R}^2$ and let $f:D\to D$ be a continuous bijective map. It is well known that $f$ could have no fixed points i. e. $f(p)\not=p$ for all $p\in D$. I wonder if the following weaker property holds: there is a point $p\in D$ such that $d(f(p),\partial D)=d(p,\partial D)$ where $d(z,\partial D)$ is the euclidean distance from a point $z\in \mathbb{R}^2$ to the boundary of $D$. Any existing reference in the literature will be appreciated.",,"['real-analysis', 'fixed-point-theorems']"
8,Is $\Delta C_c^\infty$ a dense subset of $L^p(\mathbb{R}^d)$?,Is  a dense subset of ?,\Delta C_c^\infty L^p(\mathbb{R}^d),"I'm struggling to obtain some density result. It is well known that $C^\infty_c(\mathbb{R}^d)$ is dense in $L^p (\mathbb{R}^d)$ for $1\leq p<\infty$. It is well known that for $\lambda>0$, $(\lambda -\Delta) C_c^\infty$ is dense in $L^p(\mathbb{R}^d)$ for $1\leq p <\infty$. Proof of this fact requires maximum principle and Hahn-Banach theorem, and Riesz representation theorem. (Stated in Krylov's Elliptic and Parabolic equation in Sobolev spaces) I'm wondering whether $\Delta C_c^\infty(\mathbb{R}^d)$  is dense in $L^p (\mathbb{R}^d)$. I tried by using Newtonial potential, but I fail to obtain the desired result because I found $C^\infty$ function, but I cannot make a sequence of $C^\infty$ functions with compact support. Even I tried cut-off method, I cannot guarantee the fact.","I'm struggling to obtain some density result. It is well known that $C^\infty_c(\mathbb{R}^d)$ is dense in $L^p (\mathbb{R}^d)$ for $1\leq p<\infty$. It is well known that for $\lambda>0$, $(\lambda -\Delta) C_c^\infty$ is dense in $L^p(\mathbb{R}^d)$ for $1\leq p <\infty$. Proof of this fact requires maximum principle and Hahn-Banach theorem, and Riesz representation theorem. (Stated in Krylov's Elliptic and Parabolic equation in Sobolev spaces) I'm wondering whether $\Delta C_c^\infty(\mathbb{R}^d)$  is dense in $L^p (\mathbb{R}^d)$. I tried by using Newtonial potential, but I fail to obtain the desired result because I found $C^\infty$ function, but I cannot make a sequence of $C^\infty$ functions with compact support. Even I tried cut-off method, I cannot guarantee the fact.",,"['real-analysis', 'functional-analysis', 'partial-differential-equations', 'harmonic-analysis', 'potential-theory']"
9,Spaces of functions similar to $L^p$ but with different local and global sizes,Spaces of functions similar to  but with different local and global sizes,L^p,"Obviously much of analysis on $\mathbb R^n$ considers $L^p$ spaces or other Banach spaces derived from them. The definition of $L^p(\mathbb R^n)$ looks very natural, but I've been bothered for some time that what it measures might not so natural. My issue specifically is that it measures the ""local"" size of $f$ (say $f\cdot\mathbf1_{|f|>1}$) with the same $p$ that it measures the ""global"" size (say $f\cdot\mathbf1_{|f|\leq1}$). I intuitively feel like the local and global sizes have nothing to do with each other and therefore it is arbitrary to say they should be measured with the same exponent. It seems to me then that analysis would benefit from some Banach space $X^{p,q}$ that lets you measure the function locally in $p$ and globally in $q$. To illustrate my concern, consider $|x|^{-a}$ ($a>0$), which are not in any $L^p$ spaces! But we have $|x|^{-a}\in X^{p,q}$ as long as $n/q<a<n/p$. It's then clear how $L^p=X^{p,p}$ is just a special case, and there's no $a$ to satisfy $n/p<a<n/p$. An explicit norm I thought of that might do the job is $$\|\varphi\|_{X^{p,q}}=\inf_{K\subset\subset\mathbb R^n}\left(\|\varphi\cdot\mathbf1_K\|_{L^p}+\|\varphi\cdot\mathbf1_{K^c}\|_{L^q}\right).$$ The closest I've seen to this is what Lemarie-Rieusset (Recent Developments in the Navier-Stokes Problem, 2002) calls $WL^\infty$ with norm $$\|\varphi\|_{WL^\infty}=\sum_{k\in\mathbb Z^n}\sup_{x-k\in[0,1]^n}|\varphi(x)|,$$ which resembles what my above notation would call $X^{\infty,1}$. So please tell me, am I wrong that this is something natural to consider? It seems that analysts have gotten pretty far without it. Perhaps there's something very special about measuring the local and global parts of a function in the same way. Or maybe such a space is known and I haven't come across it.","Obviously much of analysis on $\mathbb R^n$ considers $L^p$ spaces or other Banach spaces derived from them. The definition of $L^p(\mathbb R^n)$ looks very natural, but I've been bothered for some time that what it measures might not so natural. My issue specifically is that it measures the ""local"" size of $f$ (say $f\cdot\mathbf1_{|f|>1}$) with the same $p$ that it measures the ""global"" size (say $f\cdot\mathbf1_{|f|\leq1}$). I intuitively feel like the local and global sizes have nothing to do with each other and therefore it is arbitrary to say they should be measured with the same exponent. It seems to me then that analysis would benefit from some Banach space $X^{p,q}$ that lets you measure the function locally in $p$ and globally in $q$. To illustrate my concern, consider $|x|^{-a}$ ($a>0$), which are not in any $L^p$ spaces! But we have $|x|^{-a}\in X^{p,q}$ as long as $n/q<a<n/p$. It's then clear how $L^p=X^{p,p}$ is just a special case, and there's no $a$ to satisfy $n/p<a<n/p$. An explicit norm I thought of that might do the job is $$\|\varphi\|_{X^{p,q}}=\inf_{K\subset\subset\mathbb R^n}\left(\|\varphi\cdot\mathbf1_K\|_{L^p}+\|\varphi\cdot\mathbf1_{K^c}\|_{L^q}\right).$$ The closest I've seen to this is what Lemarie-Rieusset (Recent Developments in the Navier-Stokes Problem, 2002) calls $WL^\infty$ with norm $$\|\varphi\|_{WL^\infty}=\sum_{k\in\mathbb Z^n}\sup_{x-k\in[0,1]^n}|\varphi(x)|,$$ which resembles what my above notation would call $X^{\infty,1}$. So please tell me, am I wrong that this is something natural to consider? It seems that analysts have gotten pretty far without it. Perhaps there's something very special about measuring the local and global parts of a function in the same way. Or maybe such a space is known and I haven't come across it.",,"['real-analysis', 'functional-analysis', 'lp-spaces']"
10,Differentiablity at $0$ of a function $f: \mathbb R \to \mathbb R$ which is twice differentiable in $\mathbb R \setminus \{0\}$,Differentiablity at  of a function  which is twice differentiable in,0 f: \mathbb R \to \mathbb R \mathbb R \setminus \{0\},"Let $f: \mathbb R \to \mathbb R$ be a function , twice differentiable in $\mathbb R \setminus \{0\}$ such that $f'(x)<0<f''(x) , \forall x <0$ and $f'(x)>0>f''(x) , \forall x >0$ ; then is it true that $f$ cannot be differentiable at $0$ ? EDIT : My attempt : Suppose $f'$ exists at $0$ , then $f$ is differentiable everywhere , then as $f'(-1)<0<f'(1)$ , so by Darboux theorem , there is $c \in \mathbb R$ such that $f'(c)=0$ , but $f'(x) \ne 0 , \forall x \ne 0$ so $c=0$  , thus $f'(0)=0$ . I cannot make any further progress , Please help . Thanks in advance","Let $f: \mathbb R \to \mathbb R$ be a function , twice differentiable in $\mathbb R \setminus \{0\}$ such that $f'(x)<0<f''(x) , \forall x <0$ and $f'(x)>0>f''(x) , \forall x >0$ ; then is it true that $f$ cannot be differentiable at $0$ ? EDIT : My attempt : Suppose $f'$ exists at $0$ , then $f$ is differentiable everywhere , then as $f'(-1)<0<f'(1)$ , so by Darboux theorem , there is $c \in \mathbb R$ such that $f'(c)=0$ , but $f'(x) \ne 0 , \forall x \ne 0$ so $c=0$  , thus $f'(0)=0$ . I cannot make any further progress , Please help . Thanks in advance",,['real-analysis']
11,Prove that between any two roots of $f$ there exists at least one root of $g$,Prove that between any two roots of  there exists at least one root of,f g,$$f(x)= 1 - e^x\sin(x)$$   $$g(x)= 1 + e^x\cos(x)$$ Prove that between any two roots of $f$ there exists at least one root of $g$. I know Rolle's Theorem and the Intermediate Value Theorem (I think) need to be used. Can someone show a step by step proof for this?,$$f(x)= 1 - e^x\sin(x)$$   $$g(x)= 1 + e^x\cos(x)$$ Prove that between any two roots of $f$ there exists at least one root of $g$. I know Rolle's Theorem and the Intermediate Value Theorem (I think) need to be used. Can someone show a step by step proof for this?,,"['real-analysis', 'proof-explanation']"
12,When Borel functions and Baire functions are equal?,When Borel functions and Baire functions are equal?,,"Suppose $X$ is compact metric space. Let $A$ be the smallest set of complex functions containing all continuous functions such that: If $f_n \in A$ are uniformly bounded and $f_n \to f$ pointwise then $f \in A$. Is it true that $A$ is equal to the set of all Borel functions? If not, what is relation between these two? What assumptions are needed to get this? Answers as well as references will be appreciated.","Suppose $X$ is compact metric space. Let $A$ be the smallest set of complex functions containing all continuous functions such that: If $f_n \in A$ are uniformly bounded and $f_n \to f$ pointwise then $f \in A$. Is it true that $A$ is equal to the set of all Borel functions? If not, what is relation between these two? What assumptions are needed to get this? Answers as well as references will be appreciated.",,"['real-analysis', 'analysis', 'measure-theory', 'descriptive-set-theory']"
13,"There is no function from $\mathbb{R} \to (0, \infty)$ satisfying $f'(x)=f(f(x))$",There is no function from  satisfying,"\mathbb{R} \to (0, \infty) f'(x)=f(f(x))","Problem: Prove that there is no differentiable function from $\mathbb{R} \to (0, \infty)$ satisfying $f'(x)=f(f(x))$. I could not make much progress, except for observing that any derivatives (any order, whenever they exist) are always positive. Please help. Even hints are appreciated.","Problem: Prove that there is no differentiable function from $\mathbb{R} \to (0, \infty)$ satisfying $f'(x)=f(f(x))$. I could not make much progress, except for observing that any derivatives (any order, whenever they exist) are always positive. Please help. Even hints are appreciated.",,"['calculus', 'real-analysis', 'functional-equations']"
14,Limit similar to $\lim_{n \to \infty} \left(1-\frac{1}{n} \right)^n = \text{e}^{-1}$,Limit similar to,\lim_{n \to \infty} \left(1-\frac{1}{n} \right)^n = \text{e}^{-1},I want to show that  $$ \lim_{n \to \infty} \left(1-\frac{n}{n^2} \right) \left(1-\frac{n}{n^2-1} \right)  \cdot \ldots \cdot \left(1-\frac{n}{n^2-n+1} \right) = \lim_{n \to \infty} \prod_{k=0}^{n-1} \left(1-\frac{n}{n^2 - k} \right) = \mathrm{e}^{-1} $$ I know that $\lim_{n \to \infty} \left(1-\frac{1}{n} \right)^n = \text{e}^{-1}$ and so my attempt was to write $$  \left(1-\frac{n}{n^2} \right) \left(1-\frac{n}{n^2-1} \right)  \cdot \ldots \cdot \left(1-\frac{n}{n^2-n+1} \right)\\  = \underbrace{\left(1-\frac{1}{n} \right)^n}_{\to \text{e}^{-1}} \cdot \underbrace{\frac{1-\frac{n}{n^2-1}}{1-\frac{1}{n}}}_{\rightarrow 1}  \cdot \ldots \cdot \underbrace{\frac{1-\frac{n}{n^2-n+1}}{1-\frac{1}{n}}}_{\rightarrow 1} $$ which I thought would solve my problem. But after a second look I see that splitting the limits in the product cannot be allowed. This would be the same nonsense as  $$ \underbrace{\left(1-\frac{n}{n^2} \right)}_{\rightarrow 1} \underbrace{\left(1-\frac{n}{n^2-1} \right)}_{\rightarrow 1}  \cdot \ldots \cdot \underbrace{\left(1-\frac{n}{n^2-n+1} \right)}_{\rightarrow 1}.$$ Maybe anybody can make the situation clear to me.,I want to show that  $$ \lim_{n \to \infty} \left(1-\frac{n}{n^2} \right) \left(1-\frac{n}{n^2-1} \right)  \cdot \ldots \cdot \left(1-\frac{n}{n^2-n+1} \right) = \lim_{n \to \infty} \prod_{k=0}^{n-1} \left(1-\frac{n}{n^2 - k} \right) = \mathrm{e}^{-1} $$ I know that $\lim_{n \to \infty} \left(1-\frac{1}{n} \right)^n = \text{e}^{-1}$ and so my attempt was to write $$  \left(1-\frac{n}{n^2} \right) \left(1-\frac{n}{n^2-1} \right)  \cdot \ldots \cdot \left(1-\frac{n}{n^2-n+1} \right)\\  = \underbrace{\left(1-\frac{1}{n} \right)^n}_{\to \text{e}^{-1}} \cdot \underbrace{\frac{1-\frac{n}{n^2-1}}{1-\frac{1}{n}}}_{\rightarrow 1}  \cdot \ldots \cdot \underbrace{\frac{1-\frac{n}{n^2-n+1}}{1-\frac{1}{n}}}_{\rightarrow 1} $$ which I thought would solve my problem. But after a second look I see that splitting the limits in the product cannot be allowed. This would be the same nonsense as  $$ \underbrace{\left(1-\frac{n}{n^2} \right)}_{\rightarrow 1} \underbrace{\left(1-\frac{n}{n^2-1} \right)}_{\rightarrow 1}  \cdot \ldots \cdot \underbrace{\left(1-\frac{n}{n^2-n+1} \right)}_{\rightarrow 1}.$$ Maybe anybody can make the situation clear to me.,,"['calculus', 'real-analysis', 'limits', 'exponential-function']"
15,"Is there a positive function $f$ on real line such that $f(x)f(y)\le|x-y|, \forall x\in \mathbb Q , \forall y \in \mathbb R \setminus \mathbb Q$?",Is there a positive function  on real line such that ?,"f f(x)f(y)\le|x-y|, \forall x\in \mathbb Q , \forall y \in \mathbb R \setminus \mathbb Q","Does there exist a  function $f:\mathbb R \to (0,\infty)$ such that $f(x)f(y)\le|x-y|, \forall x\in \mathbb Q , \forall y \in \mathbb R \setminus \mathbb Q$ ?","Does there exist a  function $f:\mathbb R \to (0,\infty)$ such that $f(x)f(y)\le|x-y|, \forall x\in \mathbb Q , \forall y \in \mathbb R \setminus \mathbb Q$ ?",,"['real-analysis', 'inequality']"
16,Does there exist unique $u \in V$ satisfying integral equation?,Does there exist unique  satisfying integral equation?,u \in V,"Set$$V = \{v \in H^1(0, 1) : v(0) = 0\}.$$Given $f \in L^2(0, 1)$ such that ${1\over x}f(x) \in L^2(0, 1)$, does there exist a unique $u \in V$ satisfying$$\int_0^1 u'(x)v'(x)\,dx + \int_0^1 {{u(x)v(x)}\over{x^2}}\,dx = \int_0^1 {{f(x)v(x)}\over{x^2}}\,dx \text{ for all }v \in V?$$","Set$$V = \{v \in H^1(0, 1) : v(0) = 0\}.$$Given $f \in L^2(0, 1)$ such that ${1\over x}f(x) \in L^2(0, 1)$, does there exist a unique $u \in V$ satisfying$$\int_0^1 u'(x)v'(x)\,dx + \int_0^1 {{u(x)v(x)}\over{x^2}}\,dx = \int_0^1 {{f(x)v(x)}\over{x^2}}\,dx \text{ for all }v \in V?$$",,"['real-analysis', 'integration']"
17,"Are there any coherent, complete, mathematical systems that do not imply the existence of the infinite?","Are there any coherent, complete, mathematical systems that do not imply the existence of the infinite?",,"Basically, are there any systems(complete with axioms) that do not imply the existence of an infinity? Is it possible to construct a mathematical system without infinity?","Basically, are there any systems(complete with axioms) that do not imply the existence of an infinity? Is it possible to construct a mathematical system without infinity?",,"['real-analysis', 'meta-math']"
18,Explanation of a step in a proof of Steinhaus' Theorem,Explanation of a step in a proof of Steinhaus' Theorem,,"Disclaimer : I understand this theorem has been discussed before. I am not looking for a proof however, just a clarification of a mysterious step. I was reading a proof of the following version of Steinhaus' Theorem, and got stuck on one step: Theorem (Steinhaus) : Let $E \subset \mathbb{R}$ and $m(E)>0$, where $m$ denotes Lebesgue measure on $\mathbb{R}$. Then the set $E-E$   contains an interval around zero. Proof : By the Lebesgue differentiation theorem, or, equivalently, by the definition of a Lebesgue point, there exists an interval $I$   such that $$m(I\cap E) \geq (1-\epsilon)\,m(I).$$ If the result were not true, there would exist a sequence $x_n \rightarrow 0$ with $x_n \not \in E-E$. Take $n$ large enough that $|x_n|<\epsilon\, m(I)$. Then $x_n +E \subset \mathbb {R} \setminus E$, but $$\mathbf{m(I\cap (x_n+E))\geq (1-3\epsilon)\,m(I)}$$ (since Lebesgue measure is invariant under translations.) Since $E$ and $\mathbb R \setminus E$ are disjoint, this is a contradiction. The part I am having difficulty with is in bold characters. I believe it follows from the following: \begin{align*} m(I\cap (x_n+E)) & = m(x_n+(I-x_n)\cap E) \\ & = m((I-x_n)\cap E) \\ & \geq m(I) - 2|x_n| - m(I \setminus E) \\ & \geq (1-3\epsilon)\,m(I) \end{align*} Is this correct? If not, could you point out the correct steps? Thanks! Note : For the sake of completeness, $$E-E:=\{x-y:\ x,y\in E \}.$$","Disclaimer : I understand this theorem has been discussed before. I am not looking for a proof however, just a clarification of a mysterious step. I was reading a proof of the following version of Steinhaus' Theorem, and got stuck on one step: Theorem (Steinhaus) : Let $E \subset \mathbb{R}$ and $m(E)>0$, where $m$ denotes Lebesgue measure on $\mathbb{R}$. Then the set $E-E$   contains an interval around zero. Proof : By the Lebesgue differentiation theorem, or, equivalently, by the definition of a Lebesgue point, there exists an interval $I$   such that $$m(I\cap E) \geq (1-\epsilon)\,m(I).$$ If the result were not true, there would exist a sequence $x_n \rightarrow 0$ with $x_n \not \in E-E$. Take $n$ large enough that $|x_n|<\epsilon\, m(I)$. Then $x_n +E \subset \mathbb {R} \setminus E$, but $$\mathbf{m(I\cap (x_n+E))\geq (1-3\epsilon)\,m(I)}$$ (since Lebesgue measure is invariant under translations.) Since $E$ and $\mathbb R \setminus E$ are disjoint, this is a contradiction. The part I am having difficulty with is in bold characters. I believe it follows from the following: \begin{align*} m(I\cap (x_n+E)) & = m(x_n+(I-x_n)\cap E) \\ & = m((I-x_n)\cap E) \\ & \geq m(I) - 2|x_n| - m(I \setminus E) \\ & \geq (1-3\epsilon)\,m(I) \end{align*} Is this correct? If not, could you point out the correct steps? Thanks! Note : For the sake of completeness, $$E-E:=\{x-y:\ x,y\in E \}.$$",,"['real-analysis', 'measure-theory', 'lebesgue-measure']"
19,Computing $\sum{\frac{1}{m^2+n^2}}$,Computing,\sum{\frac{1}{m^2+n^2}},"I want to prove that $\sum_{1\leq m^2+n^2\leq R^2}{\frac{1}{m^2+n^2}}=2\pi\log R+O(1)$ as $R\rightarrow\infty$. For this, I'm trying to approximate the sum by using the integral $\int_{1\leq r\leq R}{\frac{1}{x^2+y^2}}dxdy=\int_{0}^{2\pi}\int_{1}^{R}{\frac{1}{r}}drd\theta=2\pi\log R$ How can I show that $\sum_{1\leq m^2+n^2\leq R^2}{\frac{1}{m^2+n^2}}=\int_{1\leq r\leq R}{\frac{1}{x^2+y^2}}dxdy$ as $R\rightarrow\infty$ rigorously? Any help will be appreciated.","I want to prove that $\sum_{1\leq m^2+n^2\leq R^2}{\frac{1}{m^2+n^2}}=2\pi\log R+O(1)$ as $R\rightarrow\infty$. For this, I'm trying to approximate the sum by using the integral $\int_{1\leq r\leq R}{\frac{1}{x^2+y^2}}dxdy=\int_{0}^{2\pi}\int_{1}^{R}{\frac{1}{r}}drd\theta=2\pi\log R$ How can I show that $\sum_{1\leq m^2+n^2\leq R^2}{\frac{1}{m^2+n^2}}=\int_{1\leq r\leq R}{\frac{1}{x^2+y^2}}dxdy$ as $R\rightarrow\infty$ rigorously? Any help will be appreciated.",,"['calculus', 'real-analysis', 'summation']"
20,Is every compact subset of $\Bbb{R}$ the support of some Borel measure?,Is every compact subset of  the support of some Borel measure?,\Bbb{R},"I have tried to prove the exercise 2.12 in Rudin's RCA: 12 Show that every compact subset of $\Bbb{R}$ is the support of a Borel measure. For perfect (i.e. no isolated point) compact $K$ with non-zero Lebesgue measure, we just take the Lebesgue measure. However I struck on the general case. My attempt is as follows: For every compact $K$, we can find a decreasing sequence of open set $\langle W_n\rangle_n$ s.t. $\bigcap_n W_n =K$ and $m(W_n-K)<1/n$. Find a continuous function $h_n$ s.t. $h_n(x) = 1$ for $x\in K$ and $h_n(x) = 0$ outside of $V_n$. (such function exists by Urysohn's lemma.) For continuous function $f$ with compact support define $$\Lambda f = \lim_{n\to\infty}\frac{\int_\Bbb{R} fh_ndm}{\int_\Bbb{R} h_ndm}.$$ I can not certain that the $\Lambda$ is well-defined. But if it is well-defined, then it gives an regular Borel measure (by Riesz representation theorem) $\mu$ s.t. $\mu(K)=1$. I guess that the $\mu$ has a support $K$ but I don't know how to get it. Thanks for any help!","I have tried to prove the exercise 2.12 in Rudin's RCA: 12 Show that every compact subset of $\Bbb{R}$ is the support of a Borel measure. For perfect (i.e. no isolated point) compact $K$ with non-zero Lebesgue measure, we just take the Lebesgue measure. However I struck on the general case. My attempt is as follows: For every compact $K$, we can find a decreasing sequence of open set $\langle W_n\rangle_n$ s.t. $\bigcap_n W_n =K$ and $m(W_n-K)<1/n$. Find a continuous function $h_n$ s.t. $h_n(x) = 1$ for $x\in K$ and $h_n(x) = 0$ outside of $V_n$. (such function exists by Urysohn's lemma.) For continuous function $f$ with compact support define $$\Lambda f = \lim_{n\to\infty}\frac{\int_\Bbb{R} fh_ndm}{\int_\Bbb{R} h_ndm}.$$ I can not certain that the $\Lambda$ is well-defined. But if it is well-defined, then it gives an regular Borel measure (by Riesz representation theorem) $\mu$ s.t. $\mu(K)=1$. I guess that the $\mu$ has a support $K$ but I don't know how to get it. Thanks for any help!",,"['real-analysis', 'measure-theory']"
21,How to take the derivative of this strange alternative to a devil's slippery staircase?,How to take the derivative of this strange alternative to a devil's slippery staircase?,,"I am trying to construct a function $f(x)$ which has derivative = $0$ almost everywhere, and is strictly increasing. I realize one can do this by playing with the Cantor function, but I would like to do it directly. However, I am having difficulty seeing if the beautiful monster has a derivative at all! Let $(a_k)$ be a sequence of numbers, such that $\sum |a_k| < \infty$, and $a_k > 0$. Define an ordering on $q_k \in \mathbb{Q}$, which one can do because $\mathbb{Q}$ is countable and totally ordered. Define $f: \mathbb{R} \to \mathbb{R}$, $$f(x) = \sum\limits_{\{k \text{ : } q_k < x\}} a_k$$ We see that $f_k$ is strictly increasing (thus is both increasing and non-constant on any open interval), for $\mathbb{Q}$ is dense in $\mathbb{R}$, and any open interval in $\mathbb{R}$ contains an element of $\mathbb{Q}$. I would think this function would have derivative zero on all but $\mathbb{Q}$, which are totally disconnected and countable, and a disjoint union of countably many points has Lesbegue measure 0. The issue is that I don't know how to rigorously show that $|f'| = 0$ a.e., indeed, I don't know how to check that $f$ is differentiable. I don't know how to see that this $$\lim_{x \to x_0} \frac{f(x) - f(x_0)}{ x - x_0}$$ has limit $0$ on $S \subset \mathbb{R}$ such that the Lesbegue measure of $\{ \mathbb{R} - S \}$ is $0$. Here is my question: How do I take the derivative of this function? Does this function satisfy the desired criterion?","I am trying to construct a function $f(x)$ which has derivative = $0$ almost everywhere, and is strictly increasing. I realize one can do this by playing with the Cantor function, but I would like to do it directly. However, I am having difficulty seeing if the beautiful monster has a derivative at all! Let $(a_k)$ be a sequence of numbers, such that $\sum |a_k| < \infty$, and $a_k > 0$. Define an ordering on $q_k \in \mathbb{Q}$, which one can do because $\mathbb{Q}$ is countable and totally ordered. Define $f: \mathbb{R} \to \mathbb{R}$, $$f(x) = \sum\limits_{\{k \text{ : } q_k < x\}} a_k$$ We see that $f_k$ is strictly increasing (thus is both increasing and non-constant on any open interval), for $\mathbb{Q}$ is dense in $\mathbb{R}$, and any open interval in $\mathbb{R}$ contains an element of $\mathbb{Q}$. I would think this function would have derivative zero on all but $\mathbb{Q}$, which are totally disconnected and countable, and a disjoint union of countably many points has Lesbegue measure 0. The issue is that I don't know how to rigorously show that $|f'| = 0$ a.e., indeed, I don't know how to check that $f$ is differentiable. I don't know how to see that this $$\lim_{x \to x_0} \frac{f(x) - f(x_0)}{ x - x_0}$$ has limit $0$ on $S \subset \mathbb{R}$ such that the Lesbegue measure of $\{ \mathbb{R} - S \}$ is $0$. Here is my question: How do I take the derivative of this function? Does this function satisfy the desired criterion?",,"['real-analysis', 'analysis', 'measure-theory', 'derivatives']"
22,Do two embeddings of a Euclidean space into a higher dimensional one only differ by a diffeomorphism?,Do two embeddings of a Euclidean space into a higher dimensional one only differ by a diffeomorphism?,,"Let $d\le n$ and $$f,g\colon\mathbb{R}^d\hookrightarrow\mathbb{R}^n$$ be two smooth embeddings. Is there a diffeomorphism $$\phi\colon\mathbb{R}^n\rightarrow \mathbb{R}^n,$$ such that $$f=\phi\circ g$$ holds? In other words, does the diffeomorpshism group $\operatorname{Diff}{(\mathbb{R}^n)}$ act transitively on the set of embeddings $\operatorname{Emb}(\mathbb{R}^d,\mathbb{R}^n)$ by postcomposition?","Let $d\le n$ and $$f,g\colon\mathbb{R}^d\hookrightarrow\mathbb{R}^n$$ be two smooth embeddings. Is there a diffeomorphism $$\phi\colon\mathbb{R}^n\rightarrow \mathbb{R}^n,$$ such that $$f=\phi\circ g$$ holds? In other words, does the diffeomorpshism group $\operatorname{Diff}{(\mathbb{R}^n)}$ act transitively on the set of embeddings $\operatorname{Emb}(\mathbb{R}^d,\mathbb{R}^n)$ by postcomposition?",,"['real-analysis', 'differential-geometry', 'manifolds', 'differential-topology', 'geometric-topology']"
23,The limit points of $\{\sin (n+n^2)\mid n\in\mathbb N\}$,The limit points of,\{\sin (n+n^2)\mid n\in\mathbb N\},"We can prove that  $\exists \{c_i\}_{i=1}^{\infty} \subseteq \mathbb N$ and $c_1<c_2<c_3<...$ such that $\lim_{i\rightarrow \infty}\sin (c_i^{\alpha})=c$, $\forall c\in [-1,1]$,where $\alpha $ is a positive rational number. But what about the limit points of { $\sin(n+n^2)\mid n\in N\} $? Is it also the closed interval $[-1,1]$? Furthermore, what about $\sin \bigg(\sum_{i=1}^ma_in^{i}\bigg)$, where all coefficients $a_i$ are rational numbers?","We can prove that  $\exists \{c_i\}_{i=1}^{\infty} \subseteq \mathbb N$ and $c_1<c_2<c_3<...$ such that $\lim_{i\rightarrow \infty}\sin (c_i^{\alpha})=c$, $\forall c\in [-1,1]$,where $\alpha $ is a positive rational number. But what about the limit points of { $\sin(n+n^2)\mid n\in N\} $? Is it also the closed interval $[-1,1]$? Furthermore, what about $\sin \bigg(\sum_{i=1}^ma_in^{i}\bigg)$, where all coefficients $a_i$ are rational numbers?",,"['real-analysis', 'analysis', 'cauchy-sequences']"
24,Haar measure - a problem from Folland,Haar measure - a problem from Folland,,"I was presented with this question from Folland's real analysis second edition involving Haar measures. It is problem 3 of chapter 11 page 347, which reads as follows: Let G be a locally compact group that is homeomorphic to an open subset U of   $ \mathbb{R}^n $ in such a way that, if we identify G with U, left translation is an affine map - that is, $ xy = A_x(y) + b_x $ where $ A_x $ is a linear transformation of $ \mathbb{R}^n $ and $ b_x \in \mathbb{R}^n $. Then $ det| A_x |^{-1} dx $ is a left Haar measure on G, where dx denotes Lebesgue measure on   $ \mathbb{R}^n $. (Similarly for right translations and right Haar measures.) I should mention we just got to Haar measures and topological groups in my class so it has not fully sunk in, and I have no idea how to do this. Thanks all helpers","I was presented with this question from Folland's real analysis second edition involving Haar measures. It is problem 3 of chapter 11 page 347, which reads as follows: Let G be a locally compact group that is homeomorphic to an open subset U of   $ \mathbb{R}^n $ in such a way that, if we identify G with U, left translation is an affine map - that is, $ xy = A_x(y) + b_x $ where $ A_x $ is a linear transformation of $ \mathbb{R}^n $ and $ b_x \in \mathbb{R}^n $. Then $ det| A_x |^{-1} dx $ is a left Haar measure on G, where dx denotes Lebesgue measure on   $ \mathbb{R}^n $. (Similarly for right translations and right Haar measures.) I should mention we just got to Haar measures and topological groups in my class so it has not fully sunk in, and I have no idea how to do this. Thanks all helpers",,"['real-analysis', 'measure-theory', 'topological-groups']"
25,Closed form of series involving Fibonacci numbers,Closed form of series involving Fibonacci numbers,,"Let $F_n$ denote the $n$-th Fibonacci number and $\phi$ be the golden ratio, that $\phi = \frac{1+\sqrt{5}}{2}$. Find a closed form for the sum: $$\sum_{n=0}^{\infty} \frac{1}{(5\phi)^n(n+2)} \sum_{k=0}^{n} \frac{F_{k+1} F_{n-k+1}}{k+1}$$ I don't know how to tackle the sum. A closed form exists as the book suggests but there is no hind nor answer on how to tackle it. And I have not come up with some idea to kill this sum. Can anyone help me find a closed form?","Let $F_n$ denote the $n$-th Fibonacci number and $\phi$ be the golden ratio, that $\phi = \frac{1+\sqrt{5}}{2}$. Find a closed form for the sum: $$\sum_{n=0}^{\infty} \frac{1}{(5\phi)^n(n+2)} \sum_{k=0}^{n} \frac{F_{k+1} F_{n-k+1}}{k+1}$$ I don't know how to tackle the sum. A closed form exists as the book suggests but there is no hind nor answer on how to tackle it. And I have not come up with some idea to kill this sum. Can anyone help me find a closed form?",,"['real-analysis', 'sequences-and-series', 'fibonacci-numbers']"
26,Challenging $\lim_{x \rightarrow 10} \frac{1}{\lfloor x \rfloor} = \frac{1}{10}$ for $\epsilon=\frac{1}{2}$.,Challenging  for .,\lim_{x \rightarrow 10} \frac{1}{\lfloor x \rfloor} = \frac{1}{10} \epsilon=\frac{1}{2},"Consider the (incorrect) claim that $$\lim_{x \rightarrow 10} \frac{1}{\lfloor x \rfloor} = \frac{1}{10}.$$ How might I find the largest $\delta$ such that I can challenge $\epsilon = 1/2$? Clearly I need to use the $\epsilon$-$\delta$ definition of a limit, but my problem is that I don't know how to find the largest delta. Here is what I know: $|f(x) - (1/10)| <1/2$ $0<|x-10|< \delta$","Consider the (incorrect) claim that $$\lim_{x \rightarrow 10} \frac{1}{\lfloor x \rfloor} = \frac{1}{10}.$$ How might I find the largest $\delta$ such that I can challenge $\epsilon = 1/2$? Clearly I need to use the $\epsilon$-$\delta$ definition of a limit, but my problem is that I don't know how to find the largest delta. Here is what I know: $|f(x) - (1/10)| <1/2$ $0<|x-10|< \delta$",,"['calculus', 'real-analysis', 'limits', 'epsilon-delta', 'ceiling-and-floor-functions']"
27,Limit of $\frac{f\left(x+g\left(x\right)\right)-f\left(g\left(x\right)\right)}{x}$ as $x\to 0$,Limit of  as,\frac{f\left(x+g\left(x\right)\right)-f\left(g\left(x\right)\right)}{x} x\to 0,"I'm trying to answer the following question: Let $f$ be continuously differentiable in all of $\mathbb{R}$ and let $g:\mathbb{R}\to\mathbb{R}$ be a function satisfying $\lim_{x\to0}g\left(x\right)=0$. Show that   $$\lim_{x\to0} \frac{f\left(x+g\left(x\right)\right)-f\left(g\left(x\right)\right)}{x}=f'(0)$$ At first I though it would be pretty simple triangular inequality from the definition of a limit, but it doesn't really work out for me... let $\epsilon>0$, we can find $\delta$ such that $$\left|x\right|<\delta\implies\left|\frac{f\left(x\right)-f\left(0\right)}{x}-f'\left(0\right)\right|<\frac{\epsilon}{2}$$     But as $g\left(x\right)\to0$ we can find $\delta_{g}$ such that $$\left|x\right|<\delta_{g}\implies\left|g\left(x\right)\right|<\frac{\delta}{2}$$ Hence for $\left|x\right|<\min\left\{ \delta_{g},\frac{\delta}{2}\right\}$ we have that $x+g\left(x\right)<\delta$, and $$\left|\frac{f\left(x+g\left(x\right)\right)-f\left(g\left(x\right)\right)}{x}-f'\left(0\right)\right|=\left|\frac{f\left(x+g\left(x\right)\right)-f\left(0\right)+f\left(0\right)-f\left(g\left(x\right)\right)}{x}-f'\left(0\right)\right|$$ Any direction?","I'm trying to answer the following question: Let $f$ be continuously differentiable in all of $\mathbb{R}$ and let $g:\mathbb{R}\to\mathbb{R}$ be a function satisfying $\lim_{x\to0}g\left(x\right)=0$. Show that   $$\lim_{x\to0} \frac{f\left(x+g\left(x\right)\right)-f\left(g\left(x\right)\right)}{x}=f'(0)$$ At first I though it would be pretty simple triangular inequality from the definition of a limit, but it doesn't really work out for me... let $\epsilon>0$, we can find $\delta$ such that $$\left|x\right|<\delta\implies\left|\frac{f\left(x\right)-f\left(0\right)}{x}-f'\left(0\right)\right|<\frac{\epsilon}{2}$$     But as $g\left(x\right)\to0$ we can find $\delta_{g}$ such that $$\left|x\right|<\delta_{g}\implies\left|g\left(x\right)\right|<\frac{\delta}{2}$$ Hence for $\left|x\right|<\min\left\{ \delta_{g},\frac{\delta}{2}\right\}$ we have that $x+g\left(x\right)<\delta$, and $$\left|\frac{f\left(x+g\left(x\right)\right)-f\left(g\left(x\right)\right)}{x}-f'\left(0\right)\right|=\left|\frac{f\left(x+g\left(x\right)\right)-f\left(0\right)+f\left(0\right)-f\left(g\left(x\right)\right)}{x}-f'\left(0\right)\right|$$ Any direction?",,"['real-analysis', 'derivatives']"
28,"Asymptotic behavior of $\int_0^1 \text{min}\{1,x^{-d}(1-x^d)^n\} dx$ as $n\to\infty$",Asymptotic behavior of  as,"\int_0^1 \text{min}\{1,x^{-d}(1-x^d)^n\} dx n\to\infty","I am trying to find bounds on \begin{equation*} \int_0^1 \text{min}\{1,x^{-d}(1-x^d)^n\} dx~\text{as}~n\to\infty,~d\in\mathbb{N}.  \end{equation*} I found this integral trying to bound some probability, therefore, I have no hope for a closed form and I am only interested in the behavior as $n\to\infty$, for fixed $d\in\mathbb{N}$. Unfortunately, I can't even get rid of the minimum myself.","I am trying to find bounds on \begin{equation*} \int_0^1 \text{min}\{1,x^{-d}(1-x^d)^n\} dx~\text{as}~n\to\infty,~d\in\mathbb{N}.  \end{equation*} I found this integral trying to bound some probability, therefore, I have no hope for a closed form and I am only interested in the behavior as $n\to\infty$, for fixed $d\in\mathbb{N}$. Unfortunately, I can't even get rid of the minimum myself.",,"['calculus', 'real-analysis']"
29,Prove every subset of $\Bbb N$ is countable.,Prove every subset of  is countable.,\Bbb N,"This isn't a homework problem.  I've seen a proof of the following statement online, and I think the proof is suspect, or at least incomplete. Theorem .  Every subset of $\Bbb N$ is countable. Proof .  Let $A \subseteq N$ .  Suppose without loss of generality $A$ is not finite.  Since $\Bbb N$ is well-ordered, $A$ has a least element $a_{1}$ .  Since $A$ is infinite, $A - \{a_{1}\} \neq \emptyset$ , and again by the well-ordering of $\Bbb N$ , there is a least element $a_{2} \in A - \{a_{1} \}$ . Proceeding inductively, for each $k \in \Bbb N$ , we can find $a_{k} \in A - \{a_{1}, a_{2}, \dots, a_{k - 1} \}$ with $a_{1} < a_{2} < \dots < a_{k}$ . Then $A = \{a_{1}, a_{2}, \dots \}$ , and so $A$ is countable, as desired. I don't think this proof is complete.  It should be shown that $A \subseteq \{a_{1}, a_{2}, \dots \}$ , right?  I don't think this is obvious because pretend we have a different ordering on $\Bbb N$ , i.e., the following ordering: $$1 < 3 <5 < \dots < 2 < 4 < 6 < \dots$$ Then if $A = \{1, 2, 3, \dots \}$ , using the above procedure, we would only get the odd numbers, so we wouldn't be able to say $A \subseteq \{1, 3, 5, \dots \}$ .  Does my objection to the proof of the theorem make sense?  How do you complete the proof (i.e., how do you show the containment I want to show)?","This isn't a homework problem.  I've seen a proof of the following statement online, and I think the proof is suspect, or at least incomplete. Theorem .  Every subset of is countable. Proof .  Let .  Suppose without loss of generality is not finite.  Since is well-ordered, has a least element .  Since is infinite, , and again by the well-ordering of , there is a least element . Proceeding inductively, for each , we can find with . Then , and so is countable, as desired. I don't think this proof is complete.  It should be shown that , right?  I don't think this is obvious because pretend we have a different ordering on , i.e., the following ordering: Then if , using the above procedure, we would only get the odd numbers, so we wouldn't be able to say .  Does my objection to the proof of the theorem make sense?  How do you complete the proof (i.e., how do you show the containment I want to show)?","\Bbb N A \subseteq N A \Bbb N A a_{1} A A - \{a_{1}\} \neq \emptyset \Bbb N a_{2} \in A - \{a_{1} \} k \in \Bbb N a_{k} \in A - \{a_{1}, a_{2}, \dots, a_{k - 1} \} a_{1} < a_{2} < \dots < a_{k} A = \{a_{1}, a_{2}, \dots \} A A \subseteq \{a_{1}, a_{2}, \dots \} \Bbb N 1 < 3 <5 < \dots < 2 < 4 < 6 < \dots A = \{1, 2, 3, \dots \} A \subseteq \{1, 3, 5, \dots \}","['real-analysis', 'elementary-set-theory']"
30,Inclusions of $\ell^p$ and $L^p$ spaces,Inclusions of  and  spaces,\ell^p L^p,"I remember seeing this some time ago, but I can't find the examples anywhere. Recall that if $p<q$, then $\ell^p\subseteq\ell^q$ and $L^q[0,1]\subseteq L^p[0,1]$. So we can ask ourselves if any of the equalities $$\ell^p=\bigcap_{q>p}\ell^q,\quad \ell^q=\bigcup_{p<q}\ell^p,\quad L^p[0,1]=\bigcup_{q>p}L^q[0,1],\quad L^q[0,1]=\bigcap_{p<q}L^p[0,1]$$ is true. If I remember it correctly, none of these is true, but I simply managed to find a counterexample for the first one: the sequence $(n^{-1/p})$ belongs to $\bigcap_{q>p}\ell^q$ but not to $\ell^p$. Moreover, I recall that $L^p(\mathbb{R})$ and $L^q(\mathbb{R})$ do not satisfy any inclusion relation. What would be counter-examples to those equalities? Thank you.","I remember seeing this some time ago, but I can't find the examples anywhere. Recall that if $p<q$, then $\ell^p\subseteq\ell^q$ and $L^q[0,1]\subseteq L^p[0,1]$. So we can ask ourselves if any of the equalities $$\ell^p=\bigcap_{q>p}\ell^q,\quad \ell^q=\bigcup_{p<q}\ell^p,\quad L^p[0,1]=\bigcup_{q>p}L^q[0,1],\quad L^q[0,1]=\bigcap_{p<q}L^p[0,1]$$ is true. If I remember it correctly, none of these is true, but I simply managed to find a counterexample for the first one: the sequence $(n^{-1/p})$ belongs to $\bigcap_{q>p}\ell^q$ but not to $\ell^p$. Moreover, I recall that $L^p(\mathbb{R})$ and $L^q(\mathbb{R})$ do not satisfy any inclusion relation. What would be counter-examples to those equalities? Thank you.",,"['real-analysis', 'lp-spaces']"
31,"How did Rudin conclude his argument there is no ""boundary"" between convergent and divergent series?","How did Rudin conclude his argument there is no ""boundary"" between convergent and divergent series?",,"I lost my baby Rudin book on real analysis book but I recall a pair of results in homework exercises that he seemed to indicate that there is no ""boundary"" between convergent and divergent series of positive decreasing terms. One result was that if $a_n$ is positive decreasing, and $\sum_n a_n$ is divergent, then $\sum_n a_n/s_n$ is also divergent where $s_n$ is the $n$th partial sum of the $a_i$. So, since the original series diverges, we can keep repeating this construction to get series that diverge slower and slower, since $\lim_{n \to \infty} s_n = \infty$. Rudin paired this with another homework exercise result about convergent series, something showing that given any convergent series (possibly of positive decreasing terms) you could construct a new series that converged ""slower"" in some obvious sense. Can someone recall that result?","I lost my baby Rudin book on real analysis book but I recall a pair of results in homework exercises that he seemed to indicate that there is no ""boundary"" between convergent and divergent series of positive decreasing terms. One result was that if $a_n$ is positive decreasing, and $\sum_n a_n$ is divergent, then $\sum_n a_n/s_n$ is also divergent where $s_n$ is the $n$th partial sum of the $a_i$. So, since the original series diverges, we can keep repeating this construction to get series that diverge slower and slower, since $\lim_{n \to \infty} s_n = \infty$. Rudin paired this with another homework exercise result about convergent series, something showing that given any convergent series (possibly of positive decreasing terms) you could construct a new series that converged ""slower"" in some obvious sense. Can someone recall that result?",,"['real-analysis', 'sequences-and-series', 'analysis', 'reference-request', 'divergent-series']"
32,Can we change order of integration and differentiation here.,Can we change order of integration and differentiation here.,,"Assume that $P(x,y)$ have continuous first order partial derivatives. Let $\int P(x,y) dx$, denote the antiderivative with respect to x. Is it then true that $\frac{\partial}{\partial y}\int P(x,y)dx=\int \frac{\partial P(x,y)}{\partial y}dx$? I am struggling with how I can show this. Does it help if we restrict ourselves to a rectangle? Update: In the comments I got a tips of a Wikipedia article: http://en.wikipedia.org/wiki/Leibniz_integral_rule . I tried to modify the proof to my situation, can someone check if it is correct? Proof: Let $\{y_n\}$ be a sequence that goes to zero, but is never actually zero. We then have: $\frac{\partial }{\partial y}\int P(x,y) dx=\frac{\partial}{\partial y} [\int_a^xP(t,y)dt +C]=lim _{n \rightarrow \infty} \frac{\int_a^xP(t,y+y_n)dt-\int_a^xP(t,y)dt}{y_n}=lim_{n \rightarrow \infty}\frac{\int_a^x[P(t,y+y_n)-P(t,y)]dt}{y_n}$. Because of the same reason here: http://en.wikipedia.org/wiki/Leibniz_integral_rule#Proof_of_basic_form that the term inside the integral is bounded, we get that our intgral is bounded.(P and it's partial derivatives are continuous, we can restrict ourselves to a closed local place, so we get compactness). Since all the terms are Riemann integrable, they are alse Lebesgue integrable, and hence we can use the bounded convergence theorem to get(they didn't say anything about riemann vs: Lebesgue in the article, but I assume we have to do that?): $=\int_a^xlim_{n \rightarrow \infty}\frac{P(t,y+y_n)-P(t,y)}{y_n}dt=\int_a^x\frac{\partial  P(t,y)}{\partial y}dt=\int\frac{\partial P(x,y)}{\partial y}dx +C$ So in conclusion we have that: $\frac{\partial }{\partial y}\int P(x,y) dx=\int\frac{\partial P(x,y)}{\partial y}dx +C$, but when working with antiderivatives, we can overlook the constant, so we get: $\int\frac{\partial P(x,y)}{\partial y}dx $ Is this proof correct?","Assume that $P(x,y)$ have continuous first order partial derivatives. Let $\int P(x,y) dx$, denote the antiderivative with respect to x. Is it then true that $\frac{\partial}{\partial y}\int P(x,y)dx=\int \frac{\partial P(x,y)}{\partial y}dx$? I am struggling with how I can show this. Does it help if we restrict ourselves to a rectangle? Update: In the comments I got a tips of a Wikipedia article: http://en.wikipedia.org/wiki/Leibniz_integral_rule . I tried to modify the proof to my situation, can someone check if it is correct? Proof: Let $\{y_n\}$ be a sequence that goes to zero, but is never actually zero. We then have: $\frac{\partial }{\partial y}\int P(x,y) dx=\frac{\partial}{\partial y} [\int_a^xP(t,y)dt +C]=lim _{n \rightarrow \infty} \frac{\int_a^xP(t,y+y_n)dt-\int_a^xP(t,y)dt}{y_n}=lim_{n \rightarrow \infty}\frac{\int_a^x[P(t,y+y_n)-P(t,y)]dt}{y_n}$. Because of the same reason here: http://en.wikipedia.org/wiki/Leibniz_integral_rule#Proof_of_basic_form that the term inside the integral is bounded, we get that our intgral is bounded.(P and it's partial derivatives are continuous, we can restrict ourselves to a closed local place, so we get compactness). Since all the terms are Riemann integrable, they are alse Lebesgue integrable, and hence we can use the bounded convergence theorem to get(they didn't say anything about riemann vs: Lebesgue in the article, but I assume we have to do that?): $=\int_a^xlim_{n \rightarrow \infty}\frac{P(t,y+y_n)-P(t,y)}{y_n}dt=\int_a^x\frac{\partial  P(t,y)}{\partial y}dt=\int\frac{\partial P(x,y)}{\partial y}dx +C$ So in conclusion we have that: $\frac{\partial }{\partial y}\int P(x,y) dx=\int\frac{\partial P(x,y)}{\partial y}dx +C$, but when working with antiderivatives, we can overlook the constant, so we get: $\int\frac{\partial P(x,y)}{\partial y}dx $ Is this proof correct?",,"['real-analysis', 'integration', 'multivariable-calculus', 'partial-derivative']"
33,How was real analysis & topology taught in the 70's?,How was real analysis & topology taught in the 70's?,,"What was the 'gold standard' textbook before Rudin? Furthermore, if anyone has knowledge of what textbooks Princeton or Harvard used back in the 1960's or 70's, I would highly appreciate it if you could tell me.","What was the 'gold standard' textbook before Rudin? Furthermore, if anyone has knowledge of what textbooks Princeton or Harvard used back in the 1960's or 70's, I would highly appreciate it if you could tell me.",,"['real-analysis', 'general-topology', 'algebraic-topology', 'education', 'book-recommendation']"
34,"How to show that the integral is finite for $(x,t)$ such that $x>0$ and $t>0$?",How to show that the integral is finite for  such that  and ?,"(x,t) x>0 t>0","Given $g:[0,+\infty) \rightarrow R$ continuous and bounded, let $$u(x,t)=\frac{x}{\sqrt{4 \pi}}\int \limits_0^t \frac{1}{(t-s)^{3/2}}e^{\frac{-x^2}{4(t-s)}}g(s)ds$$ How to show that the integral is finite for $(x,t)$ such that $x>0$ and $t>0$? I tried to use $e^{-a} \le \frac{m!}{a^m}$ for arbitrary $m$. But I still don't know how to get it is finite. Am I on the right track?","Given $g:[0,+\infty) \rightarrow R$ continuous and bounded, let $$u(x,t)=\frac{x}{\sqrt{4 \pi}}\int \limits_0^t \frac{1}{(t-s)^{3/2}}e^{\frac{-x^2}{4(t-s)}}g(s)ds$$ How to show that the integral is finite for $(x,t)$ such that $x>0$ and $t>0$? I tried to use $e^{-a} \le \frac{m!}{a^m}$ for arbitrary $m$. But I still don't know how to get it is finite. Am I on the right track?",,"['calculus', 'real-analysis']"
35,Find the closed form of the digamma related series,Find the closed form of the digamma related series,,"The question I asked here Computing $\sum_{n=1}^{\infty} \left(\psi^{(0)}\left(\frac{1+n}{2}\right)-\psi^{(0)}\left(\frac{n}{2}\right)-\frac{1}{n}\right)$ made me think  to ask for your support for another question, that is $$\sum_{n=1}^{\infty} \frac{\displaystyle \left(\psi\left(\frac{n}{2}\right)+\frac{1}{n}\right)^2-\left(\psi\left(\frac{n+1}{2}\right)\right)^2}{n}$$ And here is a supplementary question (the alternating version) $$\sum_{n=1}^{\infty} (-1)^{n+1}\frac{\displaystyle \left(\psi\left(\frac{n}{2}\right)+\frac{1}{n}\right)^2-\left(\psi\left(\frac{n+1}{2}\right)\right)^2}{n}$$ Before anything else I need a starting point that you might point out to me. Besides that, can we finish all by only using series manipulation?","The question I asked here Computing $\sum_{n=1}^{\infty} \left(\psi^{(0)}\left(\frac{1+n}{2}\right)-\psi^{(0)}\left(\frac{n}{2}\right)-\frac{1}{n}\right)$ made me think  to ask for your support for another question, that is $$\sum_{n=1}^{\infty} \frac{\displaystyle \left(\psi\left(\frac{n}{2}\right)+\frac{1}{n}\right)^2-\left(\psi\left(\frac{n+1}{2}\right)\right)^2}{n}$$ And here is a supplementary question (the alternating version) $$\sum_{n=1}^{\infty} (-1)^{n+1}\frac{\displaystyle \left(\psi\left(\frac{n}{2}\right)+\frac{1}{n}\right)^2-\left(\psi\left(\frac{n+1}{2}\right)\right)^2}{n}$$ Before anything else I need a starting point that you might point out to me. Besides that, can we finish all by only using series manipulation?",,"['calculus', 'real-analysis', 'integration', 'sequences-and-series']"
36,Differentiate $f(x)=\int_x^{10}e^{-xy^2}dy$ with respect to $x$,Differentiate  with respect to,f(x)=\int_x^{10}e^{-xy^2}dy x,"I am trying to find $f'(x)$ when $0\leq x\leq 10$. I know I could use the formula given on this wikipedia page: http://en.wikipedia.org/wiki/Differentiation_under_the_integral_sign but I have been asked to justify all steps of the calculation so this isn't allowed. I have been given a hint to let $I(a,b,c)=\int_a^bf(x,c)dx$ and then told to show that $f$ satisfies all conditions necessary for FTC1 and the theorem of differentiation of integrals depending on a parameter. The problem I am having is translating $f(x)$ into something of the same form as $I(a,b,c)$. Can anyone help? EDIT: I think I've done it now using the method described by @mvggz . Is this the final answer once the $u$ has been substituted back out: $$ f'(x)=-\frac{1}{x} \int_x^{10} e^{-xy^2} dy + \frac{5}{x} e^{-100x}-\frac{3}{2}e^{-x^3}$$","I am trying to find $f'(x)$ when $0\leq x\leq 10$. I know I could use the formula given on this wikipedia page: http://en.wikipedia.org/wiki/Differentiation_under_the_integral_sign but I have been asked to justify all steps of the calculation so this isn't allowed. I have been given a hint to let $I(a,b,c)=\int_a^bf(x,c)dx$ and then told to show that $f$ satisfies all conditions necessary for FTC1 and the theorem of differentiation of integrals depending on a parameter. The problem I am having is translating $f(x)$ into something of the same form as $I(a,b,c)$. Can anyone help? EDIT: I think I've done it now using the method described by @mvggz . Is this the final answer once the $u$ has been substituted back out: $$ f'(x)=-\frac{1}{x} \int_x^{10} e^{-xy^2} dy + \frac{5}{x} e^{-100x}-\frac{3}{2}e^{-x^3}$$",,"['calculus', 'real-analysis']"
37,A limit related to the $\zeta(3)$ and the fractional part,A limit related to the  and the fractional part,\zeta(3),"I need some clues, hints for proving that $$\lim_{n\to\infty} n\frac{\displaystyle \left\{\frac{n}{\sqrt{1}}\right\}+ \left\{\frac{n}{\sqrt{2}}\right\}+ \left\{\frac{n}{\sqrt{3}}\right\}+\cdots  +\left\{\frac{n}{\sqrt{n^2}}\right\}}{\displaystyle \left\{\frac{n}{\sqrt[3]{1}}\right\}+ \left\{\frac{n}{\sqrt[3]{2}}\right\}+ \left\{\frac{n}{\sqrt[3]{3}}\right\}+\cdots  +\left\{\frac{n}{\sqrt[3]{n^3}}\right\}}=\frac{\pi^2-12}{6\zeta(3)-9}$$ My first idea was to use Stolz–Cesàro theorem , but this doesn't seem an easy thing to do.","I need some clues, hints for proving that $$\lim_{n\to\infty} n\frac{\displaystyle \left\{\frac{n}{\sqrt{1}}\right\}+ \left\{\frac{n}{\sqrt{2}}\right\}+ \left\{\frac{n}{\sqrt{3}}\right\}+\cdots  +\left\{\frac{n}{\sqrt{n^2}}\right\}}{\displaystyle \left\{\frac{n}{\sqrt[3]{1}}\right\}+ \left\{\frac{n}{\sqrt[3]{2}}\right\}+ \left\{\frac{n}{\sqrt[3]{3}}\right\}+\cdots  +\left\{\frac{n}{\sqrt[3]{n^3}}\right\}}=\frac{\pi^2-12}{6\zeta(3)-9}$$ My first idea was to use Stolz–Cesàro theorem , but this doesn't seem an easy thing to do.",,"['calculus', 'real-analysis', 'sequences-and-series', 'limits', 'summation']"
38,Is every open set a continuous image of a closed set? (in Euclidean space),Is every open set a continuous image of a closed set? (in Euclidean space),,Let $A \subset \mathbb{R}^n$ be an open subset. The question is whether $A$ can always be written as a continuous   image of a closed subset of euclidean space $f(C) = A$ for some closed subset $C \subset \mathbb{R}^m$ (for some $m$). So far I have been able to verify that it is true for open balls and the whole space and it seems to me like it must hold but i cant figure out how to approach proving it. Additionally if it is indeed true i'd like to know if there are any other more general spaces for which this applies.,Let $A \subset \mathbb{R}^n$ be an open subset. The question is whether $A$ can always be written as a continuous   image of a closed subset of euclidean space $f(C) = A$ for some closed subset $C \subset \mathbb{R}^m$ (for some $m$). So far I have been able to verify that it is true for open balls and the whole space and it seems to me like it must hold but i cant figure out how to approach proving it. Additionally if it is indeed true i'd like to know if there are any other more general spaces for which this applies.,,['real-analysis']
39,Examples of measures that induce certain inclusions in the Lp spaces.,Examples of measures that induce certain inclusions in the Lp spaces.,,"I apologize for the terribly worded title, but I didn't know how else to title this questions (which comes from Rudin's Real & Complex Analysis chapter 3 questions). The question says: For some measures, the relation $r<s$ implies $L^r(\mu)\subset L^s(\mu)$; for others, the inclusion is reversed; and there are some for which $L^r(\mu)$ does not contain $L^s(\mu)$ if $r\neq s$. Give examples of these situations, and find conditions on $\mu$ under which these situations will occur. I am having an extremely difficult time grasping any/everything having to do with measures and Lp spaces, so this question is mind-crippling. I don't even know where/how to begin thinking about such a question. Any input/help/criticism is greatly appreciated.","I apologize for the terribly worded title, but I didn't know how else to title this questions (which comes from Rudin's Real & Complex Analysis chapter 3 questions). The question says: For some measures, the relation $r<s$ implies $L^r(\mu)\subset L^s(\mu)$; for others, the inclusion is reversed; and there are some for which $L^r(\mu)$ does not contain $L^s(\mu)$ if $r\neq s$. Give examples of these situations, and find conditions on $\mu$ under which these situations will occur. I am having an extremely difficult time grasping any/everything having to do with measures and Lp spaces, so this question is mind-crippling. I don't even know where/how to begin thinking about such a question. Any input/help/criticism is greatly appreciated.",,"['real-analysis', 'measure-theory', 'lp-spaces']"
40,Closed form of $\sum_{n=1}^{\infty}(-1)^{n+1} n (\log(n^2+1)-\log(n^2))$,Closed form of,\sum_{n=1}^{\infty}(-1)^{n+1} n (\log(n^2+1)-\log(n^2)),"How would you start computing this series? $$\sum_{n=1}^{\infty}(-1)^{n+1} n (\log(n^2+1)-\log(n^2))$$ One of the ways to think of would be Frullani integral with the exponential function , but it's troublesome due to the power of  $n$ under logarithm. What else might I try?","How would you start computing this series? $$\sum_{n=1}^{\infty}(-1)^{n+1} n (\log(n^2+1)-\log(n^2))$$ One of the ways to think of would be Frullani integral with the exponential function , but it's troublesome due to the power of  $n$ under logarithm. What else might I try?",,"['calculus', 'real-analysis', 'integration', 'sequences-and-series']"
41,Is the derivative of a differentiable function continuous a.e.?,Is the derivative of a differentiable function continuous a.e.?,,"Let $f:[a,b]\rightarrow \mathbb{R}$ be a differentiable function. I know that $f'$ does not need to be continuous on $[a,b]$. However, all counterexamples I know has finite discontinuities. I want to know whether $f'$ is continuius a.e. on $[a,b]$. (Of course, under the Lebesgue measure)","Let $f:[a,b]\rightarrow \mathbb{R}$ be a differentiable function. I know that $f'$ does not need to be continuous on $[a,b]$. However, all counterexamples I know has finite discontinuities. I want to know whether $f'$ is continuius a.e. on $[a,b]$. (Of course, under the Lebesgue measure)",,['real-analysis']
42,Showing that $\sum_{i=1}^n \frac{1}{|x-p_i|} \leq 8n \left( 1 + \frac{1}{3} + \frac{1}{5} + \cdots + \frac{1}{2n-1} \right)$,Showing that,\sum_{i=1}^n \frac{1}{|x-p_i|} \leq 8n \left( 1 + \frac{1}{3} + \frac{1}{5} + \cdots + \frac{1}{2n-1} \right),"I'm taking a summer analysis course and preparing for our final exam later this week. Our professor gave us the following problem on our mock exam, and I can't seem to get anywhere on it. Does anyone have an idea of how one might proceed? I've tried thinking about it in terms of a geometric series, but that got me nowhere. I've also studied the behavior of the first several partial sums, but again to no avail. Let $0 \leq p_i \leq 1$ for $i = 1, 2, \dots, n$. Show that $$ \sum_{i=1}^n \frac{1}{|x-p_i|} \leq 8n \left( 1 + \frac{1}{3} + \frac{1}{5} + \cdots + \frac{1}{2n-1} \right)$$ for some $x$ satisfying $0 \leq x \leq 1$.","I'm taking a summer analysis course and preparing for our final exam later this week. Our professor gave us the following problem on our mock exam, and I can't seem to get anywhere on it. Does anyone have an idea of how one might proceed? I've tried thinking about it in terms of a geometric series, but that got me nowhere. I've also studied the behavior of the first several partial sums, but again to no avail. Let $0 \leq p_i \leq 1$ for $i = 1, 2, \dots, n$. Show that $$ \sum_{i=1}^n \frac{1}{|x-p_i|} \leq 8n \left( 1 + \frac{1}{3} + \frac{1}{5} + \cdots + \frac{1}{2n-1} \right)$$ for some $x$ satisfying $0 \leq x \leq 1$.",,"['real-analysis', 'sequences-and-series', 'summation']"
43,$\frac{db^x}{dx}$ without $e$,without,\frac{db^x}{dx} e,"For no other reason other than interest, I'm trying to find the general derivative of $b^x$ without using a definition of $e$ from a different context. I feel like, chronologically in history, this would have been the first time $e$ would have popped up in the context of calculus. Every proof of $\frac{db^x}{dx}$ I can find uses the result of $\frac{de^x}{dx}=e^x$. But at the time (and correct me if I'm wrong), $e$ wasn't really popularized. It was (almost) being used in disguise by Napier, only because $(1-10^{-7})^{10^7} \approx e^{-1}$. When Netwon came around, Bernoulli may have been looking to find the value of  $\lim_{n\to \infty}(1+1/n)^n$, but I don't see any motivation to consider $$\frac{d\left(\lim_{n\to \infty}(1+1/n)^n\right)^x}{dx}$$ before the general case $\frac{db^x}{dx}$. I'm sure along the way of finding the derivative of $b^x$, a  clear motivation for defining $e$ will pop-up .. but I'd like to find a proof that starts off assuming no prior knowledge of $e$. If you start from definition, you very quickly arrive at $$\frac{db^x}{dx} = b^x \lim_{h\to 0}{\frac{b^h-1}{h}}$$ but from here I'm stuck. How to show $\exists c \in \Re$ such that $$\lim_{x \to 0}\frac{c^x-1}{x}=1$$ to proceed?","For no other reason other than interest, I'm trying to find the general derivative of $b^x$ without using a definition of $e$ from a different context. I feel like, chronologically in history, this would have been the first time $e$ would have popped up in the context of calculus. Every proof of $\frac{db^x}{dx}$ I can find uses the result of $\frac{de^x}{dx}=e^x$. But at the time (and correct me if I'm wrong), $e$ wasn't really popularized. It was (almost) being used in disguise by Napier, only because $(1-10^{-7})^{10^7} \approx e^{-1}$. When Netwon came around, Bernoulli may have been looking to find the value of  $\lim_{n\to \infty}(1+1/n)^n$, but I don't see any motivation to consider $$\frac{d\left(\lim_{n\to \infty}(1+1/n)^n\right)^x}{dx}$$ before the general case $\frac{db^x}{dx}$. I'm sure along the way of finding the derivative of $b^x$, a  clear motivation for defining $e$ will pop-up .. but I'd like to find a proof that starts off assuming no prior knowledge of $e$. If you start from definition, you very quickly arrive at $$\frac{db^x}{dx} = b^x \lim_{h\to 0}{\frac{b^h-1}{h}}$$ but from here I'm stuck. How to show $\exists c \in \Re$ such that $$\lim_{x \to 0}\frac{c^x-1}{x}=1$$ to proceed?",,"['calculus', 'real-analysis', 'exponential-function']"
44,Rapidly Decreasing Functions,Rapidly Decreasing Functions,,"Can someone explain the notion of a rapidly decreasing function? Namely, a function in the Schwartz space: $$\mathscr{S}(\mathbb{R}^n):= \{ f \in C^{\infty} (\mathbb{R}^n) : ||f||_{\alpha, \beta} < \infty \, \, \forall \alpha, \beta\}$$ with  $$ ||f||_{\alpha,\beta}:= \sup_{x \in \mathbb{R}^n} |x^{\alpha} D^{\beta} f(x)|$$ What exactly is this trying to say? I'm not quite familiar with the notation $x^\alpha$. Is this the same $x \in \mathbb{R}^n$ that we take the sup over? I understand that $\alpha, \beta$ are multi-indices, and I understand the notation $$D^\beta : = \frac{\partial^{ |\beta|}}{\partial x_1^{\beta_1} \cdots \partial x_n^{\beta_n}}$$, but I just don't quite understand why these functions are ""rapidly decreasing"". So, an explanation of the definition and notation would be appreciated.","Can someone explain the notion of a rapidly decreasing function? Namely, a function in the Schwartz space: $$\mathscr{S}(\mathbb{R}^n):= \{ f \in C^{\infty} (\mathbb{R}^n) : ||f||_{\alpha, \beta} < \infty \, \, \forall \alpha, \beta\}$$ with  $$ ||f||_{\alpha,\beta}:= \sup_{x \in \mathbb{R}^n} |x^{\alpha} D^{\beta} f(x)|$$ What exactly is this trying to say? I'm not quite familiar with the notation $x^\alpha$. Is this the same $x \in \mathbb{R}^n$ that we take the sup over? I understand that $\alpha, \beta$ are multi-indices, and I understand the notation $$D^\beta : = \frac{\partial^{ |\beta|}}{\partial x_1^{\beta_1} \cdots \partial x_n^{\beta_n}}$$, but I just don't quite understand why these functions are ""rapidly decreasing"". So, an explanation of the definition and notation would be appreciated.",,"['real-analysis', 'analysis', 'definition', 'schwartz-space']"
45,$f:\mathbb R \to \mathbb R$ is continuous and lim$_{n\to \infty} f(nx)=0$ for all real $x$ $\implies $ lim$_{x \to \infty}f(x)=0$,is continuous and lim for all real   lim,f:\mathbb R \to \mathbb R _{n\to \infty} f(nx)=0 x \implies  _{x \to \infty}f(x)=0,"Let $f:\mathbb R \to \mathbb R$ be a continuous function such that for all real $x$ , lim$_{n\to \infty} f(nx)=0$ , then how to prove that lim$_{x \to \infty}f(x)=0$ ? Please help and please don't use Baire Category theorem , I don't know it","Let $f:\mathbb R \to \mathbb R$ be a continuous function such that for all real $x$ , lim$_{n\to \infty} f(nx)=0$ , then how to prove that lim$_{x \to \infty}f(x)=0$ ? Please help and please don't use Baire Category theorem , I don't know it",,['real-analysis']
46,A Riemann integrable function must have infinitely many points of continuity,A Riemann integrable function must have infinitely many points of continuity,,"I was wondering whether anyone would be so kind as to briefly check my proof? I am supposed to prove the statement without using any theorems which would render the proof trivial. If $\displaystyle\int_a^b f$ exists then $f$ has infinitely many points of continuity in $[a,b]$ For the sake of contradiction suppose $f$ has finitely many. It suffices to show that there is an interval $[u,v]\subset [a,b]$ over which $f$ is not Riemann integrable. Pick any $[u,b]\subset [a,b]$ such that $f$ is discontinuous everywhere in $[u,v]$. For any finite partition $D=\{x_1\cdots x_n\}$ of $[u,v]$ let: $$s(f,D)=\sum_i (x_{i+1}-x_i)\inf_{x\in[x_{i},x_{i+1}]}f\quad\text{and}\quad S(f,D)=\sum_i (x_{i+1}-x_i)\sup_{x\in[x_{i},x_{i+1}]}f$$ We prove $\sup_D s(f,D) <\inf_D S(f,D)\;(1)$. To achieve this consider an arbitrary chain: $$D_0\subset D_1\subset \cdots$$ Pick any $I_0\subset I_1\subset \cdots$ where $I_k=[u_k,v_k]$ and $I_k$ is an interval of the partition $D_k$. Without loss of generality $\bigcap_i I_i=z$. Given that $f$ is discontinuous everywhere, there exists $\epsilon>0$ such that for any $\delta>0$ there exists $y$ such that $|z-y|<\delta$ yet $|f(z)-f(y)|\geq \epsilon$. This immediately implies $\displaystyle\sup_{I_n} f>\inf_{I_n} f$ and hence $\displaystyle\sup_{D_n} s(f,D)<\inf_{D_n} S(f,D)$. This is valid for any sequence $D_n$ so claim $(1)$ holds, contradicting the integrability of $f$.","I was wondering whether anyone would be so kind as to briefly check my proof? I am supposed to prove the statement without using any theorems which would render the proof trivial. If $\displaystyle\int_a^b f$ exists then $f$ has infinitely many points of continuity in $[a,b]$ For the sake of contradiction suppose $f$ has finitely many. It suffices to show that there is an interval $[u,v]\subset [a,b]$ over which $f$ is not Riemann integrable. Pick any $[u,b]\subset [a,b]$ such that $f$ is discontinuous everywhere in $[u,v]$. For any finite partition $D=\{x_1\cdots x_n\}$ of $[u,v]$ let: $$s(f,D)=\sum_i (x_{i+1}-x_i)\inf_{x\in[x_{i},x_{i+1}]}f\quad\text{and}\quad S(f,D)=\sum_i (x_{i+1}-x_i)\sup_{x\in[x_{i},x_{i+1}]}f$$ We prove $\sup_D s(f,D) <\inf_D S(f,D)\;(1)$. To achieve this consider an arbitrary chain: $$D_0\subset D_1\subset \cdots$$ Pick any $I_0\subset I_1\subset \cdots$ where $I_k=[u_k,v_k]$ and $I_k$ is an interval of the partition $D_k$. Without loss of generality $\bigcap_i I_i=z$. Given that $f$ is discontinuous everywhere, there exists $\epsilon>0$ such that for any $\delta>0$ there exists $y$ such that $|z-y|<\delta$ yet $|f(z)-f(y)|\geq \epsilon$. This immediately implies $\displaystyle\sup_{I_n} f>\inf_{I_n} f$ and hence $\displaystyle\sup_{D_n} s(f,D)<\inf_{D_n} S(f,D)$. This is valid for any sequence $D_n$ so claim $(1)$ holds, contradicting the integrability of $f$.",,"['real-analysis', 'integration', 'analysis', 'riemann-sum']"
47,Integral $\int_0^\infty \frac{x^n}{(x^2+\alpha^2)^2(e^x-1)^2}dx$,Integral,\int_0^\infty \frac{x^n}{(x^2+\alpha^2)^2(e^x-1)^2}dx,"Hey I am trying to integrate $$ I_n:=\int_0^\infty \frac{x^n}{(x^2+\alpha^2)^2(e^x-1)^2}dx,\quad \alpha,n \geq 1. $$ Thanks. This integral is old. I am also looking for literature on these integrals as I have seen many for small values of n, and variations of this. Thanks. Maybe we can use residues. How can I make a contour with the $x^n$ piece involved with the possible quadrupole pole? Thanks","Hey I am trying to integrate $$ I_n:=\int_0^\infty \frac{x^n}{(x^2+\alpha^2)^2(e^x-1)^2}dx,\quad \alpha,n \geq 1. $$ Thanks. This integral is old. I am also looking for literature on these integrals as I have seen many for small values of n, and variations of this. Thanks. Maybe we can use residues. How can I make a contour with the $x^n$ piece involved with the possible quadrupole pole? Thanks",,"['calculus', 'real-analysis', 'integration', 'definite-integrals', 'contour-integration']"
48,Prove that $g$ is equal to a constant from a given integral.,Prove that  is equal to a constant from a given integral.,g,"Suppose $g : [0,1] \rightarrow \mathbb{R}$ is bounded and measurable and  $$ \int_{0}^{1}f(x)g(x)dx = 0 $$ whenever $f$ is continuous and $\int_{0}^{1}f(x)dx = 0$. Prove that $g$ is equal to a constant a.e How do I solve it? I tried integration by parts, but it didn't work...","Suppose $g : [0,1] \rightarrow \mathbb{R}$ is bounded and measurable and  $$ \int_{0}^{1}f(x)g(x)dx = 0 $$ whenever $f$ is continuous and $\int_{0}^{1}f(x)dx = 0$. Prove that $g$ is equal to a constant a.e How do I solve it? I tried integration by parts, but it didn't work...",,"['real-analysis', 'measure-theory', 'lebesgue-integral']"
49,The set of discontinuous points is countable union of closed sets,The set of discontinuous points is countable union of closed sets,,"Im reading chapter9 Category, Real Analysis, Carothers, 1ed, talking about discontinuous functions of metric space. Here is a proof for a theorem that the set of discontinuous points is the countable union of closed sets in R: Definition of D(f) is: Definition of $ω_f(a)$ is: Definition of ω(f; I) is: Actually, I have some questions,: Let $F_n= \{a : ω_f(a) \geq 1/n \}$, then $F_1$ belongs to $F_2$, $F_2$ belongs to $F_3$, $F_3$ belongs to $F_4$ and so forth, is that right? What's the reason for ""why""? I mean if question1 is yes, then union of $F_n$ seems to be no sense cos union of $F_n$ will = $lim F_n$ when $n$ goes to infinity. In $\{a : ω_f(a) \geq \xi \text{ for some } \xi > 0\}$, $\xi$ should as small as possible(basically, $\xi>0$), is that right? Is {a:$ω_f(a) < r$} a subset of or equivalent to ${I: ω(f:I) < r}$ in $\mathbb{R}$?","Im reading chapter9 Category, Real Analysis, Carothers, 1ed, talking about discontinuous functions of metric space. Here is a proof for a theorem that the set of discontinuous points is the countable union of closed sets in R: Definition of D(f) is: Definition of $ω_f(a)$ is: Definition of ω(f; I) is: Actually, I have some questions,: Let $F_n= \{a : ω_f(a) \geq 1/n \}$, then $F_1$ belongs to $F_2$, $F_2$ belongs to $F_3$, $F_3$ belongs to $F_4$ and so forth, is that right? What's the reason for ""why""? I mean if question1 is yes, then union of $F_n$ seems to be no sense cos union of $F_n$ will = $lim F_n$ when $n$ goes to infinity. In $\{a : ω_f(a) \geq \xi \text{ for some } \xi > 0\}$, $\xi$ should as small as possible(basically, $\xi>0$), is that right? Is {a:$ω_f(a) < r$} a subset of or equivalent to ${I: ω(f:I) < r}$ in $\mathbb{R}$?",,"['real-analysis', 'general-topology', 'analysis']"
50,Positivity of the Coulomb energy in 2d,Positivity of the Coulomb energy in 2d,,"Let $$D(f,g):=\int_{\mathbb{R}^3\times\mathbb{R}^3}\frac{1}{|x-y|}\overline{f(x)}g(y)~dxdy$$ with $f,g$ real valued and sufficiently integrable be the usual Coulomb energy. Under the assumption $D(|f|,|f|)<\infty$ it can be seen that $D(f,f)\geq 0$ (see for example Lieb-Loss, Analysis 9.8). The reason for that is basically that the Fourier transform of $\frac{1}{|\cdot|}$ is non-negative. In two dimensions the Newton kernel is given by $-\log|\cdot|$ which does not have a positive Fourier transform anymore. I saw the claim that under further assumptions the positivity of the Coulomb energy however does still hold true. Precisely: If $f\in L^1(\mathbb{R}^2)\cap L^{1+\epsilon}(\mathbb{R}^2)$ for some $\epsilon>0$ such that $$\int_{\mathbb{R}^2}\log(2+|x|)|f(x)|~dx<\infty$$   and $$\int f=0,$$ then $$D(f,f):=-\int_{\mathbb{R}^2\times\mathbb{R}^2}\log{|x-y|}\overline{f(x)}f(y)~dxdy\geq0.$$ The reference given is Carlen, Loss: Competing symmetries, the logarithmic HLS inequalitiy and Onofir's inequality on $S^n$ . I don't see how the claim follows from that paper. Do you have any other reference for the claim above or see a reason why it should be true?","Let $$D(f,g):=\int_{\mathbb{R}^3\times\mathbb{R}^3}\frac{1}{|x-y|}\overline{f(x)}g(y)~dxdy$$ with $f,g$ real valued and sufficiently integrable be the usual Coulomb energy. Under the assumption $D(|f|,|f|)<\infty$ it can be seen that $D(f,f)\geq 0$ (see for example Lieb-Loss, Analysis 9.8). The reason for that is basically that the Fourier transform of $\frac{1}{|\cdot|}$ is non-negative. In two dimensions the Newton kernel is given by $-\log|\cdot|$ which does not have a positive Fourier transform anymore. I saw the claim that under further assumptions the positivity of the Coulomb energy however does still hold true. Precisely: If $f\in L^1(\mathbb{R}^2)\cap L^{1+\epsilon}(\mathbb{R}^2)$ for some $\epsilon>0$ such that $$\int_{\mathbb{R}^2}\log(2+|x|)|f(x)|~dx<\infty$$   and $$\int f=0,$$ then $$D(f,f):=-\int_{\mathbb{R}^2\times\mathbb{R}^2}\log{|x-y|}\overline{f(x)}f(y)~dxdy\geq0.$$ The reference given is Carlen, Loss: Competing symmetries, the logarithmic HLS inequalitiy and Onofir's inequality on $S^n$ . I don't see how the claim follows from that paper. Do you have any other reference for the claim above or see a reason why it should be true?",,"['real-analysis', 'inequality', 'reference-request']"
51,One-parameter group of diffeomorphisms generated by vector field,One-parameter group of diffeomorphisms generated by vector field,,"Let $S^1$ be the unit sphere $x_1^2+x_2^2=1$ in $\mathbb{R}^2$ and let $X=S^1\times S^1\in\mathbb{R}^4$ with defining equations $f_1=x_1^2+x_2^2-1=0, f_2=x_3^2+x_4^2-1=0$. The vector field $$w=x_1\frac\partial{\partial x_2}-x_2\frac\partial{\partial x_1}+\lambda\left(x_4\frac\partial{\partial x_3}-x_3\frac\partial{\partial x_4}\right)$$ ($\lambda\in\mathbb{R}$) is tangent to $X$ and hence defines by restriction a vector field $v$ on $X$. What is the one-parameter group of diffeomorphisms that $v$ generates? The definition of a one-parameter group of diffeomorphisms that I'm using is the following: Let $U$ be an open subset of $\mathbb{R}^n$ and $F : U \times \mathbb{R} \rightarrow U$ a $C^{\infty}$ mapping. The family of mappings $f_t: U \rightarrow U$ , $f_t(x) = F(x, t)$   is said to be a one-parameter group of diffeomorphisms of $U$ if $f_0$ is the identity   map and $f_s \cdot f_t = f_{s+t}$   for all s and t. First of all, I'm confused how this definition can be applied to our situation. The vector field $v$ is not present anywhere in the definition of a one-parameter group of diffeomorphisms. But it has to be relevant somewhere, right?","Let $S^1$ be the unit sphere $x_1^2+x_2^2=1$ in $\mathbb{R}^2$ and let $X=S^1\times S^1\in\mathbb{R}^4$ with defining equations $f_1=x_1^2+x_2^2-1=0, f_2=x_3^2+x_4^2-1=0$. The vector field $$w=x_1\frac\partial{\partial x_2}-x_2\frac\partial{\partial x_1}+\lambda\left(x_4\frac\partial{\partial x_3}-x_3\frac\partial{\partial x_4}\right)$$ ($\lambda\in\mathbb{R}$) is tangent to $X$ and hence defines by restriction a vector field $v$ on $X$. What is the one-parameter group of diffeomorphisms that $v$ generates? The definition of a one-parameter group of diffeomorphisms that I'm using is the following: Let $U$ be an open subset of $\mathbb{R}^n$ and $F : U \times \mathbb{R} \rightarrow U$ a $C^{\infty}$ mapping. The family of mappings $f_t: U \rightarrow U$ , $f_t(x) = F(x, t)$   is said to be a one-parameter group of diffeomorphisms of $U$ if $f_0$ is the identity   map and $f_s \cdot f_t = f_{s+t}$   for all s and t. First of all, I'm confused how this definition can be applied to our situation. The vector field $v$ is not present anywhere in the definition of a one-parameter group of diffeomorphisms. But it has to be relevant somewhere, right?",,"['real-analysis', 'ordinary-differential-equations', 'dynamical-systems']"
52,Classifying non-unique solutions to ODEs,Classifying non-unique solutions to ODEs,,"The canonical example of an ODE with nonunique solutions is $y'=\sqrt{y}$ with $y(0)=0$. The solutions are $y=\frac{1}{4}t^2$ and $y=0$. We also know that just by looking at $\sqrt{y}$, the fact that it is not Lipschitz immediately suggests this ODE may have multiple solutions. I have a few questions regarding this: 1) Let $y'=f(y,t)$ with $y(0)=0$ where $f$ is not Lipchitz in $y$. Is this always enough to guarantee a nonunique solution to $y$? I suppose this is probably equivalent to asking how multiple fixed points arise when a mapping is non-contractive. 2) Supposing we are assured that $y'=f(y,t)$ does not have a unique solution, is there a systematic way of characterizing all nonunique solutions? In particular, most of the examples I've seen such as the $\sqrt{y}$ example have the solution $y=0$ and I would be interested to see drastically different solutions. More to the point, for something like $y'=\sqrt{y}$ is there a way to conclude that $y=\frac{1}{4}(t-a)^2$ and $y=0$ are the only solutions? Does this generalize to harder problems? On the subject of what one might call particularly badly nonunique ODEs I found papers such as this one which show the existence of equations of the form $y'=f(y,t)$ with $y(t_0)=y_0$ and $f(y,t)$ is continuous in both coordinates, that have more than one solution for every $(t_0,y_0)$ on every interval $[t_0,t_0+\epsilon]$ and $[t_0-\epsilon,t_0]$ for small enough $\epsilon$.","The canonical example of an ODE with nonunique solutions is $y'=\sqrt{y}$ with $y(0)=0$. The solutions are $y=\frac{1}{4}t^2$ and $y=0$. We also know that just by looking at $\sqrt{y}$, the fact that it is not Lipschitz immediately suggests this ODE may have multiple solutions. I have a few questions regarding this: 1) Let $y'=f(y,t)$ with $y(0)=0$ where $f$ is not Lipchitz in $y$. Is this always enough to guarantee a nonunique solution to $y$? I suppose this is probably equivalent to asking how multiple fixed points arise when a mapping is non-contractive. 2) Supposing we are assured that $y'=f(y,t)$ does not have a unique solution, is there a systematic way of characterizing all nonunique solutions? In particular, most of the examples I've seen such as the $\sqrt{y}$ example have the solution $y=0$ and I would be interested to see drastically different solutions. More to the point, for something like $y'=\sqrt{y}$ is there a way to conclude that $y=\frac{1}{4}(t-a)^2$ and $y=0$ are the only solutions? Does this generalize to harder problems? On the subject of what one might call particularly badly nonunique ODEs I found papers such as this one which show the existence of equations of the form $y'=f(y,t)$ with $y(t_0)=y_0$ and $f(y,t)$ is continuous in both coordinates, that have more than one solution for every $(t_0,y_0)$ on every interval $[t_0,t_0+\epsilon]$ and $[t_0-\epsilon,t_0]$ for small enough $\epsilon$.",,"['real-analysis', 'analysis', 'ordinary-differential-equations']"
53,"Limit points of $\left\{\frac{1}{n}+\frac{1}{m}:m,n\in{\Bbb N}\right\}$?",Limit points of ?,"\left\{\frac{1}{n}+\frac{1}{m}:m,n\in{\Bbb N}\right\}","Let  $$S=\left\{\frac{1}{n}+\frac{1}{m}:m,n\in{\Bbb N}\right\}$$ and $S'$ be the set of limit points of $S$. All the results I've found on Google or Math.SE only give the following $$ \left\{\frac1n:n\geq 1\right\}\cup\{0\}\subset S'.  $$ Here is my question: Is $$ \left\{\frac1n:n\geq 1\right\}\cup\{0\}\supset S' $$ also true?","Let  $$S=\left\{\frac{1}{n}+\frac{1}{m}:m,n\in{\Bbb N}\right\}$$ and $S'$ be the set of limit points of $S$. All the results I've found on Google or Math.SE only give the following $$ \left\{\frac1n:n\geq 1\right\}\cup\{0\}\subset S'.  $$ Here is my question: Is $$ \left\{\frac1n:n\geq 1\right\}\cup\{0\}\supset S' $$ also true?",,['real-analysis']
54,Why does at least two intervals overlap in an uncountable family of intervals?,Why does at least two intervals overlap in an uncountable family of intervals?,,"Prove that there are uncountably many intervals $(a,b)$ in $\mathbb{R}, a\neq b$. Assume $X$ be an uncountable family of intervals. Show that there exists at least two intervals in this family that overlap. First was not difficult. I used the arguments similar to Cantor's Diagonal Argument (used to show $\mathbb{R}$ is uncountable.) My attempt for 2 : Assume $X$ be an uncountable family of pairwise disjoint intervals, i.e. $(a_i,b_i) \cap (a_j,b_j) = \emptyset, \quad \forall i\neq j\in I$. We know there exists a rational number in each of these intervals. This implies there are uncountably many rational numbers. Contradiction, since $\mathbb{Q}$ is countable. Thus, $X$ must have at least two intervals that overlap. $\blacksquare$ Is there any problem with this reasoning?","Prove that there are uncountably many intervals $(a,b)$ in $\mathbb{R}, a\neq b$. Assume $X$ be an uncountable family of intervals. Show that there exists at least two intervals in this family that overlap. First was not difficult. I used the arguments similar to Cantor's Diagonal Argument (used to show $\mathbb{R}$ is uncountable.) My attempt for 2 : Assume $X$ be an uncountable family of pairwise disjoint intervals, i.e. $(a_i,b_i) \cap (a_j,b_j) = \emptyset, \quad \forall i\neq j\in I$. We know there exists a rational number in each of these intervals. This implies there are uncountably many rational numbers. Contradiction, since $\mathbb{Q}$ is countable. Thus, $X$ must have at least two intervals that overlap. $\blacksquare$ Is there any problem with this reasoning?",,"['real-analysis', 'elementary-set-theory']"
55,Prove that $\sup(A+B) = \sup(A) + \sup(B)$ and why does $\sup(A+B)$ exist?,Prove that  and why does  exist?,\sup(A+B) = \sup(A) + \sup(B) \sup(A+B),"We want to show that $\sup(A)+\sup(B)$ is the least upper bound of the set $A + B$. First, we need to show that $\sup(A) + \sup(B)$ is an upper bound for the set $A + B$. Indeed, if $z\in A + B$, then there exists $a\in A$ and $b\in B$ such that $z = a + b$. But by definition of $\sup(A)$ and $\sup(B)$, $a \leq \sup(A)$ and $b \leq \sup(B)$, so $z = a + b \leq \sup(A) + \sup(B)$. So, $\sup(A) + \sup(B)$ is an upper bound for $A + B$. We now wish to show that $\sup(A) + \sup(B)$ is the least upper bound for the set $A + B$. So, if $u$ is an upper bound for $A + B$, we need to show that $\sup(A)+\sup(B) \leq u$. We will use part (i): that is, we need to show that there exists $\varepsilon > 0$, $\sup(A) + \sup(B) < u + \varepsilon$. To do this, note that since $\sup(A)$ is the least upper bound for $A$, $\sup(A) - \varepsilon/2$ is not an upper bound for A, so there exists an $a\in A$ so that $\sup(A) - \varepsilon/2 < a$. Similarly, there is a $b\in B$ so that $\sup(B) - \varepsilon/2 < b$. Adding these two inequalities gives $$ \sup(A) + \sup(B) - \varepsilon  < a + b; $$ in other words $$ \sup(A) + \sup(B) < a + b + \varepsilon. $$ But $u$ is an upper bound for the set $A + B$, so $a + b \leq u$, and hence we have $$ \sup(A) + \sup(B) < u + \varepsilon. $$ Thus, by part (i), $\sup(A) + \sup(B) \leq u$, so $\sup(A) + \sup(B)$ is the least upper bound for $A + B$, as required. Now how do I show that $\sup(A+B)$ exists?","We want to show that $\sup(A)+\sup(B)$ is the least upper bound of the set $A + B$. First, we need to show that $\sup(A) + \sup(B)$ is an upper bound for the set $A + B$. Indeed, if $z\in A + B$, then there exists $a\in A$ and $b\in B$ such that $z = a + b$. But by definition of $\sup(A)$ and $\sup(B)$, $a \leq \sup(A)$ and $b \leq \sup(B)$, so $z = a + b \leq \sup(A) + \sup(B)$. So, $\sup(A) + \sup(B)$ is an upper bound for $A + B$. We now wish to show that $\sup(A) + \sup(B)$ is the least upper bound for the set $A + B$. So, if $u$ is an upper bound for $A + B$, we need to show that $\sup(A)+\sup(B) \leq u$. We will use part (i): that is, we need to show that there exists $\varepsilon > 0$, $\sup(A) + \sup(B) < u + \varepsilon$. To do this, note that since $\sup(A)$ is the least upper bound for $A$, $\sup(A) - \varepsilon/2$ is not an upper bound for A, so there exists an $a\in A$ so that $\sup(A) - \varepsilon/2 < a$. Similarly, there is a $b\in B$ so that $\sup(B) - \varepsilon/2 < b$. Adding these two inequalities gives $$ \sup(A) + \sup(B) - \varepsilon  < a + b; $$ in other words $$ \sup(A) + \sup(B) < a + b + \varepsilon. $$ But $u$ is an upper bound for the set $A + B$, so $a + b \leq u$, and hence we have $$ \sup(A) + \sup(B) < u + \varepsilon. $$ Thus, by part (i), $\sup(A) + \sup(B) \leq u$, so $\sup(A) + \sup(B)$ is the least upper bound for $A + B$, as required. Now how do I show that $\sup(A+B)$ exists?",,"['real-analysis', 'proof-writing']"
56,The unsolved mathematical light beam problem,The unsolved mathematical light beam problem,,"I have the following problem: Imagine that you have a sphere sitting at the interface of two media(like water and oil). And the position(the heigth) of the interface to the center of the sphere is fixed and known(I drew a picture for two different situations). Now think about this: A beam of light (parallel rays) that enclose an angle alpha to the interface enters (the direction is supposed to be: they first enter the lower medium and then go to the upper one) hits this interface with the sphere. Now the question is: Can we find an analytical expression for the maximum area of the sphere perpendicular to the direction of the rays that is defined by the rays that enter the sphere without going first into the other medium? If this is unclear look at the picture: In situation 1 I drew 2 rays (of the infinitely many that enter at this angle) that fulfill the condition that they first hit the sphere without going in the upper medium. Now especially the left one is crucial since if I had chosen one that was even more slightly shifted to the left side, this one would have been in the upper medium, so no longer a reasonable candidate. The right one is not a restriction. The right picture is poorly drawn, since I wanted to have one, where both rays restrict the accessible area. But I think you have the idea now, but what do I mean by area? I am looking for the biggest area(=PROJECTION OF THE SURFACE AREA of points that fulfill this on a plane)  perpendicular to the direction of the rays inside the sphere, that consists of rays that enter the sphere and fulfill the property above . So in the first picture this would probably the area going through the center and ""somewhat enclosed by the two arrays"" and in the second part, this one should be the ""imaginary interface"" inside the sphere. If you have any questions, please do not hesitate to post them","I have the following problem: Imagine that you have a sphere sitting at the interface of two media(like water and oil). And the position(the heigth) of the interface to the center of the sphere is fixed and known(I drew a picture for two different situations). Now think about this: A beam of light (parallel rays) that enclose an angle alpha to the interface enters (the direction is supposed to be: they first enter the lower medium and then go to the upper one) hits this interface with the sphere. Now the question is: Can we find an analytical expression for the maximum area of the sphere perpendicular to the direction of the rays that is defined by the rays that enter the sphere without going first into the other medium? If this is unclear look at the picture: In situation 1 I drew 2 rays (of the infinitely many that enter at this angle) that fulfill the condition that they first hit the sphere without going in the upper medium. Now especially the left one is crucial since if I had chosen one that was even more slightly shifted to the left side, this one would have been in the upper medium, so no longer a reasonable candidate. The right one is not a restriction. The right picture is poorly drawn, since I wanted to have one, where both rays restrict the accessible area. But I think you have the idea now, but what do I mean by area? I am looking for the biggest area(=PROJECTION OF THE SURFACE AREA of points that fulfill this on a plane)  perpendicular to the direction of the rays inside the sphere, that consists of rays that enter the sphere and fulfill the property above . So in the first picture this would probably the area going through the center and ""somewhat enclosed by the two arrays"" and in the second part, this one should be the ""imaginary interface"" inside the sphere. If you have any questions, please do not hesitate to post them",,"['real-analysis', 'linear-algebra']"
57,To prove the equivalence definition of Riemann integral.,To prove the equivalence definition of Riemann integral.,,"I have some trouble with the Riemann integral, specifically, the definition of it in an article on wikipedia. We say that the Riemann integral of $f$ equals s if the following condition holds: For a given $\varepsilon>0$, there exists $\delta$ such that for any tagged partition $x_0,\cdots,x_n$ and $t_0,\cdots,t_{n-1}$ whose mesh is less than $\delta$, we have $$\left|\sum_{i=0}^{n-1}f(t_i)(x_{i+1}-x_i)-s\right|<\varepsilon.$$ Unfortunately, this definition is very difficult to use. It would help to develop an equivalent definition of the Riemann integral which is easier to work with. We develop this definition now, with a proof of equivalence following .(?) Our new definition says that the Riemann integral of f equals s if the following condition holds: For all $\varepsilon>0$, there exists a tagged partition $x_0,\cdots,x_n$ and $t_0,\cdots,t_{n-1}$ such that for any refinement $y_0,\cdots,y_m$ and $s_0,\cdots,s_{m-1}$ of $x_0,\cdots,x_n$ and $t_0,\cdots,t_{n-1}$, we have $$\left|\sum_{i=0}^{m-1}f(s_i)(y_{i+1}-y_i)-s\right|<\varepsilon.$$ It is easy to show that the first definition implies the second. To show that the second definition implies the first, it is easiest to use the Darboux integral. First one shows that the second definition is equivalent to the definition of the Darboux integral; for this I want you to give me some hints . However, I know that it is also easy to show that a Darboux integrable function satisfies the first definition. The original article says that the second definition is equivalent to the definition of the Darboux integral; for this see the article on Darboux integration . Well, I couldn't find anything useful information.","I have some trouble with the Riemann integral, specifically, the definition of it in an article on wikipedia. We say that the Riemann integral of $f$ equals s if the following condition holds: For a given $\varepsilon>0$, there exists $\delta$ such that for any tagged partition $x_0,\cdots,x_n$ and $t_0,\cdots,t_{n-1}$ whose mesh is less than $\delta$, we have $$\left|\sum_{i=0}^{n-1}f(t_i)(x_{i+1}-x_i)-s\right|<\varepsilon.$$ Unfortunately, this definition is very difficult to use. It would help to develop an equivalent definition of the Riemann integral which is easier to work with. We develop this definition now, with a proof of equivalence following .(?) Our new definition says that the Riemann integral of f equals s if the following condition holds: For all $\varepsilon>0$, there exists a tagged partition $x_0,\cdots,x_n$ and $t_0,\cdots,t_{n-1}$ such that for any refinement $y_0,\cdots,y_m$ and $s_0,\cdots,s_{m-1}$ of $x_0,\cdots,x_n$ and $t_0,\cdots,t_{n-1}$, we have $$\left|\sum_{i=0}^{m-1}f(s_i)(y_{i+1}-y_i)-s\right|<\varepsilon.$$ It is easy to show that the first definition implies the second. To show that the second definition implies the first, it is easiest to use the Darboux integral. First one shows that the second definition is equivalent to the definition of the Darboux integral; for this I want you to give me some hints . However, I know that it is also easy to show that a Darboux integrable function satisfies the first definition. The original article says that the second definition is equivalent to the definition of the Darboux integral; for this see the article on Darboux integration . Well, I couldn't find anything useful information.",,"['real-analysis', 'riemann-sum']"
58,Proof that every cauchy sequence converges in $\mathbb R^k$,Proof that every cauchy sequence converges in,\mathbb R^k,"I'm having a hard time understanding this proof (the portion in bold). I know $E_N$ is bounded but how is the finite set $\{x_1, \ldots, x_{n-1}\}\,$ bounded? (Is it because every finite set in $\mathbb R^k$ is bounded?) I didn't get the last sentence at all (""Since every bounded …, ( c ) follows from ( b )). Can you please explain this? Theorem ( a ) In any metric space $X$, every convergent sequence is a Cauchy sequence. ( b ) If $X$ is a compact metric space and if $\{p_n\}$ is a Cauchy sequence in $X$, then $\{p_n\}$ converges to some point of $X$. ( c ) In $\Bbb R^k$, every Cauchy sequence converges. Proof Let $\{\mathbf x_k\}$ be a Cauchy sequence in $\Bbb R^k$. Define $E_n$ as in $(b)$, with $\Bbb x_i$ in place of $p_i$. For some $N$, $\operatorname{diam}E_n<1$. The range of $\{\mathbf x_k\}$ is the union of $E_n$ and the finite set $\{\mathbf x_1, \dots, \mathbf x_{N-1}\}$ . Hence $\{\mathbf x_k\}$ is bounded. Since every bounded subset of $\Bbb R^k$ has compact closure in $\Bbb R^k$ (Theorem 2.41), ( c ) follows from ( b ) . Theorem 2.41 If a set $E$ in $\Bbb R^k$ has one of the following three properties, then it has the other two: ( a ) $E$ is closed and bounded. ( b ) $E$ is compact. ( c ) Every infinite subset of $E$ has a limit point in $E$.","I'm having a hard time understanding this proof (the portion in bold). I know $E_N$ is bounded but how is the finite set $\{x_1, \ldots, x_{n-1}\}\,$ bounded? (Is it because every finite set in $\mathbb R^k$ is bounded?) I didn't get the last sentence at all (""Since every bounded …, ( c ) follows from ( b )). Can you please explain this? Theorem ( a ) In any metric space $X$, every convergent sequence is a Cauchy sequence. ( b ) If $X$ is a compact metric space and if $\{p_n\}$ is a Cauchy sequence in $X$, then $\{p_n\}$ converges to some point of $X$. ( c ) In $\Bbb R^k$, every Cauchy sequence converges. Proof Let $\{\mathbf x_k\}$ be a Cauchy sequence in $\Bbb R^k$. Define $E_n$ as in $(b)$, with $\Bbb x_i$ in place of $p_i$. For some $N$, $\operatorname{diam}E_n<1$. The range of $\{\mathbf x_k\}$ is the union of $E_n$ and the finite set $\{\mathbf x_1, \dots, \mathbf x_{N-1}\}$ . Hence $\{\mathbf x_k\}$ is bounded. Since every bounded subset of $\Bbb R^k$ has compact closure in $\Bbb R^k$ (Theorem 2.41), ( c ) follows from ( b ) . Theorem 2.41 If a set $E$ in $\Bbb R^k$ has one of the following three properties, then it has the other two: ( a ) $E$ is closed and bounded. ( b ) $E$ is compact. ( c ) Every infinite subset of $E$ has a limit point in $E$.",,['real-analysis']
59,"$\sum_{n\ge1}\frac{a_n}{(s_n)^\alpha} \text{converges} \iff \alpha > 1$ , where $\sum_{n\ge1} a_n$ is divergent",", where  is divergent",\sum_{n\ge1}\frac{a_n}{(s_n)^\alpha} \text{converges} \iff \alpha > 1 \sum_{n\ge1} a_n,"Let $\forall n \quad a_n >0, \quad s_n=\sum_{k=1}^n a_k$ If $\sum_{n\ge1} a_n$ diverges, than show that $$\sum_{n\ge1}\frac{a_n}{(s_n)^\alpha} \text{converges} \iff \alpha > 1$$","Let $\forall n \quad a_n >0, \quad s_n=\sum_{k=1}^n a_k$ If $\sum_{n\ge1} a_n$ diverges, than show that $$\sum_{n\ge1}\frac{a_n}{(s_n)^\alpha} \text{converges} \iff \alpha > 1$$",,['real-analysis']
60,A (not necessarily continuous) function on a compact metric space attaining its maximum.,A (not necessarily continuous) function on a compact metric space attaining its maximum.,,"I am studying for an exam and my study partners and I are having a dispute about my reasoning for $f$ being continuous by way of open and closed pullbacks (see below).  Please help me correct my thinking.  Here is the problem and my proposed solution: Let $(K, d)$ be a compact metric space, and let $f: K \rightarrow \mathbb{R}$ be a function satisfying that for each $\alpha \in \mathbb{R}$ the set {$x \in K: f(x) \ge \alpha$} is a closed subset of $K$.  Show that $f$ attains a maximum value on $K$. Proof: Notice that $A :=$ {$x \in K: f(x) \ge \alpha$} is precisely $f^{-1}[\alpha, \infty)$.  Since $[\alpha, \infty)$ is closed in $\mathbb{R}$ and $A$ is assumed to be closed in $K$, then it follows that $f$ is continuous on $A$.  On the other hand, $K-A = f^{-1}(-\infty, \alpha)$ is open in $K$ since $A$ is closed in $K$.  And since $(\alpha, \infty)$ is open in $\mathbb{R}$ and $K - A$ is open in $K$, then if follow that $f$ is continuous on $K - A$, hence $f$ is continuous on $K$.  Since $K$ is compact and $f$ is continuous, then $f(K)$ is compact in $\mathbb{R}$.  Compact sets in $\mathbb{R}$ are closed and bounded intervals.  Thus $\sup{f(K)} = \max{f(K)} = f(x_0)$ for some $x_0 \in K$.  Thus $f$ indeed attains its maximum value on $K$.  $\blacksquare$","I am studying for an exam and my study partners and I are having a dispute about my reasoning for $f$ being continuous by way of open and closed pullbacks (see below).  Please help me correct my thinking.  Here is the problem and my proposed solution: Let $(K, d)$ be a compact metric space, and let $f: K \rightarrow \mathbb{R}$ be a function satisfying that for each $\alpha \in \mathbb{R}$ the set {$x \in K: f(x) \ge \alpha$} is a closed subset of $K$.  Show that $f$ attains a maximum value on $K$. Proof: Notice that $A :=$ {$x \in K: f(x) \ge \alpha$} is precisely $f^{-1}[\alpha, \infty)$.  Since $[\alpha, \infty)$ is closed in $\mathbb{R}$ and $A$ is assumed to be closed in $K$, then it follows that $f$ is continuous on $A$.  On the other hand, $K-A = f^{-1}(-\infty, \alpha)$ is open in $K$ since $A$ is closed in $K$.  And since $(\alpha, \infty)$ is open in $\mathbb{R}$ and $K - A$ is open in $K$, then if follow that $f$ is continuous on $K - A$, hence $f$ is continuous on $K$.  Since $K$ is compact and $f$ is continuous, then $f(K)$ is compact in $\mathbb{R}$.  Compact sets in $\mathbb{R}$ are closed and bounded intervals.  Thus $\sup{f(K)} = \max{f(K)} = f(x_0)$ for some $x_0 \in K$.  Thus $f$ indeed attains its maximum value on $K$.  $\blacksquare$",,"['real-analysis', 'metric-spaces', 'compactness']"
61,prove 3rd derivative of a function in a point,prove 3rd derivative of a function in a point,,Let $f$ be twice differentiable and $f'''$ exists in one point $x\in D$. I want to show for this one point $$f'''(x)=\lim_{h\rightarrow0}\frac{f(x+3h)-3f(x+2h)+3f(x+h)-f(x)}{h^3}$$ I did the following: I know $\lim_{h_1\rightarrow0}\frac{f(x+h_1)-f(x)}{h_1}=f'(x)$ and so follows by pluging in $$f''(x)=\lim_{h_2\rightarrow0}\frac{f'(x+h_2)-f'(x)}{h_2}=...=\lim_{h_1\rightarrow0}\frac{f(x+h_1)+f(x-h_1)-2f(x)}{h_1^2}$$ Doing this one more time I'll get ($f'''$ exists in $x$) $$f'''(x)=\lim_{h_3\rightarrow0}\frac{f''(x+h_3)-f''(x)}{h_3}=...=\lim_{h_1\rightarrow0}\frac{f(x+3h_1)-3f(x+2h_1)+3f(x+h_1)-f(x)}{h_1^3}$$ First question: Can you do it like that? And my second question: Why can we always choose 'the same $h$' for the limits? Formally for e.g. $f''$ it's $$\lim_{h_2\rightarrow0}\frac{\lim_{h_1\rightarrow0}\frac{f(x+h_1+h_2)-f(x+h_2)}{h_1}-\lim_{h_1\rightarrow0}\frac{f(x+h_1)-f(x)}{h_1}}{h_2}$$ and I have $h_1$ and $h_2$ in it. Edit Using L'Hospital twice on the right side I get $$\dots=\lim_{h\rightarrow0}\frac{3f''(x+3h)-4f''(x+2h)+f''(x+h)}{2h}$$ And so we get $\begin{align}&\dots\\&=\lim_{h\rightarrow0}\left\{\frac16\frac{3f''(x+h)-3f''(x)}{h}-\frac13\frac{12f''(x+2h)-12f''(x)}{2h}+\frac12\frac{9f''(x+3h)-9f''(x)}{3h}\right\}\\&=\frac36\lim_{h\rightarrow0}\frac{f''(x+h)-f''(x)}{h}-\frac{12}3\lim_{h\rightarrow0}\frac{f''(x+2h)-f''(x)}{2h}+\frac92\lim_{h\rightarrow0}\frac{f''(x+3h)-f''(x)}{3h}\\&=f'''(x)\end{align}$,Let $f$ be twice differentiable and $f'''$ exists in one point $x\in D$. I want to show for this one point $$f'''(x)=\lim_{h\rightarrow0}\frac{f(x+3h)-3f(x+2h)+3f(x+h)-f(x)}{h^3}$$ I did the following: I know $\lim_{h_1\rightarrow0}\frac{f(x+h_1)-f(x)}{h_1}=f'(x)$ and so follows by pluging in $$f''(x)=\lim_{h_2\rightarrow0}\frac{f'(x+h_2)-f'(x)}{h_2}=...=\lim_{h_1\rightarrow0}\frac{f(x+h_1)+f(x-h_1)-2f(x)}{h_1^2}$$ Doing this one more time I'll get ($f'''$ exists in $x$) $$f'''(x)=\lim_{h_3\rightarrow0}\frac{f''(x+h_3)-f''(x)}{h_3}=...=\lim_{h_1\rightarrow0}\frac{f(x+3h_1)-3f(x+2h_1)+3f(x+h_1)-f(x)}{h_1^3}$$ First question: Can you do it like that? And my second question: Why can we always choose 'the same $h$' for the limits? Formally for e.g. $f''$ it's $$\lim_{h_2\rightarrow0}\frac{\lim_{h_1\rightarrow0}\frac{f(x+h_1+h_2)-f(x+h_2)}{h_1}-\lim_{h_1\rightarrow0}\frac{f(x+h_1)-f(x)}{h_1}}{h_2}$$ and I have $h_1$ and $h_2$ in it. Edit Using L'Hospital twice on the right side I get $$\dots=\lim_{h\rightarrow0}\frac{3f''(x+3h)-4f''(x+2h)+f''(x+h)}{2h}$$ And so we get $\begin{align}&\dots\\&=\lim_{h\rightarrow0}\left\{\frac16\frac{3f''(x+h)-3f''(x)}{h}-\frac13\frac{12f''(x+2h)-12f''(x)}{2h}+\frac12\frac{9f''(x+3h)-9f''(x)}{3h}\right\}\\&=\frac36\lim_{h\rightarrow0}\frac{f''(x+h)-f''(x)}{h}-\frac{12}3\lim_{h\rightarrow0}\frac{f''(x+2h)-f''(x)}{2h}+\frac92\lim_{h\rightarrow0}\frac{f''(x+3h)-f''(x)}{3h}\\&=f'''(x)\end{align}$,,['calculus']
62,the integral of $\frac{\sin x}{x}$,the integral of,\frac{\sin x}{x},"in the evaluation of $\int_{0}^{+\infty}\frac{\sin x}{x}$, there was such a strategy as to evaluate it as $\lim_{s \to \infty}\int_{0}^{s}\frac{\sin x}{x}= \lim_{s \to \infty}\int_{0}^{s}\sin x \int_{0}^{+\infty}e^{-xt}dt $, and then we change the order of the integration by Fubini's theorem. but to use Fubini's theorem we have to know that the function is absolutely integrable in advance. How do we know that $\sin x e^{-xt}$ is absolutely integral in $(0,s)\times(0,+\infty)$?","in the evaluation of $\int_{0}^{+\infty}\frac{\sin x}{x}$, there was such a strategy as to evaluate it as $\lim_{s \to \infty}\int_{0}^{s}\frac{\sin x}{x}= \lim_{s \to \infty}\int_{0}^{s}\sin x \int_{0}^{+\infty}e^{-xt}dt $, and then we change the order of the integration by Fubini's theorem. but to use Fubini's theorem we have to know that the function is absolutely integrable in advance. How do we know that $\sin x e^{-xt}$ is absolutely integral in $(0,s)\times(0,+\infty)$?",,['real-analysis']
63,Inequality concerning limsup and liminf of Cesaro mean of a sequence [duplicate],Inequality concerning limsup and liminf of Cesaro mean of a sequence [duplicate],,This question already has answers here : If $\sigma_n=\frac{s_1+s_2+\cdots+s_n}{n}$ then $\operatorname{{lim sup}}\sigma_n \leq \operatorname{lim sup} s_n$ (3 answers) Closed 9 years ago . Let $\{x_n \}$ be a sequence of real numbers and let $y_n = \frac{(x_1 + x_2 + ... + x_n)}{n}$. (a) Prove that $\liminf x_n \le \liminf y_n \le \limsup y_n \le \limsup x_n$ (b) Give an example of a sequence $\{x_n\}$ for which all inequalities of part (a) are strict. I honestly have no idea where to start on this. I can observe some of the easier things such as $\lim \inf x_n  \le \lim \sup x_n$ Any hints would be appreciated.,This question already has answers here : If $\sigma_n=\frac{s_1+s_2+\cdots+s_n}{n}$ then $\operatorname{{lim sup}}\sigma_n \leq \operatorname{lim sup} s_n$ (3 answers) Closed 9 years ago . Let $\{x_n \}$ be a sequence of real numbers and let $y_n = \frac{(x_1 + x_2 + ... + x_n)}{n}$. (a) Prove that $\liminf x_n \le \liminf y_n \le \limsup y_n \le \limsup x_n$ (b) Give an example of a sequence $\{x_n\}$ for which all inequalities of part (a) are strict. I honestly have no idea where to start on this. I can observe some of the easier things such as $\lim \inf x_n  \le \lim \sup x_n$ Any hints would be appreciated.,,['real-analysis']
64,The proof of Raabe's Test for absolute convergence,The proof of Raabe's Test for absolute convergence,,"In Introduction to Real Analysis second edition by Bartle & Sherbert's, there is a proof of Raabe's Test for absolute convergence. The problem is that I don't understand why some part of the proof is necessary. I will show you first the proof as it is in the book, and then explain what I don't understand. Part (a) of the test is as follows: Raabe's Test: Let $X:=(x_n)$ be a sequence of nonzero real numbers. If there exists numbers $a>1$ and $K\in\mathbb{N}$ such that $$\left|\frac{x_{n+1}}{x_n}\right|\leq 1-\frac{a}{n}\quad\text{for}\quad n\geq K,$$ then $\sum x_n$ is absolutely convergent. Proof: If the inequality holds, then we have $$k|x_{k+1}|\leq(k-1)|x_k|-(a-1)|x_k|\quad\text{for}\quad k\geq K$$ On reorganizing the inequality, we have $$(k-1)|x_k|-k|x_{k+1}|\geq(a-1)|x_k|>0\quad\text{for}\quad k\geq K$$ from which we deduce that the sequence $(k|x_{k+1}|)$ is decreasing for $k\geq K$. If we add the last inequality for $k=K,\ldots,n$ and note that the left side telescopes, we get $$(K-1)|x_K|-n|x_{n+1}|\geq(a-1)(|x_K|+\cdots+|x_n|).$$ This shows (why?) that the partial sums of $\sum|x_n|$ are bounded and establishes the absolute convergence of the series. Q.E.D. Now, I don't see why it is important to show that the sequence $(k|x_{k+1}|)$ is decreasing. From the inequality $$(K-1)|x_K|-n|x_{n+1}|\geq(a-1)(|x_K|+\cdots+|x_n|).$$ we have $$(a-1)(|x_K|+\cdots+|x_n|)\leq (K-1)|x_K|\quad\text{for}\quad k\geq K$$ independently if $(k|x_{k+1}|)$ is decreasing or not since $n|x_{n+1}|>0$. So the partial sums of $\sum|x_n|$ are bounded anyway. Can someone explain me what I am missing?","In Introduction to Real Analysis second edition by Bartle & Sherbert's, there is a proof of Raabe's Test for absolute convergence. The problem is that I don't understand why some part of the proof is necessary. I will show you first the proof as it is in the book, and then explain what I don't understand. Part (a) of the test is as follows: Raabe's Test: Let $X:=(x_n)$ be a sequence of nonzero real numbers. If there exists numbers $a>1$ and $K\in\mathbb{N}$ such that $$\left|\frac{x_{n+1}}{x_n}\right|\leq 1-\frac{a}{n}\quad\text{for}\quad n\geq K,$$ then $\sum x_n$ is absolutely convergent. Proof: If the inequality holds, then we have $$k|x_{k+1}|\leq(k-1)|x_k|-(a-1)|x_k|\quad\text{for}\quad k\geq K$$ On reorganizing the inequality, we have $$(k-1)|x_k|-k|x_{k+1}|\geq(a-1)|x_k|>0\quad\text{for}\quad k\geq K$$ from which we deduce that the sequence $(k|x_{k+1}|)$ is decreasing for $k\geq K$. If we add the last inequality for $k=K,\ldots,n$ and note that the left side telescopes, we get $$(K-1)|x_K|-n|x_{n+1}|\geq(a-1)(|x_K|+\cdots+|x_n|).$$ This shows (why?) that the partial sums of $\sum|x_n|$ are bounded and establishes the absolute convergence of the series. Q.E.D. Now, I don't see why it is important to show that the sequence $(k|x_{k+1}|)$ is decreasing. From the inequality $$(K-1)|x_K|-n|x_{n+1}|\geq(a-1)(|x_K|+\cdots+|x_n|).$$ we have $$(a-1)(|x_K|+\cdots+|x_n|)\leq (K-1)|x_K|\quad\text{for}\quad k\geq K$$ independently if $(k|x_{k+1}|)$ is decreasing or not since $n|x_{n+1}|>0$. So the partial sums of $\sum|x_n|$ are bounded anyway. Can someone explain me what I am missing?",,['real-analysis']
65,Non-differentiability in $\mathbb R\setminus\mathbb Q$ of the modification of the Thomae's function,Non-differentiability in  of the modification of the Thomae's function,\mathbb R\setminus\mathbb Q,"Here is the problem I'm struggling with: Where is the following function continuous, differentiable, continuously differentiable? $$f(x) = \begin{cases} q^{-2}  & \text{if $x=\frac{p}{q}$ in lowest terms, $q\in\mathbb N$  } \\ 0 & \text{if $x$ is irrational or $x=0$}  \\ \end{cases} $$ As you can see, it's a modification of the Thomae's function : $q^{-2}$ here instead of the original $q^{-1}$. So far, I've proved that this function (unlike the Thomae's function) is differentiable in $x=0$. And I expect it to be non-differentiable in $\mathbb R\setminus\mathbb Q$ (my guess is largely based the Proposition 4.1 , yet the general proof from the paper is too advanced for me). I was very glad to find the proof for the case $q^{-1}$ : it was very beneficial for me to work through it and to try it here, yet it doesn't seem to work in my case. Any help is hugely appreciated.","Here is the problem I'm struggling with: Where is the following function continuous, differentiable, continuously differentiable? $$f(x) = \begin{cases} q^{-2}  & \text{if $x=\frac{p}{q}$ in lowest terms, $q\in\mathbb N$  } \\ 0 & \text{if $x$ is irrational or $x=0$}  \\ \end{cases} $$ As you can see, it's a modification of the Thomae's function : $q^{-2}$ here instead of the original $q^{-1}$. So far, I've proved that this function (unlike the Thomae's function) is differentiable in $x=0$. And I expect it to be non-differentiable in $\mathbb R\setminus\mathbb Q$ (my guess is largely based the Proposition 4.1 , yet the general proof from the paper is too advanced for me). I was very glad to find the proof for the case $q^{-1}$ : it was very beneficial for me to work through it and to try it here, yet it doesn't seem to work in my case. Any help is hugely appreciated.",,"['calculus', 'real-analysis', 'functions', 'derivatives']"
66,"Limit of $a_{k+1}=\dfrac{a_k+b_k}{2}$, $b_{k+1}=\sqrt{a_kb_k}$?","Limit of , ?",a_{k+1}=\dfrac{a_k+b_k}{2} b_{k+1}=\sqrt{a_kb_k},"Let $a,b>0$ and let $a_0=a$, $b_0=b $, $a_{k+1}=\dfrac{a_k+b_k} 2$,$b_{k+1}=\sqrt{a_kb_k}$   $\quad k\geq0$. This converges  to a number between a and b. Also $a_k>b_k$ for $k\geq1$ (AM-GM inequality). Can we find the limit explicitly in terms of $a$ and $b$?","Let $a,b>0$ and let $a_0=a$, $b_0=b $, $a_{k+1}=\dfrac{a_k+b_k} 2$,$b_{k+1}=\sqrt{a_kb_k}$   $\quad k\geq0$. This converges  to a number between a and b. Also $a_k>b_k$ for $k\geq1$ (AM-GM inequality). Can we find the limit explicitly in terms of $a$ and $b$?",,"['real-analysis', 'sequences-and-series', 'limits']"
67,Change of variables formula for Riemann and Lebesgue integration,Change of variables formula for Riemann and Lebesgue integration,,"In the setting of Riemann integration, we have the following change of variables formula: Let $[a,b]$ be a closed interval, and let $\phi:[a,b]\to[\phi(a),\phi(b)]$ be a continuous monotone increasing function. Let $f:[\phi(a),\phi(b)]\to{\Bbb R}$ be a Riemann integrable function on $[\phi(a),\phi(b)]$. The $f\circ\phi:[a,b]\to{\Bbb R}$ is Riemann-Stieltjes integrable with respect to $\phi$ on $[a,b]$ and    $$ \int_{[a,b]}f\circ\phi\ d\phi=\int_{[\phi(a),\phi(b)]}f. \tag{1} $$ In the setting of Lebesgue integration, we have the following: Let $(X,{\mathcal B},\mu)$ be a measure space, and let $\phi:X\to Y$ be a measurable morphism from $(X,{\mathcal B})$ to another measurable space $(Y,{\mathcal C})$. Define the pushforward $\phi_*\mu:{\mathcal C}\to[0,+\infty]$ of $\mu$ by $\phi$ by the formula   $\phi_*\mu(E):=\mu(\phi^{-1}(E))$. If $f:Y\to[0,+\infty]$ is measurable, then   $$ \int_Y f\ d\phi_*\mu=\int_X(f\circ\phi)\ d\mu. \tag{2} $$ My question is: how can I interpret (1) in terms of (2)?","In the setting of Riemann integration, we have the following change of variables formula: Let $[a,b]$ be a closed interval, and let $\phi:[a,b]\to[\phi(a),\phi(b)]$ be a continuous monotone increasing function. Let $f:[\phi(a),\phi(b)]\to{\Bbb R}$ be a Riemann integrable function on $[\phi(a),\phi(b)]$. The $f\circ\phi:[a,b]\to{\Bbb R}$ is Riemann-Stieltjes integrable with respect to $\phi$ on $[a,b]$ and    $$ \int_{[a,b]}f\circ\phi\ d\phi=\int_{[\phi(a),\phi(b)]}f. \tag{1} $$ In the setting of Lebesgue integration, we have the following: Let $(X,{\mathcal B},\mu)$ be a measure space, and let $\phi:X\to Y$ be a measurable morphism from $(X,{\mathcal B})$ to another measurable space $(Y,{\mathcal C})$. Define the pushforward $\phi_*\mu:{\mathcal C}\to[0,+\infty]$ of $\mu$ by $\phi$ by the formula   $\phi_*\mu(E):=\mu(\phi^{-1}(E))$. If $f:Y\to[0,+\infty]$ is measurable, then   $$ \int_Y f\ d\phi_*\mu=\int_X(f\circ\phi)\ d\mu. \tag{2} $$ My question is: how can I interpret (1) in terms of (2)?",,['real-analysis']
68,What is $ \lim_{x \to 0} \log_0(x) $?,What is ?, \lim_{x \to 0} \log_0(x) ,As per the title; what is $ \lim_{x \to 0} \log_0(x) $ ? According to WolframAlpha : $$ \lim_{x \to 0} \log_0(x) = 0 $$ but how is this possible? Surely the limit should be indeterminate since $\log_0(x) = \frac{\log(x)}{\log(0)} $ and $ \log(0) = $ indeterminate?,As per the title; what is $ \lim_{x \to 0} \log_0(x) $ ? According to WolframAlpha : $$ \lim_{x \to 0} \log_0(x) = 0 $$ but how is this possible? Surely the limit should be indeterminate since $\log_0(x) = \frac{\log(x)}{\log(0)} $ and $ \log(0) = $ indeterminate?,,"['real-analysis', 'limits', 'logarithms']"
69,Cauchy sequence property,Cauchy sequence property,,"I do not see how this is even valid. Could someone point this out to me: Assume that $x_n$ is a cauchy sequence of rational numbers satisfying $|x_n| \geq r$ for all $n\in\mathbb{N}$. Show that there is $N\in\mathbb{N}$ s.t. either $x_n > r$ for all $n \geq N$ or $x_n < -r$ for all $n\geq N$. Here is my immediate thought. If $x_n$ is a constant sequence then it satisfies the conditions in the assumption. So, I was thinking perhaps that fact that it is Cauchy must guarantee what I need to prove. However, it must be Cauchy as $x_n-x_m=0$ for all $n\in\mathbb{N}$. So the definition: $\forall_{\epsilon>0} \exists_N \forall_{m,n}\implies |x_n - x_m| < \epsilon$ Holds. This is contrary to what I am supposed to prove since $x_n \not > r$ as it is equal to $r$ for all $n\in\mathbb{N}$ Am I overlooking something? Thanks, all feedback is welcomed and appreciated.","I do not see how this is even valid. Could someone point this out to me: Assume that $x_n$ is a cauchy sequence of rational numbers satisfying $|x_n| \geq r$ for all $n\in\mathbb{N}$. Show that there is $N\in\mathbb{N}$ s.t. either $x_n > r$ for all $n \geq N$ or $x_n < -r$ for all $n\geq N$. Here is my immediate thought. If $x_n$ is a constant sequence then it satisfies the conditions in the assumption. So, I was thinking perhaps that fact that it is Cauchy must guarantee what I need to prove. However, it must be Cauchy as $x_n-x_m=0$ for all $n\in\mathbb{N}$. So the definition: $\forall_{\epsilon>0} \exists_N \forall_{m,n}\implies |x_n - x_m| < \epsilon$ Holds. This is contrary to what I am supposed to prove since $x_n \not > r$ as it is equal to $r$ for all $n\in\mathbb{N}$ Am I overlooking something? Thanks, all feedback is welcomed and appreciated.",,"['real-analysis', 'sequences-and-series', 'analysis']"
70,Another improper integral,Another improper integral,,"Show that : $$\int_0^1\frac{(\sin ^{-1}x)^2}{x}\text{d}x=\frac{\pi ^2\ln 2}{4}-\frac78\zeta(3)$$ This integral is in ""irresistible integrals"" on page 122. I can't prove this one.","Show that : $$\int_0^1\frac{(\sin ^{-1}x)^2}{x}\text{d}x=\frac{\pi ^2\ln 2}{4}-\frac78\zeta(3)$$ This integral is in ""irresistible integrals"" on page 122. I can't prove this one.",,"['calculus', 'real-analysis', 'sequences-and-series', 'integration', 'riemann-zeta']"
71,$f(x)=\displaystyle \sum\limits_{n=0}^{\infty}(-1)^n\frac{x^n}{n!n!}$ decreasing?,decreasing?,f(x)=\displaystyle \sum\limits_{n=0}^{\infty}(-1)^n\frac{x^n}{n!n!},"Define a function $f: \Bbb{R} \to \Bbb{R}$ by  $f(x)=\displaystyle \sum\limits_{n=0}^{\infty}(-1)^n\frac{x^n}{n!n!}$ Show that f is decreasing on on  $x \in [0,2]$ and that there exists a unique $x_0 \in [0,2]$ for which $f(x_0)=0$. I started by calculating $f'$ but I do not know what to do next.","Define a function $f: \Bbb{R} \to \Bbb{R}$ by  $f(x)=\displaystyle \sum\limits_{n=0}^{\infty}(-1)^n\frac{x^n}{n!n!}$ Show that f is decreasing on on  $x \in [0,2]$ and that there exists a unique $x_0 \in [0,2]$ for which $f(x_0)=0$. I started by calculating $f'$ but I do not know what to do next.",,"['real-analysis', 'sequences-and-series']"
72,Continuity criteria for Radon-Nikodym derivative,Continuity criteria for Radon-Nikodym derivative,,"I have been looking for results or theorems which give me regularity conditions of the Radon-Nikodym derivative, but I have not found any :( For instance, we know that if $\nu\ll\mu$ then there exists $f\in L^1$ s.t. $\nu = \int f d\mu$. I wonder if, under extra conditions, we can say more about $f$, like $f\in \mathcal{C}$ or similar. Are there results? Thank you very much for any help!","I have been looking for results or theorems which give me regularity conditions of the Radon-Nikodym derivative, but I have not found any :( For instance, we know that if $\nu\ll\mu$ then there exists $f\in L^1$ s.t. $\nu = \int f d\mu$. I wonder if, under extra conditions, we can say more about $f$, like $f\in \mathcal{C}$ or similar. Are there results? Thank you very much for any help!",,"['calculus', 'real-analysis', 'functional-analysis', 'measure-theory']"
73,Can we give a definition of the cotangent based on a functional equation?,Can we give a definition of the cotangent based on a functional equation?,,I've recently learned that the cotangent satisfies the following functional equation: $$\dfrac1{f(z)}=f(z)-2f(2z)$$ (true for $f(z)\neq 0$). Can we solve this equation for real or complex functions $f?$ Can we give additional conditions such that $\cot$ is the only real or complex function satisfying these conditions and the equation? Or is there perhaps a different functional equation better suited for this purpose? I'm asking this because I know about such a characterization of the real function $\exp$. Please note that I know very little about functional equations. I've only seen two examples dealt with in my courses.,I've recently learned that the cotangent satisfies the following functional equation: $$\dfrac1{f(z)}=f(z)-2f(2z)$$ (true for $f(z)\neq 0$). Can we solve this equation for real or complex functions $f?$ Can we give additional conditions such that $\cot$ is the only real or complex function satisfying these conditions and the equation? Or is there perhaps a different functional equation better suited for this purpose? I'm asking this because I know about such a characterization of the real function $\exp$. Please note that I know very little about functional equations. I've only seen two examples dealt with in my courses.,,"['real-analysis', 'complex-analysis', 'trigonometry', 'definition', 'functional-equations']"
74,Show that $\lim_{n\to\infty}n\int_0^1f(x)g(x^n)dx=f(1)\int_0^1\frac{g(x)}{x}dx$,Show that,\lim_{n\to\infty}n\int_0^1f(x)g(x^n)dx=f(1)\int_0^1\frac{g(x)}{x}dx,"Let $g:[0,1]\mapsto\mathbb{R}$ be a continuous function, and $\lim_{x\to0^+}g(x)/x$ exists and is finite. Prove that $\forall f:[0,1]\mapsto\mathbb{R}$ , $$\lim_{n\to\infty}n\int_0^1f(x)g(x^n)dx=f(1)\int_0^1\frac{g(x)}{x}dx$$","Let be a continuous function, and exists and is finite. Prove that ,","g:[0,1]\mapsto\mathbb{R} \lim_{x\to0^+}g(x)/x \forall f:[0,1]\mapsto\mathbb{R} \lim_{n\to\infty}n\int_0^1f(x)g(x^n)dx=f(1)\int_0^1\frac{g(x)}{x}dx","['calculus', 'real-analysis', 'analysis', 'integration']"
75,Showing the distance between sets is indeed a metric.,Showing the distance between sets is indeed a metric.,,"Let $(X,d)$ be a metric space, and let $A$ be a non-empty subset of $X$. Define the distance between a point and a set by the function $$d(x,A) = \inf_{z \in A}d(x,z).$$ Prove that for all $x,y \in X$, $$d(x,A) \leq d(x,y) + d(y,A).$$ Note that it is rather trivial to show that $d(x,A) \geq 0$ for all $x \in X$ and $A \subseteq X$ (follows directly from non-negativity of a metric); $d(x,A) = 0$ iff $x$ is in the closure of $A$; and $d(x,A) = d(A,x)$ (by abuse of notation). By proving the final property of the triangle inequality, we can conclude that the modified distance $d: X \times 2^X \to R$ and its mirrored version $d: 2^X \times X \to R$ forms a sort of pseudo-metric where $d(x,A) = 0$ does not define an equivalence relation. If we were to modify this function further by first defining $d(x,A)$ to be substituted with $d({x},A)$, we can form a more general pseudo-metric space $(2^X, d:2^X \times 2^X \to R)$ via defining $$d(A,B) = \inf\{d(a,b) : a \in A, b \in B\}.$$ It should be noted that in this particular metric, $A = B$ means that $A$ and $B$ are not separated. That is, if $A'$ denotes the closure of $A$, then $A \subseteq B'$ and $B \subseteq A'$. Then we know that (and can verify 4) $d(A,B) \geq 0$; $d(A,B) > 0$ if and only if $A$ and $B$ are separated (i.e., $A$ is not in the closure of $B$, and $B$ is not in the closure of $A$); $d(A,B) = d(B,A)$; and $d(A,B) \leq d(A,C) + d(C,B)$, for all subsets $C \subseteq X$. Note that since (2) does not define an equivalence relation on $2^X$, this particular function does not define an actual metric space.","Let $(X,d)$ be a metric space, and let $A$ be a non-empty subset of $X$. Define the distance between a point and a set by the function $$d(x,A) = \inf_{z \in A}d(x,z).$$ Prove that for all $x,y \in X$, $$d(x,A) \leq d(x,y) + d(y,A).$$ Note that it is rather trivial to show that $d(x,A) \geq 0$ for all $x \in X$ and $A \subseteq X$ (follows directly from non-negativity of a metric); $d(x,A) = 0$ iff $x$ is in the closure of $A$; and $d(x,A) = d(A,x)$ (by abuse of notation). By proving the final property of the triangle inequality, we can conclude that the modified distance $d: X \times 2^X \to R$ and its mirrored version $d: 2^X \times X \to R$ forms a sort of pseudo-metric where $d(x,A) = 0$ does not define an equivalence relation. If we were to modify this function further by first defining $d(x,A)$ to be substituted with $d({x},A)$, we can form a more general pseudo-metric space $(2^X, d:2^X \times 2^X \to R)$ via defining $$d(A,B) = \inf\{d(a,b) : a \in A, b \in B\}.$$ It should be noted that in this particular metric, $A = B$ means that $A$ and $B$ are not separated. That is, if $A'$ denotes the closure of $A$, then $A \subseteq B'$ and $B \subseteq A'$. Then we know that (and can verify 4) $d(A,B) \geq 0$; $d(A,B) > 0$ if and only if $A$ and $B$ are separated (i.e., $A$ is not in the closure of $B$, and $B$ is not in the closure of $A$); $d(A,B) = d(B,A)$; and $d(A,B) \leq d(A,C) + d(C,B)$, for all subsets $C \subseteq X$. Note that since (2) does not define an equivalence relation on $2^X$, this particular function does not define an actual metric space.",,"['real-analysis', 'metric-spaces']"
76,Application of Stone-Weierstrass Theorem,Application of Stone-Weierstrass Theorem,,"Suppose that $f \colon [0,1] \rightarrow \mathbb{R}$  is a continuous function on $[0,1]$ with $$\int_0^1 f(x)\ dx  = \int_0^1 f(x)(x^n+x^{n+2})\ dx$$ for all $n=0,1,2, \dots$. Show that $f\equiv 0$. Can someone help me with this question? Is this one of the questions where we apply the Stone-Weierstrass Theorem? Thanks","Suppose that $f \colon [0,1] \rightarrow \mathbb{R}$  is a continuous function on $[0,1]$ with $$\int_0^1 f(x)\ dx  = \int_0^1 f(x)(x^n+x^{n+2})\ dx$$ for all $n=0,1,2, \dots$. Show that $f\equiv 0$. Can someone help me with this question? Is this one of the questions where we apply the Stone-Weierstrass Theorem? Thanks",,['real-analysis']
77,"There exists a unique function $u\in C^0[-a,a]$ which satisfies this property",There exists a unique function  which satisfies this property,"u\in C^0[-a,a]","The problem: Let $a>0$ and let $g\in C^0([-a,a])$. Prove that there exists a unique function $u\in C^0([-a,a])$ such that $$u(x)=\frac x2u\left(\frac x2\right)+g(x),$$ for all $x\in[-a,a]$. My attempt At first sight I thought to approach this problem as a fixed point problem from $C^0([-a,a])$ to $C^0([-2a,2a])$, which are both Banach spaces if equipped with the maximum norm. However i needed to define a contraction, because as it stands it is not clear wether my operator $$(Tu)(x)=\frac x2u\left(\frac x2\right)+g(x)$$ is a contraction or not. Therefore I tried to slightly modify the operator and I picked a $c>a>0$ and defined $$T_cu=\frac 1cTu.$$ $T_cu$ is in fact a contraction, hence by the contraction lemma i have for granted the existence and the uniqueness of a function $u_c\in C^0([-a,a])$, which is a fixed point for $T_cu.$ Clearly this is not what I wanted and it seems difficult to me to finish using this approach. Am I right, is all what I have done useless? And if this were the case, how to solve this problem? Thanks in advance.","The problem: Let $a>0$ and let $g\in C^0([-a,a])$. Prove that there exists a unique function $u\in C^0([-a,a])$ such that $$u(x)=\frac x2u\left(\frac x2\right)+g(x),$$ for all $x\in[-a,a]$. My attempt At first sight I thought to approach this problem as a fixed point problem from $C^0([-a,a])$ to $C^0([-2a,2a])$, which are both Banach spaces if equipped with the maximum norm. However i needed to define a contraction, because as it stands it is not clear wether my operator $$(Tu)(x)=\frac x2u\left(\frac x2\right)+g(x)$$ is a contraction or not. Therefore I tried to slightly modify the operator and I picked a $c>a>0$ and defined $$T_cu=\frac 1cTu.$$ $T_cu$ is in fact a contraction, hence by the contraction lemma i have for granted the existence and the uniqueness of a function $u_c\in C^0([-a,a])$, which is a fixed point for $T_cu.$ Clearly this is not what I wanted and it seems difficult to me to finish using this approach. Am I right, is all what I have done useless? And if this were the case, how to solve this problem? Thanks in advance.",,"['real-analysis', 'banach-spaces']"
78,Measure theory convergence question.,Measure theory convergence question.,,"Show $$\lim_{n \to \infty} \int_0^1 \frac{n+n^k x^k}{(1+\sqrt{x})^n}dx=0$$ for $k=1,2,3,...$ It's clear that the functions converge pointwise to $0$ on $(0,1]$ but I can't seem to find an integrable dominating function. Any hints would be much appreciated.","Show $$\lim_{n \to \infty} \int_0^1 \frac{n+n^k x^k}{(1+\sqrt{x})^n}dx=0$$ for $k=1,2,3,...$ It's clear that the functions converge pointwise to $0$ on $(0,1]$ but I can't seem to find an integrable dominating function. Any hints would be much appreciated.",,"['real-analysis', 'measure-theory']"
79,Is there some consensus on the dimensions of a Jacobian matrix and of a gradient?,Is there some consensus on the dimensions of a Jacobian matrix and of a gradient?,,"According to Wikipedia, given a differentiable mapping $F: \mathbb{R}^n \to \mathbb{R}^m$, its Jacobian matrix is a $m \times n$ matrix defined as: $$ J_F=\begin{bmatrix} \dfrac{\partial y_1}{\partial x_1} & \cdots & \dfrac{\partial y_1}{\partial x_n} \\ \vdots & \ddots & \vdots \\ \dfrac{\partial y_m}{\partial x_1} & \cdots & \dfrac{\partial y_m}{\partial x_n} \end{bmatrix}.  $$ Specially when $m=1$, the Jacobian matrix is also called the gradient $\nabla F$. So when trying to compute a differential, it is $J_F \Delta x$ or $\nabla F \Delta x$. In real analysis, optimization, ..., some texts agree with Wikipedia's definitions. However, in some others, a Jacobian matrix or a gradient of a differentiable mapping is defined to be  the transpose of the Wikipedia definitions. Moreover, in baby Rudin, $J_F$ is of $m \times n$ dimension, while when $m=1$, $\nabla F$ is of $n \times 1$ dimension. When it comes to writing my own formulas, I wonder which way is mostly adopted? Thanks and regards!","According to Wikipedia, given a differentiable mapping $F: \mathbb{R}^n \to \mathbb{R}^m$, its Jacobian matrix is a $m \times n$ matrix defined as: $$ J_F=\begin{bmatrix} \dfrac{\partial y_1}{\partial x_1} & \cdots & \dfrac{\partial y_1}{\partial x_n} \\ \vdots & \ddots & \vdots \\ \dfrac{\partial y_m}{\partial x_1} & \cdots & \dfrac{\partial y_m}{\partial x_n} \end{bmatrix}.  $$ Specially when $m=1$, the Jacobian matrix is also called the gradient $\nabla F$. So when trying to compute a differential, it is $J_F \Delta x$ or $\nabla F \Delta x$. In real analysis, optimization, ..., some texts agree with Wikipedia's definitions. However, in some others, a Jacobian matrix or a gradient of a differentiable mapping is defined to be  the transpose of the Wikipedia definitions. Moreover, in baby Rudin, $J_F$ is of $m \times n$ dimension, while when $m=1$, $\nabla F$ is of $n \times 1$ dimension. When it comes to writing my own formulas, I wonder which way is mostly adopted? Thanks and regards!",,['real-analysis']
80,Global energy conservation in 3D Burgers' equation?,Global energy conservation in 3D Burgers' equation?,,"Is the energy $\| u \|^2_{L^2}$ a conserved quantity for the 3D Burgers' equation for smooth solutions that decay rapidly? Finite time singularities can appear, but I am interested in the behavior BEFORE the the blow-ups. The 3D Burgers' equation  $ {\partial v \over \partial t} + (v \cdot \nabla) v =0 $ can be written as $Dv/Dt =0$ where $D/Dt$ is the material derivative. The  energy density $v^2$ is thus advected, i.e. the energy is conserved ""locally"". But is it conserved globally? Specifically, is the norm $\| u \|_{L^2}$ a conserved quantity? In 1D it is easy to show that the equivalent quantity is conserved, but in 3D I am not so sure. Either way, I would like a proof, or reference, etc. so that I can see it for myself. (I am concerned only about smooth and rapidly decaying solutions...)","Is the energy $\| u \|^2_{L^2}$ a conserved quantity for the 3D Burgers' equation for smooth solutions that decay rapidly? Finite time singularities can appear, but I am interested in the behavior BEFORE the the blow-ups. The 3D Burgers' equation  $ {\partial v \over \partial t} + (v \cdot \nabla) v =0 $ can be written as $Dv/Dt =0$ where $D/Dt$ is the material derivative. The  energy density $v^2$ is thus advected, i.e. the energy is conserved ""locally"". But is it conserved globally? Specifically, is the norm $\| u \|_{L^2}$ a conserved quantity? In 1D it is easy to show that the equivalent quantity is conserved, but in 3D I am not so sure. Either way, I would like a proof, or reference, etc. so that I can see it for myself. (I am concerned only about smooth and rapidly decaying solutions...)",,"['real-analysis', 'partial-differential-equations', 'sobolev-spaces']"
81,Countable subadditivity of the Lebesgue measure,Countable subadditivity of the Lebesgue measure,,"Let $\lbrace F_n \rbrace$ be a sequence of sets in a $\sigma$-algebra $\mathcal{A}$. I want to show that $$m\left(\bigcup F_n\right)\leq \sum m\left(F_n\right)$$ where $m$ is a countable additive measure defined for all sets in a $\sigma$ algebra $\mathcal{A}$. I think I have to use the monotonicity property somewhere in the proof, but I don't how to start it. I'd appreciate a little help. Thanks. Added: From Hans' answer I make the following additions.  From the construction given in Hans' answer, it is clear the $\bigcup F_n = \bigcup G_n$ and $G_n \cap G_m = \emptyset$ for all $m\neq n$. So $$m\left(\bigcup F_n\right)=m\left(\bigcup G_n\right) = \sum m\left(G_n\right).$$ Also from the construction, we have $G_n \subset F_n$ for all $n$ and so by monotonicity, we have $m\left(G_n\right) \leq m\left(F_n\right)$. Finally we would have $$\sum m(G_n) \leq  \sum m(F_n).$$ and the result follows.","Let $\lbrace F_n \rbrace$ be a sequence of sets in a $\sigma$-algebra $\mathcal{A}$. I want to show that $$m\left(\bigcup F_n\right)\leq \sum m\left(F_n\right)$$ where $m$ is a countable additive measure defined for all sets in a $\sigma$ algebra $\mathcal{A}$. I think I have to use the monotonicity property somewhere in the proof, but I don't how to start it. I'd appreciate a little help. Thanks. Added: From Hans' answer I make the following additions.  From the construction given in Hans' answer, it is clear the $\bigcup F_n = \bigcup G_n$ and $G_n \cap G_m = \emptyset$ for all $m\neq n$. So $$m\left(\bigcup F_n\right)=m\left(\bigcup G_n\right) = \sum m\left(G_n\right).$$ Also from the construction, we have $G_n \subset F_n$ for all $n$ and so by monotonicity, we have $m\left(G_n\right) \leq m\left(F_n\right)$. Finally we would have $$\sum m(G_n) \leq  \sum m(F_n).$$ and the result follows.",,"['real-analysis', 'measure-theory']"
82,"""On the consequences of an exact de Bruijn Function"", or ""If Ramanujan had more time...""","""On the consequences of an exact de Bruijn Function"", or ""If Ramanujan had more time...""",,"In this question on Math.SE, I asked about Ramanujan's (ridiculously close) approximation for counting the number of 3-smooth integers less than or equal to a given positive integer $N$, namely,  \begin{eqnarray} \frac{\log 2 N \ \log 3 N}{2 \log 2 \ \log 3}. \end{eqnarray} In his posthumously published notebook , Ramanujan generalized this formula to similarly counting numbers of the form $b_1^{r_1} b_{2}^{r_{2}}$, where $b_1, b_2 > 1$ are (not necessarily distinct) natural numbers and $r_1, r_2 \geq 0$: \begin{eqnarray} \frac{\log b_1 N \ \log b_2 N}{2 \log b_1 \ \log b_2}, \end{eqnarray} where a factor of $\frac{1}{2}$ is to be added if $N$ is of the prescribed form. Both of these problems could be understood by counting the number of non-negative integer solutions of the Diophantine inequality: $(\log b_1) x_1 + (\log b_2) x_2 \leq \log N$, which also counts the number of $\mathbb{Z}$-lattice points in the $\log N$ dilate of the $2$-polytope $\mathcal{P} = \textbf{conv}(\mathbf{0}, (\log b_{1})^{-1} \mathbf{e}_1, (\log b_2)^{-1} \mathbf{e}_{2})$. Here, $\{ \mathbf{e}_{i}\}$ is the standard basis of $\mathbb{R}^{n}$. Unfortunately, counting lattice points in real polytopes is hard . My question, now, is how important a contribution to mathematics would it be if one had an exact formula for counting $y$-smooth integers less than or equal to a real $x > 0$, i.e., an exact formula for the de Bruijn function $\Psi(x,y)$? This would, at the same time, translate into an exact formula for counting lattice points of real polytopes of the form $\textbf{conv}(\mathbf{0}, a_{1} \mathbf{e}_{1}, \dots, a_{n} \mathbf{e}_{n})$ with $a_{i} \in \mathbb{R}_{> 0}$. I'm aware of the review articles by Pomerance , Granville , Hildebrand and Tenenbaum , which each deal with various estimates of the De Bruijn function and its useful in the Quadratic Sieve Method of factorization and applications to cryptography, and even Waring's Problem. However, none of these review articles deal with the consequences of having an exact formula.","In this question on Math.SE, I asked about Ramanujan's (ridiculously close) approximation for counting the number of 3-smooth integers less than or equal to a given positive integer $N$, namely,  \begin{eqnarray} \frac{\log 2 N \ \log 3 N}{2 \log 2 \ \log 3}. \end{eqnarray} In his posthumously published notebook , Ramanujan generalized this formula to similarly counting numbers of the form $b_1^{r_1} b_{2}^{r_{2}}$, where $b_1, b_2 > 1$ are (not necessarily distinct) natural numbers and $r_1, r_2 \geq 0$: \begin{eqnarray} \frac{\log b_1 N \ \log b_2 N}{2 \log b_1 \ \log b_2}, \end{eqnarray} where a factor of $\frac{1}{2}$ is to be added if $N$ is of the prescribed form. Both of these problems could be understood by counting the number of non-negative integer solutions of the Diophantine inequality: $(\log b_1) x_1 + (\log b_2) x_2 \leq \log N$, which also counts the number of $\mathbb{Z}$-lattice points in the $\log N$ dilate of the $2$-polytope $\mathcal{P} = \textbf{conv}(\mathbf{0}, (\log b_{1})^{-1} \mathbf{e}_1, (\log b_2)^{-1} \mathbf{e}_{2})$. Here, $\{ \mathbf{e}_{i}\}$ is the standard basis of $\mathbb{R}^{n}$. Unfortunately, counting lattice points in real polytopes is hard . My question, now, is how important a contribution to mathematics would it be if one had an exact formula for counting $y$-smooth integers less than or equal to a real $x > 0$, i.e., an exact formula for the de Bruijn function $\Psi(x,y)$? This would, at the same time, translate into an exact formula for counting lattice points of real polytopes of the form $\textbf{conv}(\mathbf{0}, a_{1} \mathbf{e}_{1}, \dots, a_{n} \mathbf{e}_{n})$ with $a_{i} \in \mathbb{R}_{> 0}$. I'm aware of the review articles by Pomerance , Granville , Hildebrand and Tenenbaum , which each deal with various estimates of the De Bruijn function and its useful in the Quadratic Sieve Method of factorization and applications to cryptography, and even Waring's Problem. However, none of these review articles deal with the consequences of having an exact formula.",,"['number-theory', 'real-analysis', 'combinatorics', 'reference-request']"
83,Bounding higher moments of truncated normal,Bounding higher moments of truncated normal,,"I'm looking for a convenient upper bound on the integral \begin{equation*} \int_y^\infty x^k \exp(-(x-\mu)^2/2) dx  \end{equation*} for (possibly large) positive integer $k.$  This is equivalent to finding higher moments of a truncated normal distribution. A bound that works for non-integer $k$ as well would be even better. Of course ""convenient"" is in the eye of the beholder, but I'd like some sort of fairly simple expression that I can use in further calculations.  For example, an upper bound of the form $f(x) \exp( -g(x))$ where where $f$ and $g$ are low-degree polynomials would be great. I'm more interested in simplicity of form than in obtaining the tightest possible bound.","I'm looking for a convenient upper bound on the integral \begin{equation*} \int_y^\infty x^k \exp(-(x-\mu)^2/2) dx  \end{equation*} for (possibly large) positive integer $k.$  This is equivalent to finding higher moments of a truncated normal distribution. A bound that works for non-integer $k$ as well would be even better. Of course ""convenient"" is in the eye of the beholder, but I'd like some sort of fairly simple expression that I can use in further calculations.  For example, an upper bound of the form $f(x) \exp( -g(x))$ where where $f$ and $g$ are low-degree polynomials would be great. I'm more interested in simplicity of form than in obtaining the tightest possible bound.",,"['real-analysis', 'probability-theory']"
84,Absolute continuity on an open interval of the real line?,Absolute continuity on an open interval of the real line?,,"In classical real analysis I've only seen absolute continuity defined for functions on compact interval $[a,b]$, where the two equivalent definitions are: $f:[a,b]\rightarrow\mathbb{R}$ is AC if (1)  Given $\epsilon > 0$ there is a $\delta > 0$ such that $\sum_{i=1}^n |f(y_i)-f(x_i)|< \epsilon$ for every finite collection of nonoverlapping intervals $( (x_i,y_i) )_{i=1}^n $ each contained in $[a,b]$ with $\sum_{i=1}^n |y_i-x_i|< \delta$. Or, (2) $f'$ exists a.e. on $[a,b]$, $f'$ is integrable on $[a,b]$, and $f(x) = \int_a^x f'(y) dy + f(a)$ for all $x \in [a,b]$. Is there an accepted definition for absolute continuity of a function on an open, possibly unbounded, interval $(a,b)$ where $-\infty \leq a < b \leq \infty$? It seems that definition (1) extends easily to this case if we replace $[a,b]$ by $(a,b)$.  If we call this condition (1'), then it's easy to show (1') is equivalent to (see answer by Jonas below).  A natural extension of (2) to this case would be (2') $f$ is AC on an open set $U$ if, for all compact intervals $[c,d] \subset (a,b)$, $f$ is AC in the sense of (2) on $[c,d]$. Which of these is the best extension of the definition?  I don't know enough about the notion of absolute continuity of measures to know if my extended definitions are consistent with that generalization as well.","In classical real analysis I've only seen absolute continuity defined for functions on compact interval $[a,b]$, where the two equivalent definitions are: $f:[a,b]\rightarrow\mathbb{R}$ is AC if (1)  Given $\epsilon > 0$ there is a $\delta > 0$ such that $\sum_{i=1}^n |f(y_i)-f(x_i)|< \epsilon$ for every finite collection of nonoverlapping intervals $( (x_i,y_i) )_{i=1}^n $ each contained in $[a,b]$ with $\sum_{i=1}^n |y_i-x_i|< \delta$. Or, (2) $f'$ exists a.e. on $[a,b]$, $f'$ is integrable on $[a,b]$, and $f(x) = \int_a^x f'(y) dy + f(a)$ for all $x \in [a,b]$. Is there an accepted definition for absolute continuity of a function on an open, possibly unbounded, interval $(a,b)$ where $-\infty \leq a < b \leq \infty$? It seems that definition (1) extends easily to this case if we replace $[a,b]$ by $(a,b)$.  If we call this condition (1'), then it's easy to show (1') is equivalent to (see answer by Jonas below).  A natural extension of (2) to this case would be (2') $f$ is AC on an open set $U$ if, for all compact intervals $[c,d] \subset (a,b)$, $f$ is AC in the sense of (2) on $[c,d]$. Which of these is the best extension of the definition?  I don't know enough about the notion of absolute continuity of measures to know if my extended definitions are consistent with that generalization as well.",,['real-analysis']
85,Class of functions whose composition with any moment-generating/characteristic function remain MGF/CF,Class of functions whose composition with any moment-generating/characteristic function remain MGF/CF,,"Are there any known results on the class of functions $f$ for which $f(M_X)$ remains an mgf/cf for any MGF/CF $M_X$ . Elementary examples of such functions are $$f_{c}(x)=c,$$ $$f_{n}(x)=x^n,$$ and nice examples are $$f_{\lambda}(x)=x e^{ \lambda \left(x -1\right)}$$ $$f_{\alpha, \lambda}(x)=\alpha e^{-\lambda}+e^{ \lambda \left(x -1\right)}$$ appeared in this answer ( $\alpha, \lambda,  c>0$ , $n \in \mathbb N$ ). Also, inspired from $f_{\lambda}$ , we have the following function: $$f_{Y}(x)=\mathbb E \left (x^Y \right)=M_Y(\log x) $$ for any non-negative integer-valued random variable $Y$ with finite mgf $M_Y$ over $\mathbb R _{\, \ge 0}$ (e.g., $Y$ with geometric distribution cannot be used here); $f_{\lambda}$ is obtained from $Y$ with Poisson distribution. Indeed, the set of such functions is a convex set. Moreover, the set is closed under multiplication . It seems that every member of this set is obtained from multiplication/convex-combination of a number of the above functions (there may be a set of basic functions that generate all the set). 1) Can we determine the whole set, 2) show some interesting properties of the set, or 3) provide some other nice members, basically different from the above instances? Update: An interesting result provided in @Snoop's answer that characterizes the class of all functions whose composition with any (multivariate) CF remain CF. I think the result is correct (while I am not an expert in complex analysis). Using the results given earlier in the OP, we can see that any infinitely differentiable function $f: [1,\infty) \to [1,\infty)$ with the following three sufficient conditions: The  Maclaurin series of $f$ converges for any $x$ The coefficients of the Maclaurin series are non-negative The coefficients of the Maclaurin series sum to 1 is a function whose composition with any MGF remains an MGF. I think using the result developed for CFs and noting $M_X(t)=\varphi_X(-ti)$ , we can prove the above conditions are also necessary .","Are there any known results on the class of functions for which remains an mgf/cf for any MGF/CF . Elementary examples of such functions are and nice examples are appeared in this answer ( , ). Also, inspired from , we have the following function: for any non-negative integer-valued random variable with finite mgf over (e.g., with geometric distribution cannot be used here); is obtained from with Poisson distribution. Indeed, the set of such functions is a convex set. Moreover, the set is closed under multiplication . It seems that every member of this set is obtained from multiplication/convex-combination of a number of the above functions (there may be a set of basic functions that generate all the set). 1) Can we determine the whole set, 2) show some interesting properties of the set, or 3) provide some other nice members, basically different from the above instances? Update: An interesting result provided in @Snoop's answer that characterizes the class of all functions whose composition with any (multivariate) CF remain CF. I think the result is correct (while I am not an expert in complex analysis). Using the results given earlier in the OP, we can see that any infinitely differentiable function with the following three sufficient conditions: The  Maclaurin series of converges for any The coefficients of the Maclaurin series are non-negative The coefficients of the Maclaurin series sum to 1 is a function whose composition with any MGF remains an MGF. I think using the result developed for CFs and noting , we can prove the above conditions are also necessary .","f f(M_X) M_X f_{c}(x)=c, f_{n}(x)=x^n, f_{\lambda}(x)=x e^{ \lambda \left(x -1\right)} f_{\alpha, \lambda}(x)=\alpha e^{-\lambda}+e^{ \lambda \left(x -1\right)} \alpha, \lambda,  c>0 n \in \mathbb N f_{\lambda} f_{Y}(x)=\mathbb E \left (x^Y \right)=M_Y(\log x)  Y M_Y \mathbb R _{\, \ge 0} Y f_{\lambda} Y f: [1,\infty) \to [1,\infty) f x M_X(t)=\varphi_X(-ti)","['real-analysis', 'probability', 'functional-analysis', 'complex-analysis', 'statistics']"
86,"Prove that $0 \leq u(x) \leq 1$ for every $x \in [0, a]$ if $u''(x) + u(x) (1 - u(x)) = 0$ and $u(0) = u(a) = 0$, where $a > 0$","Prove that  for every  if  and , where","0 \leq u(x) \leq 1 x \in [0, a] u''(x) + u(x) (1 - u(x)) = 0 u(0) = u(a) = 0 a > 0","Let $a$ be a positive number. Suppose that $u$ : $[0, a] \to \mathbb{R}$ is a continuous function, that $u(0) = u(a) = 0$ , that the first- and second-order derivatives of $u$ are continuous in $(0, a)$ , and that $u''(x) + u(x) (1 - u(x)) = 0$ for every $x \in (0, a)$ . Prove that $0 \leq u(x) \leq 1$ for every $x \in [0, a]$ . One part is easy. Assume on the contrary that $u(t_0) > 1$ for some $t_0 \in [0, a]$ . Since $u$ is continuous, its maximum exists. There exists some $t_1 \in [0, a]$ such that $u(t_1) \geq u(x)$ for every $x \in [0, a]$ . Then $u(t_1) \geq u(t_0) > 1$ and by calculus, $u''(t_1) \leq 0$ (note that $t_1$ cannot be $0$ or $a$ ). Then $$ 0 = u''(t_1) + u(t_1) (1 - u(t_1)) < 0, $$ a contradiction. The other part does not seem to be easy, however. I tried to do it by mimicking the way in which I solved the part above, but I did not get a contradiction. The reason for which I raised the question: I take a course in partial differential equations. The instructor uses the second edition of An Introduction to Nonlinear Partial Differential Equations by J. David Logan as the textbook. I am actually concerned about the existence of the solution to some boundary value problem. Here is the context. $D$ is an open, bounded, connected subset of $\mathbb{R}^n$ . The operator $L$ is ""uniformly elliptic"" in $D$ , which means that $L$ is of the form $$ Lu := \sum_{1 \leq i,j \leq n} {a_{i,j} (x) \frac{\partial^2 u}{\partial x_i \partial x_j}} + \sum_{1 \leq j \leq n} {b_j (x) \frac{\partial u}{\partial x_j}}, $$ where the coefficients $a_{i,j}$ and $b_j$ are continuously differentiable functions on $D$ that are continuous on $D$ , and that there exists a positive number $\mu$ such that $$ \sum_{1 \leq i,j \leq n} {a_{i,j} (x) \xi_i \xi_j} \geq \mu \sum_{1 \leq j \leq n} {\xi_j^2} $$ for every $x \in D$ and $\xi_j \in \mathbb{R}$ . (Note that there are typos in (7.2.7) and (7.2.8); $(x, y)$ , instead of $x$ , should be used.) The example intends to demonstrate how the existence theorem is used. The author tries to find an upper solution $\overline{u}$ , and a lower solution $\underline{u}$ that is not greater than $\overline{u}$ . I do not understand why the author says ""Then $\overline{u}(x) \geq 0$ on $\partial D$ and $-\Delta \overline{u} = -{\overline{u}}'' = \overline{u} (1 - \overline{u}) \geq 0$ ."" That $\overline{u} \leq 1$ has been justified above, but the other part has not. After reading your replies, I guess that the author makes a mistake here. If so, what should I do in order to amend the demonstration?","Let be a positive number. Suppose that : is a continuous function, that , that the first- and second-order derivatives of are continuous in , and that for every . Prove that for every . One part is easy. Assume on the contrary that for some . Since is continuous, its maximum exists. There exists some such that for every . Then and by calculus, (note that cannot be or ). Then a contradiction. The other part does not seem to be easy, however. I tried to do it by mimicking the way in which I solved the part above, but I did not get a contradiction. The reason for which I raised the question: I take a course in partial differential equations. The instructor uses the second edition of An Introduction to Nonlinear Partial Differential Equations by J. David Logan as the textbook. I am actually concerned about the existence of the solution to some boundary value problem. Here is the context. is an open, bounded, connected subset of . The operator is ""uniformly elliptic"" in , which means that is of the form where the coefficients and are continuously differentiable functions on that are continuous on , and that there exists a positive number such that for every and . (Note that there are typos in (7.2.7) and (7.2.8); , instead of , should be used.) The example intends to demonstrate how the existence theorem is used. The author tries to find an upper solution , and a lower solution that is not greater than . I do not understand why the author says ""Then on and ."" That has been justified above, but the other part has not. After reading your replies, I guess that the author makes a mistake here. If so, what should I do in order to amend the demonstration?","a u [0, a] \to \mathbb{R} u(0) = u(a) = 0 u (0, a) u''(x) + u(x) (1 - u(x)) = 0 x \in (0, a) 0 \leq u(x) \leq 1 x \in [0, a] u(t_0) > 1 t_0 \in [0, a] u t_1 \in [0, a] u(t_1) \geq u(x) x \in [0, a] u(t_1) \geq u(t_0) > 1 u''(t_1) \leq 0 t_1 0 a 
0 = u''(t_1) + u(t_1) (1 - u(t_1)) < 0,
 D \mathbb{R}^n L D L 
Lu := \sum_{1 \leq i,j \leq n} {a_{i,j} (x) \frac{\partial^2 u}{\partial x_i \partial x_j}} + \sum_{1 \leq j \leq n} {b_j (x) \frac{\partial u}{\partial x_j}},
 a_{i,j} b_j D D \mu 
\sum_{1 \leq i,j \leq n} {a_{i,j} (x) \xi_i \xi_j} \geq \mu \sum_{1 \leq j \leq n} {\xi_j^2}
 x \in D \xi_j \in \mathbb{R} (x, y) x \overline{u} \underline{u} \overline{u} \overline{u}(x) \geq 0 \partial D -\Delta \overline{u} = -{\overline{u}}'' = \overline{u} (1 - \overline{u}) \geq 0 \overline{u} \leq 1","['real-analysis', 'calculus', 'ordinary-differential-equations', 'derivatives']"
87,Does such a convergent series exist?,Does such a convergent series exist?,,"This is the question that arose in my mind while contemplating the convergence of this series: $$\sum_{n\geq 1} \frac{\sin(\sqrt{n})}{n}$$ In this problem, if we assume $A_N =  \sum_{n=1}^{N}\sin(\sqrt{n})$ , and consider the method of partial sums: $$\sum_{n=1}^{N} \frac{\sin(\sqrt{n})}{n} = \frac{A_N}{N}+\sum_{n=1}^{N-1}\frac{A_n}{n(n+1)}$$ Then, due to the estimation of $A_N = O(\sqrt{N})$ , we know that the series converges. At this point, I have a question regarding the summation of general form like $\sum_{n\geq 1} \frac{a_n}{n}$ , we can make the same assumption that $A_N = \sum_{n=1}^{N}a_n$ , and apply the method of partial summation $$\sum_{n=1}^{N} \frac{a_n}{n} = \frac{A_N}{N}+\sum_{n=1}^{N-1}\frac{A_n}{n(n+1)}$$ In this case, if $|A_N| \gg N$ , then this method cannot determine the convergence of the series; it can only conclude that the series is not absolutely convergent. For example, let $A_N = (-1)^N N$ , in this case, the series does not converges. However, does these exist $|A_N| \gg N$ such that the series $\sum_{n=1}^{\infty} \frac{a_n}{n}$ conditionally converges?","This is the question that arose in my mind while contemplating the convergence of this series: In this problem, if we assume , and consider the method of partial sums: Then, due to the estimation of , we know that the series converges. At this point, I have a question regarding the summation of general form like , we can make the same assumption that , and apply the method of partial summation In this case, if , then this method cannot determine the convergence of the series; it can only conclude that the series is not absolutely convergent. For example, let , in this case, the series does not converges. However, does these exist such that the series conditionally converges?",\sum_{n\geq 1} \frac{\sin(\sqrt{n})}{n} A_N =  \sum_{n=1}^{N}\sin(\sqrt{n}) \sum_{n=1}^{N} \frac{\sin(\sqrt{n})}{n} = \frac{A_N}{N}+\sum_{n=1}^{N-1}\frac{A_n}{n(n+1)} A_N = O(\sqrt{N}) \sum_{n\geq 1} \frac{a_n}{n} A_N = \sum_{n=1}^{N}a_n \sum_{n=1}^{N} \frac{a_n}{n} = \frac{A_N}{N}+\sum_{n=1}^{N-1}\frac{A_n}{n(n+1)} |A_N| \gg N A_N = (-1)^N N |A_N| \gg N \sum_{n=1}^{\infty} \frac{a_n}{n},"['real-analysis', 'sequences-and-series', 'examples-counterexamples']"
88,proof of almost everywhere differentiability of monotone functions,proof of almost everywhere differentiability of monotone functions,,"I'm reading ""An introduction to measure theory"" from Terry Tao and I'm stuck understanding part of the proof of the following theorem:(whole argument can be found in his weblog Theorem 53.) (Monotone differentiation theorem) Any function ${F: {\bf R} \rightarrow {\bf R}}$ which is monotone (either monotone non-decreasing or monotone non-increasing) is differentiable almost everywhere. The idea to prove this theorem, is considering Dini Derivatives of a function $F$ .namely: $\bullet$ The upper right derivative $${\overline{D^+} F(x) := \limsup_{h \rightarrow 0^+} \frac{F(x+h)-F(x)}{h}}$$ $\bullet$ The lower right derivative $${\underline{D^+} F(x) := \liminf_{h \rightarrow 0^+} \frac{F(x+h)-F(x)}{h}}$$ $\bullet$ The upper left derivative $${\overline{D^-} F(x) := \limsup_{h \rightarrow 0^-} \frac{F(x+h)-F(x)}{h}}$$ $\bullet$ The lower right derivative $${\underline{D^-} F(x) := \liminf_{h \rightarrow 0^-} \frac{F(x+h)-F(x)}{h}}$$ now to show a.e differentiability we should show these 4 quantities agree a.e but it's easy to see that the only thing that is needed to be proved is ${\overline{D^+} F(x)} = {\underline{D^-} F(x)}$ a.e. to prove this the idea is to show that $E_{r,R} := \{x \in R: {\overline{D^+} F(x)} > R > r > {\underline{D^-} F(x)}\}$ is a null set and then by letting $R>r$ both iterate over all rational numbers, conclusion follows. Tao proceeds with showing that $E_{r,R}$ has no Lebesgue point and it would give the desired conclusion. Hence the only thing one need to prove is following lemma: For any interval ${[a,b]}$ and any ${0 < r < R}$ , one has ${m( E_{r,R} \cap [a,b] ) \leq \frac{r}{R} |b-a|}$ . The proof is as follows: We begin by applying the rising sun lemma to the function ${G(x) := r x + F(-x)}$ on ${[-b,-a]}$ ; the large number of negative signs present here is needed in order to properly deal with the lower left Dini derivative ${\underline{D_-} F}$ . This gives an at most countable family of disjoint intervals ${-I_n = (-b_n,-a_n)}$ in ${(-b,-a)}$ , such that ${G(-a_n) \geq G(-b_n)}$ for all ${n}$ , and such that ${G(-x) \leq G(-y)}$ whenever ${-x \leq -y \leq -a}$ and ${-x \in (-b,-a)}$ lies outside of all of the ${-I_n}$ . Observe that if ${x \in (a,b)}$ , and ${G(-x) \leq G(-y)}$ for all ${-x \leq -y \leq -a}$ , then ${\underline{D_-} F(x) \geq r}$ . Thus we see that ${E_{r,R} \cap (a,b)}$ is contained inside the union of the intervals ${I_n = (a_n,b_n)}$ ... (full proof is not given here and can be found in the first link given above) I don't understand how rising sun lemma is applied to $G$ since I think applying rising sun lemma to $G$ gives an at most countable family of disjoint intervals ${-I_n = (-b_n,-a_n)}$ in ${(-b,-a)}$ , such that ${G(-a_n) \geq G(-b_n)}$ for all ${n}$ , and such that ${G(-x) \geq G(-y)}$ whenever ${-x \leq -y \leq -a}$ and ${-x \in (-b,-a)}$ lies outside of all of the ${-I_n}$ .I don' understand why in proof given by tao, it's said that ${G(-x) \leq G(-y)}$ whenever ${-x \leq -y \leq -a}$ and ${-x \in (-b,-a)}$ lies outside of all of the ${-I_n}$ . Thanks.","I'm reading ""An introduction to measure theory"" from Terry Tao and I'm stuck understanding part of the proof of the following theorem:(whole argument can be found in his weblog Theorem 53.) (Monotone differentiation theorem) Any function which is monotone (either monotone non-decreasing or monotone non-increasing) is differentiable almost everywhere. The idea to prove this theorem, is considering Dini Derivatives of a function .namely: The upper right derivative The lower right derivative The upper left derivative The lower right derivative now to show a.e differentiability we should show these 4 quantities agree a.e but it's easy to see that the only thing that is needed to be proved is a.e. to prove this the idea is to show that is a null set and then by letting both iterate over all rational numbers, conclusion follows. Tao proceeds with showing that has no Lebesgue point and it would give the desired conclusion. Hence the only thing one need to prove is following lemma: For any interval and any , one has . The proof is as follows: We begin by applying the rising sun lemma to the function on ; the large number of negative signs present here is needed in order to properly deal with the lower left Dini derivative . This gives an at most countable family of disjoint intervals in , such that for all , and such that whenever and lies outside of all of the . Observe that if , and for all , then . Thus we see that is contained inside the union of the intervals ... (full proof is not given here and can be found in the first link given above) I don't understand how rising sun lemma is applied to since I think applying rising sun lemma to gives an at most countable family of disjoint intervals in , such that for all , and such that whenever and lies outside of all of the .I don' understand why in proof given by tao, it's said that whenever and lies outside of all of the . Thanks.","{F: {\bf R} \rightarrow {\bf R}} F \bullet {\overline{D^+} F(x) := \limsup_{h \rightarrow 0^+} \frac{F(x+h)-F(x)}{h}} \bullet {\underline{D^+} F(x) := \liminf_{h \rightarrow 0^+} \frac{F(x+h)-F(x)}{h}} \bullet {\overline{D^-} F(x) := \limsup_{h \rightarrow 0^-} \frac{F(x+h)-F(x)}{h}} \bullet {\underline{D^-} F(x) := \liminf_{h \rightarrow 0^-} \frac{F(x+h)-F(x)}{h}} {\overline{D^+} F(x)} = {\underline{D^-} F(x)} E_{r,R} := \{x \in R: {\overline{D^+} F(x)} > R > r > {\underline{D^-} F(x)}\} R>r E_{r,R} {[a,b]} {0 < r < R} {m( E_{r,R} \cap [a,b] ) \leq \frac{r}{R} |b-a|} {G(x) := r x + F(-x)} {[-b,-a]} {\underline{D_-} F} {-I_n = (-b_n,-a_n)} {(-b,-a)} {G(-a_n) \geq G(-b_n)} {n} {G(-x) \leq G(-y)} {-x \leq -y \leq -a} {-x \in (-b,-a)} {-I_n} {x \in (a,b)} {G(-x) \leq G(-y)} {-x \leq -y \leq -a} {\underline{D_-} F(x) \geq r} {E_{r,R} \cap (a,b)} {I_n = (a_n,b_n)} G G {-I_n = (-b_n,-a_n)} {(-b,-a)} {G(-a_n) \geq G(-b_n)} {n} {G(-x) \geq G(-y)} {-x \leq -y \leq -a} {-x \in (-b,-a)} {-I_n} {G(-x) \leq G(-y)} {-x \leq -y \leq -a} {-x \in (-b,-a)} {-I_n}","['real-analysis', 'measure-theory', 'proof-explanation']"
89,Calculate the integral $\int\limits_{0}^{3}\frac{\sqrt{\arcsin \frac{x}{3}}}{\sqrt[4]{9-x^2}}dx$,Calculate the integral,\int\limits_{0}^{3}\frac{\sqrt{\arcsin \frac{x}{3}}}{\sqrt[4]{9-x^2}}dx,"Calculate the integral $$\int\limits_{0}^{3}\frac{\sqrt{\arcsin \frac{x}{3}}}{\sqrt[4]{9-x^2}}dx$$ I was only able to get a grade, but it gives me nothing. I can't think of a way to solve this integral. You can't use numerical methods if they don't give you an approximately accurate answer through non elementary functions. I was able to get a numerical approximation on the computer, but how do I solve the integral analytically? $$\int\limits_{0}^{3}\frac{\sqrt{\arcsin \frac{x}{3}}}{\sqrt[4]{9-x^2}}dx \approx \sum_{i=0}^{n-1} \frac{h}{6} \left(\frac{\sqrt{\arcsin \frac{x_i}{3}}}{\sqrt[4]{9-x_i^2}} + 4\frac{\sqrt{\arcsin \frac{x_i + x_{i+1}}{6}}}{\sqrt[4]{9-(x_i + x_{i+1})^2}} + \frac{\sqrt{\arcsin \frac{x_{i+1}}{3}}}{\sqrt[4]{9-x_{i+1}^2}}\right)$$ where $h = (3-0)/n$ , $x_i = 0 + i \cdot h$ , and $n$ is the number of sub-sections into which the interval $[0, 3]$ is divided","Calculate the integral I was only able to get a grade, but it gives me nothing. I can't think of a way to solve this integral. You can't use numerical methods if they don't give you an approximately accurate answer through non elementary functions. I was able to get a numerical approximation on the computer, but how do I solve the integral analytically? where , , and is the number of sub-sections into which the interval is divided","\int\limits_{0}^{3}\frac{\sqrt{\arcsin \frac{x}{3}}}{\sqrt[4]{9-x^2}}dx \int\limits_{0}^{3}\frac{\sqrt{\arcsin \frac{x}{3}}}{\sqrt[4]{9-x^2}}dx \approx \sum_{i=0}^{n-1} \frac{h}{6} \left(\frac{\sqrt{\arcsin \frac{x_i}{3}}}{\sqrt[4]{9-x_i^2}} + 4\frac{\sqrt{\arcsin \frac{x_i + x_{i+1}}{6}}}{\sqrt[4]{9-(x_i + x_{i+1})^2}} + \frac{\sqrt{\arcsin \frac{x_{i+1}}{3}}}{\sqrt[4]{9-x_{i+1}^2}}\right) h = (3-0)/n x_i = 0 + i \cdot h n [0, 3]","['real-analysis', 'calculus', 'integration']"
90,"A strictly monotone, continuous path with maximal length","A strictly monotone, continuous path with maximal length",,"If $f:[0,1]\to [0,1]$ is an increasing continuous function such that $f(0)=0$ and $f(1)=1$ , then the length of the graph $\{(t,f(t))|t\in [0,1]\}$ is bounded above by $2$ . This is easily proven by considering an arbitrary partition $$0=t_0<t_1<\cdots < t_n=1$$ and noting that $$\sum_{k=1}^n\sqrt{(t_k-t_{k-1})^2+(f(t_k)-f(t_{k-1}))^2}\leq\sum_{k=1}^n|t_k-t_{k-1}|+|f(t_k)-f(t_{k-1})|=1+f(1)-f(0)=2$$ A somewhat less trivial argument shows that for the Cantor function there is an equality: the length of the graph is exactly equal to $2$ . The Cantor function is not strictly monotone. In fact, it is constant on every open interval that was removed in the process of constructing the Cantor set . By considering graphs of the form above, it is clear that for every $\varepsilon>0$ there exists a strictly monotone continuous function such that $f(0)=0$ and $f(1)=1$ , whose graph has length larger than $2-\varepsilon$ . Question : Is there a strictly monotone continuous function with $f(0)=0$ and $f(1)=1$ whose graph has length $2$ ?","If is an increasing continuous function such that and , then the length of the graph is bounded above by . This is easily proven by considering an arbitrary partition and noting that A somewhat less trivial argument shows that for the Cantor function there is an equality: the length of the graph is exactly equal to . The Cantor function is not strictly monotone. In fact, it is constant on every open interval that was removed in the process of constructing the Cantor set . By considering graphs of the form above, it is clear that for every there exists a strictly monotone continuous function such that and , whose graph has length larger than . Question : Is there a strictly monotone continuous function with and whose graph has length ?","f:[0,1]\to [0,1] f(0)=0 f(1)=1 \{(t,f(t))|t\in [0,1]\} 2 0=t_0<t_1<\cdots < t_n=1 \sum_{k=1}^n\sqrt{(t_k-t_{k-1})^2+(f(t_k)-f(t_{k-1}))^2}\leq\sum_{k=1}^n|t_k-t_{k-1}|+|f(t_k)-f(t_{k-1})|=1+f(1)-f(0)=2 2 \varepsilon>0 f(0)=0 f(1)=1 2-\varepsilon f(0)=0 f(1)=1 2","['real-analysis', 'geometry', 'examples-counterexamples', 'monotone-functions']"
91,Show that $x_n \to x$ as $n \to \infty.$,Show that  as,x_n \to x n \to \infty.,Let $\{x_n\}_{n \geq 1}$ be a sequence of real numbers such that $\sin (t x_n) \to \sin (t x)$ for all $t \in \mathbb R.$ Then show that $x_n \to x.$ My aim is to take the power series of cosine and then differentiate it term by term which respects convergence. But for that we need to first show that $\{x_n\}_{n \geq 1}$ is bounded. But I am having hard time showing this. Could anyone please help me? Thanks a bunch!,Let be a sequence of real numbers such that for all Then show that My aim is to take the power series of cosine and then differentiate it term by term which respects convergence. But for that we need to first show that is bounded. But I am having hard time showing this. Could anyone please help me? Thanks a bunch!,\{x_n\}_{n \geq 1} \sin (t x_n) \to \sin (t x) t \in \mathbb R. x_n \to x. \{x_n\}_{n \geq 1},"['real-analysis', 'sequences-and-series', 'convergence-divergence']"
92,"The Cantor set + the Cantor set = $[0,2]$",The Cantor set + the Cantor set =,"[0,2]","Let $C$ be the Cantor set .  Prove $ C + C = [0,2]$ .  ( Notation : If $S, R$ are sets, then $S + R$ is the set of all $s + r$ with $s \in S, r \in R$ .) Proofs of this are readily available.  This question asks to help me complete my proof.  I ask that you do not give away the entire answer, but simply help me take the next step. Definitions: Let $I$ be a closed interval $[a,b]$ .  Then $I'$ , the cantorization of $I$ , is $[a, a + \delta] \cup [b - \delta, b]$ with $\delta = \frac{b-a}{3}$ .  The interval $[a, a + \delta]$ is called the lower third , the interval $[b - \delta, b]$ is called the upper third , and $I \setminus I'$ is called the middle third . Let $S$ be a finite union of disjoint, separated, closed intervals, each of uniform (finite) width.  Then $S'$ , the cantorization of $S$ , is the union of $I'$ for all $I$ where $I$ is a closed interval, a subset of $S$ , and separated from $S \setminus I$ . Lemma: $S + S = S' + S'.$ Proof: Let $I_1$ and $I_2$ be two suitable intervals in $S$ such $I_1 = [a, a + \delta], I_2 = [b, b + \delta], a \leq b$ .  Then $$(I_1 \cup I_2) + (I_1 \cup I_2) = [2a, 2a + 2\delta] \cup [a+b, a +b + 2\delta] \cup [2b, 2b + 2\delta] \\$$ and $$I_1' = [a, a + \frac{1}{3}\delta] \cup [a + \frac{2}{3}\delta, a + \delta] \\ (I_1' + I_1') = [2a, 2a + \frac{2}{3}\delta] \cup [2a + \frac{2}{3}\delta, 2a + \frac{4}{3}\delta] \cup [2a + \frac{4}{3}\delta, 2a + 2\delta] = [2a, 2a + 2\delta] \\ I_2' = [b, b + \frac{1}{3}\delta] \cup [b + \frac{2}{3}\delta, b + \delta] \\ (I_2' + I_2') = [2b, 2b + \frac{2}{3}\delta] \cup [2b + \frac{2}{3}\delta, 2b + \frac{4}{3}\delta] \cup [2b + \frac{4}{3}\delta, 2b + 2\delta] = [2b, 2b + 2\delta] \\ (I_1' + I_2') = [a + b, a + b + \frac{2}{3}\delta] \cup [a + b + \frac{2}{3}\delta, a + b + \frac{4}{3}\delta] \cup [a + b + \frac{4}{3}\delta, a + b + 2\delta] \\ (I_1' + I_2') = [a + b, a + b + 2\delta] \\ (I_1' \cup I_2') + (I_1' \cup I_2') = [2a, 2a + 2\delta] \cup [a+b, a +b + 2\delta] \cup [2b, 2b + 2\delta] \\ $$ so $$ (I_1 \cup I_2) + (I_1 \cup I_2)  = (I_1' \cup I_2') + (I_1' \cup I_2')$$ Since this holds for any $I_1, I_2$ in $S$ , $S + S = S' + S'.$ Main Proof: By induction using the lemma, for any $n \in \mathbb{n}, C_n + C_n = [0,2].$ We now need to show this for $C$ itself, and here is where I need some help. It is not enough that for any $n$ , there exists $x_n, y_n \in C_n$ such that $x_n + y_n = t$ .  We need to show that the $x$ and $y$ are in $C$ , that is, that the same $x_n, y_n$ are in all $C_n$ . One way to do this is to apply that sequences within compact sets converge to limits within the compact set.  Thus, $(x_n) \to x \in C_n, (y_n) \to y \in C_n$ .  But does that show that if for all $n$ , $x_n + y_n = t$ , then $x + y = t$ ? Another approach might be to take for each $n$ the intersection of $C_n$ and the set of ( $x, y \in [0,2]$ that sum to $t$ ).  Since both of these are compact, their intersection is compact.  But defining the set ( $x, y \in [0,2]$ that sum to $t$ ) is tricky.","Let be the Cantor set .  Prove .  ( Notation : If are sets, then is the set of all with .) Proofs of this are readily available.  This question asks to help me complete my proof.  I ask that you do not give away the entire answer, but simply help me take the next step. Definitions: Let be a closed interval .  Then , the cantorization of , is with .  The interval is called the lower third , the interval is called the upper third , and is called the middle third . Let be a finite union of disjoint, separated, closed intervals, each of uniform (finite) width.  Then , the cantorization of , is the union of for all where is a closed interval, a subset of , and separated from . Lemma: Proof: Let and be two suitable intervals in such .  Then and so Since this holds for any in , Main Proof: By induction using the lemma, for any We now need to show this for itself, and here is where I need some help. It is not enough that for any , there exists such that .  We need to show that the and are in , that is, that the same are in all . One way to do this is to apply that sequences within compact sets converge to limits within the compact set.  Thus, .  But does that show that if for all , , then ? Another approach might be to take for each the intersection of and the set of ( that sum to ).  Since both of these are compact, their intersection is compact.  But defining the set ( that sum to ) is tricky.","C  C + C = [0,2] S, R S + R s + r s \in S, r \in R I [a,b] I' I [a, a + \delta] \cup [b - \delta, b] \delta = \frac{b-a}{3} [a, a + \delta] [b - \delta, b] I \setminus I' S S' S I' I I S S \setminus I S + S = S' + S'. I_1 I_2 S I_1 = [a, a + \delta], I_2 = [b, b + \delta], a \leq b (I_1 \cup I_2) + (I_1 \cup I_2) = [2a, 2a + 2\delta] \cup [a+b, a +b + 2\delta] \cup [2b, 2b + 2\delta] \\ I_1' = [a, a + \frac{1}{3}\delta] \cup [a + \frac{2}{3}\delta, a + \delta] \\
(I_1' + I_1') = [2a, 2a + \frac{2}{3}\delta] \cup [2a + \frac{2}{3}\delta, 2a + \frac{4}{3}\delta] \cup [2a + \frac{4}{3}\delta, 2a + 2\delta] = [2a, 2a + 2\delta] \\
I_2' = [b, b + \frac{1}{3}\delta] \cup [b + \frac{2}{3}\delta, b + \delta] \\
(I_2' + I_2') = [2b, 2b + \frac{2}{3}\delta] \cup [2b + \frac{2}{3}\delta, 2b + \frac{4}{3}\delta] \cup [2b + \frac{4}{3}\delta, 2b + 2\delta] = [2b, 2b + 2\delta] \\
(I_1' + I_2') = [a + b, a + b + \frac{2}{3}\delta] \cup [a + b + \frac{2}{3}\delta, a + b + \frac{4}{3}\delta] \cup [a + b + \frac{4}{3}\delta, a + b + 2\delta] \\
(I_1' + I_2') = [a + b, a + b + 2\delta] \\
(I_1' \cup I_2') + (I_1' \cup I_2') = [2a, 2a + 2\delta] \cup [a+b, a +b + 2\delta] \cup [2b, 2b + 2\delta] \\
 
(I_1 \cup I_2) + (I_1 \cup I_2)
 = (I_1' \cup I_2') + (I_1' \cup I_2') I_1, I_2 S S + S = S' + S'. n \in \mathbb{n}, C_n + C_n = [0,2]. C n x_n, y_n \in C_n x_n + y_n = t x y C x_n, y_n C_n (x_n) \to x \in C_n, (y_n) \to y \in C_n n x_n + y_n = t x + y = t n C_n x, y \in [0,2] t x, y \in [0,2] t","['real-analysis', 'cantor-set']"
93,Is $l_2$ on $\mathbb{R}^n$ the only norm for which it is equal to its dual norm?,Is  on  the only norm for which it is equal to its dual norm?,l_2 \mathbb{R}^n,"Given any norm $\|.\|$ on $\mathbb{R}^n$ , its dual norm $\|.\|^D$ is defined as the following: $\|v\|^D = \sup_{\|x\|\leq 1} |(v,x)|$ , where $(,)$ is the standard Euclidean Inner product. Under that definition, it turns out that the dual norm of the $l_p$ norm is $l_q$ , when $1/p+1/q=1$ . That implies that for $l_2$ , it is equal to the dual norm. Now I wanted to think about the converse: If $\|.\|$ is a norm such that $\|.\| = \|.\|^D$ , then is $\|.\|=l_2$ ? After being unable to find anything online, I wrote the following justification: Consider any vector in the boundary of the ball of radius $1$ in $\|.\|$ : \begin{align*}     1 = \|x\| = \|x\|^D = \sup\left\{\left|\sum^n_i x_iv_i\right|: \|v\| \leq 1\right\} \end{align*} Using C-S Inequality (with $(,)$ denoting the usual inner product) we have: \begin{align*}     |(x,v)| \leq \|x\|_2 \|v\|_2  \end{align*} where equality holds if and only if $x,v$ are linearly dependent. Considering that, let $v = x$ . Then $\|v\| = 1$ and we have: \begin{align*}      |{(x,v)}| = \|x\|_2^2 \implies \|x\|^D = \|x\| = 1 \geq \|x\|^2_2 \implies 1 \geq \|x\|_2 \end{align*} Suppose $\exists v_0$ such that $\|v_0\|<1$ , and $\|x\|^D \leq |{(x,v_0)}|$ . Then consider $v' = \frac{v}{\|v_0\|}$ . We will have $\|{(x,v)}\| \leq \|{x}\|^D \leq |(x,v_0)| \implies \|v_0\|\geq 1$ . Hence we have a contradiction. Consider the functional defined by $f: \overline{B_1}(0)\rightarrow \mathbb{R}$ as $f(v) = |(x,v)|$ , where $B$ denotes the ball in $\|.\|$ . Then by the compactness of the unit ball in any norm, and continuity of $f$ , it attains maxima; let that point be denoted by $v_0$ . Then by the earlier paragraph, $\|v_0\|=1$ . However, we showed that whenever $\|v_0\| =1$ , we should get $\|v_0\|_2 \leq 1$ . Hence we get: \begin{align*}     \left|{\sum^n_i x_iv_i}\right| \leq |{(x,v_0)}| \leq \|x\|_2\|v_0\|_2 \leq \|x\|_2 \implies 1 = \|x\|^D \leq \|x\|_2 \end{align*} Hence we get the following for any unit vector: \begin{align*}    \|x\| = \|x\|_2 \end{align*} Now given a general non zero vector, consider: \begin{align*}     1 = \|{\frac{x}{\|x\|}}\| =  \|{\frac{x}{\|x\|}}\|_2 \implies \|x\|= \|x\|_2\  \square \end{align*} Is the proposed solution correct? Is there a better way to see the result? Can this notion be meaniningfully extended in any sense to infinite dimensions?","Given any norm on , its dual norm is defined as the following: , where is the standard Euclidean Inner product. Under that definition, it turns out that the dual norm of the norm is , when . That implies that for , it is equal to the dual norm. Now I wanted to think about the converse: If is a norm such that , then is ? After being unable to find anything online, I wrote the following justification: Consider any vector in the boundary of the ball of radius in : Using C-S Inequality (with denoting the usual inner product) we have: where equality holds if and only if are linearly dependent. Considering that, let . Then and we have: Suppose such that , and . Then consider . We will have . Hence we have a contradiction. Consider the functional defined by as , where denotes the ball in . Then by the compactness of the unit ball in any norm, and continuity of , it attains maxima; let that point be denoted by . Then by the earlier paragraph, . However, we showed that whenever , we should get . Hence we get: Hence we get the following for any unit vector: Now given a general non zero vector, consider: Is the proposed solution correct? Is there a better way to see the result? Can this notion be meaniningfully extended in any sense to infinite dimensions?","\|.\| \mathbb{R}^n \|.\|^D \|v\|^D = \sup_{\|x\|\leq 1} |(v,x)| (,) l_p l_q 1/p+1/q=1 l_2 \|.\| \|.\| = \|.\|^D \|.\|=l_2 1 \|.\| \begin{align*}
    1 = \|x\| = \|x\|^D = \sup\left\{\left|\sum^n_i x_iv_i\right|: \|v\| \leq 1\right\}
\end{align*} (,) \begin{align*}
    |(x,v)| \leq \|x\|_2 \|v\|_2 
\end{align*} x,v v = x \|v\| = 1 \begin{align*}
     |{(x,v)}| = \|x\|_2^2 \implies \|x\|^D = \|x\| = 1 \geq \|x\|^2_2 \implies 1 \geq \|x\|_2
\end{align*} \exists v_0 \|v_0\|<1 \|x\|^D \leq |{(x,v_0)}| v' = \frac{v}{\|v_0\|} \|{(x,v)}\| \leq \|{x}\|^D \leq |(x,v_0)| \implies \|v_0\|\geq 1 f: \overline{B_1}(0)\rightarrow \mathbb{R} f(v) = |(x,v)| B \|.\| f v_0 \|v_0\|=1 \|v_0\| =1 \|v_0\|_2 \leq 1 \begin{align*}
    \left|{\sum^n_i x_iv_i}\right| \leq |{(x,v_0)}| \leq \|x\|_2\|v_0\|_2 \leq \|x\|_2 \implies 1 = \|x\|^D \leq \|x\|_2
\end{align*} \begin{align*}
   \|x\| = \|x\|_2
\end{align*} \begin{align*}
    1 = \|{\frac{x}{\|x\|}}\| =  \|{\frac{x}{\|x\|}}\|_2 \implies \|x\|= \|x\|_2\  \square
\end{align*}","['real-analysis', 'linear-algebra', 'functional-analysis', 'dual-spaces', 'matrix-analysis']"
94,Show that a countable and dense set $A$ of $\mathbb{R}$ is not closed.,Show that a countable and dense set  of  is not closed.,A \mathbb{R},"Show that a countable and dense set $A$ of $\mathbb{R}$ is not closed. Deduce that $\mathbb{R}$ is not countable. Hint: Let $(a_n)$ be a sequence obtained by ranking the points of $A$ in a certain order, define a sequence of intervals $[b_n , c_n ]$ such that $b_{n-1} < b_n < c_n < c_{n-1}$ , whatever $n$ is, and that $[b_n,c_n]$ contains no point $a_k$ such that $k < n$ . It's hard for me to see how I can define such a sequence of intervals with only the sequence $a_n$ . Then I think that I have to apply the Borel-Lebesgue theorem, saying that any compact set A is closed and bounded.","Show that a countable and dense set of is not closed. Deduce that is not countable. Hint: Let be a sequence obtained by ranking the points of in a certain order, define a sequence of intervals such that , whatever is, and that contains no point such that . It's hard for me to see how I can define such a sequence of intervals with only the sequence . Then I think that I have to apply the Borel-Lebesgue theorem, saying that any compact set A is closed and bounded.","A \mathbb{R} \mathbb{R} (a_n) A [b_n , c_n ] b_{n-1} < b_n < c_n < c_{n-1} n [b_n,c_n] a_k k < n a_n","['real-analysis', 'general-topology', 'compactness']"
95,What can we say about $f(x)=\prod_{n=2}^\infty(1-n^{-1/x})$?,What can we say about ?,f(x)=\prod_{n=2}^\infty(1-n^{-1/x}),"Main Question: What can we say about $f(x)=\prod_{n=2}^\infty(1-n^{-1/x})$ ? Is $f(x)$ integrable from $0$ to $1$ ? Is it continuous? If we have an affirmative answer to the question on integrability... $$I=\int_{0}^{1}f(x)\,dx=\int_{1}^{\infty}\frac{1}{x^2}\prod_{n=2}^{\infty}\left(1-\frac{1}{n^{x}}\right)dx$$ Can we get bounds on $I$ ? Can we get a closed form for I? Can we get a decent approximation for $I$ ? Motivations I saw this post wherein I found $$ \prod_{n=2}^{\infty} \left(1 - \frac{1}{n^p}\right) = \prod_{\omega : \omega^p = 1} \frac{1}{\Gamma(2-\omega)}. $$ After plotting $f(x)$ , I found myself unsatisfied when I couldn't get a handle on $I$ using desmos. Maybe the issue is that $f_m(x)=\prod_{n=2}^{m}(1-n^{-1/x})$ don't converge fast enough? As I run $m\to \infty$ I observe $I_m= \int_0^1 f_m(x)\,dx$ wiggling.  I'm not quite sure how to proceed in my curiosities. Thanks for any insights.","Main Question: What can we say about ? Is integrable from to ? Is it continuous? If we have an affirmative answer to the question on integrability... Can we get bounds on ? Can we get a closed form for I? Can we get a decent approximation for ? Motivations I saw this post wherein I found After plotting , I found myself unsatisfied when I couldn't get a handle on using desmos. Maybe the issue is that don't converge fast enough? As I run I observe wiggling.  I'm not quite sure how to proceed in my curiosities. Thanks for any insights.","f(x)=\prod_{n=2}^\infty(1-n^{-1/x}) f(x) 0 1 I=\int_{0}^{1}f(x)\,dx=\int_{1}^{\infty}\frac{1}{x^2}\prod_{n=2}^{\infty}\left(1-\frac{1}{n^{x}}\right)dx I I  \prod_{n=2}^{\infty} \left(1 - \frac{1}{n^p}\right) = \prod_{\omega : \omega^p = 1} \frac{1}{\Gamma(2-\omega)}.  f(x) I f_m(x)=\prod_{n=2}^{m}(1-n^{-1/x}) m\to \infty I_m= \int_0^1 f_m(x)\,dx","['real-analysis', 'integration', 'number-theory', 'infinite-product', 'weierstrass-factorization']"
96,Existence of probability density on $S^1$ with first circular moment on $S^1$,Existence of probability density on  with first circular moment on,S^1 S^1,"I am searching for a function $f$ on the unit circle $S^1$ with the following properties. $f \geq 0$ almost everywhere. $\int_{S^1} f = 1$ . $\int_{S^1} e^{i \theta} f (\theta) d \theta = 1$ . In other words, I would like a probability density $f$ on the unit circle whose first circular moment also lies on the unit circle. Using a little Fourier series, the conditions 2 and 3 above are equivalent to saying $f$ has the form $$ f(\theta) = \frac{1}{2 \pi} \left( 1 + 2 \cos{\theta} \right) + \sum_{n \in \mathbb{Z} \backslash \{ \pm 1 , 0 \}} a_n e^{in\theta} \tag{*} $$ for some coefficients $a_n \in \mathbb{C}$ . The question then becomes whether it is possible to choose the $a_n$ so that $f$ is non-negative. My intuition tells me, if such a function exists, then it would be rather pathological. Question: Is some able to demonstrate the existence/non-existence of such a function? If such a function exists and is ""nice"" (e.g. smooth), is a closed-form possible? Context: I've been studying some directional statistics and I've noticed that many of the wrapped distributions have the property that their circular moments do not lie on the unit circle. For example, the wrapped normal distribution has circular moments $\langle e^{in\theta} \rangle = e^{in\mu - n^2 \sigma^2 / 2}$ , where $\mu$ and $\sigma$ are the mean and standard deviation of the unwrapped distribution. Clearly, this lies on $S^1$ only if $\sigma = 0$ , in which case the wrapped normal becomes a Dirac comb distribution. Part of me feels like this behavior is reasonable: depending on where the distribution is concentrated, it will ""pull"" the unit vector $e^{in\theta}$ toward the concentration center (in this case $\mu$ ), but not ""all the way"". For a uniform distribution, e.g. $\sigma \rightarrow + \infty$ , the circular moments all lie at the origin (i.e., they all get ""pulled"" equally in all directions). I started to wonder if this is always the case for probability densities on the unit circle. Or could I, at the very least, make the first moment lie on the unit circle?","I am searching for a function on the unit circle with the following properties. almost everywhere. . . In other words, I would like a probability density on the unit circle whose first circular moment also lies on the unit circle. Using a little Fourier series, the conditions 2 and 3 above are equivalent to saying has the form for some coefficients . The question then becomes whether it is possible to choose the so that is non-negative. My intuition tells me, if such a function exists, then it would be rather pathological. Question: Is some able to demonstrate the existence/non-existence of such a function? If such a function exists and is ""nice"" (e.g. smooth), is a closed-form possible? Context: I've been studying some directional statistics and I've noticed that many of the wrapped distributions have the property that their circular moments do not lie on the unit circle. For example, the wrapped normal distribution has circular moments , where and are the mean and standard deviation of the unwrapped distribution. Clearly, this lies on only if , in which case the wrapped normal becomes a Dirac comb distribution. Part of me feels like this behavior is reasonable: depending on where the distribution is concentrated, it will ""pull"" the unit vector toward the concentration center (in this case ), but not ""all the way"". For a uniform distribution, e.g. , the circular moments all lie at the origin (i.e., they all get ""pulled"" equally in all directions). I started to wonder if this is always the case for probability densities on the unit circle. Or could I, at the very least, make the first moment lie on the unit circle?","f S^1 f \geq 0 \int_{S^1} f = 1 \int_{S^1} e^{i \theta} f (\theta) d \theta = 1 f f  f(\theta) = \frac{1}{2 \pi} \left( 1 + 2 \cos{\theta} \right) + \sum_{n \in \mathbb{Z} \backslash \{ \pm 1 , 0 \}} a_n e^{in\theta} \tag{*}  a_n \in \mathbb{C} a_n f \langle e^{in\theta} \rangle = e^{in\mu - n^2 \sigma^2 / 2} \mu \sigma S^1 \sigma = 0 e^{in\theta} \mu \sigma \rightarrow + \infty","['real-analysis', 'probability', 'probability-theory', 'probability-distributions', 'fourier-series']"
97,Does a random variable with differentiable distribution function have density?,Does a random variable with differentiable distribution function have density?,,"Question: Suppose that $X: \Omega \to \mathbb{R}$ is a random variable and it's distribution function $F(x) = \mathbf{P}(\xi \le x)$ is differentiable for all $x$ . Is it true that $F'(x)$ is density? What do I know: Remark 1 . If $F'$ is continuous then $F'$ is density - see, e.g., A random variable $X$ with differentiable distribution function has a density Hence if there's counterexample $F$ then $F'$ is not continuous everywhere. Remark 2 . Absolutely continuous measures on $\mathbb{R}$ are precisely those that have densities. And if $g$ is differentiable everywhere then it doesn't follow that $g$ is absolutely continuous. Does the everywhere differentiability of $f$ imply it is absolutely continuous on a compact interval? Unfortunately, the example from the link doesn't help. Remark 3 (close to remark 2) . In order to make a counterexample it's sufficient to find $F$ such that $F'$ is not intergrable. It's easy to find differentiable function with nonintergable derivative. Consider $g(x) = x^2 \sin(\frac{1}{x^2})$ is differentiable for all $x$ and $\int_{0}^1 g(x) dx$ doesn't exist. Unfortunatelly, $g(x)$ is not a counterexample in our problem, because it's not monotone and hence it's not a distribution function. A function $(g(x) + 1000 x) \cdot const$ also doesn't ""work"". Addition: I found the similar question here https://math.stackexchange.com/questions/3387905/derivation-of-distribution-function-is-density-function/3387913 but there're no proofs there, so in fact there's still no answer. Addition2: If $F'$ exists for all $x$ then it doesn't follow that $F'$ is continious. Counterexample: we may consider $\tilde{F}(x) = \frac{g(x) - g(-5)}{g(5)}I_{|x|\le 5} + I_{x > 5}$ where $g(x) = x^2 \sin (\frac{1}x) + 5x + 10$ and make $F(x)$ which is smooth, nondecreasing and which coinsides with $F$ for all $x \in \mathbb{R} \backslash (U_{\frac1{100}}(-5) \cup U_{\frac1{100}}(5))$ .","Question: Suppose that is a random variable and it's distribution function is differentiable for all . Is it true that is density? What do I know: Remark 1 . If is continuous then is density - see, e.g., A random variable $X$ with differentiable distribution function has a density Hence if there's counterexample then is not continuous everywhere. Remark 2 . Absolutely continuous measures on are precisely those that have densities. And if is differentiable everywhere then it doesn't follow that is absolutely continuous. Does the everywhere differentiability of $f$ imply it is absolutely continuous on a compact interval? Unfortunately, the example from the link doesn't help. Remark 3 (close to remark 2) . In order to make a counterexample it's sufficient to find such that is not intergrable. It's easy to find differentiable function with nonintergable derivative. Consider is differentiable for all and doesn't exist. Unfortunatelly, is not a counterexample in our problem, because it's not monotone and hence it's not a distribution function. A function also doesn't ""work"". Addition: I found the similar question here https://math.stackexchange.com/questions/3387905/derivation-of-distribution-function-is-density-function/3387913 but there're no proofs there, so in fact there's still no answer. Addition2: If exists for all then it doesn't follow that is continious. Counterexample: we may consider where and make which is smooth, nondecreasing and which coinsides with for all .",X: \Omega \to \mathbb{R} F(x) = \mathbf{P}(\xi \le x) x F'(x) F' F' F F' \mathbb{R} g g F F' g(x) = x^2 \sin(\frac{1}{x^2}) x \int_{0}^1 g(x) dx g(x) (g(x) + 1000 x) \cdot const F' x F' \tilde{F}(x) = \frac{g(x) - g(-5)}{g(5)}I_{|x|\le 5} + I_{x > 5} g(x) = x^2 \sin (\frac{1}x) + 5x + 10 F(x) F x \in \mathbb{R} \backslash (U_{\frac1{100}}(-5) \cup U_{\frac1{100}}(5)),"['real-analysis', 'probability', 'probability-theory', 'measure-theory', 'density-function']"
98,"proved that$f (x)$ is uniformly continuous on $[0,+\infty)$ if and only if $\lim\limits_{x\to+\infty}\frac{f(x)}{x}$ exists.",proved that is uniformly continuous on  if and only if  exists.,"f (x) [0,+\infty) \lim\limits_{x\to+\infty}\frac{f(x)}{x}","Let $f(x)$ be second-order differentiable on $[0,+\infty)$ , $f''(x)\geqslant\sin x(\forall x\in[0,+\infty))$ .Prove that $f (x)$ is uniformly continuous on $[0,+\infty)$ if and only if $\lim\limits_{x\to+\infty}\frac{f(x)}{x}$ exists.This is our monthly exam question,I try to expand it with Taylor's theorem: $$f(x+h)=f(x)+f'(x)h+\frac{f''(\xi)}{2}h^2\geqslant f(x)+f'(x)h +\frac{\sin x}{2}h^2.$$ But then I don't know how to deal with it, I also know that if $f (+\infty)$ exists, then $f (x)$ is uniformly continuous, but it doesn't seem to help the problem.","Let be second-order differentiable on , .Prove that is uniformly continuous on if and only if exists.This is our monthly exam question,I try to expand it with Taylor's theorem: But then I don't know how to deal with it, I also know that if exists, then is uniformly continuous, but it doesn't seem to help the problem.","f(x) [0,+\infty) f''(x)\geqslant\sin x(\forall x\in[0,+\infty)) f (x) [0,+\infty) \lim\limits_{x\to+\infty}\frac{f(x)}{x} f(x+h)=f(x)+f'(x)h+\frac{f''(\xi)}{2}h^2\geqslant f(x)+f'(x)h +\frac{\sin x}{2}h^2. f (+\infty) f (x)","['real-analysis', 'uniform-continuity']"
99,"Convergence of a positive series, given that $\sum_{k=1}^n(a_k-a_n)$ is bounded","Convergence of a positive series, given that  is bounded",\sum_{k=1}^n(a_k-a_n),"Question: Assume $\sum_{n=1}^\infty a_n$ is a positive series, with $a_n$ decreases to $0$ , and $$\sum_{k=1}^n(a_k-a_n)$$ is bounded with respect to $n$ . Prove that $\sum_{n=1}^\infty a_n$ is convergent. My idea: Suppose $$(a_1-a_n)+(a_2-a_n)+\cdots+(a_n-a_n)\leq M$$ for any integer $n$ . Then for any integer $m$ , choose a greater $n$ to have $$\begin{align}&\quad\,(a_1-a_n)+(a_2-a_n)+\cdots+(a_m-a_n)\\ &\leq (a_1-a_n)+(a_2-a_n)+\cdots+(a_m-a_n)+\cdots+(a_n-a_n)\\ &\leq M\end{align}$$ since every term in brackets is nonnegative. Let $n\to\infty$ in $$(a_1-a_n)+(a_2-a_n)+\cdots+(a_m-a_n)\leq M$$ to conclude $a_1+a_2+\cdots+a_m\leq M$ . Then let $m\to\infty$ to conclude that $\sum a_n$ is convergent. Is that a reasonable proof? Thank you!!","Question: Assume is a positive series, with decreases to , and is bounded with respect to . Prove that is convergent. My idea: Suppose for any integer . Then for any integer , choose a greater to have since every term in brackets is nonnegative. Let in to conclude . Then let to conclude that is convergent. Is that a reasonable proof? Thank you!!","\sum_{n=1}^\infty a_n a_n 0 \sum_{k=1}^n(a_k-a_n) n \sum_{n=1}^\infty a_n (a_1-a_n)+(a_2-a_n)+\cdots+(a_n-a_n)\leq M n m n \begin{align}&\quad\,(a_1-a_n)+(a_2-a_n)+\cdots+(a_m-a_n)\\
&\leq (a_1-a_n)+(a_2-a_n)+\cdots+(a_m-a_n)+\cdots+(a_n-a_n)\\
&\leq M\end{align} n\to\infty (a_1-a_n)+(a_2-a_n)+\cdots+(a_m-a_n)\leq M a_1+a_2+\cdots+a_m\leq M m\to\infty \sum a_n","['real-analysis', 'sequences-and-series', 'convergence-divergence']"
