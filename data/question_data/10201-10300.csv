,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,Integral $\int_0^\infty\frac{\tanh^2(x)}{x^2}dx$,Integral,\int_0^\infty\frac{\tanh^2(x)}{x^2}dx,"It appears that $$\int_0^\infty\frac{\tanh^2(x)}{x^2}dx\stackrel{\color{gray}?}=\frac{14\,\zeta(3)}{\pi^2}.\tag1$$ (so far I have about $1000$ decimal digits to confirm that). After changing variable $x=-\tfrac12\ln z$, it takes an equivalent form $$\int_0^1\frac{(1-z)^2}{z\,(1+z)^2 \ln^2z}dz\stackrel{\color{gray}?}=\frac{7\,\zeta(3)}{\pi^2}.\tag2$$ Quick lookup in Gradshteyn—Ryzhik and Prudnikov et al. did not find this integral, and it also is returned unevaluated by Mathematica and Maple . How can we prove this result? Am I overlooking anything trivial? Further questions: Is it possible to generalize it and find a closed form of  $$\mathcal A(a)=\int_0^\infty\frac{\tanh(x)\tanh(ax)}{x^2}dx,\tag3$$ or at least of a particular case with $a=2$? Can we generalize it to higher powers $$\mathcal B(n)=\int_0^\infty\left(\frac{\tanh(x)}x\right)^ndx?\tag4$$ Thanks to nospoon 's comment below, we know that  $$\mathcal B(3)=\frac{186\,\zeta(5)}{\pi^4}-\frac{7\,\zeta(3)}{\pi^2}\tag5$$ I checked higher powers for this pattern, and, indeed, it appears that $$\begin{align}&\mathcal B(4)\stackrel{\color{gray}?}=-\frac{496\,\zeta(5)}{3\,\pi^4}+\frac{2540\,\zeta(7)}{\pi^6}\\ &\mathcal B(5)\stackrel{\color{gray}?}=\frac{31\,\zeta(5)}{\pi^4}-\frac{3175\,\zeta(7)}{\pi^6}+\frac{35770\,\zeta(9)}{\pi^8}\\ &\mathcal B(6)\stackrel{\color{gray}?}=\frac{5842\,\zeta(7)}{5\,\pi^6}-\frac{57232\,\zeta(9)}{\pi^8}+\frac{515844\,\zeta(11)}{\pi^{10}}\end{align}\tag6$$","It appears that $$\int_0^\infty\frac{\tanh^2(x)}{x^2}dx\stackrel{\color{gray}?}=\frac{14\,\zeta(3)}{\pi^2}.\tag1$$ (so far I have about $1000$ decimal digits to confirm that). After changing variable $x=-\tfrac12\ln z$, it takes an equivalent form $$\int_0^1\frac{(1-z)^2}{z\,(1+z)^2 \ln^2z}dz\stackrel{\color{gray}?}=\frac{7\,\zeta(3)}{\pi^2}.\tag2$$ Quick lookup in Gradshteyn—Ryzhik and Prudnikov et al. did not find this integral, and it also is returned unevaluated by Mathematica and Maple . How can we prove this result? Am I overlooking anything trivial? Further questions: Is it possible to generalize it and find a closed form of  $$\mathcal A(a)=\int_0^\infty\frac{\tanh(x)\tanh(ax)}{x^2}dx,\tag3$$ or at least of a particular case with $a=2$? Can we generalize it to higher powers $$\mathcal B(n)=\int_0^\infty\left(\frac{\tanh(x)}x\right)^ndx?\tag4$$ Thanks to nospoon 's comment below, we know that  $$\mathcal B(3)=\frac{186\,\zeta(5)}{\pi^4}-\frac{7\,\zeta(3)}{\pi^2}\tag5$$ I checked higher powers for this pattern, and, indeed, it appears that $$\begin{align}&\mathcal B(4)\stackrel{\color{gray}?}=-\frac{496\,\zeta(5)}{3\,\pi^4}+\frac{2540\,\zeta(7)}{\pi^6}\\ &\mathcal B(5)\stackrel{\color{gray}?}=\frac{31\,\zeta(5)}{\pi^4}-\frac{3175\,\zeta(7)}{\pi^6}+\frac{35770\,\zeta(9)}{\pi^8}\\ &\mathcal B(6)\stackrel{\color{gray}?}=\frac{5842\,\zeta(7)}{5\,\pi^6}-\frac{57232\,\zeta(9)}{\pi^8}+\frac{515844\,\zeta(11)}{\pi^{10}}\end{align}\tag6$$",,"['calculus', 'integration', 'definite-integrals', 'closed-form', 'hyperbolic-functions']"
1,How do I integrate the following? $\int{\frac{(1+x^{2})\mathrm dx}{(1-x^{2})\sqrt{1+x^{4}}}}$,How do I integrate the following?,\int{\frac{(1+x^{2})\mathrm dx}{(1-x^{2})\sqrt{1+x^{4}}}},"$$\int{\frac{1+x^2}{(1-x^2)\sqrt{1+x^4}}}\mathrm dx$$ This was a Calc 2 problem for extra credit (we have done hyperbolic trig functions too, if that helps) and I didn't get it (don't think anyone did) -- how would you go about it?","$$\int{\frac{1+x^2}{(1-x^2)\sqrt{1+x^4}}}\mathrm dx$$ This was a Calc 2 problem for extra credit (we have done hyperbolic trig functions too, if that helps) and I didn't get it (don't think anyone did) -- how would you go about it?",,"['calculus', 'integration']"
2,"Why does $ \int_0^1 \lceil { x\sin({1 \over x})} \rceil \,dx = 1 - \frac{\log(4)}{2\pi} $?",Why does ?," \int_0^1 \lceil { x\sin({1 \over x})} \rceil \,dx = 1 - \frac{\log(4)}{2\pi} ","One time I was bored and played around a bit with integrals and wolfram alpha and tested the following integral: http://www.wolframalpha.com/input/?i=integral_0%5E1+ceil%28x*sin%281%2Fx%29%29 Note: The result at my laptop shows: $ \int_0^1 \lceil { x\sin({1 \over x})} \rceil  \,dx  = 1 - \frac{\log(4)}{2\pi} $ I was a bit surprised seeing this and wondered, if this result makes any sense and if yes, if there were an explanation, where $\frac{\log(4)}{2\pi}$ exactly comes from. The numbers look too neat to be random, so there might be a way to derive those values directly without using the help of a calculator etc. As always: Thanks in advance for any constructive answer/comment.","One time I was bored and played around a bit with integrals and wolfram alpha and tested the following integral: http://www.wolframalpha.com/input/?i=integral_0%5E1+ceil%28x*sin%281%2Fx%29%29 Note: The result at my laptop shows: $ \int_0^1 \lceil { x\sin({1 \over x})} \rceil  \,dx  = 1 - \frac{\log(4)}{2\pi} $ I was a bit surprised seeing this and wondered, if this result makes any sense and if yes, if there were an explanation, where $\frac{\log(4)}{2\pi}$ exactly comes from. The numbers look too neat to be random, so there might be a way to derive those values directly without using the help of a calculator etc. As always: Thanks in advance for any constructive answer/comment.",,"['calculus', 'integration']"
3,Not understanding derivative of a matrix-matrix product.,Not understanding derivative of a matrix-matrix product.,,"I am trying to figure out a the derivative of a matrix-matrix multiplication, but to no avail. This document seems to show me the answer, but I am having a hard time parsing it and understanding it. Here is my problem: We have $\mathbf{D} \in \Re^{m n}$ , $\mathbf{W} \in \Re^{m q}$ , and $\mathbf{X} \in \Re^{q n}$ . Furthermore, $\mathbf{D} = \mathbf{W}\mathbf{X}$ . (NOT an element wise multiplication - a normal matrix-matrix multiply). I am trying to derive the derivative of $\mathbf{D}$ , w.r.t $\mathbf{W}$ , and the derivative of $\mathbf{D}$ , w.r.t $\mathbf{X}$ . My class note this is taken from seems to indicate that $$ \frac{\delta \mathbf{D}}{\delta \mathbf{W}} = \mathbf{X}^{T} \text{ and that } \frac{\delta \mathbf{D}}{\delta \mathbf{X}} = \mathbf{W}^{T},  $$ but I am floored as to how he derived this. Furthermore, in taking the derivatives, we are asking ourselves how every element in $\mathbf{D}$ changes with perturbations by every element in, say, $\mathbf{X}$ , - so wouldn't the resulting combinations blow up to be a-lot more than what $\mathbf{W}^{T}$ has? I cant even see how the dimensionality is right here. EDIT: Id like to add the context of this question. It's coming from here , and here is my marked screen-shot of my problem. How are they deriving those terms? (Note: I understand the chain-rule aspect, and I am not wondering about that. I am asking about the simpler intermediate step). Thanks.","I am trying to figure out a the derivative of a matrix-matrix multiplication, but to no avail. This document seems to show me the answer, but I am having a hard time parsing it and understanding it. Here is my problem: We have , , and . Furthermore, . (NOT an element wise multiplication - a normal matrix-matrix multiply). I am trying to derive the derivative of , w.r.t , and the derivative of , w.r.t . My class note this is taken from seems to indicate that but I am floored as to how he derived this. Furthermore, in taking the derivatives, we are asking ourselves how every element in changes with perturbations by every element in, say, , - so wouldn't the resulting combinations blow up to be a-lot more than what has? I cant even see how the dimensionality is right here. EDIT: Id like to add the context of this question. It's coming from here , and here is my marked screen-shot of my problem. How are they deriving those terms? (Note: I understand the chain-rule aspect, and I am not wondering about that. I am asking about the simpler intermediate step). Thanks.","\mathbf{D} \in \Re^{m n} \mathbf{W} \in \Re^{m q} \mathbf{X} \in \Re^{q n} \mathbf{D} = \mathbf{W}\mathbf{X} \mathbf{D} \mathbf{W} \mathbf{D} \mathbf{X} 
\frac{\delta \mathbf{D}}{\delta \mathbf{W}} = \mathbf{X}^{T} \text{ and that } \frac{\delta \mathbf{D}}{\delta \mathbf{X}} = \mathbf{W}^{T}, 
 \mathbf{D} \mathbf{X} \mathbf{W}^{T}","['calculus', 'matrices', 'derivatives', 'matrix-calculus']"
4,Possibility to simplify $\sum\limits_{k = - \infty }^\infty {\frac{{{{\left( { - 1} \right)}^k}}}{{a + k}} = \frac{\pi }{{\sin \pi a}}} $,Possibility to simplify,\sum\limits_{k = - \infty }^\infty {\frac{{{{\left( { - 1} \right)}^k}}}{{a + k}} = \frac{\pi }{{\sin \pi a}}} ,"Is there any way to show that $$\sum\limits_{k =  - \infty }^\infty  {\frac{{{{\left( { - 1} \right)}^k}}}{{a + k}} = \frac{1}{a} + \sum\limits_{k = 1}^\infty  {{{\left( { - 1} \right)}^k}\left( {\frac{1}{{a - k}} + \frac{1}{{a + k}}} \right)}=\frac{\pi }{{\sin \pi a}}} $$ Where $0 < a = \dfrac{n+1}{m} < 1$ The infinite series is equal to $$\int\limits_{ - \infty }^\infty  {\frac{{{e^{at}}}}{{{e^t} + 1}}dt} $$ To get to the result, I split the integral at $x=0$ and use the convergent series in $(0,\infty)$ and $(-\infty,0)$ respectively: $$\frac{1}{{1 + {e^t}}} = \sum\limits_{k = 0}^\infty  {{{\left( { - 1} \right)}^k}{e^{ - \left( {k + 1} \right)t}}} $$ $$\frac{1}{{1 + {e^t}}} = \sum\limits_{k = 0}^\infty  {{{\left( { - 1} \right)}^k}{e^{kt}}} $$ Since $0 < a < 1$ $$\eqalign{   & \mathop {\lim }\limits_{t \to 0} \frac{{{e^{\left( {k + a} \right)t}}}}{{k + a}} - \mathop {\lim }\limits_{t \to  - \infty } \frac{{{e^{\left( {k + a} \right)t}}}}{{k + a}} = \frac{1}{{k + a}}  \cr    & \mathop {\lim }\limits_{t \to \infty } \frac{{{e^{\left( {a - k - 1} \right)t}}}}{{k + a}} - \mathop {\lim }\limits_{t \to 0} \frac{{{e^{\left( {a - k - 1} \right)t}}}}{{k + a}} =  - \frac{1}{{a - \left( {k + 1} \right)}} \cr} $$ A change in the indices will give the desired series. Although I don't mind direct solutions from tables and other sources, I prefer an elaborated answer. Here's the solution in terms of $\psi(x)$. By separating even and odd indices we can get $$\eqalign{   & \sum\limits_{k = 0}^\infty  {\frac{{{{\left( { - 1} \right)}^k}}}{{a + k}}}  = \sum\limits_{k = 0}^\infty  {\frac{1}{{a + 2k}}}  - \sum\limits_{k = 0}^\infty  {\frac{1}{{a + 2k + 1}}}   \cr    & \sum\limits_{k = 0}^\infty  {\frac{{{{\left( { - 1} \right)}^k}}}{{a - k}}}  = \sum\limits_{k = 0}^\infty  {\frac{1}{{a - 2k}}}  - \sum\limits_{k = 0}^\infty  {\frac{1}{{a - 2k - 1}}}  \cr} $$ which gives $$\sum\limits_{k = 0}^\infty  {\frac{{{{\left( { - 1} \right)}^k}}}{{a + k}}}  = \frac{1}{2}\psi \left( {\frac{{a + 1}}{2}} \right) - \frac{1}{2}\psi \left( {\frac{a}{2}} \right)$$ $$\sum\limits_{k = 0}^\infty  {\frac{{{{\left( { - 1} \right)}^k}}}{{a - k}}}  = \frac{1}{2}\psi \left( {1 - \frac{a}{2}} \right) - \frac{1}{2}\psi \left( {1 - \frac{{a + 1}}{2}} \right) + \frac{1}{a}$$ Then $$\eqalign{   & \sum\limits_{k =  - \infty }^\infty  {\frac{{{{\left( { - 1} \right)}^k}}}{{a + k}}}  = \sum\limits_{k = 0}^\infty  {\frac{{{{\left( { - 1} \right)}^k}}}{{a + k}}}  + \sum\limits_{k = 0}^\infty  {\frac{{{{\left( { - 1} \right)}^k}}}{{a - k}}}  - \frac{1}{a} =   \cr    &  = \left\{ {\frac{1}{2}\psi \left( {1 - \frac{a}{2}} \right) - \frac{1}{2}\psi \left( {\frac{a}{2}} \right)} \right\} - \left\{ {\frac{1}{2}\psi \left( {1 - \frac{{a + 1}}{2}} \right) - \frac{1}{2}\psi \left( {\frac{{a + 1}}{2}} \right)} \right\} \cr} $$ But using the reflection formula one has $$\eqalign{   & \frac{1}{2}\psi \left( {1 - \frac{a}{2}} \right) - \frac{1}{2}\psi \left( {\frac{a}{2}} \right) = \frac{\pi }{2}\cot \frac{{\pi a}}{2}  \cr    & \frac{1}{2}\psi \left( {1 - \frac{{a + 1}}{2}} \right) - \frac{1}{2}\psi \left( {\frac{{a + 1}}{2}} \right) = \frac{\pi }{2}\cot \frac{{\pi \left( {a + 1} \right)}}{2} =  - \frac{\pi }{2}\tan \frac{{\pi a}}{2} \cr} $$ So the series become $$\eqalign{   & \sum\limits_{k =  - \infty }^\infty  {\frac{{{{\left( { - 1} \right)}^k}}}{{a + k}}}  = \frac{\pi }{2}\left\{ {\cot \frac{{\pi a}}{2} + \tan \frac{{\pi a}}{2}} \right\}  \cr    & \sum\limits_{k =  - \infty }^\infty  {\frac{{{{\left( { - 1} \right)}^k}}}{{a + k}}}  = \pi \csc \pi a \cr} $$ The last being an application of a trigonometric identity.","Is there any way to show that $$\sum\limits_{k =  - \infty }^\infty  {\frac{{{{\left( { - 1} \right)}^k}}}{{a + k}} = \frac{1}{a} + \sum\limits_{k = 1}^\infty  {{{\left( { - 1} \right)}^k}\left( {\frac{1}{{a - k}} + \frac{1}{{a + k}}} \right)}=\frac{\pi }{{\sin \pi a}}} $$ Where $0 < a = \dfrac{n+1}{m} < 1$ The infinite series is equal to $$\int\limits_{ - \infty }^\infty  {\frac{{{e^{at}}}}{{{e^t} + 1}}dt} $$ To get to the result, I split the integral at $x=0$ and use the convergent series in $(0,\infty)$ and $(-\infty,0)$ respectively: $$\frac{1}{{1 + {e^t}}} = \sum\limits_{k = 0}^\infty  {{{\left( { - 1} \right)}^k}{e^{ - \left( {k + 1} \right)t}}} $$ $$\frac{1}{{1 + {e^t}}} = \sum\limits_{k = 0}^\infty  {{{\left( { - 1} \right)}^k}{e^{kt}}} $$ Since $0 < a < 1$ $$\eqalign{   & \mathop {\lim }\limits_{t \to 0} \frac{{{e^{\left( {k + a} \right)t}}}}{{k + a}} - \mathop {\lim }\limits_{t \to  - \infty } \frac{{{e^{\left( {k + a} \right)t}}}}{{k + a}} = \frac{1}{{k + a}}  \cr    & \mathop {\lim }\limits_{t \to \infty } \frac{{{e^{\left( {a - k - 1} \right)t}}}}{{k + a}} - \mathop {\lim }\limits_{t \to 0} \frac{{{e^{\left( {a - k - 1} \right)t}}}}{{k + a}} =  - \frac{1}{{a - \left( {k + 1} \right)}} \cr} $$ A change in the indices will give the desired series. Although I don't mind direct solutions from tables and other sources, I prefer an elaborated answer. Here's the solution in terms of $\psi(x)$. By separating even and odd indices we can get $$\eqalign{   & \sum\limits_{k = 0}^\infty  {\frac{{{{\left( { - 1} \right)}^k}}}{{a + k}}}  = \sum\limits_{k = 0}^\infty  {\frac{1}{{a + 2k}}}  - \sum\limits_{k = 0}^\infty  {\frac{1}{{a + 2k + 1}}}   \cr    & \sum\limits_{k = 0}^\infty  {\frac{{{{\left( { - 1} \right)}^k}}}{{a - k}}}  = \sum\limits_{k = 0}^\infty  {\frac{1}{{a - 2k}}}  - \sum\limits_{k = 0}^\infty  {\frac{1}{{a - 2k - 1}}}  \cr} $$ which gives $$\sum\limits_{k = 0}^\infty  {\frac{{{{\left( { - 1} \right)}^k}}}{{a + k}}}  = \frac{1}{2}\psi \left( {\frac{{a + 1}}{2}} \right) - \frac{1}{2}\psi \left( {\frac{a}{2}} \right)$$ $$\sum\limits_{k = 0}^\infty  {\frac{{{{\left( { - 1} \right)}^k}}}{{a - k}}}  = \frac{1}{2}\psi \left( {1 - \frac{a}{2}} \right) - \frac{1}{2}\psi \left( {1 - \frac{{a + 1}}{2}} \right) + \frac{1}{a}$$ Then $$\eqalign{   & \sum\limits_{k =  - \infty }^\infty  {\frac{{{{\left( { - 1} \right)}^k}}}{{a + k}}}  = \sum\limits_{k = 0}^\infty  {\frac{{{{\left( { - 1} \right)}^k}}}{{a + k}}}  + \sum\limits_{k = 0}^\infty  {\frac{{{{\left( { - 1} \right)}^k}}}{{a - k}}}  - \frac{1}{a} =   \cr    &  = \left\{ {\frac{1}{2}\psi \left( {1 - \frac{a}{2}} \right) - \frac{1}{2}\psi \left( {\frac{a}{2}} \right)} \right\} - \left\{ {\frac{1}{2}\psi \left( {1 - \frac{{a + 1}}{2}} \right) - \frac{1}{2}\psi \left( {\frac{{a + 1}}{2}} \right)} \right\} \cr} $$ But using the reflection formula one has $$\eqalign{   & \frac{1}{2}\psi \left( {1 - \frac{a}{2}} \right) - \frac{1}{2}\psi \left( {\frac{a}{2}} \right) = \frac{\pi }{2}\cot \frac{{\pi a}}{2}  \cr    & \frac{1}{2}\psi \left( {1 - \frac{{a + 1}}{2}} \right) - \frac{1}{2}\psi \left( {\frac{{a + 1}}{2}} \right) = \frac{\pi }{2}\cot \frac{{\pi \left( {a + 1} \right)}}{2} =  - \frac{\pi }{2}\tan \frac{{\pi a}}{2} \cr} $$ So the series become $$\eqalign{   & \sum\limits_{k =  - \infty }^\infty  {\frac{{{{\left( { - 1} \right)}^k}}}{{a + k}}}  = \frac{\pi }{2}\left\{ {\cot \frac{{\pi a}}{2} + \tan \frac{{\pi a}}{2}} \right\}  \cr    & \sum\limits_{k =  - \infty }^\infty  {\frac{{{{\left( { - 1} \right)}^k}}}{{a + k}}}  = \pi \csc \pi a \cr} $$ The last being an application of a trigonometric identity.",,"['calculus', 'sequences-and-series', 'trigonometry', 'special-functions']"
5,Improper Integral $\int_0^1\left(\left\{\frac1x\right\}-\frac12\right)\frac{\log(x)}xdx$,Improper Integral,\int_0^1\left(\left\{\frac1x\right\}-\frac12\right)\frac{\log(x)}xdx,My initial question was to find if this integral  $$ \int_0^1 \left(\left\{\frac 1x\right\}-\frac12\right)\frac{\log(x)}{x}dx$$ is convergent or divergent. ($\left\{\frac 1x\right\}$ is the fractional part of $\frac 1x$ ). My try :: \begin{align}\int_0^1\left(\left\{\frac 1x\right\}-\frac 12\right)\frac{\log(x)}{x} dx & =-\int_1^\infty (\left\{y\right\}-1/2)\frac{\log(y)}{y} dy \\ & = \sum_{m=1}^{\infty} \int_{m}^{m+1} (\left\{y\right\}-1/2)\frac{\log(y)}{y} dx \\ & = \frac14\sum_{m=1}^{\infty} \left(\log^2 (m+1)+\log^2(m)-2\int_0^1\log^2(x+m)  dx \right) \\ &= ... \end{align} Finally the integral is convergent since the series obtained is convergent. The curious thing is that Mathematica returns $0.\times 10^{-2}$ by numerical integration. Then my question is: Is this integral equal to zero? Thank you for your help.,My initial question was to find if this integral  $$ \int_0^1 \left(\left\{\frac 1x\right\}-\frac12\right)\frac{\log(x)}{x}dx$$ is convergent or divergent. ($\left\{\frac 1x\right\}$ is the fractional part of $\frac 1x$ ). My try :: \begin{align}\int_0^1\left(\left\{\frac 1x\right\}-\frac 12\right)\frac{\log(x)}{x} dx & =-\int_1^\infty (\left\{y\right\}-1/2)\frac{\log(y)}{y} dy \\ & = \sum_{m=1}^{\infty} \int_{m}^{m+1} (\left\{y\right\}-1/2)\frac{\log(y)}{y} dx \\ & = \frac14\sum_{m=1}^{\infty} \left(\log^2 (m+1)+\log^2(m)-2\int_0^1\log^2(x+m)  dx \right) \\ &= ... \end{align} Finally the integral is convergent since the series obtained is convergent. The curious thing is that Mathematica returns $0.\times 10^{-2}$ by numerical integration. Then my question is: Is this integral equal to zero? Thank you for your help.,,"['calculus', 'integration', 'sequences-and-series', 'definite-integrals', 'closed-form']"
6,"Is there a ""greatest function"" that converges?","Is there a ""greatest function"" that converges?",,"We just hit convergence tests in calculus, and learned that $\sum_{n=1}^{\infty} \frac{1}{n^p}$ converges for all $p \gt 1$ . I thought that this was sort of a ""barrier"" between what converges and what diverges. Specifically, that setting $a_n=\frac{1}{n^{1+\epsilon}}$ is sort of the ""greatest function"" (I'll make this precise later) for which $\sum a_n$ converges. But, I did realize that there are functions that dominate $\frac{1}{n^{1+\epsilon}}$ but not $\frac1n$ , such as $\frac{1}{n\log(n)}$ . Now, the sum of that specific example diverges, but it got me wondering about whether $\frac{1}{n}$ is truly the ""boundary"". So, this leads me to two questions. 1) Is there a function $f$ that dominates $\frac{1}{n^p}$ for all $p>1$ , meaning: $$\lim_{x\to\infty} \frac{f(x)}{\frac{1}{x^p}}=\infty$$ Such that: $$\sum_{n=1}^\infty f(n)$$ converges? 2) If so, up to a constant is there a function $g$ such that $\sum_{n=1}^\infty g(n)$ converges, such that $g$ dominates $f$ for all other functions $f$ such that $\sum_{n=1}^\infty f(n)$ converges? I'm just a freshman in high school so I apologize if this is a stupid question.","We just hit convergence tests in calculus, and learned that converges for all . I thought that this was sort of a ""barrier"" between what converges and what diverges. Specifically, that setting is sort of the ""greatest function"" (I'll make this precise later) for which converges. But, I did realize that there are functions that dominate but not , such as . Now, the sum of that specific example diverges, but it got me wondering about whether is truly the ""boundary"". So, this leads me to two questions. 1) Is there a function that dominates for all , meaning: Such that: converges? 2) If so, up to a constant is there a function such that converges, such that dominates for all other functions such that converges? I'm just a freshman in high school so I apologize if this is a stupid question.",\sum_{n=1}^{\infty} \frac{1}{n^p} p \gt 1 a_n=\frac{1}{n^{1+\epsilon}} \sum a_n \frac{1}{n^{1+\epsilon}} \frac1n \frac{1}{n\log(n)} \frac{1}{n} f \frac{1}{n^p} p>1 \lim_{x\to\infty} \frac{f(x)}{\frac{1}{x^p}}=\infty \sum_{n=1}^\infty f(n) g \sum_{n=1}^\infty g(n) g f f \sum_{n=1}^\infty f(n),"['calculus', 'sequences-and-series', 'convergence-divergence']"
7,"Prove: $\int_0^\infty \sin (x^2) \, dx$ converges.",Prove:  converges.,"\int_0^\infty \sin (x^2) \, dx","$\sin x^2$ does not converge as $x \to \infty$, yet its integral from $0$ to $\infty$ does. I'm trying to understand why and would like some help in working towards a formal proof.","$\sin x^2$ does not converge as $x \to \infty$, yet its integral from $0$ to $\infty$ does. I'm trying to understand why and would like some help in working towards a formal proof.",,"['calculus', 'integration', 'improper-integrals']"
8,Why doesn't L'Hôpital's rule work in this case?,Why doesn't L'Hôpital's rule work in this case?,,I have a very simple question. Suppose I want to evaluate this limit: $$\lim_{x\to \infty} \frac{x}{x-\sin x}$$ It is easy to evaluate this limit using the Squeeze theorem (the answer is $1$). But here both the numerator and the denominator are going to infinity as $x\to \infty$ so I tried using L'Hospital's rule: $$\lim_{x\to \infty} \frac{x}{x-\sin x}=\lim_{x\to \infty} \frac{1}{1-\cos x}$$ However there's no finite $L$ such that $$\lim_{x\to \infty} \frac{1}{1-\cos x}=L$$ which is a contradiction. I don't understand why in this case L'Hopital's rule doesn't work. Both the numerator and the denominator are differentiable everywhere and both are tending to infinity - which is all we need to use this rule.,I have a very simple question. Suppose I want to evaluate this limit: $$\lim_{x\to \infty} \frac{x}{x-\sin x}$$ It is easy to evaluate this limit using the Squeeze theorem (the answer is $1$). But here both the numerator and the denominator are going to infinity as $x\to \infty$ so I tried using L'Hospital's rule: $$\lim_{x\to \infty} \frac{x}{x-\sin x}=\lim_{x\to \infty} \frac{1}{1-\cos x}$$ However there's no finite $L$ such that $$\lim_{x\to \infty} \frac{1}{1-\cos x}=L$$ which is a contradiction. I don't understand why in this case L'Hopital's rule doesn't work. Both the numerator and the denominator are differentiable everywhere and both are tending to infinity - which is all we need to use this rule.,,"['calculus', 'limits']"
9,explaining the derivative of $x^x$,explaining the derivative of,x^x,"You set the following exercise to your calculus class: Q1. Differentiate $y(x) = x^x$. A student submits the following solution: Let $g(a)=a^x$ and $f(x)=x$.  Then $y(x) = g(f(x))$, so by the chain rule, $y'(x) = f'(x) g'(f(x)) = 1 \cdot x \cdot (x^{x-1}) = x^x$. How would you explain to the student why their solution is incorrect? To be clear, I know why this is wrong but am interested in good ways to explain it to undergraduate or high school students. In this question someone has problems differentiating $x^x$, but they didn't take the approach of my hypothetical student.","You set the following exercise to your calculus class: Q1. Differentiate $y(x) = x^x$. A student submits the following solution: Let $g(a)=a^x$ and $f(x)=x$.  Then $y(x) = g(f(x))$, so by the chain rule, $y'(x) = f'(x) g'(f(x)) = 1 \cdot x \cdot (x^{x-1}) = x^x$. How would you explain to the student why their solution is incorrect? To be clear, I know why this is wrong but am interested in good ways to explain it to undergraduate or high school students. In this question someone has problems differentiating $x^x$, but they didn't take the approach of my hypothetical student.",,"['calculus', 'education', 'fake-proofs']"
10,"What is $\, _4F_3\left(1,1,1,\frac{3}{2};\frac{5}{2},\frac{5}{2},\frac{5}{2};1\right)$?",What is ?,"\, _4F_3\left(1,1,1,\frac{3}{2};\frac{5}{2},\frac{5}{2},\frac{5}{2};1\right)","I have been trying to evaluate the series $$\, _4F_3\left(1,1,1,\frac{3}{2};\frac{5}{2},\frac{5}{2},\frac{5}{2};1\right) = 1.133928715547935...$$ using integration techniques, and I was wondering if there is any simple way of finding a closed-form evaluation of this hypergeometric series.  What is a closed-form expression for the above series?","I have been trying to evaluate the series $$\, _4F_3\left(1,1,1,\frac{3}{2};\frac{5}{2},\frac{5}{2},\frac{5}{2};1\right) = 1.133928715547935...$$ using integration techniques, and I was wondering if there is any simple way of finding a closed-form evaluation of this hypergeometric series.  What is a closed-form expression for the above series?",,"['calculus', 'sequences-and-series', 'closed-form', 'hypergeometric-function']"
11,Which derivatives are eventually periodic?,Which derivatives are eventually periodic?,,"Which derivatives are eventually periodic? I have noticed that is $a_{n}=f^{(n)}(x)$, the sequence $a_{n}$ becomes eventually periodic for a multitude of $f(x)$. If $f(x)$ was a polynomial, and $\operatorname{deg}(f(x))=n$, note that $f^{(n)}(x)=C$ if $C$ is a constant. This implies that  $f^{(n+i)}(x)=0$ for every $i$ which is a natural number. If $f(x)=e^x$, note that $f(x)=f'(x)$. This implies that $f^{(n)}(x)=e^x$ for every natural number $n$. If $f(x)=\sin(x)$, note that $f'(x)=\cos(x), f''(x)=-\sin(x), f'''(x)=-\cos(x), f''''(x)=\sin(x)$. This implies that $f^{(4n)}(x)=f(x)$ for every natural number $n$. In a similar way, if $f(x)=\cos(x)$, $f^{(4n)}(x)=f(x)$ for every natural number $n$. These appear to be the only functions whose derivatives become eventually periodic. What are other functions whose derivatives become eventually periodic? What is known about them? Any help would be appreciated.","Which derivatives are eventually periodic? I have noticed that is $a_{n}=f^{(n)}(x)$, the sequence $a_{n}$ becomes eventually periodic for a multitude of $f(x)$. If $f(x)$ was a polynomial, and $\operatorname{deg}(f(x))=n$, note that $f^{(n)}(x)=C$ if $C$ is a constant. This implies that  $f^{(n+i)}(x)=0$ for every $i$ which is a natural number. If $f(x)=e^x$, note that $f(x)=f'(x)$. This implies that $f^{(n)}(x)=e^x$ for every natural number $n$. If $f(x)=\sin(x)$, note that $f'(x)=\cos(x), f''(x)=-\sin(x), f'''(x)=-\cos(x), f''''(x)=\sin(x)$. This implies that $f^{(4n)}(x)=f(x)$ for every natural number $n$. In a similar way, if $f(x)=\cos(x)$, $f^{(4n)}(x)=f(x)$ for every natural number $n$. These appear to be the only functions whose derivatives become eventually periodic. What are other functions whose derivatives become eventually periodic? What is known about them? Any help would be appreciated.",,"['calculus', 'derivatives']"
12,Derivative of max function,Derivative of max function,,"I was just wondering what the derivative of $f(x) = \max(0,1-x)^{2}$ would be. What technique do you use to determine this derivative?",I was just wondering what the derivative of would be. What technique do you use to determine this derivative?,"f(x) = \max(0,1-x)^{2}",['calculus']
13,Rain droplets falling on a table,Rain droplets falling on a table,,"Suppose you have a circular table of radius $R$. This table has been left outside, and it begins to rain at a constant rate of one droplet per second. The drops, which can be considered points as they fall, can only land in such a way such that they impact the surface of the table. Once they strike the table, they form a puddle of radius $r$, centered at their point of impact. What is the expected number of droplets it takes to cover the table in water? The answer should be left in terms of $R$ and $r$. However, if you can simulate this, while changing both $r$ and $R$ so that some regression might be applied to find the approximate relation, that would be nice as well. I'd really like some intuition as to how the two are related. I have tried decomposing the problem by considering only the 1-dimensional case with line segments, but even its solution has eluded me. A potential starting point could be the discrete case of marbles falling into buckets. The following edit was suggested by Jbeuh : A more formal restatement of the problem : Consider a sequence $X_1,\ldots,X_n$ of independent random variables following a uniform distribution on a disk $D$ of radius $R$. For each $i$, let $C_i$ be the disk centered at $X_i$ with radius $r$. Let $Y$ be the first $i$ such that the disks $C_1,\ldots,C_i$ forms a cover of $D$. What is the expected value of $Y$?","Suppose you have a circular table of radius $R$. This table has been left outside, and it begins to rain at a constant rate of one droplet per second. The drops, which can be considered points as they fall, can only land in such a way such that they impact the surface of the table. Once they strike the table, they form a puddle of radius $r$, centered at their point of impact. What is the expected number of droplets it takes to cover the table in water? The answer should be left in terms of $R$ and $r$. However, if you can simulate this, while changing both $r$ and $R$ so that some regression might be applied to find the approximate relation, that would be nice as well. I'd really like some intuition as to how the two are related. I have tried decomposing the problem by considering only the 1-dimensional case with line segments, but even its solution has eluded me. A potential starting point could be the discrete case of marbles falling into buckets. The following edit was suggested by Jbeuh : A more formal restatement of the problem : Consider a sequence $X_1,\ldots,X_n$ of independent random variables following a uniform distribution on a disk $D$ of radius $R$. For each $i$, let $C_i$ be the disk centered at $X_i$ with radius $r$. Let $Y$ be the first $i$ such that the disks $C_1,\ldots,C_i$ forms a cover of $D$. What is the expected value of $Y$?",,"['calculus', 'probability', 'integration', 'probability-theory', 'geometric-probability']"
14,Prove that $\int_0^1{\left\lfloor{1\over x}\right\rfloor}^{-1}\!dx={1\over2^2}+{1\over3^2}+{1\over4^2}+\cdots.$,Prove that,\int_0^1{\left\lfloor{1\over x}\right\rfloor}^{-1}\!dx={1\over2^2}+{1\over3^2}+{1\over4^2}+\cdots.,"Question. Let $$ f(x)=\!\left\{\,\,\, \begin{array}{ccc}  \displaystyle{\left\lfloor{1\over x}\right\rfloor}^{-1}_{\hphantom{|_|}}&\text{if} & 0\lt x\le 1, \\ & \\ 0^{\hphantom{|^|}} &\text{if}  & x=0. \end{array}\right. $$ Is f(x) Riemann integrable on $[0,1]$? If it is Riemann integrable, then what is the value of the integral $\,\int_0^1{\left\lfloor{1\over x}\right\rfloor}^{-1}dx$? An attempt: Since $f$ is increasing, non-negative and bounded the integral does exist. Choosing the partition $P=\big\{0,\frac{1}{n},\frac{1}{n-1},...,1\big\}$, we have the following  upper and lower sums  $$ U(f,P)=\sum_{i=1}^{n-1}{1\over n-i}\left [ {1\over n-i}-{1\over n-i+1}\right] - {1\over n^2},\\L(f,P)=\sum_{i=1}^{n-1}{1\over n-i+1}\left[{1\over n-i} - {1\over n-i+1} \right]. $$  Simplifying we obtain $$ U(f,P)= \sum_{i=1}^n{1\over i^2}+ {1\over n} -1, \quad L(f,P)= 2-{1\over n}-\sum_{i=1}^n{1\over n^2}. $$ As $n\to\infty$, $U(f,P)\to{\pi^2\over 6}-1$ and $L(f,P)\to2-{\pi^2\over 6}$. Therefore $2-{\pi^2\over 6}\le\int_0^1f(x)\,dx\le{\pi^2\over 6}-1$. Direct calculation using MATLAB shows $\int_0^1f(x)\,dx={\pi^2\over 6}-1$.","Question. Let $$ f(x)=\!\left\{\,\,\, \begin{array}{ccc}  \displaystyle{\left\lfloor{1\over x}\right\rfloor}^{-1}_{\hphantom{|_|}}&\text{if} & 0\lt x\le 1, \\ & \\ 0^{\hphantom{|^|}} &\text{if}  & x=0. \end{array}\right. $$ Is f(x) Riemann integrable on $[0,1]$? If it is Riemann integrable, then what is the value of the integral $\,\int_0^1{\left\lfloor{1\over x}\right\rfloor}^{-1}dx$? An attempt: Since $f$ is increasing, non-negative and bounded the integral does exist. Choosing the partition $P=\big\{0,\frac{1}{n},\frac{1}{n-1},...,1\big\}$, we have the following  upper and lower sums  $$ U(f,P)=\sum_{i=1}^{n-1}{1\over n-i}\left [ {1\over n-i}-{1\over n-i+1}\right] - {1\over n^2},\\L(f,P)=\sum_{i=1}^{n-1}{1\over n-i+1}\left[{1\over n-i} - {1\over n-i+1} \right]. $$  Simplifying we obtain $$ U(f,P)= \sum_{i=1}^n{1\over i^2}+ {1\over n} -1, \quad L(f,P)= 2-{1\over n}-\sum_{i=1}^n{1\over n^2}. $$ As $n\to\infty$, $U(f,P)\to{\pi^2\over 6}-1$ and $L(f,P)\to2-{\pi^2\over 6}$. Therefore $2-{\pi^2\over 6}\le\int_0^1f(x)\,dx\le{\pi^2\over 6}-1$. Direct calculation using MATLAB shows $\int_0^1f(x)\,dx={\pi^2\over 6}-1$.",,"['calculus', 'integration', 'sequences-and-series', 'definite-integrals', 'improper-integrals']"
15,Finding the sum- $x+x^{2}+x^{4}+x^{8}+x^{16}\cdots$,Finding the sum-,x+x^{2}+x^{4}+x^{8}+x^{16}\cdots,If $S = x+x^{2}+x^{4}+x^{8}+x^{16}\cdots$ Find S. Note:This is not a GP series .The powers are in GP. My Attempts so far: 1)If $S(x)=x+x^{2}+x^{4}+x^{8}+x^{16}\cdots$ Then $$S(x)-S(x^{2})=x$$ 2)I tried finding $S^{2}$ and higher powers of S to find some kind of recursive relation. 3)When all failed I even tried differentiating and integrating S.Obviously that was of no good either. Could anyone give me a hint to solve this?Thanks!,If $S = x+x^{2}+x^{4}+x^{8}+x^{16}\cdots$ Find S. Note:This is not a GP series .The powers are in GP. My Attempts so far: 1)If $S(x)=x+x^{2}+x^{4}+x^{8}+x^{16}\cdots$ Then $$S(x)-S(x^{2})=x$$ 2)I tried finding $S^{2}$ and higher powers of S to find some kind of recursive relation. 3)When all failed I even tried differentiating and integrating S.Obviously that was of no good either. Could anyone give me a hint to solve this?Thanks!,,"['calculus', 'sequences-and-series', 'recurrence-relations', 'summation']"
16,How to know whether Lagrange multipliers gives maximum or minimum?,How to know whether Lagrange multipliers gives maximum or minimum?,,"My book tells me that of the solutions to the Lagrange system, the smallest is the minimum of the function given the constraint and the largest is the maximum given that one actually exists. But what if we only have one point as a solution? How to know whether Lagrange multipliers gives maximum or minimum?","My book tells me that of the solutions to the Lagrange system, the smallest is the minimum of the function given the constraint and the largest is the maximum given that one actually exists. But what if we only have one point as a solution? How to know whether Lagrange multipliers gives maximum or minimum?",,"['calculus', 'multivariable-calculus', 'optimization', 'lagrange-multiplier']"
17,"Is ""imposing"" one function onto another ever used in mathematics?","Is ""imposing"" one function onto another ever used in mathematics?",,"First of all, let me define what I mean by ""imposing,"" and let me clarify that I've only studied this operation in 2D Euclidean space. Now then, to impose one function onto another, you need two things: A function upon which to impose, called the receiver . A function to impose, called the imposer . Now, let me first explain the concept generally. The idea is that, rather than graphing some function with respect to the x-axis, we treat the receiver as the x-axis, graphing the imposer with respect to receiver . So, what do I mean by ""graphing some function with respect to the x-axis?"" Well, first we'll let $p_0$ be the point on the x-axis at some $x$, and we'll let $l$ be the line which is normal to the x-axis at $p_0$. It should be clear that $p_0=(x,0)$ and that $l$ is a vertical line which passes through $p_0$. Then, for some function $g$, let $p_1$ be the point on $l$ whose distance is equal to $g(x)$. It should be clear that $p_1=(x,g(x))$ since the distance from $(x,0)$ to $(x,g(x))$ is equal to $g(x)$. If you do the previous procedure for all $x$ and plot every $p_1$ on a graph, you will have successfully graphed $g$ with respect to the x-axis. So, to reiterate my second paragraph, the idea is that we can graph any function with respect to some other function. The way we do this is by following the same procedure we used in the last paragraph. However, there are two main differences: Instead of letting $p_0$ be the point on the x-axis at some $x$, we let $p_0$ be the point on some parametric function $f(t)$, the receiver , for some $t$. Instead of letting $l$ be the line which is normal to the x-axis at $p_0$, we let $l$ be the line which is normal to $f$, the receiver , at $p_0$. For example, this is $g(t)=cos(t)$, the imposer , imposed upon $f(t)=(t,a \cdot sin(t))$, the receiver , where $a$ is simply a real value which oscillates between $-1$ and $1$ with time. Basically, $a$ is the reason functions below are moving. The black function which resembles a standing wave , as I said before, is an oscillating sine function, and it is also the receiver . The blue function which, if you look closely, occasionally looks like a cosine function is the function resulting from imposing $g(t)$ onto $f(t)$. The green line segments are to illustrate the act of finding the point on the normal of $f(t)$ at $t$ with a distance of $g(t)$, like we covered in the above paragraphs. If you're interested, here's the raw math to impose one function onto another: Given some parametric equation $f:f(t) = (x(t),y(t))$ upon which we wish to impose some function $g(t)$: $$ Let\;h(t)=\frac{\frac{d}{dt}(y(t))}{\frac{d}{dt}(x(t))}=f'(t) $$ $$ Let\;j(t)=tan^{-1}(h(t))\pm\frac{\pi}{2} $$ Then $g$ imposed upon $f$ becomes the following parametric equation, in terms of $t$. $$ (g(t)cos(j(t))+x(t),g(t)sin(j(t))+y(t)) $$ I feel as though I should clarify now that, for most $f$ and $g$, this operation will produce two resultant functions. This fact is a result of the way I have defined this operation. That is, we are looking for any point $p_1$ on the normal line of $f$ at $p_0$ such that the distance between $p_0$ and $p_1$ is equal to $g(t)$. Put more simply, we're looking on a line for a point which is a specific distance away from another point on the line. We already know that, for any point $p$ on a line $l$, there will always be exactly two points on $l$ with a distance $\delta$ away from $p$, for all $\delta > 0$. This fact is the reason for which we are adding or subtracting $\frac{\pi}{2}$ in $j$. The only case I can think of wherein this operation does not produce two resultant functions is when $g(t) = 0$, as each resulting function will be exactly equal to the parametric $f(t)$; however, there very well may be more. So, ignoring any incorrect notation or terminology I might have used, is this type of transformation used anywhere in mathematics? If so, could you show me where I could get some more information on it?","First of all, let me define what I mean by ""imposing,"" and let me clarify that I've only studied this operation in 2D Euclidean space. Now then, to impose one function onto another, you need two things: A function upon which to impose, called the receiver . A function to impose, called the imposer . Now, let me first explain the concept generally. The idea is that, rather than graphing some function with respect to the x-axis, we treat the receiver as the x-axis, graphing the imposer with respect to receiver . So, what do I mean by ""graphing some function with respect to the x-axis?"" Well, first we'll let $p_0$ be the point on the x-axis at some $x$, and we'll let $l$ be the line which is normal to the x-axis at $p_0$. It should be clear that $p_0=(x,0)$ and that $l$ is a vertical line which passes through $p_0$. Then, for some function $g$, let $p_1$ be the point on $l$ whose distance is equal to $g(x)$. It should be clear that $p_1=(x,g(x))$ since the distance from $(x,0)$ to $(x,g(x))$ is equal to $g(x)$. If you do the previous procedure for all $x$ and plot every $p_1$ on a graph, you will have successfully graphed $g$ with respect to the x-axis. So, to reiterate my second paragraph, the idea is that we can graph any function with respect to some other function. The way we do this is by following the same procedure we used in the last paragraph. However, there are two main differences: Instead of letting $p_0$ be the point on the x-axis at some $x$, we let $p_0$ be the point on some parametric function $f(t)$, the receiver , for some $t$. Instead of letting $l$ be the line which is normal to the x-axis at $p_0$, we let $l$ be the line which is normal to $f$, the receiver , at $p_0$. For example, this is $g(t)=cos(t)$, the imposer , imposed upon $f(t)=(t,a \cdot sin(t))$, the receiver , where $a$ is simply a real value which oscillates between $-1$ and $1$ with time. Basically, $a$ is the reason functions below are moving. The black function which resembles a standing wave , as I said before, is an oscillating sine function, and it is also the receiver . The blue function which, if you look closely, occasionally looks like a cosine function is the function resulting from imposing $g(t)$ onto $f(t)$. The green line segments are to illustrate the act of finding the point on the normal of $f(t)$ at $t$ with a distance of $g(t)$, like we covered in the above paragraphs. If you're interested, here's the raw math to impose one function onto another: Given some parametric equation $f:f(t) = (x(t),y(t))$ upon which we wish to impose some function $g(t)$: $$ Let\;h(t)=\frac{\frac{d}{dt}(y(t))}{\frac{d}{dt}(x(t))}=f'(t) $$ $$ Let\;j(t)=tan^{-1}(h(t))\pm\frac{\pi}{2} $$ Then $g$ imposed upon $f$ becomes the following parametric equation, in terms of $t$. $$ (g(t)cos(j(t))+x(t),g(t)sin(j(t))+y(t)) $$ I feel as though I should clarify now that, for most $f$ and $g$, this operation will produce two resultant functions. This fact is a result of the way I have defined this operation. That is, we are looking for any point $p_1$ on the normal line of $f$ at $p_0$ such that the distance between $p_0$ and $p_1$ is equal to $g(t)$. Put more simply, we're looking on a line for a point which is a specific distance away from another point on the line. We already know that, for any point $p$ on a line $l$, there will always be exactly two points on $l$ with a distance $\delta$ away from $p$, for all $\delta > 0$. This fact is the reason for which we are adding or subtracting $\frac{\pi}{2}$ in $j$. The only case I can think of wherein this operation does not produce two resultant functions is when $g(t) = 0$, as each resulting function will be exactly equal to the parametric $f(t)$; however, there very well may be more. So, ignoring any incorrect notation or terminology I might have used, is this type of transformation used anywhere in mathematics? If so, could you show me where I could get some more information on it?",,"['calculus', 'graphing-functions', 'parametric']"
18,Is there a naive proof that $x - \frac{x^3}{3!} + \frac{x^5}{5!} - \ldots$ has period $2\pi$?,Is there a naive proof that  has period ?,x - \frac{x^3}{3!} + \frac{x^5}{5!} - \ldots 2\pi,"I recently visited the far away land of Polynomia.  The mathematicians in Polynomia are quite sophisticated algebraists: they know a lot about polynomials and their associated machinery - rings, fields, algebraic geometry, etc.  But they aren't very good at analysis; they don't know much about differential equations and don't like sophisticated estimates.  They're pretty good with the theory of power series because it involves taking limits of polynomials (which they love), and so they've managed to figure out at least some complex analysis. In my recent visit I got into a discussion about the power series $$f(x) = x - \frac{x^3}{3!} + \frac{x^5}{5!} - \ldots$$ They knew how to prove that this power series converges everywhere on the complex plane, but they were astonished when I told them that $f$ is periodic with period $2\pi$.  (They are aware that the polynomial equation $x^2 + y^2 = 1$ defines a curve in $\mathbb{R}^2$, and they define $2\pi$ to be its arclength.)  You see, since they don't really like differential equations they don't know about functions like $\sin x$, $e^x$, etc. So the Polynomians were pretty incredulous about my claim and they demanded that I prove it.  The proofs I know rely heavily on methods like path integrals of transcendental functions, and their eyes just glazed over.  They're looking for some property of the partial sums of $f$ which, in the limit, guarantees that $f$ is periodic with period $2 \pi$.  Circles are almost certainly going to have to enter into it and I can probably convince to accept path integrals of polynomials along a circle, but the more algebraic the argument the better.  Can anyone help?","I recently visited the far away land of Polynomia.  The mathematicians in Polynomia are quite sophisticated algebraists: they know a lot about polynomials and their associated machinery - rings, fields, algebraic geometry, etc.  But they aren't very good at analysis; they don't know much about differential equations and don't like sophisticated estimates.  They're pretty good with the theory of power series because it involves taking limits of polynomials (which they love), and so they've managed to figure out at least some complex analysis. In my recent visit I got into a discussion about the power series $$f(x) = x - \frac{x^3}{3!} + \frac{x^5}{5!} - \ldots$$ They knew how to prove that this power series converges everywhere on the complex plane, but they were astonished when I told them that $f$ is periodic with period $2\pi$.  (They are aware that the polynomial equation $x^2 + y^2 = 1$ defines a curve in $\mathbb{R}^2$, and they define $2\pi$ to be its arclength.)  You see, since they don't really like differential equations they don't know about functions like $\sin x$, $e^x$, etc. So the Polynomians were pretty incredulous about my claim and they demanded that I prove it.  The proofs I know rely heavily on methods like path integrals of transcendental functions, and their eyes just glazed over.  They're looking for some property of the partial sums of $f$ which, in the limit, guarantees that $f$ is periodic with period $2 \pi$.  Circles are almost certainly going to have to enter into it and I can probably convince to accept path integrals of polynomials along a circle, but the more algebraic the argument the better.  Can anyone help?",,"['calculus', 'sequences-and-series']"
19,Is there a function with the property$ f(n)=f^{(n)}(a)$,Is there a function with the property, f(n)=f^{(n)}(a),"Is there a not identically zero, real-analytic function $f\colon\mathbb R\to\mathbb R$, which satisfies $$f(n)=f^{(n)}(a),n\in\mathbb N \text{ or }\mathbb N^+?$$ and $a\in \mathbb R$ I saw a special case when $a=0$ I try to solve it by : $$f(x)=e^{cx}$$ $$f(n)=e^{nc}$$ $$f^{(n)}(x)=c^ne^{cx}$$ $$f^{(n)}(a)=c^ne^{ca}$$ so $$e^{nc}=c^ne^{ca}$$ so $$c=\frac{nW(\frac{a-n}{n})}{a-n}$$ the problem is we always see  n with c  but the special case when a=0 give $$c=\frac{nW(\frac{0-n}{n})}{0-n}$$ $$c=\frac{W(\frac{-1}{1})}{-1}=-W(-1)$$ I think there is no solution when $a\neq 0$ may be there is another function can solve it Is there any solution in general? thanks for all","Is there a not identically zero, real-analytic function $f\colon\mathbb R\to\mathbb R$, which satisfies $$f(n)=f^{(n)}(a),n\in\mathbb N \text{ or }\mathbb N^+?$$ and $a\in \mathbb R$ I saw a special case when $a=0$ I try to solve it by : $$f(x)=e^{cx}$$ $$f(n)=e^{nc}$$ $$f^{(n)}(x)=c^ne^{cx}$$ $$f^{(n)}(a)=c^ne^{ca}$$ so $$e^{nc}=c^ne^{ca}$$ so $$c=\frac{nW(\frac{a-n}{n})}{a-n}$$ the problem is we always see  n with c  but the special case when a=0 give $$c=\frac{nW(\frac{0-n}{n})}{0-n}$$ $$c=\frac{W(\frac{-1}{1})}{-1}=-W(-1)$$ I think there is no solution when $a\neq 0$ may be there is another function can solve it Is there any solution in general? thanks for all",,['calculus']
20,Fun Geometric Series Puzzle,Fun Geometric Series Puzzle,,"I recently was reminded of a puzzle I solved in college and thought I'd give it a shot again. However, being distanced from college math, I am having a harder time remembering how I arrived at the solution. The problem is as follows: Imagine you are standing in the middle of an open field. You walk forward 16 feet, turn right and walk 8 feet, turn right and walk 4 feet, and so on. This continues indefinitely. When you finally reach an infinite number of turns, how far will you be from your original starting point (as the crow flies)? Generalized, here is what the problem looks like: I did manage to find the original solution that I came up with for the problem. However, I did not show my work so the process is lost. After attempting to resolve this without any luck, I thought I would toss this out to the community to solve as a fun puzzle. For reference, this is what I believe to be the solution: $$\frac{a}{\sqrt{r^2 + 1}}$$","I recently was reminded of a puzzle I solved in college and thought I'd give it a shot again. However, being distanced from college math, I am having a harder time remembering how I arrived at the solution. The problem is as follows: Imagine you are standing in the middle of an open field. You walk forward 16 feet, turn right and walk 8 feet, turn right and walk 4 feet, and so on. This continues indefinitely. When you finally reach an infinite number of turns, how far will you be from your original starting point (as the crow flies)? Generalized, here is what the problem looks like: I did manage to find the original solution that I came up with for the problem. However, I did not show my work so the process is lost. After attempting to resolve this without any luck, I thought I would toss this out to the community to solve as a fun puzzle. For reference, this is what I believe to be the solution: $$\frac{a}{\sqrt{r^2 + 1}}$$",,"['calculus', 'puzzle']"
21,Integrate square of the log-sine integral: $\int_0^{\frac{\pi}{2}}\ln^{2}(\sin(x))dx$,Integrate square of the log-sine integral:,\int_0^{\frac{\pi}{2}}\ln^{2}(\sin(x))dx,"$\displaystyle \int_{0}^{\frac{\pi}{2}} \ln(\sin(x))dx=-\frac{\pi}{2}\ln(2)$ is an integral that is common. But, how can we show $\displaystyle\int_{0}^{\frac{\pi}{2}}\ln^{2}(\sin(x))dx=\frac{{\pi}^{3}}{24}+\frac{\pi}{2}\ln^{2}(2)$?. Does anyone have any ideas on how to approach $\displaystyle\int_{0}^{\frac{\pi}{2}}\ln^{2}(\sin(x))dx$?. Thank you very much.","$\displaystyle \int_{0}^{\frac{\pi}{2}} \ln(\sin(x))dx=-\frac{\pi}{2}\ln(2)$ is an integral that is common. But, how can we show $\displaystyle\int_{0}^{\frac{\pi}{2}}\ln^{2}(\sin(x))dx=\frac{{\pi}^{3}}{24}+\frac{\pi}{2}\ln^{2}(2)$?. Does anyone have any ideas on how to approach $\displaystyle\int_{0}^{\frac{\pi}{2}}\ln^{2}(\sin(x))dx$?. Thank you very much.",,"['calculus', 'integration']"
22,Where does the relation $\nabla^2(1/r)=-4\pi\delta^3({\bf r})$ between Laplacian and Dirac delta function come from?,Where does the relation  between Laplacian and Dirac delta function come from?,\nabla^2(1/r)=-4\pi\delta^3({\bf r}),"It is often quoted in physics textbooks for finding the electric potential using Green's function that $$\nabla ^2 \left(\frac{1}{r}\right)=-4\pi\delta^3({\bf r}),$$ or more generally $$\nabla ^2 \left(\frac{1}{|| \vec x - \vec x'||}\right)=-4\pi\delta^3(\vec x - \vec x'),$$ where $\delta^3$ is the 3-dimensional Dirac delta distribution . However I don't understand how/where this comes from. Would anyone mind explaining?","It is often quoted in physics textbooks for finding the electric potential using Green's function that $$\nabla ^2 \left(\frac{1}{r}\right)=-4\pi\delta^3({\bf r}),$$ or more generally $$\nabla ^2 \left(\frac{1}{|| \vec x - \vec x'||}\right)=-4\pi\delta^3(\vec x - \vec x'),$$ where $\delta^3$ is the 3-dimensional Dirac delta distribution . However I don't understand how/where this comes from. Would anyone mind explaining?",,"['calculus', 'derivatives', 'distribution-theory']"
23,Why are integrals called integrals?,Why are integrals called integrals?,,"What is the historical background for this term? I cannot quite see what is integral about an integral, even if we go back to the viewing it as the area under a curve.  It seems to me a strange choice of word.","What is the historical background for this term? I cannot quite see what is integral about an integral, even if we go back to the viewing it as the area under a curve.  It seems to me a strange choice of word.",,"['calculus', 'integration', 'soft-question', 'terminology', 'math-history']"
24,Computing $ \int_0^\infty \frac{\log x}{\exp x} \ dx $ [duplicate],Computing  [duplicate], \int_0^\infty \frac{\log x}{\exp x} \ dx ,"This question already has answers here : Integral representation of Euler's constant (4 answers) Closed 7 years ago . I know that $$ \int_0^\infty \frac{\log x}{\exp x} = -\gamma $$ where $ \gamma $ is the Euler-Mascheroni constant, but I have no idea how to prove this. The series definition of $ \gamma $ leads me to believe that I should break the integrand into a series and interchange the summation and integration, but I can't think of a good series. The Maclaurin series of $ \ln x $ isn't applicable as the domain of $ x $ is not correct and I can't seem to manipulate the integrand so that such a Maclaurin series will work. Another thing I thought of was using $ x \mapsto \log u $ to get $ \int\limits_{-\infty}^\infty \frac{\log \log u}{u^2} \ du $ and use some sort of contour integration, but I can't see how that would work out either.","This question already has answers here : Integral representation of Euler's constant (4 answers) Closed 7 years ago . I know that $$ \int_0^\infty \frac{\log x}{\exp x} = -\gamma $$ where $ \gamma $ is the Euler-Mascheroni constant, but I have no idea how to prove this. The series definition of $ \gamma $ leads me to believe that I should break the integrand into a series and interchange the summation and integration, but I can't think of a good series. The Maclaurin series of $ \ln x $ isn't applicable as the domain of $ x $ is not correct and I can't seem to manipulate the integrand so that such a Maclaurin series will work. Another thing I thought of was using $ x \mapsto \log u $ to get $ \int\limits_{-\infty}^\infty \frac{\log \log u}{u^2} \ du $ and use some sort of contour integration, but I can't see how that would work out either.",,"['calculus', 'integration', 'closed-form']"
25,Addition is to Integration as Multiplication is to ______,Addition is to Integration as Multiplication is to ______,,"Addition is to Integration as Multiplication is to ______ ? Everyone knows that definite integration  is ""a way to sum continuum-many terms"" in a rough sense. Can we ""multiply continuum-many factors""  in a similar sense?","Addition is to Integration as Multiplication is to ______ ? Everyone knows that definite integration  is ""a way to sum continuum-many terms"" in a rough sense. Can we ""multiply continuum-many factors""  in a similar sense?",,"['calculus', 'analysis']"
26,When the integral of products is the product of integrals.,When the integral of products is the product of integrals.,,"I'm self-studying and was doing the following integral: $$I = \int \frac{e^{\frac{1}{x}+\tan^{-1}x}}{x^2+x^4} dx $$ I solved it fine by letting $ u = \frac{1}{x} + \tan^{-1}x$. My question is about an alternative method I saw in which it seems the product rule was not applied: $$ I = \int \left(\frac { e^{\frac{1}{x}}} {x^2}\right) \left( \frac{e^{\tan^{-1}x}}{x^2+1}\right) dx $$ $$ = \int \frac {e^{\frac{1}{x}}}{x^2} dx \cdot \int \frac{e^{\tan^{-1}x}}{x^2+1}dx$$ Completing the work following this step leads to the same solution as I originally found. It is this step that has confused me. I have checked using Wolfram and the two statements are equivalent but I do not understand why. Why are we able to write the integral of products as the product of integrals here, and not apply the product rule? Thanks in advance.","I'm self-studying and was doing the following integral: $$I = \int \frac{e^{\frac{1}{x}+\tan^{-1}x}}{x^2+x^4} dx $$ I solved it fine by letting $ u = \frac{1}{x} + \tan^{-1}x$. My question is about an alternative method I saw in which it seems the product rule was not applied: $$ I = \int \left(\frac { e^{\frac{1}{x}}} {x^2}\right) \left( \frac{e^{\tan^{-1}x}}{x^2+1}\right) dx $$ $$ = \int \frac {e^{\frac{1}{x}}}{x^2} dx \cdot \int \frac{e^{\tan^{-1}x}}{x^2+1}dx$$ Completing the work following this step leads to the same solution as I originally found. It is this step that has confused me. I have checked using Wolfram and the two statements are equivalent but I do not understand why. Why are we able to write the integral of products as the product of integrals here, and not apply the product rule? Thanks in advance.",,"['calculus', 'integration', 'indefinite-integrals']"
27,Proof only by transformation that : $ \int_0^\infty \cos(x^2) dx = \int_0^\infty \sin(x^2) dx $,Proof only by transformation that :, \int_0^\infty \cos(x^2) dx = \int_0^\infty \sin(x^2) dx ,"This was a  question in our exam and I did not know which change of variables or trick to apply How to show  by inspection ( change of variables or whatever trick  ) that $$ \int_0^\infty \cos(x^2) dx = \int_0^\infty \sin(x^2) dx \tag{I} $$ Computing the values of these integrals are known as routine. Further from their values, the equality holds. But can we show equality beforehand? Note : I am not asking for computation since it can be found here and we have as well that, $$ \int_0^\infty \cos(x^2) dx = \int_0^\infty \sin(x^2) dx =\sqrt{\frac{\pi}{8}}$$ and the result can be recover here, Evaluating $\int_0^\infty \sin x^2\, dx$ with real methods? . Is there any trick to prove the equality in (I) without computing the exact values of these integrals beforehand?","This was a  question in our exam and I did not know which change of variables or trick to apply How to show  by inspection ( change of variables or whatever trick  ) that Computing the values of these integrals are known as routine. Further from their values, the equality holds. But can we show equality beforehand? Note : I am not asking for computation since it can be found here and we have as well that, and the result can be recover here, Evaluating $\int_0^\infty \sin x^2\, dx$ with real methods? . Is there any trick to prove the equality in (I) without computing the exact values of these integrals beforehand?", \int_0^\infty \cos(x^2) dx = \int_0^\infty \sin(x^2) dx \tag{I}   \int_0^\infty \cos(x^2) dx = \int_0^\infty \sin(x^2) dx =\sqrt{\frac{\pi}{8}},"['calculus', 'integration', 'complex-analysis', 'analysis', 'fresnel-integrals']"
28,Show $\int_{0}^{\frac{\pi}{2}}\frac{x^{2}}{x^{2}+\ln^{2}(2\cos(x))}dx=\frac{\pi}{8}\left(1-\gamma+\ln(2\pi)\right)$,Show,\int_{0}^{\frac{\pi}{2}}\frac{x^{2}}{x^{2}+\ln^{2}(2\cos(x))}dx=\frac{\pi}{8}\left(1-\gamma+\ln(2\pi)\right),"Here is an interesting, albeit tough, integral I ran across. It has an interesting solution which leads me to think it is doable. But, what would be a good strategy?. $$\int_{0}^{\frac{\pi}{2}}\frac{x^{2}}{x^{2}+\ln^{2}(2\cos(x))}dx=\frac{\pi}{8}\left(1-\gamma+\ln(2\pi)\right)$$ This looks rough. What would be a good start?.  I tried various subs in order to get it into some sort of shape to use series, LaPlace, something, but made no real progress. I even tried writing a geometric series. But that didn't really result in anything encouraging. $$\int_{0}^{\frac{\pi}{2}}\sum_{n=0}^{\infty}(-1)^{k}\left(\frac{\ln(2\cos(x))}{x}\right)^{2k}$$ Thanks all.","Here is an interesting, albeit tough, integral I ran across. It has an interesting solution which leads me to think it is doable. But, what would be a good strategy?. $$\int_{0}^{\frac{\pi}{2}}\frac{x^{2}}{x^{2}+\ln^{2}(2\cos(x))}dx=\frac{\pi}{8}\left(1-\gamma+\ln(2\pi)\right)$$ This looks rough. What would be a good start?.  I tried various subs in order to get it into some sort of shape to use series, LaPlace, something, but made no real progress. I even tried writing a geometric series. But that didn't really result in anything encouraging. $$\int_{0}^{\frac{\pi}{2}}\sum_{n=0}^{\infty}(-1)^{k}\left(\frac{\ln(2\cos(x))}{x}\right)^{2k}$$ Thanks all.",,"['calculus', 'integration', 'definite-integrals']"
29,$n^{th}$ derivative of a tetration function,derivative of a tetration function,n^{th},"I stumbled upon this very peculiar function last summer, namely: $f(x)=x^{x^{x^{...^{x}}}}$, where there is a number $n$ of $x$'s in the exponent, I tried to find the derivative for the function and I was successful, it turned out not to be the most elegant formula but it worked. (Firstly, I invented a new notation, namely, a function such as $f(x)$ we can write it as the following: $f(x) =x^{\langle x \vert n\rangle}$ where $x$ is the exponent that is getting ""powered"" up $n$ times.) The formula I obtained by pattern matching was: $$f^{\prime}(x)=x^{\langle x \vert n\rangle +\langle x \vert n-1\rangle -1}\left[1+\prod_{i=0}^{n-2}x^{\langle x \vert i\rangle}\cdot \ln(x)^n+\sum_{j=1}^{n-1}\prod_{k=n-1-j}^{n-2}x^{\langle x \vert k\rangle}\cdot \ln(x)^j\right]\tag{$n\geqslant 2$}.$$ I know this looks like a mad mess and I am aware that people like this have done it more elegantely, but now for the question. This is only the first derivative of the function, is there a way, or rather is there a general derivative i.e a $n^{th}$ derivative of this function? Update: December 23th I have tried to approach the problem myself since I asked the question and I have not gotten to a stage to say if it is impossible or possible to do, however I think I am on the right track. At first, I thought of distributing the factor $x^{\langle x \vert n \rangle +\langle x \vert n-1 \rangle -1}$ to all the terms in the parentheses, but I quickly realized I had to deal with at least derivatives of triple products. Now I have come to realize that the easiest way is to differentiate the function just as it is and get a normal product and thus I must use the following formula: $$(f \cdot g)^{(n)}=\sum_{k=0}^{n}{n\choose k}f^{(k)}\cdot g^{(n-k)}$$ where $f =x^{\langle x \vert n \rangle +\langle x \vert n-1 \rangle -1}$ and $g=1+\prod_{i=0}^{n-2}x^{\langle x \vert i\rangle}\cdot \ln(x)^n+\sum_{j=1}^{n-1}\prod_{k=n-1-j}^{n-2}x^{\langle x \vert k\rangle}\cdot \ln(x)^j$. Since $k$ and $n-k$ are arbitrary numbers this leads us to find the general derivative for $f$ and $g$, this is where I am right now. (I do realize that I am trying to find the $n^{th}$ derivative of the first derivative but that is easily fixed later). Please come with suggestions on how to tackle this problem. Update: December 24th I have made progress with the help of Maple 17, namely, I have found a repeating pattern in at least a part of the general derivative, but there is still a part of it I cannot yet explain. Nonetheless, I present to you the part of the general derivative I have found: $$D_x^{\xi}f(x) = x^{\langle x \vert n\rangle +\langle x \vert n-1\rangle -\xi} \Big[(-1)^{\xi}\cdot\xi! +O(x)\Big]$$ I renamed the degree of the derivative as $\xi$ since $n$ is taken for the number of $x$s. The $O(x)$ is the (perhaps) series which I am currently working on finding, I do think I am on the right track though. The approach above with the product rule turned out to be less successful.","I stumbled upon this very peculiar function last summer, namely: $f(x)=x^{x^{x^{...^{x}}}}$, where there is a number $n$ of $x$'s in the exponent, I tried to find the derivative for the function and I was successful, it turned out not to be the most elegant formula but it worked. (Firstly, I invented a new notation, namely, a function such as $f(x)$ we can write it as the following: $f(x) =x^{\langle x \vert n\rangle}$ where $x$ is the exponent that is getting ""powered"" up $n$ times.) The formula I obtained by pattern matching was: $$f^{\prime}(x)=x^{\langle x \vert n\rangle +\langle x \vert n-1\rangle -1}\left[1+\prod_{i=0}^{n-2}x^{\langle x \vert i\rangle}\cdot \ln(x)^n+\sum_{j=1}^{n-1}\prod_{k=n-1-j}^{n-2}x^{\langle x \vert k\rangle}\cdot \ln(x)^j\right]\tag{$n\geqslant 2$}.$$ I know this looks like a mad mess and I am aware that people like this have done it more elegantely, but now for the question. This is only the first derivative of the function, is there a way, or rather is there a general derivative i.e a $n^{th}$ derivative of this function? Update: December 23th I have tried to approach the problem myself since I asked the question and I have not gotten to a stage to say if it is impossible or possible to do, however I think I am on the right track. At first, I thought of distributing the factor $x^{\langle x \vert n \rangle +\langle x \vert n-1 \rangle -1}$ to all the terms in the parentheses, but I quickly realized I had to deal with at least derivatives of triple products. Now I have come to realize that the easiest way is to differentiate the function just as it is and get a normal product and thus I must use the following formula: $$(f \cdot g)^{(n)}=\sum_{k=0}^{n}{n\choose k}f^{(k)}\cdot g^{(n-k)}$$ where $f =x^{\langle x \vert n \rangle +\langle x \vert n-1 \rangle -1}$ and $g=1+\prod_{i=0}^{n-2}x^{\langle x \vert i\rangle}\cdot \ln(x)^n+\sum_{j=1}^{n-1}\prod_{k=n-1-j}^{n-2}x^{\langle x \vert k\rangle}\cdot \ln(x)^j$. Since $k$ and $n-k$ are arbitrary numbers this leads us to find the general derivative for $f$ and $g$, this is where I am right now. (I do realize that I am trying to find the $n^{th}$ derivative of the first derivative but that is easily fixed later). Please come with suggestions on how to tackle this problem. Update: December 24th I have made progress with the help of Maple 17, namely, I have found a repeating pattern in at least a part of the general derivative, but there is still a part of it I cannot yet explain. Nonetheless, I present to you the part of the general derivative I have found: $$D_x^{\xi}f(x) = x^{\langle x \vert n\rangle +\langle x \vert n-1\rangle -\xi} \Big[(-1)^{\xi}\cdot\xi! +O(x)\Big]$$ I renamed the degree of the derivative as $\xi$ since $n$ is taken for the number of $x$s. The $O(x)$ is the (perhaps) series which I am currently working on finding, I do think I am on the right track though. The approach above with the product rule turned out to be less successful.",,"['calculus', 'derivatives', 'tetration']"
30,Calculus conjecture,Calculus conjecture,,"When I was a senior in high school in 2001, as I took calculus, I made the following conjecture that proves resistive to attack. It goes like this: For every positive integer $n$, there are exactly $n$ positive real zeroes of $$\frac {d^n}{dx^n}x^{1/x}$$ and no two derivatives has a common root. A few things to keep in mind are that it is not hard to show that $$\lim_{x\rightarrow 0^+}\frac {d^n}{dx^n}x^{1/x}=\lim_{x\rightarrow \infty}\frac {d^n}{dx^n}x^{1/x}=0$$ for all $n\geq 1$ and that $x^{1/x}$ is smooth and then to use these to show that the $n$th derivative has at least $n$ positive real zeroes. The proof of smoothness uses induction on $n$.","When I was a senior in high school in 2001, as I took calculus, I made the following conjecture that proves resistive to attack. It goes like this: For every positive integer $n$, there are exactly $n$ positive real zeroes of $$\frac {d^n}{dx^n}x^{1/x}$$ and no two derivatives has a common root. A few things to keep in mind are that it is not hard to show that $$\lim_{x\rightarrow 0^+}\frac {d^n}{dx^n}x^{1/x}=\lim_{x\rightarrow \infty}\frac {d^n}{dx^n}x^{1/x}=0$$ for all $n\geq 1$ and that $x^{1/x}$ is smooth and then to use these to show that the $n$th derivative has at least $n$ positive real zeroes. The proof of smoothness uses induction on $n$.",,['calculus']
31,Why does the Newton-Raphson method not converge for some functions?,Why does the Newton-Raphson method not converge for some functions?,,"$f(x)=2x^2-x^3-2$. This is a cubic type graph as shown.  The real root of this graph is $(-0.839,0)$. So, the question is to use Newton's approximation method twice to approximate a solution to this $f(x)$. I use an initial starting value of $x_0=1$. My first approximation is $x_1=2$, and my second one is $x_2=1.5$. I seem to not move any closer to the real solution as I keep iterating through the method. Am I misunderstanding how to use this approximation?  Is the issue that my first guess was too far from the actual solution?","$f(x)=2x^2-x^3-2$. This is a cubic type graph as shown.  The real root of this graph is $(-0.839,0)$. So, the question is to use Newton's approximation method twice to approximate a solution to this $f(x)$. I use an initial starting value of $x_0=1$. My first approximation is $x_1=2$, and my second one is $x_2=1.5$. I seem to not move any closer to the real solution as I keep iterating through the method. Am I misunderstanding how to use this approximation?  Is the issue that my first guess was too far from the actual solution?",,"['calculus', 'numerical-methods']"
32,Integral: $\int_{-\infty}^{\infty} x^2 e^{-x^2}\mathrm dx$,Integral:,\int_{-\infty}^{\infty} x^2 e^{-x^2}\mathrm dx,I don't know how to evaluate it. I know there is one method using the gamma function. BUT I want to know the solution using a calculus method like polar coordinates. $$\int_{-\infty}^\infty x^2 e^{-x^2}\mathrm dx$$ I will wait for a solution. Thank you.,I don't know how to evaluate it. I know there is one method using the gamma function. BUT I want to know the solution using a calculus method like polar coordinates. $$\int_{-\infty}^\infty x^2 e^{-x^2}\mathrm dx$$ I will wait for a solution. Thank you.,,"['calculus', 'integration', 'improper-integrals']"
33,Can a function have two derivatives?,Can a function have two derivatives?,,"I am a senior in high school so I know I am simply misunderstanding something but I don't know what, please have patience. I was tasked to find the derivative for the following function: $$ y = \frac{ (4x)^{1/5} }{5} + { \left( \frac{1}{x^3} \right) } ^ {1/4} $$ Simplifying: $$ y = \frac{ 4^{1/5} }{5} x^{1/5} + { \frac{1 ^ {1/4}}{x ^ {3/4}} } $$ $$ y = \frac{ 4^{1/5} }{5} x^{1/5} + { \frac{\pm 1}{x ^ {3/4}} } $$ Because $ 1 ^ {1/n} = \pm 1 $ , given $n$ is even $$ y = \frac{ 4^{1/5} }{5} x^{1/5} \pm { x ^ {-3/4} } $$ Taking the derivative using power rule: $$ \frac{dy}{dx} = \frac{ 4^{1/5} }{25} x^{-4/5} \pm \frac{-3}{4} { x ^ {-7/4} } $$ which is the same as $$ \frac{dy}{dx} = \frac{ 4^{1/5} }{25} x^{-4/5} \pm \frac{3}{4} { x ^ {-7/4} } $$ And that is the part that I find difficult to understand. I know that I should be adding the second term(I graphed it multiple times to make sure), but I cannot catch my error and my teacher did't want to discuss it. So I know I am doing something wrong because one function cannot have more than one derivative.","I am a senior in high school so I know I am simply misunderstanding something but I don't know what, please have patience. I was tasked to find the derivative for the following function: Simplifying: Because , given is even Taking the derivative using power rule: which is the same as And that is the part that I find difficult to understand. I know that I should be adding the second term(I graphed it multiple times to make sure), but I cannot catch my error and my teacher did't want to discuss it. So I know I am doing something wrong because one function cannot have more than one derivative.", y = \frac{ (4x)^{1/5} }{5} + { \left( \frac{1}{x^3} \right) } ^ {1/4}   y = \frac{ 4^{1/5} }{5} x^{1/5} + { \frac{1 ^ {1/4}}{x ^ {3/4}} }   y = \frac{ 4^{1/5} }{5} x^{1/5} + { \frac{\pm 1}{x ^ {3/4}} }   1 ^ {1/n} = \pm 1  n  y = \frac{ 4^{1/5} }{5} x^{1/5} \pm { x ^ {-3/4} }   \frac{dy}{dx} = \frac{ 4^{1/5} }{25} x^{-4/5} \pm \frac{-3}{4} { x ^ {-7/4} }   \frac{dy}{dx} = \frac{ 4^{1/5} }{25} x^{-4/5} \pm \frac{3}{4} { x ^ {-7/4} } ,"['calculus', 'derivatives']"
34,Is the integral $\int_0^\infty \frac{\mathrm{d} x}{(1+x^2)(1+x^a)}$ equal for all $a \neq 0$?,Is the integral  equal for all ?,\int_0^\infty \frac{\mathrm{d} x}{(1+x^2)(1+x^a)} a \neq 0,Let $a$ be a non-zero real number. Is it true that the value of $$\int\limits_0^\infty \frac{\mathrm{d} x}{(1+x^2)(1+x^a)}$$  is independent on $a$?,Let $a$ be a non-zero real number. Is it true that the value of $$\int\limits_0^\infty \frac{\mathrm{d} x}{(1+x^2)(1+x^a)}$$  is independent on $a$?,,"['calculus', 'integration', 'definite-integrals']"
35,How to determine if a function is one-to-one?,How to determine if a function is one-to-one?,,"I am looking for the ""best"" way to determine whether a function is one-to-one, either algebraically or with calculus. I know a common, yet arguably unreliable method for determining this answer would be to graph the function. However, this can prove to be a risky method for finding such an answer at it heavily depends on the precision of your graphing calculator, your zoom, etc... What is the best method for finding that a function is one-to-one? In your description, could you please elaborate by showing that it can prove the following: $\frac{x-3}{x+2}$ is one-to-one. $\frac{x-3}{x^3}$ is not one-to-one.","I am looking for the ""best"" way to determine whether a function is one-to-one, either algebraically or with calculus. I know a common, yet arguably unreliable method for determining this answer would be to graph the function. However, this can prove to be a risky method for finding such an answer at it heavily depends on the precision of your graphing calculator, your zoom, etc... What is the best method for finding that a function is one-to-one? In your description, could you please elaborate by showing that it can prove the following: is one-to-one. is not one-to-one.",\frac{x-3}{x+2} \frac{x-3}{x^3},"['calculus', 'algebra-precalculus', 'functions']"
36,Evaluating $\int_0^1 \log \log \left(\frac{1}{x}\right) \frac{dx}{1+x^2}$,Evaluating,\int_0^1 \log \log \left(\frac{1}{x}\right) \frac{dx}{1+x^2},Show that $\displaystyle{\int_0^1 \log \log \left(\frac{1}{x}\right) \frac{dx}{1+x^2} = \frac{\pi}{2}\log \left(\sqrt{2\pi} \Gamma\left(\frac{3}{4}\right) / \Gamma\left(\frac{1}{4}\right)\right)}$ This question was posted as part of this question: Solve the integral $S_k = (-1)^k \int_0^1 (\log(\sin \pi x))^k dx$ I cannot think of a change of variable nor other integrating methods. Maybe there is a known method that I am missing.,Show that This question was posted as part of this question: Solve the integral $S_k = (-1)^k \int_0^1 (\log(\sin \pi x))^k dx$ I cannot think of a change of variable nor other integrating methods. Maybe there is a known method that I am missing.,\displaystyle{\int_0^1 \log \log \left(\frac{1}{x}\right) \frac{dx}{1+x^2} = \frac{\pi}{2}\log \left(\sqrt{2\pi} \Gamma\left(\frac{3}{4}\right) / \Gamma\left(\frac{1}{4}\right)\right)},"['calculus', 'integration', 'definite-integrals', 'logarithms', 'gamma-function']"
37,"Physicists, not mathematicians, can multiply both sides with $dx$ - why?","Physicists, not mathematicians, can multiply both sides with  - why?",dx,"The following question is asked without malicious intentions - it's not intended as a flamebait! In my physics textbooks ( Young & Freedman in particular) I have often seen derivations of equations that use multiplication of, say, $dx$ on both sides, and then integrating. Taking courses both at the math and physics department I've noticed that real mathematicians often frown at this. Why is that?","The following question is asked without malicious intentions - it's not intended as a flamebait! In my physics textbooks ( Young & Freedman in particular) I have often seen derivations of equations that use multiplication of, say, $dx$ on both sides, and then integrating. Taking courses both at the math and physics department I've noticed that real mathematicians often frown at this. Why is that?",,"['calculus', 'analysis', 'physics']"
38,Why is the 2nd derivative written as $\frac{\mathrm d^2y}{\mathrm dx^2}$?,Why is the 2nd derivative written as ?,\frac{\mathrm d^2y}{\mathrm dx^2},"In Leibniz notation, the 2nd derivative is written as $$\dfrac{\mathrm d^2y}{\mathrm dx^2}\ ?$$ Why is the location of the $2$ in different places in the $\mathrm dy/\mathrm dx$ terms?","In Leibniz notation, the 2nd derivative is written as $$\dfrac{\mathrm d^2y}{\mathrm dx^2}\ ?$$ Why is the location of the $2$ in different places in the $\mathrm dy/\mathrm dx$ terms?",,"['calculus', 'derivatives', 'notation']"
39,Find the infinite sum of the series $\sum_{n=1}^\infty \frac{1}{n^2 +1}$,Find the infinite sum of the series,\sum_{n=1}^\infty \frac{1}{n^2 +1},"This is a homework question whereby I am supposed to evaluate: $$\sum_{n=1}^\infty \frac{1}{n^2 +1}$$ Wolfram Alpha outputs the answer as $$\frac{1}{2}(\pi \coth(\pi) - 1)$$ But I have no idea how to get there. Tried partial fractions (by splitting into imaginary components), tried comparing with the Basel problem (turns out there's little similarities), nothing worked.","This is a homework question whereby I am supposed to evaluate: $$\sum_{n=1}^\infty \frac{1}{n^2 +1}$$ Wolfram Alpha outputs the answer as $$\frac{1}{2}(\pi \coth(\pi) - 1)$$ But I have no idea how to get there. Tried partial fractions (by splitting into imaginary components), tried comparing with the Basel problem (turns out there's little similarities), nothing worked.",,"['calculus', 'sequences-and-series']"
40,"Is $f(x)=1/x$ continuous on $(0,\infty)$?",Is  continuous on ?,"f(x)=1/x (0,\infty)","I've never actually done a delta-epsilon proof, so I thought I'd try my hand at one. I decided to try it out for $f(x)=1/x$. If I understand correctly from the wikipedia article, I want to show for any $\varepsilon>0$, there exists a $\delta>0$ such that if $|x-c|<\delta$, then $|f(x)-f(c)|<\varepsilon$. Anyway, I noticed that I want something like $$|f(x)-f(c)|= \left| \frac{1}{x} - \frac{1}{c} \right|=\frac{|x-c|}{|xc|}<\varepsilon .$$ So $|x-c|<|xc|\varepsilon$, which looks similar to the fact that I want $|x-c|<\delta$. However, I've also heard that one is never supposed to let $\delta$ depend on $x$. Is this the right direction? How would I use this information to find a corresponding $\delta$ for each $\epsilon$? Thanks!","I've never actually done a delta-epsilon proof, so I thought I'd try my hand at one. I decided to try it out for $f(x)=1/x$. If I understand correctly from the wikipedia article, I want to show for any $\varepsilon>0$, there exists a $\delta>0$ such that if $|x-c|<\delta$, then $|f(x)-f(c)|<\varepsilon$. Anyway, I noticed that I want something like $$|f(x)-f(c)|= \left| \frac{1}{x} - \frac{1}{c} \right|=\frac{|x-c|}{|xc|}<\varepsilon .$$ So $|x-c|<|xc|\varepsilon$, which looks similar to the fact that I want $|x-c|<\delta$. However, I've also heard that one is never supposed to let $\delta$ depend on $x$. Is this the right direction? How would I use this information to find a corresponding $\delta$ for each $\epsilon$? Thanks!",,['calculus']
41,Calculate the following infinite sum in a closed form $\sum_{n=1}^\infty(n\ \text{arccot}\ n-1)$?,Calculate the following infinite sum in a closed form ?,\sum_{n=1}^\infty(n\ \text{arccot}\ n-1),"Is it possible to calculate the following infinite sum in a closed form? If yes, please point me to the right direction. $$\sum_{n=1}^\infty(n\ \text{arccot}\ n-1)$$","Is it possible to calculate the following infinite sum in a closed form? If yes, please point me to the right direction. $$\sum_{n=1}^\infty(n\ \text{arccot}\ n-1)$$",,"['calculus', 'sequences-and-series', 'trigonometry', 'summation', 'closed-form']"
42,Differentiating both sides of an equation,Differentiating both sides of an equation,,"I'm going through the MIT lecture on implicit differentiation, and the first two steps are shown below, taking the derivative of both sides: $$x^2 + y^2 = 1$$ $$\frac{d}{dx} x^2 + \frac{d}{dx} y^2 = \frac{d}{dx} 1$$ $$2x + \frac{d}{dx}y^2 = 0$$ That makes some sense, but what about this example: $$x = 5$$ $$\frac{d}{dx} x = \frac{d}{dx} 5$$ $$1 = 0$$ Why is the first example correct, while the second is obviously wrong?","I'm going through the MIT lecture on implicit differentiation, and the first two steps are shown below, taking the derivative of both sides: $$x^2 + y^2 = 1$$ $$\frac{d}{dx} x^2 + \frac{d}{dx} y^2 = \frac{d}{dx} 1$$ $$2x + \frac{d}{dx}y^2 = 0$$ That makes some sense, but what about this example: $$x = 5$$ $$\frac{d}{dx} x = \frac{d}{dx} 5$$ $$1 = 0$$ Why is the first example correct, while the second is obviously wrong?",,"['calculus', 'derivatives', 'implicit-differentiation']"
43,"Prove $\int_{0}^{\pi/2} \ln \left(x^{2} + (\ln\cos x)^2 \right) \, dx=\pi\ln\ln2 $",Prove,"\int_{0}^{\pi/2} \ln \left(x^{2} + (\ln\cos x)^2 \right) \, dx=\pi\ln\ln2 ","How to prove   $$ \int_{0}^{\pi/2}\ln\left(\,x^{2} + \ln^{2}\left(\,\cos\left(\,x\,\right)\,\right) \,\right)\,{\rm d}x\ =\ \pi\ln\left(\,\ln\left(\, 2\,\right)\,\right) $$ I don't know how to answer it. When I asked this integral to my brother, after less than half hours he said it has a nice closed-form involving $\pi$ and $\ln\left(2\right)$ but, as always, he didn't tell me the closed-form and how to obtain it ( I didn't believe him and I think he tried to mess around with me ). I have also searched the similar question here but it looks like nothing is similar or related. Could anyone here please help me to obtain the closed form of the integral preferably with elementary ways ( high school methods )?. Any help would be greatly appreciated. Thank you. Edit: He is being a little bit nice to me today, he said the closed form is $\pi\ln\ln2$ and it's numerically correct. This is not a duplicate problem, I am looking for a proof without using complex analysis.","How to prove   $$ \int_{0}^{\pi/2}\ln\left(\,x^{2} + \ln^{2}\left(\,\cos\left(\,x\,\right)\,\right) \,\right)\,{\rm d}x\ =\ \pi\ln\left(\,\ln\left(\, 2\,\right)\,\right) $$ I don't know how to answer it. When I asked this integral to my brother, after less than half hours he said it has a nice closed-form involving $\pi$ and $\ln\left(2\right)$ but, as always, he didn't tell me the closed-form and how to obtain it ( I didn't believe him and I think he tried to mess around with me ). I have also searched the similar question here but it looks like nothing is similar or related. Could anyone here please help me to obtain the closed form of the integral preferably with elementary ways ( high school methods )?. Any help would be greatly appreciated. Thank you. Edit: He is being a little bit nice to me today, he said the closed form is $\pi\ln\ln2$ and it's numerically correct. This is not a duplicate problem, I am looking for a proof without using complex analysis.",,"['calculus', 'integration', 'definite-integrals', 'improper-integrals', 'closed-form']"
44,Prove inequality $\arccos \left( \frac{\sin 1-\sin x}{1-x} \right) \leq \sqrt{\frac{1+x+x^2}{3}}$,Prove inequality,\arccos \left( \frac{\sin 1-\sin x}{1-x} \right) \leq \sqrt{\frac{1+x+x^2}{3}},"I was trying to figure out if the following function can serve as a mean (see mean value theorem): $$\arccos \left( \frac{\sin y-\sin x}{y-x} \right)$$ And turns out that for $x,y \leq \pi$ it does serve as a mean admirably. But then I've noticed that for $0<x<1$ the following two functions are very close (see the picture): Now how would you prove: $$\arccos \left( \frac{\sin 1-\sin x}{1-x} \right) \leq \sqrt{\frac{1+x+x^2}{3}}$$ It's probably easier to consider another equivalent inequality: $$\frac{\sin 1-\sin x}{1-x}  \geq \cos \sqrt{\frac{1+x+x^2}{3}}$$ Or even: $$ \text{sinc} \left(\frac{1-x}{2} \right) \cos \left(\frac{1+x}{2} \right)  \geq \cos \sqrt{\frac{1+x+x^2}{3}}$$ We could use Taylor series, but that's too cumbersome in my opinion. Another way would be Mean value theorem itself, but I encounter the same problem. Is there a simple way to prove this inequality? My calculus is not as sharp as it used to be (just kidding, it was never sharp). Edit Just to confirm (numerically) that the inequality holds, here is the plot of the difference between the two functions:","I was trying to figure out if the following function can serve as a mean (see mean value theorem): And turns out that for it does serve as a mean admirably. But then I've noticed that for the following two functions are very close (see the picture): Now how would you prove: It's probably easier to consider another equivalent inequality: Or even: We could use Taylor series, but that's too cumbersome in my opinion. Another way would be Mean value theorem itself, but I encounter the same problem. Is there a simple way to prove this inequality? My calculus is not as sharp as it used to be (just kidding, it was never sharp). Edit Just to confirm (numerically) that the inequality holds, here is the plot of the difference between the two functions:","\arccos \left( \frac{\sin y-\sin x}{y-x} \right) x,y \leq \pi 0<x<1 \arccos \left( \frac{\sin 1-\sin x}{1-x} \right) \leq \sqrt{\frac{1+x+x^2}{3}} \frac{\sin 1-\sin x}{1-x}  \geq \cos \sqrt{\frac{1+x+x^2}{3}}  \text{sinc} \left(\frac{1-x}{2} \right) \cos \left(\frac{1+x}{2} \right)  \geq \cos \sqrt{\frac{1+x+x^2}{3}}","['calculus', 'trigonometry', 'inequality', 'means']"
45,How to find ${\large\int}_1^\infty\frac{1-x+\ln x}{x \left(1+x^2\right) \ln^2 x} \mathrm dx$? [closed],How to find ? [closed],{\large\int}_1^\infty\frac{1-x+\ln x}{x \left(1+x^2\right) \ln^2 x} \mathrm dx,"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 4 years ago . Improve this question Please help me to find a closed form for this integral: $$ I=\int_1^\infty\frac{1-x+\ln x}{x \left(1+x^2\right) \ln^2 x} \mathrm dx $$ Routine textbook methods for this complicated integral fail.","Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 4 years ago . Improve this question Please help me to find a closed form for this integral: $$ I=\int_1^\infty\frac{1-x+\ln x}{x \left(1+x^2\right) \ln^2 x} \mathrm dx $$ Routine textbook methods for this complicated integral fail.",,"['calculus', 'integration', 'definite-integrals', 'logarithms', 'closed-form']"
46,Does $1.0000000000\cdots 1$ with an infinite number of $0$ in it exist?,Does  with an infinite number of  in it exist?,1.0000000000\cdots 1 0,Does $1.0000000000\cdots 1$ (with  an infinite number of  $0$ in it) exist?,Does $1.0000000000\cdots 1$ (with  an infinite number of  $0$ in it) exist?,,"['calculus', 'limits', 'real-numbers', 'infinity', 'decimal-expansion']"
47,Integral of periodic function over the length of the period is the same everywhere,Integral of periodic function over the length of the period is the same everywhere,,"I am stuck on a question that involves the intergral of a periodic function. The question is phrased as follows: Definition. A function is periodic with period $a$ if $f(x)=f(x+a)$ for all $x$ . Question. If $f$ is continuous and periodic with period $a$ , then show that $$\int_{0}^{a}f(t)dt=\int_{b}^{b+a}f(t)dt$$ for all $b\in \mathbb{R}$ . I understand the equality, but I am having trouble showing that it is true for all $b$ . I've tried writing it in different forms such as $F(a)=F(b+a)-F(b)$ . This led me to the following, though I am not sure how this shows the equality is true for all $b$ , $$\int_{0}^{a}f(t)dt-\int_{b}^{b+a}f(t)dt=0$$ $$=F(a)-F(0)-F(b+a)-F(b)$$ $$=(F(b+a)-F(a))-F(b)$$ $$=\int_{a}^{b+a}f(t)dt-\int_{0}^{b+a}f(t)dt=0$$ So, this leaves me with $$\int_{a}^{b+a}f(t)dt-\int_{0}^{b+a}f(t)dt=\int_{0}^{a}f(t)dt-\int_{b}^{b+a}f(t)dt$$ I feel I am close, and I've made myself a diagram of a sine function to visualize what each of the above integrals might describe, but the power to explain the above equality evades me.","I am stuck on a question that involves the intergral of a periodic function. The question is phrased as follows: Definition. A function is periodic with period if for all . Question. If is continuous and periodic with period , then show that for all . I understand the equality, but I am having trouble showing that it is true for all . I've tried writing it in different forms such as . This led me to the following, though I am not sure how this shows the equality is true for all , So, this leaves me with I feel I am close, and I've made myself a diagram of a sine function to visualize what each of the above integrals might describe, but the power to explain the above equality evades me.",a f(x)=f(x+a) x f a \int_{0}^{a}f(t)dt=\int_{b}^{b+a}f(t)dt b\in \mathbb{R} b F(a)=F(b+a)-F(b) b \int_{0}^{a}f(t)dt-\int_{b}^{b+a}f(t)dt=0 =F(a)-F(0)-F(b+a)-F(b) =(F(b+a)-F(a))-F(b) =\int_{a}^{b+a}f(t)dt-\int_{0}^{b+a}f(t)dt=0 \int_{a}^{b+a}f(t)dt-\int_{0}^{b+a}f(t)dt=\int_{0}^{a}f(t)dt-\int_{b}^{b+a}f(t)dt,"['calculus', 'integration', 'periodic-functions']"
48,Proof of the derivative of $\ln(x)$,Proof of the derivative of,\ln(x),"I'm trying to prove that $\frac{\mathrm{d} }{\mathrm{d} x}\ln x = \frac{1}{x}$ . Here's what I've got so far: $$ \begin{align} \frac{\mathrm{d}}{\mathrm{d} x}\ln x &= \lim_{h\to0} \frac{\ln(x + h) - \ln(x)}{h} \\ &= \lim_{h\to0} \frac{\ln(\frac{x + h}{x})}{h} \\ &= \lim_{h\to0} \frac{\ln(1 + \frac{h}{x})}{h} \\ \end{align} $$ To simplify the logarithm: $$ \lim_{h\to0}\left (1 + \frac{h}{x}\right )^{\frac{1}{h}} = e^{\frac{1}{x}} $$ This is the line I have trouble with. I can see that it is true by putting numbers in, but I can't prove it. I know that $e^{\frac{1}{x}} = \lim_{h\to0}\left (1 + h \right )^{\frac{h}{x}}$ , but I can't work out how to get from the above line to that. $$ \lim_{h\to0}\left ( \left (1 + \frac{h}{x}\right )^{\frac{1}{h}}\right )^{h} = e^{\frac{h}{x}} $$ Going back to the derivative: $$ \begin{align} \frac{\mathrm{d}}{\mathrm{d} x}\ln x &= \lim_{h\to0} \frac{\ln(e^{\frac{h}{x}})}{h} \\ &= \lim_{h\to0} \frac{\frac{h}{x}\ln(e)}{h} \\ &= \lim_{h\to0} \frac{h}{x} \div h\\ &= \frac{1}{x} \\ \end{align} $$ This proof seems fine, apart from the middle step to get $e^{\frac{1}{x}}$ . How could I prove that part?","I'm trying to prove that . Here's what I've got so far: To simplify the logarithm: This is the line I have trouble with. I can see that it is true by putting numbers in, but I can't prove it. I know that , but I can't work out how to get from the above line to that. Going back to the derivative: This proof seems fine, apart from the middle step to get . How could I prove that part?","\frac{\mathrm{d} }{\mathrm{d} x}\ln x = \frac{1}{x} 
\begin{align}
\frac{\mathrm{d}}{\mathrm{d} x}\ln x &= \lim_{h\to0} \frac{\ln(x + h) - \ln(x)}{h} \\
&= \lim_{h\to0} \frac{\ln(\frac{x + h}{x})}{h} \\
&= \lim_{h\to0} \frac{\ln(1 + \frac{h}{x})}{h} \\
\end{align}
 
\lim_{h\to0}\left (1 + \frac{h}{x}\right )^{\frac{1}{h}} = e^{\frac{1}{x}}
 e^{\frac{1}{x}} = \lim_{h\to0}\left (1 + h \right )^{\frac{h}{x}} 
\lim_{h\to0}\left ( \left (1 + \frac{h}{x}\right )^{\frac{1}{h}}\right )^{h} = e^{\frac{h}{x}}
 
\begin{align}
\frac{\mathrm{d}}{\mathrm{d} x}\ln x &= \lim_{h\to0} \frac{\ln(e^{\frac{h}{x}})}{h} \\
&= \lim_{h\to0} \frac{\frac{h}{x}\ln(e)}{h} \\
&= \lim_{h\to0} \frac{h}{x} \div h\\
&= \frac{1}{x} \\
\end{align}
 e^{\frac{1}{x}}","['calculus', 'derivatives', 'solution-verification', 'logarithms', 'exponential-function']"
49,A ping-pong ball lying inside a wine glass,A ping-pong ball lying inside a wine glass,,"Show that a ping-pong ball with radius $r$, lying inside a wine glass described by the function $x^2$, has its center at $r^2+\frac{1}{4}$ units above the bottom of the glass. Here is a visualization of the problem My best attempt is trying to find the derivative of the circle and the function to find some relationship at the point where they meet. The problem looked very simple at first, but I can't figure it out now.","Show that a ping-pong ball with radius $r$, lying inside a wine glass described by the function $x^2$, has its center at $r^2+\frac{1}{4}$ units above the bottom of the glass. Here is a visualization of the problem My best attempt is trying to find the derivative of the circle and the function to find some relationship at the point where they meet. The problem looked very simple at first, but I can't figure it out now.",,"['calculus', 'functions']"
50,$e$ to 50 billion decimal places,to 50 billion decimal places,e,"Sorry if this is a really naive question, but in my reading of a lot of textbooks and articles, there is a lot of mention of how many decimals we know of a certain number today, such as $\pi$ or $e$. An excerpt from my textbook: In 1748, Leonard Euler used the sum of the infinite series of $e$ (mentioned in the book in a section about Taylor Series) to find the value of $e$ to 23 digits. In 2003, Shigeru Kondo, again using the series, computed $e$ to 50 billion decimals places My question is why does it matter how many decimals we know? Isn't this just a huge waste of time? What could we ever do with so many decimal places? And, if $e$ can be represented as a sum of infinite series of $1/n!$, can't we just plug that into a computer that just loops the same equation but increasing $n$ every iteration, and find as many decimals of $e$ as we like? (Once again, I realize this may be an ignorant/naive question, but I've always been curious about this)","Sorry if this is a really naive question, but in my reading of a lot of textbooks and articles, there is a lot of mention of how many decimals we know of a certain number today, such as $\pi$ or $e$. An excerpt from my textbook: In 1748, Leonard Euler used the sum of the infinite series of $e$ (mentioned in the book in a section about Taylor Series) to find the value of $e$ to 23 digits. In 2003, Shigeru Kondo, again using the series, computed $e$ to 50 billion decimals places My question is why does it matter how many decimals we know? Isn't this just a huge waste of time? What could we ever do with so many decimal places? And, if $e$ can be represented as a sum of infinite series of $1/n!$, can't we just plug that into a computer that just loops the same equation but increasing $n$ every iteration, and find as many decimals of $e$ as we like? (Once again, I realize this may be an ignorant/naive question, but I've always been curious about this)",,"['calculus', 'soft-question', 'taylor-expansion']"
51,"Will moving differentiation from inside, to outside an integral, change the result?","Will moving differentiation from inside, to outside an integral, change the result?",,"I'm interested in the potential of such a technique.  I got the idea from Moron's answer to this question , which uses the technique of differentiation under the integral. Now, I'd like to consider this integral: $$\int_{-\pi}^\pi \cos{(y(1-e^{i\cdot n \cdot t}))}\mathrm dt$$ I'd like to differentiate with respect to y.  This will give the integral: $$\int_{-\pi}^\pi -(1-e^{i\cdot n \cdot t})(\sin{(y(1-e^{i\cdot n \cdot t}))}\mathrm dt$$ ...If I'm correct.  Anyways, I'm interested in obtaining the results to this second integral, using this technique.  So I'm wondering if solving the first integral can help give results for the second integral.  I'm thinking of setting $y=1$ in the second integral.  This should eliminate $y$ from the result, and give me the integral involving $x$. The trouble is, I'm not sure I can use the technique of differentiation under the integral.  I want to know how I can apply this technique to the integrals above.  Any pointers are appreciated. For instance, for what values of $y$ is this valid?","I'm interested in the potential of such a technique.  I got the idea from Moron's answer to this question , which uses the technique of differentiation under the integral. Now, I'd like to consider this integral: $$\int_{-\pi}^\pi \cos{(y(1-e^{i\cdot n \cdot t}))}\mathrm dt$$ I'd like to differentiate with respect to y.  This will give the integral: $$\int_{-\pi}^\pi -(1-e^{i\cdot n \cdot t})(\sin{(y(1-e^{i\cdot n \cdot t}))}\mathrm dt$$ ...If I'm correct.  Anyways, I'm interested in obtaining the results to this second integral, using this technique.  So I'm wondering if solving the first integral can help give results for the second integral.  I'm thinking of setting $y=1$ in the second integral.  This should eliminate $y$ from the result, and give me the integral involving $x$. The trouble is, I'm not sure I can use the technique of differentiation under the integral.  I want to know how I can apply this technique to the integrals above.  Any pointers are appreciated. For instance, for what values of $y$ is this valid?",,"['calculus', 'integration', 'leibniz-integral-rule']"
52,Definition of convergence of a nested radical $\sqrt{a_1 + \sqrt{a_2 + \sqrt{a_3 + \sqrt{a_4+\cdots}}}}$?,Definition of convergence of a nested radical ?,\sqrt{a_1 + \sqrt{a_2 + \sqrt{a_3 + \sqrt{a_4+\cdots}}}},"In my answer to the recent question Nested Square Roots , @GEdgar correctly raised the issue that the proof is incomplete unless I show that the intermediate expressions do converge to a (finite) limit. One such quantity was the nested radical  $$ \sqrt{1 + \sqrt{1+\sqrt{1 + \sqrt{1 + \cdots}}}} \tag{1} $$ To assign a value $Y$ to such an expression, I proposed the following definition. Define the sequence $\{ y_n \}$ by:  $$ y_1 = \sqrt{1}, y_{n+1} = \sqrt{1+y_n}. $$ Then we say that this expression evaluates to $Y$ if the sequence $y_n$ converges to $Y$. For the expression (1), I could show that the $y_n$ converges to $\phi = (\sqrt{5}+1)/2$. (To give more details, I showed, by induction, that $y_n$ increases monotonically and is bounded by $\phi$, so that it has a limit $Y < \infty$. Furthermore, this limit must satisfy $Y = \sqrt{1+Y}$.) Hence we could safely say (1) evaluates to $\phi$, and all seems to be good. My trouble. Let us now test my proposed idea with a more general expression of the form $$\sqrt{a_1 + \sqrt{a_2 + \sqrt{a_3 + \sqrt{a_4+\cdots}}}} \tag{2}$$  (Note that the linked question involves one such expression, with $a_n = 5^{2^n}$.) How do we decide if this expression converges? Mimicking the above definition, we can write: $$ y_1 = \sqrt{a_1}, y_{n+1} = \sqrt{a_{n+1}+y_n}. $$ However, unrolling this definition, one get the sequence $$ \sqrt{a_1}, \sqrt{a_{2}+ \sqrt{a_1}}, \sqrt{a_3 + \sqrt{a_2 + \sqrt{a_1}}}, \sqrt{a_4+\sqrt{a_3 + \sqrt{a_2 + \sqrt{a_1}}}}, \ldots $$ but this seems little to do with the expression (2) that we started with. I could not come up with any satisfactory ways to resolve the issue. So, my question is: How do I rigorously define when an expression of the form (2) converges, and also assign a value to it when it does converge? Thanks.","In my answer to the recent question Nested Square Roots , @GEdgar correctly raised the issue that the proof is incomplete unless I show that the intermediate expressions do converge to a (finite) limit. One such quantity was the nested radical  $$ \sqrt{1 + \sqrt{1+\sqrt{1 + \sqrt{1 + \cdots}}}} \tag{1} $$ To assign a value $Y$ to such an expression, I proposed the following definition. Define the sequence $\{ y_n \}$ by:  $$ y_1 = \sqrt{1}, y_{n+1} = \sqrt{1+y_n}. $$ Then we say that this expression evaluates to $Y$ if the sequence $y_n$ converges to $Y$. For the expression (1), I could show that the $y_n$ converges to $\phi = (\sqrt{5}+1)/2$. (To give more details, I showed, by induction, that $y_n$ increases monotonically and is bounded by $\phi$, so that it has a limit $Y < \infty$. Furthermore, this limit must satisfy $Y = \sqrt{1+Y}$.) Hence we could safely say (1) evaluates to $\phi$, and all seems to be good. My trouble. Let us now test my proposed idea with a more general expression of the form $$\sqrt{a_1 + \sqrt{a_2 + \sqrt{a_3 + \sqrt{a_4+\cdots}}}} \tag{2}$$  (Note that the linked question involves one such expression, with $a_n = 5^{2^n}$.) How do we decide if this expression converges? Mimicking the above definition, we can write: $$ y_1 = \sqrt{a_1}, y_{n+1} = \sqrt{a_{n+1}+y_n}. $$ However, unrolling this definition, one get the sequence $$ \sqrt{a_1}, \sqrt{a_{2}+ \sqrt{a_1}}, \sqrt{a_3 + \sqrt{a_2 + \sqrt{a_1}}}, \sqrt{a_4+\sqrt{a_3 + \sqrt{a_2 + \sqrt{a_1}}}}, \ldots $$ but this seems little to do with the expression (2) that we started with. I could not come up with any satisfactory ways to resolve the issue. So, my question is: How do I rigorously define when an expression of the form (2) converges, and also assign a value to it when it does converge? Thanks.",,"['calculus', 'sequences-and-series', 'definition', 'nested-radicals']"
53,"Closed form for ${\large\int}_0^1\frac{\ln(1-x)\,\ln(1+x)\,\ln(1+2x)}{1+2x}dx$",Closed form for,"{\large\int}_0^1\frac{\ln(1-x)\,\ln(1+x)\,\ln(1+2x)}{1+2x}dx","Here is another integral I'm trying to evaluate: $$I=\int_0^1\frac{\ln(1-x)\,\ln(1+x)\,\ln(1+2x)}{1+2x}dx.\tag1$$ A numeric approximation is: $$I\approx-0.19902842515384155925817158058508204141843184171999583129...\tag2$$ (click here to see more digits). Unfortunately, so far I have made no progress in finding a closed form for it. Could you please suggest any ideas how to do that?","Here is another integral I'm trying to evaluate: $$I=\int_0^1\frac{\ln(1-x)\,\ln(1+x)\,\ln(1+2x)}{1+2x}dx.\tag1$$ A numeric approximation is: $$I\approx-0.19902842515384155925817158058508204141843184171999583129...\tag2$$ (click here to see more digits). Unfortunately, so far I have made no progress in finding a closed form for it. Could you please suggest any ideas how to do that?",,"['calculus', 'integration', 'definite-integrals', 'logarithms', 'polylogarithm']"
54,Integral $\int_0^1\frac{x^9\left(x^4+x^2-x-1-5\ln x\right)}{\left(x^{10}-1\right)\ln x}\mathrm dx$,Integral,\int_0^1\frac{x^9\left(x^4+x^2-x-1-5\ln x\right)}{\left(x^{10}-1\right)\ln x}\mathrm dx,"A friend of mine sent me an integral that she had not been able to crack, and me neither. It comes with a result, but without a proof (I suppose it originated in some math contest). Could you please suggest an approach to prove the result? $$\int_0^1\frac{x^9\left(x^4+x^2-x-1-5\ln x\right)}{\left(x^{10}-1\right)\ln x}\mathrm dx=\frac12\gamma+\frac{11}5\ln2-\frac54\ln5+\frac12\ln\pi-\frac12\ln\phi,$$ where $\gamma$ is the Euler–Mascheroni constant, and $\phi$ is the golden ratio.","A friend of mine sent me an integral that she had not been able to crack, and me neither. It comes with a result, but without a proof (I suppose it originated in some math contest). Could you please suggest an approach to prove the result? $$\int_0^1\frac{x^9\left(x^4+x^2-x-1-5\ln x\right)}{\left(x^{10}-1\right)\ln x}\mathrm dx=\frac12\gamma+\frac{11}5\ln2-\frac54\ln5+\frac12\ln\pi-\frac12\ln\phi,$$ where $\gamma$ is the Euler–Mascheroni constant, and $\phi$ is the golden ratio.",,"['calculus', 'integration', 'logarithms', 'contest-math', 'closed-form']"
55,Why is a straight line the shortest distance between two points?,Why is a straight line the shortest distance between two points?,,"The first application I was shown of the calculus of variations was proving that the shortest distance between two points is a straight line. Define a functional measuring the length of a curve between two points: $$ I(y) = \int_{x_1}^{x_2} \sqrt{1 + (y')^2}\, dx, $$ apply the Euler-Langrange equation, and Bob's your uncle. So far so good, but then I started thinking: That functional was derived by splitting the curve into (infinitesimal) - wait for it - straight lines, and summing them up their lengths, and each length was defined as being the Euclidean distance between its endpoints*. As such, it seems to me that the proof, while correct, is rather meaningless. It's an obvious consequence of the facts that (a) the Euclidean norm satisfies the triangle inequality and (b) the length of a curve was defined as a sum of Euclidean norms. Getting slightly philosophical, I would conjecture that proving that the shortest distance between two points is a straight line is looking at things the wrong way round. Perhaps a better way would be to say that Euclidean geometry was designed to conform to our sensory experience of the physical world: the length of string joining two points is minimized by stretching the string, and at that point, it happens to look/feel straight. I'm just wondering whether people would agree with this, and hoping that I may get some additional or deeper insights. Perhaps an interesting question to ask to try to go deeper would be: why does a stretched string look and feel straight ? *: To illustrate my point further, imagine we had chosen to define the length of a line as the Manhattan distance between its endpoints. We could integrate again, and this time it would turn out that the length of any curve between two points is the Manhattan distance between those points.","The first application I was shown of the calculus of variations was proving that the shortest distance between two points is a straight line. Define a functional measuring the length of a curve between two points: $$ I(y) = \int_{x_1}^{x_2} \sqrt{1 + (y')^2}\, dx, $$ apply the Euler-Langrange equation, and Bob's your uncle. So far so good, but then I started thinking: That functional was derived by splitting the curve into (infinitesimal) - wait for it - straight lines, and summing them up their lengths, and each length was defined as being the Euclidean distance between its endpoints*. As such, it seems to me that the proof, while correct, is rather meaningless. It's an obvious consequence of the facts that (a) the Euclidean norm satisfies the triangle inequality and (b) the length of a curve was defined as a sum of Euclidean norms. Getting slightly philosophical, I would conjecture that proving that the shortest distance between two points is a straight line is looking at things the wrong way round. Perhaps a better way would be to say that Euclidean geometry was designed to conform to our sensory experience of the physical world: the length of string joining two points is minimized by stretching the string, and at that point, it happens to look/feel straight. I'm just wondering whether people would agree with this, and hoping that I may get some additional or deeper insights. Perhaps an interesting question to ask to try to go deeper would be: why does a stretched string look and feel straight ? *: To illustrate my point further, imagine we had chosen to define the length of a line as the Manhattan distance between its endpoints. We could integrate again, and this time it would turn out that the length of any curve between two points is the Manhattan distance between those points.",,"['calculus', 'geometry', 'soft-question']"
56,What are the applications of the Mean Value Theorem?,What are the applications of the Mean Value Theorem?,,"I'm going through my first year of teaching AP Calculus.  One of the things I like to do is to impress upon my students why the topics I introduce are interesting and relevant to the big picture of understanding the nature of change. That being said, while I know that the Mean Value Theorem is one of the central facts in the study of calculus, I'm not really clear on why.  I feel that it's a bit like IVT for the derivatives of a continuous and differentiable function.  But I feel that the only thing I did with it when I studied Calc is to identify the point where the tangent was parallel to the secant of the endpoints.  If the class had been a little smaller or I had been a little bolder, I might have raised my hand and asked the professor ""So what?""  But I didn't, so here we are. To be clear, I am not at all arguing that MVT is not critical, so I don't plan on the answers being opinion-based.  But can you discuss some uses of MVT that justify the lofty place it has in the curriculum?","I'm going through my first year of teaching AP Calculus.  One of the things I like to do is to impress upon my students why the topics I introduce are interesting and relevant to the big picture of understanding the nature of change. That being said, while I know that the Mean Value Theorem is one of the central facts in the study of calculus, I'm not really clear on why.  I feel that it's a bit like IVT for the derivatives of a continuous and differentiable function.  But I feel that the only thing I did with it when I studied Calc is to identify the point where the tangent was parallel to the secant of the endpoints.  If the class had been a little smaller or I had been a little bolder, I might have raised my hand and asked the professor ""So what?""  But I didn't, so here we are. To be clear, I am not at all arguing that MVT is not critical, so I don't plan on the answers being opinion-based.  But can you discuss some uses of MVT that justify the lofty place it has in the curriculum?",,"['calculus', 'soft-question']"
57,The problem of instant velocity,The problem of instant velocity,,"The concept of velocity is by definition the movement divided by the interval of time between initial position and final position. If $f(t)$ is the position of a particle at time $t$; the velocity in the interval $[t_0;t_1]$ is $\dfrac{f(t_1)-f(t_0)}{t_1-t_0}$ The problem is that in a single instant there is no movement and the time is not changed; so no velocity. I can consider $\lim_{t_1 \to t_0} \dfrac{f(t_1)-f(t_0)}{t_1-t_0}$, but mathematically it is only the limit of average velocity function and doesn't represent velocity at instant $t_0$ What are your views about this problem ?","The concept of velocity is by definition the movement divided by the interval of time between initial position and final position. If $f(t)$ is the position of a particle at time $t$; the velocity in the interval $[t_0;t_1]$ is $\dfrac{f(t_1)-f(t_0)}{t_1-t_0}$ The problem is that in a single instant there is no movement and the time is not changed; so no velocity. I can consider $\lim_{t_1 \to t_0} \dfrac{f(t_1)-f(t_0)}{t_1-t_0}$, but mathematically it is only the limit of average velocity function and doesn't represent velocity at instant $t_0$ What are your views about this problem ?",,"['calculus', 'soft-question', 'intuition', 'philosophy']"
58,"Closed form for $\int_0^\infty\arctan\Bigl(\frac{2\pi}{x-\ln\,x+\ln(\frac\pi2)}\Bigr)\frac{dx}{x+1}$ [closed]",Closed form for  [closed],"\int_0^\infty\arctan\Bigl(\frac{2\pi}{x-\ln\,x+\ln(\frac\pi2)}\Bigr)\frac{dx}{x+1}","Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 5 years ago . Improve this question I'm trying to find a closed form for this integral: $$I=\int_0^\infty\arctan\left(\frac{2\pi}{x-\ln\,x+\ln\left(\frac\pi2\right)}\right)\frac{dx}{x+1}$$ Its approximate numeric value is $$I\approx3.3805825284453469793953592216276992165696856825906055108192183...$$ Any help is appreciated. Thanks!","Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 5 years ago . Improve this question I'm trying to find a closed form for this integral: $$I=\int_0^\infty\arctan\left(\frac{2\pi}{x-\ln\,x+\ln\left(\frac\pi2\right)}\right)\frac{dx}{x+1}$$ Its approximate numeric value is $$I\approx3.3805825284453469793953592216276992165696856825906055108192183...$$ Any help is appreciated. Thanks!",,"['calculus', 'integration', 'definite-integrals', 'logarithms', 'closed-form']"
59,What is the difference between partial and normal derivatives?,What is the difference between partial and normal derivatives?,,"I have a clarifying question about this question: What is the difference between $d$ and $\partial$? I understand the idea that $\frac{d}{dx}$ is the derivative where all variables are assumed to be functions of other variables, while with $\frac{\partial}{\partial x}$ one assumes that $x$ is the only variable and every thing else is a constant (as stated in one of the answers). Example 1 : If $z = xa + x$, then I would guess that $$ \frac{\partial z}{\partial x} = a + 1 $$ and $$ \frac{d z}{d x} = a + x\frac{da}{dx} + 1. $$ since now $a$ should be considered a function. When we in calculus 1 have $y = ax^2 + bx + c$, then technically we should use $\partial$ as we are assuming $a, b$, and $c$ are constants? Is this correct? Example 2 : Maybe the thing that is confusing me is that when we do implicit differentiation we use $d$. So if $$ x^2 + y^2 = 1 $$ then taking $\frac{d}{dx}$ gives $$ 2x + 2y\frac{dy}{dx} = 0 $$ again because $y$ is considered a function. How would taking $\frac{\partial}{\partial x}$ of an equation like $x^2 + y^2 =1$ work? Does that even make sense? Example 3 : Is it ever possible that using $\partial$ and $d$ can give the same? If, for example $y = x^2$, does it make sense to say that $$ \frac{\partial}{\partial x} y = 2x? $$ Edit: My overall question, I guess, is how the notations of partial derivatives vs. ordinary derivatives are formally defined. I am looking for a bit more background.","I have a clarifying question about this question: What is the difference between $d$ and $\partial$? I understand the idea that $\frac{d}{dx}$ is the derivative where all variables are assumed to be functions of other variables, while with $\frac{\partial}{\partial x}$ one assumes that $x$ is the only variable and every thing else is a constant (as stated in one of the answers). Example 1 : If $z = xa + x$, then I would guess that $$ \frac{\partial z}{\partial x} = a + 1 $$ and $$ \frac{d z}{d x} = a + x\frac{da}{dx} + 1. $$ since now $a$ should be considered a function. When we in calculus 1 have $y = ax^2 + bx + c$, then technically we should use $\partial$ as we are assuming $a, b$, and $c$ are constants? Is this correct? Example 2 : Maybe the thing that is confusing me is that when we do implicit differentiation we use $d$. So if $$ x^2 + y^2 = 1 $$ then taking $\frac{d}{dx}$ gives $$ 2x + 2y\frac{dy}{dx} = 0 $$ again because $y$ is considered a function. How would taking $\frac{\partial}{\partial x}$ of an equation like $x^2 + y^2 =1$ work? Does that even make sense? Example 3 : Is it ever possible that using $\partial$ and $d$ can give the same? If, for example $y = x^2$, does it make sense to say that $$ \frac{\partial}{\partial x} y = 2x? $$ Edit: My overall question, I guess, is how the notations of partial derivatives vs. ordinary derivatives are formally defined. I am looking for a bit more background.",,"['calculus', 'derivatives', 'partial-derivative']"
60,A closed form of $\int_0^\infty\frac{\sqrt[\phi]{x}\ \arctan x}{\left(x^\phi+1\right)^2}dx$,A closed form of,\int_0^\infty\frac{\sqrt[\phi]{x}\ \arctan x}{\left(x^\phi+1\right)^2}dx,"Is it possible to evaluate the following integral in a closed form? $$\int_0^\infty\frac{\sqrt[\phi]{x}\ \arctan x}{\left(x^\phi+1\right)^2}dx,$$ where $\phi$ is the golden ratio : $$\phi=\frac{1+\sqrt{5}}{2}.$$","Is it possible to evaluate the following integral in a closed form? $$\int_0^\infty\frac{\sqrt[\phi]{x}\ \arctan x}{\left(x^\phi+1\right)^2}dx,$$ where $\phi$ is the golden ratio : $$\phi=\frac{1+\sqrt{5}}{2}.$$",,"['calculus', 'integration', 'definite-integrals', 'closed-form', 'golden-ratio']"
61,Series that converge to $\pi$ quickly,Series that converge to  quickly,\pi,"I know the series, $4-{4\over3}+{4\over5}-{4\over7}...$ converges to $\pi$ but I have heard many people say that while this is a classic example, there are series that converge much faster.  Does anyone know of any?","I know the series, $4-{4\over3}+{4\over5}-{4\over7}...$ converges to $\pi$ but I have heard many people say that while this is a classic example, there are series that converge much faster.  Does anyone know of any?",,"['calculus', 'sequences-and-series', 'convergence-divergence', 'pi']"
62,Why isn't the directional derivative generally scaled down to the unit vector?,Why isn't the directional derivative generally scaled down to the unit vector?,,"I'm starting to learn how to intuitively interpret the directional derivative, and I can't understand why you wouldn't scale down your direction vector $\vec{v}$ to be a unit vector. Currently, my intuition is the idea of slicing the 3D graph of the function along its direction vector and then computing the slope of the curve created by the intersection of the plane. But I can't really understand how the directional derivative would be a directional derivative if it were not scaled down to be a change in unit length in the direction of $\vec{v}$. Is there an intuitive understanding I can grasp onto? I'm just starting out so maybe I haven't gotten there yet. Note, I think there may be a nice analogy to linearization, like if you take ""twice as big of a step"" in the direction of $\vec{v}$ , then the change to the function due to the change in this step is twice as big. Is this an okay way to think about it?","I'm starting to learn how to intuitively interpret the directional derivative, and I can't understand why you wouldn't scale down your direction vector $\vec{v}$ to be a unit vector. Currently, my intuition is the idea of slicing the 3D graph of the function along its direction vector and then computing the slope of the curve created by the intersection of the plane. But I can't really understand how the directional derivative would be a directional derivative if it were not scaled down to be a change in unit length in the direction of $\vec{v}$. Is there an intuitive understanding I can grasp onto? I'm just starting out so maybe I haven't gotten there yet. Note, I think there may be a nice analogy to linearization, like if you take ""twice as big of a step"" in the direction of $\vec{v}$ , then the change to the function due to the change in this step is twice as big. Is this an okay way to think about it?",,"['calculus', 'multivariable-calculus', 'derivatives', 'vector-analysis']"
63,Is positive the same as non-negative?,Is positive the same as non-negative?,,"I would assume the answer to my question is yes, but I want to make sure because my book uses both terminologies.  Please also indicate where zero falls into the mix. UPDATE: Here is an excerpt from my book: The definition of $\Theta(g(n))$ requires   that every member $f(n) \in \Theta(g(n))$ be   asymptotically non-negative , that is,   that $f(n)$ be non-negative whenever n   is sufficiently large. (An   asymptotically positive function is   one that is positive for all   sufficiently large $n$.)","I would assume the answer to my question is yes, but I want to make sure because my book uses both terminologies.  Please also indicate where zero falls into the mix. UPDATE: Here is an excerpt from my book: The definition of $\Theta(g(n))$ requires   that every member $f(n) \in \Theta(g(n))$ be   asymptotically non-negative , that is,   that $f(n)$ be non-negative whenever n   is sufficiently large. (An   asymptotically positive function is   one that is positive for all   sufficiently large $n$.)",,"['calculus', 'statistics', 'terminology']"
64,How to show that $\int_0^1 \left(\sqrt[3]{1-x^7} - \sqrt[7]{1-x^3}\right)\;dx = 0$,How to show that,\int_0^1 \left(\sqrt[3]{1-x^7} - \sqrt[7]{1-x^3}\right)\;dx = 0,"Evaluate the integral: $$ \int_0^1 \left(\sqrt[3]{1-x^7} - \sqrt[7]{1-x^3}\right)\;dx$$ The answer is $0,$ but I am unable to get it. There is some symmetry I can not see.","Evaluate the integral: $$ \int_0^1 \left(\sqrt[3]{1-x^7} - \sqrt[7]{1-x^3}\right)\;dx$$ The answer is $0,$ but I am unable to get it. There is some symmetry I can not see.",,"['calculus', 'integration']"
65,How did Newton find derivative of basic functions before formulating systematic Calculus?,How did Newton find derivative of basic functions before formulating systematic Calculus?,,I think I read somewhere that Newton tried to find derivatives of basic functions like $x^2$ before formulating systematic calculus; how did/would he do it?,I think I read somewhere that Newton tried to find derivatives of basic functions like $x^2$ before formulating systematic calculus; how did/would he do it?,,"['calculus', 'derivatives', 'math-history']"
66,Find the value of $\int_0^{\infty}\frac{x^3}{(x^4+1)(e^x-1)}\mathrm dx$,Find the value of,\int_0^{\infty}\frac{x^3}{(x^4+1)(e^x-1)}\mathrm dx,I need to find a closed-form for the following integral. Please give me some ideas how to approach it: $$\int_0^{\infty}\frac{x^3}{(x^4+1)(e^x-1)}\mathrm dx$$,I need to find a closed-form for the following integral. Please give me some ideas how to approach it: $$\int_0^{\infty}\frac{x^3}{(x^4+1)(e^x-1)}\mathrm dx$$,,"['calculus', 'integration', 'definite-integrals', 'improper-integrals', 'closed-form']"
67,"Prove ${\large\int}_0^\infty\frac{\ln x}{\sqrt{x}\ \sqrt{x+1}\ \sqrt{2x+1}}dx\stackrel?=\frac{\pi^{3/2}\,\ln2}{2^{3/2}\Gamma^2\left(\tfrac34\right)}$",Prove,"{\large\int}_0^\infty\frac{\ln x}{\sqrt{x}\ \sqrt{x+1}\ \sqrt{2x+1}}dx\stackrel?=\frac{\pi^{3/2}\,\ln2}{2^{3/2}\Gamma^2\left(\tfrac34\right)}","I discovered the following conjecture by evaluating the integral numerically and then using some inverse symbolic calculation methods to find a possible closed form: $$\int_0^\infty\frac{\ln x}{\sqrt{x\vphantom{1}}\ \sqrt{x+1}\ \sqrt{2x+1}}dx\stackrel{\color{#808080}?}=\frac{\pi^{3/2}\,\ln2}{2^{3/2}\,\Gamma^2\left(\tfrac34\right)}.\tag1$$ The equality holds numerically with a precision of at least $1000$ decimal digits. But so far I was not able to find a proof of it. Because the integral can be represented as a derivative of a hypergeometic function with respect to its parameter, the conjecture can be rewritten as $$\frac{d}{da}{_2F_1}\left(a,\ \tfrac12;\ 1;\ \tfrac12\right)\Bigg|_{a=\frac12}\stackrel{\color{#808080}?}=\frac{\sqrt\pi\,\ln2}{2\,\Gamma^2\left(\tfrac34\right)}\tag2$$ or, using a series expansion of the hypergeometric function, as $${\large\sum}_{n=0}^\infty\frac{H_{n-\frac12}\ \Gamma^2\left(n+\tfrac12\right)}{2^n\ \Gamma^2\left(n+1\right)}\stackrel{\color{#808080}?}=-\frac{3\,\pi^{3/2}\,\ln2}{2\,\Gamma^2\left(\tfrac34\right)}\tag3,$$ where $H_q$ is the generalized harmonic number , $H_q=\gamma+\psi_0\left(q+1\right).$ Could you suggest any ideas how to prove this?","I discovered the following conjecture by evaluating the integral numerically and then using some inverse symbolic calculation methods to find a possible closed form: $$\int_0^\infty\frac{\ln x}{\sqrt{x\vphantom{1}}\ \sqrt{x+1}\ \sqrt{2x+1}}dx\stackrel{\color{#808080}?}=\frac{\pi^{3/2}\,\ln2}{2^{3/2}\,\Gamma^2\left(\tfrac34\right)}.\tag1$$ The equality holds numerically with a precision of at least $1000$ decimal digits. But so far I was not able to find a proof of it. Because the integral can be represented as a derivative of a hypergeometic function with respect to its parameter, the conjecture can be rewritten as $$\frac{d}{da}{_2F_1}\left(a,\ \tfrac12;\ 1;\ \tfrac12\right)\Bigg|_{a=\frac12}\stackrel{\color{#808080}?}=\frac{\sqrt\pi\,\ln2}{2\,\Gamma^2\left(\tfrac34\right)}\tag2$$ or, using a series expansion of the hypergeometric function, as $${\large\sum}_{n=0}^\infty\frac{H_{n-\frac12}\ \Gamma^2\left(n+\tfrac12\right)}{2^n\ \Gamma^2\left(n+1\right)}\stackrel{\color{#808080}?}=-\frac{3\,\pi^{3/2}\,\ln2}{2\,\Gamma^2\left(\tfrac34\right)}\tag3,$$ where $H_q$ is the generalized harmonic number , $H_q=\gamma+\psi_0\left(q+1\right).$ Could you suggest any ideas how to prove this?",,"['calculus', 'sequences-and-series', 'definite-integrals', 'improper-integrals', 'hypergeometric-function']"
68,"Integral $\int_0^\infty\left(x+5\,x^5\right)\operatorname{erfc}\left(x+x^5\right)\,dx$",Integral,"\int_0^\infty\left(x+5\,x^5\right)\operatorname{erfc}\left(x+x^5\right)\,dx","Is it possible to find a closed form (possibly using known special functions) for this integral? $$\int_0^\infty\left(5\,x^5+x\right)\operatorname{erfc}\left(x^5+x\right)\,dx$$ where $\operatorname{erfc}$ is the complementary error function $$\operatorname{erfc} x=\frac{2}{\sqrt{\pi}}\int_x^{\infty}e^{-z^2}dz.$$","Is it possible to find a closed form (possibly using known special functions) for this integral? $$\int_0^\infty\left(5\,x^5+x\right)\operatorname{erfc}\left(x^5+x\right)\,dx$$ where $\operatorname{erfc}$ is the complementary error function $$\operatorname{erfc} x=\frac{2}{\sqrt{\pi}}\int_x^{\infty}e^{-z^2}dz.$$",,"['calculus', 'integration', 'definite-integrals', 'closed-form', 'error-function']"
69,"Function which is continuous everywhere in its domain, but differentiable only at one point","Function which is continuous everywhere in its domain, but differentiable only at one point",,"I am new in this forum.  My question: Suppose a real valued function $f: \mathbb{R} \rightarrow \mathbb{R}$ is continuous everywhere. Is it possible to construct $f$ that is differentiable at only one point? If possible, please give an example. Note:  I am aware that there is a function which is differentiable at a single point but discontinuous elsewhere. I also know about Weierstrass function that continuous everywhere but nowhere differentiable. But is there a function which is continuous but only differentiable in one point? In fact, I found this discussion but unfortunately it still does not give a definitive answer. Moreover they consider only in an interval, whereas my problem is for the entire domain. Thank you very much","I am new in this forum.  My question: Suppose a real valued function $f: \mathbb{R} \rightarrow \mathbb{R}$ is continuous everywhere. Is it possible to construct $f$ that is differentiable at only one point? If possible, please give an example. Note:  I am aware that there is a function which is differentiable at a single point but discontinuous elsewhere. I also know about Weierstrass function that continuous everywhere but nowhere differentiable. But is there a function which is continuous but only differentiable in one point? In fact, I found this discussion but unfortunately it still does not give a definitive answer. Moreover they consider only in an interval, whereas my problem is for the entire domain. Thank you very much",,['calculus']
70,Proving $\lim_{x \to 0+} \sum_{n=0}^\infty \frac{(-1)^n}{n!^x} = \frac{1}{2}$,Proving,\lim_{x \to 0+} \sum_{n=0}^\infty \frac{(-1)^n}{n!^x} = \frac{1}{2},Prove that $$ \lim_{x \to 0+} \sum_{n=0}^\infty \frac{(-1)^n}{n!^x} =  \frac{1}{2}. $$ We know that $$ \sum_{n=0}^\infty \frac{(-1)^n}{n!^x}$$ converges for any $x>0$. So I try to evaluate the limit as $x$ approaches $0$ numerically. It seems that the limit approaches $\displaystyle \frac{1}{2}$. I know that $$\sum_{n=0}^\infty \frac{(-1)^n}{n!} = \frac{1}{e}.$$ Does it help to solve this problem?,Prove that $$ \lim_{x \to 0+} \sum_{n=0}^\infty \frac{(-1)^n}{n!^x} =  \frac{1}{2}. $$ We know that $$ \sum_{n=0}^\infty \frac{(-1)^n}{n!^x}$$ converges for any $x>0$. So I try to evaluate the limit as $x$ approaches $0$ numerically. It seems that the limit approaches $\displaystyle \frac{1}{2}$. I know that $$\sum_{n=0}^\infty \frac{(-1)^n}{n!} = \frac{1}{e}.$$ Does it help to solve this problem?,,"['calculus', 'limits', 'summation']"
71,Fourier series of Log sine and Log cos,Fourier series of Log sine and Log cos,,I saw the two identities  $$ -\log(\sin(x))=\sum_{k=1}^\infty\frac{\cos(2kx)}{k}+\log(2) $$ and $$ -\log(\cos(x))=\sum_{k=1}^\infty(-1)^k\frac{\cos(2kx)}{k}+\log(2) $$ here: twist on classic log of sine and cosine integral .  How can one prove these two identities?,I saw the two identities  $$ -\log(\sin(x))=\sum_{k=1}^\infty\frac{\cos(2kx)}{k}+\log(2) $$ and $$ -\log(\cos(x))=\sum_{k=1}^\infty(-1)^k\frac{\cos(2kx)}{k}+\log(2) $$ here: twist on classic log of sine and cosine integral .  How can one prove these two identities?,,"['calculus', 'integration']"
72,How to evaluate $\sum_{k=3}^{\infty} \frac{\ln (k)}{k^2 - 4}$?,How to evaluate ?,\sum_{k=3}^{\infty} \frac{\ln (k)}{k^2 - 4},"Is it possible to evaluate the sum: $$\sum_{k=3}^{\infty} \frac{\ln (k)}{k^2 - 4}$$ I expect it may be related to $\zeta^{\prime} (2)$ : $$\zeta^{\prime} (2) = - \sum_{k=2}^{\infty} \frac{\ln(k)}{k^2}$$ Is there an identity that works for my series, involving the natural logarithm, that is similar to the identity that: $$\sum_{n=0}^{\infty} \frac{1}{(n+a)(n+b)} = \frac{\psi(a) - \psi(b)}{a-b}$$ Also potentially related, the Lüroth analogue of Khintchine’s constant can be defined as the following: $$\sum_{n=1}^{\infty} \frac{\ln (n)}{n(n+1)}$$ as mentioned here . After some work, the following can be shown: $$\sum_{k=3}^{\infty} \frac{\ln (k)}{k^2 - 4} = \frac{5\ln(2) + 4\ln(3)}{16} + \frac{1}{2} \sum_{k=3}^{\infty} \frac{1}{k} \text{tanh}^{-1} \left( \frac{2}{k} \right)$$ and furthermore: $$\sum_{k=3}^{\infty} \frac{1}{k} \text{tanh}^{-1} \left( \frac{2}{k} \right) = \int_{0}^{2} \left( \frac{\left(1-\pi x \cot(\pi x) \right)}{2x^2} + \frac{1}{x^2 - 1} + \frac{1}{x^2 -4} \right) \, dx$$ EDIT I have derived yet another form for my sum of interest, however, I found this one interesting as it seems like it could potentially be solvable? $$\sum_{k=3}^{\infty} \frac{\ln (k)}{k^2 - 4} = \int_{0}^{\infty} \left( \frac{\psi^{(0)} (s+3) + \gamma}{(s+2)(s-2)} - \frac{25}{16 (s-2)(s+1)} \right) \, ds$$ From this, it is possible to obtain the following: $$\sum_{k=3}^{\infty} \frac{\ln (k)}{k^2 - 4} = \frac{\pi \gamma}{4} i + \frac{25}{48} (\ln (2) - i \pi) - \frac{1}{8} + \frac{1}{16} i \pi + \frac{1}{4} \int_{0}^{i \pi} \psi^{(0)} \left( \frac{4}{1+ e^{u}} \right)  \, du$$ $$\sum_{k=3}^{\infty} \frac{\ln (k)}{k^2 - 4} = \frac{\pi \gamma}{4}i+\frac{25}{48} (\ln (2)-i \pi )+\frac{7 i \pi }{48}-\frac{1}{8}-\frac{\ln (2)}{3} -2 \int_0^{\infty } \frac{t \ln (\Gamma (1-i t))}{\left(t^2+4\right)^2} \, dt$$ $$\sum_{k=3}^{\infty} \frac{\ln (k)}{k^2 - 4} = -\frac{1}{8}-\frac{i \pi }{4}+\frac{i \gamma  \pi }{4}-\frac{\ln (2)}{16} - 2 \int_{0}^{\infty} \frac{t \ln (\Gamma (-i t)) }{(4+t^2)^2} \, dt$$ $$\implies \sum_{k=3}^{\infty} \frac{\ln (k)}{k^2 - 4} =\frac{25}{48} \ln (2) -\frac{1}{8} + \int_{1}^{\infty} \frac{\ln (v-1) \text{li} (v^2)}{v^5} \, dv$$ Where $\text{li}$ is the logarithmic integral function. $$\sum_{k=3}^{\infty} \frac{\ln(k)}{k^2-4} = \frac{3 \ln (2)}{16} - \frac{\pi^2+1}{8} - \frac{\pi}{2} \int_{0}^{\infty} \sin(4\pi x) (\psi (x) - \ln (x)) \, dx$$","Is it possible to evaluate the sum: I expect it may be related to : Is there an identity that works for my series, involving the natural logarithm, that is similar to the identity that: Also potentially related, the Lüroth analogue of Khintchine’s constant can be defined as the following: as mentioned here . After some work, the following can be shown: and furthermore: EDIT I have derived yet another form for my sum of interest, however, I found this one interesting as it seems like it could potentially be solvable? From this, it is possible to obtain the following: Where is the logarithmic integral function.","\sum_{k=3}^{\infty} \frac{\ln (k)}{k^2 - 4} \zeta^{\prime} (2) \zeta^{\prime} (2) = - \sum_{k=2}^{\infty} \frac{\ln(k)}{k^2} \sum_{n=0}^{\infty} \frac{1}{(n+a)(n+b)} = \frac{\psi(a) - \psi(b)}{a-b} \sum_{n=1}^{\infty} \frac{\ln (n)}{n(n+1)} \sum_{k=3}^{\infty} \frac{\ln (k)}{k^2 - 4} = \frac{5\ln(2) + 4\ln(3)}{16} + \frac{1}{2} \sum_{k=3}^{\infty} \frac{1}{k} \text{tanh}^{-1} \left( \frac{2}{k} \right) \sum_{k=3}^{\infty} \frac{1}{k} \text{tanh}^{-1} \left( \frac{2}{k} \right) = \int_{0}^{2} \left( \frac{\left(1-\pi x \cot(\pi x) \right)}{2x^2} + \frac{1}{x^2 - 1} + \frac{1}{x^2 -4} \right) \, dx \sum_{k=3}^{\infty} \frac{\ln (k)}{k^2 - 4} = \int_{0}^{\infty} \left( \frac{\psi^{(0)} (s+3) + \gamma}{(s+2)(s-2)} - \frac{25}{16 (s-2)(s+1)} \right) \, ds \sum_{k=3}^{\infty} \frac{\ln (k)}{k^2 - 4} = \frac{\pi \gamma}{4} i + \frac{25}{48} (\ln (2) - i \pi) - \frac{1}{8} + \frac{1}{16} i \pi + \frac{1}{4} \int_{0}^{i \pi} \psi^{(0)} \left( \frac{4}{1+ e^{u}} \right)  \, du \sum_{k=3}^{\infty} \frac{\ln (k)}{k^2 - 4} = \frac{\pi \gamma}{4}i+\frac{25}{48} (\ln (2)-i \pi )+\frac{7 i \pi }{48}-\frac{1}{8}-\frac{\ln (2)}{3} -2 \int_0^{\infty } \frac{t \ln (\Gamma (1-i t))}{\left(t^2+4\right)^2} \, dt \sum_{k=3}^{\infty} \frac{\ln (k)}{k^2 - 4} = -\frac{1}{8}-\frac{i \pi }{4}+\frac{i \gamma  \pi }{4}-\frac{\ln (2)}{16} - 2 \int_{0}^{\infty} \frac{t \ln (\Gamma (-i t)) }{(4+t^2)^2} \, dt \implies \sum_{k=3}^{\infty} \frac{\ln (k)}{k^2 - 4} =\frac{25}{48} \ln (2) -\frac{1}{8} + \int_{1}^{\infty} \frac{\ln (v-1) \text{li} (v^2)}{v^5} \, dv \text{li} \sum_{k=3}^{\infty} \frac{\ln(k)}{k^2-4} = \frac{3 \ln (2)}{16} - \frac{\pi^2+1}{8} - \frac{\pi}{2} \int_{0}^{\infty} \sin(4\pi x) (\psi (x) - \ln (x)) \, dx","['calculus', 'sequences-and-series', 'analysis', 'definite-integrals', 'closed-form']"
73,Why do we use big Oh in taylor series?,Why do we use big Oh in taylor series?,,"In the taylor series for sin(x), we write: $$ \sin{x} = x - \frac{x^3}{6} + \frac{x^5}{120} + O(x^7) $$ Meaning that $\sin{x} = x - \frac{x^3}{6} + \frac{x^5}{120}$ and terms of order $x^7$ and higher, so we say that those 'higher order terms' are equal to $O(x^7)$. However, according to wikipedia, the definition of $f(x) = O(g(x))$ is that for all $x > x_o$ for some $x_o$, $\frac{|f(x)|}{|g(x)|} < M $ for some constant M. According to this definition, the terms after the $x^7$th term in the taylor expansion of $\sin{x}$ are /not/ $O(x^7)$, because as $x$ approaches infinity, the higher order terms should dominate the $O(x^7)$ term, not be bounded by it. Am I missing something here?","In the taylor series for sin(x), we write: $$ \sin{x} = x - \frac{x^3}{6} + \frac{x^5}{120} + O(x^7) $$ Meaning that $\sin{x} = x - \frac{x^3}{6} + \frac{x^5}{120}$ and terms of order $x^7$ and higher, so we say that those 'higher order terms' are equal to $O(x^7)$. However, according to wikipedia, the definition of $f(x) = O(g(x))$ is that for all $x > x_o$ for some $x_o$, $\frac{|f(x)|}{|g(x)|} < M $ for some constant M. According to this definition, the terms after the $x^7$th term in the taylor expansion of $\sin{x}$ are /not/ $O(x^7)$, because as $x$ approaches infinity, the higher order terms should dominate the $O(x^7)$ term, not be bounded by it. Am I missing something here?",,"['calculus', 'notation', 'asymptotics', 'taylor-expansion']"
74,Can you define arc length using a piece of string?,Can you define arc length using a piece of string?,,"In calculus, how we calculate the arc length of a curve is by approximating the curve with a series of line segments, and then we take the limit as the number of line segments goes to infinity.  This is a perfectly valid approach to calculating arc length, and obviously it will allow you calculate correctly the length of any (rectifiable) curve.  But it's obviously not the way people intuitively think about the length of a curve. Here is how they introduced arclength to us in elementary school.  If you want to measure the length of a straight line segment, use a ruler.  If you want to measure the length of a curve, overlay the curve with a piece of string, then straighten the string and measure it with a ruler. So I was wondering if it's possible to make a definition of arc length that preserves the spirit of that definition.  Without using the calculus-based definition of length, is there any way to define what it means for one curve to be a ""length-preserving deformation"" of another curve?  If that's possible, we could construct equivalence classes of curves that are length-preserving deformations of one another, and we can define the length associated with an equivalence class to be the length of the straight line that's in the class. Is there anything in topology that would allow us to make such a definition?  We'd need to account for the Euclidean metric somehow, since, e.g. in Taxicab geometry the circumference of a circle is $8r$ rather than $2\pi r$ (which is why your friends keep sending you that dumb $\pi = 4$ picture). Any help would be greatly appreciated. Thank You in Advance.","In calculus, how we calculate the arc length of a curve is by approximating the curve with a series of line segments, and then we take the limit as the number of line segments goes to infinity.  This is a perfectly valid approach to calculating arc length, and obviously it will allow you calculate correctly the length of any (rectifiable) curve.  But it's obviously not the way people intuitively think about the length of a curve. Here is how they introduced arclength to us in elementary school.  If you want to measure the length of a straight line segment, use a ruler.  If you want to measure the length of a curve, overlay the curve with a piece of string, then straighten the string and measure it with a ruler. So I was wondering if it's possible to make a definition of arc length that preserves the spirit of that definition.  Without using the calculus-based definition of length, is there any way to define what it means for one curve to be a ""length-preserving deformation"" of another curve?  If that's possible, we could construct equivalence classes of curves that are length-preserving deformations of one another, and we can define the length associated with an equivalence class to be the length of the straight line that's in the class. Is there anything in topology that would allow us to make such a definition?  We'd need to account for the Euclidean metric somehow, since, e.g. in Taxicab geometry the circumference of a circle is $8r$ rather than $2\pi r$ (which is why your friends keep sending you that dumb $\pi = 4$ picture). Any help would be greatly appreciated. Thank You in Advance.",,"['calculus', 'general-topology', 'geometry', 'metric-spaces', 'arc-length']"
75,I'm looking for some mathematics that will challenge me as a year $12$ student. [closed],I'm looking for some mathematics that will challenge me as a year  student. [closed],12,"Closed . This question needs to be more focused . It is not currently accepting answers. Want to improve this question? Update the question so it focuses on one problem only by editing this post . Closed 7 years ago . Improve this question I am an upcoming year $12$ student, school holidays are coming up in a few days and I've realised I'm probably going to be extremely bored. So I'm looking for some suggestions. I want a challenge, some mathematics that I can attempt to learn/master. Obviously nothing impossible, but mathematics is my number $1$ favorite thing and I really want something to keep me busy and something that can further my understanding of mathematics. Also I would be interested in any mathematical focused book suggestions. So far in school I've done the usual: Matrices, transformation matrices, Sine Cosine and Tangent (graphs and proofs), lots and lots of parabolas/quadratics, statistics, growth and decay, calculus intro, Calculus derivation and integration, vectors, proof by induction and complex numbers. Any suggestions would be heavily appreciated.","Closed . This question needs to be more focused . It is not currently accepting answers. Want to improve this question? Update the question so it focuses on one problem only by editing this post . Closed 7 years ago . Improve this question I am an upcoming year $12$ student, school holidays are coming up in a few days and I've realised I'm probably going to be extremely bored. So I'm looking for some suggestions. I want a challenge, some mathematics that I can attempt to learn/master. Obviously nothing impossible, but mathematics is my number $1$ favorite thing and I really want something to keep me busy and something that can further my understanding of mathematics. Also I would be interested in any mathematical focused book suggestions. So far in school I've done the usual: Matrices, transformation matrices, Sine Cosine and Tangent (graphs and proofs), lots and lots of parabolas/quadratics, statistics, growth and decay, calculus intro, Calculus derivation and integration, vectors, proof by induction and complex numbers. Any suggestions would be heavily appreciated.",,"['calculus', 'self-learning', 'education', 'learning']"
76,Simpler way to compute a definite integral without resorting to partial fractions?,Simpler way to compute a definite integral without resorting to partial fractions?,,"I found the method of partial fractions very laborious to solve this definite integral : $$\int_0^\infty \frac{\sqrt[3]{x}}{1 + x^2}\,dx$$ Is there a simpler way to do this ?","I found the method of partial fractions very laborious to solve this definite integral : $$\int_0^\infty \frac{\sqrt[3]{x}}{1 + x^2}\,dx$$ Is there a simpler way to do this ?",,"['calculus', 'integration', 'definite-integrals', 'improper-integrals']"
77,"Integrate $\int_0^{\pi/2} \frac{1}{1+\tan^\alpha{x}}\,\mathrm{d}x$",Integrate,"\int_0^{\pi/2} \frac{1}{1+\tan^\alpha{x}}\,\mathrm{d}x","Evaluate the integral $$\int_0^{\pi/2} \frac{1}{1+\tan^\alpha{x}}\,\mathrm{d}x$$","Evaluate the integral $$\int_0^{\pi/2} \frac{1}{1+\tan^\alpha{x}}\,\mathrm{d}x$$",,"['calculus', 'integration', 'trigonometry', 'definite-integrals']"
78,"How can a structure have infinite length and infinite surface area, but have finite volume?","How can a structure have infinite length and infinite surface area, but have finite volume?",,"Consider the curve $\frac{1}{x}$  where $x \geq 1$. Rotate this curve around the x-axis. One Dimension - Clearly this structure is infinitely long. Two Dimensions - Surface Area = $2\pi\int_∞^1\frac{1}{x}dx = 2\pi(\ln ∞ - \ln 1) = ∞$ Three Dimensions -  Volume = $\pi\int_∞^1{x}^{-2}dx = \pi(-\frac{1}{∞} + \frac{1}{1}) = \pi$ So this structure has infinite length and infinite surface area. However it has finite volume, which just does not make sense. Even more interesting, the ""walls"" of this structure are infinitely thin. Since the volume is finite, we could fill this structure with a finite amount of paint. To fill the structure the paint would need to cover the complete surface area of the inside of this structure. Since the ""walls"" are infinitely thin, why would a finite amount of paint not be able to cover the outside of the ""walls"" too? Please help me make sense of this whole thing.","Consider the curve $\frac{1}{x}$  where $x \geq 1$. Rotate this curve around the x-axis. One Dimension - Clearly this structure is infinitely long. Two Dimensions - Surface Area = $2\pi\int_∞^1\frac{1}{x}dx = 2\pi(\ln ∞ - \ln 1) = ∞$ Three Dimensions -  Volume = $\pi\int_∞^1{x}^{-2}dx = \pi(-\frac{1}{∞} + \frac{1}{1}) = \pi$ So this structure has infinite length and infinite surface area. However it has finite volume, which just does not make sense. Even more interesting, the ""walls"" of this structure are infinitely thin. Since the volume is finite, we could fill this structure with a finite amount of paint. To fill the structure the paint would need to cover the complete surface area of the inside of this structure. Since the ""walls"" are infinitely thin, why would a finite amount of paint not be able to cover the outside of the ""walls"" too? Please help me make sense of this whole thing.",,"['calculus', 'infinity', 'paradoxes']"
79,How can I answer this Putnam question more rigorously?,How can I answer this Putnam question more rigorously?,,"Given real numbers $a_0, a_1, ..., a_n$ such that $\dfrac {a_0}{1} + \dfrac {a_1}{2} + \cdots + \dfrac {a_n}{n+1}=0,$ prove that $a_0 + a_1 x + a_2 x^2 + \cdots + a_n x^n=0$ has at least one real solution. My solution: Let $$f(x) = a_0 + a_1 x + a_2 x^2 + \cdots + a_n x^n$$ $$\int f(x) = \dfrac {a_0}{1} x + \dfrac {a_1}{2}x^2 + \cdots + \dfrac {a_n}{n+1} x^{n+1} + C$$ $$\int_0^1 f(x) = \left[ \dfrac {a_0}{1} + \dfrac {a_1}{2} + \cdots + \dfrac {a_n}{n+1} \right]-0$$ $$\int_0^1 f(x) = 0$$ Since $f$ is continuous, by the area interpretation of integration, it must have at least one zero. My question is, is this rigorous enough? Do I need to prove the last statement, perhaps by contradiction using Riemann sums? Is this a theorem I can/should quote?","Given real numbers $a_0, a_1, ..., a_n$ such that $\dfrac {a_0}{1} + \dfrac {a_1}{2} + \cdots + \dfrac {a_n}{n+1}=0,$ prove that $a_0 + a_1 x + a_2 x^2 + \cdots + a_n x^n=0$ has at least one real solution. My solution: Let $$f(x) = a_0 + a_1 x + a_2 x^2 + \cdots + a_n x^n$$ $$\int f(x) = \dfrac {a_0}{1} x + \dfrac {a_1}{2}x^2 + \cdots + \dfrac {a_n}{n+1} x^{n+1} + C$$ $$\int_0^1 f(x) = \left[ \dfrac {a_0}{1} + \dfrac {a_1}{2} + \cdots + \dfrac {a_n}{n+1} \right]-0$$ $$\int_0^1 f(x) = 0$$ Since $f$ is continuous, by the area interpretation of integration, it must have at least one zero. My question is, is this rigorous enough? Do I need to prove the last statement, perhaps by contradiction using Riemann sums? Is this a theorem I can/should quote?",,"['calculus', 'proof-writing', 'contest-math']"
80,Using Integration By Parts results in 0 = 1,Using Integration By Parts results in 0 = 1,,"I've run into a strange situation while trying to apply Integration By Parts, and I can't seem to come up with an explanation.  I start with the following equation: $$\int \frac{1}{f} \frac{df}{dx} dx$$ I let: $$u = \frac{1}{f} \text{ and } dv = \frac{df}{dx} dx$$ Then I find: $$du = -\frac{1}{f^2} \frac{df}{dx} dx \text{ and } v = f$$ I can then substitute into the usual IBP formula: $$\int udv = uv - \int v du$$ $$\int \frac{1}{f} \frac{df}{dx} dx = \frac{1}{f} f - \int f \left(-\frac{1}{f^2} \frac{df}{dx}\right) dx$$ $$\int \frac{1}{f} \frac{df}{dx} dx = 1 + \int \frac{1}{f} \frac{df}{dx} dx$$ Then subtracting the integral from both sides, I've now shown that: $$0 = 1$$ Obviously there must be a problem in my derivation here...  What wrong assumption have I made, or what error have I made?  I'm baffled.","I've run into a strange situation while trying to apply Integration By Parts, and I can't seem to come up with an explanation.  I start with the following equation: $$\int \frac{1}{f} \frac{df}{dx} dx$$ I let: $$u = \frac{1}{f} \text{ and } dv = \frac{df}{dx} dx$$ Then I find: $$du = -\frac{1}{f^2} \frac{df}{dx} dx \text{ and } v = f$$ I can then substitute into the usual IBP formula: $$\int udv = uv - \int v du$$ $$\int \frac{1}{f} \frac{df}{dx} dx = \frac{1}{f} f - \int f \left(-\frac{1}{f^2} \frac{df}{dx}\right) dx$$ $$\int \frac{1}{f} \frac{df}{dx} dx = 1 + \int \frac{1}{f} \frac{df}{dx} dx$$ Then subtracting the integral from both sides, I've now shown that: $$0 = 1$$ Obviously there must be a problem in my derivation here...  What wrong assumption have I made, or what error have I made?  I'm baffled.",,"['calculus', 'integration', 'fake-proofs']"
81,Definite integrals solvable using the Feynman Trick,Definite integrals solvable using the Feynman Trick,,I'm looking for definite integrals that are solvable using the method of differentiation under the integral sign (also called the Feynman Trick) in order to practice using this technique. Does anyone know of any good ones to tackle?,I'm looking for definite integrals that are solvable using the method of differentiation under the integral sign (also called the Feynman Trick) in order to practice using this technique. Does anyone know of any good ones to tackle?,,"['calculus', 'integration']"
82,What is the length of a sine wave from $0$ to $2\pi$?,What is the length of a sine wave from  to ?,0 2\pi,"What is the length of a sine wave from $0$ to $2\pi$ ? Physically I would plot $$y=\sin(x),\quad 0\le x\le {2\pi}$$ and measure line length. I think part of the answer is to integrate this: $$ \int_0^{2\pi} \sqrt{ 1 + (\sin(x))^2} \, {\rm d}x $$ Any ideas?",What is the length of a sine wave from to ? Physically I would plot and measure line length. I think part of the answer is to integrate this: Any ideas?,"0 2\pi y=\sin(x),\quad 0\le x\le {2\pi} 
\int_0^{2\pi} \sqrt{ 1 + (\sin(x))^2} \, {\rm d}x
","['calculus', 'integration', 'trigonometry', 'arc-length']"
83,The entry-level PhD integral: $\int_0^\infty\frac{\sin 3x\sin 4x\sin5x\cos6x}{x\sin^2 x\cosh x}\ dx$,The entry-level PhD integral:,\int_0^\infty\frac{\sin 3x\sin 4x\sin5x\cos6x}{x\sin^2 x\cosh x}\ dx,"I hope you find this integral interesting. Evaluate   $$\int_0^\infty\frac{\sin\left(\,3x\,\right)\sin\left(\,4x\,\right) \sin\left(\,5x\,\right)\cos\left(\,6x\,\right)}{x\,\sin^{2}\left(\,x\,\right)\cosh\left(\,x\,\right)}\,\,\mathrm{d}x\tag1$$ This problem is taken from the PhD graduate entry tests in my college. I've tried to use product-to-sum trigonometric identities $$2\sin 4x\sin 3x=\cos x-\cos 5x$$ and $$2\cos 6x\sin 5x=\sin 11x-\sin x$$ I got a bunch of the following form  $$\int_0^\infty\frac{\sin \alpha x\cos \beta x}{x\sin^2 x\cosh x}\ dx\quad\Longrightarrow\quad\int_0^\infty\frac{\sin \gamma x}{x\sin^2 x\cosh x}\ dx\tag2$$ I tried $$I'(\gamma)=\int_0^\infty\frac{\cos \gamma x}{\sin^2 x\cosh x}\ dx\tag3$$ but the latter form is not easy to evaluate either. Can anyone here help me to evaluate $(1)$? Thanks in advance.","I hope you find this integral interesting. Evaluate   $$\int_0^\infty\frac{\sin\left(\,3x\,\right)\sin\left(\,4x\,\right) \sin\left(\,5x\,\right)\cos\left(\,6x\,\right)}{x\,\sin^{2}\left(\,x\,\right)\cosh\left(\,x\,\right)}\,\,\mathrm{d}x\tag1$$ This problem is taken from the PhD graduate entry tests in my college. I've tried to use product-to-sum trigonometric identities $$2\sin 4x\sin 3x=\cos x-\cos 5x$$ and $$2\cos 6x\sin 5x=\sin 11x-\sin x$$ I got a bunch of the following form  $$\int_0^\infty\frac{\sin \alpha x\cos \beta x}{x\sin^2 x\cosh x}\ dx\quad\Longrightarrow\quad\int_0^\infty\frac{\sin \gamma x}{x\sin^2 x\cosh x}\ dx\tag2$$ I tried $$I'(\gamma)=\int_0^\infty\frac{\cos \gamma x}{\sin^2 x\cosh x}\ dx\tag3$$ but the latter form is not easy to evaluate either. Can anyone here help me to evaluate $(1)$? Thanks in advance.",,"['calculus', 'integration', 'trigonometry', 'definite-integrals', 'closed-form']"
84,Finding a limit using change of variable- how come it works? [duplicate],Finding a limit using change of variable- how come it works? [duplicate],,"This question already has answers here : Formal basis for variable substitution in limits (6 answers) Closed 5 years ago . I'm a student just starting calculus in college, and my math skills are pretty stale. So... how come finding limits using change of variable works? For example: $$\lim_{x \to 1}\frac{x\cos(x-1) -1}{x-1}$$ A way to solve this is to invent ""out of thin air"" $t = x-1$, and then the limit above is equal to: $$\lim_{t \to 0}\frac{(t + 1)\cos(t) - 1}{t}$$ How come this works? A limit is not an algebraic equation. what about domains of definition?  We are actually finding a different limit of a different function in a different place, how come they are equal (in general)? just to clarify I'm not asking about this specific example. I'm asking in general, when can you do this to find limits? when not? and why?","This question already has answers here : Formal basis for variable substitution in limits (6 answers) Closed 5 years ago . I'm a student just starting calculus in college, and my math skills are pretty stale. So... how come finding limits using change of variable works? For example: $$\lim_{x \to 1}\frac{x\cos(x-1) -1}{x-1}$$ A way to solve this is to invent ""out of thin air"" $t = x-1$, and then the limit above is equal to: $$\lim_{t \to 0}\frac{(t + 1)\cos(t) - 1}{t}$$ How come this works? A limit is not an algebraic equation. what about domains of definition?  We are actually finding a different limit of a different function in a different place, how come they are equal (in general)? just to clarify I'm not asking about this specific example. I'm asking in general, when can you do this to find limits? when not? and why?",,"['calculus', 'limits']"
85,Are there other cases similar to Herglotz's integral $\int_0^1\frac{\ln\left(1+t^{4+\sqrt{15}}\right)}{1+t}\ \mathrm dt$?,Are there other cases similar to Herglotz's integral ?,\int_0^1\frac{\ln\left(1+t^{4+\sqrt{15}}\right)}{1+t}\ \mathrm dt,"This post of Boris Bukh mentions amazing Gustav Herglotz's integral $$\int_0^1\frac{\ln\left(1+t^{\,4\,+\,\sqrt{\vphantom{\large A}\,15\,}\,}\right)}{1+t}\ \mathrm dt=-\frac{\pi^2}{12}\left(\sqrt{15}-2\right)+\ln2\cdot\ln\left(\sqrt3+\sqrt5\right)+\ln\frac{1+\sqrt5}{2}\cdot\ln\left(2+\sqrt3\right).  $$ I wonder if there are other irrational real algebraic exponents $\alpha$ such that the integral $$ \int_{0}^{1} \frac{\ln\left(1 + t^{\,{\large\alpha}}\right)}{1 + t}\,{\rm d}t $$ has a closed-form representation? Is there a general formula giving results for such cases? Are there such algebraic $\alpha$ of degree $> 2$ ?","This post of Boris Bukh mentions amazing Gustav Herglotz's integral $$\int_0^1\frac{\ln\left(1+t^{\,4\,+\,\sqrt{\vphantom{\large A}\,15\,}\,}\right)}{1+t}\ \mathrm dt=-\frac{\pi^2}{12}\left(\sqrt{15}-2\right)+\ln2\cdot\ln\left(\sqrt3+\sqrt5\right)+\ln\frac{1+\sqrt5}{2}\cdot\ln\left(2+\sqrt3\right).  $$ I wonder if there are other irrational real algebraic exponents $\alpha$ such that the integral $$ \int_{0}^{1} \frac{\ln\left(1 + t^{\,{\large\alpha}}\right)}{1 + t}\,{\rm d}t $$ has a closed-form representation? Is there a general formula giving results for such cases? Are there such algebraic $\alpha$ of degree $> 2$ ?",,"['calculus', 'integration', 'definite-integrals', 'logarithms', 'closed-form']"
86,Crafty solutions to the following limit,Crafty solutions to the following limit,,"The following problem came up at dinner, I know some ways to solve it but they are quite ugly and as some wise man said: There is no place in the world for ugly mathematics. These methods are using l'Hôpital, but that becomes quite hideous very quickly or by using series expansions. So I'm looking for slick solutions to the following problem: Compute $\displaystyle \lim_{x \to 0} \frac{\sin(\tan x) - \tan(\sin x)}{\arcsin(\arctan x) - \arctan(\arcsin x)}$. I'm curious what you guys will make of this.","The following problem came up at dinner, I know some ways to solve it but they are quite ugly and as some wise man said: There is no place in the world for ugly mathematics. These methods are using l'Hôpital, but that becomes quite hideous very quickly or by using series expansions. So I'm looking for slick solutions to the following problem: Compute $\displaystyle \lim_{x \to 0} \frac{\sin(\tan x) - \tan(\sin x)}{\arcsin(\arctan x) - \arctan(\arcsin x)}$. I'm curious what you guys will make of this.",,"['calculus', 'limits']"
87,Did I just discover this integration formula?,Did I just discover this integration formula?,,"One night, I discovered an integration relationship. That relationship allows to quickly integrate squares of functions (and even more, but I will talk about this at the end). I was wondering if anyone has found a formula like this before. So I researched on the internet, but couldn't find anything like this. The formula: $$\int f(x)^2\,dx\;=\;xf(x)^2\;-\;2f(x)\cdot F^{-1}_{(1)}(f(x))\;+\;2\,\cdot F^{-1}_{(2)}(f(x))\;+\;C$$ Where $F^{-1}_{(n)}$ denotes the $n$ th anti -derivative of the inverse function of $f$ . The formula looks rather complicated, but it's really not, on further inspection. Note: the formula doesn't work on the function $f(x)=x$ for a reason that I wasn't able to determine yet. Edit: it actually works. An example in action: Let's compute $\int \ln^2x\,dx$ . We have then: $f(x)=\ln x$ $f^{-1}(x)=e^x$ $F^{-1}_{(1)}(x)=e^x$ $F^{-1}_{(2)}(x)=e^x$ Applying the formula, the integral becomes: $$\int\ln^2x\,dx=x\ln^2x-2\ln x\cdot e^{\ln x}+2\cdot e^{\ln x}+C$$ $$=x\ln^2x-2x\ln x+2x+C$$ Derivation (for the curious): I derived this formula by substituting for inverse functions and doing repeated integration by parts. First, substitute $x=f^{-1}(u)$ . Then, we have $dx=df^{-1}(u)$ . This changes the original integral to: $$\int f(x)^2\,dx=\int f(f^{-1}(u))^2\,df^{-1}(u)=\int u^2\,df^{-1}(u)$$ At this point, I did integration by parts (probably the funkiest integration by parts you have ever seen). Note, that I am using the ""DI table"" trick for integration by parts, where in one column, derivatives of one function are specified, and integrals of another are put into the other column. Terms are multiplied diagonally left-down. $$\begin{array}{ l | c | r } \pm & D & I \\ \hline + & u^2 & df^{-1}(u) \\ - & 2u & f^{-1}(u) \\ + & 2 & F^{-1}_{(1)}(u) \\ - & 0 & F^{-1}_{(2)}(u) \\ \end{array}$$ $$\int u^2\,df^{-1}(u)\;=\;u^2f^{-1}(u)-2uF^{-1}_{(1)}(u)+2F^{-1}_{(2)}(u)+C$$ Now, simply subsitute back $u=f(x)$ and we arrive at the formula. Generalization to composition of functions: It didn't take long for me to realize that this method can be extended to compositions of functions. Just for the curious folks, this is my integral of composition formula: $$\int f(g(x))\,dx\;=\;\sum^{\infty}_{k=0}(-1)^k\cdot D_k(g(x))\cdot A_k(g(x)) + C$$ Where $D_k={d^k f\over dx^k}$ and $A_k={d^{-k}g^{-1}\over dx^{-k}}$ . Interesting, isn't it? Back to the question: Have I discovered this formula? If I didn't can someone point me to a further reading on this topic? I really can't find anything myself, probably because I don't know how to search.","One night, I discovered an integration relationship. That relationship allows to quickly integrate squares of functions (and even more, but I will talk about this at the end). I was wondering if anyone has found a formula like this before. So I researched on the internet, but couldn't find anything like this. The formula: Where denotes the th anti -derivative of the inverse function of . The formula looks rather complicated, but it's really not, on further inspection. Note: the formula doesn't work on the function for a reason that I wasn't able to determine yet. Edit: it actually works. An example in action: Let's compute . We have then: Applying the formula, the integral becomes: Derivation (for the curious): I derived this formula by substituting for inverse functions and doing repeated integration by parts. First, substitute . Then, we have . This changes the original integral to: At this point, I did integration by parts (probably the funkiest integration by parts you have ever seen). Note, that I am using the ""DI table"" trick for integration by parts, where in one column, derivatives of one function are specified, and integrals of another are put into the other column. Terms are multiplied diagonally left-down. Now, simply subsitute back and we arrive at the formula. Generalization to composition of functions: It didn't take long for me to realize that this method can be extended to compositions of functions. Just for the curious folks, this is my integral of composition formula: Where and . Interesting, isn't it? Back to the question: Have I discovered this formula? If I didn't can someone point me to a further reading on this topic? I really can't find anything myself, probably because I don't know how to search.","\int f(x)^2\,dx\;=\;xf(x)^2\;-\;2f(x)\cdot F^{-1}_{(1)}(f(x))\;+\;2\,\cdot F^{-1}_{(2)}(f(x))\;+\;C F^{-1}_{(n)} n f f(x)=x \int \ln^2x\,dx f(x)=\ln x f^{-1}(x)=e^x F^{-1}_{(1)}(x)=e^x F^{-1}_{(2)}(x)=e^x \int\ln^2x\,dx=x\ln^2x-2\ln x\cdot e^{\ln x}+2\cdot e^{\ln x}+C =x\ln^2x-2x\ln x+2x+C x=f^{-1}(u) dx=df^{-1}(u) \int f(x)^2\,dx=\int f(f^{-1}(u))^2\,df^{-1}(u)=\int u^2\,df^{-1}(u) \begin{array}{ l | c | r }
\pm & D & I \\
\hline
+ & u^2 & df^{-1}(u) \\
- & 2u & f^{-1}(u) \\
+ & 2 & F^{-1}_{(1)}(u) \\
- & 0 & F^{-1}_{(2)}(u) \\
\end{array} \int u^2\,df^{-1}(u)\;=\;u^2f^{-1}(u)-2uF^{-1}_{(1)}(u)+2F^{-1}_{(2)}(u)+C u=f(x) \int f(g(x))\,dx\;=\;\sum^{\infty}_{k=0}(-1)^k\cdot D_k(g(x))\cdot A_k(g(x)) + C D_k={d^k f\over dx^k} A_k={d^{-k}g^{-1}\over dx^{-k}}","['calculus', 'integration']"
88,The formalism behind integration by substitution,The formalism behind integration by substitution,,"When you are doing an integration by substitution you do the following working. $$\begin{align*} u&=f(x)\\ \Rightarrow\frac{du}{dx}&=f^{\prime}(x)\\ \Rightarrow du&=f^{\prime}(x)dx&(1)\\ \Rightarrow dx&=\frac{du}{f^{\prime}(x)}\\ \end{align*}$$ My question is: what on earth is going on at line $(1)$?!? This has been bugging me for, like, forever! You see, when I was taught this in my undergrad I was told something along the lines of the following: You just treat $\frac{du}{dx}$ like a fraction. Similarly, when you are doing the chain rule $\frac{dy}{dx}=\frac{dy}{dv}\times\frac{dv}{dx}$ you ""cancel"" the $dv$ terms. They are just like fractions. However, never, ever say this to a pure mathematician. Now, I am a pure mathematician. And quite frankly I don't care if people think of these as fractions or not. I know that they are not fractions (but rather is the limit of the difference fractions as the difference tends to zero). But I figure I should start caring now...So, more precisely, $\frac{du}{dx}$ has a meaning, but so far as I know $du$ and $dx$ do not have a meaning. Therefore, why can we treat $\frac{du}{dx}$ as a fraction when we are doing integration by substitution? What is actually going on at line $(1)$?","When you are doing an integration by substitution you do the following working. $$\begin{align*} u&=f(x)\\ \Rightarrow\frac{du}{dx}&=f^{\prime}(x)\\ \Rightarrow du&=f^{\prime}(x)dx&(1)\\ \Rightarrow dx&=\frac{du}{f^{\prime}(x)}\\ \end{align*}$$ My question is: what on earth is going on at line $(1)$?!? This has been bugging me for, like, forever! You see, when I was taught this in my undergrad I was told something along the lines of the following: You just treat $\frac{du}{dx}$ like a fraction. Similarly, when you are doing the chain rule $\frac{dy}{dx}=\frac{dy}{dv}\times\frac{dv}{dx}$ you ""cancel"" the $dv$ terms. They are just like fractions. However, never, ever say this to a pure mathematician. Now, I am a pure mathematician. And quite frankly I don't care if people think of these as fractions or not. I know that they are not fractions (but rather is the limit of the difference fractions as the difference tends to zero). But I figure I should start caring now...So, more precisely, $\frac{du}{dx}$ has a meaning, but so far as I know $du$ and $dx$ do not have a meaning. Therefore, why can we treat $\frac{du}{dx}$ as a fraction when we are doing integration by substitution? What is actually going on at line $(1)$?",,"['calculus', 'integration', 'notation']"
89,A closed form of $\sum_{k=1}^\infty \psi^{(1)} (k+a)\psi^{(1)} (k+b)$?,A closed form of ?,\sum_{k=1}^\infty \psi^{(1)} (k+a)\psi^{(1)} (k+b),"The following result $$ \sum_{k=1}^\infty\left(\psi^{(1)} (k)\right)^2 = 3\zeta(3) $$ where $\psi^{(1)}$ is the polygamma function makes me think there is a nice sum for the series $$ \sum_{k=1}^\infty \left(\psi^{(1)} (k+a)\right)^2  $$ or $$ \sum_{k=1}^\infty \psi^{(1)} (k+a)\psi^{(1)} (k+b) $$ where $a$ and $b$ are any real numbers such that $a >-1, b>-1.$ Could you help me to find it?","The following result $$ \sum_{k=1}^\infty\left(\psi^{(1)} (k)\right)^2 = 3\zeta(3) $$ where $\psi^{(1)}$ is the polygamma function makes me think there is a nice sum for the series $$ \sum_{k=1}^\infty \left(\psi^{(1)} (k+a)\right)^2  $$ or $$ \sum_{k=1}^\infty \psi^{(1)} (k+a)\psi^{(1)} (k+b) $$ where $a$ and $b$ are any real numbers such that $a >-1, b>-1.$ Could you help me to find it?",,"['calculus', 'integration', 'sequences-and-series', 'closed-form', 'polygamma']"
90,How to integrate $ \int x^n e^x dx$?,How to integrate ?, \int x^n e^x dx,"How can I solve this indefinite integral for an arbitrary integer $n>0$? $$ \int{x^n e^x dx}$$ I could partially integrate it for small $n$, but that's not really a solution. Edit: (TB) This question is closely related to: Is there a closed form solution for $\int x^n e^{cx}$? , but it is more elementary, because $n$ is an integer here.","How can I solve this indefinite integral for an arbitrary integer $n>0$? $$ \int{x^n e^x dx}$$ I could partially integrate it for small $n$, but that's not really a solution. Edit: (TB) This question is closely related to: Is there a closed form solution for $\int x^n e^{cx}$? , but it is more elementary, because $n$ is an integer here.",,"['calculus', 'integration']"
91,Why does Newton's method work?,Why does Newton's method work?,,"I find many sites explaining how to use Newton's method, but none explaining why it works. Could someone give me the intuition behind it? Thanks.","I find many sites explaining how to use Newton's method, but none explaining why it works. Could someone give me the intuition behind it? Thanks.",,"['calculus', 'functions', 'numerical-methods', 'intuition']"
92,Integral ${\large\int}_0^\infty\frac{\ln x}{1+x}\sqrt{\frac{x+\sqrt{1+x^2}}{1+x^2}}\ \mathrm dx$ [closed],Integral  [closed],{\large\int}_0^\infty\frac{\ln x}{1+x}\sqrt{\frac{x+\sqrt{1+x^2}}{1+x^2}}\ \mathrm dx,"Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 5 years ago . Improve this question Please help me to evaluate this integral: $$ I={\large\int}_{0}^{\infty}{\ln\left(x\right) \over 1 + x}\, \,\sqrt{\,x + \sqrt{\,1 + x^{2}\,}\, \over 1 + x^{2}\,}\,\,{\rm d}x.\tag1 $$ Mathematica could not evaluate it in a closed form. A numerical integration returned $$I \approx 4.25314982536869548103063\ldots\,,\tag2$$ but neither WolframAlpha nor ISC+ could find a plausible closed form for this.","Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 5 years ago . Improve this question Please help me to evaluate this integral: $$ I={\large\int}_{0}^{\infty}{\ln\left(x\right) \over 1 + x}\, \,\sqrt{\,x + \sqrt{\,1 + x^{2}\,}\, \over 1 + x^{2}\,}\,\,{\rm d}x.\tag1 $$ Mathematica could not evaluate it in a closed form. A numerical integration returned $$I \approx 4.25314982536869548103063\ldots\,,\tag2$$ but neither WolframAlpha nor ISC+ could find a plausible closed form for this.",,"['calculus', 'integration', 'definite-integrals', 'improper-integrals', 'closed-form']"
93,A closed form of $\sum_{k=1}^\infty\frac{(-1)^{k+1}}{k!}\Gamma^2\left(\frac{k}{2}\right)$,A closed form of,\sum_{k=1}^\infty\frac{(-1)^{k+1}}{k!}\Gamma^2\left(\frac{k}{2}\right),I am looking for a closed form of the following series \begin{equation} \mathcal{I}=\sum_{k=1}^\infty\frac{(-1)^{k+1}}{k!}\Gamma^2\left(\frac{k}{2}\right) \end{equation} I have no idea how to answer this question. Wolfram Alpha gives me result: $$\mathcal{I}\approx2.7415567780803776$$ Could anyone here please help me to obtain the closed form of the series preferably ( if possible ) with elementary ways (high school methods)? Any help would be greatly appreciated. Thank you.,I am looking for a closed form of the following series \begin{equation} \mathcal{I}=\sum_{k=1}^\infty\frac{(-1)^{k+1}}{k!}\Gamma^2\left(\frac{k}{2}\right) \end{equation} I have no idea how to answer this question. Wolfram Alpha gives me result: $$\mathcal{I}\approx2.7415567780803776$$ Could anyone here please help me to obtain the closed form of the series preferably ( if possible ) with elementary ways (high school methods)? Any help would be greatly appreciated. Thank you.,,"['calculus', 'sequences-and-series', 'summation', 'factorial', 'gamma-function']"
94,Uses of the symmetric derivative $\lim \limits_{h\to 0} \frac{f(x+h)-f(x-h)}{2h}$?,Uses of the symmetric derivative ?,\lim \limits_{h\to 0} \frac{f(x+h)-f(x-h)}{2h},"I have been wondering whether the following limit is being used somehow, as a variation of the derivative: $$\lim_{h\to 0} \frac{f(x+h)-f(x-h)}{2h} .$$ Edit: I know that this limit is defined in some places where the derivative is not defined, but it gives us some useful information. The question is not whether this limit is similar to the derivative, but whether it is useful somehow. Thanks.","I have been wondering whether the following limit is being used somehow, as a variation of the derivative: $$\lim_{h\to 0} \frac{f(x+h)-f(x-h)}{2h} .$$ Edit: I know that this limit is defined in some places where the derivative is not defined, but it gives us some useful information. The question is not whether this limit is similar to the derivative, but whether it is useful somehow. Thanks.",,"['calculus', 'limits', 'derivatives']"
95,Maximizing the value of $\int_0^1 f(x)f^{-1}(x)\ \mathrm dx$,Maximizing the value of,\int_0^1 f(x)f^{-1}(x)\ \mathrm dx,"I am trying to find the maximum size of the integral $\int_0^1 f(x)f^{-1}(x)\ \mathrm dx$ for differentiable, increasing $f$ with $f(0)=0$ and $f(1)=1$ . I made up this exercise for myself and thought it would be easy, but I can't do it. I feel the answer should be $\frac 1 3$ intuitively, which comes from $f(x)=x$ . So far I've tried integration by parts but then I don't know what to do. Edit: here is the integration by parts I tried, though I think it doesn't lead anywhere: $$\int^1_0 f(x)f^{-1}(x)\ \mathrm dx=\int_0^1f^{-1}(x)\ \mathrm dx-\int_0^1f'(x)\left(\int_0^x f^{-1}(t)\ \mathrm dt\right)\ \mathrm dx\text.$$ I thought this could help because $f'(x)>0$ since $f$ is increasing and the other factor in this integral is also positive by default.","I am trying to find the maximum size of the integral for differentiable, increasing with and . I made up this exercise for myself and thought it would be easy, but I can't do it. I feel the answer should be intuitively, which comes from . So far I've tried integration by parts but then I don't know what to do. Edit: here is the integration by parts I tried, though I think it doesn't lead anywhere: I thought this could help because since is increasing and the other factor in this integral is also positive by default.",\int_0^1 f(x)f^{-1}(x)\ \mathrm dx f f(0)=0 f(1)=1 \frac 1 3 f(x)=x \int^1_0 f(x)f^{-1}(x)\ \mathrm dx=\int_0^1f^{-1}(x)\ \mathrm dx-\int_0^1f'(x)\left(\int_0^x f^{-1}(t)\ \mathrm dt\right)\ \mathrm dx\text. f'(x)>0 f,"['calculus', 'integration', 'optimization']"
96,Meaning of derivatives of vector fields,Meaning of derivatives of vector fields,,"I have a doubt about the real meaning of the derivative of a vector field. This question seems silly at first but the doubt came when I was studying the definition of tangent space. If I understood well a vector is a directional derivative operator, i.e.: a vector is an operator that can produce derivatives of scalar fields. If that's the case then a vector acts on a scalar field and tells me how the field changes on that point. However, if a vector is a derivative operator, a vector field defines a different derivative operator at each point. So differentiate a vector would be differentiate a derivate operator, and that seems strange to me at first. I thought for example that the total derivative of a vector field would produce rates of change of the field, but my studies led me to a different approach, where the total derivative produces rates of change only for scalar fields and for vector fields it produces the pushforward. So, what's the real meaning of differentiating a vector field knowing all of this?","I have a doubt about the real meaning of the derivative of a vector field. This question seems silly at first but the doubt came when I was studying the definition of tangent space. If I understood well a vector is a directional derivative operator, i.e.: a vector is an operator that can produce derivatives of scalar fields. If that's the case then a vector acts on a scalar field and tells me how the field changes on that point. However, if a vector is a derivative operator, a vector field defines a different derivative operator at each point. So differentiate a vector would be differentiate a derivate operator, and that seems strange to me at first. I thought for example that the total derivative of a vector field would produce rates of change of the field, but my studies led me to a different approach, where the total derivative produces rates of change only for scalar fields and for vector fields it produces the pushforward. So, what's the real meaning of differentiating a vector field knowing all of this?",,"['calculus', 'geometry', 'differential-geometry', 'smooth-manifolds', 'vector-fields']"
97,Generalizing the trick for integrating $\int_{-\infty}^\infty e^{-x^2}\mathrm dx$?,Generalizing the trick for integrating ?,\int_{-\infty}^\infty e^{-x^2}\mathrm dx,"There is a well-known trick for integrating $\int_{-\infty}^\infty e^{-x^2}\mathrm dx$, which is to write it as $\sqrt{\int_{-\infty}^\infty e^{-x^2}\mathrm dx\int_{-\infty}^\infty e^{-y^2}\mathrm dy}$, which can then be reexpressed in polar coordinates as an easy integral. Is this trick a one-hit wonder, or are there other cases where this trick works and is also necessary? It seems to depend on the defining property of the exponential function that $f(a+b)=f(a)f(b)$, which would make me think that it would only allow fairly trivial generalizations, e.g., to $\int_{-\infty}^\infty 7^{-x^2}\mathrm dx$ or $\int_{-\infty}^\infty a^{bx^2+cx+d}\mathrm dx$. Can it be adapted through rotation in the complex plane to do integrals like $\int_{-\infty}^\infty \sin(x^2)\mathrm dx$? Here I find myself confused by trying to simultaneously visualize both the complex plane and the $(x,y)$ plane. WP http://en.wikipedia.org/wiki/Gaussian_integral discusses integrals that have a similar form and seem to require different methods, but I'd be more interested in integrals that have different forms but can be conquered by the same trick. The trick involves expanding from 1 dimension to 2. Is there a useful generalization where you expand from $m$ dimensions to $n$? This is not homework.","There is a well-known trick for integrating $\int_{-\infty}^\infty e^{-x^2}\mathrm dx$, which is to write it as $\sqrt{\int_{-\infty}^\infty e^{-x^2}\mathrm dx\int_{-\infty}^\infty e^{-y^2}\mathrm dy}$, which can then be reexpressed in polar coordinates as an easy integral. Is this trick a one-hit wonder, or are there other cases where this trick works and is also necessary? It seems to depend on the defining property of the exponential function that $f(a+b)=f(a)f(b)$, which would make me think that it would only allow fairly trivial generalizations, e.g., to $\int_{-\infty}^\infty 7^{-x^2}\mathrm dx$ or $\int_{-\infty}^\infty a^{bx^2+cx+d}\mathrm dx$. Can it be adapted through rotation in the complex plane to do integrals like $\int_{-\infty}^\infty \sin(x^2)\mathrm dx$? Here I find myself confused by trying to simultaneously visualize both the complex plane and the $(x,y)$ plane. WP http://en.wikipedia.org/wiki/Gaussian_integral discusses integrals that have a similar form and seem to require different methods, but I'd be more interested in integrals that have different forms but can be conquered by the same trick. The trick involves expanding from 1 dimension to 2. Is there a useful generalization where you expand from $m$ dimensions to $n$? This is not homework.",,['calculus']
98,"A closed form for $\int_0^1{_2F_1}\left(-\frac{1}{4},\frac{5}{4};\,1;\,\frac{x}{2}\right)^2dx$",A closed form for,"\int_0^1{_2F_1}\left(-\frac{1}{4},\frac{5}{4};\,1;\,\frac{x}{2}\right)^2dx","Is it possible to evaluate in a closed form integrals containing a squared hypergeometric function, like in this example? $$\begin{align}S&=\int_0^1{_2F_1}\left(-\frac{1}{4},\frac{5}{4};\,1;\,\frac{x}{2}\right)^2dx\\\vphantom{=}\\&=\frac{1}{4\pi}\int_0^1\left(\sum_{n=0}^\infty\frac{4n+1}{8^n}\cdot\frac{\Gamma\left(2n-\frac{1}{2}\right)}{\Gamma(n+1)^2}\cdot x^n\right)^2dx\end{align}$$ It is approximately $$S\approx0.8263551866500213413164525287...$$","Is it possible to evaluate in a closed form integrals containing a squared hypergeometric function, like in this example? $$\begin{align}S&=\int_0^1{_2F_1}\left(-\frac{1}{4},\frac{5}{4};\,1;\,\frac{x}{2}\right)^2dx\\\vphantom{=}\\&=\frac{1}{4\pi}\int_0^1\left(\sum_{n=0}^\infty\frac{4n+1}{8^n}\cdot\frac{\Gamma\left(2n-\frac{1}{2}\right)}{\Gamma(n+1)^2}\cdot x^n\right)^2dx\end{align}$$ It is approximately $$S\approx0.8263551866500213413164525287...$$",,"['calculus', 'integration', 'special-functions', 'closed-form', 'hypergeometric-function']"
99,Interesting log sine integrals $\int_0^{\pi/3} \log^2 \left(2\sin \frac{x}{2} \right)dx= \frac{7\pi^3}{108}$,Interesting log sine integrals,\int_0^{\pi/3} \log^2 \left(2\sin \frac{x}{2} \right)dx= \frac{7\pi^3}{108},Show that $$\begin{aligned} \int_0^{\pi/3} \log^2 \left(2\sin \frac{x}{2}  \right)dx &= \frac{7\pi^3}{108} \\ \int_0^{\pi/3}x\log^2  \left(2\sin\frac{x}{2} \right)dx &= \frac{17\pi^4}{6480}\end{aligned}$$ I can solve $\displaystyle \int_0^\pi \log^2 \left(2\sin \frac{x}{2}  \right)dx $ but I don't know what to do if the limits are from $0$ to $\pi/3$. I have no idea what to do if the integrand contains an $x$. I feel that the Polylogarithm function will be involved however I don't know how it can be implemented here. It would be really great if someone could take the initiative to prove these.,Show that $$\begin{aligned} \int_0^{\pi/3} \log^2 \left(2\sin \frac{x}{2}  \right)dx &= \frac{7\pi^3}{108} \\ \int_0^{\pi/3}x\log^2  \left(2\sin\frac{x}{2} \right)dx &= \frac{17\pi^4}{6480}\end{aligned}$$ I can solve $\displaystyle \int_0^\pi \log^2 \left(2\sin \frac{x}{2}  \right)dx $ but I don't know what to do if the limits are from $0$ to $\pi/3$. I have no idea what to do if the integrand contains an $x$. I feel that the Polylogarithm function will be involved however I don't know how it can be implemented here. It would be really great if someone could take the initiative to prove these.,,"['calculus', 'integration', 'special-functions', 'definite-integrals']"
