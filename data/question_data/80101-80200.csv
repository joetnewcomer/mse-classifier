,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,"Why is the reduced echelon form of a set of independent vectors, the identity matrix?","Why is the reduced echelon form of a set of independent vectors, the identity matrix?",,"If a matrix has linearly independent rows, then its  reduced echelon form is  the identity matrix. I haven't found a concise explanation for this... I have the whole notion in my head but I cannot express this in words.  Can someone explain it?","If a matrix has linearly independent rows, then its  reduced echelon form is  the identity matrix. I haven't found a concise explanation for this... I have the whole notion in my head but I cannot express this in words.  Can someone explain it?",,"['linear-algebra', 'matrices']"
1,The column space of a matrix A is the set of solutions of Ax = b?,The column space of a matrix A is the set of solutions of Ax = b?,,"Is it true to state , The column space of a matrix A is the set of solutions of Ax = b? Why?","Is it true to state , The column space of a matrix A is the set of solutions of Ax = b? Why?",,"['linear-algebra', 'matrices']"
2,Why does elementary row operation on combination of Identity matrix and invertible matrix yields inverse matrix?,Why does elementary row operation on combination of Identity matrix and invertible matrix yields inverse matrix?,,"Why does elementary row operation on combination of Identity matrix and invertible matrix yields inverse matrix? $$A = \begin{pmatrix} 1 & 3\\  4 & 2 \end{pmatrix}$$ if we combine $A$ with $I_{2\times 2}$ in which $I$ is an identity matrix, and do elementary row operation on it to get identity matrix on the other side, the opposite side(block) yields out the inverse of the matrix. The question is, why is it true?","Why does elementary row operation on combination of Identity matrix and invertible matrix yields inverse matrix? $$A = \begin{pmatrix} 1 & 3\\  4 & 2 \end{pmatrix}$$ if we combine $A$ with $I_{2\times 2}$ in which $I$ is an identity matrix, and do elementary row operation on it to get identity matrix on the other side, the opposite side(block) yields out the inverse of the matrix. The question is, why is it true?",,"['linear-algebra', 'matrices', 'block-matrices']"
3,How to prove a property of ranks: $\operatorname{rank}(AB)= \operatorname{rank}(B)- \dim(\operatorname{Im} B \cap \ker A)$,How to prove a property of ranks:,\operatorname{rank}(AB)= \operatorname{rank}(B)- \dim(\operatorname{Im} B \cap \ker A),"$\newcommand{\rank}{\operatorname{rank}}\renewcommand\Im{\operatorname{Im}}$ Let $A$ and $B$ be real matrices of sizes $m\times n$ and $n\times p$, respectively. I have to prove that $\rank(AB)= \rank(B)- \dim(\Im B \cap \ker A)$ I haven't got much idea... but I started like this: Using the first isomorphism theorem, we get the following relations: $p= \rank(AB)+ \dim(\ker(AB))$ $p= \rank(B)+\dim(\ker B)$ and $n= \rank(A)+\dim(\ker A)$ From the first and second relation we get that: $$\rank(AB)+ \dim(\ker(AB)) = \rank(B) + \dim(\ker B)$$ I don't know how to continue or if I am on the right way to prove it. Thank you for your time and help. And sorry for my poor English.","$\newcommand{\rank}{\operatorname{rank}}\renewcommand\Im{\operatorname{Im}}$ Let $A$ and $B$ be real matrices of sizes $m\times n$ and $n\times p$, respectively. I have to prove that $\rank(AB)= \rank(B)- \dim(\Im B \cap \ker A)$ I haven't got much idea... but I started like this: Using the first isomorphism theorem, we get the following relations: $p= \rank(AB)+ \dim(\ker(AB))$ $p= \rank(B)+\dim(\ker B)$ and $n= \rank(A)+\dim(\ker A)$ From the first and second relation we get that: $$\rank(AB)+ \dim(\ker(AB)) = \rank(B) + \dim(\ker B)$$ I don't know how to continue or if I am on the right way to prove it. Thank you for your time and help. And sorry for my poor English.",,"['linear-algebra', 'matrices', 'matrix-rank']"
4,When is the solution to a n initial value problem matrix differential equation invertible?,When is the solution to a n initial value problem matrix differential equation invertible?,,"Suppose $A (t,s)$ a $n\times n$ matrix is the solution of the initial value problem below, where $B_s$ is also an $n\times n$ matrix, invertible for all $s$: $$\dfrac{d A(t,s)}{ds} = B_s A(t,s)$$ $$ A(t,t) =I_n$$ I know that if $B$ is constant, the solution is invertible as an exponential matrix. If $B_s$ is not constant, is the solution still invertible? My answer is yes since by the Magnus expansion the solution is of the form $A(t,s) = exp(\Omega(t,s))A(t,t)$ which is again is invertible because it's an exponential and $A(t,t)=I_n$. Can anyone confirm if my argument is correct? or is there any other way to see it without using the Magnus expansion argument? Thanks","Suppose $A (t,s)$ a $n\times n$ matrix is the solution of the initial value problem below, where $B_s$ is also an $n\times n$ matrix, invertible for all $s$: $$\dfrac{d A(t,s)}{ds} = B_s A(t,s)$$ $$ A(t,t) =I_n$$ I know that if $B$ is constant, the solution is invertible as an exponential matrix. If $B_s$ is not constant, is the solution still invertible? My answer is yes since by the Magnus expansion the solution is of the form $A(t,s) = exp(\Omega(t,s))A(t,t)$ which is again is invertible because it's an exponential and $A(t,t)=I_n$. Can anyone confirm if my argument is correct? or is there any other way to see it without using the Magnus expansion argument? Thanks",,['linear-algebra']
5,"Matrix with entries from $1$ to $16$, each occuring once, and determinant $40800$","Matrix with entries from  to , each occuring once, and determinant",1 16 40800,"In OEIS, it is claimed, that the largest possible determinant of a $4\ x \ 4$-matrix with the entries from $1$ to $16$, each occuring once, is $40800$. Unfortunately, the article does not mention a concrete matrix with this determinant. I tried to find a matrix with turbo pascal, but without success so far. Can anyone help ?","In OEIS, it is claimed, that the largest possible determinant of a $4\ x \ 4$-matrix with the entries from $1$ to $16$, each occuring once, is $40800$. Unfortunately, the article does not mention a concrete matrix with this determinant. I tried to find a matrix with turbo pascal, but without success so far. Can anyone help ?",,"['matrices', 'determinant']"
6,Counting diagonalizable matrices in $\mathcal{M}_{n}(\mathbb{Z}/p\mathbb{Z})$,Counting diagonalizable matrices in,\mathcal{M}_{n}(\mathbb{Z}/p\mathbb{Z}),"How many diagonalizable matrices are there in $\mathcal{M}_{n}(\mathbb{Z}/p\mathbb{Z})$ ? Where $p$ is a prime number. Attempt : By definition a matrix is called diagonalizable if there exists an invertible matrix $P$ such that $P^{−1}AP$ is diagonal. Given that the number of non-singular matrices of $\mathcal{G}\mathcal{L}_{n}(\Bbb{Z}/p\Bbb{Z})$ is $\prod\limits^{n-1}_{i=0}(p^{n}-p^{i})$, we can prove this by counting the number of basis of $\Bbb{Z}/p\Bbb{Z}^{n}$. And the number of diagonal matrices is $p^n$. So basically I would say that the result is $$\prod\limits^{n-1}_{i=0}(p^{n}-p^{i})\times p^{n}.$$ Does I miss something ? EDIT : Ok it's wrong according to Blue comments.","How many diagonalizable matrices are there in $\mathcal{M}_{n}(\mathbb{Z}/p\mathbb{Z})$ ? Where $p$ is a prime number. Attempt : By definition a matrix is called diagonalizable if there exists an invertible matrix $P$ such that $P^{−1}AP$ is diagonal. Given that the number of non-singular matrices of $\mathcal{G}\mathcal{L}_{n}(\Bbb{Z}/p\Bbb{Z})$ is $\prod\limits^{n-1}_{i=0}(p^{n}-p^{i})$, we can prove this by counting the number of basis of $\Bbb{Z}/p\Bbb{Z}^{n}$. And the number of diagonal matrices is $p^n$. So basically I would say that the result is $$\prod\limits^{n-1}_{i=0}(p^{n}-p^{i})\times p^{n}.$$ Does I miss something ? EDIT : Ok it's wrong according to Blue comments.",,"['linear-algebra', 'matrices']"
7,Solving a set of non-linear matrix equations,Solving a set of non-linear matrix equations,,"Consider the following set of equations $$\begin{cases}PAQ^{-1}&=T \\ QBR^{-1}&=T\\ RCP^{-1}&=T, \end{cases} $$ where A,B,C and T are known real-valued $3\times3$ matrices and P, Q, R are the unknown (real valued, $3\times3$ sized, invertible matrices). Is it there an analytic solution for this set of equations? If not, then consider one or more of the following relaxations: $T = I_3$ or $T=\begin{bmatrix}1 & 0 & 0\\ 0 & 0 & 0\\ 0 & 0 & 0\end{bmatrix}$ A, B, C are invertible Numeric solutions are also acceptable","Consider the following set of equations $$\begin{cases}PAQ^{-1}&=T \\ QBR^{-1}&=T\\ RCP^{-1}&=T, \end{cases} $$ where A,B,C and T are known real-valued $3\times3$ matrices and P, Q, R are the unknown (real valued, $3\times3$ sized, invertible matrices). Is it there an analytic solution for this set of equations? If not, then consider one or more of the following relaxations: $T = I_3$ or $T=\begin{bmatrix}1 & 0 & 0\\ 0 & 0 & 0\\ 0 & 0 & 0\end{bmatrix}$ A, B, C are invertible Numeric solutions are also acceptable",,"['matrices', 'inverse']"
8,If diagonalizable matrices commute does it neccesarily mean that they can be simultaneously diagonalized?,If diagonalizable matrices commute does it neccesarily mean that they can be simultaneously diagonalized?,,"If matrices $M_1$ and $M_2$ can be simultaneously diagonalized, than they commute, which can be easily shown: \begin{align} M_1M_2&=P^{-1}D_1PP^{-1}D_2P \\ &=P^{-1}D_1D_2P \\ &=P^{-1}D_2D_1P \\ &=P^{-1}D_2PP^{-1}D_1P \\ &=M_2M_1 \end{align} But is converse also true? If diagonalizable matrices $M_1$,...,$M_n$ all mutually commute, does this mean that they can be simultaneously diagonalized? If so, how to show this? If not, what is the simplest counterexample?","If matrices $M_1$ and $M_2$ can be simultaneously diagonalized, than they commute, which can be easily shown: \begin{align} M_1M_2&=P^{-1}D_1PP^{-1}D_2P \\ &=P^{-1}D_1D_2P \\ &=P^{-1}D_2D_1P \\ &=P^{-1}D_2PP^{-1}D_1P \\ &=M_2M_1 \end{align} But is converse also true? If diagonalizable matrices $M_1$,...,$M_n$ all mutually commute, does this mean that they can be simultaneously diagonalized? If so, how to show this? If not, what is the simplest counterexample?",,"['linear-algebra', 'matrices', 'diagonalization']"
9,Diagonal of pseudoinverse of Laplacian matrix,Diagonal of pseudoinverse of Laplacian matrix,,"I have to find the diagonal of the pseudoinverse of a Laplacian matrix evaluated on a directed and weighted graph. My Laplacian is defined as $L=D-A$ , where $D$ is a diagonal matrix (with $D_{i,i}$ being the sum of the weights of in or out edges from node $i$ ) and $A$ is the adjacency matrix where $A_{i,j} = \operatorname{weight}(i,j)$ . My Laplacian is quite big, about $20$ k * $20$ k and hence its pseudoinversion requires about $10$ hours with the Python scipy.pinv $2$ command and a lot of RAM, over $15$ GB. I need only the diagonal entries of this matrix.  Is there any mathematical rule that I can use in order to avoid the evaluation of the entire pseudoinverse? Is there any faster method than using the whole pseudoinverse method? I need to have best performance possible (both memory and speed.) Thank you.","I have to find the diagonal of the pseudoinverse of a Laplacian matrix evaluated on a directed and weighted graph. My Laplacian is defined as , where is a diagonal matrix (with being the sum of the weights of in or out edges from node ) and is the adjacency matrix where . My Laplacian is quite big, about k * k and hence its pseudoinversion requires about hours with the Python scipy.pinv command and a lot of RAM, over GB. I need only the diagonal entries of this matrix.  Is there any mathematical rule that I can use in order to avoid the evaluation of the entire pseudoinverse? Is there any faster method than using the whole pseudoinverse method? I need to have best performance possible (both memory and speed.) Thank you.","L=D-A D D_{i,i} i A A_{i,j} = \operatorname{weight}(i,j) 20 20 10 2 15","['linear-algebra', 'matrices', 'discrete-mathematics', 'graph-laplacian', 'pseudoinverse']"
10,What causes commutativity of matrices?,What causes commutativity of matrices?,,"My understanding is that the multiplication of two matrices is NOT commutative most of the time. One exception is two matrices, A and B, that are inverses of the other. This condition leads in turn, to some important restrictions. The two matrices are square matrices of the same dimension. One equals the transpose of the other, divided by their (common) determinant, to ""unitize"" the inverse matrix. They are ""nonsingular"" insofar as their determinant is non-zero (can't divide by zero in 2, above). What causes such matrices to be commutative under multiplication? Is invertibility a necessary and/or sufficient condition for two matrices to be commutative in this way? In this post , there was an answer that the required condition was ""a common basis of generalized eigenvectors."" How does that allow commutativity between the two matrices? Is is because they are ""bijective"" (injective and surjective)?","My understanding is that the multiplication of two matrices is NOT commutative most of the time. One exception is two matrices, A and B, that are inverses of the other. This condition leads in turn, to some important restrictions. The two matrices are square matrices of the same dimension. One equals the transpose of the other, divided by their (common) determinant, to ""unitize"" the inverse matrix. They are ""nonsingular"" insofar as their determinant is non-zero (can't divide by zero in 2, above). What causes such matrices to be commutative under multiplication? Is invertibility a necessary and/or sufficient condition for two matrices to be commutative in this way? In this post , there was an answer that the required condition was ""a common basis of generalized eigenvectors."" How does that allow commutativity between the two matrices? Is is because they are ""bijective"" (injective and surjective)?",,"['linear-algebra', 'matrices']"
11,Is the space of symmetric matrices generated by rank-$1$ matrices?,Is the space of symmetric matrices generated by rank- matrices?,1,"Let $\mbox{Sym}_n (\mathbb K)$ the $\mathbb K$ -linear space of symmetric matrices over a field $\mathbb K$ . Is $\mbox{Sym}_n(\mathbb K)$ generated by symmetric rank 1 matrices ? If yes, can an explicit basis of rank- $1$ matrices be described ? If $\mathbb K = \mathbb R$ , then the answer of question 1 seems to be ""yes"" because of the singular value decomposition. But I have no idea on how to answer question 2 by giving $\binom{n+1}{2}$ linearly independent rank- $1$ symmetric matrices. I am also wondering if question 1 would depend on the field $\mathbb K$ . Many thanks!","Let the -linear space of symmetric matrices over a field . Is generated by symmetric rank 1 matrices ? If yes, can an explicit basis of rank- matrices be described ? If , then the answer of question 1 seems to be ""yes"" because of the singular value decomposition. But I have no idea on how to answer question 2 by giving linearly independent rank- symmetric matrices. I am also wondering if question 1 would depend on the field . Many thanks!",\mbox{Sym}_n (\mathbb K) \mathbb K \mathbb K \mbox{Sym}_n(\mathbb K) 1 \mathbb K = \mathbb R \binom{n+1}{2} 1 \mathbb K,"['linear-algebra', 'matrices', 'symmetric-matrices']"
12,Saddle Points on Matrices,Saddle Points on Matrices,,"Let $n$, $m$ be positive integers. Suppose that $A$ is a $2$ x $n$ or an $m$ x $2$ matrix and that it has a saddle point. Show that among the saddle points of $A$ there exists at least one which can be reached by using dominance relations. My knowledge: A saddle point in a matrix is an entry which is the minimum in its row and maximum in its column. I also know that for dominance relations the $i^{th}$ row dominates the $j^{th}$ row if all the entries in the $i^{th}$ row are $\geq$ the corresponding entries in the $j^{th}$ row. The the $i^{th}$ column dominates the $j^{th}$ column if all the entries in the $i^{th}$ column are $\leq$ the corresponding entries in the $j^{th}$ column. I was teaching myself about dominance relations and saddle points after a friend of mine started discussing it with me and how it relates to games.  I learned how to do saddle points and would like to see a proof of this question to expand my knowledge.","Let $n$, $m$ be positive integers. Suppose that $A$ is a $2$ x $n$ or an $m$ x $2$ matrix and that it has a saddle point. Show that among the saddle points of $A$ there exists at least one which can be reached by using dominance relations. My knowledge: A saddle point in a matrix is an entry which is the minimum in its row and maximum in its column. I also know that for dominance relations the $i^{th}$ row dominates the $j^{th}$ row if all the entries in the $i^{th}$ row are $\geq$ the corresponding entries in the $j^{th}$ row. The the $i^{th}$ column dominates the $j^{th}$ column if all the entries in the $i^{th}$ column are $\leq$ the corresponding entries in the $j^{th}$ column. I was teaching myself about dominance relations and saddle points after a friend of mine started discussing it with me and how it relates to games.  I learned how to do saddle points and would like to see a proof of this question to expand my knowledge.",,"['matrices', 'proof-writing', 'game-theory', 'combinatorial-game-theory']"
13,Calculating the adjustment translation to be applied after rotating and scaling so that operations pivot about a given point.,Calculating the adjustment translation to be applied after rotating and scaling so that operations pivot about a given point.,,"I have a matrix for transforming an image into a target frame.  The matrix is a function of a scale, $s$  rotation angle, $\theta$, and a translation that is applied after rotating, $tx, ty$.  The affine transform t is calculated by multiplying scale, then rotation then translation: $$ t=         \begin{bmatrix}         s & 0 & 0 \\         0 & s & 0 \\         0 & 0 & 1 \\         \end{bmatrix} .         \begin{bmatrix}         cos(\theta) & sin(\theta) & 0 \\         -sin(\theta) & cos(\theta) & 0 \\         0 & 0 & 1 \\         \end{bmatrix} .         \begin{bmatrix}         1 & 0 & 0 \\         0 & 1 & 0 \\         tx & ty & 1 \\         \end{bmatrix} $$ This works well, I can adjust the scale, angle or translation to adjust the output image. However the rotation and scale happens about the anchor point {0, 0}.  I would like to be able to scale about an arbitrary anchor point {rX, rY} (in the co-ordinate space of the target frame). What translation {tx, ty} should be applied (after scale and translation) so that the image rotates about {rX, rY} (instead of {0,0})? I have looked at similar questions but I can not make it work for this problem.","I have a matrix for transforming an image into a target frame.  The matrix is a function of a scale, $s$  rotation angle, $\theta$, and a translation that is applied after rotating, $tx, ty$.  The affine transform t is calculated by multiplying scale, then rotation then translation: $$ t=         \begin{bmatrix}         s & 0 & 0 \\         0 & s & 0 \\         0 & 0 & 1 \\         \end{bmatrix} .         \begin{bmatrix}         cos(\theta) & sin(\theta) & 0 \\         -sin(\theta) & cos(\theta) & 0 \\         0 & 0 & 1 \\         \end{bmatrix} .         \begin{bmatrix}         1 & 0 & 0 \\         0 & 1 & 0 \\         tx & ty & 1 \\         \end{bmatrix} $$ This works well, I can adjust the scale, angle or translation to adjust the output image. However the rotation and scale happens about the anchor point {0, 0}.  I would like to be able to scale about an arbitrary anchor point {rX, rY} (in the co-ordinate space of the target frame). What translation {tx, ty} should be applied (after scale and translation) so that the image rotates about {rX, rY} (instead of {0,0})? I have looked at similar questions but I can not make it work for this problem.",,"['matrices', 'geometry', 'analytic-geometry', 'transformation']"
14,Proof that frobenius norm is a norm [duplicate],Proof that frobenius norm is a norm [duplicate],,"This question already has answers here : Frobenius Norm Triangle Inequality (2 answers) Closed 10 years ago . It's pretty basic and I'm sure I'm missing something dumb here, but I'd like to know why $||A+B||_F \leq ||A||_F+||B||_F$ The way I understand it, $||A+B||^2_F=tr((A+B)^T(A+B))=tr((A^T+B^T)(A+B))=tr(A^TA+A^TB+B^TA+B^TB)$ Now using the property the trace is linear: $tr(A^TA+A^TB+B^TA+B^TB) = tr(A^TA)+tr(B^TB)+tr(A^TB+B^TA)=||A||^2_F+||B||^2_F+tr(A^TB+B^TA)$ Now if we were to prove that $2||A||_F||B||_F \geq tr(A^TB+B^TA)$ that would solve the question. But I don't see how that's trivial, and generally multiplying $\sum$s together is something I avoid like the plague. Is this indeed the way? would someone help me with this last step?","This question already has answers here : Frobenius Norm Triangle Inequality (2 answers) Closed 10 years ago . It's pretty basic and I'm sure I'm missing something dumb here, but I'd like to know why $||A+B||_F \leq ||A||_F+||B||_F$ The way I understand it, $||A+B||^2_F=tr((A+B)^T(A+B))=tr((A^T+B^T)(A+B))=tr(A^TA+A^TB+B^TA+B^TB)$ Now using the property the trace is linear: $tr(A^TA+A^TB+B^TA+B^TB) = tr(A^TA)+tr(B^TB)+tr(A^TB+B^TA)=||A||^2_F+||B||^2_F+tr(A^TB+B^TA)$ Now if we were to prove that $2||A||_F||B||_F \geq tr(A^TB+B^TA)$ that would solve the question. But I don't see how that's trivial, and generally multiplying $\sum$s together is something I avoid like the plague. Is this indeed the way? would someone help me with this last step?",,"['linear-algebra', 'matrices', 'normed-spaces', 'trace']"
15,What does the inverted V represent in math,What does the inverted V represent in math,,"I know that A V B represents Logical disjunction which means A OR B and the result of it is false only when both A and B are false . But I still didn't understand what an inverted V means as shown in the image below. I know that cij , ail and blj are cells in a matrix but I dont understand the meaning on the whole. Can someone please help.","I know that A V B represents Logical disjunction which means A OR B and the result of it is false only when both A and B are false . But I still didn't understand what an inverted V means as shown in the image below. I know that cij , ail and blj are cells in a matrix but I dont understand the meaning on the whole. Can someone please help.",,"['matrices', 'discrete-mathematics']"
16,Transformation matrix from quadrilateral to rectangle,Transformation matrix from quadrilateral to rectangle,,There exists a rectangle somewhere in space with some orientation. A camera from the coordinate center point is looking along the z axis and is seeing the rectangle as a quadrilateral (due to perspective transformation). How do I get the position and rotation of the rectangle given the points of the quadrilateral?,There exists a rectangle somewhere in space with some orientation. A camera from the coordinate center point is looking along the z axis and is seeing the rectangle as a quadrilateral (due to perspective transformation). How do I get the position and rotation of the rectangle given the points of the quadrilateral?,,"['matrices', 'geometry', 'transformation', 'projective-geometry']"
17,The effect of changing one of the matrix of matrix multiplication on the rank of the resulting new matrix?,The effect of changing one of the matrix of matrix multiplication on the rank of the resulting new matrix?,,"Consider matrix multiplication $C=AB$, where $A\in\mathbb{F}_q^{m\times n}$, $B\in\mathbb{F}_q^{n\times k}$, $q$ is the finite field size (the finite field constraint is actually not relevant to the problem). The ranks of matrices are denoted as $\text{rank}(A)$, $\text{rank}(B)$ and $\text{rank}(C)$. My question is, suppose that I change some rows of $B$ to obtain a new matrix $B'$ which has $\text{rank}(B')<\text{rank}(B)$ and denote $C'=AB'$. Is that safe to say that $\text{rank}(C')<\text{rank}(C)$ (or $\text{rank}(C')\leq \text{rank}(C)$)? If yes, how to rigorously prove? What if another constraint is added to the question that $B'$ is obtained by replacing one/several row(s) of $B$ with linear combinations of existing rows of $B$? Similarly, is that possible to have a similar argument on the change of rank if $C'=A'B$ where $\text{rank}(A')<\text{rank}(A)$? Thanks.","Consider matrix multiplication $C=AB$, where $A\in\mathbb{F}_q^{m\times n}$, $B\in\mathbb{F}_q^{n\times k}$, $q$ is the finite field size (the finite field constraint is actually not relevant to the problem). The ranks of matrices are denoted as $\text{rank}(A)$, $\text{rank}(B)$ and $\text{rank}(C)$. My question is, suppose that I change some rows of $B$ to obtain a new matrix $B'$ which has $\text{rank}(B')<\text{rank}(B)$ and denote $C'=AB'$. Is that safe to say that $\text{rank}(C')<\text{rank}(C)$ (or $\text{rank}(C')\leq \text{rank}(C)$)? If yes, how to rigorously prove? What if another constraint is added to the question that $B'$ is obtained by replacing one/several row(s) of $B$ with linear combinations of existing rows of $B$? Similarly, is that possible to have a similar argument on the change of rank if $C'=A'B$ where $\text{rank}(A')<\text{rank}(A)$? Thanks.",,"['linear-algebra', 'matrices']"
18,"LU factorization problem - Writing a code, don't understand partial pivoting","LU factorization problem - Writing a code, don't understand partial pivoting",,"I'm trying to write a matlab code for the following question: The program gets a matrix $A$ (lets say square matrix) and it returns $P,L,U$ such that $PA=PLU$ and $P$ is the permutation matrix, the partial pivoting matrix. The obvious way to go about this is to: 1) find matrix $P$ via switching rows with the maximum value, and then doing gauss eliminiation, and continuing with the row switching. 2) doing regular LU decomposition of $A$ 3) We now have $PA=PLU$ The problem here is that we essential do gauss elimination twice. once to find the $P$ matrix, and then to find $LU$ such that $A=LU$ The second problem, is that the entire point of partial pivoting is so we don't have to find such $LU$ matrices such that $A=LU$ since we don't know how numerically stable it is. So it defeats the point. My problem is this : after switching rows, I don't know what value to put in matrix $L$. For example: $A=\begin{pmatrix} 3 & 1 & 4\\ 1 & 5 & 9 \\ 9 & 2 & 6\end{pmatrix}$ A row change is needed since $9>3$, so we now have: $P=\begin{pmatrix} 0 & 0 & 1 \\ 0 & 1 & 0 \\ 1 & 0 & 0\end{pmatrix}$ and $PA = \begin{pmatrix} 9 & 2 & 6\\1 & 5 & 9\\ 3 & 1 & 4\end{pmatrix}$ Now I somehow need to find the original $L$ that I would get if I hadn't done the row change, by doing gauss elimination on $PA$ and  I don't understand how that's possible. Take heart, we need to find such matrices $L,U$ such that $A=LU$ without actually doing an lu decomposition of $A$, but of the more numerically stable $PA$ rather. It's kind of hard to explain","I'm trying to write a matlab code for the following question: The program gets a matrix $A$ (lets say square matrix) and it returns $P,L,U$ such that $PA=PLU$ and $P$ is the permutation matrix, the partial pivoting matrix. The obvious way to go about this is to: 1) find matrix $P$ via switching rows with the maximum value, and then doing gauss eliminiation, and continuing with the row switching. 2) doing regular LU decomposition of $A$ 3) We now have $PA=PLU$ The problem here is that we essential do gauss elimination twice. once to find the $P$ matrix, and then to find $LU$ such that $A=LU$ The second problem, is that the entire point of partial pivoting is so we don't have to find such $LU$ matrices such that $A=LU$ since we don't know how numerically stable it is. So it defeats the point. My problem is this : after switching rows, I don't know what value to put in matrix $L$. For example: $A=\begin{pmatrix} 3 & 1 & 4\\ 1 & 5 & 9 \\ 9 & 2 & 6\end{pmatrix}$ A row change is needed since $9>3$, so we now have: $P=\begin{pmatrix} 0 & 0 & 1 \\ 0 & 1 & 0 \\ 1 & 0 & 0\end{pmatrix}$ and $PA = \begin{pmatrix} 9 & 2 & 6\\1 & 5 & 9\\ 3 & 1 & 4\end{pmatrix}$ Now I somehow need to find the original $L$ that I would get if I hadn't done the row change, by doing gauss elimination on $PA$ and  I don't understand how that's possible. Take heart, we need to find such matrices $L,U$ such that $A=LU$ without actually doing an lu decomposition of $A$, but of the more numerically stable $PA$ rather. It's kind of hard to explain",,"['linear-algebra', 'matrices', 'gaussian-elimination', 'matrix-decomposition']"
19,A question on eigenvalues,A question on eigenvalues,,"Let $A,B\in M_{2}(\mathbb{R})$ so that $A^2 = B^2 = I$. Which are eigenvalues of $AB$? 1) $1\pm \sqrt 3$ 2)$3 \pm 2\sqrt2$ 3)$\dfrac {1}{2},2$ 4)$2 \pm  2\sqrt 3$","Let $A,B\in M_{2}(\mathbb{R})$ so that $A^2 = B^2 = I$. Which are eigenvalues of $AB$? 1) $1\pm \sqrt 3$ 2)$3 \pm 2\sqrt2$ 3)$\dfrac {1}{2},2$ 4)$2 \pm  2\sqrt 3$",,"['matrices', 'eigenvalues-eigenvectors']"
20,Why does matrix-matrix product come close to the peak performance of a system?,Why does matrix-matrix product come close to the peak performance of a system?,,"In this paper , I read "" The most important operation is GEMM (GEneral Matrix Multiply), which typically deﬁnes the practical peak performance of a computer system."" But why? Why does matrix-matrix product come close to or even define the peak performance of a system?","In this paper , I read "" The most important operation is GEMM (GEneral Matrix Multiply), which typically deﬁnes the practical peak performance of a computer system."" But why? Why does matrix-matrix product come close to or even define the peak performance of a system?",,['matrices']
21,Commutator subgroup of $GL_{2}(\mathbb{Z}/p^{2}\mathbb{Z})$ is $SL_{2}(\mathbb{Z}/p^{2}\mathbb{Z})$,Commutator subgroup of  is,GL_{2}(\mathbb{Z}/p^{2}\mathbb{Z}) SL_{2}(\mathbb{Z}/p^{2}\mathbb{Z}),"How would I go about showing this, where $p$ is an odd prime? The inclusion $[GL_{2}(\mathbb{Z}/p^{2}\mathbb{Z}),GL_{2}(\mathbb{Z}/p^{2}\mathbb{Z})] \subseteq SL_{2}(\mathbb{Z}/p^{2}\mathbb{Z})$ is relatively clear. I'm wondering if the proof for the other inclusion follows from the result that $[GL_{2}(\mathbb{Z}/p\mathbb{Z}),GL_{2}(\mathbb{Z}/p\mathbb{Z})]=SL_{2}(\mathbb{Z}/p\mathbb{Z})$. I've tried to use the result above with the natural homomorphism from $GL_{2}(\mathbb{Z}/p^{2}\mathbb{Z})$ to $GL_{2}(\mathbb{Z}/p\mathbb{Z})$ to prove this but to no avail.","How would I go about showing this, where $p$ is an odd prime? The inclusion $[GL_{2}(\mathbb{Z}/p^{2}\mathbb{Z}),GL_{2}(\mathbb{Z}/p^{2}\mathbb{Z})] \subseteq SL_{2}(\mathbb{Z}/p^{2}\mathbb{Z})$ is relatively clear. I'm wondering if the proof for the other inclusion follows from the result that $[GL_{2}(\mathbb{Z}/p\mathbb{Z}),GL_{2}(\mathbb{Z}/p\mathbb{Z})]=SL_{2}(\mathbb{Z}/p\mathbb{Z})$. I've tried to use the result above with the natural homomorphism from $GL_{2}(\mathbb{Z}/p^{2}\mathbb{Z})$ to $GL_{2}(\mathbb{Z}/p\mathbb{Z})$ to prove this but to no avail.",,"['abstract-algebra', 'group-theory', 'matrices', 'linear-groups']"
22,How are the eigenvalues affected for a kind of similarity transformation,How are the eigenvalues affected for a kind of similarity transformation,,"Let $\mathbf{A}$ be a $N\times N$ Hermitian Matrix. Let $\mathbf{Q}$ be a $N\times N$ unitary matrix. Let $\mathbf{Q}_r$ be the matrix formed by using the first $N-1$ columns of $\mathbf{Q}$. Can we say anything about the eigenvaules of $\mathbf{Q}_r^H\mathbf{AQ}_r$? I know they are real and should be in the interval $[\lambda_{min}(\mathbf{A}),\lambda_{max}(\mathbf{A})]$.","Let $\mathbf{A}$ be a $N\times N$ Hermitian Matrix. Let $\mathbf{Q}$ be a $N\times N$ unitary matrix. Let $\mathbf{Q}_r$ be the matrix formed by using the first $N-1$ columns of $\mathbf{Q}$. Can we say anything about the eigenvaules of $\mathbf{Q}_r^H\mathbf{AQ}_r$? I know they are real and should be in the interval $[\lambda_{min}(\mathbf{A}),\lambda_{max}(\mathbf{A})]$.",,"['linear-algebra', 'matrices']"
23,Calculating eigenvectors and eigenvalues of a 2x2 complex matrix,Calculating eigenvectors and eigenvalues of a 2x2 complex matrix,,"I've previously asked elsewhere, https://stackoverflow.com/questions/21118820/non-trivial-eigenvectors-of-a-22-matrix-in-code , how to calculate the eigenvectors and eigenvalues of a 2x2 matrix in a programming language. I am still working with a 2x2 matrix ($A$) but it's now complex, and takes the form: $$         A = \begin{bmatrix}         a+jb & c + jd \\         e + jf & g +jh \\         \end{bmatrix} $$ where $j$ is the square root of $-1$. Do the equations in http://www.math.harvard.edu/archive/21b_fall_04/exhibits/2dmatrices/index.html (which is used as a part of the solution in the previous link) still hold?","I've previously asked elsewhere, https://stackoverflow.com/questions/21118820/non-trivial-eigenvectors-of-a-22-matrix-in-code , how to calculate the eigenvectors and eigenvalues of a 2x2 matrix in a programming language. I am still working with a 2x2 matrix ($A$) but it's now complex, and takes the form: $$         A = \begin{bmatrix}         a+jb & c + jd \\         e + jf & g +jh \\         \end{bmatrix} $$ where $j$ is the square root of $-1$. Do the equations in http://www.math.harvard.edu/archive/21b_fall_04/exhibits/2dmatrices/index.html (which is used as a part of the solution in the previous link) still hold?",,"['matrices', 'eigenvalues-eigenvectors', 'computational-mathematics', 'matrix-equations']"
24,Which of the following sets are compact? [duplicate],Which of the following sets are compact? [duplicate],,"This question already has an answer here : Which of the following sets are compact in $\mathbb{M}_n(\mathbb{R})$[NBHM_PhD Screening Test-2013, Topology] (1 answer) Closed 7 years ago . The set of all upper triangular matrices in $\mathbb M(n,\mathbb R)$ such that all their eigenvalues satisfy $|\lambda| \leq 2$. The set of all real symmetric matrices in $\mathbb M(n,\mathbb R)$ such that all their eigenvalues satisfy $|\lambda| \leq 2$. The set of all diagonalisable matrices in $\mathbb M(n,\mathbb R)$ such that all their eigenvalues satisfy $|\lambda| \leq 2$.","This question already has an answer here : Which of the following sets are compact in $\mathbb{M}_n(\mathbb{R})$[NBHM_PhD Screening Test-2013, Topology] (1 answer) Closed 7 years ago . The set of all upper triangular matrices in $\mathbb M(n,\mathbb R)$ such that all their eigenvalues satisfy $|\lambda| \leq 2$. The set of all real symmetric matrices in $\mathbb M(n,\mathbb R)$ such that all their eigenvalues satisfy $|\lambda| \leq 2$. The set of all diagonalisable matrices in $\mathbb M(n,\mathbb R)$ such that all their eigenvalues satisfy $|\lambda| \leq 2$.",,"['real-analysis', 'matrices']"
25,Is anybody here able to construct this example?,Is anybody here able to construct this example?,,"I need to construct an example for such a situation: Let $x_1,x_2$ and $v_1,v_2$ be four vectors in $\mathbb{C}^2$, so that they are mutually different from each other. Further, there have to be $p_1,p_2,q_1,q_2$ such that all are $\in [0,1]$ and $p_1+p_2=q_1+q_2=1$ and we have that: $\sum_{i=1}^2 p_ix_ix_i^T = \sum_{i=1}^2 q_i v_i v_i^T$ (So they represent the ""same map""). Remark: This should become an example that the density operator in physics is not unique in general.","I need to construct an example for such a situation: Let $x_1,x_2$ and $v_1,v_2$ be four vectors in $\mathbb{C}^2$, so that they are mutually different from each other. Further, there have to be $p_1,p_2,q_1,q_2$ such that all are $\in [0,1]$ and $p_1+p_2=q_1+q_2=1$ and we have that: $\sum_{i=1}^2 p_ix_ix_i^T = \sum_{i=1}^2 q_i v_i v_i^T$ (So they represent the ""same map""). Remark: This should become an example that the density operator in physics is not unique in general.",,['linear-algebra']
26,A complex matrix minus a diagonal matrix to be nilpotent...,A complex matrix minus a diagonal matrix to be nilpotent...,,"Let $A$ be a complex matrix, show that there exists a diagonal matrix $B$ and an integer $m$ such that $(A-B)^m=0$.","Let $A$ be a complex matrix, show that there exists a diagonal matrix $B$ and an integer $m$ such that $(A-B)^m=0$.",,"['linear-algebra', 'matrices']"
27,Unitary map between sets of vectors,Unitary map between sets of vectors,,"Suppose I have two sets of vectors, $E_1=\{v_i\}_{i=1}^{k}$ and $E_2=\{u_i\}_{i=1}^{k}$, with each vector belonging to $\mathbb{C}^k$. When is it possible to find a unitary matrix that maps $E_1$ to $E_2$? Is enough for the pairwise inner products between vectors in the set to be equal?","Suppose I have two sets of vectors, $E_1=\{v_i\}_{i=1}^{k}$ and $E_2=\{u_i\}_{i=1}^{k}$, with each vector belonging to $\mathbb{C}^k$. When is it possible to find a unitary matrix that maps $E_1$ to $E_2$? Is enough for the pairwise inner products between vectors in the set to be equal?",,"['linear-algebra', 'matrices', 'vector-spaces']"
28,"Let $\mathrm {A,B}$ be matrices does $\mathrm A^k = \mathrm B^k \implies \mathrm A = \mathrm B$?",Let  be matrices does ?,"\mathrm {A,B} \mathrm A^k = \mathrm B^k \implies \mathrm A = \mathrm B","Let $A, B$ be real $n\times n$ matrices with positive eigenvalues. If $A^2 = B^2$, then $A = B$. What about $A^k = B^k, k\geq2$? Could Anybody help me? Thanks.","Let $A, B$ be real $n\times n$ matrices with positive eigenvalues. If $A^2 = B^2$, then $A = B$. What about $A^k = B^k, k\geq2$? Could Anybody help me? Thanks.",,"['linear-algebra', 'matrices']"
29,What are sufficient conditions for a matrix to have the same eigenvectors as its exponential?,What are sufficient conditions for a matrix to have the same eigenvectors as its exponential?,,"If $\boldsymbol{A}$ is a square matrix, then it is straightforward to show that each eigenvector to $\boldsymbol{A}$ is also an eigenvector to $e^\boldsymbol{A}$. On the other hand, an eigenvector to $e^\boldsymbol{A}$ is not necessarily an eigenvector to $\boldsymbol{A}$. For example, the eigenvectors to $$ \boldsymbol{A}= \begin{bmatrix} 2\pi i& 0 \\ 0 & 4\pi i \end{bmatrix} $$ are the scalar multiples of $\begin{bmatrix}1 & 0\end{bmatrix}^T$ and $\begin{bmatrix}0 & 1\end{bmatrix}^T$, while any non-zero vector is an eigenvector to $$ e^\boldsymbol{A}= \begin{bmatrix} 1 & 0 \\ 0 & 1 \end{bmatrix}. $$ Are there any interesting conditions which guarantee that $\boldsymbol{A}$ and $e^\boldsymbol{A}$ have exactly the same eigenvectors?","If $\boldsymbol{A}$ is a square matrix, then it is straightforward to show that each eigenvector to $\boldsymbol{A}$ is also an eigenvector to $e^\boldsymbol{A}$. On the other hand, an eigenvector to $e^\boldsymbol{A}$ is not necessarily an eigenvector to $\boldsymbol{A}$. For example, the eigenvectors to $$ \boldsymbol{A}= \begin{bmatrix} 2\pi i& 0 \\ 0 & 4\pi i \end{bmatrix} $$ are the scalar multiples of $\begin{bmatrix}1 & 0\end{bmatrix}^T$ and $\begin{bmatrix}0 & 1\end{bmatrix}^T$, while any non-zero vector is an eigenvector to $$ e^\boldsymbol{A}= \begin{bmatrix} 1 & 0 \\ 0 & 1 \end{bmatrix}. $$ Are there any interesting conditions which guarantee that $\boldsymbol{A}$ and $e^\boldsymbol{A}$ have exactly the same eigenvectors?",,"['matrices', 'eigenvalues-eigenvectors']"
30,Transpose a square matrix code,Transpose a square matrix code,,"I know it's not programming area , but I think it's more related to math. I have the following function: public void transpose()     {         for(int i = 0; i < mat_size; ++i) {             for(int j = 0; j < i ; ++j) {                 double tmpJI = get(j, i);                 put(j, i, get(i, j));                 put(i, j, tmpJI);             }         }     } Or in plain english, suppose matrix size = matix's number rows = matrix's num cols. from $i = 0 $ to $size$ do: --> for $j = 0$ to $i$ do: -----> replace $mat_{i,j}$ with $mat_{j,i}$ --> make $j$ bigger in 1 make $i$ bigger in 1. For the following matrix: $$\begin{matrix}         1 & 0 & 0 \\         5 & 1 & 0 \\         6 & 5 & 1 \\         \end{matrix}$$ the transpose output is: $$\begin{matrix}         1 & 0 & 6 \\         5 & 1 & 0 \\         0 & 5 & 1 \\         \end{matrix}$$ when the correct transpose is: $$\begin{matrix}         1 & 5 & 6 \\         0 & 1 & 5 \\         0 & 0 & 1 \\         \end{matrix}$$ What is my problem?","I know it's not programming area , but I think it's more related to math. I have the following function: public void transpose()     {         for(int i = 0; i < mat_size; ++i) {             for(int j = 0; j < i ; ++j) {                 double tmpJI = get(j, i);                 put(j, i, get(i, j));                 put(i, j, tmpJI);             }         }     } Or in plain english, suppose matrix size = matix's number rows = matrix's num cols. from $i = 0 $ to $size$ do: --> for $j = 0$ to $i$ do: -----> replace $mat_{i,j}$ with $mat_{j,i}$ --> make $j$ bigger in 1 make $i$ bigger in 1. For the following matrix: $$\begin{matrix}         1 & 0 & 0 \\         5 & 1 & 0 \\         6 & 5 & 1 \\         \end{matrix}$$ the transpose output is: $$\begin{matrix}         1 & 0 & 6 \\         5 & 1 & 0 \\         0 & 5 & 1 \\         \end{matrix}$$ when the correct transpose is: $$\begin{matrix}         1 & 5 & 6 \\         0 & 1 & 5 \\         0 & 0 & 1 \\         \end{matrix}$$ What is my problem?",,"['matrices', 'applications']"
31,Simplifying $\mathbf{X}(\mathbf{X}+a\mathbf{I})^{-1}$,Simplifying,\mathbf{X}(\mathbf{X}+a\mathbf{I})^{-1},"I'm having trouble simplifying the following expression in matrix form: $$\mathbf{X}(\mathbf{X}+a\mathbf{I})^{-1}$$ Where $\mathbf{X}$ is an invertible $n \times n$ matrix, $a$ is a scalar value, and $\mathbf{I}$ is the identity matrix. I reasoned that since the product of a matrix times its inverse is the identity matrix, the product of a matrix times the inverse of a ""shifted"" matrix is simply the shift value. In other words, the above would simplify to $a\mathbf{I}$. However, I don't know if that is correct, and if it is I don't know the linear algebra steps to prove it.","I'm having trouble simplifying the following expression in matrix form: $$\mathbf{X}(\mathbf{X}+a\mathbf{I})^{-1}$$ Where $\mathbf{X}$ is an invertible $n \times n$ matrix, $a$ is a scalar value, and $\mathbf{I}$ is the identity matrix. I reasoned that since the product of a matrix times its inverse is the identity matrix, the product of a matrix times the inverse of a ""shifted"" matrix is simply the shift value. In other words, the above would simplify to $a\mathbf{I}$. However, I don't know if that is correct, and if it is I don't know the linear algebra steps to prove it.",,"['linear-algebra', 'matrices', 'matrix-equations']"
32,Solving a system of three linear equations with three unknowns,Solving a system of three linear equations with three unknowns,,"Is my working correct or am I completely wrong? Have I missed anything out? Any feedback is appreciated. Question: Consider the following system of equations $2x + 2y + z = 2$ $−x + 2y − z = −5$ $x − 3y + 2z = 8$ Form an augmented matrix, then reduce this matrix to reduced row echelon form and solve the system. My answer/working: Given: $2x + 2y + z = 2$ $-x + 2y - z = -5$ $x - 3y + 2z = 8$ Matrix form: $\begin{pmatrix} 2 & 2 & 1 & 2\\ -1 & 2 & -1 & -5 \\ 1& -3& 2 & 8 \end{pmatrix}$ $\begin{pmatrix}2 & 0 & 0 & 2\\ 0 & 3 & 0 & -3\\ 0 & 0 & \frac56 & \frac53\end{pmatrix}$ Solution: $x = 1; y = -1; z = 2;$","Is my working correct or am I completely wrong? Have I missed anything out? Any feedback is appreciated. Question: Consider the following system of equations $2x + 2y + z = 2$ $−x + 2y − z = −5$ $x − 3y + 2z = 8$ Form an augmented matrix, then reduce this matrix to reduced row echelon form and solve the system. My answer/working: Given: $2x + 2y + z = 2$ $-x + 2y - z = -5$ $x - 3y + 2z = 8$ Matrix form: $\begin{pmatrix} 2 & 2 & 1 & 2\\ -1 & 2 & -1 & -5 \\ 1& -3& 2 & 8 \end{pmatrix}$ $\begin{pmatrix}2 & 0 & 0 & 2\\ 0 & 3 & 0 & -3\\ 0 & 0 & \frac56 & \frac53\end{pmatrix}$ Solution: $x = 1; y = -1; z = 2;$",,"['linear-algebra', 'matrices', 'systems-of-equations']"
33,A question about Golden - Thompson inequality,A question about Golden - Thompson inequality,,"Given two hermitian matrices $A$ and $B$, the Golden - Thompson inequality states: $$tr\left(e^{(A+B)}\right)\le tr\left(e^Ae^B\right)$$ My question is: when the two traces are equal? Thanks.","Given two hermitian matrices $A$ and $B$, the Golden - Thompson inequality states: $$tr\left(e^{(A+B)}\right)\le tr\left(e^Ae^B\right)$$ My question is: when the two traces are equal? Thanks.",,['matrices']
34,Regression with a Vandermonde matrix.,Regression with a Vandermonde matrix.,,"From what I understand, when doing a least squares regression with a Vandermonde matrix, you're essentially solving the equation $y=Xa$ Where $y$ is a vector of $y$ -values, $X$ is the Vandermonde matrix, and $a$ is a vector of coefficients. When you solve this equation for $a$ , $a=(X^TX)^{-1}X^Ty$ You get the above expression for $a$ . My understanding is that this should be a solution to the set of equations. However, it is possible to fit $n$ points of data to a $k$ -th degree polynomial, where $n>k$ . This would imply that there are more equations than unknowns, which results in no possible solutions. However, the vector a can be calculated. This resulting vector does not completely perfectly satisfy $y=Xa$ But instead is a good approximation, as with regression. Why can we get a value for $a$ ? As I do not believe this could be possible if we were dealing with $n$ equations, and $k$ unknowns. Where does the matrix solution differ from solving $k$ unknowns with $n$ equations?","From what I understand, when doing a least squares regression with a Vandermonde matrix, you're essentially solving the equation Where is a vector of -values, is the Vandermonde matrix, and is a vector of coefficients. When you solve this equation for , You get the above expression for . My understanding is that this should be a solution to the set of equations. However, it is possible to fit points of data to a -th degree polynomial, where . This would imply that there are more equations than unknowns, which results in no possible solutions. However, the vector a can be calculated. This resulting vector does not completely perfectly satisfy But instead is a good approximation, as with regression. Why can we get a value for ? As I do not believe this could be possible if we were dealing with equations, and unknowns. Where does the matrix solution differ from solving unknowns with equations?",y=Xa y y X a a a=(X^TX)^{-1}X^Ty a n k n>k y=Xa a n k k n,"['matrices', 'regression', 'least-squares']"
35,A basic question on determinant and rank of a matrix,A basic question on determinant and rank of a matrix,,How to prove that if the determinant of a $n \times n$ matrix is zero then the rank is less than $n$. I can prove the converse. Only a hint is enough. My definition of rank is the maximum number of linearly dependent columns.,How to prove that if the determinant of a $n \times n$ matrix is zero then the rank is less than $n$. I can prove the converse. Only a hint is enough. My definition of rank is the maximum number of linearly dependent columns.,,['linear-algebra']
36,Similar matrices [duplicate],Similar matrices [duplicate],,"This question already has an answer here : If $\mathrm{Tr}(A)=0$ then $T=R^{-1}AR$ has all entries on its main diagonal equal to $0$ (1 answer) Closed 4 years ago . Let $A$ be a complex $n×n$ matrix with $tr(A) = 0$. Show that there exists an invertible $n×n$ matrix $B$ such that all diagonal entries of $BAB^{-1}$ are zeros. $\bf Edit:$ Is the fact that $\forall A\in M_{m\times n}(D)$, $D$ is $PID$, then $A$ is equivalent to a matrix which has the diagonal form $diag\{d_1,...,d_2,0,..,0\}$ where $d_i|d_j, i\leq j$and the other entries are zeros, applicable in this question?","This question already has an answer here : If $\mathrm{Tr}(A)=0$ then $T=R^{-1}AR$ has all entries on its main diagonal equal to $0$ (1 answer) Closed 4 years ago . Let $A$ be a complex $n×n$ matrix with $tr(A) = 0$. Show that there exists an invertible $n×n$ matrix $B$ such that all diagonal entries of $BAB^{-1}$ are zeros. $\bf Edit:$ Is the fact that $\forall A\in M_{m\times n}(D)$, $D$ is $PID$, then $A$ is equivalent to a matrix which has the diagonal form $diag\{d_1,...,d_2,0,..,0\}$ where $d_i|d_j, i\leq j$and the other entries are zeros, applicable in this question?",,"['linear-algebra', 'matrices', 'modules']"
37,Trying to prove a matrix is always convergent.,Trying to prove a matrix is always convergent.,,"I have a matrix $Z$ of the form $Z = \left[Q^{-1}-Q^{-1}A^T\left(AQ^{-1}A^T\right)^{-1}AQ^{-1}\right]\Phi$ where, $\Phi$ is a diagonal matrix of real non-negative values. $\Theta$ (not explicitly in the equation above) is another diagonal matrix of real non-negative values. $Q = \Phi + \Theta$ such that $Q$ is a diagonal matrix of real positive values. $A$ is a rectangular matrix of the form $\left[J \quad -I\right]$ where the elements of $J$ are real values and $I$ is the identity matrix of appropriate size. Matrix dimensions are as follows: $\Phi, \Theta, Q$ are of dimension $(n+m)\times (n+m)$. $A$ is of dimension $m\times (n+m)$. $J$ is of dimension $m\times n$. $I$ is of dimension $m\times m$. $Z$ is the plant matrix of a discrete time linear time-invariant state space equation, and I have reason to believe that, given the properties above, $Z$ is always convergent. As the state space equation is in discrete time, the conditions for a convergent plant are that all eigenvalues must have magnitude $\leq 1$. Also, should there be an eigenvalue with magnitude $= 1$, its multiplicity must be 1 to be at least Lyapunov stable. I would appreciate any help in either proving or disproving my claim. So far, I have been able to prove that $Q^{-1}\Phi$ and $Q^{-1}A^T\left(AQ^{-1}A^T\right)^{-1}AQ^{-1}\Phi$ are both symmetric positive semi-definite. I have tried applying the Sherman–Morrison–Woodbury formula only to show that $Z$ is singular. I can also show that $Q^{-1}\Phi$ has a spectral radius $\leq 1$ but not the same for $Q^{-1}A^T\left(AQ^{-1}A^T\right)^{-1}AQ^{-1}\Phi$.","I have a matrix $Z$ of the form $Z = \left[Q^{-1}-Q^{-1}A^T\left(AQ^{-1}A^T\right)^{-1}AQ^{-1}\right]\Phi$ where, $\Phi$ is a diagonal matrix of real non-negative values. $\Theta$ (not explicitly in the equation above) is another diagonal matrix of real non-negative values. $Q = \Phi + \Theta$ such that $Q$ is a diagonal matrix of real positive values. $A$ is a rectangular matrix of the form $\left[J \quad -I\right]$ where the elements of $J$ are real values and $I$ is the identity matrix of appropriate size. Matrix dimensions are as follows: $\Phi, \Theta, Q$ are of dimension $(n+m)\times (n+m)$. $A$ is of dimension $m\times (n+m)$. $J$ is of dimension $m\times n$. $I$ is of dimension $m\times m$. $Z$ is the plant matrix of a discrete time linear time-invariant state space equation, and I have reason to believe that, given the properties above, $Z$ is always convergent. As the state space equation is in discrete time, the conditions for a convergent plant are that all eigenvalues must have magnitude $\leq 1$. Also, should there be an eigenvalue with magnitude $= 1$, its multiplicity must be 1 to be at least Lyapunov stable. I would appreciate any help in either proving or disproving my claim. So far, I have been able to prove that $Q^{-1}\Phi$ and $Q^{-1}A^T\left(AQ^{-1}A^T\right)^{-1}AQ^{-1}\Phi$ are both symmetric positive semi-definite. I have tried applying the Sherman–Morrison–Woodbury formula only to show that $Z$ is singular. I can also show that $Q^{-1}\Phi$ has a spectral radius $\leq 1$ but not the same for $Q^{-1}A^T\left(AQ^{-1}A^T\right)^{-1}AQ^{-1}\Phi$.",,"['linear-algebra', 'matrices', 'eigenvalues-eigenvectors', 'dynamical-systems']"
38,Derivative of trace of a matrix,Derivative of trace of a matrix,,$$\dfrac{\partial\operatorname{Trace}\left[\left(AB\right)^{T}Q\left(AB\right)\right]}{\partial A}=\text{ ?}$$ where $Q = Q^T>0$,$$\dfrac{\partial\operatorname{Trace}\left[\left(AB\right)^{T}Q\left(AB\right)\right]}{\partial A}=\text{ ?}$$ where $Q = Q^T>0$,,"['linear-algebra', 'matrices', 'partial-derivative']"
39,Is it really true that $A^2 = -A \Leftrightarrow (I + A)^2 = A$?,Is it really true that ?,A^2 = -A \Leftrightarrow (I + A)^2 = A,"$A$ is a generic square matrix and $I$ is the identity matrix. I failed to prove that, but I managed to disprove it: \begin{align*}   A &= [-1] \\   A^2 &= ([-1])^2 = [1] = -A \\   (I + A)^2 &= ([1] + [-1])^2 = [0] \neq A \end{align*} So... am I missing something? Or is the text wrong?","$A$ is a generic square matrix and $I$ is the identity matrix. I failed to prove that, but I managed to disprove it: \begin{align*}   A &= [-1] \\   A^2 &= ([-1])^2 = [1] = -A \\   (I + A)^2 &= ([1] + [-1])^2 = [0] \neq A \end{align*} So... am I missing something? Or is the text wrong?",,"['linear-algebra', 'matrices']"
40,Matrices with trace zero. [duplicate],Matrices with trace zero. [duplicate],,"This question already has an answer here : If $\mathrm{Tr}(A)=0$ then $T=R^{-1}AR$ has all entries on its main diagonal equal to $0$ (1 answer) Closed 4 years ago . I would like to show that every trace zero square matrix is similar to one with zero diagonal elements. This question has been asked before, and has had an answer by Don Antonio . And my problem is that I cannot understand the cited paper. In the cited paper , (proof 4), one finds the sentence Since $\text{Tr}(K) = \text{Tr}(B^{–1}SB) = \text{Tr}(S) = 0$, this step can be    repeated to replace $K$ by a matrix whose every diagonal element is zero ( thereby changing $c$ and $r^T$ ) thus constructing $C$ so that every diagonal element of $C^{–1}SC$ is zero. Question I thought that, since $\text{Tr}(K)=0$, we can, by the induction hypothesis, find an invertible $D$, such that $D^{-1}KD$ has diagonal elements $=0$. But how does this enable us to replace $K$ by a matrix with zero diagonal elements, at the cost of changing $r^T$ and $c$? I tried to constuct some matrix $C$ from $D$ such that $C^{-1}\begin{bmatrix}0&r^T\\c&K\end{bmatrix}C=\begin{bmatrix}0&r'^T\\c'&K'\end{bmatrix}$ with $K'$ having zero diagonal elements, but to no avail. I have tried various choices of $C$, but the result of the multiplication refuses to be of the required form, so I wonder if I am missing something here? Any hint is well-appreciated.","This question already has an answer here : If $\mathrm{Tr}(A)=0$ then $T=R^{-1}AR$ has all entries on its main diagonal equal to $0$ (1 answer) Closed 4 years ago . I would like to show that every trace zero square matrix is similar to one with zero diagonal elements. This question has been asked before, and has had an answer by Don Antonio . And my problem is that I cannot understand the cited paper. In the cited paper , (proof 4), one finds the sentence Since $\text{Tr}(K) = \text{Tr}(B^{–1}SB) = \text{Tr}(S) = 0$, this step can be    repeated to replace $K$ by a matrix whose every diagonal element is zero ( thereby changing $c$ and $r^T$ ) thus constructing $C$ so that every diagonal element of $C^{–1}SC$ is zero. Question I thought that, since $\text{Tr}(K)=0$, we can, by the induction hypothesis, find an invertible $D$, such that $D^{-1}KD$ has diagonal elements $=0$. But how does this enable us to replace $K$ by a matrix with zero diagonal elements, at the cost of changing $r^T$ and $c$? I tried to constuct some matrix $C$ from $D$ such that $C^{-1}\begin{bmatrix}0&r^T\\c&K\end{bmatrix}C=\begin{bmatrix}0&r'^T\\c'&K'\end{bmatrix}$ with $K'$ having zero diagonal elements, but to no avail. I have tried various choices of $C$, but the result of the multiplication refuses to be of the required form, so I wonder if I am missing something here? Any hint is well-appreciated.",,"['linear-algebra', 'matrices']"
41,What is wrong with this Jordan normal form computation?,What is wrong with this Jordan normal form computation?,,"The question I am working on is to compute the Jordan normal form of $$A := \begin{pmatrix} 2 & 1 & 5 \\ 0 & 1 & 3\\ 1 & 0 & 1\end{pmatrix}.$$ The characteristic polynomial and minimal polynomial of $A$ is $x^{2}(x - 4)$. Then the Jordan normal form of $A$ is given by $$J := \begin{pmatrix} 0 & 1 & 0\\ 0 & 0 & 0\\ 0 & 0 & 4 \end{pmatrix}.$$ Then there exist a matrix $P$ such that $P^{-1}AP = J$. I am having an issue finding $P$. From the theory of Jordan normal forms, $P = [w_{1}\, w_{2}\, w_{3}]$ where $w_{1}, w_{2}$ is the basis of the nullspace of $(A - 0\cdot I)^{2}$ (where $I$ is the identity matrix) and $w_{3}$ is the basis of the nullspace of $(A - 4\cdot I)$. We first consider $(A - 0 \cdot I)^{2}$. As $$A^{2} = \begin{pmatrix} 9 & 3 & 18\\ 3 & 1 & 6\\ 3& 1 & 6\end{pmatrix} \sim \begin{pmatrix} 1 & 1/3 & 2 \\ 0 & 0 & 0 \\ 0 & 0 & 0 \end{pmatrix}$$ (where the $\sim$ denotes row equivalence), the nullspace of $A^{2}$ is spanned the column vectors $\{(-2, 0, 1)^{t}, (-1, 3, 0)^{t}\}$. Next we consider $(A - 4 \cdot I)$. We have $$A - 4I = \begin{pmatrix} -2 & 1 & 5\\ 0 & -3 & 3\\ 1 & 0 & -3 \end{pmatrix} \sim \begin{pmatrix} 1 & 0 & -3\\0 & 1 & -1\\ 0 & 0 & 0\end{pmatrix}.$$ Then the nullspace is spanned by the column vector $(3, 1, 1)^{t}$. Therefore we should have $$P = \begin{pmatrix} -2 & -1 & 3\\0 & 3 & 1\\ 1 & 0 & 1 \end{pmatrix}.$$ However, when I compute $P^{-1}AP$, I get $$P^{-1}AP = \begin{pmatrix} -1 & -1 & 0\\ 1 & 1 & 0\\0 & 0 & 4\end{pmatrix} \neq J.$$ Where did I go wrong? Is there something wrong on how I computed $P$?","The question I am working on is to compute the Jordan normal form of $$A := \begin{pmatrix} 2 & 1 & 5 \\ 0 & 1 & 3\\ 1 & 0 & 1\end{pmatrix}.$$ The characteristic polynomial and minimal polynomial of $A$ is $x^{2}(x - 4)$. Then the Jordan normal form of $A$ is given by $$J := \begin{pmatrix} 0 & 1 & 0\\ 0 & 0 & 0\\ 0 & 0 & 4 \end{pmatrix}.$$ Then there exist a matrix $P$ such that $P^{-1}AP = J$. I am having an issue finding $P$. From the theory of Jordan normal forms, $P = [w_{1}\, w_{2}\, w_{3}]$ where $w_{1}, w_{2}$ is the basis of the nullspace of $(A - 0\cdot I)^{2}$ (where $I$ is the identity matrix) and $w_{3}$ is the basis of the nullspace of $(A - 4\cdot I)$. We first consider $(A - 0 \cdot I)^{2}$. As $$A^{2} = \begin{pmatrix} 9 & 3 & 18\\ 3 & 1 & 6\\ 3& 1 & 6\end{pmatrix} \sim \begin{pmatrix} 1 & 1/3 & 2 \\ 0 & 0 & 0 \\ 0 & 0 & 0 \end{pmatrix}$$ (where the $\sim$ denotes row equivalence), the nullspace of $A^{2}$ is spanned the column vectors $\{(-2, 0, 1)^{t}, (-1, 3, 0)^{t}\}$. Next we consider $(A - 4 \cdot I)$. We have $$A - 4I = \begin{pmatrix} -2 & 1 & 5\\ 0 & -3 & 3\\ 1 & 0 & -3 \end{pmatrix} \sim \begin{pmatrix} 1 & 0 & -3\\0 & 1 & -1\\ 0 & 0 & 0\end{pmatrix}.$$ Then the nullspace is spanned by the column vector $(3, 1, 1)^{t}$. Therefore we should have $$P = \begin{pmatrix} -2 & -1 & 3\\0 & 3 & 1\\ 1 & 0 & 1 \end{pmatrix}.$$ However, when I compute $P^{-1}AP$, I get $$P^{-1}AP = \begin{pmatrix} -1 & -1 & 0\\ 1 & 1 & 0\\0 & 0 & 4\end{pmatrix} \neq J.$$ Where did I go wrong? Is there something wrong on how I computed $P$?",,"['linear-algebra', 'matrices', 'solution-verification', 'jordan-normal-form']"
42,Spectrum of Lyapunov exponents of a linear system,Spectrum of Lyapunov exponents of a linear system,,"Question : How to show that the eigenvalues of matrices $\mathbf{A}$ and $ \mathbf{L} = \log \lim_{t \to \infty} \left((e^{\mathbf{A}t}e^{\mathbf{A^T}t})^{\frac{1}{2t}}\right) $ have equal real parts? Motivation : Consider a system $$ \dot{\mathbf{x}} = \mathbf{Ax} $$ where $\mathbf{A}$ is some constant matrx and $\mathbf{M}(t) := e^{\mathbf{A}t}$ is the evolution operator. The Lyapunov exponents of this system are then given by the eigenvalues of $$ \mathbf{L} := \log \lim_{t \to \infty} \left((\mathbf{M}(t)\mathbf{M^T}(t))^{\frac{1}{2t}}\right) =  \log \lim_{t \to \infty} \left((e^{\mathbf{A}t}e^{\mathbf{A^T}t})^{\frac{1}{2t}}\right) $$ Now, on the other hand, the rate of expansion for a system above is constant and is also given by the real parts of $\mathbf{A}$'s eigenvalues. Therefore, eigenvalues of $\mathbf{L}$ are equal to real parts of eigenvalues of $\mathbf{A}$. I have checked it numerically and it also makes sense intuitively, but I don't see why, technically, the relation holds.","Question : How to show that the eigenvalues of matrices $\mathbf{A}$ and $ \mathbf{L} = \log \lim_{t \to \infty} \left((e^{\mathbf{A}t}e^{\mathbf{A^T}t})^{\frac{1}{2t}}\right) $ have equal real parts? Motivation : Consider a system $$ \dot{\mathbf{x}} = \mathbf{Ax} $$ where $\mathbf{A}$ is some constant matrx and $\mathbf{M}(t) := e^{\mathbf{A}t}$ is the evolution operator. The Lyapunov exponents of this system are then given by the eigenvalues of $$ \mathbf{L} := \log \lim_{t \to \infty} \left((\mathbf{M}(t)\mathbf{M^T}(t))^{\frac{1}{2t}}\right) =  \log \lim_{t \to \infty} \left((e^{\mathbf{A}t}e^{\mathbf{A^T}t})^{\frac{1}{2t}}\right) $$ Now, on the other hand, the rate of expansion for a system above is constant and is also given by the real parts of $\mathbf{A}$'s eigenvalues. Therefore, eigenvalues of $\mathbf{L}$ are equal to real parts of eigenvalues of $\mathbf{A}$. I have checked it numerically and it also makes sense intuitively, but I don't see why, technically, the relation holds.",,"['matrices', 'ordinary-differential-equations', 'eigenvalues-eigenvectors']"
43,The index of nilpotency of a nilpotent matrix,The index of nilpotency of a nilpotent matrix,,"Let $A$ a   matrix in $\mathcal{M}_5(\mathbb C)$ such that $A^5=0$ and $\mathrm{rank}(A^2)=2$, how prove that $A$ is nilpotent with index of nilpotency $4$? Thanks in advance.","Let $A$ a   matrix in $\mathcal{M}_5(\mathbb C)$ such that $A^5=0$ and $\mathrm{rank}(A^2)=2$, how prove that $A$ is nilpotent with index of nilpotency $4$? Thanks in advance.",,['linear-algebra']
44,"For $A(t)$ differentiable, taking positive matrices as values, how show $\sqrt{A(t)}$ is differentiable?","For  differentiable, taking positive matrices as values, how show  is differentiable?",A(t) \sqrt{A(t)},"On p. 150 of Lax's Linear Algebra , he mentions that is is not hard to show that if $R(t)$ is a differentiable matrix-valued function of a single variable, whose values are positive matrices, then the square root function $\sqrt{R(t)}$, is differentiable as well. I have been trying to prove this without success. The definition Lax gives for the derivative of a matrix valued function is that $$\lim_{h\to 0}\left\|\frac{A(t_0+h) - A(t_0)}{h}- \Omega_{t_0}\right\| = 0 $$ for some matrix $\Omega_{t_0}$, where the double bars denotes the matrix norm. I believe this is equivalent to the existence of the $n^2$ entry-wise derivatives. I have been trying to manipulate the limit $$\lim_{h\to 0}\frac{\sqrt{A(t_0+h)}-\sqrt{A(t_0)}}{h}$$ In the scalar-valued case, the proof I have seen for the differentiability of the square root function involves rationalizing the numerator by multiplying top and bottom by $\sqrt{x_0+h} + \sqrt{x_0}$. We might try a similar thing with the matrix-valued function, but we would need $\sqrt{A(t_0 + h)}$ to commute with $\sqrt{A(t_0)}$ in order to clear the numerator, and they don't necessarily commute. So I'm kind of out of ideas. How might we prove the differentiability of $\sqrt{A(t)}$?","On p. 150 of Lax's Linear Algebra , he mentions that is is not hard to show that if $R(t)$ is a differentiable matrix-valued function of a single variable, whose values are positive matrices, then the square root function $\sqrt{R(t)}$, is differentiable as well. I have been trying to prove this without success. The definition Lax gives for the derivative of a matrix valued function is that $$\lim_{h\to 0}\left\|\frac{A(t_0+h) - A(t_0)}{h}- \Omega_{t_0}\right\| = 0 $$ for some matrix $\Omega_{t_0}$, where the double bars denotes the matrix norm. I believe this is equivalent to the existence of the $n^2$ entry-wise derivatives. I have been trying to manipulate the limit $$\lim_{h\to 0}\frac{\sqrt{A(t_0+h)}-\sqrt{A(t_0)}}{h}$$ In the scalar-valued case, the proof I have seen for the differentiability of the square root function involves rationalizing the numerator by multiplying top and bottom by $\sqrt{x_0+h} + \sqrt{x_0}$. We might try a similar thing with the matrix-valued function, but we would need $\sqrt{A(t_0 + h)}$ to commute with $\sqrt{A(t_0)}$ in order to clear the numerator, and they don't necessarily commute. So I'm kind of out of ideas. How might we prove the differentiability of $\sqrt{A(t)}$?",,"['calculus', 'linear-algebra', 'matrices', 'derivatives']"
45,Minimal spectral radius of a primitive matrix,Minimal spectral radius of a primitive matrix,,"Given the set of all primitive matrices of dimensions $m$ by $m$ that are non-negative and integer - which one is the matrix with the minimal spectral radius? Edit (according to the first comment): The matrix without the red '1' is an irreducible matrix with minimal (1) eigenvalue (in order to be irreducible it must have at least one '1' in each row). This matrix is not primitive, because of the locations of the '1's. In order to get a primitive matrix, we must add at least one '1'. The matrix below is indeed primitive with only one additional '1' (proof is needed...). Addition of more '1's will result in a larger spectral radius (for non negative matrices, $A \ge B$ implies $\rho(A) \ge \rho(B)$). The question is: how do we know that this specific location of the red '1' will give the minimum?","Given the set of all primitive matrices of dimensions $m$ by $m$ that are non-negative and integer - which one is the matrix with the minimal spectral radius? Edit (according to the first comment): The matrix without the red '1' is an irreducible matrix with minimal (1) eigenvalue (in order to be irreducible it must have at least one '1' in each row). This matrix is not primitive, because of the locations of the '1's. In order to get a primitive matrix, we must add at least one '1'. The matrix below is indeed primitive with only one additional '1' (proof is needed...). Addition of more '1's will result in a larger spectral radius (for non negative matrices, $A \ge B$ implies $\rho(A) \ge \rho(B)$). The question is: how do we know that this specific location of the red '1' will give the minimum?",,"['matrices', 'graph-theory', 'eigenvalues-eigenvectors']"
46,"Given a triangle with points in $\mathbb{R}^3$, find the coordinates of a point perpendicular to a side","Given a triangle with points in , find the coordinates of a point perpendicular to a side",\mathbb{R}^3,"Consider the triangle ABC in $\mathbb{R}^3$ formed by the point $A(3,2,1)$, $B(4,4,2)$, $C(6,1,0)$. Find the coordinates of the point $D$ on $BC$ such that $AD$ is perpendicular to $BC$. I believe this uses projections, but I can't seem to get started. I tried the projection of $AC$ onto $BD$ and $AB$ onto $BC$, but to no avail. Any help is loved! Thanks.","Consider the triangle ABC in $\mathbb{R}^3$ formed by the point $A(3,2,1)$, $B(4,4,2)$, $C(6,1,0)$. Find the coordinates of the point $D$ on $BC$ such that $AD$ is perpendicular to $BC$. I believe this uses projections, but I can't seem to get started. I tried the projection of $AC$ onto $BD$ and $AB$ onto $BC$, but to no avail. Any help is loved! Thanks.",,['matrices']
47,Determinants: A Special Condition,Determinants: A Special Condition,,"Under what conditions is $$ \det(A_1 + \cdots + A_n) = \det(A_1)+\cdots+\det(A_n), $$ just curious.","Under what conditions is $$ \det(A_1 + \cdots + A_n) = \det(A_1)+\cdots+\det(A_n), $$ just curious.",,"['linear-algebra', 'matrices', 'determinant']"
48,Skew symmetric matrix decomposes,Skew symmetric matrix decomposes,,"I am supposed to show that for a skew-symmetric matrix $A$ with $det(A) \neq 0$, meaning that is has an even number of columns and rows, there is an invertible matrix $ R $ such that  $ R^T A R = M$, where $M$ is a block matrix of the form $\begin{bmatrix} 0 & Id &  \\ -Id & 0 &  \end{bmatrix}$. This excercise is so general that I don't know which approach/idea or observation is useful to solve this excercise.","I am supposed to show that for a skew-symmetric matrix $A$ with $det(A) \neq 0$, meaning that is has an even number of columns and rows, there is an invertible matrix $ R $ such that  $ R^T A R = M$, where $M$ is a block matrix of the form $\begin{bmatrix} 0 & Id &  \\ -Id & 0 &  \end{bmatrix}$. This excercise is so general that I don't know which approach/idea or observation is useful to solve this excercise.",,"['linear-algebra', 'matrices']"
49,Matrix Norm set #2,Matrix Norm set #2,,"As a complement of the question Matrix Norm set and in order to complete the Problem 1.4-5 from the book: Numerical Linear Algebra and Optimisaton by Ciarlet. I have this additional conditions: (3) if $\|\cdot\|^{\prime}$ be any matrix norm, then there exists (at least) one subordinate matrix norm $\|\cdot\|$ satisfying $\|\cdot\| \leq \|\cdot\|^{\prime}$. (4) the matrix norm $\|\cdot\|$ is subordinate if and only if it is minimal element of the set $\mathcal{N}$. Then, show that there exists matrix norms $\|\cdot\|$ satisfying $\|I\| = 1$, which are yet not subrodinate. Where my definition of matrix norm is: \begin{eqnarray*} \|A\| & = & 0 \Leftrightarrow A=0, \mbox{ and } \|A\|\geq 0\\ \|\alpha A\| & = & |\alpha| \|A\|\\ \|A + B\| & \leq & \|A\| + \|B\|\\ \|AB\| & \leq & \|A\|\cdot\|B\| \end{eqnarray*} for all $A,B\in M_n$ and $\alpha\in\mathbb{R}$. Besides, my definition of subordinate matrix norm is: there exists a vector norm $\|\cdot\|$ , such that $$\|A\|\ =\ \sup_{v\neq 0}\frac{\|Av\|}{\|v\|}.$$ Please someone knows how prove it? Thanks in advance.","As a complement of the question Matrix Norm set and in order to complete the Problem 1.4-5 from the book: Numerical Linear Algebra and Optimisaton by Ciarlet. I have this additional conditions: (3) if $\|\cdot\|^{\prime}$ be any matrix norm, then there exists (at least) one subordinate matrix norm $\|\cdot\|$ satisfying $\|\cdot\| \leq \|\cdot\|^{\prime}$. (4) the matrix norm $\|\cdot\|$ is subordinate if and only if it is minimal element of the set $\mathcal{N}$. Then, show that there exists matrix norms $\|\cdot\|$ satisfying $\|I\| = 1$, which are yet not subrodinate. Where my definition of matrix norm is: \begin{eqnarray*} \|A\| & = & 0 \Leftrightarrow A=0, \mbox{ and } \|A\|\geq 0\\ \|\alpha A\| & = & |\alpha| \|A\|\\ \|A + B\| & \leq & \|A\| + \|B\|\\ \|AB\| & \leq & \|A\|\cdot\|B\| \end{eqnarray*} for all $A,B\in M_n$ and $\alpha\in\mathbb{R}$. Besides, my definition of subordinate matrix norm is: there exists a vector norm $\|\cdot\|$ , such that $$\|A\|\ =\ \sup_{v\neq 0}\frac{\|Av\|}{\|v\|}.$$ Please someone knows how prove it? Thanks in advance.",,"['linear-algebra', 'matrices']"
50,Matrix decomposition into two arbitrary sized matrices,Matrix decomposition into two arbitrary sized matrices,,"Given a matrix $A$ of dimensions $m\times{}n$, I am interested in decomposing $A$ into the product $BC$ where $B$ is a $m\times{}p$ matrix and $C$ is a $p\times{}n$ matrix. What are the methods to perform such a decomposition? What are the possible family of solutions? Are these solutions exhaustive? Background: From an image processing routine, I have an equation $A = BC$ where $A$ is a known $1\times{}12$ vector, $B$ is an unknown $1\times{}6$ vector that contain physical quantities to be recovered, and $C$ is an unknown $6\times{}12$ matrix.","Given a matrix $A$ of dimensions $m\times{}n$, I am interested in decomposing $A$ into the product $BC$ where $B$ is a $m\times{}p$ matrix and $C$ is a $p\times{}n$ matrix. What are the methods to perform such a decomposition? What are the possible family of solutions? Are these solutions exhaustive? Background: From an image processing routine, I have an equation $A = BC$ where $A$ is a known $1\times{}12$ vector, $B$ is an unknown $1\times{}6$ vector that contain physical quantities to be recovered, and $C$ is an unknown $6\times{}12$ matrix.",,['matrices']
51,"How to show that $\left \| AB \right \|_F \leq \min (\left \| A \right \|_F \left \| B \right \|_2 , \left \| A \right \|_2 \left \| B \right \|_F)$?",How to show that ?,"\left \| AB \right \|_F \leq \min (\left \| A \right \|_F \left \| B \right \|_2 , \left \| A \right \|_2 \left \| B \right \|_F)","For any matrices $A \in \mathbb{C}^{m \times k}$ and $B \in \mathbb{C}^{k \times n}$, show that $\left \| AB \right \|_F \leq \min (\left \| A \right \|_F \left \| B \right \|_2 , \left \| A \right \|_2 \left \| B \right \|_F)$ where $\left \| \cdot \right \|_F$ and $\left \| \cdot \right \|_2$ are the Frobenius-norm and 2-norm, respectively, defined as: $\left \| A \right \|_F = \sqrt{\mathrm{trace} \left ( A^H A \right )}$,  $\left \| A \right \|_2 = \sqrt{\lambda_{max} \left ( A^H A \right )}$","For any matrices $A \in \mathbb{C}^{m \times k}$ and $B \in \mathbb{C}^{k \times n}$, show that $\left \| AB \right \|_F \leq \min (\left \| A \right \|_F \left \| B \right \|_2 , \left \| A \right \|_2 \left \| B \right \|_F)$ where $\left \| \cdot \right \|_F$ and $\left \| \cdot \right \|_2$ are the Frobenius-norm and 2-norm, respectively, defined as: $\left \| A \right \|_F = \sqrt{\mathrm{trace} \left ( A^H A \right )}$,  $\left \| A \right \|_2 = \sqrt{\lambda_{max} \left ( A^H A \right )}$",,"['linear-algebra', 'matrices', 'normed-spaces']"
52,Diagonalise a matrix and show the formula,Diagonalise a matrix and show the formula,,"I have diagonlised P to get $$P=\left(\begin{matrix} -1 &0 &0\\ 0 &0 &0\\ 0 &0 &1 \end{matrix}\right)$$ however am unsure on how to proceed, would appreciate any help! By diagonalising P by a transformation of similarity, show that $$    e^{Pt} = (I_3 - P^2) + P \sinh( t) + P^2 \cosh(t)$$ when $P$ is the matrix $$P=\left(\begin{matrix} 0 &1 &0\\ 0 &0 &1\\ 0 &1 &0 \end{matrix}\right)$$ and where $I_3$ is the identity matrix of a $3\times3$","I have diagonlised P to get $$P=\left(\begin{matrix} -1 &0 &0\\ 0 &0 &0\\ 0 &0 &1 \end{matrix}\right)$$ however am unsure on how to proceed, would appreciate any help! By diagonalising P by a transformation of similarity, show that $$    e^{Pt} = (I_3 - P^2) + P \sinh( t) + P^2 \cosh(t)$$ when $P$ is the matrix $$P=\left(\begin{matrix} 0 &1 &0\\ 0 &0 &1\\ 0 &1 &0 \end{matrix}\right)$$ and where $I_3$ is the identity matrix of a $3\times3$",,['matrices']
53,Matrix with rank $1$,Matrix with rank,1,"Let $A=(a_{ij})_n$ a symmetric matrix with positive coefficients. We suppose that there is $\alpha>0$ such that, for all permutation $\sigma$ of $\{1,\ldots,n\}$, we have $$a_{1\sigma(1)}a_{2\sigma(2)}\cdots a_{n\sigma(n)}=\alpha.$$ How to prove that the rank of $A$ is $1$? Please any suggestions?","Let $A=(a_{ij})_n$ a symmetric matrix with positive coefficients. We suppose that there is $\alpha>0$ such that, for all permutation $\sigma$ of $\{1,\ldots,n\}$, we have $$a_{1\sigma(1)}a_{2\sigma(2)}\cdots a_{n\sigma(n)}=\alpha.$$ How to prove that the rank of $A$ is $1$? Please any suggestions?",,['linear-algebra']
54,Rayleigh-Ritz Theorem,Rayleigh-Ritz Theorem,,"Let $U$ be an $n$-dimensional subspace of $L:=L_2([-1,1])$. Let $F$ be an acting on $L$, given at $f \in L$ $$ (Ff)(x):=\int_{-1}^1 \frac{\sin a(x-y)}{(x-y)}f(y) dy, \quad x \in [-1,1], \quad a>0. $$ Let $\lambda_n(a), \, n=0,1,2\ldots,$ be the eigenvalues of $F$. Using Rayleigh-Ritz Theorem show that for all $n>a$ $$ \lambda_n(a)\geq \min_{U/\{0\}}\frac{\|Ff\|_L}{\|f\|_L}. $$ Thank you.","Let $U$ be an $n$-dimensional subspace of $L:=L_2([-1,1])$. Let $F$ be an acting on $L$, given at $f \in L$ $$ (Ff)(x):=\int_{-1}^1 \frac{\sin a(x-y)}{(x-y)}f(y) dy, \quad x \in [-1,1], \quad a>0. $$ Let $\lambda_n(a), \, n=0,1,2\ldots,$ be the eigenvalues of $F$. Using Rayleigh-Ritz Theorem show that for all $n>a$ $$ \lambda_n(a)\geq \min_{U/\{0\}}\frac{\|Ff\|_L}{\|f\|_L}. $$ Thank you.",,"['matrices', 'operator-theory', 'eigenvalues-eigenvectors', 'mathematical-physics', 'eigenfunctions']"
55,Saturating Horn's Inequalities,Saturating Horn's Inequalities,,"If I have a matrix product of the form: $C = AB$ where $A = UDU^*$ With A and B square, Hermitian and positive semidefinite, D diagonal, U a unitary and * representing the conjugate transpose, then Horn's inequalities give: $c_{i+j-1} \leq d_i b_j \ $ Where $b_i$, $c_i$ and $d_i$ are the eigenvalues of B, C and D respectively, $b_1 > b_2 > ... > b_n$ and similarly for c and d. Is it generally possible to saturate these inequalities (i.e. replace the $\leq$ with an = for the minimum value of the RHS) by the correct choice of U?","If I have a matrix product of the form: $C = AB$ where $A = UDU^*$ With A and B square, Hermitian and positive semidefinite, D diagonal, U a unitary and * representing the conjugate transpose, then Horn's inequalities give: $c_{i+j-1} \leq d_i b_j \ $ Where $b_i$, $c_i$ and $d_i$ are the eigenvalues of B, C and D respectively, $b_1 > b_2 > ... > b_n$ and similarly for c and d. Is it generally possible to saturate these inequalities (i.e. replace the $\leq$ with an = for the minimum value of the RHS) by the correct choice of U?",,"['linear-algebra', 'matrices', 'eigenvalues-eigenvectors']"
56,Diagonalization of a bisymmetric matrix,Diagonalization of a bisymmetric matrix,,"Is there some way to easily diagonalize a rank- $n$ bisymmetric Toeplitz matrix with only zeros on its main diagonal? Direct calculation is out of the question. I need some trick. Addendum: I don't know why I didn't do it before, but I can write the matrix down. $$A= \begin{pmatrix} 0 & 1 & 1/2 & 1/3 & \cdots & 1/(n-1) \\ 1 & 0 & 1 & 1/2 & \cdots & 1/(n-2) \\ 1/2 & 1 & 0 & 1 &  \cdots & 1/(n-3) \\ \vdots &  & \vdots & \ & \ddots & \vdots \\ 1/i & \cdots & 1/|i-j| &\cdots& & 1/(n-i) \\ \vdots &  & \vdots & \ &  & \vdots \\ 1/(n-1) & \cdots & 1/3 & 1/2 & 1 & 0\\ \end{pmatrix}$$ or, simpler yet $$A_{i,j}=\begin{cases}  0 & i=j \\ \frac{1}{|i-j|} & i,j=1\dots n\\ \end{cases}$$","Is there some way to easily diagonalize a rank- bisymmetric Toeplitz matrix with only zeros on its main diagonal? Direct calculation is out of the question. I need some trick. Addendum: I don't know why I didn't do it before, but I can write the matrix down. or, simpler yet","n A= \begin{pmatrix}
0 & 1 & 1/2 & 1/3 & \cdots & 1/(n-1) \\
1 & 0 & 1 & 1/2 & \cdots & 1/(n-2) \\
1/2 & 1 & 0 & 1 &  \cdots & 1/(n-3) \\
\vdots &  & \vdots & \ & \ddots & \vdots \\
1/i & \cdots & 1/|i-j| &\cdots& & 1/(n-i) \\
\vdots &  & \vdots & \ &  & \vdots \\
1/(n-1) & \cdots & 1/3 & 1/2 & 1 & 0\\
\end{pmatrix} A_{i,j}=\begin{cases} 
0 & i=j \\
\frac{1}{|i-j|} & i,j=1\dots n\\
\end{cases}","['matrices', 'eigenvalues-eigenvectors', 'diagonalization', 'symmetric-matrices', 'toeplitz-matrices']"
57,Where have I made my mistake in calculating $P^{-1}AP$?,Where have I made my mistake in calculating ?,P^{-1}AP,"I have $$Q = \begin{pmatrix} -\mu & \mu \\ \lambda & -\lambda \end{pmatrix}$$ and I want to work out the value of $\mathbb{P}(t) = \exp(Qt)$ So I diagonalised $Q$ and then worked out the exponential of the diagonal matrix. I got this to be: $${Q}t = \pmatrix{-\mu t &\mu t \\ \lambda t & -\lambda t} = \pmatrix{1 & -\frac{\mu }{\lambda } \\ 1 & 1}^{-1} \cdot \pmatrix{0 & 0 \\ 0 & -t(\lambda + \mu)} \cdot \pmatrix{1 & -\frac{\mu }{\lambda } \\ 1 & 1}.$$ So using the middle matrix, I got $$\exp (Qt) = \pmatrix{1 & 0 \\ 0 & \exp(-t(\lambda + \mu))} = \pmatrix{1 & 0 \\ 0 & \exp(T)},$$ where $T = -t(\lambda + \mu)$. Then, using $\exp (P^{-1}AP) = P^{-1}e^AP$, I was supposed to get $$\mathbb{P}(t) = \exp({Q} t) = \frac{1}{\lambda + \mu}\pmatrix{\lambda + \mu \exp(T) & \mu - \mu \exp(T) \\ \lambda - \lambda\exp(T) & \mu + \lambda\exp(T)}.$$ This is what I did. To first work out $P^{-1}$ I got $$P^{-1} = \frac{1}{1 + \frac{\mu}{\lambda}} \pmatrix{1 & \frac{\mu}{\lambda} \\ -1 & 1} = \frac{\lambda}{\lambda + \mu} \pmatrix{1 & \frac{\mu}{\lambda} \\ -1 & 1}$$ Then doing $P^{-1}e^A$ gave me $$\frac{\lambda}{\lambda + \mu} \pmatrix{1 & \frac{\mu}{\lambda} \exp (T) \\ -1 & \exp (T)}$$ Then doing this times $P$ gave me $$\frac{\lambda}{\lambda + \mu} \pmatrix{1 + \frac{\mu}{\lambda} \exp (T) & -\frac{\mu}{\lambda} + \frac{\mu}{\lambda} \exp (T) \\ -1 + \exp (T) & \frac{\mu}{\lambda} + \exp (T)}$$ Multiplying through by $\lambda$ gives me $$\frac{\lambda}{\lambda + \mu} \pmatrix{\lambda + \mu \exp (T) & - \mu - \mu \exp (T) \\ - \lambda + \exp(T) & \mu + \lambda \exp (T)}$$ Clearly it's started going wrong in the matrix before this but I can't see where I've made my mistakes. Can someone help please? Thank you","I have $$Q = \begin{pmatrix} -\mu & \mu \\ \lambda & -\lambda \end{pmatrix}$$ and I want to work out the value of $\mathbb{P}(t) = \exp(Qt)$ So I diagonalised $Q$ and then worked out the exponential of the diagonal matrix. I got this to be: $${Q}t = \pmatrix{-\mu t &\mu t \\ \lambda t & -\lambda t} = \pmatrix{1 & -\frac{\mu }{\lambda } \\ 1 & 1}^{-1} \cdot \pmatrix{0 & 0 \\ 0 & -t(\lambda + \mu)} \cdot \pmatrix{1 & -\frac{\mu }{\lambda } \\ 1 & 1}.$$ So using the middle matrix, I got $$\exp (Qt) = \pmatrix{1 & 0 \\ 0 & \exp(-t(\lambda + \mu))} = \pmatrix{1 & 0 \\ 0 & \exp(T)},$$ where $T = -t(\lambda + \mu)$. Then, using $\exp (P^{-1}AP) = P^{-1}e^AP$, I was supposed to get $$\mathbb{P}(t) = \exp({Q} t) = \frac{1}{\lambda + \mu}\pmatrix{\lambda + \mu \exp(T) & \mu - \mu \exp(T) \\ \lambda - \lambda\exp(T) & \mu + \lambda\exp(T)}.$$ This is what I did. To first work out $P^{-1}$ I got $$P^{-1} = \frac{1}{1 + \frac{\mu}{\lambda}} \pmatrix{1 & \frac{\mu}{\lambda} \\ -1 & 1} = \frac{\lambda}{\lambda + \mu} \pmatrix{1 & \frac{\mu}{\lambda} \\ -1 & 1}$$ Then doing $P^{-1}e^A$ gave me $$\frac{\lambda}{\lambda + \mu} \pmatrix{1 & \frac{\mu}{\lambda} \exp (T) \\ -1 & \exp (T)}$$ Then doing this times $P$ gave me $$\frac{\lambda}{\lambda + \mu} \pmatrix{1 + \frac{\mu}{\lambda} \exp (T) & -\frac{\mu}{\lambda} + \frac{\mu}{\lambda} \exp (T) \\ -1 + \exp (T) & \frac{\mu}{\lambda} + \exp (T)}$$ Multiplying through by $\lambda$ gives me $$\frac{\lambda}{\lambda + \mu} \pmatrix{\lambda + \mu \exp (T) & - \mu - \mu \exp (T) \\ - \lambda + \exp(T) & \mu + \lambda \exp (T)}$$ Clearly it's started going wrong in the matrix before this but I can't see where I've made my mistakes. Can someone help please? Thank you",,"['linear-algebra', 'matrices']"
58,How to differentiate $\frac {\partial \mathrm{tr}(Q^TQAQ^TQA)}{\partial q_i}$,How to differentiate,\frac {\partial \mathrm{tr}(Q^TQAQ^TQA)}{\partial q_i},"The problem is $\frac {\partial \mathrm{tr}(Q^TQAQ^TQA)}{\partial q_i}$, where $Q=[q_1,...,q_N]$, $q_i$ is $N$ dimensional vector and $Q$ is $N\times N$ matrix. I have think of using chain rule, but I am confusing on using chain rule on matrix calculus. For example if we let $X=Q^TQA$, the problem becomes $\frac {\partial \mathrm{tr}(X^2)}{\partial q_i}$, if I use the chain rule in scalar differentiation it will becomes $\frac {\partial \mathrm{tr}(X^2)}{\partial X} $ $\frac {\partial X}{\partial q_i}$ and it seems to be invalid.","The problem is $\frac {\partial \mathrm{tr}(Q^TQAQ^TQA)}{\partial q_i}$, where $Q=[q_1,...,q_N]$, $q_i$ is $N$ dimensional vector and $Q$ is $N\times N$ matrix. I have think of using chain rule, but I am confusing on using chain rule on matrix calculus. For example if we let $X=Q^TQA$, the problem becomes $\frac {\partial \mathrm{tr}(X^2)}{\partial q_i}$, if I use the chain rule in scalar differentiation it will becomes $\frac {\partial \mathrm{tr}(X^2)}{\partial X} $ $\frac {\partial X}{\partial q_i}$ and it seems to be invalid.",,"['calculus', 'matrices', 'derivatives']"
59,Sorting Matrix to Block structure,Sorting Matrix to Block structure,,"I have a symmetric matrix and I want it to be as block-like as possible. I don't have a clear definition. I want the smallest number of groups of non zero elements or maybe the most non-zero elements as close as possible to the diagonal. Example: $$ X = \begin{pmatrix}1& 0 & .3& 0& .5& 0\\ 0& 1& 0& .2& 0& .4\\ .3 &0 &1& 0& .3& .1\\ 0 & .2 & 0 & 1 & 0 & .3\\ .5 & 0 & .3 & 0 & 1 & 0\\ 0 & .4 & .1 & .3 & 0 & 1\end{pmatrix}$$ Output:  $$ Y = \begin{pmatrix}1& .5 & .3& 0& 0& 0\\ .5 & 1 & .3 & 0 & 0 & 0\\ .3 & .3 & 1 & 0 & 0 & .1\\ 0 & 0 & 0 & 1 & .2 & .3\\ 0 & 0 & 0 & .2 & 1 & .4\\ 0 & 0 & .1 & .3 & .4 & 1\end{pmatrix}$$ I only had to swap column and row 2 with 5 to get this result. My algorithm was that I swapped rows and colums for the largest non-diagonal element (in the upper triangle) to a closer position to the diagonal. Is there a Matlab/Octave function that does this? What is this process called? What should a good algorithm that does this look for? (When to stop, which row to swap, etc). What should be the definition of what I want? The domain my question is related to is that these is a term-term affinity matrix (from Information Retrieval). So if $a_{ij}$ is large then the terms $i$ and $j$ occur often in the same documents. I want to sort the matrix such that the blocks (which broadly refer to topics) can be visualised. In the current state (see image) one can't see much. EDIT: I found out that this is called matrix permutation.","I have a symmetric matrix and I want it to be as block-like as possible. I don't have a clear definition. I want the smallest number of groups of non zero elements or maybe the most non-zero elements as close as possible to the diagonal. Example: $$ X = \begin{pmatrix}1& 0 & .3& 0& .5& 0\\ 0& 1& 0& .2& 0& .4\\ .3 &0 &1& 0& .3& .1\\ 0 & .2 & 0 & 1 & 0 & .3\\ .5 & 0 & .3 & 0 & 1 & 0\\ 0 & .4 & .1 & .3 & 0 & 1\end{pmatrix}$$ Output:  $$ Y = \begin{pmatrix}1& .5 & .3& 0& 0& 0\\ .5 & 1 & .3 & 0 & 0 & 0\\ .3 & .3 & 1 & 0 & 0 & .1\\ 0 & 0 & 0 & 1 & .2 & .3\\ 0 & 0 & 0 & .2 & 1 & .4\\ 0 & 0 & .1 & .3 & .4 & 1\end{pmatrix}$$ I only had to swap column and row 2 with 5 to get this result. My algorithm was that I swapped rows and colums for the largest non-diagonal element (in the upper triangle) to a closer position to the diagonal. Is there a Matlab/Octave function that does this? What is this process called? What should a good algorithm that does this look for? (When to stop, which row to swap, etc). What should be the definition of what I want? The domain my question is related to is that these is a term-term affinity matrix (from Information Retrieval). So if $a_{ij}$ is large then the terms $i$ and $j$ occur often in the same documents. I want to sort the matrix such that the blocks (which broadly refer to topics) can be visualised. In the current state (see image) one can't see much. EDIT: I found out that this is called matrix permutation.",,"['matrices', 'block-matrices']"
60,Universal parametrization for orthogonal matrices,Universal parametrization for orthogonal matrices,,"Let $k$ be a field whose characteristic is zero and let $n\geq 1$. Say that a matrix $M\in {\cal M}_{n\times n}(k)$ is almost orthogonal if $M^{T}M$ is a nonzero multiple of the identity. Denote the set of those matrices by $AO_n(k)$. When $n=2$, there is a nice parametric description  of $AO_2(k)$ : $$ AO_2(k)=\Bigg\lbrace \bigg(\begin{matrix} a & b \\ -b&a \end{matrix}\bigg) \Bigg| (a,b) \in k^2, (a,b)\neq (0,0) \Bigg\rbrace $$ Are there similar exhaustive formulas with polynomial entries for $n>2$ ?","Let $k$ be a field whose characteristic is zero and let $n\geq 1$. Say that a matrix $M\in {\cal M}_{n\times n}(k)$ is almost orthogonal if $M^{T}M$ is a nonzero multiple of the identity. Denote the set of those matrices by $AO_n(k)$. When $n=2$, there is a nice parametric description  of $AO_2(k)$ : $$ AO_2(k)=\Bigg\lbrace \bigg(\begin{matrix} a & b \\ -b&a \end{matrix}\bigg) \Bigg| (a,b) \in k^2, (a,b)\neq (0,0) \Bigg\rbrace $$ Are there similar exhaustive formulas with polynomial entries for $n>2$ ?",,"['abstract-algebra', 'matrices', 'algebraic-geometry', 'algebraic-groups']"
61,Trace of a matrix to the $n$,Trace of a matrix to the,n,"Why is it that if $A(t), B(t)$ are two $n\times n$ complex matrices and $${d\over dt}A=AB-BA$$ then the trace of the matrix $A^n$ where $n\in \mathbb Z$ is a constant for all $t$?","Why is it that if $A(t), B(t)$ are two $n\times n$ complex matrices and $${d\over dt}A=AB-BA$$ then the trace of the matrix $A^n$ where $n\in \mathbb Z$ is a constant for all $t$?",,"['linear-algebra', 'matrices']"
62,Multiplying a Matrix by its Transpose,Multiplying a Matrix by its Transpose,,Let's assume that $A$ is a $m \times n$ matrix with linearly independent columns. Why are the columns of $A(A^T)$ also linearly independent? Is this new matrix invertible? What about $(A^T)A$?,Let's assume that $A$ is a $m \times n$ matrix with linearly independent columns. Why are the columns of $A(A^T)$ also linearly independent? Is this new matrix invertible? What about $(A^T)A$?,,"['linear-algebra', 'matrices']"
63,How to solve / set up three equations three unknowns with a matrix?,How to solve / set up three equations three unknowns with a matrix?,,"How do I solve these equations with a matrix to get $x$, $y$, and $z$? I'm unsure of how to start. $$0.09x + 0.10y + 0.12z = 52,000 \\ x + y + z = 500,000 \\ y = 2.5x$$","How do I solve these equations with a matrix to get $x$, $y$, and $z$? I'm unsure of how to start. $$0.09x + 0.10y + 0.12z = 52,000 \\ x + y + z = 500,000 \\ y = 2.5x$$",,"['linear-algebra', 'matrices']"
64,A matrix inequality,A matrix inequality,,If $A=(a_{ij})$ is a real positive definite symmetric matrix of order $n$.  How to show $(n-1)\prod_{i=1}^na_{ii}+\det A\ge \sum_{i=1}^na_{ii}\det A(i)$? $A(i)$ means the submatrix of $A$ by deleting the $i$th row and $i$th column.,If $A=(a_{ij})$ is a real positive definite symmetric matrix of order $n$.  How to show $(n-1)\prod_{i=1}^na_{ii}+\det A\ge \sum_{i=1}^na_{ii}\det A(i)$? $A(i)$ means the submatrix of $A$ by deleting the $i$th row and $i$th column.,,"['linear-algebra', 'matrices']"
65,Incremental approach of calculating the Singular Value Decomposition,Incremental approach of calculating the Singular Value Decomposition,,"I have a fairly large array, a billion or so by 500,000 array. I need to calculate the singular value decomposition of this array. The problem is that my computer RAM will not be able to handle the whole matrix at once. I need an incremental approach of calculating the SVD. This would mean that I could take one or a couple or a couple hundred/thousand (not too much though) rows of data at one time, do what I need to do with those numbers, and then throw them away so that I can address memory toward getting the rest of the data. People have posted a couple of papers on similar issues such as http://www.bradblock.com/Incremental_singular_value_decomposition_of_uncertain_data_with_missing_values.pdf and http://www.jofcis.com/publishedpapers/2012_8_8_3207_3214.pdf . I am wondering if anyone has done any previous research or has any suggestions on how should go on approaching this? I really do need the FASTEST approach, without losing too much accuracy in the data.","I have a fairly large array, a billion or so by 500,000 array. I need to calculate the singular value decomposition of this array. The problem is that my computer RAM will not be able to handle the whole matrix at once. I need an incremental approach of calculating the SVD. This would mean that I could take one or a couple or a couple hundred/thousand (not too much though) rows of data at one time, do what I need to do with those numbers, and then throw them away so that I can address memory toward getting the rest of the data. People have posted a couple of papers on similar issues such as http://www.bradblock.com/Incremental_singular_value_decomposition_of_uncertain_data_with_missing_values.pdf and http://www.jofcis.com/publishedpapers/2012_8_8_3207_3214.pdf . I am wondering if anyone has done any previous research or has any suggestions on how should go on approaching this? I really do need the FASTEST approach, without losing too much accuracy in the data.",,"['linear-algebra', 'matrices', 'numerical-linear-algebra']"
66,the solution of ode can be encoded as first order logic,the solution of ode can be encoded as first order logic,,"Consider the following sytem of ODEs $\dot{x}= Ax$, and given $x(0)$, where $A$ is a $n\times n$ matrix with rational entries. Can I encode the solution, say $x(t)$ for a given $t$, as a first order logic formula over $(R, +, \cdot, e^x, 0, 1)$? Here, let us assume that $x(t)$ has real entries. The difficulty I can see is that some eigenvalue of A is an algebraic number, so the solution invloes some sin and cosin. I do not know how to overcome this. Thanks.","Consider the following sytem of ODEs $\dot{x}= Ax$, and given $x(0)$, where $A$ is a $n\times n$ matrix with rational entries. Can I encode the solution, say $x(t)$ for a given $t$, as a first order logic formula over $(R, +, \cdot, e^x, 0, 1)$? Here, let us assume that $x(t)$ has real entries. The difficulty I can see is that some eigenvalue of A is an algebraic number, so the solution invloes some sin and cosin. I do not know how to overcome this. Thanks.",,"['matrices', 'ordinary-differential-equations', 'logic', 'model-theory']"
67,Why is the matrix representing a non-degenerate sesquilinear form invertible?,Why is the matrix representing a non-degenerate sesquilinear form invertible?,,"Let's consider a finite-dimensional vector space $E$ on the field $\mathbb{K}$ (where $\mathbb{K}=\mathbb{C} \ \text{or}\ \mathbb{R}$) and a sesquilinear (or bilinear if $\mathbb{K}=\mathbb{R}$) form $q:E\times E \rightarrow \mathbb{K}$. The definition for a non-degenerate form is that $q(x,y)=0\ \forall y\in E$ implies $x=0$. Now if we represent $q(x,y)$ with a matrix, so $q(x,y) =x^HAy$, why does the condition that the form be non-degenerate impose that $A$ is non-singular? I tried to see it using the dual space as $M(x,A)=x^HA\in E^*$, so that $M:E\times L(E,E)\rightarrow E^*$, where $L(E,E)$ is the vector space of all linear transformations from $E$ to $E$ and playing with the nullspace of $A$, but I just can't see it","Let's consider a finite-dimensional vector space $E$ on the field $\mathbb{K}$ (where $\mathbb{K}=\mathbb{C} \ \text{or}\ \mathbb{R}$) and a sesquilinear (or bilinear if $\mathbb{K}=\mathbb{R}$) form $q:E\times E \rightarrow \mathbb{K}$. The definition for a non-degenerate form is that $q(x,y)=0\ \forall y\in E$ implies $x=0$. Now if we represent $q(x,y)$ with a matrix, so $q(x,y) =x^HAy$, why does the condition that the form be non-degenerate impose that $A$ is non-singular? I tried to see it using the dual space as $M(x,A)=x^HA\in E^*$, so that $M:E\times L(E,E)\rightarrow E^*$, where $L(E,E)$ is the vector space of all linear transformations from $E$ to $E$ and playing with the nullspace of $A$, but I just can't see it",,"['linear-algebra', 'matrices']"
68,Find an orthogonal basis consisting of eigenvectors,Find an orthogonal basis consisting of eigenvectors,,Find an orthogonal basis for $\mathbb R^3$ consisting of the eigenvectors of the matrix $$\begin{bmatrix} 1 & 2 & 2 \\ 2 & 1 & 2 \\ 2 & 2 & 1 \end{bmatrix}$$ Isn't this question basically just asking 'find the eigenvectors of this matrix'? And the part about finding 'an orthogonal basis' is irrelevant?,Find an orthogonal basis for $\mathbb R^3$ consisting of the eigenvectors of the matrix $$\begin{bmatrix} 1 & 2 & 2 \\ 2 & 1 & 2 \\ 2 & 2 & 1 \end{bmatrix}$$ Isn't this question basically just asking 'find the eigenvectors of this matrix'? And the part about finding 'an orthogonal basis' is irrelevant?,,"['linear-algebra', 'matrices']"
69,"Why is $SO(3, \mathbb{C}) \cong PSL(2, \mathbb{C})$?",Why is ?,"SO(3, \mathbb{C}) \cong PSL(2, \mathbb{C})","Why is $SO(3, \mathbb{C}) \cong PSL(2, \mathbb{C})$? I can't seem to be able to construct an explicit isomorphism between them.","Why is $SO(3, \mathbb{C}) \cong PSL(2, \mathbb{C})$? I can't seem to be able to construct an explicit isomorphism between them.",,"['abstract-algebra', 'matrices']"
70,Computing the derivative of a quadratic form and matrix chain rule,Computing the derivative of a quadratic form and matrix chain rule,,"I'm working on using the Generalized Method of Moments to analyze some yogurt purchase data, and in the course of trying to implement the standard Hansen method (i.e. not an empirical likelihood method), I need to compute first and second derivatives of the following function: $$Q(\theta) = \biggl[\frac{1}{N}\sum_{i=1}^{N}\psi(Z_{i},\theta)\biggr]^{T}C\biggl[\frac{1}{N}\sum_{i=1}^{N}\psi(Z_{i},\theta)\biggr].$$ Here, $\psi(Z_{i},\theta)$ is a vector function (in my case a 9-by-1 column vector of the moment conditions; you can just think of each component as a function of the scalar parameter $\theta$ if you wish). The $Z_{i}$ are the individual purchase data. $C$ is a weight matrix derived from the model assumptions, but you can treat it as just the identity matrix of suitable size if you want; it shouldn't matter as it isn't a function of $\theta$. If I let $$F(\theta) = \biggl[\frac{1}{N}\sum_{i=1}^{N}\psi(Z_{i},\theta)\biggr]$$ for simplicity, then the place I am getting stuck is computing the first and second derivatives of $Q_{C}$ w.r.t. $\theta$. This is a scalar-valued objective function of a single variable, so everything involved should work out to be a scalar. Based on the Wikipedia article on Matrix Calculus , here is what I have tried thus far. $$\frac{dQ}{d\theta} = \frac{dQ}{dF}\cdot{}\frac{dF}{d\theta} = \biggl[ F(\theta)^{T}(C+C^{T})\biggr]\cdot{}\biggl[\frac{d}{d\theta}F(\theta)\biggr]$$ Next I want to take the derivative again, so I use the matrix chain and product rules. In my case, it happens that the final term, $\frac{d}{d\theta}F(\theta)$ is no longer a function of $\theta$ (just constants in all components), so its derivative will be zero and we only need to worry about the first part of the product. $$\frac{d}{d\theta}\biggl[ F(\theta)^{T}(C+C^{T})\biggr]\cdot{}\biggl[\frac{d}{d\theta}F(\theta)\biggr].$$ As far as I can tell, this just results in the following: $$ \frac{d}{d\theta}\biggl[ F(\theta)^{T}(C+C^{T})\biggr]\cdot{}\biggl[\frac{d}{d\theta}F(\theta)\biggr] = \biggl(\frac{d}{d\theta}F(\theta) \biggr)^{T}(C+C^{T})\biggl(\frac{d}{d\theta}F(\theta) \biggr).$$ This gives a nice formula, but when I use these results for the first and second derivatives to program up Newton's method to find the value of $\theta$ that minimizes the quadratic form, the method is not converging, and I am concerned that it is because I have calculated the derivatives incorrectly (missing a transpose, or something like that). Additionally, links to good, clearly written references that explain the logic behind matrix calculus, especially when and why transpositions occur, would be appreciated. Almost all references I could find in 30+ minutes of Googling were absolutely inscrutable and tended to just state results with no expositions at all.","I'm working on using the Generalized Method of Moments to analyze some yogurt purchase data, and in the course of trying to implement the standard Hansen method (i.e. not an empirical likelihood method), I need to compute first and second derivatives of the following function: $$Q(\theta) = \biggl[\frac{1}{N}\sum_{i=1}^{N}\psi(Z_{i},\theta)\biggr]^{T}C\biggl[\frac{1}{N}\sum_{i=1}^{N}\psi(Z_{i},\theta)\biggr].$$ Here, $\psi(Z_{i},\theta)$ is a vector function (in my case a 9-by-1 column vector of the moment conditions; you can just think of each component as a function of the scalar parameter $\theta$ if you wish). The $Z_{i}$ are the individual purchase data. $C$ is a weight matrix derived from the model assumptions, but you can treat it as just the identity matrix of suitable size if you want; it shouldn't matter as it isn't a function of $\theta$. If I let $$F(\theta) = \biggl[\frac{1}{N}\sum_{i=1}^{N}\psi(Z_{i},\theta)\biggr]$$ for simplicity, then the place I am getting stuck is computing the first and second derivatives of $Q_{C}$ w.r.t. $\theta$. This is a scalar-valued objective function of a single variable, so everything involved should work out to be a scalar. Based on the Wikipedia article on Matrix Calculus , here is what I have tried thus far. $$\frac{dQ}{d\theta} = \frac{dQ}{dF}\cdot{}\frac{dF}{d\theta} = \biggl[ F(\theta)^{T}(C+C^{T})\biggr]\cdot{}\biggl[\frac{d}{d\theta}F(\theta)\biggr]$$ Next I want to take the derivative again, so I use the matrix chain and product rules. In my case, it happens that the final term, $\frac{d}{d\theta}F(\theta)$ is no longer a function of $\theta$ (just constants in all components), so its derivative will be zero and we only need to worry about the first part of the product. $$\frac{d}{d\theta}\biggl[ F(\theta)^{T}(C+C^{T})\biggr]\cdot{}\biggl[\frac{d}{d\theta}F(\theta)\biggr].$$ As far as I can tell, this just results in the following: $$ \frac{d}{d\theta}\biggl[ F(\theta)^{T}(C+C^{T})\biggr]\cdot{}\biggl[\frac{d}{d\theta}F(\theta)\biggr] = \biggl(\frac{d}{d\theta}F(\theta) \biggr)^{T}(C+C^{T})\biggl(\frac{d}{d\theta}F(\theta) \biggr).$$ This gives a nice formula, but when I use these results for the first and second derivatives to program up Newton's method to find the value of $\theta$ that minimizes the quadratic form, the method is not converging, and I am concerned that it is because I have calculated the derivatives incorrectly (missing a transpose, or something like that). Additionally, links to good, clearly written references that explain the logic behind matrix calculus, especially when and why transpositions occur, would be appreciated. Almost all references I could find in 30+ minutes of Googling were absolutely inscrutable and tended to just state results with no expositions at all.",,"['matrices', 'reference-request', 'multivariable-calculus', 'derivatives']"
71,"Linear Algebra: Image, Kernel, Span, Dimension?","Linear Algebra: Image, Kernel, Span, Dimension?",,"$A = \left(\begin{array}{crc} 1 & 0 & 1 \\ 0 & -1 & 2 \\ 1 & -1 & 3  \end{array}\right)$ What is the dimension of im(A)? What is the dimension of ker(A)? I know that the im(A) is the span of the column vectors of A. However, I don't know how to find the span of a vector column. I'm also confused about how to find the kernel since I think that involves a transformation matrix T but I only have the normal matrix A...","$A = \left(\begin{array}{crc} 1 & 0 & 1 \\ 0 & -1 & 2 \\ 1 & -1 & 3  \end{array}\right)$ What is the dimension of im(A)? What is the dimension of ker(A)? I know that the im(A) is the span of the column vectors of A. However, I don't know how to find the span of a vector column. I'm also confused about how to find the kernel since I think that involves a transformation matrix T but I only have the normal matrix A...",,"['linear-algebra', 'matrices']"
72,No identical rectangles in a matrix,No identical rectangles in a matrix,,"I have a matrix of dimensions N x M. Every cell has an integer. Now, I want for every 'rectangle', to verify that all its corners are not the same. Example: This matrix is fine: This matrix is not: The naive solution is to check every possible rectangle, therefore $\binom N2\binom M2$ checks. Is there any way or algorithm that I can use to make less checks? This was an assignment I had last semester, eventually I used the naive solution, but the question still bothers me...","I have a matrix of dimensions N x M. Every cell has an integer. Now, I want for every 'rectangle', to verify that all its corners are not the same. Example: This matrix is fine: This matrix is not: The naive solution is to check every possible rectangle, therefore $\binom N2\binom M2$ checks. Is there any way or algorithm that I can use to make less checks? This was an assignment I had last semester, eventually I used the naive solution, but the question still bothers me...",,"['matrices', 'algorithms', 'optimization']"
73,Matrix properties,Matrix properties,,"What can one conclude about a matrix, $M$, if its single eigenvalue is 1? (I think the question is trying to demonstrate a contrast with the case where it is 0 instead of 1, in which we could conclude that the matrix is nilpotent.) Can I conclude that the matrix is the identity matrix? Since $(M-I)^n=0$ by the Cayley-Hamilton theorem? Is there anything else? Thanks.","What can one conclude about a matrix, $M$, if its single eigenvalue is 1? (I think the question is trying to demonstrate a contrast with the case where it is 0 instead of 1, in which we could conclude that the matrix is nilpotent.) Can I conclude that the matrix is the identity matrix? Since $(M-I)^n=0$ by the Cayley-Hamilton theorem? Is there anything else? Thanks.",,['matrices']
74,Question about how linear independence of module elements gives conditions on the determinant of a matrix,Question about how linear independence of module elements gives conditions on the determinant of a matrix,,"I have been trying to prove linear independence of rows implies linear independence of columns in kind of an abstract setting of modules following some exercises and notes from an old course.  The following fact is something I think we are supposed to use but I did not see any way to prove it so I would like to understand the proof so I can potentially adapt it to a couple of similar problems. Let $A = (a_{i,j})$ be a square matrix with $n$ rows and columns and with entries in a commutative ring with identity $R$ and let $M$ be an $R$-module. Let $x_1, \ldots ,x_n \in M$ and suppose that$\sum_{j=1}^{n}a_{i,j} x_j = 0$ for $1  \leq i \leq n$ . Then how do you show $\det(A)x_i = 0$ for every $i = 1, \ldots, n$.","I have been trying to prove linear independence of rows implies linear independence of columns in kind of an abstract setting of modules following some exercises and notes from an old course.  The following fact is something I think we are supposed to use but I did not see any way to prove it so I would like to understand the proof so I can potentially adapt it to a couple of similar problems. Let $A = (a_{i,j})$ be a square matrix with $n$ rows and columns and with entries in a commutative ring with identity $R$ and let $M$ be an $R$-module. Let $x_1, \ldots ,x_n \in M$ and suppose that$\sum_{j=1}^{n}a_{i,j} x_j = 0$ for $1  \leq i \leq n$ . Then how do you show $\det(A)x_i = 0$ for every $i = 1, \ldots, n$.",,"['linear-algebra', 'abstract-algebra', 'matrices', 'modules']"
75,Is this matrix positive semidefinite?,Is this matrix positive semidefinite?,,"Let $A$ be a $n\times n$ real (symmetric) positive definite matrix with spectrum contained in $[m, M]$ and also let $X$ be an $n\times p$ matrix such that $X'X=I_p$.  It is known that $$\frac{(M+m)^2}{4Mm}(X'AX)^{-1}-X'A^{-1}X \tag{$\star$}$$ is positive semidefinite. Is it true that $$\frac{(M+m)^4}{16M^2m^2}(X'AX)^{-2}-(X'A^{-1}X)^2 \tag{$\dagger$}$$ is also positive semidefinite?","Let $A$ be a $n\times n$ real (symmetric) positive definite matrix with spectrum contained in $[m, M]$ and also let $X$ be an $n\times p$ matrix such that $X'X=I_p$.  It is known that $$\frac{(M+m)^2}{4Mm}(X'AX)^{-1}-X'A^{-1}X \tag{$\star$}$$ is positive semidefinite. Is it true that $$\frac{(M+m)^4}{16M^2m^2}(X'AX)^{-2}-(X'A^{-1}X)^2 \tag{$\dagger$}$$ is also positive semidefinite?",,['matrices']
76,Right Inverse of a matrix,Right Inverse of a matrix,,I'm reading Linear Algebra by Bill Jacob and am having trouble with his development of the theory behind the right inverse of a matrix.  I did an internet search but didn't find anything useful.  Does anyone know of a reference that might explain the concept a little more clearly?,I'm reading Linear Algebra by Bill Jacob and am having trouble with his development of the theory behind the right inverse of a matrix.  I did an internet search but didn't find anything useful.  Does anyone know of a reference that might explain the concept a little more clearly?,,"['linear-algebra', 'reference-request', 'matrices']"
77,fast and reliable method for testing invertibility of Hermitian Toeplitz matrix,fast and reliable method for testing invertibility of Hermitian Toeplitz matrix,,"Is there a reliable method for testing how invertible a Hermitian Toeplitz matrix is without going through the work of actually inverting it? A determinant is obviously easy to compute, but I'm not sure what threshold to compare it against. The algorithm I am using is Levinson recursion.  The function can detect the divide-by-zero condition, but I am interested in also detecting/scoring the matrices that are almost singular.","Is there a reliable method for testing how invertible a Hermitian Toeplitz matrix is without going through the work of actually inverting it? A determinant is obviously easy to compute, but I'm not sure what threshold to compare it against. The algorithm I am using is Levinson recursion.  The function can detect the divide-by-zero condition, but I am interested in also detecting/scoring the matrices that are almost singular.",,"['linear-algebra', 'matrices', 'numerical-methods']"
78,"$AA^\top + A + A^\top = 0$, then $|\det A|\leq 2^n$",", then",AA^\top + A + A^\top = 0 |\det A|\leq 2^n,"Let $A\in \mathbb R^{n\times n}$ such that $AA^\top + A + A^\top = 0$ . Prove that $|\det A|\leq 2^n$ . $AA^\top + A + A^\top = 0$ rewrites as $(A+I_n)(A^\top +I_n) = I_n$ , hence $A+I_n$ is an orthogonal matrix and $\det(A+I_n)\in \{-1,1\}$ . If $\lambda$ is a real eigenvalue of $A$ (with an eigenvector in $\mathbb R^n$ ) then $\lambda\in \{0,-2\}$ . However $A$ may have complex eigenvalues, or the eigenvectors may have complex entries. I cannot make further progress. I'm not supposed to know that an orthogonal matrix can be diagonalized over $\mathbb C$ with eigenvalues having modulus $1$ , I'm thus looking for a solution which does not leverage this fact.","Let such that . Prove that . rewrites as , hence is an orthogonal matrix and . If is a real eigenvalue of (with an eigenvector in ) then . However may have complex eigenvalues, or the eigenvectors may have complex entries. I cannot make further progress. I'm not supposed to know that an orthogonal matrix can be diagonalized over with eigenvalues having modulus , I'm thus looking for a solution which does not leverage this fact.","A\in \mathbb R^{n\times n} AA^\top + A + A^\top = 0 |\det A|\leq 2^n AA^\top + A + A^\top = 0 (A+I_n)(A^\top +I_n) = I_n A+I_n \det(A+I_n)\in \{-1,1\} \lambda A \mathbb R^n \lambda\in \{0,-2\} A \mathbb C 1","['linear-algebra', 'matrices', 'inequality', 'determinant']"
79,Normal block upper triangular matrix proof,Normal block upper triangular matrix proof,,"Prove that if a block upper triangular matrix is normal then its off-diagonal blocks is zero and each of its diagonal blocks is normal. This question was asked before, but it got just one answer which contains a mistake(it does not take into consideration the order of multiplication of blocks). A complex square matrix $A$ is normal if it commutes with its conjugate transpose $A^*$ ( $A$ is normal $<=>$ $A^*A=AA^*$ ). So let’s say the matrix M is normal and it is block upper triangular, so it looks like this: $$M = \begin{pmatrix} A & B \\ 0 & C \end{pmatrix} $$ $$M^* = \begin{pmatrix} A^* & 0 \\ B^* & C^* \end{pmatrix} $$ We know that $A$ and $C$ are square, and we want to prove that $A$ and $C$ are normal, $B=0$ . If $M$ is normal, doing the block-computations gives us four following equations 1) $AA^* + BB^* = A^*A$ , 2) $A^*B=BA^*$ , 3) $B^*A=CB^*$ , 4) $C^*C+B^*B=CC^*$ . This is as far as I was able to get, have no idea how we can prove that $B=0$ from this. For context, I have completed a year long course of linear algebra, so I would say I know all the basic characteristics of normal matrices, such as spectral theorem and also some facts about Gram matrix of columns of matrix M(that it is a Hermitian matrix for example) and so on.","Prove that if a block upper triangular matrix is normal then its off-diagonal blocks is zero and each of its diagonal blocks is normal. This question was asked before, but it got just one answer which contains a mistake(it does not take into consideration the order of multiplication of blocks). A complex square matrix is normal if it commutes with its conjugate transpose ( is normal ). So let’s say the matrix M is normal and it is block upper triangular, so it looks like this: We know that and are square, and we want to prove that and are normal, . If is normal, doing the block-computations gives us four following equations 1) , 2) , 3) , 4) . This is as far as I was able to get, have no idea how we can prove that from this. For context, I have completed a year long course of linear algebra, so I would say I know all the basic characteristics of normal matrices, such as spectral theorem and also some facts about Gram matrix of columns of matrix M(that it is a Hermitian matrix for example) and so on.",A A^* A <=> A^*A=AA^* M = \begin{pmatrix} A & B \\ 0 & C \end{pmatrix}  M^* = \begin{pmatrix} A^* & 0 \\ B^* & C^* \end{pmatrix}  A C A C B=0 M AA^* + BB^* = A^*A A^*B=BA^* B^*A=CB^* C^*C+B^*B=CC^* B=0,"['linear-algebra', 'matrices', 'block-matrices']"
80,Recover a matrix from its Schur complements,Recover a matrix from its Schur complements,,"Suppose I have a matrix: $$ M = \begin{bmatrix}A & B \\ C & D\end{bmatrix} $$ With Schur complements: $$ M/A = D - CA^{-1}B \\ M/D = A - BD^{-1}C \\ $$ Given only the Schur complements $M/A$ and $M/D$ , and the off-diagonal blocks $B$ and $C$ , can I recover the original matrix $M$ ?  (That is, without knowing $A$ or $D$ ?)  It seems like this should be a lemma in a textbook somewhere, but I haven't come across it. Note: in my actual application the matrix is symmetric positive definite such that $C=B^\top$ . Edit Some resources I've looked at include Henderson & Searles on deriving the inverse of a sum of matrices and Chris Yeh's blog, Schur Complements and the Matrix Inversion Lemma . The closest I've come is the following expressions obtained by expressing $M^{-1}$ in terms of $M/A$ and $M/D$ and then, because the matrix inverse is unique, equating the two different expressions for $M^{-1}$ : $$ (M/A)^{-1} = D^{-1} + D^{-1}C(M/D)^{-1}BD^{-1} \\ (M/D)^{-1} = A^{-1} + A^{-1}B(M/A)^{-1}CA^{-1} $$ These at least express each of $A$ without knowing $D$ and $D$ without knowing $A$ , but I still cannot figure out how to actually solve for $A$ or $D$ from the expressions.","Suppose I have a matrix: With Schur complements: Given only the Schur complements and , and the off-diagonal blocks and , can I recover the original matrix ?  (That is, without knowing or ?)  It seems like this should be a lemma in a textbook somewhere, but I haven't come across it. Note: in my actual application the matrix is symmetric positive definite such that . Edit Some resources I've looked at include Henderson & Searles on deriving the inverse of a sum of matrices and Chris Yeh's blog, Schur Complements and the Matrix Inversion Lemma . The closest I've come is the following expressions obtained by expressing in terms of and and then, because the matrix inverse is unique, equating the two different expressions for : These at least express each of without knowing and without knowing , but I still cannot figure out how to actually solve for or from the expressions."," M = \begin{bmatrix}A & B \\ C & D\end{bmatrix}  
M/A = D - CA^{-1}B \\
M/D = A - BD^{-1}C \\
 M/A M/D B C M A D C=B^\top M^{-1} M/A M/D M^{-1} 
(M/A)^{-1} = D^{-1} + D^{-1}C(M/D)^{-1}BD^{-1} \\
(M/D)^{-1} = A^{-1} + A^{-1}B(M/A)^{-1}CA^{-1}
 A D D A A D","['matrices', 'block-matrices', 'schur-complement']"
81,Find vectors $ \bf x $ and $ \bf y $ of binaries that maximize $ \bf xCy $.,Find vectors  and  of binaries that maximize ., \bf x   \bf y   \bf xCy ,"Given a matrix of integers, $ \bf C $ , find row vector $ \bf x $ of binaries and column vector $ \bf y $ of binaries that maximize the product $ \bf xCy $ . Is there an efficient algorithm to either find a solution or approximate solution? I've been researching (Integer) Linear Programming and can't seem to find an algorithm for this particular variant. Here's an example. You're given matrix $$ \textbf{C} = \begin{bmatrix} -2 & 4 & -4\\ 8 & 4 & -7\\ -5&3&1\\ 2&2&2 \end{bmatrix} $$ . The solution of binary row/column vectors is: $$  \textbf{x} = \begin{bmatrix} 1 & 1 & 0 & 1 \end{bmatrix}, \textbf{y} = \begin{bmatrix} 1 \\ 1 \\ 0 \end{bmatrix}  $$ where $ \textbf{xCy} = 18$","Given a matrix of integers, , find row vector of binaries and column vector of binaries that maximize the product . Is there an efficient algorithm to either find a solution or approximate solution? I've been researching (Integer) Linear Programming and can't seem to find an algorithm for this particular variant. Here's an example. You're given matrix . The solution of binary row/column vectors is: where"," \bf C   \bf x   \bf y   \bf xCy   \textbf{C} = \begin{bmatrix}
-2 & 4 & -4\\
8 & 4 & -7\\
-5&3&1\\
2&2&2
\end{bmatrix}   
\textbf{x} = \begin{bmatrix} 1 & 1 & 0 & 1 \end{bmatrix},
\textbf{y} = \begin{bmatrix} 1 \\ 1 \\ 0 \end{bmatrix} 
  \textbf{xCy} = 18","['linear-algebra', 'matrices', 'optimization', 'linear-programming', 'discrete-optimization']"
82,Average Determinant,Average Determinant,,"Let $S$ be the set of all $n \times n$ real matrices ( $n$ > 1) with entries from a finite set $M$ . Can we say that the average value of determinants of matrices in $S$ is always zero? See that $S$ is closed under row interchange. So, we have matrices with equal determinant but of opposite sign. I think, we have to show that there is a bijective map between the subset of matrices with positive determinant and subset of matrices with negative determinant. Is that enough? Observe that the problem is independent of nature of the entry set $M$ .","Let be the set of all real matrices ( > 1) with entries from a finite set . Can we say that the average value of determinants of matrices in is always zero? See that is closed under row interchange. So, we have matrices with equal determinant but of opposite sign. I think, we have to show that there is a bijective map between the subset of matrices with positive determinant and subset of matrices with negative determinant. Is that enough? Observe that the problem is independent of nature of the entry set .",S n \times n n M S S M,"['matrices', 'determinant', 'average']"
83,How to find the characteristic polynomial of the given matrix?,How to find the characteristic polynomial of the given matrix?,,"I am stuck at finding the characteristic polynomial of the following matrix. $$A=\begin{bmatrix} 		\ell a^2 & a & a &  a&\dotso & \dotso& a &a 		\\ 		a &  t &0&0 	&	\dotso & \dotso &0&0 	\\ 	a & 0 & t &0 &\dotso 	&	\dotso & 0 &0 	\\ 	a & 0& 0& t &\dotso 	&	\dotso &0 &0 	\\ 	\dotso & \dotso& \dotso& \dotso &\dotso 	&	\dotso & \dotso & \dotso 	\\ 	\dotso & \dotso& \dotso& \dotso &\dotso 	&	\dotso & \dotso & \dotso 	\\ 	\dotso & \dotso& \dotso& \dotso &\dotso 	&	\dotso & \dotso & \dotso 	\\ 	a&  0 &0&0 	&	\dotso & \dotso&t&0 	\\ 	a&  0 &0&0 	&	\dotso & \dotso&0&t 			\end{bmatrix}_{(\ell+1) \times (\ell +1)}.$$ Here $t,l,a$ are constants. My try : The characteristic polynomial of the matrix is $\det(xI-A). We have, $\det(xI-A)=\begin{bmatrix} 		x-\ell a^2 & -a & -a &  -a&\dotso & \dotso& -a &-a 		\\ 		-a &  x-t &0&0 	&	\dotso & \dotso &0&0 	\\ 	-a & 0 & x-t &0 &\dotso 	&	\dotso & 0 &0 	\\ 	-a & 0& 0& x-t &\dotso 	&	\dotso &0 &0 	\\ 	\dotso & \dotso& \dotso& \dotso &\dotso 	&	\dotso & \dotso & \dotso 	\\ 	\dotso & \dotso& \dotso& \dotso &\dotso 	&	\dotso & \dotso & \dotso 	\\ 	\dotso & \dotso& \dotso& \dotso &\dotso 	&	\dotso & \dotso & \dotso 	\\ 	-a&  0 &0&0 	&	\dotso & \dotso&x-t&0 	\\ 	-a&  0 &0&0 	&	\dotso & \dotso&0&x-t 			\end{bmatrix}_{(\ell+1) \times (\ell +1)}.$ I expanded along the last row. I got $(x-t)\times\det(M)-a \times \det (N)$ where $M=\begin{bmatrix} 		x-\ell a^2 & -a &-a &  -a&\dotso & \dotso& -a  		\\ 		-a &  t &0&0 	&	\dotso & \dotso &0 	\\ 	-a & 0 &t &0 &\dotso 	&	\dotso & 0  	\\ 	-a & 0& 0& t &\dotso 	&	\dotso &0  	\\ 	\dotso & \dotso& \dotso& \dotso &\dotso 	&	\dotso & \dotso 	\\ 	\dotso & \dotso& \dotso& \dotso &\dotso 	&	\dotso & \dotso 	\\ 	\dotso & \dotso& \dotso& \dotso &\dotso 	&	\dotso & \dotso 	\\ 	-a&  0 &0&0 	&	\dotso & \dotso&t\end{bmatrix}$ and $N=\begin{bmatrix} 		-a &-a &  -a&\dotso & \dotso& -a  		\\ 		 x-t &0&0 	&	\dotso & \dotso &0 	\\ 	0 &x-t &0 &\dotso 	&	\dotso & 0  	\\ 	0& 0& x-t &\dotso 	&	\dotso &0  	\\ 	\dotso& \dotso& \dotso &\dotso 	&	\dotso & \dotso 	\\ 	\dotso & \dotso& \dotso& \dotso &\dotso 	&	\dotso  	\\ 	\dotso & \dotso& \dotso& \dotso &\dotso 	&	\dotso  	\\ 	0 &0&0 	&	\dotso & x-t&0\end{bmatrix}$ . But I am stuck from here. Can someone please help me out? Also, it is my request to everyone that please don't delete my question. This is my first question. I am trying really hard to show to the community what I have tried in the question. I am not so good at writing, I am a first year student studying Bachelors in Mathematics.","I am stuck at finding the characteristic polynomial of the following matrix. Here are constants. My try : The characteristic polynomial of the matrix is $\det(xI-A). We have, I expanded along the last row. I got where and . But I am stuck from here. Can someone please help me out? Also, it is my request to everyone that please don't delete my question. This is my first question. I am trying really hard to show to the community what I have tried in the question. I am not so good at writing, I am a first year student studying Bachelors in Mathematics.","A=\begin{bmatrix}
		\ell a^2 & a & a &  a&\dotso & \dotso& a &a
		\\
		a &  t &0&0
	&	\dotso & \dotso &0&0
	\\
	a & 0 & t &0 &\dotso
	&	\dotso & 0 &0
	\\
	a & 0& 0& t &\dotso
	&	\dotso &0 &0
	\\
	\dotso & \dotso& \dotso& \dotso &\dotso
	&	\dotso & \dotso & \dotso
	\\
	\dotso & \dotso& \dotso& \dotso &\dotso
	&	\dotso & \dotso & \dotso
	\\
	\dotso & \dotso& \dotso& \dotso &\dotso
	&	\dotso & \dotso & \dotso
	\\
	a&  0 &0&0
	&	\dotso & \dotso&t&0
	\\
	a&  0 &0&0
	&	\dotso & \dotso&0&t
			\end{bmatrix}_{(\ell+1) \times (\ell +1)}. t,l,a \det(xI-A)=\begin{bmatrix}
		x-\ell a^2 & -a & -a &  -a&\dotso & \dotso& -a &-a
		\\
		-a &  x-t &0&0
	&	\dotso & \dotso &0&0
	\\
	-a & 0 & x-t &0 &\dotso
	&	\dotso & 0 &0
	\\
	-a & 0& 0& x-t &\dotso
	&	\dotso &0 &0
	\\
	\dotso & \dotso& \dotso& \dotso &\dotso
	&	\dotso & \dotso & \dotso
	\\
	\dotso & \dotso& \dotso& \dotso &\dotso
	&	\dotso & \dotso & \dotso
	\\
	\dotso & \dotso& \dotso& \dotso &\dotso
	&	\dotso & \dotso & \dotso
	\\
	-a&  0 &0&0
	&	\dotso & \dotso&x-t&0
	\\
	-a&  0 &0&0
	&	\dotso & \dotso&0&x-t
			\end{bmatrix}_{(\ell+1) \times (\ell +1)}. (x-t)\times\det(M)-a \times \det (N) M=\begin{bmatrix}
		x-\ell a^2 & -a &-a &  -a&\dotso & \dotso& -a 
		\\
		-a &  t &0&0
	&	\dotso & \dotso &0
	\\
	-a & 0 &t &0 &\dotso
	&	\dotso & 0 
	\\
	-a & 0& 0& t &\dotso
	&	\dotso &0 
	\\
	\dotso & \dotso& \dotso& \dotso &\dotso
	&	\dotso & \dotso
	\\
	\dotso & \dotso& \dotso& \dotso &\dotso
	&	\dotso & \dotso
	\\
	\dotso & \dotso& \dotso& \dotso &\dotso
	&	\dotso & \dotso
	\\
	-a&  0 &0&0
	&	\dotso & \dotso&t\end{bmatrix} N=\begin{bmatrix}
		-a &-a &  -a&\dotso & \dotso& -a 
		\\
		 x-t &0&0
	&	\dotso & \dotso &0
	\\
	0 &x-t &0 &\dotso
	&	\dotso & 0 
	\\
	0& 0& x-t &\dotso
	&	\dotso &0 
	\\
	\dotso& \dotso& \dotso &\dotso
	&	\dotso & \dotso
	\\
	\dotso & \dotso& \dotso& \dotso &\dotso
	&	\dotso 
	\\
	\dotso & \dotso& \dotso& \dotso &\dotso
	&	\dotso 
	\\
	0 &0&0
	&	\dotso & x-t&0\end{bmatrix}","['linear-algebra', 'matrices', 'determinant', 'characteristic-polynomial']"
84,"Suppose $v^T(A^{-1} - B^{-1})v\geq 0$ for any vector $v$, show that $u^T(B - A)u \geq 0$ for any vector $u$","Suppose  for any vector , show that  for any vector",v^T(A^{-1} - B^{-1})v\geq 0 v u^T(B - A)u \geq 0 u,"Suppose $A$ and $B$ are $p\times p$ invertible, symmetric matrices with positive eigenvalues (i.e., positive definite) and that $$v^T(A^{-1} - B^{-1})v\geq 0$$ for any vector $v \in \mathbb{R}^p$ . How can I show that $$u^T(B - A)u \geq 0$$ for any vector $u \in \mathbb{R}^p$ ? Since $u^TBu$ and $u^TAu$ are scalars, I was hoping to take the inverse, i.e., $\frac{1}{u^TBu}$ and $\frac{1}{u^TAu}$ and relate them to $A^{-1}$ and $B^{-1}$ somehow. But I'm not sure how to go about this.","Suppose and are invertible, symmetric matrices with positive eigenvalues (i.e., positive definite) and that for any vector . How can I show that for any vector ? Since and are scalars, I was hoping to take the inverse, i.e., and and relate them to and somehow. But I'm not sure how to go about this.",A B p\times p v^T(A^{-1} - B^{-1})v\geq 0 v \in \mathbb{R}^p u^T(B - A)u \geq 0 u \in \mathbb{R}^p u^TBu u^TAu \frac{1}{u^TBu} \frac{1}{u^TAu} A^{-1} B^{-1},"['linear-algebra', 'matrices', 'inequality']"
85,"What's the intuitive meaning of matrix trace, if any?","What's the intuitive meaning of matrix trace, if any?",,"3Blue1Brown has great visual explanations of what some commonly used properties of matrices mean, such as determinant, rank, and kernel, but I have absolutely no idea what the trace of a matrix has in linear algebra and what its visual representation (if any) is.","3Blue1Brown has great visual explanations of what some commonly used properties of matrices mean, such as determinant, rank, and kernel, but I have absolutely no idea what the trace of a matrix has in linear algebra and what its visual representation (if any) is.",,"['linear-algebra', 'matrices', 'intuition', 'trace']"
86,Principal series representation isomorphism,Principal series representation isomorphism,,"The problem: Let $G = \mathrm{GL}_2(\mathbb Q_p)$ and $k$ be an algebraically closed field of characteristic $p.$ Denote by $\overline B$ the subgroup of all lower triangular matrices in $G$ and by $U$ the subgroup of all matrices of the form $\begin{bmatrix} 1 & \star \\ 0 & 1 \end{bmatrix}$ . Let $\chi_1, \chi_2 \colon \mathbb Q_p^{\times} \to k^{\times}$ be two smooth characters. Consider the smooth representation $$ \chi \colon \overline B \to k^\times,\quad \begin{bmatrix} \alpha & 0 \\ \gamma & \delta\end{bmatrix} \mapsto \chi_1(\alpha) \chi_2(\delta).$$ This induces a smooth $G$ -representation $\operatorname{Ind}_{\overline B}^G \chi.$ Show that there is a vector space isomorphism: \begin{align*}\{f \in \operatorname{Ind}_{\overline B}^G \chi \colon \operatorname{Supp} f \subseteq \overline B U\} &\longrightarrow \mathcal C_{\mathrm{cpt}}^{\infty}(\mathbb Q_p, k)\\ f & \longmapsto \left(f \mapsto f \begin{bmatrix}1 & x \\ 0 & 1\end{bmatrix}\right),\end{align*} where $\mathcal C_{\mathrm{cpt}}^{\infty}(\mathbb Q_p, k)$ is the space of all locally constant, compactly supported functions $\mathbb Q_p \to k.$ I understand that the image of any $f$ in the LHS is a locally constant function. But I cannot see how to show that the image is also compactly supported. Injectivity of the map is obvious. For surjectivity, we can do the following: Let $\phi$ be any locally constant, compactly supported, $k$ -valued map from $\mathbb Q_p.$ Define $f \colon G \to k$ as follows-- First define $f \colon U \to k$ as $f\begin{bmatrix} 1 & x \\ 0 & 1\end{bmatrix} = \phi(x)$ Note that $\overline B \cap U = \{1\}.$ Hence we may can extend it to a map $f \colon \overline B U \to k,$ $f \left(T\begin{bmatrix} 1 & x \\ 0 & 1\end{bmatrix}\right) = T\phi(x),\text{ for all } T \in \overline B.$ Just extend by zero to get a function $f \colon G \to k.$ I need to show that $f$ is fixed by an open subgroup. Any help will be appreciated.","The problem: Let and be an algebraically closed field of characteristic Denote by the subgroup of all lower triangular matrices in and by the subgroup of all matrices of the form . Let be two smooth characters. Consider the smooth representation This induces a smooth -representation Show that there is a vector space isomorphism: where is the space of all locally constant, compactly supported functions I understand that the image of any in the LHS is a locally constant function. But I cannot see how to show that the image is also compactly supported. Injectivity of the map is obvious. For surjectivity, we can do the following: Let be any locally constant, compactly supported, -valued map from Define as follows-- First define as Note that Hence we may can extend it to a map Just extend by zero to get a function I need to show that is fixed by an open subgroup. Any help will be appreciated.","G = \mathrm{GL}_2(\mathbb Q_p) k p. \overline B G U \begin{bmatrix} 1 & \star \\ 0 & 1 \end{bmatrix} \chi_1, \chi_2 \colon \mathbb Q_p^{\times} \to k^{\times}  \chi \colon \overline B \to k^\times,\quad \begin{bmatrix} \alpha & 0 \\ \gamma & \delta\end{bmatrix} \mapsto \chi_1(\alpha) \chi_2(\delta). G \operatorname{Ind}_{\overline B}^G \chi. \begin{align*}\{f \in \operatorname{Ind}_{\overline B}^G \chi \colon \operatorname{Supp} f \subseteq \overline B U\} &\longrightarrow \mathcal C_{\mathrm{cpt}}^{\infty}(\mathbb Q_p, k)\\ f & \longmapsto \left(f \mapsto f \begin{bmatrix}1 & x \\ 0 & 1\end{bmatrix}\right),\end{align*} \mathcal C_{\mathrm{cpt}}^{\infty}(\mathbb Q_p, k) \mathbb Q_p \to k. f \phi k \mathbb Q_p. f \colon G \to k f \colon U \to k f\begin{bmatrix} 1 & x \\ 0 & 1\end{bmatrix} = \phi(x) \overline B \cap U = \{1\}. f \colon \overline B U \to k, f \left(T\begin{bmatrix} 1 & x \\ 0 & 1\end{bmatrix}\right) = T\phi(x),\text{ for
all } T \in \overline B. f \colon G \to k. f","['matrices', 'group-theory', 'number-theory', 'representation-theory', 'general-linear-group']"
87,Let A and B are square matrices of order 2 with real elements such that AB = $A^2B^2 - (AB)^2$,Let A and B are square matrices of order 2 with real elements such that AB =,A^2B^2 - (AB)^2,"A and B are 2 x 2 matrices with real elements and AB = $A^2B^2 - (AB)^2$ and |B| = 3, Find the value of |A+2B| - |B+2A|. Using the given relation, i managed to get |A| = 0 by the following; AB = $A^2B^2 - (AB)^2$ A = $A^2B - ABA$ A(I - AB - BA) = 0 [ taking determinant on both sides ] |A||I - AB - BA| = 0 It's here I was stuck on and later on seeing the solution it said this; "" For 2x2 matrices A and B, the following relationship holds true for some scalar x, |A+xB| = |A| + $x^2|B|$ + x[Tr(A)Tr(B)-Tr(AB)] "" Could anyone explain how this relationship was derived or any alternates to solve this problem.","A and B are 2 x 2 matrices with real elements and AB = and |B| = 3, Find the value of |A+2B| - |B+2A|. Using the given relation, i managed to get |A| = 0 by the following; AB = A = A(I - AB - BA) = 0 [ taking determinant on both sides ] |A||I - AB - BA| = 0 It's here I was stuck on and later on seeing the solution it said this; "" For 2x2 matrices A and B, the following relationship holds true for some scalar x, |A+xB| = |A| + + x[Tr(A)Tr(B)-Tr(AB)] "" Could anyone explain how this relationship was derived or any alternates to solve this problem.",A^2B^2 - (AB)^2 A^2B^2 - (AB)^2 A^2B - ABA x^2|B|,"['linear-algebra', 'matrices', 'determinant']"
88,Generalization of $y-x$ vector space,Generalization of  vector space,y-x,"I was wondering about, if we have two vectors $x=\begin{pmatrix} 2 \\ 1 \end{pmatrix}$ and $y=\begin{pmatrix} 3 \\ 2 \end{pmatrix} \in \Bbb R^2$ , then $y-x=\begin{pmatrix} 1 \\ 1 \end{pmatrix}$ . If we think about the subspace spanned by this $y-x$ vector, then we know it could be represented by the line along the $\begin{pmatrix} 1 \\ 1 \end{pmatrix}$ diagonal. However, I’m wondering how this generalizes to higher dimensions. Imagine the $3 \times 2$ case, where $x$ and $y$ are both $3 \times 2$ matrices and would represent planes in three dimensional space. How do you represent the difference between the basis vectors? I imagine you can’t just look at $y-x$ because then you’re only looking at the difference between the first column of $y$ and the first column of $x$ and so on for the second column. I hope this idea makes sense, thanks if possible.","I was wondering about, if we have two vectors and , then . If we think about the subspace spanned by this vector, then we know it could be represented by the line along the diagonal. However, I’m wondering how this generalizes to higher dimensions. Imagine the case, where and are both matrices and would represent planes in three dimensional space. How do you represent the difference between the basis vectors? I imagine you can’t just look at because then you’re only looking at the difference between the first column of and the first column of and so on for the second column. I hope this idea makes sense, thanks if possible.",x=\begin{pmatrix} 2 \\ 1 \end{pmatrix} y=\begin{pmatrix} 3 \\ 2 \end{pmatrix} \in \Bbb R^2 y-x=\begin{pmatrix} 1 \\ 1 \end{pmatrix} y-x \begin{pmatrix} 1 \\ 1 \end{pmatrix} 3 \times 2 x y 3 \times 2 y-x y x,"['linear-algebra', 'matrices', 'vector-spaces', 'vectors']"
89,"$(-1,0,1)$-square matrix has different line sums?",-square matrix has different line sums?,"(-1,0,1)","Let $A$ be a $n\times n$ matrix with coefficients from the set $\{-1,0,1\}$ . Let $r_i$ and $c_i$ denote the sum of the elements of the $i$ -th row and column of $A$ respectively. For which $n$ is it possible that the numbers $r_1,...,r_n,c_1,...,c_n$ be different pairwise? If $n$ is even, I am able to construct such a matrix, which I call $A_n$ . $$ A_2=\begin{pmatrix}1&1\\-1&0\end{pmatrix}\\ A_4=\begin{pmatrix}1&1&1&1\\-1&1&1&1\\-1&-1&0&1\\-1&-1&-1&0\end{pmatrix}\\ A_{n+2}=\begin{pmatrix}1& \begin{matrix}1&\cdots&1\end{matrix} &1\\ \begin{matrix}-1\\\vdots\\-1\end{matrix} &A_n&\begin{matrix}1\\\vdots\\1\end{matrix}\\-1&\begin{matrix}-1&\cdots&-1\end{matrix}&0\end{pmatrix} $$ Not hard to check that these matrices have all different line (row or column) sums. However, if $n$ is odd, it seems impossible to construct such a matrix. I'd like to prove that if $n$ is odd, any $n\times n$ matrix with coefficients from the set $\{-1,0,1\}$ has two lines with the same sum of elements. Any ideas on how to proceed?","Let be a matrix with coefficients from the set . Let and denote the sum of the elements of the -th row and column of respectively. For which is it possible that the numbers be different pairwise? If is even, I am able to construct such a matrix, which I call . Not hard to check that these matrices have all different line (row or column) sums. However, if is odd, it seems impossible to construct such a matrix. I'd like to prove that if is odd, any matrix with coefficients from the set has two lines with the same sum of elements. Any ideas on how to proceed?","A n\times n \{-1,0,1\} r_i c_i i A n r_1,...,r_n,c_1,...,c_n n A_n 
A_2=\begin{pmatrix}1&1\\-1&0\end{pmatrix}\\
A_4=\begin{pmatrix}1&1&1&1\\-1&1&1&1\\-1&-1&0&1\\-1&-1&-1&0\end{pmatrix}\\
A_{n+2}=\begin{pmatrix}1& \begin{matrix}1&\cdots&1\end{matrix} &1\\ \begin{matrix}-1\\\vdots\\-1\end{matrix} &A_n&\begin{matrix}1\\\vdots\\1\end{matrix}\\-1&\begin{matrix}-1&\cdots&-1\end{matrix}&0\end{pmatrix}
 n n n\times n \{-1,0,1\}","['matrices', 'number-theory', 'block-matrices', 'random-matrices']"
90,Solving general multivariable quadratic equations,Solving general multivariable quadratic equations,,"Consider the variables $\mathbf{x}\in\mathbb{R}^n$ and the known coefficients $\mathbf{A}_i \in \mathbb{R}^{n\times n}, \mathbf{b}_i \in \mathbb{R}^n,$ and $c_i \in \mathbb{R}$ for $i=1,2,\cdots, n$ . They satisfy an $n$ -equation system given by: $$ \mathbf{x}^T\mathbf{A}_i \mathbf{x} + \mathbf{b}_i^T \mathbf{x} + c_i = 0, \quad i=1, 2,\cdots,n. $$ Can anyone suggest a general solver that can be used for this type of system? I'm not well-versed in algebra and would appreciate it if you could provide me with some method names that I can use.",Consider the variables and the known coefficients and for . They satisfy an -equation system given by: Can anyone suggest a general solver that can be used for this type of system? I'm not well-versed in algebra and would appreciate it if you could provide me with some method names that I can use.,"\mathbf{x}\in\mathbb{R}^n \mathbf{A}_i \in \mathbb{R}^{n\times n}, \mathbf{b}_i \in \mathbb{R}^n, c_i \in \mathbb{R} i=1,2,\cdots, n n 
\mathbf{x}^T\mathbf{A}_i \mathbf{x} + \mathbf{b}_i^T \mathbf{x} + c_i = 0, \quad i=1, 2,\cdots,n.
","['matrices', 'vectors', 'systems-of-equations', 'matrix-equations', 'matrix-calculus']"
91,$N$-fixed vectors in smooth representations are also fixed by ${\rm SL}_2(\mathbf{Q}_p)$.,-fixed vectors in smooth representations are also fixed by .,N {\rm SL}_2(\mathbf{Q}_p),"Let $N = \left(\begin{matrix} 1 & * \\ 0 & 1 \end{matrix}\right)$ be the upper triangular unipotent subgroup in ${\rm GL}_2(\mathbf{Q}_p)$ and $K_n = 1 + p^n M_2(\mathbf{Z}_p)$ the usual compact opens in ${\rm GL}_2(\mathbf{Q}_p)$ . I am trying to show that the subgroup generated by $N$ and $K_r$ would contain ${\rm SL}_2(\mathbf{Q}_p)$ for every $r \gg 0$ . This would prove the claim in the title. I believe that, I could make this work if I showed that the group $\langle N, K_r \rangle$ contains $K_{r-1} \cap N^t$ where $N^t$ is the lower triangular unipotent subgroup. I can't quite think of the right matrices from $N$ and $K_r$ to multiply to get the power of $p$ in the lower right hand corner to decrease... Any thoughts would be appreciated.","Let be the upper triangular unipotent subgroup in and the usual compact opens in . I am trying to show that the subgroup generated by and would contain for every . This would prove the claim in the title. I believe that, I could make this work if I showed that the group contains where is the lower triangular unipotent subgroup. I can't quite think of the right matrices from and to multiply to get the power of in the lower right hand corner to decrease... Any thoughts would be appreciated.","N = \left(\begin{matrix} 1 & * \\ 0 & 1 \end{matrix}\right) {\rm GL}_2(\mathbf{Q}_p) K_n = 1 + p^n M_2(\mathbf{Z}_p) {\rm GL}_2(\mathbf{Q}_p) N K_r {\rm SL}_2(\mathbf{Q}_p) r \gg 0 \langle N, K_r \rangle K_{r-1} \cap N^t N^t N K_r p","['matrices', 'group-theory', 'representation-theory', 'p-adic-number-theory', 'automorphic-forms']"
92,Continuity of the thin QR factorization.,Continuity of the thin QR factorization.,,"In section 5.2 of the book Matrix Computations by Golub & Van Loan we can find the following result: Let $A\in\mathbb R^{m\times k}$ be a matrix with full column rank. Then we have the factorization $A=QR$ for some matrix $Q\in \mathbb R^{m\times k}$ with orthonormal columns and some upper triangular matrix $R \in \mathbb R^{k\times k}$ with positive diagonal entries. Moreover the factorization is unique and is called the thin QR factorization of $A$ . I want to show that the factorization is continuous with respect to $A$ . So let $(A_n)$ be a sequence of matrices in $\mathbb R^{m\times k}$ , each with full column rank, and suppose that $A_n\to A$ . For each $n$ , let $Q_nR_n$ denote the thin factorization of $A_n$ . Suppose $Q_{n'}\to Q_0 \in \mathbb R^{m\times k}$ for some subsequence $n'$ . Then $Q_0^\top Q_0=\lim Q_{n'}^\top Q_{n'} =I_k$ so $Q_0$ has orthonormal columns. Moreover we have $R_{n'}=Q^\top_{n'}Q_{n'}R_{n'}=Q^\top_{n'}A_{n'}\to R_0$ where $R_0=Q^\top_0A$ . Since each $R_{n'}$ is upper triangular with nonnegative diagonal entries, so is $R_0$ . Moreover we have $$QR=\lim Q_{n'}R_{n'}=Q_0R_0$$ and so $\text{rank}(R_0)=\text{rank}(Q_0R_0)=\text{rank}(QR)=\text{rank}(R)=k$ . Therefore the diagonal entries of $R_0$ are in fact positive. From the uniqueness of the thin factorization we conclude that $Q_0=Q$ and $R_0=R$ . We have shown that any convergent subsequence of $(Q_n)$ must converge to $Q$ . We claim that this implies that $Q_n\to Q$ . Otherwise we can construct a subsequence $(Q_{n'})$ which is bounded away from $Q$ by some distance greater than some $\epsilon>0$ . Since $\|Q_n\|=\sqrt{k}$ for all $n$ , the sequence $(Q_{n'})$ is bounded, and so we can extract a further subsequence converging to some matrix other than $Q$ , a contradiction. Hence $Q_n\to Q$ , and repeating the first part of the proof we get $R_n\to R$ as well. Is this proof correct? Thanks a lot for your help.","In section 5.2 of the book Matrix Computations by Golub & Van Loan we can find the following result: Let be a matrix with full column rank. Then we have the factorization for some matrix with orthonormal columns and some upper triangular matrix with positive diagonal entries. Moreover the factorization is unique and is called the thin QR factorization of . I want to show that the factorization is continuous with respect to . So let be a sequence of matrices in , each with full column rank, and suppose that . For each , let denote the thin factorization of . Suppose for some subsequence . Then so has orthonormal columns. Moreover we have where . Since each is upper triangular with nonnegative diagonal entries, so is . Moreover we have and so . Therefore the diagonal entries of are in fact positive. From the uniqueness of the thin factorization we conclude that and . We have shown that any convergent subsequence of must converge to . We claim that this implies that . Otherwise we can construct a subsequence which is bounded away from by some distance greater than some . Since for all , the sequence is bounded, and so we can extract a further subsequence converging to some matrix other than , a contradiction. Hence , and repeating the first part of the proof we get as well. Is this proof correct? Thanks a lot for your help.",A\in\mathbb R^{m\times k} A=QR Q\in \mathbb R^{m\times k} R \in \mathbb R^{k\times k} A A (A_n) \mathbb R^{m\times k} A_n\to A n Q_nR_n A_n Q_{n'}\to Q_0 \in \mathbb R^{m\times k} n' Q_0^\top Q_0=\lim Q_{n'}^\top Q_{n'} =I_k Q_0 R_{n'}=Q^\top_{n'}Q_{n'}R_{n'}=Q^\top_{n'}A_{n'}\to R_0 R_0=Q^\top_0A R_{n'} R_0 QR=\lim Q_{n'}R_{n'}=Q_0R_0 \text{rank}(R_0)=\text{rank}(Q_0R_0)=\text{rank}(QR)=\text{rank}(R)=k R_0 Q_0=Q R_0=R (Q_n) Q Q_n\to Q (Q_{n'}) Q \epsilon>0 \|Q_n\|=\sqrt{k} n (Q_{n'}) Q Q_n\to Q R_n\to R,"['linear-algebra', 'matrices', 'continuity', 'matrix-decomposition', 'matrix-rank']"
93,"Kalman filter, intuitively","Kalman filter, intuitively",,"I am currently working my way through the Kalman filter equations. Be warned, I think do have a solid understanding of math, yet I am just an engineer. So first of all there is this excellent website that provides quite an easy to follow introduction to the matter. I get along with it really well but there is one caveat that really gives me headaches. Consider the multidimensional expression for the covariance extrapolation: $$ \mathbf P_{n+1,n} = \mathbf F \mathbf P_{n,n} \mathbf F^\top + \mathbf Q $$ Note the scheme $ \mathbf A \mathbf B \mathbf A^\top $ that occurs in other occasions again. How could one describe this intuitively ? For comparison, in $ \mathbf A \vec x = \vec b $ my mind intuitively grasps $\mathbf A$ as a recipe to make the components for $\vec b$ from the components of $\vec a$ on the lowest level. A little elevated from that I see $\mathbf A$ as a function/projection/... But I just cannot come up with something mnemonic for the above scheme, and I'd be thankful if someone could shine a light.","I am currently working my way through the Kalman filter equations. Be warned, I think do have a solid understanding of math, yet I am just an engineer. So first of all there is this excellent website that provides quite an easy to follow introduction to the matter. I get along with it really well but there is one caveat that really gives me headaches. Consider the multidimensional expression for the covariance extrapolation: Note the scheme that occurs in other occasions again. How could one describe this intuitively ? For comparison, in my mind intuitively grasps as a recipe to make the components for from the components of on the lowest level. A little elevated from that I see as a function/projection/... But I just cannot come up with something mnemonic for the above scheme, and I'd be thankful if someone could shine a light."," \mathbf P_{n+1,n} = \mathbf F \mathbf P_{n,n} \mathbf F^\top + \mathbf Q   \mathbf A \mathbf B \mathbf A^\top   \mathbf A \vec x = \vec b  \mathbf A \vec b \vec a \mathbf A","['linear-algebra', 'matrices', 'kalman-filter']"
94,An injective or surjective square matrix is bijective?,An injective or surjective square matrix is bijective?,,"Is the argument below valid? Let $A$ a square matrix $n \times n$ . Suppose $A$ is injective, ie $Ker(A) = {0}$ . Therefore the columns of $A$ are linearly independent. We have $n$ vectors that are linearly independent, which means the set of vectors (the columns of A) are a basis of $R^n$ . Therefore, all vectors $\in R^n$ are a (unique) linear combination of the columns of $A$ . Therefore, $A$ has a unique solution to all vectors in $R^n$ . Therefore, $Im(A) = R^n$ . Therefore $A$ is surjective. Therefore $A$ is bijective. Let $A$ a square matrix $n \times n$ . Suppose $A$ is surjective, ie $Im(A) = R^n$ . Therefore every element of $R^n$ is a linear combination of the columns of $A$ . We have $n$ columns, meaning a generating set of $R^n$ of $n$ vectors. Therefore the columns of $A$ must be linearly independent. Therefore $Ker(A) = {0}$ . Therefore $A$ is injective. Therefore $A$ is bijective.","Is the argument below valid? Let a square matrix . Suppose is injective, ie . Therefore the columns of are linearly independent. We have vectors that are linearly independent, which means the set of vectors (the columns of A) are a basis of . Therefore, all vectors are a (unique) linear combination of the columns of . Therefore, has a unique solution to all vectors in . Therefore, . Therefore is surjective. Therefore is bijective. Let a square matrix . Suppose is surjective, ie . Therefore every element of is a linear combination of the columns of . We have columns, meaning a generating set of of vectors. Therefore the columns of must be linearly independent. Therefore . Therefore is injective. Therefore is bijective.",A n \times n A Ker(A) = {0} A n R^n \in R^n A A R^n Im(A) = R^n A A A n \times n A Im(A) = R^n R^n A n R^n n A Ker(A) = {0} A A,"['linear-algebra', 'matrices', 'vector-spaces']"
95,Does this relationship between representations in different characteristics hold?,Does this relationship between representations in different characteristics hold?,,"Is it possible for the dimension of the smallest faithful representation of a group to be the same or larger in all finite characteristics than in any field of characteristic $0?$ I know that this happens for the Thompson sporadic group, which has a faithful representation of dimension $248,$ but can this happen for groups where the smallest faithful representation has dimension $249$ or more?","Is it possible for the dimension of the smallest faithful representation of a group to be the same or larger in all finite characteristics than in any field of characteristic I know that this happens for the Thompson sporadic group, which has a faithful representation of dimension but can this happen for groups where the smallest faithful representation has dimension or more?","0? 248, 249","['abstract-algebra', 'matrices', 'group-theory', 'representation-theory']"
96,Eigenvalues of $A^\dagger A$,Eigenvalues of,A^\dagger A,"With a $2\times 2$ matrix $A$ , let $u$ be an eigenvector of $B=A^\dagger A$ . My question is: when can the eigenvalue $\lambda = u^\dagger B u$ lie between $0$ and $1$ , i.e., what are the conditions under which $\lambda \in [0,1]$ ? Here $\dagger$ denotes the Conjugate-Transpose. Also, $u$ is normalized i.e., $u^\dagger u = I$ .","With a matrix , let be an eigenvector of . My question is: when can the eigenvalue lie between and , i.e., what are the conditions under which ? Here denotes the Conjugate-Transpose. Also, is normalized i.e., .","2\times 2 A u B=A^\dagger A \lambda = u^\dagger B u 0 1 \lambda \in [0,1] \dagger u u^\dagger u = I","['matrices', 'eigenvalues-eigenvectors']"
97,What does $[\mathbf{a}]_\times\mathbf{b}$ mean?,What does  mean?,[\mathbf{a}]_\times\mathbf{b},"The left means $A$ cross $B$ . What does the right mean? $$\mathbf{a}\times\mathbf{b}=[\mathbf{a}]_\times\mathbf{b}$$ It came from the GitHub post ""How to derive essential matrix from camera projection matrices?"" by Chen Feng (simbaforrest).","The left means cross . What does the right mean? It came from the GitHub post ""How to derive essential matrix from camera projection matrices?"" by Chen Feng (simbaforrest).",A B \mathbf{a}\times\mathbf{b}=[\mathbf{a}]_\times\mathbf{b},"['linear-algebra', 'matrices', 'notation', 'vectors']"
98,Inner product inequality for symmetric matrix,Inner product inequality for symmetric matrix,,"I could use some guidance regarding this question. Let $A$ be an $n\times n$ symmetric matrix with non-negative elements. Prove that for any nonzero $x \in \mathbb{R}^n$ with non-negative elements the following inequality holds: $$ \Bigg(\frac{\langle x, Ax\rangle}{\langle x, x\rangle}\Bigg)^m \leq \frac{\langle x, A^mx\rangle}{\langle x, x\rangle}, \quad m \in \mathbb{Z}^+ $$ where $\langle \cdot, \cdot \rangle$ denotes the inner (dot) product. Hint: Use induction. All I have so far is this after the inductive step: $$ \Bigg(\frac{\langle x, Ax\rangle}{\langle x, x\rangle}\Bigg)^k \Bigg(\frac{\langle x, Ax\rangle}{\langle x, x\rangle}\Bigg) \leq \Bigg(\frac{\langle x, A^kx\rangle}{\langle x, x\rangle}\Bigg) \Bigg(\frac{\langle x, Ax\rangle}{\langle x, x\rangle}\Bigg) $$ for some $k \in \mathbb{Z}^+$ , trying to show the inequality for $k+1$ . I messed around with the symmetric matrix fact and transposes on the RHS, but I did not get anywhere. I asked around at the department, but I did not get much help. A few questions right away: What fundamental thing am I missing here? There must be some piece of information or some overall concept that I am not seeing. These look like Rayleigh quotients (although I do not know much about them). Are they key to solving this problem? What does non-negative entries give you? Is it simply to ensure that you don't divide by zero, or is there something else going on? I know that each $x$ in the statement is essentially a unit vector. Does this have something to do with the problem? When you apply $A$ though, I do not think that $x$ remains a unit vector in general. UPDATE(1): I'm pretty sure this has to do with the spectra of real symmetric matrices. I just looked up Rayleigh quotients, and it seems like there are some theorems more generally that involve forms like those in this question. UPDATE(2): I was given a suggestion (not a hint, so I do not know yet if it is fruitful) to try to prove the case for $m=2$ and then prove the inductive step of $m$ implies $m + 2$ . I have not yet had the time to try this, but it seems promising.","I could use some guidance regarding this question. Let be an symmetric matrix with non-negative elements. Prove that for any nonzero with non-negative elements the following inequality holds: where denotes the inner (dot) product. Hint: Use induction. All I have so far is this after the inductive step: for some , trying to show the inequality for . I messed around with the symmetric matrix fact and transposes on the RHS, but I did not get anywhere. I asked around at the department, but I did not get much help. A few questions right away: What fundamental thing am I missing here? There must be some piece of information or some overall concept that I am not seeing. These look like Rayleigh quotients (although I do not know much about them). Are they key to solving this problem? What does non-negative entries give you? Is it simply to ensure that you don't divide by zero, or is there something else going on? I know that each in the statement is essentially a unit vector. Does this have something to do with the problem? When you apply though, I do not think that remains a unit vector in general. UPDATE(1): I'm pretty sure this has to do with the spectra of real symmetric matrices. I just looked up Rayleigh quotients, and it seems like there are some theorems more generally that involve forms like those in this question. UPDATE(2): I was given a suggestion (not a hint, so I do not know yet if it is fruitful) to try to prove the case for and then prove the inductive step of implies . I have not yet had the time to try this, but it seems promising.","A n\times n x \in \mathbb{R}^n 
\Bigg(\frac{\langle x, Ax\rangle}{\langle x, x\rangle}\Bigg)^m \leq \frac{\langle x, A^mx\rangle}{\langle x, x\rangle}, \quad m \in \mathbb{Z}^+
 \langle \cdot, \cdot \rangle 
\Bigg(\frac{\langle x, Ax\rangle}{\langle x, x\rangle}\Bigg)^k \Bigg(\frac{\langle x, Ax\rangle}{\langle x, x\rangle}\Bigg) \leq \Bigg(\frac{\langle x, A^kx\rangle}{\langle x, x\rangle}\Bigg) \Bigg(\frac{\langle x, Ax\rangle}{\langle x, x\rangle}\Bigg)
 k \in \mathbb{Z}^+ k+1 x A x m=2 m m + 2","['linear-algebra', 'matrices', 'symmetric-matrices']"
99,"Existence of an integer matrix with maximal subdeterminants $a_1, \ldots, a_n$",Existence of an integer matrix with maximal subdeterminants,"a_1, \ldots, a_n","Given $n \geq 2$ and integers $a_1, \ldots, a_n$ , does there exist an integer $(n-1) \times n$ matrix whose maximal subdeterminants are $a_1, \ldots, a_n$ (with fixed ordering)? Example: $n = 3$ , $(a_1, a_2, a_3) = (19, 4, 22)$ . The matrix $$\begin{pmatrix}0 & 11 & 2 \\ -2 &95&19\end{pmatrix}$$ has $i$ th subdeterminants (with $i$ th column removed) equal to $(19, 4, 22)$ . Context: The $n=3$ case is precisely this question from the newsletter: Is every vector in $\mathbb Z^3$ a cross product? . (This is where the example comes from.) The general case would give an alternative proof for this question: Can the determinant of an integer matrix with a given row be any multiple of the gcd of that row? by taking $(a_1, \ldots, a_n)$ to be coefficients in Bézout's theorem.","Given and integers , does there exist an integer matrix whose maximal subdeterminants are (with fixed ordering)? Example: , . The matrix has th subdeterminants (with th column removed) equal to . Context: The case is precisely this question from the newsletter: Is every vector in $\mathbb Z^3$ a cross product? . (This is where the example comes from.) The general case would give an alternative proof for this question: Can the determinant of an integer matrix with a given row be any multiple of the gcd of that row? by taking to be coefficients in Bézout's theorem.","n \geq 2 a_1, \ldots, a_n (n-1) \times n a_1, \ldots, a_n n = 3 (a_1, a_2, a_3) = (19, 4, 22) \begin{pmatrix}0 & 11 & 2 \\
-2 &95&19\end{pmatrix} i i (19, 4, 22) n=3 (a_1, \ldots, a_n)","['linear-algebra', 'matrices', 'determinant', 'integers']"
