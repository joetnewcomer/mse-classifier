,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,Finding the 10th root of a matrix,Finding the 10th root of a matrix,,"I want to find a $2 \times 2$ matrix, named $A$ in this situation, such that: $$A^{10}=\begin {bmatrix} 1 & 1 \\ 0 & 1 \end {bmatrix} $$ How can I get started? I was thinking about filling $A$ with arbitrary values $a, b, c, d$ and then multiplying it by itself ten times, then setting those values equal to the given values but I quickly realized that would take too long. Is there a more efficient way?","I want to find a $2 \times 2$ matrix, named $A$ in this situation, such that: $$A^{10}=\begin {bmatrix} 1 & 1 \\ 0 & 1 \end {bmatrix} $$ How can I get started? I was thinking about filling $A$ with arbitrary values $a, b, c, d$ and then multiplying it by itself ten times, then setting those values equal to the given values but I quickly realized that would take too long. Is there a more efficient way?",,"['linear-algebra', 'matrices', 'exponentiation', 'matrix-equations']"
1,Is reducing a matrix to row echelon form useful at all?,Is reducing a matrix to row echelon form useful at all?,,"I have just started auditing Linear Algebra, and the first thing we learned was how to solve a system of linear equations by reducing it to row echelon form (or reduced row echelon form). I know how to solve a system of linear equations using determinants, and so the using row echelon form seems to be very inefficient and an easy way to make mistakes. The lecturer seemed to indicate that this will be useful in the future for something else, but a friend who has a PhD in mathematics said it is totally useless. Is this true? Is this technique totally useless or is there some ""higher mathematics"" field/ technique where row echelon form will be useful?","I have just started auditing Linear Algebra, and the first thing we learned was how to solve a system of linear equations by reducing it to row echelon form (or reduced row echelon form). I know how to solve a system of linear equations using determinants, and so the using row echelon form seems to be very inefficient and an easy way to make mistakes. The lecturer seemed to indicate that this will be useful in the future for something else, but a friend who has a PhD in mathematics said it is totally useless. Is this true? Is this technique totally useless or is there some ""higher mathematics"" field/ technique where row echelon form will be useful?",,['linear-algebra']
2,Intuition for Formal Definition of Linear Independence,Intuition for Formal Definition of Linear Independence,,"I learned about this a long time ago but it never really clicked, which led me to these questions: How the formal definition (at the bottom) works. I have a rough intuition: linear independence is where the variables are independent and don't affect each other. But I don't follow the formal definition. I would like to have a deep understanding of the formal definition based on these linear combination equations. I'm not sure how a linear combination constructed in a certain way can tell you the variables are independent or not. Why set the linear combination equations to $\vec{0}$. I don't see how setting to zero helps determine independence or not. Why choose $a_i$ to be non-zero in one case. It seems arbitrary. From Wikipedia : A subset $S=\{{\vec {v}}_{1},{\vec {v}}_{2},\dots ,{\vec {v}}_{n}\}$ of a vector space $V$ is linearly dependent if there exist a finite number of distinct vectors ${\vec {v}}_{1},{\vec {v}}_{2},\dots ,{\vec {v}}_{k}$ in $S$ and scalars $a_{1},a_{2},\dots ,a_{k}$, not all zero, such that $$a_{1}{\vec {v}}_{1}+a_{2}{\vec {v}}_{2}+\cdots +a_{k}{\vec {v}}_{k}={\vec {0}}$$ where ${\vec {0}}$ denotes the zero vector. The vectors in a set $T=\{{\vec {v}}_{1},{\vec {v}}_{2},\dots ,{\vec {v}}_{n}\}$ are linearly independent if the equation $$a_{1}{\vec {v}}_{1}+a_{2}{\vec {v}}_{2}+\cdots +a_{n}{\vec {v}}_{n}={\vec {0}}$$ can only be satisfied by $a_{i}=0$ for $i=1,\dots ,n$. So my understanding is, there are two subsets $S$ and $T$ of $V$. In one of them, the coefficients are not all zero, in the other they are all zero. In one case they are linearly dependent, in the other not. I don't understand why though; that's as much as I understand. Not sure why the equations were constructed like this in the first place.","I learned about this a long time ago but it never really clicked, which led me to these questions: How the formal definition (at the bottom) works. I have a rough intuition: linear independence is where the variables are independent and don't affect each other. But I don't follow the formal definition. I would like to have a deep understanding of the formal definition based on these linear combination equations. I'm not sure how a linear combination constructed in a certain way can tell you the variables are independent or not. Why set the linear combination equations to $\vec{0}$. I don't see how setting to zero helps determine independence or not. Why choose $a_i$ to be non-zero in one case. It seems arbitrary. From Wikipedia : A subset $S=\{{\vec {v}}_{1},{\vec {v}}_{2},\dots ,{\vec {v}}_{n}\}$ of a vector space $V$ is linearly dependent if there exist a finite number of distinct vectors ${\vec {v}}_{1},{\vec {v}}_{2},\dots ,{\vec {v}}_{k}$ in $S$ and scalars $a_{1},a_{2},\dots ,a_{k}$, not all zero, such that $$a_{1}{\vec {v}}_{1}+a_{2}{\vec {v}}_{2}+\cdots +a_{k}{\vec {v}}_{k}={\vec {0}}$$ where ${\vec {0}}$ denotes the zero vector. The vectors in a set $T=\{{\vec {v}}_{1},{\vec {v}}_{2},\dots ,{\vec {v}}_{n}\}$ are linearly independent if the equation $$a_{1}{\vec {v}}_{1}+a_{2}{\vec {v}}_{2}+\cdots +a_{n}{\vec {v}}_{n}={\vec {0}}$$ can only be satisfied by $a_{i}=0$ for $i=1,\dots ,n$. So my understanding is, there are two subsets $S$ and $T$ of $V$. In one of them, the coefficients are not all zero, in the other they are all zero. In one case they are linearly dependent, in the other not. I don't understand why though; that's as much as I understand. Not sure why the equations were constructed like this in the first place.",,"['linear-algebra', 'vector-spaces', 'intuition']"
3,How to solve this determinant equation in a simpler way,How to solve this determinant equation in a simpler way,,"Question Statement:- Solve the following equation   $$\begin{vmatrix} x & 2 & 3 \\  4 & x & 1 \\ x & 2 & 5 \\ \end{vmatrix}=0$$ My Solution:- $$\begin{vmatrix} x & 2 & 3 \\  4 & x & 1 \\ x & 2 & 5 \\ \end{vmatrix}= \begin{vmatrix} x+5 & 2 & 3 \\  x+5 & x & 1 \\ x+7 & 2 & 5 \\ \end{vmatrix} \tag{$C_1\rightarrow C_1+C_2+C_3$}$$ $$=\begin{vmatrix} 0 & 2 & 3 \\  0 & x & 1 \\ 2 & 2 & 5 \\ \end{vmatrix}+ (x+5)\begin{vmatrix} 1 & 2 & 3 \\  1 & x & 1 \\ 1 & 2 & 5 \\ \end{vmatrix}\tag{1}$$ On opening the first determinant in the last step above we get $2(2-3x)$. On simplifying the secind determinant we get, $$(x+5)\begin{vmatrix} 1 & 2 & 3 \\  1 & x & 1 \\ 1 & 2 & 5 \\ \end{vmatrix}=(x+5)\begin{vmatrix} 1 & 2 & 3 \\  0 & x-2 & -2 \\ 0 & 0 & 2 \\ \end{vmatrix}  (R_2\rightarrow R_2-R_1)  (R_3\rightarrow R_3-R_1)$$ $=2(x+5)(x-2)$ Substituting the values obtained above in $(1)$, we get $$=\begin{vmatrix} 0 & 2 & 3 \\  0 & x & 1 \\ 2 & 2 & 5 \\ \end{vmatrix}+ (x+5)\begin{vmatrix} 1 & 2 & 3 \\  1 & x & 1 \\ 1 & 2 & 5 \\ \end{vmatrix}=2(2-3x)+2(x+5)(x-2)=2(2-3x+x^2+3x-10)=2(x^2-8)$$ Now, as $\begin{vmatrix} x & 2 & 3 \\  4 & x & 1 \\ x & 2 & 5 \\ \end{vmatrix}=0$, $\therefore 2(x^2-8)=0\implies x=\pm2\sqrt2$ As you can see there was lot of work in my solution so if anyone can provide me with some techniques to solve it faster, or a technique which includes less amount of pen and more thinking.","Question Statement:- Solve the following equation   $$\begin{vmatrix} x & 2 & 3 \\  4 & x & 1 \\ x & 2 & 5 \\ \end{vmatrix}=0$$ My Solution:- $$\begin{vmatrix} x & 2 & 3 \\  4 & x & 1 \\ x & 2 & 5 \\ \end{vmatrix}= \begin{vmatrix} x+5 & 2 & 3 \\  x+5 & x & 1 \\ x+7 & 2 & 5 \\ \end{vmatrix} \tag{$C_1\rightarrow C_1+C_2+C_3$}$$ $$=\begin{vmatrix} 0 & 2 & 3 \\  0 & x & 1 \\ 2 & 2 & 5 \\ \end{vmatrix}+ (x+5)\begin{vmatrix} 1 & 2 & 3 \\  1 & x & 1 \\ 1 & 2 & 5 \\ \end{vmatrix}\tag{1}$$ On opening the first determinant in the last step above we get $2(2-3x)$. On simplifying the secind determinant we get, $$(x+5)\begin{vmatrix} 1 & 2 & 3 \\  1 & x & 1 \\ 1 & 2 & 5 \\ \end{vmatrix}=(x+5)\begin{vmatrix} 1 & 2 & 3 \\  0 & x-2 & -2 \\ 0 & 0 & 2 \\ \end{vmatrix}  (R_2\rightarrow R_2-R_1)  (R_3\rightarrow R_3-R_1)$$ $=2(x+5)(x-2)$ Substituting the values obtained above in $(1)$, we get $$=\begin{vmatrix} 0 & 2 & 3 \\  0 & x & 1 \\ 2 & 2 & 5 \\ \end{vmatrix}+ (x+5)\begin{vmatrix} 1 & 2 & 3 \\  1 & x & 1 \\ 1 & 2 & 5 \\ \end{vmatrix}=2(2-3x)+2(x+5)(x-2)=2(2-3x+x^2+3x-10)=2(x^2-8)$$ Now, as $\begin{vmatrix} x & 2 & 3 \\  4 & x & 1 \\ x & 2 & 5 \\ \end{vmatrix}=0$, $\therefore 2(x^2-8)=0\implies x=\pm2\sqrt2$ As you can see there was lot of work in my solution so if anyone can provide me with some techniques to solve it faster, or a technique which includes less amount of pen and more thinking.",,"['linear-algebra', 'algebra-precalculus', 'determinant']"
4,What is the Definition of Linear Algebra?,What is the Definition of Linear Algebra?,,"For introducing a field of science, usually we require a definition which summarizes the goal which is intended to be achieved by this field. Linear Algebra is one of the most important parts of mathematics which has not only interesting pure mathematical ideas but also a lot of applications in physics and engineering. So it seems reasonable to have a Complete , Clear , Brief , and Delicate definition for this important branch of mathematics. I am wondering that what is the best fit for such a definition. For example, the very first sentence that the Linear Algebra Done Right by Sheldon Axler starts with is Linear Algebra is the study of linear maps on finite dimensional vector spaces. What is your definition of Linear Algebra in few sentences?","For introducing a field of science, usually we require a definition which summarizes the goal which is intended to be achieved by this field. Linear Algebra is one of the most important parts of mathematics which has not only interesting pure mathematical ideas but also a lot of applications in physics and engineering. So it seems reasonable to have a Complete , Clear , Brief , and Delicate definition for this important branch of mathematics. I am wondering that what is the best fit for such a definition. For example, the very first sentence that the Linear Algebra Done Right by Sheldon Axler starts with is Linear Algebra is the study of linear maps on finite dimensional vector spaces. What is your definition of Linear Algebra in few sentences?",,"['linear-algebra', 'soft-question', 'definition']"
5,Why multiplication of matrix is not done in the same way as matrix addition (i.e. adding corresponding entries)? [duplicate],Why multiplication of matrix is not done in the same way as matrix addition (i.e. adding corresponding entries)? [duplicate],,"This question already has answers here : Fast(est) and intuitive ways to look at matrix multiplication? (7 answers) Intuition behind Matrix Multiplication (14 answers) Closed 6 years ago . Why multiplication of matrix is not done in the same way as matrix addition (i.e. adding corresponding entries)? I know it is related to linear transformation, but by reading book I'm unable to visualize it.","This question already has answers here : Fast(est) and intuitive ways to look at matrix multiplication? (7 answers) Intuition behind Matrix Multiplication (14 answers) Closed 6 years ago . Why multiplication of matrix is not done in the same way as matrix addition (i.e. adding corresponding entries)? I know it is related to linear transformation, but by reading book I'm unable to visualize it.",,"['linear-algebra', 'matrices']"
6,Proof for Cauchy-Schwarz inequality for Trace [closed],Proof for Cauchy-Schwarz inequality for Trace [closed],,"Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 5 years ago . Improve this question Cauchy-Schwarz inequality applied to Trace of two products $\mathbf{Tr}(A'B)$ has the form $$ \mathbf{Tr}(A'B) \leq \sqrt{\mathbf{Tr}(A'A)} \sqrt{\mathbf{Tr}(B'B)} $$ I saw many places where people use this inequality.  But did not see a formal proof.  Is it difficult to prove ?  Anyone can give a simple proof ?","Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 5 years ago . Improve this question Cauchy-Schwarz inequality applied to Trace of two products has the form I saw many places where people use this inequality.  But did not see a formal proof.  Is it difficult to prove ?  Anyone can give a simple proof ?","\mathbf{Tr}(A'B) 
\mathbf{Tr}(A'B) \leq \sqrt{\mathbf{Tr}(A'A)} \sqrt{\mathbf{Tr}(B'B)}
","['linear-algebra', 'trace', 'cauchy-schwarz-inequality']"
7,Find $A^{1000}$ by using Cayley-Hamilton Theorem,Find  by using Cayley-Hamilton Theorem,A^{1000},I get stuck at the following question: Consider the matrix $$A=\begin{bmatrix} 0 & 2 & 0 \\ 1 & 1 & -1 \\ -1 & 1 & 1\\ \end{bmatrix}$$ Find $A^{1000}$ by using the Cayley-Hamilton theorem. I find the characteristic polynomial by $P(A) = -A^{3} + 2A^2 = 0$ (by Cayley-Hamilton) but I don't see how to find $A^{1000}$ by this characteristic polynomial.,I get stuck at the following question: Consider the matrix $$A=\begin{bmatrix} 0 & 2 & 0 \\ 1 & 1 & -1 \\ -1 & 1 & 1\\ \end{bmatrix}$$ Find $A^{1000}$ by using the Cayley-Hamilton theorem. I find the characteristic polynomial by $P(A) = -A^{3} + 2A^2 = 0$ (by Cayley-Hamilton) but I don't see how to find $A^{1000}$ by this characteristic polynomial.,,"['linear-algebra', 'matrix-equations']"
8,Find the determinant of $I + A$ [closed],Find the determinant of  [closed],I + A,"Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 7 years ago . Improve this question What is the determinant of an $n \times n$ matrix $B=A+I$, where $I$ is the $n\times n$ identity matrix and $A$ is the $n\times n$ matrix  $$ A=\begin{pmatrix} a_1 & a_2 & \dots & a_n \\ a_1 & a_2 & \dots & a_n \\ \vdots & \vdots & \ddots & \vdots \\ a_1 & a_2 & \dots & a_n \\ \end{pmatrix} $$","Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 7 years ago . Improve this question What is the determinant of an $n \times n$ matrix $B=A+I$, where $I$ is the $n\times n$ identity matrix and $A$ is the $n\times n$ matrix  $$ A=\begin{pmatrix} a_1 & a_2 & \dots & a_n \\ a_1 & a_2 & \dots & a_n \\ \vdots & \vdots & \ddots & \vdots \\ a_1 & a_2 & \dots & a_n \\ \end{pmatrix} $$",,"['linear-algebra', 'matrices', 'determinant']"
9,Proving that $\cos(2\pi/n)$ is algebraic,Proving that  is algebraic,\cos(2\pi/n),I want to prove this without using any of the properties about the field of algebraic numbers (specifically that it is one). Essentially I just want to find a polynomial for which $\cos\frac{2\pi}{n}$ is a root. I know roots of unity and De Moivre's theorem is clearly going to be important here but I just can't see how to actually construct the polynomial from these facts.,I want to prove this without using any of the properties about the field of algebraic numbers (specifically that it is one). Essentially I just want to find a polynomial for which $\cos\frac{2\pi}{n}$ is a root. I know roots of unity and De Moivre's theorem is clearly going to be important here but I just can't see how to actually construct the polynomial from these facts.,,"['linear-algebra', 'abstract-algebra', 'algebraic-number-theory']"
10,Determinant of a particular matrix.,Determinant of a particular matrix.,,"What is the best way to find determinant of the following matrix? $$A=\left(\begin{matrix} 1&ax&a^2+x^2\\1&ay&a^2+y^2\\ 1&az&a^2+z^2 \end{matrix}\right)$$ I thought it looks like a Vandermonde matrix, but not exactly. I can't use $|A+B|=|A|+|B|$ to form a Vandermonde matrix. Please suggest. Thanks.","What is the best way to find determinant of the following matrix? I thought it looks like a Vandermonde matrix, but not exactly. I can't use to form a Vandermonde matrix. Please suggest. Thanks.","A=\left(\begin{matrix}
1&ax&a^2+x^2\\1&ay&a^2+y^2\\ 1&az&a^2+z^2
\end{matrix}\right) |A+B|=|A|+|B|","['linear-algebra', 'matrices', 'determinant']"
11,What is the relation between the eigenspace of a matrix and its column space?,What is the relation between the eigenspace of a matrix and its column space?,,"I was wondering if there is any relation between the space span by all eigenvector of a matrix $A$ and its column space. Also, is there any condition on $A$ that ensures that these two spaces are the same?","I was wondering if there is any relation between the space span by all eigenvector of a matrix and its column space. Also, is there any condition on that ensures that these two spaces are the same?",A A,"['linear-algebra', 'matrices', 'eigenvalues-eigenvectors']"
12,Linear algebra doubt about the use of the word 'finite',Linear algebra doubt about the use of the word 'finite',,Let $V$ be a vector space and $S$ be a non empty subset of $V$. If $v\in\operatorname{Span}(S)$ then it is a linear combination of a 'finite' number of vectors in the subset $S$. What does the word finite signify? Could have just said the linear combination of the vectors of $S$.,Let $V$ be a vector space and $S$ be a non empty subset of $V$. If $v\in\operatorname{Span}(S)$ then it is a linear combination of a 'finite' number of vectors in the subset $S$. What does the word finite signify? Could have just said the linear combination of the vectors of $S$.,,"['linear-algebra', 'terminology']"
13,Is Wolfram Alpha linear independence wrong or am I missing something?,Is Wolfram Alpha linear independence wrong or am I missing something?,,"Maybe it's because you can't ask those questions to wolfram & I should use a matrix instead but when imputting linear independence {$t$, $t^2+1$, $t^2+1-t$} It says the three functions are linearly independent when the third one is clearly a linear combination of the other two. How should I input this to get a valid answer? I wanna check whether or not my results are correct.","Maybe it's because you can't ask those questions to wolfram & I should use a matrix instead but when imputting linear independence {$t$, $t^2+1$, $t^2+1-t$} It says the three functions are linearly independent when the third one is clearly a linear combination of the other two. How should I input this to get a valid answer? I wanna check whether or not my results are correct.",,"['linear-algebra', 'functions', 'wolfram-alpha']"
14,Proving that $\mathbb{F}^\infty$ is infinite-dimensional.,Proving that  is infinite-dimensional.,\mathbb{F}^\infty,"I'm supposed to prove that $\mathbb{F}^\infty$ is infinite-dimensional. I was planning on doing a proof by induction to show that $(1,0,...),(0,1,0,...),...$ is a basis. Is this permissible? Also, I think I could do a proof by contradiction and suppose $\mathbb{F}^\infty$ to be finite-dimensional, and thus have a finite-length basis, and then show that there exists some $v\in\mathbb{F}^\infty$ such that $v$ is not in the span of the basis. I'm not quite sure how to show that rigorously though.","I'm supposed to prove that $\mathbb{F}^\infty$ is infinite-dimensional. I was planning on doing a proof by induction to show that $(1,0,...),(0,1,0,...),...$ is a basis. Is this permissible? Also, I think I could do a proof by contradiction and suppose $\mathbb{F}^\infty$ to be finite-dimensional, and thus have a finite-length basis, and then show that there exists some $v\in\mathbb{F}^\infty$ such that $v$ is not in the span of the basis. I'm not quite sure how to show that rigorously though.",,"['linear-algebra', 'vector-spaces']"
15,"Why Aren't ""Similar"" Matrices Actually the Same?","Why Aren't ""Similar"" Matrices Actually the Same?",,"In linear algebra, a matrix $B$ is said to be ""similar"" to $A$ if $B=C^{-1}AC$, that is $B$ = a matrix $A$ multiplied by a third matrix $C$, and its inverse, $C^{-1}$. In regular algebra, if I take a number $x$, and multiply it by $\frac{1}{2}$ and then $2$, the latter terms cancel out, and I get $x$, the same and not a ""similar"" variable. Wouldn't you also have this result in linear algebra? What am I missing?","In linear algebra, a matrix $B$ is said to be ""similar"" to $A$ if $B=C^{-1}AC$, that is $B$ = a matrix $A$ multiplied by a third matrix $C$, and its inverse, $C^{-1}$. In regular algebra, if I take a number $x$, and multiply it by $\frac{1}{2}$ and then $2$, the latter terms cancel out, and I get $x$, the same and not a ""similar"" variable. Wouldn't you also have this result in linear algebra? What am I missing?",,['linear-algebra']
16,How to calculate the rank of a matrix?,How to calculate the rank of a matrix?,,"I need to calculate the rank of the matrix $A$, shown below:   $$  A= \begin{bmatrix} 3 & 2 & -1\\ 2 & -3 & -5\\ -1 & -4 &- 3 \end{bmatrix} $$ I know that I need to calculate $\det(A)$ and if $\det(A) \neq 0$ then the rank will be equal to $3$, but in this case I'm required to zero-out first column of matrix $A$ using element $a_{31} = -1$.","I need to calculate the rank of the matrix $A$, shown below:   $$  A= \begin{bmatrix} 3 & 2 & -1\\ 2 & -3 & -5\\ -1 & -4 &- 3 \end{bmatrix} $$ I know that I need to calculate $\det(A)$ and if $\det(A) \neq 0$ then the rank will be equal to $3$, but in this case I'm required to zero-out first column of matrix $A$ using element $a_{31} = -1$.",,"['linear-algebra', 'matrices', 'matrix-rank']"
17,How to prove that homomorphism from field to ring is injective or zero? [duplicate],How to prove that homomorphism from field to ring is injective or zero? [duplicate],,This question already has answers here : Closed 11 years ago . Possible Duplicate: Abstract Algebra ring homomorphism $f:K\rightarrow R$ is a homomorphism from field to ring. To prove is that it is injective or zero.,This question already has answers here : Closed 11 years ago . Possible Duplicate: Abstract Algebra ring homomorphism $f:K\rightarrow R$ is a homomorphism from field to ring. To prove is that it is injective or zero.,,"['linear-algebra', 'abstract-algebra', 'group-theory']"
18,Let $A^2 = I$. Prove that $A = I$ if all eigenvalues of $A$ are $1$,Let . Prove that  if all eigenvalues of  are,A^2 = I A = I A 1,"Need help with this: Let $A^2 = I$, and all eigenvalues of $A$ are $1$. Prove that $A = I$. ($A$ is over the complexes) I thought that because $A^2=I$, then $A$ is reversible and $A^{-1} = A$, and there are only two matrices that do this: the identity matrix and the zero matrix. But it's only intuition and I couldn't prove that.","Need help with this: Let $A^2 = I$, and all eigenvalues of $A$ are $1$. Prove that $A = I$. ($A$ is over the complexes) I thought that because $A^2=I$, then $A$ is reversible and $A^{-1} = A$, and there are only two matrices that do this: the identity matrix and the zero matrix. But it's only intuition and I couldn't prove that.",,"['linear-algebra', 'matrices']"
19,Are all bilinear forms identically zero?,Are all bilinear forms identically zero?,,"Let $V$ be a finite-dimensional vector space and $\beta$ be a bilinear form. According to the definition of the bilinear forms is possible to do the following: $$\begin{align}\beta(u,v)&=\beta(u+w-w,v)=\beta(u+w,v)-\beta(w,v)\\ &=\beta(u+w,v)+\beta(w,-v)=\beta(u+2w,v-v)\\ &= \beta(u+2w,0)=0\end{align}$$ I find it curious, I think there's some mistake but I can not identify it, I ask for your help in this matter.","Let $V$ be a finite-dimensional vector space and $\beta$ be a bilinear form. According to the definition of the bilinear forms is possible to do the following: $$\begin{align}\beta(u,v)&=\beta(u+w-w,v)=\beta(u+w,v)-\beta(w,v)\\ &=\beta(u+w,v)+\beta(w,-v)=\beta(u+2w,v-v)\\ &= \beta(u+2w,0)=0\end{align}$$ I find it curious, I think there's some mistake but I can not identify it, I ask for your help in this matter.",,"['linear-algebra', 'multilinear-algebra', 'bilinear-form']"
20,Prove:$A B$ and $B A$ has the same characteristic polynomial. [duplicate],Prove: and  has the same characteristic polynomial. [duplicate],A B B A,"This question already has answers here : Do matrices $ AB $ and $ BA $ have the same minimal and characteristic polynomials? (13 answers) Closed 10 years ago . Suppose $A B$ are all $n\times n$ matrix. Prove:$A B$ and $B A$ has the same characteristic polynomial. If A is invertible \begin{align}P(A B)=|\lambda  E-A B|=\left|\lambda  A\cdot A^{-1}-A B\right|=\left|A\left(\lambda  A^{-1}-B\right)\right|=\left|A\left\|\text{$\lambda $A}^{-1}-B\right.\right|\end{align} \begin{align}P(B A)=|\lambda  E-B A|=\left|\lambda  A^{-1}\cdot A-B A\right|=\left|\left(\lambda  A^{-1}-B\right)A\right|=\left|\left.\text{$\lambda $A}^{-1}-B\right\|A\right|\end{align} Is it true? If A is not invertible How to prove? Edit: I found a proof by Block Matrix Multiplication, it's more straightforward, is it right? \begin{align}\left( \begin{array}{cc}  E & 0 \\  -A & E \\ \end{array} \right).\left( \begin{array}{cc}  \lambda  E & B \\  \lambda  A & \lambda  E \\ \end{array} \right)=\left( \begin{array}{cc}  \lambda  E & B \\  0 & \lambda  E-A B \\ \end{array} \right)\end{align} \begin{align}\left( \begin{array}{cc}  \lambda  E & B \\  \lambda  A & \lambda  E \\ \end{array} \right).\left( \begin{array}{cc}  E & 0 \\  -A & E \\ \end{array} \right)=\left( \begin{array}{cc}  \lambda  E-B A & B \\  0 & \lambda  E \\ \end{array} \right)\end{align} Another proof is by Sylvester determinant theorem which uses LU block decomposition.","This question already has answers here : Do matrices $ AB $ and $ BA $ have the same minimal and characteristic polynomials? (13 answers) Closed 10 years ago . Suppose $A B$ are all $n\times n$ matrix. Prove:$A B$ and $B A$ has the same characteristic polynomial. If A is invertible \begin{align}P(A B)=|\lambda  E-A B|=\left|\lambda  A\cdot A^{-1}-A B\right|=\left|A\left(\lambda  A^{-1}-B\right)\right|=\left|A\left\|\text{$\lambda $A}^{-1}-B\right.\right|\end{align} \begin{align}P(B A)=|\lambda  E-B A|=\left|\lambda  A^{-1}\cdot A-B A\right|=\left|\left(\lambda  A^{-1}-B\right)A\right|=\left|\left.\text{$\lambda $A}^{-1}-B\right\|A\right|\end{align} Is it true? If A is not invertible How to prove? Edit: I found a proof by Block Matrix Multiplication, it's more straightforward, is it right? \begin{align}\left( \begin{array}{cc}  E & 0 \\  -A & E \\ \end{array} \right).\left( \begin{array}{cc}  \lambda  E & B \\  \lambda  A & \lambda  E \\ \end{array} \right)=\left( \begin{array}{cc}  \lambda  E & B \\  0 & \lambda  E-A B \\ \end{array} \right)\end{align} \begin{align}\left( \begin{array}{cc}  \lambda  E & B \\  \lambda  A & \lambda  E \\ \end{array} \right).\left( \begin{array}{cc}  E & 0 \\  -A & E \\ \end{array} \right)=\left( \begin{array}{cc}  \lambda  E-B A & B \\  0 & \lambda  E \\ \end{array} \right)\end{align} Another proof is by Sylvester determinant theorem which uses LU block decomposition.",,"['linear-algebra', 'matrices', 'solution-verification']"
21,Derivative of spectral norm of symmetric matrix,Derivative of spectral norm of symmetric matrix,,I want to calculate the derivative of the spectral norm of a symmetric square matrix $W$: $$ \frac{\partial}{\partial w_{ij}} \|W\|_2  $$ How should I go about this?,I want to calculate the derivative of the spectral norm of a symmetric square matrix $W$: $$ \frac{\partial}{\partial w_{ij}} \|W\|_2  $$ How should I go about this?,,"['linear-algebra', 'matrices', 'derivatives', 'matrix-calculus', 'spectral-norm']"
22,Why can't you divide matrices?,Why can't you divide matrices?,,"I was just wondering that because one can multiply and add and subtract matrices, why can't    one divide them?","I was just wondering that because one can multiply and add and subtract matrices, why can't    one divide them?",,"['linear-algebra', 'abstract-algebra', 'matrices']"
23,Computing the determinant of a large matrix?,Computing the determinant of a large matrix?,,"How would I go about computing the determinant of large matrices, such as $6 \times 6$. I believe that I need to use multilinear maps, but I am not sure how I can go about computing the determinant in a nice and efficient way. Can anyone show me how I can determine the determinant of the matrix below in a simple and efficient way? \begin{pmatrix} 0 &  0&  1&  1& 1 & 1\\  1 & 0 & 0 &  0&  0& 1\\  1 & 0 &  1& 1 & 1 &1 \\  0 & 1 & 1 & 1 & 0 &1 \\  0 & 1 &  0& 1 &  0& 0\\  0 &  0&  1& 0 &  0& 0 \end{pmatrix}","How would I go about computing the determinant of large matrices, such as $6 \times 6$. I believe that I need to use multilinear maps, but I am not sure how I can go about computing the determinant in a nice and efficient way. Can anyone show me how I can determine the determinant of the matrix below in a simple and efficient way? \begin{pmatrix} 0 &  0&  1&  1& 1 & 1\\  1 & 0 & 0 &  0&  0& 1\\  1 & 0 &  1& 1 & 1 &1 \\  0 & 1 & 1 & 1 & 0 &1 \\  0 & 1 &  0& 1 &  0& 0\\  0 &  0&  1& 0 &  0& 0 \end{pmatrix}",,"['linear-algebra', 'matrices', 'determinant']"
24,What are some usual norms for matrices?,What are some usual norms for matrices?,,"I am familiar with norms on vectors and functions, but do there exist norms for spaces of matrices i.e. $A$ some $n \times m$ matrix? If so, that would that imply matrices also form some sort of vector space?","I am familiar with norms on vectors and functions, but do there exist norms for spaces of matrices i.e. $A$ some $n \times m$ matrix? If so, that would that imply matrices also form some sort of vector space?",,"['linear-algebra', 'matrices', 'vector-spaces', 'normed-spaces', 'matrix-norms']"
25,Is sum of two orthogonal matrices singular?,Is sum of two orthogonal matrices singular?,,"I am trying to solve following problem. Let $A, B \in \mathbb{R}^{n\times n}$ be an orthogonal matrices and $\det(A) = -\det(B)$ . How can it be proven that $A+B$ is singular? I could start with implication: $\det(A)=-\det(B) \Rightarrow B$ is created from $A$ by swapping two lines or columns. But I am not sure if this implication is correct.",I am trying to solve following problem. Let be an orthogonal matrices and . How can it be proven that is singular? I could start with implication: is created from by swapping two lines or columns. But I am not sure if this implication is correct.,"A, B \in \mathbb{R}^{n\times n} \det(A) = -\det(B) A+B \det(A)=-\det(B) \Rightarrow B A","['linear-algebra', 'matrices', 'determinant', 'orthogonal-matrices']"
26,Show linear independence of a set of vectors close to an orthonormal basis,Show linear independence of a set of vectors close to an orthonormal basis,,"This is a question from Linear Algebra Done Right . Suppose $e_1, \dots, e_n$ is an orthonormal basis of $V$ and $v_1, \dots, v_n$ are vectors in $V$ such that $$ \| e_j - v_j \| < \frac{1} {\sqrt{n}} $$ for each $j$. Prove that $v_1, \dots, v_n$ is a basis of $V$. I tried to show this if $V$ is a two dimensional space by contradiction. However, it seems too messy to use the same idea for a general case. Does anyone have hints for this problem?","This is a question from Linear Algebra Done Right . Suppose $e_1, \dots, e_n$ is an orthonormal basis of $V$ and $v_1, \dots, v_n$ are vectors in $V$ such that $$ \| e_j - v_j \| < \frac{1} {\sqrt{n}} $$ for each $j$. Prove that $v_1, \dots, v_n$ is a basis of $V$. I tried to show this if $V$ is a two dimensional space by contradiction. However, it seems too messy to use the same idea for a general case. Does anyone have hints for this problem?",,"['linear-algebra', 'hilbert-spaces']"
27,Why is a singular matrix rare?,Why is a singular matrix rare?,,"I am exploring patterns of integers in $n\times n$ matrices.  I have two matrices that have a determinant of $0$ and a circulant matrix that has positive determinants that differ depending on $n$ . I snipped this from Wikipedia and bolded the important part: A square matrix that is not invertible is called singular or degenerate. A square matrix is singular if and only if its determinant is 0. Singular matrices are rare in the sense that if you pick a random square matrix over a continuous uniform distribution on its entries, it will almost surely not be singular. The Good: $$\left( \begin{array}{ccccc}  1 & 2 & 3 & 4 & 5 \\  1 & 2 & 3 & 4 & 5 \\  1 & 2 & 3 & 4 & 5 \\  1 & 2 & 3 & 4 & 5 \\  1 & 2 & 3 & 4 & 5 \\ \end{array} \right)$$ The Bad: $$\left( \begin{array}{ccccc}  1 & 2 & 3 & 4 & 5 \\  5 & 1 & 2 & 3 & 4 \\  4 & 5 & 1 & 2 & 3 \\  3 & 4 & 5 & 1 & 2 \\  2 & 3 & 4 & 5 & 1 \\ \end{array} \right)$$ And the Ugly: $$\left( \begin{array}{ccccc}  11 & 12 & 13 & 14 & 15 \\  16 & 17 & 18 & 19 & 20 \\  21 & 22 & 23 & 24 & 25 \\  26 & 27 & 28 & 29 & 30 \\  31 & 32 & 33 & 34 & 35 \\ \end{array} \right)$$ The Good is same as Ugly mod n and both are singular. The Bad is the circulant Good and has determinant $>0$ . Two questions: What makes a singular matrix rare? Has anyone documented the differences? (preferably, using $n$ or $n^2$ )","I am exploring patterns of integers in matrices.  I have two matrices that have a determinant of and a circulant matrix that has positive determinants that differ depending on . I snipped this from Wikipedia and bolded the important part: A square matrix that is not invertible is called singular or degenerate. A square matrix is singular if and only if its determinant is 0. Singular matrices are rare in the sense that if you pick a random square matrix over a continuous uniform distribution on its entries, it will almost surely not be singular. The Good: The Bad: And the Ugly: The Good is same as Ugly mod n and both are singular. The Bad is the circulant Good and has determinant . Two questions: What makes a singular matrix rare? Has anyone documented the differences? (preferably, using or )","n\times n 0 n \left(
\begin{array}{ccccc}
 1 & 2 & 3 & 4 & 5 \\
 1 & 2 & 3 & 4 & 5 \\
 1 & 2 & 3 & 4 & 5 \\
 1 & 2 & 3 & 4 & 5 \\
 1 & 2 & 3 & 4 & 5 \\
\end{array}
\right) \left(
\begin{array}{ccccc}
 1 & 2 & 3 & 4 & 5 \\
 5 & 1 & 2 & 3 & 4 \\
 4 & 5 & 1 & 2 & 3 \\
 3 & 4 & 5 & 1 & 2 \\
 2 & 3 & 4 & 5 & 1 \\
\end{array}
\right) \left(
\begin{array}{ccccc}
 11 & 12 & 13 & 14 & 15 \\
 16 & 17 & 18 & 19 & 20 \\
 21 & 22 & 23 & 24 & 25 \\
 26 & 27 & 28 & 29 & 30 \\
 31 & 32 & 33 & 34 & 35 \\
\end{array}
\right) >0 n n^2","['linear-algebra', 'matrices', 'inverse', 'intuition']"
28,Definition of an affine subspace,Definition of an affine subspace,,"I am reading this introduction to Mechanics and the definition it gives (just after Proposition 1.1.2) for an affine subspace puzzles me. I cite: A subset $B$ of a $\mathbb{R}$-affine space $A$ modelled on $V$ is an affine subspace if there is a subspace $U$ of $V$ with the property that $y−x \in U$ for every $x,y \in B$ It later says that this definition is equivalent to to the usual one, namely that of closeness under sum with elements of a $U$, but it seems to me that there is a problem with the first definition. Just imagine the usual $\mathbb{R}^2$ plane as an affine space modeled on $\mathbb{R}^2$. According to this definition the subset $\{(0,0);(0,1)\}$ is an affine subspace, while this is not so according to the usual definition of an affine subspace. Is there an error in the book?","I am reading this introduction to Mechanics and the definition it gives (just after Proposition 1.1.2) for an affine subspace puzzles me. I cite: A subset $B$ of a $\mathbb{R}$-affine space $A$ modelled on $V$ is an affine subspace if there is a subspace $U$ of $V$ with the property that $y−x \in U$ for every $x,y \in B$ It later says that this definition is equivalent to to the usual one, namely that of closeness under sum with elements of a $U$, but it seems to me that there is a problem with the first definition. Just imagine the usual $\mathbb{R}^2$ plane as an affine space modeled on $\mathbb{R}^2$. According to this definition the subset $\{(0,0);(0,1)\}$ is an affine subspace, while this is not so according to the usual definition of an affine subspace. Is there an error in the book?",,"['linear-algebra', 'affine-geometry']"
29,Geometry: How to determine if two lines are parallel in 3D based on coordinates of 2 points on each line?,Geometry: How to determine if two lines are parallel in 3D based on coordinates of 2 points on each line?,,"I am a Belgian engineer working on software in C# to provide smart bending solutions to a manufacturer of press brakes. In this context I am searching for the best way to determine if two lines are parallel, based on the following information: Each line has two points of which the coordinates are known These coordinates are relative to the same frame So to be clear, we have four points: A (ax, ay, az), B (bx,by,bz), C (cx,cy,cz) and D (dx,dy,dz) Which is the best way to be able to return a simple boolean that says if these two lines are parallel or not? Can someone please help me out? Edit after reading answers Below is my C#-code, where I use two home-made objects, CS3DLine and CSVector, but the meaning of the objects speaks for itself. A toleratedPercentageDifference is used as well. public static bool AreParallelLinesIn3D(CS3DLine left, CS3DLine right)     {         double toleratedPercentageDifference = 1;         CSVector vLeft = new CSVector(left.p1, left.p2);         CSVector vRight = new CSVector(right.p1, right.p2);         double ricoX = vLeft.X / vRight.X ;         double ricoY = vLeft.Y / vRight.Y ;         double ricoZ = vLeft.Z / vRight.Z ;         if (Math.Abs(ricoX - ricoY) > Math.Abs(toleratedPercentageDifference * ricoX / 100)) return false;         if (Math.Abs(ricoX - ricoZ) > Math.Abs(toleratedPercentageDifference * ricoX / 100)) return false;         return true;     }","I am a Belgian engineer working on software in C# to provide smart bending solutions to a manufacturer of press brakes. In this context I am searching for the best way to determine if two lines are parallel, based on the following information: Each line has two points of which the coordinates are known These coordinates are relative to the same frame So to be clear, we have four points: A (ax, ay, az), B (bx,by,bz), C (cx,cy,cz) and D (dx,dy,dz) Which is the best way to be able to return a simple boolean that says if these two lines are parallel or not? Can someone please help me out? Edit after reading answers Below is my C#-code, where I use two home-made objects, CS3DLine and CSVector, but the meaning of the objects speaks for itself. A toleratedPercentageDifference is used as well. public static bool AreParallelLinesIn3D(CS3DLine left, CS3DLine right)     {         double toleratedPercentageDifference = 1;         CSVector vLeft = new CSVector(left.p1, left.p2);         CSVector vRight = new CSVector(right.p1, right.p2);         double ricoX = vLeft.X / vRight.X ;         double ricoY = vLeft.Y / vRight.Y ;         double ricoZ = vLeft.Z / vRight.Z ;         if (Math.Abs(ricoX - ricoY) > Math.Abs(toleratedPercentageDifference * ricoX / 100)) return false;         if (Math.Abs(ricoX - ricoZ) > Math.Abs(toleratedPercentageDifference * ricoX / 100)) return false;         return true;     }",,['linear-algebra']
30,English names for vector beginning and end,English names for vector beginning and end,,"I've done some research, but since English is not my native language, I'm struggling to find an answer to this: Given a vector, what do you call its beginning and end points? The best I've found so far is the word ""base"" for the beginning point of a vector, but I have no clue if that's correct. The best I've got for the end point is the ""tip"" of the vector, although the same applies as before.","I've done some research, but since English is not my native language, I'm struggling to find an answer to this: Given a vector, what do you call its beginning and end points? The best I've found so far is the word ""base"" for the beginning point of a vector, but I have no clue if that's correct. The best I've got for the end point is the ""tip"" of the vector, although the same applies as before.",,"['linear-algebra', 'vectors', 'terminology']"
31,Show that two matrices with the same eigenvalues are similar,Show that two matrices with the same eigenvalues are similar,,"First assume that $A$ and $B$ are $p \times p$ matrices and that $\lambda_1,\ldots , \lambda_p$ are distinct eigenvalues of $A$ and $B$. I want to show that $A$ and $B$ are similar. Here is my approach:  The goal is to show that there is a nonsingular $p \times p$ matrix  $P$ such that $B = P^{-1}A P$. We know the following is satisfied for each eigenvalue $$ A \textbf x_j = \lambda_j \textbf x _j  \text { where } j = 1,\ldots,p$$ and $\textbf x_j$ are the eigenvectors. Since $A$ and $B$ share the same eigenvectors, $B$ must satisfy the following  $$ B \textbf x_j = \lambda_j \textbf x _j  \text { where } j = 1,\ldots,p$$ Thus we have $$ B \textbf x_j = A \textbf x_j$$ We can take the $\textbf x_j$ as the columns of the matrix $P$ and get $$BP=AP$$ we know that the columns of $P $ are linearly independent so the matrix must be non singular. Unfortunately this means that  $$A=B$$ but this is not what we wanted to show. Am I on the right track? Is there an obvious mistake I am making?","First assume that $A$ and $B$ are $p \times p$ matrices and that $\lambda_1,\ldots , \lambda_p$ are distinct eigenvalues of $A$ and $B$. I want to show that $A$ and $B$ are similar. Here is my approach:  The goal is to show that there is a nonsingular $p \times p$ matrix  $P$ such that $B = P^{-1}A P$. We know the following is satisfied for each eigenvalue $$ A \textbf x_j = \lambda_j \textbf x _j  \text { where } j = 1,\ldots,p$$ and $\textbf x_j$ are the eigenvectors. Since $A$ and $B$ share the same eigenvectors, $B$ must satisfy the following  $$ B \textbf x_j = \lambda_j \textbf x _j  \text { where } j = 1,\ldots,p$$ Thus we have $$ B \textbf x_j = A \textbf x_j$$ We can take the $\textbf x_j$ as the columns of the matrix $P$ and get $$BP=AP$$ we know that the columns of $P $ are linearly independent so the matrix must be non singular. Unfortunately this means that  $$A=B$$ but this is not what we wanted to show. Am I on the right track? Is there an obvious mistake I am making?",,"['linear-algebra', 'proof-verification']"
32,What is an additive group?,What is an additive group?,,"Is an additive group a group which only has an addition operation, or can it also have other operations on it? Thanks","Is an additive group a group which only has an addition operation, or can it also have other operations on it? Thanks",,"['linear-algebra', 'group-theory']"
33,Does the definition of the linear span of a subset of a vector space require that the set be countable?,Does the definition of the linear span of a subset of a vector space require that the set be countable?,,"My book gives this definition of linear span of a subset $S$ of a vector space $V$ : Now if $S$ is any subset of $V$, we let $L(S)$ be the set of all finite linear combinations of elements of $S$. Thus   $$L(S) = \left\{\sum_{i=1}^k{\alpha_i v_i \mid k \in \mathbb{N}, v_i \in S, \alpha_i \in \mathbb{R}}\right\}.$$ But when we write $k \in \mathbb{N}$, aren't we implying that $S$ is at most countable? That is, $S=\{v_i:i=1(1)k,k \in \mathbb{N}\}$. But $S$ may very well be an uncountable subset.","My book gives this definition of linear span of a subset $S$ of a vector space $V$ : Now if $S$ is any subset of $V$, we let $L(S)$ be the set of all finite linear combinations of elements of $S$. Thus   $$L(S) = \left\{\sum_{i=1}^k{\alpha_i v_i \mid k \in \mathbb{N}, v_i \in S, \alpha_i \in \mathbb{R}}\right\}.$$ But when we write $k \in \mathbb{N}$, aren't we implying that $S$ is at most countable? That is, $S=\{v_i:i=1(1)k,k \in \mathbb{N}\}$. But $S$ may very well be an uncountable subset.",,"['linear-algebra', 'vector-spaces']"
34,Is a positive semidefinite matrix always non-negative?,Is a positive semidefinite matrix always non-negative?,,"I'm trying to get some intuition behind the meaning of a positive semidefinite matrix, which I learned a long time ago in undergrad but clearly didn't internalize properly. As I understand, a symmetric matrix $M \in \textbf{R}^{n~\times~n}$ is positive semidefinite iff $z^TMz \ge 0$, $\forall z \in \textbf{R}^n$. Note that I'd like to use this particular definition, not a more general one that involves complex numbers. As such, $z^TMz \in \textbf{R}$. This definition makes sense to me, and this question clarified it further, but then I was reading Boyd's textbook and became confused by an unrelated definition explained in $\S$3.1.4, which implies that the Hessian matrix $\textbf{H}$ of function $f$ is positive semidefinite if $\textbf{H} \succcurlyeq 0$, where the $""\succcurlyeq""$ symbol refers to a componentwise inequality between matrices. Thus, can a positive semidefinite matrix contain negative entries? EDIT: This question turned out to be silly, but if you have this question and am as rusty with linear algebra as I am, this post might be useful.","I'm trying to get some intuition behind the meaning of a positive semidefinite matrix, which I learned a long time ago in undergrad but clearly didn't internalize properly. As I understand, a symmetric matrix $M \in \textbf{R}^{n~\times~n}$ is positive semidefinite iff $z^TMz \ge 0$, $\forall z \in \textbf{R}^n$. Note that I'd like to use this particular definition, not a more general one that involves complex numbers. As such, $z^TMz \in \textbf{R}$. This definition makes sense to me, and this question clarified it further, but then I was reading Boyd's textbook and became confused by an unrelated definition explained in $\S$3.1.4, which implies that the Hessian matrix $\textbf{H}$ of function $f$ is positive semidefinite if $\textbf{H} \succcurlyeq 0$, where the $""\succcurlyeq""$ symbol refers to a componentwise inequality between matrices. Thus, can a positive semidefinite matrix contain negative entries? EDIT: This question turned out to be silly, but if you have this question and am as rusty with linear algebra as I am, this post might be useful.",,['linear-algebra']
35,what would the value of determinant of a matrix be if a specific entry changed?,what would the value of determinant of a matrix be if a specific entry changed?,,"What will the value of determinant of matrix $A=\pmatrix{1&3&4\\5&2&a\\6&-2&3}$ be change if we change $a$ to $a+2$. This is an easy problem because $|A|=-127+20a$ and if we did that changing, we would get $-87+20a$. My question here is: Can we do this request by doing another way? Where does that $+40$ come from regarding the whole entries of the matrix? Thanks","What will the value of determinant of matrix $A=\pmatrix{1&3&4\\5&2&a\\6&-2&3}$ be change if we change $a$ to $a+2$. This is an easy problem because $|A|=-127+20a$ and if we did that changing, we would get $-87+20a$. My question here is: Can we do this request by doing another way? Where does that $+40$ come from regarding the whole entries of the matrix? Thanks",,['linear-algebra']
36,Decomposition of a nonsquare affine matrix,Decomposition of a nonsquare affine matrix,,"I have a $2\times 3$ affine matrix  $$ M = \pmatrix{a &b &c\\ d &e &f}  $$ which transforms a point $(x,y)$ into $x' = a x + by + c, y' = d x + e y + f$ Is there a way to decompose such matrix into shear, rotation, translation,and scale ? I know there's something for $4\times 4$ matrixes, but for a $2\times 3$ matrix ?","I have a $2\times 3$ affine matrix  $$ M = \pmatrix{a &b &c\\ d &e &f}  $$ which transforms a point $(x,y)$ into $x' = a x + by + c, y' = d x + e y + f$ Is there a way to decompose such matrix into shear, rotation, translation,and scale ? I know there's something for $4\times 4$ matrixes, but for a $2\times 3$ matrix ?",,"['linear-algebra', 'geometry', 'matrices', 'affine-geometry']"
37,A vector space over an infinite field is not a finite union of proper subspaces? [duplicate],A vector space over an infinite field is not a finite union of proper subspaces? [duplicate],,"This question already has answers here : If a field $F$ is such that $\left|F\right|>n-1$ why is $V$ a vector space over $F$ not equal to the union of $n$ proper subspaces of $V$ (3 answers) Closed 7 years ago . Show that if $V$ is a vector space over an infinite field $\mathbb{F}$, then $V$ cannot be written as set-theoretic union of a finite number of proper subspaces.","This question already has answers here : If a field $F$ is such that $\left|F\right|>n-1$ why is $V$ a vector space over $F$ not equal to the union of $n$ proper subspaces of $V$ (3 answers) Closed 7 years ago . Show that if $V$ is a vector space over an infinite field $\mathbb{F}$, then $V$ cannot be written as set-theoretic union of a finite number of proper subspaces.",,"['linear-algebra', 'vector-spaces']"
38,Powers of adjacency matrix doesn't seem to correspond to observed number of paths on graph,Powers of adjacency matrix doesn't seem to correspond to observed number of paths on graph,,"I would really appreciate some help on this! $A^n$ represents $n^{th}$ power of the adjacency matrix of a graph. I keep reading that the $A^n_{ij}$ entry equals ""the number of paths of length n joining nodes i and j."" But I tried the following simple example and it doesn't work out. Am I misunderstanding the definition? E.g. $A^3_{12}=5$ but how is it there are 5 paths of length 3 joining nodes 1 and 2?? A=[0 1 1 1; 1 0 0 1; 1 0 0 1; 1 1 1 0] $A^3$ =[4 5 5 5; 5 2 2 5; 5 2 2 5; 5 5 5 4]","I would really appreciate some help on this! represents power of the adjacency matrix of a graph. I keep reading that the entry equals ""the number of paths of length n joining nodes i and j."" But I tried the following simple example and it doesn't work out. Am I misunderstanding the definition? E.g. but how is it there are 5 paths of length 3 joining nodes 1 and 2?? A=[0 1 1 1; 1 0 0 1; 1 0 0 1; 1 1 1 0] =[4 5 5 5; 5 2 2 5; 5 2 2 5; 5 5 5 4]",A^n n^{th} A^n_{ij} A^3_{12}=5 A^3,"['linear-algebra', 'combinatorics', 'matrices', 'graph-theory', 'network']"
39,Origin of the modern definition of the tensor product,Origin of the modern definition of the tensor product,,"Due to whom is the modern (i.e. via its universal property) definition of the tensor product, and in which article was it communicated?","Due to whom is the modern (i.e. via its universal property) definition of the tensor product, and in which article was it communicated?",,"['linear-algebra', 'abstract-algebra']"
40,Does this $3\times 3$ matrix exist? [duplicate],Does this  matrix exist? [duplicate],3\times 3,This question already has answers here : Existence of some type matrix (3 answers) Closed 4 years ago . Does a real $3\times 3$ matrix $A$ that satisfies conditions $\operatorname{tr}(A)=0$ and $A^2+A^T=I$ ($I$ is an identity matrix) exist? Thank you for your help.,This question already has answers here : Existence of some type matrix (3 answers) Closed 4 years ago . Does a real $3\times 3$ matrix $A$ that satisfies conditions $\operatorname{tr}(A)=0$ and $A^2+A^T=I$ ($I$ is an identity matrix) exist? Thank you for your help.,,"['linear-algebra', 'matrices', 'matrix-equations']"
41,Rank of matrix $AB$ when $A$ and $B$ have full rank,Rank of matrix  when  and  have full rank,AB A B,"Let $A$ be a $m \times n$ matrix with rank $n$ , and let $B$ be a $n \times p$ matrix with rank $p$ . Calculate the rank of matrix $C=AB$ . The rank of a matrix is the number of linearly independent rows.","Let be a matrix with rank , and let be a matrix with rank . Calculate the rank of matrix . The rank of a matrix is the number of linearly independent rows.",A m \times n n B n \times p p C=AB,"['linear-algebra', 'matrices', 'transformation', 'matrix-rank']"
42,What are some lesser-known examples where increasing the dimensionality makes the problem easier to solve? [duplicate],What are some lesser-known examples where increasing the dimensionality makes the problem easier to solve? [duplicate],,"This question already has answers here : What problems are easier to solve in a higher dimension, i.e. 3D vs 2D? (3 answers) Closed 3 years ago . I feel like there is a common pattern in mathematics where increasing the dimensionality makes the problem easier to solve or provides a solution where otherwise one would not exist. Some examples: Going from real numbers to complex numbers Working with quaternions (4D) instead of (roll, pitch, yaw) or rotation matrices which have singularities Laplace transform which lets us solve differential equations with algebra (ok this isn't really increasing the dimensionality, more like working in a different dimension) I'm not a mathematician but I imagine this appears in other areas. What else is there? Is there a ""method"" or systematic way for increasing the dimensionality of a problem to make it easier to solve? Bonus if there is something in the field of optimization and/or linear algebra, which I am most interested in at the moment.","This question already has answers here : What problems are easier to solve in a higher dimension, i.e. 3D vs 2D? (3 answers) Closed 3 years ago . I feel like there is a common pattern in mathematics where increasing the dimensionality makes the problem easier to solve or provides a solution where otherwise one would not exist. Some examples: Going from real numbers to complex numbers Working with quaternions (4D) instead of (roll, pitch, yaw) or rotation matrices which have singularities Laplace transform which lets us solve differential equations with algebra (ok this isn't really increasing the dimensionality, more like working in a different dimension) I'm not a mathematician but I imagine this appears in other areas. What else is there? Is there a ""method"" or systematic way for increasing the dimensionality of a problem to make it easier to solve? Bonus if there is something in the field of optimization and/or linear algebra, which I am most interested in at the moment.",,"['linear-algebra', 'general-topology', 'optimization']"
43,Similar Matrices and their Jordan Canonical Forms [duplicate],Similar Matrices and their Jordan Canonical Forms [duplicate],,This question already has answers here : Are two matrices similar iff they have the same Jordan Canonical form? (2 answers) Closed 8 years ago . Let $A$ and $B$ be two matrices in $M_n$. Is the following ture: $A$ and $B$ are similar $\iff$ $A$ and $B$ have the same jordan canonical form. Could someone explain?,This question already has answers here : Are two matrices similar iff they have the same Jordan Canonical form? (2 answers) Closed 8 years ago . Let $A$ and $B$ be two matrices in $M_n$. Is the following ture: $A$ and $B$ are similar $\iff$ $A$ and $B$ have the same jordan canonical form. Could someone explain?,,"['linear-algebra', 'jordan-normal-form']"
44,Waves of differing frequency are orthogonal - help me understand,Waves of differing frequency are orthogonal - help me understand,,"I know that sinusoidal waves of different frequencies are orthogonal to each other.  For instance: # Shows that 1Hz and 2Hz waves are orthogonal import numpy, scipy x = numpy.linspace(0, 1, 1000) wave_1hz = scipy.sin(1 * 2*scipy.pi*x) wave_2hz = scipy.sin(2 * 2*scipy.pi*x) numpy.dot(wave_1hz, wave_2hz) # This prints a value very near 0, showing they are orthogonal I am wondering if someone can give me an analogy to help me understand ""why"" waves of different frequencies are orthogonal. To give a better idea of what I am looking for, I intuitively understand that if you have 2  $\Bbb R_2$ vectors which are at a right angle, if you look at the dot product as projecting one onto the other, there it will be 0 (like shining a flashlight straight down on a vertical poll).  This helps me understand what orthogonality means in the context of $\Bbb R_2$ vectors.  But I don't have any such analogy for waves of different frequencies.","I know that sinusoidal waves of different frequencies are orthogonal to each other.  For instance: # Shows that 1Hz and 2Hz waves are orthogonal import numpy, scipy x = numpy.linspace(0, 1, 1000) wave_1hz = scipy.sin(1 * 2*scipy.pi*x) wave_2hz = scipy.sin(2 * 2*scipy.pi*x) numpy.dot(wave_1hz, wave_2hz) # This prints a value very near 0, showing they are orthogonal I am wondering if someone can give me an analogy to help me understand ""why"" waves of different frequencies are orthogonal. To give a better idea of what I am looking for, I intuitively understand that if you have 2  $\Bbb R_2$ vectors which are at a right angle, if you look at the dot product as projecting one onto the other, there it will be 0 (like shining a flashlight straight down on a vertical poll).  This helps me understand what orthogonality means in the context of $\Bbb R_2$ vectors.  But I don't have any such analogy for waves of different frequencies.",,"['linear-algebra', 'signal-processing']"
45,The real numbers form a vector space over the rationals (i.e. with Q as the scalar),The real numbers form a vector space over the rationals (i.e. with Q as the scalar),,How would one go about proving this? I'd like someone to just point in the right direction.,How would one go about proving this? I'd like someone to just point in the right direction.,,['linear-algebra']
46,The determinant function is the only one satisfying the conditions,The determinant function is the only one satisfying the conditions,,"How can I prove that the determinant function satisfying the following properties is unique: $\det(I)=1$ where  $I$ is identity matrix, the function $\det(A)$ is linear in the rows of the matrix and if two adjacent rows of a matrix $A$ are equal, then $\det A=0$. This is how Artin has stated the properties.I find Artin's first chapter rough going and would appreciate some help on this one.","How can I prove that the determinant function satisfying the following properties is unique: $\det(I)=1$ where  $I$ is identity matrix, the function $\det(A)$ is linear in the rows of the matrix and if two adjacent rows of a matrix $A$ are equal, then $\det A=0$. This is how Artin has stated the properties.I find Artin's first chapter rough going and would appreciate some help on this one.",,"['linear-algebra', 'determinant', 'multilinear-algebra']"
47,A normal matrix with real eigenvalues is Hermitian,A normal matrix with real eigenvalues is Hermitian,,"$A$ is a normal matrix (i.e. $AA^*=A^*A$, where * denotes the hermitian conjugate). If all its eigenvalues are real, prove that it is Hermitian (i.e. $A^*=A$). I have tried many things but could not complete a proof. Could anybody please provide some help?","$A$ is a normal matrix (i.e. $AA^*=A^*A$, where * denotes the hermitian conjugate). If all its eigenvalues are real, prove that it is Hermitian (i.e. $A^*=A$). I have tried many things but could not complete a proof. Could anybody please provide some help?",,"['linear-algebra', 'matrices', 'eigenvalues-eigenvectors', 'matrix-decomposition', 'hermitian-matrices']"
48,Inverse of a symmetric tridiagonal matrix.,Inverse of a symmetric tridiagonal matrix.,,"Hello, everyone! I am trying to find the inverse of an $N\times N$ matrix with ones on the diagonal and $-\frac{1}{2}$ in all entries of the subdiagonal and superdiagonal.  For example, with $N=3$, $$A = \left(\begin{array}{ccc}1 & -1/2 & 0 \\ -1/2 & 1 & -1/2 \\ 0 & -1/2 & 1 \end{array}\right);\,\,A^{-1} = \left(\begin{array}{ccc}3/2 & 1 & 1/2 \\ 1 & 2 & 1 \\ 1/2 & 1 & 3/2 \end{array}\right).$$ In fact, I really only need to determine the value of the first entry.  I believe that it should be $\frac{2N}{N+1}$, but that is only by a heuristic method, when I'm really hoping for a proof by induction. Thanks in advance for any assistance. -Jason","Hello, everyone! I am trying to find the inverse of an $N\times N$ matrix with ones on the diagonal and $-\frac{1}{2}$ in all entries of the subdiagonal and superdiagonal.  For example, with $N=3$, $$A = \left(\begin{array}{ccc}1 & -1/2 & 0 \\ -1/2 & 1 & -1/2 \\ 0 & -1/2 & 1 \end{array}\right);\,\,A^{-1} = \left(\begin{array}{ccc}3/2 & 1 & 1/2 \\ 1 & 2 & 1 \\ 1/2 & 1 & 3/2 \end{array}\right).$$ In fact, I really only need to determine the value of the first entry.  I believe that it should be $\frac{2N}{N+1}$, but that is only by a heuristic method, when I'm really hoping for a proof by induction. Thanks in advance for any assistance. -Jason",,"['linear-algebra', 'matrices']"
49,Why can a vector from an infinite-dimensional vector space be written as finite linear combination?,Why can a vector from an infinite-dimensional vector space be written as finite linear combination?,,"Suppose $V$ is a infinite-dimensional vector space over $\mathbb{R}$ or $\mathbb{C}$ and $\beta$ is a basis for $V$ . I have seen the following claim: For every $v$ $\in$ $V$ , there exist $v_1,...,v_n \in \beta $ such that $$v = \sum_{i=1}^{n} a_i v_i.$$ But when $v$ is a linear combination of infinite number of vectors in $\beta$ , there does not exist such a integer $n$ . How can we still choose vectors like this? A related question is about $\textrm{span}(\beta)$ . I know that $\textrm{span}$ is the set of all linear combinations of finite vectors in the set. But then it would not include vectors that are obtained by a linear combination of infinite number of vectors. How can this be the case?","Suppose is a infinite-dimensional vector space over or and is a basis for . I have seen the following claim: For every , there exist such that But when is a linear combination of infinite number of vectors in , there does not exist such a integer . How can we still choose vectors like this? A related question is about . I know that is the set of all linear combinations of finite vectors in the set. But then it would not include vectors that are obtained by a linear combination of infinite number of vectors. How can this be the case?","V \mathbb{R} \mathbb{C} \beta V v \in V v_1,...,v_n \in \beta  v = \sum_{i=1}^{n} a_i v_i. v \beta n \textrm{span}(\beta) \textrm{span}","['linear-algebra', 'vector-spaces']"
50,Intersection of a line and a curve.,Intersection of a line and a curve.,,"Given that the line $y = 2x + 3$ intersects the curve $y = x^2 + 3x + 1$ at two separate points, I have to find these two points. Here is what I did: $$2x + 3 = x^2 + 3x + 1$$ $$0 = x^2 + 1x - 2$$ Using factorisation: $$x = -2 \text{ or } 1$$ Substituting each values of $x$ obtained into the equation of the  straight line gives two points of intersections at $(-2, -1)$ and $(1, 5)$ Here is my issue: Why does this work? Equating the curve and the straight line means they share a single similar value of $y$ while they clearly share two.","Given that the line $y = 2x + 3$ intersects the curve $y = x^2 + 3x + 1$ at two separate points, I have to find these two points. Here is what I did: $$2x + 3 = x^2 + 3x + 1$$ $$0 = x^2 + 1x - 2$$ Using factorisation: $$x = -2 \text{ or } 1$$ Substituting each values of $x$ obtained into the equation of the  straight line gives two points of intersections at $(-2, -1)$ and $(1, 5)$ Here is my issue: Why does this work? Equating the curve and the straight line means they share a single similar value of $y$ while they clearly share two.",,"['linear-algebra', 'algebra-precalculus', 'quadratics']"
51,Nilpotent matrix and relation between its powers and dimension of kernels,Nilpotent matrix and relation between its powers and dimension of kernels,,"Given a 4x4 matrix $T$ over $\mathbb{R}$ such that $T^4 = 0 $, $k_i = \textsf{dim} Ker(T^i)$, I need to check which of the following sequences, $$k_1\leq k_2 \leq k_3 \leq k_4,$$ is NOT possible : $ 1)\; 1\leq 3 \leq 4 \leq 4$ $2) \; 2\leq3\leq4\leq4$ $3) \; 3 \leq 4 \leq 4\leq 4$ $4)\; 2 \leq 4 \leq 4 \leq 4$ The only relevant thing I could recall relating to this is the fact that for nilpotent operators $$ \{0 \} \subset Ker(T) \subset Ker(T^2) \subset \ldots \subset Ker(T^{n-1})$$ But the equality in the choices is putting me off. A hint would be welcome. Thanks in advance.","Given a 4x4 matrix $T$ over $\mathbb{R}$ such that $T^4 = 0 $, $k_i = \textsf{dim} Ker(T^i)$, I need to check which of the following sequences, $$k_1\leq k_2 \leq k_3 \leq k_4,$$ is NOT possible : $ 1)\; 1\leq 3 \leq 4 \leq 4$ $2) \; 2\leq3\leq4\leq4$ $3) \; 3 \leq 4 \leq 4\leq 4$ $4)\; 2 \leq 4 \leq 4 \leq 4$ The only relevant thing I could recall relating to this is the fact that for nilpotent operators $$ \{0 \} \subset Ker(T) \subset Ker(T^2) \subset \ldots \subset Ker(T^{n-1})$$ But the equality in the choices is putting me off. A hint would be welcome. Thanks in advance.",,"['linear-algebra', 'matrices', 'nilpotence']"
52,1985 Putnam A1 Solution,1985 Putnam A1 Solution,,"I dont see what they mean by bijection of triples of subsets of $\{1, \ldots, 10\}$ and the $10\times3$ matrix with $0, 1$ entries? How is that created?","I dont see what they mean by bijection of triples of subsets of $\{1, \ldots, 10\}$ and the $10\times3$ matrix with $0, 1$ entries? How is that created?",,"['linear-algebra', 'combinatorics', 'analysis', 'contest-math']"
53,"$T$ be linear operator on $V$ by $T(B)=AB-BA$. Prove that if A is a nilpotent matrix, then $T$ is a nilpotent operator.","be linear operator on  by . Prove that if A is a nilpotent matrix, then  is a nilpotent operator.",T V T(B)=AB-BA T,"Let $V$ be a vector space of $n\times n$ matrices over a field F, and let $A$ be a fixed  $n\times n$ matrix.  $T$ be linear operator on $V$ by $T(B)=AB-BA$. Prove that if A is a nilpotent matrix, then $T$ is a nilpotent operator. I have done in a way that $T^2(B)= T(AB-BA)= A^2B-2ABA+BA^2\Rightarrow T^3(B)=A^3B-3A^2BA+3ABA^2-BA^3$  Proceeding in this way since A is nilpotent $\exists m\in\Bbb N$ s.t   $T^m(B)=0$. Hence T is also a nilpotent operator. This a problem of Hoffman Kunze of Primary Decomposition Chapter. So can anyone give me any other solution because this solution depends on some sort of intution. Please don't use any Rational and Jordan Forms formula.","Let $V$ be a vector space of $n\times n$ matrices over a field F, and let $A$ be a fixed  $n\times n$ matrix.  $T$ be linear operator on $V$ by $T(B)=AB-BA$. Prove that if A is a nilpotent matrix, then $T$ is a nilpotent operator. I have done in a way that $T^2(B)= T(AB-BA)= A^2B-2ABA+BA^2\Rightarrow T^3(B)=A^3B-3A^2BA+3ABA^2-BA^3$  Proceeding in this way since A is nilpotent $\exists m\in\Bbb N$ s.t   $T^m(B)=0$. Hence T is also a nilpotent operator. This a problem of Hoffman Kunze of Primary Decomposition Chapter. So can anyone give me any other solution because this solution depends on some sort of intution. Please don't use any Rational and Jordan Forms formula.",,['linear-algebra']
54,How to find a basis for $2\times 2$ matrix,How to find a basis for  matrix,2\times 2,"Consider $W$ the subset of the Vector space $V$ where $V$ is all 2x2 matrices: $$ W = \left\{ \begin{pmatrix} a & a \\ a & b \end{pmatrix} \mid a,b \in \mathbb{R} \right\} $$ How would I find a basis for $W$?","Consider $W$ the subset of the Vector space $V$ where $V$ is all 2x2 matrices: $$ W = \left\{ \begin{pmatrix} a & a \\ a & b \end{pmatrix} \mid a,b \in \mathbb{R} \right\} $$ How would I find a basis for $W$?",,"['linear-algebra', 'vector-spaces']"
55,"Given a matrix $A$, find $A^n$","Given a matrix , find",A A^n,"Given the matrix $$A = \left[{9\atop20}{-4\atop-9}\right]$$ how do I find $A^7$ or $A^{54}$ or $A^{2008}$ (etc.) ? I know I need the eigenvalues of A, but I'm not sure what to do afterwards. Is the answer a $2 \times 2$ matrix? What role do the eigenvalues play in solving the problem? Whats the set up for the solution? Can this process be used on larger square matrices? ($A_{3\times3}$, or $A_{4\times4}$, or $A_{n\times n}$)? Eigenvalues: $$\det(\lambda I_n-A)=0$$ $$\det\left( \lambda \left[{1\atop0}{0\atop1}\right] -\left[{9\atop20}{(-4)\atop(-9)}\right] \right)=0$$ $$\det\left( \left[{\lambda\atop0}{0\atop\lambda}\right] -\left[{9\atop20}{(-4)\atop(-9)}\right] \right)=0$$ $$\det\left( \left[{(\lambda-9)\atop(-20)}{4\atop(\lambda+9)}\right] \right)=0$$ $$(\lambda-9)(\lambda+9) -(20)(-4)=0$$ $$(\lambda^2-9\lambda+9\lambda-81) +80=0$$ $$\lambda^2-1=0$$ $$\lambda^2=1$$ $$\lambda=\pm1$$","Given the matrix $$A = \left[{9\atop20}{-4\atop-9}\right]$$ how do I find $A^7$ or $A^{54}$ or $A^{2008}$ (etc.) ? I know I need the eigenvalues of A, but I'm not sure what to do afterwards. Is the answer a $2 \times 2$ matrix? What role do the eigenvalues play in solving the problem? Whats the set up for the solution? Can this process be used on larger square matrices? ($A_{3\times3}$, or $A_{4\times4}$, or $A_{n\times n}$)? Eigenvalues: $$\det(\lambda I_n-A)=0$$ $$\det\left( \lambda \left[{1\atop0}{0\atop1}\right] -\left[{9\atop20}{(-4)\atop(-9)}\right] \right)=0$$ $$\det\left( \left[{\lambda\atop0}{0\atop\lambda}\right] -\left[{9\atop20}{(-4)\atop(-9)}\right] \right)=0$$ $$\det\left( \left[{(\lambda-9)\atop(-20)}{4\atop(\lambda+9)}\right] \right)=0$$ $$(\lambda-9)(\lambda+9) -(20)(-4)=0$$ $$(\lambda^2-9\lambda+9\lambda-81) +80=0$$ $$\lambda^2-1=0$$ $$\lambda^2=1$$ $$\lambda=\pm1$$",,"['linear-algebra', 'matrices', 'eigenvalues-eigenvectors', 'exponential-function']"
56,$A$ is some fixed matrix. Let $U(B)=AB-BA$. If $A$ is diagonalizable then so is $U$?,is some fixed matrix. Let . If  is diagonalizable then so is ?,A U(B)=AB-BA A U,This is from Hoffman and Kunze 6.4.13. I am studying for an exam and trying to solve some problems in Hoffman and Kunze. Here is the question. Let $V$ be the space of $n\times n$ matrices over a field $F$. Let $A$ be a fixed matrix in $V$. Let $T$ and $U$ be linear operators on $V$ defined by $T(B)=AB$ and $U(B)=AB-BA$. a) (True or False) If $A$ is diagonalizable then $T$ is diagonalizable. This is true. I can show that both A and T have the same minimal polynomial. So if  $A$ is diagonalizable then the minimal polynomial of $T$ should be a product of distinct linear factors. Proving that $T$ is diagonalizable. but its the next question I am having trouble with. b)(True or false) If $A$ is diagonalizable then $U$ is diagonlizable.  I am thinking this is false. But I can't think of of a counter example. May be I am wrong. Can anybody help?. Thanks for all your help.,This is from Hoffman and Kunze 6.4.13. I am studying for an exam and trying to solve some problems in Hoffman and Kunze. Here is the question. Let $V$ be the space of $n\times n$ matrices over a field $F$. Let $A$ be a fixed matrix in $V$. Let $T$ and $U$ be linear operators on $V$ defined by $T(B)=AB$ and $U(B)=AB-BA$. a) (True or False) If $A$ is diagonalizable then $T$ is diagonalizable. This is true. I can show that both A and T have the same minimal polynomial. So if  $A$ is diagonalizable then the minimal polynomial of $T$ should be a product of distinct linear factors. Proving that $T$ is diagonalizable. but its the next question I am having trouble with. b)(True or false) If $A$ is diagonalizable then $U$ is diagonlizable.  I am thinking this is false. But I can't think of of a counter example. May be I am wrong. Can anybody help?. Thanks for all your help.,,"['linear-algebra', 'matrices', 'diagonalization']"
57,Eigenvectors of a circulant matrix,Eigenvectors of a circulant matrix,,"Show that for the circulant matrix $$C =  \begin{bmatrix} c_0     & c_{n-1} & \dots  & c_{2} & c_{1}  \\ c_{1} & c_0    & c_{n-1} &         & c_{2}  \\ \vdots  & c_{1}& c_0    & \ddots  & \vdots   \\ c_{n-2}  &        & \ddots & \ddots  & c_{n-1}   \\ c_{n-1}  & c_{n-2} & \dots  & c_{1} & c_0 \\ \end{bmatrix}$$ the eigenvectors are $$v_j = \left(1,w_j,w_j^2,...w_j^{n-1}\right), \qquad j=0,1,..,n-1,$$ where $$w_j = \exp\left(2\pi i j/n\right)$$ is the $n$ -th root of unity. Show that the $\left(v_j\right)$ are linearly independent. To show that the $v_j$ are eigenvector the only way I know is to solve the difference equation associated to the characteristic polynomial of $C$ to get a unique eigenvalue $\lambda$ . Then find $\text{ker}\left(C-\lambda I\right)$ . Is there an another way? As for linear independence, I don't see how to reduce the matrix $\{v_j\}$ whose column are the eigenvector $v_j$ .","Show that for the circulant matrix the eigenvectors are where is the -th root of unity. Show that the are linearly independent. To show that the are eigenvector the only way I know is to solve the difference equation associated to the characteristic polynomial of to get a unique eigenvalue . Then find . Is there an another way? As for linear independence, I don't see how to reduce the matrix whose column are the eigenvector .","C = 
\begin{bmatrix}
c_0     & c_{n-1} & \dots  & c_{2} & c_{1}  \\
c_{1} & c_0    & c_{n-1} &         & c_{2}  \\
\vdots  & c_{1}& c_0    & \ddots  & \vdots   \\
c_{n-2}  &        & \ddots & \ddots  & c_{n-1}   \\
c_{n-1}  & c_{n-2} & \dots  & c_{1} & c_0 \\
\end{bmatrix} v_j = \left(1,w_j,w_j^2,...w_j^{n-1}\right), \qquad j=0,1,..,n-1, w_j = \exp\left(2\pi i j/n\right) n \left(v_j\right) v_j C \lambda \text{ker}\left(C-\lambda I\right) \{v_j\} v_j","['linear-algebra', 'matrices']"
58,Is this possible? AB- BA=I [duplicate],Is this possible? AB- BA=I [duplicate],,"This question already has answers here : $AB-BA=I$ having no solutions (6 answers) Closed 8 years ago . I have just started linear functionals when I faced the following problem: If $A$ and $B$ are $n \times n$ complex matrices, show $AB - BA=\Bbb{I}$ is impossible. Can someone help me?","This question already has answers here : $AB-BA=I$ having no solutions (6 answers) Closed 8 years ago . I have just started linear functionals when I faced the following problem: If $A$ and $B$ are $n \times n$ complex matrices, show $AB - BA=\Bbb{I}$ is impossible. Can someone help me?",,"['linear-algebra', 'matrices', 'matrix-equations']"
59,The definition of span,The definition of span,,"In Linear Algebra by Friedberg, Insel and Spence, the definition of span (pg-$30$) is given as: Let $S$ be a nonempty subset of a vector space $V$. The span of $S$,   denoted by span $(S)$, is the set containing of all linear   combinations of vectors in $S$. For convenience, we define span $(\emptyset)=\{0\}$. In Linear Algebra by Hoffman and Kunze, the definition of span (pg-$36$) is given as: Let $S$ be a set of vectors in a vector space $V$. The subspace   spanned by $S$ is defined to be intersection $W$ of all subspaces of   $V$ which contain $S$. When $S$ is finite set of vectors, $S = \{\alpha_1, \alpha_2, ..., \alpha_n  \}$, we shall simply call $W$ the   subspace spanned by the vectors $\alpha_1, \alpha_2, ..., \alpha_n$. I am not able to understand the second definition completely. How do I relate ""set of all linear combinations"" and ""intersection $W$ of all subspaces""? Please help. Thanks.","In Linear Algebra by Friedberg, Insel and Spence, the definition of span (pg-$30$) is given as: Let $S$ be a nonempty subset of a vector space $V$. The span of $S$,   denoted by span $(S)$, is the set containing of all linear   combinations of vectors in $S$. For convenience, we define span $(\emptyset)=\{0\}$. In Linear Algebra by Hoffman and Kunze, the definition of span (pg-$36$) is given as: Let $S$ be a set of vectors in a vector space $V$. The subspace   spanned by $S$ is defined to be intersection $W$ of all subspaces of   $V$ which contain $S$. When $S$ is finite set of vectors, $S = \{\alpha_1, \alpha_2, ..., \alpha_n  \}$, we shall simply call $W$ the   subspace spanned by the vectors $\alpha_1, \alpha_2, ..., \alpha_n$. I am not able to understand the second definition completely. How do I relate ""set of all linear combinations"" and ""intersection $W$ of all subspaces""? Please help. Thanks.",,"['linear-algebra', 'vector-spaces']"
60,Odd and even functions- a direct sum?,Odd and even functions- a direct sum?,,"Question: Let $V$ be the vector space of all functions $\Bbb R\to \Bbb R$ . Show that $V=U \oplus W$ for $$U=\{f\ | \ f(x)=f(-x)\ \ \forall x\}, \quad W=\{f \ |\ f(x)=-f(-x) \ \ \forall x\}$$ What I did : I did prove that $U \cap W$ ={ $0$ }. But proving that any function from $\mathbb{R}$ to $\mathbb{R}$ can be displayed as a sum of odds and evens wasn't a success. I tried saying that for $v \in V, w \in W: v=v-w+w$ and proving that $v-w \in U$ but that didn't work (that trick worked with some linear transformations we saw, but this isn't a linear transformation).","Question: Let be the vector space of all functions . Show that for What I did : I did prove that ={ }. But proving that any function from to can be displayed as a sum of odds and evens wasn't a success. I tried saying that for and proving that but that didn't work (that trick worked with some linear transformations we saw, but this isn't a linear transformation).","V \Bbb R\to \Bbb R V=U \oplus W U=\{f\ | \ f(x)=f(-x)\ \ \forall x\}, \quad W=\{f \ |\ f(x)=-f(-x) \ \ \forall x\} U \cap W 0 \mathbb{R} \mathbb{R} v \in V, w \in W: v=v-w+w v-w \in U",['linear-algebra']
61,Requirements on fields for determinants to bust dependence.,Requirements on fields for determinants to bust dependence.,,"Being very much used to working on $\mathbb R^n$ $\mathbb C^n$ I just played around a bit with $$M = \left[\begin{array}{cc} 2&1\\ 1&2 \end{array}\right]$$ If the elements are in ""integers mod $3$"" field, then $$\det(M) = 2\cdot 2-1\cdot 1=4-1=3\equiv 0 (\mod3)$$ Being so used to real matrices I would at first glance guess they were linearly independent. But if I double first column I actually get $[4,2]^T \equiv [1,2]^T (\text{mod } 3)$. So they actually are dependent anyway? My real question is : will determinants being $0$ tell us linear dependence for all finite fields in the same way they do if we work over $\mathbb R$ or $\mathbb C$ or is there some special requirement on the field for determinants to bust dependence?","Being very much used to working on $\mathbb R^n$ $\mathbb C^n$ I just played around a bit with $$M = \left[\begin{array}{cc} 2&1\\ 1&2 \end{array}\right]$$ If the elements are in ""integers mod $3$"" field, then $$\det(M) = 2\cdot 2-1\cdot 1=4-1=3\equiv 0 (\mod3)$$ Being so used to real matrices I would at first glance guess they were linearly independent. But if I double first column I actually get $[4,2]^T \equiv [1,2]^T (\text{mod } 3)$. So they actually are dependent anyway? My real question is : will determinants being $0$ tell us linear dependence for all finite fields in the same way they do if we work over $\mathbb R$ or $\mathbb C$ or is there some special requirement on the field for determinants to bust dependence?",,"['linear-algebra', 'abstract-algebra', 'matrices', 'determinant', 'independence']"
62,Fock space used in Quantum mechanic : how can we have direct sum of spaces of different dimensions?,Fock space used in Quantum mechanic : how can we have direct sum of spaces of different dimensions?,,"In physics we work with the Fock space when we have to deal with an undefinite number of particles. But there is something I misunderstand : how can we have a direct sum of spaces that are not of the same dimension ? Indeed we have : $$ F=\bigoplus_{n=0}^{\infty}S_\nu H^{\otimes n} $$ Each space in the direct sum hasn't the same dimension as another one. To make the question more simple lets just take : $$A=\mathbb{R} \oplus \mathbb{R}^2$$ I don't understand how such a space is defined as we would need a law of addition between $\mathbb{R}$ and $\mathbb{R}^2$. Let $z$ be in $A$, we would have $z=x+(y,z)$ but what does that + symbol means ? I just know basics linear algebra so simple answers would be nice !","In physics we work with the Fock space when we have to deal with an undefinite number of particles. But there is something I misunderstand : how can we have a direct sum of spaces that are not of the same dimension ? Indeed we have : $$ F=\bigoplus_{n=0}^{\infty}S_\nu H^{\otimes n} $$ Each space in the direct sum hasn't the same dimension as another one. To make the question more simple lets just take : $$A=\mathbb{R} \oplus \mathbb{R}^2$$ I don't understand how such a space is defined as we would need a law of addition between $\mathbb{R}$ and $\mathbb{R}^2$. Let $z$ be in $A$, we would have $z=x+(y,z)$ but what does that + symbol means ? I just know basics linear algebra so simple answers would be nice !",,"['linear-algebra', 'hilbert-spaces']"
63,"Let $\left\{\Delta_1,\Delta_2,.....,\Delta_n\right\}$ be the set of all determinants of order 3 that can be made with the distinct real numbers",Let  be the set of all determinants of order 3 that can be made with the distinct real numbers,"\left\{\Delta_1,\Delta_2,.....,\Delta_n\right\}","Let $\left\{\Delta_1,\Delta_2,.....,\Delta_n\right\}$ be the set of all determinants of order 3 that can be made with the distinct real numbers from the set $S=\left\{1,2,3,4,5,6,7,,8,9\right\}$.Then prove that $\sum_{i=1}^{n}\Delta_i=0$ I know that the total number of determinants that can be formed by using distinct real numbers from the given set is $9!$.But i dont know how to prove that their sum is zero.Please help me.Thanks","Let $\left\{\Delta_1,\Delta_2,.....,\Delta_n\right\}$ be the set of all determinants of order 3 that can be made with the distinct real numbers from the set $S=\left\{1,2,3,4,5,6,7,,8,9\right\}$.Then prove that $\sum_{i=1}^{n}\Delta_i=0$ I know that the total number of determinants that can be formed by using distinct real numbers from the given set is $9!$.But i dont know how to prove that their sum is zero.Please help me.Thanks",,"['linear-algebra', 'determinant']"
64,Let $\lambda$ be an eigenvalue of $A$. Prove that $\lambda^{-1}$ is an eigenvalue of $A^{-1}$.,Let  be an eigenvalue of . Prove that  is an eigenvalue of .,\lambda A \lambda^{-1} A^{-1},"Let $\lambda$ be an eigenvalue of $A$. Prove that $\lambda^{-1}$ is an eigenvalue of $A^{-1}$. My approach: Suppose $\lambda$ is an eigenvalue of $A$. Then $Ax=\lambda x$ for some $x\neq 0$. Since $A$ is invertible, $Ax=\lambda x \implies A^{-1}Ax=A^{-1}\lambda x$. So: $$Ix=A^{-1}\lambda x \iff Ix-A^{-1}\lambda x=0\iff x(I-A^{-1}\lambda)=0 $$ Since $x \neq 0$ and $A$ is invertible, $\lambda^{-1}=A^{-1}$ (I'm unsure of this equality). I think this somewhat shows it, this might seem elementary but aren't $\lambda, \lambda^{-1}$ scalars? So basically, if this proof is true $\lambda\cdot\lambda^{-1}=I$ also? Any other way to show this? Thanks","Let $\lambda$ be an eigenvalue of $A$. Prove that $\lambda^{-1}$ is an eigenvalue of $A^{-1}$. My approach: Suppose $\lambda$ is an eigenvalue of $A$. Then $Ax=\lambda x$ for some $x\neq 0$. Since $A$ is invertible, $Ax=\lambda x \implies A^{-1}Ax=A^{-1}\lambda x$. So: $$Ix=A^{-1}\lambda x \iff Ix-A^{-1}\lambda x=0\iff x(I-A^{-1}\lambda)=0 $$ Since $x \neq 0$ and $A$ is invertible, $\lambda^{-1}=A^{-1}$ (I'm unsure of this equality). I think this somewhat shows it, this might seem elementary but aren't $\lambda, \lambda^{-1}$ scalars? So basically, if this proof is true $\lambda\cdot\lambda^{-1}=I$ also? Any other way to show this? Thanks",,[]
65,Do the matrices with maximum determinant always have integral values?,Do the matrices with maximum determinant always have integral values?,,"Consider all $n$ by $n$ matrices whose elements are real values in $[0,1]$.  Now for a given $n$, consider all those matrices with maximum determinant. Are all the elements of these matrices always either exactly $0$ or $1$? From numerical experiments this seems to be the case but I don't see how to prove it. A very nice counterexample was given to my question as I didn't phrase it correctly. It should have been: Can the maximum determinant always be reached by a matrix with only   $1$ and $0$ entries?","Consider all $n$ by $n$ matrices whose elements are real values in $[0,1]$.  Now for a given $n$, consider all those matrices with maximum determinant. Are all the elements of these matrices always either exactly $0$ or $1$? From numerical experiments this seems to be the case but I don't see how to prove it. A very nice counterexample was given to my question as I didn't phrase it correctly. It should have been: Can the maximum determinant always be reached by a matrix with only   $1$ and $0$ entries?",,['linear-algebra']
66,The meaning of Inverse Matrix,The meaning of Inverse Matrix,,"I am studying Linear Algebra, I have 3 questions in my mind What does an inverse matrix mean. I am trying to have a meaning of it, but I don't really understand. When a matrix does not have an inverse matrix, what does it mean? Hope to hear your expertise. I am sorry if I have place my questions in the wrong places. Thank you.","I am studying Linear Algebra, I have 3 questions in my mind What does an inverse matrix mean. I am trying to have a meaning of it, but I don't really understand. When a matrix does not have an inverse matrix, what does it mean? Hope to hear your expertise. I am sorry if I have place my questions in the wrong places. Thank you.",,['linear-algebra']
67,Raising a square matrix to a negative half power,Raising a square matrix to a negative half power,,"I want to implement the following formula (taken from Kaiser, 1970) in R where $R$ is square matrix of correlations: $$S = (\textrm{diag } R^{-1})^{-1/2}$$ I understand the diagonal and inverse operations, but I am unclear on the meaning of raising a square matrix to a negative half power. Thus, my questions What does it mean to raise a square matrix to a negative half power? What general ideas of linear algebra does this assume? (if this is in scope) How would this be implemented in R? References Kaiser, H. F. (1970). A second generation little jiffy. Psychometrika, 35(4), 401-415.","I want to implement the following formula (taken from Kaiser, 1970) in R where $R$ is square matrix of correlations: $$S = (\textrm{diag } R^{-1})^{-1/2}$$ I understand the diagonal and inverse operations, but I am unclear on the meaning of raising a square matrix to a negative half power. Thus, my questions What does it mean to raise a square matrix to a negative half power? What general ideas of linear algebra does this assume? (if this is in scope) How would this be implemented in R? References Kaiser, H. F. (1970). A second generation little jiffy. Psychometrika, 35(4), 401-415.",,"['linear-algebra', 'matrices', 'exponentiation']"
68,Geometric argument that operators on $\mathbb{R}^3$ have an eigenvalue?,Geometric argument that operators on  have an eigenvalue?,\mathbb{R}^3,"This question came up when trying to trying to find a $3\times3$ real matrix $A$ such that $Ax$ is nonzero for nonzero $x$ $Ax$ is orthogonal to $x$ for any $x$ in $\mathbb{R}^3$ We know such a matrix cannot exist because $A$ must have an eigenvalue (thus there is some $x$ such that either $Ax = 0$ or $Ax$ is parallel to $x$) However, Is there a nice, purely geometric way to justify that every operator on $\mathbb{R}^3 $ has a (real) eigenvalue? To clarify: I'm looking an intuitive way to visualize why an eigenvalue must exist in this case.  In particular, no polynomials and no determinants are allowed!","This question came up when trying to trying to find a $3\times3$ real matrix $A$ such that $Ax$ is nonzero for nonzero $x$ $Ax$ is orthogonal to $x$ for any $x$ in $\mathbb{R}^3$ We know such a matrix cannot exist because $A$ must have an eigenvalue (thus there is some $x$ such that either $Ax = 0$ or $Ax$ is parallel to $x$) However, Is there a nice, purely geometric way to justify that every operator on $\mathbb{R}^3 $ has a (real) eigenvalue? To clarify: I'm looking an intuitive way to visualize why an eigenvalue must exist in this case.  In particular, no polynomials and no determinants are allowed!",,['linear-algebra']
69,Nonsymmetric matrix has real eigenvalues,Nonsymmetric matrix has real eigenvalues,,"Consider a $n\times n$ symmetric matrix $A=\{a_{i,j}\}_{i,j}$ with nonnegative values, i.e., $a_{i,j}\ge0$ . It is known that $A$ must have real eigenvalues. Now, consider numbers $s_1,\dots, s_n\ge0$ . Is it true that $\{s_ia_{i,j}\}_{i,j}$ has real eigenvalues? For a better visualization of the matrix: \begin{align*} \{s_ia_{i,j}\}_{i,j}=\begin{bmatrix}     s_1 a_{11} & s_1 a_{12} & s_1 a_{13} & \dots  & s_1 a_{1n} \\     s_2 a_{21} & s_2 a_{22} & s_2 a_{23} & \dots  & s_2 a_{2n} \\     \vdots & \vdots & \vdots & \ddots & \vdots \\     s_n a_{n1} & s_n a_{n2} & s_n a_{n3} & \dots  & s_n a_{nn}. \end{bmatrix} \end{align*} In the two dimensional case $n=2$ , it is not hard to see that indeed there cannot be complex eigenvalues. However, calculations become immediately very messy in the three dimensional case. Some numerical tests in the 3-dimensional case suggests the conjecture should be true.","Consider a symmetric matrix with nonnegative values, i.e., . It is known that must have real eigenvalues. Now, consider numbers . Is it true that has real eigenvalues? For a better visualization of the matrix: In the two dimensional case , it is not hard to see that indeed there cannot be complex eigenvalues. However, calculations become immediately very messy in the three dimensional case. Some numerical tests in the 3-dimensional case suggests the conjecture should be true.","n\times n A=\{a_{i,j}\}_{i,j} a_{i,j}\ge0 A s_1,\dots, s_n\ge0 \{s_ia_{i,j}\}_{i,j} \begin{align*}
\{s_ia_{i,j}\}_{i,j}=\begin{bmatrix}
    s_1 a_{11} & s_1 a_{12} & s_1 a_{13} & \dots  & s_1 a_{1n} \\
    s_2 a_{21} & s_2 a_{22} & s_2 a_{23} & \dots  & s_2 a_{2n} \\
    \vdots & \vdots & \vdots & \ddots & \vdots \\
    s_n a_{n1} & s_n a_{n2} & s_n a_{n3} & \dots  & s_n a_{nn}.
\end{bmatrix}
\end{align*} n=2","['linear-algebra', 'eigenvalues-eigenvectors', 'symmetric-matrices']"
70,Why are the eigenvalues of a covariance matrix equal to the variance of its eigenvectors?,Why are the eigenvalues of a covariance matrix equal to the variance of its eigenvectors?,,"This assertion came up in a Deep Learning course I am taking. I understand intuitively that the eigenvector with the largest eigenvalue will be the direction in which the most variance occurs. I understand why we use the covariance matrix's eigenvectors for Principal Component Analysis. However, I do not get why the eigenvectors' variance are equal to their respective eigenvalues. I would prefer a formal proof, but an intuitive explanation may be acceptable. (Note: this is not a duplicate of this question. )","This assertion came up in a Deep Learning course I am taking. I understand intuitively that the eigenvector with the largest eigenvalue will be the direction in which the most variance occurs. I understand why we use the covariance matrix's eigenvectors for Principal Component Analysis. However, I do not get why the eigenvectors' variance are equal to their respective eigenvalues. I would prefer a formal proof, but an intuitive explanation may be acceptable. (Note: this is not a duplicate of this question. )",,"['linear-algebra', 'eigenvalues-eigenvectors', 'machine-learning', 'covariance', 'variance']"
71,Is there a name for matrix product with reversed indices?,Is there a name for matrix product with reversed indices?,,"The typical matrix product is as follows: $$ (\mathbf{A}\mathbf{B})_{ij} = \sum_{k=1}^m A_{ik}B_{kj}\,. $$ Is there a name or characterization for one such as   $$(\mathbf{A}\mathbf{B})_{ij} = \sum_{k=1}^m A_{ki}B_{jk}\,? $$ Furthermore, what can be said about the matrix vector product  $$ (\mathbf{A}\mathbf{b})_{i} = \sum_{k=1}^m A_{ki}b_{k}\,? $$ Is there any way to express the above matrix-vector product in terms of traditional linear algebra?","The typical matrix product is as follows: $$ (\mathbf{A}\mathbf{B})_{ij} = \sum_{k=1}^m A_{ik}B_{kj}\,. $$ Is there a name or characterization for one such as   $$(\mathbf{A}\mathbf{B})_{ij} = \sum_{k=1}^m A_{ki}B_{jk}\,? $$ Furthermore, what can be said about the matrix vector product  $$ (\mathbf{A}\mathbf{b})_{i} = \sum_{k=1}^m A_{ki}b_{k}\,? $$ Is there any way to express the above matrix-vector product in terms of traditional linear algebra?",,"['linear-algebra', 'matrices', 'vectors']"
72,Proof of Eckart-Young-Mirsky theorem,Proof of Eckart-Young-Mirsky theorem,,"Could someone please explain why in this Wiki page one says "" we know  that $\exists(k+1)$ dimension space $(v_1,v_2, \dots, v_n)$"" ?","Could someone please explain why in this Wiki page one says "" we know  that $\exists(k+1)$ dimension space $(v_1,v_2, \dots, v_n)$"" ?",,"['linear-algebra', 'proof-explanation', 'svd']"
73,How to prove that $B^\vee$ is a base for coroots?,How to prove that  is a base for coroots?,B^\vee,"Let $\Phi$ be a root system in a real inner product space $E$. Define $\alpha^\vee = \frac{2\alpha}{(\alpha, \alpha)}$. Then the set $\Phi^\vee = \{\alpha^\vee: \alpha \in \Phi \}$ is also a root system. Let $B$ be a base for the root system $\Phi$, ie. $B$ is a basis for $E$ and each $\alpha \in \Phi$ can be written as $\alpha = \sum_{\beta \in B} {k_\beta} \beta$ such that $k_\beta$ are integers of same sign. Question: How to prove that $B^\vee = \{ \alpha^\vee: \alpha \in B\}$ is a base for the root system $\Phi^\vee$? It is easy to see that $B^\vee$ is a basis for $E$. For the other property, here is what I have so far. Let $\alpha = \sum_{\beta \in B} {k_\beta} \beta$ be a root. Then $$\alpha^\vee = \sum_{\beta \in B} \frac{k_\beta (\beta, \beta)}{(\alpha, \alpha)} \beta^\vee$$ so to prove the claim, it is necessary to prove that $\frac{k_\beta (\beta, \beta)}{(\alpha, \alpha)}$ is an integer. I don't see how that follows. In the case where $(\alpha, \beta) = 0$ the proportion $\frac{(\beta, \beta)}{(\alpha, \alpha)}$ could be anything, so I am a bit confused.","Let $\Phi$ be a root system in a real inner product space $E$. Define $\alpha^\vee = \frac{2\alpha}{(\alpha, \alpha)}$. Then the set $\Phi^\vee = \{\alpha^\vee: \alpha \in \Phi \}$ is also a root system. Let $B$ be a base for the root system $\Phi$, ie. $B$ is a basis for $E$ and each $\alpha \in \Phi$ can be written as $\alpha = \sum_{\beta \in B} {k_\beta} \beta$ such that $k_\beta$ are integers of same sign. Question: How to prove that $B^\vee = \{ \alpha^\vee: \alpha \in B\}$ is a base for the root system $\Phi^\vee$? It is easy to see that $B^\vee$ is a basis for $E$. For the other property, here is what I have so far. Let $\alpha = \sum_{\beta \in B} {k_\beta} \beta$ be a root. Then $$\alpha^\vee = \sum_{\beta \in B} \frac{k_\beta (\beta, \beta)}{(\alpha, \alpha)} \beta^\vee$$ so to prove the claim, it is necessary to prove that $\frac{k_\beta (\beta, \beta)}{(\alpha, \alpha)}$ is an integer. I don't see how that follows. In the case where $(\alpha, \beta) = 0$ the proportion $\frac{(\beta, \beta)}{(\alpha, \alpha)}$ could be anything, so I am a bit confused.",,"['linear-algebra', 'lie-algebras', 'root-systems']"
74,Finding determinant for a matrix with one value on the diagonal and another everywhere else [duplicate],Finding determinant for a matrix with one value on the diagonal and another everywhere else [duplicate],,"This question already has answers here : Determinant of a matrix with diagonal entries $a$ and off-diagonal entries $b$ [duplicate] (9 answers) Closed 5 years ago . Let us look the the matrix $\left(\begin{array}{ccccc} a & b & b & b & b\\ b & a & b & b & b\\ b & b & a & b & b\\ b & b & b & a & b\\ b & b & b & b & a \end{array}\right)$ It has one value, $a$, on the main diagonal, and another value, $b$ everywhere else. Let us assume that we are over a ring and that $a$ is invertible. I wish to find the determinant of every $n\times n$ matrix of this form ($a$ on the diagonal, $b$ everywhere else). Using row and column operations I have managed to transform the matrix to upper-triangular form and found formula for specific cases. Generalizing it I got to the following formula: $\det(A) = a\left(a-b\right)^{n-2}\left(a+\left(n-2\right)b-\frac{\left(n-1\right)b^{2}}{a}\right)$ I think I can prove it with row-operations in the general case with a little patience. However, I'm wondering if there is a ""smart"" way of getting to this formula that I'm missing and if there is a nicer representation of it. Also, what can be said when $a$ is not invertible? (esp. the case where we are over a field and $a=0$).","This question already has answers here : Determinant of a matrix with diagonal entries $a$ and off-diagonal entries $b$ [duplicate] (9 answers) Closed 5 years ago . Let us look the the matrix $\left(\begin{array}{ccccc} a & b & b & b & b\\ b & a & b & b & b\\ b & b & a & b & b\\ b & b & b & a & b\\ b & b & b & b & a \end{array}\right)$ It has one value, $a$, on the main diagonal, and another value, $b$ everywhere else. Let us assume that we are over a ring and that $a$ is invertible. I wish to find the determinant of every $n\times n$ matrix of this form ($a$ on the diagonal, $b$ everywhere else). Using row and column operations I have managed to transform the matrix to upper-triangular form and found formula for specific cases. Generalizing it I got to the following formula: $\det(A) = a\left(a-b\right)^{n-2}\left(a+\left(n-2\right)b-\frac{\left(n-1\right)b^{2}}{a}\right)$ I think I can prove it with row-operations in the general case with a little patience. However, I'm wondering if there is a ""smart"" way of getting to this formula that I'm missing and if there is a nicer representation of it. Also, what can be said when $a$ is not invertible? (esp. the case where we are over a field and $a=0$).",,"['linear-algebra', 'matrices', 'determinant']"
75,Non-degenerate bilinear forms and invertible matrices,Non-degenerate bilinear forms and invertible matrices,,"I have the following question: Show that a symmetric bilinear form is nondegenerate if and only if its matrix in a basis is invertible Ok so both directions ""if non-degenerate then the matrix is invertible"" and ""if matrix is invertible then the form is nondegenerate"" have to be proven for this. For the first direction. If the bilinear form is non-degenerate its null space is $\left\{0\right\}$, so for every $v \neq 0$ there exists a $v'$ such that $\langle v, v' \rangle \neq 0$ and so there are no zero eigenvalues, I'm pretty sure this somehow implies the determinant of the matrix is also non-zero, as it is the product of the eigenvalues and hence invertible (but not sure how to show this). For the other direction, if the matrix is invertible, then the determinant is non-zero and, if my previous assumption is correct, this implies there are no zero eigenvectors, which implies the null space is $\left\{0\right\}$ and hence non-degenerate? Thanks for any guidance, the notes I have on this topic are pretty poor so I'm having a hard time fully understanding everything. Thanks again.","I have the following question: Show that a symmetric bilinear form is nondegenerate if and only if its matrix in a basis is invertible Ok so both directions ""if non-degenerate then the matrix is invertible"" and ""if matrix is invertible then the form is nondegenerate"" have to be proven for this. For the first direction. If the bilinear form is non-degenerate its null space is $\left\{0\right\}$, so for every $v \neq 0$ there exists a $v'$ such that $\langle v, v' \rangle \neq 0$ and so there are no zero eigenvalues, I'm pretty sure this somehow implies the determinant of the matrix is also non-zero, as it is the product of the eigenvalues and hence invertible (but not sure how to show this). For the other direction, if the matrix is invertible, then the determinant is non-zero and, if my previous assumption is correct, this implies there are no zero eigenvectors, which implies the null space is $\left\{0\right\}$ and hence non-degenerate? Thanks for any guidance, the notes I have on this topic are pretty poor so I'm having a hard time fully understanding everything. Thanks again.",,"['linear-algebra', 'bilinear-form']"
76,Is every invertible matrix a change of basis matrix?,Is every invertible matrix a change of basis matrix?,,"In the course that I am having, we are treating change of basis matrices as the matrices of the identity operation from one basis S to another basis say B . So, our instructor introduced a theorem : If A and B are similar, i.e., S −1 AS = B for an invertible matrix S , then they have the same characteristic polynomial. In particular, they have the same eigenvalues, det( A ) = det( B ) and Trace( A ) = Trace( B ). And hence came the question here... Is every invertible matrix a change of basis matrix?  Every change of basis matrix is certainly invertible, is the converse true?","In the course that I am having, we are treating change of basis matrices as the matrices of the identity operation from one basis S to another basis say B . So, our instructor introduced a theorem : If A and B are similar, i.e., S −1 AS = B for an invertible matrix S , then they have the same characteristic polynomial. In particular, they have the same eigenvalues, det( A ) = det( B ) and Trace( A ) = Trace( B ). And hence came the question here... Is every invertible matrix a change of basis matrix?  Every change of basis matrix is certainly invertible, is the converse true?",,"['linear-algebra', 'matrices', 'inverse']"
77,A general element of U(2),A general element of U(2),,"The group U(2) is the group of all $2\times 2$ matrices such that $U^\dagger U=I$ . Evidently it has $4$ real parameters, and can be represented as: $$ U(2) = \{\begin{bmatrix}a&b\\ 0&d \end{bmatrix}:\,\{a,b,d\}\in\mathbb{C}\wedge|a|^2=|d|^2=1\}\cup\{\begin{bmatrix}a&0\\ c&d \end{bmatrix}:\,\{a,c,d\}\in\mathbb{C}\wedge|a|^2=|d|^2=1\}$$ I know that the Pauli matrices ""generate"" $SU(2)$ in the sense that I could write a general element of $SU(2)$ as $\exp(i\theta\mathbf{\hat{n}}\cdot\mathbf{\sigma})$ where $\theta\in\mathbb{R}$ , $\mathbf{\hat{n}}\in S^2$ and $\mathbf{\sigma}$ is the vector of the Pauli matrices; Furthermore I can write a general element of $U(1)$ as $\exp(ix)$ where $x\in\mathbb{R}$ . To put things together, I know that $U(2)\simeq SU(2)\cup U(1)$ . So does that mean I could write: $U(2) =\{\exp(ix) \exp(i\theta\mathbf{\hat{n}}\cdot\mathbf{\sigma}) \,:\,\{x,\theta\}\in\mathbb{R}\wedge\mathbf{\hat{n}}\in S^2 \}$ or $U(2) =\{\exp(ix)I+ \exp(i\theta\mathbf{\hat{n}}\cdot\mathbf{\sigma}) \,:\,\{x,\theta\}\in\mathbb{R}\wedge\mathbf{\hat{n}}\in S^2 \}$ Which one of these options is true, and how do I get to know that just from the prescription $U(2)\simeq \langle SU(2)\cup U(1)\rangle$ ? (where $\langle A\rangle$ stands for the subgroup generated by the subset $A$ ).","The group U(2) is the group of all matrices such that . Evidently it has real parameters, and can be represented as: I know that the Pauli matrices ""generate"" in the sense that I could write a general element of as where , and is the vector of the Pauli matrices; Furthermore I can write a general element of as where . To put things together, I know that . So does that mean I could write: or Which one of these options is true, and how do I get to know that just from the prescription ? (where stands for the subgroup generated by the subset ).","2\times 2 U^\dagger U=I 4  U(2) = \{\begin{bmatrix}a&b\\ 0&d \end{bmatrix}:\,\{a,b,d\}\in\mathbb{C}\wedge|a|^2=|d|^2=1\}\cup\{\begin{bmatrix}a&0\\ c&d \end{bmatrix}:\,\{a,c,d\}\in\mathbb{C}\wedge|a|^2=|d|^2=1\} SU(2) SU(2) \exp(i\theta\mathbf{\hat{n}}\cdot\mathbf{\sigma}) \theta\in\mathbb{R} \mathbf{\hat{n}}\in S^2 \mathbf{\sigma} U(1) \exp(ix) x\in\mathbb{R} U(2)\simeq SU(2)\cup U(1) U(2) =\{\exp(ix) \exp(i\theta\mathbf{\hat{n}}\cdot\mathbf{\sigma}) \,:\,\{x,\theta\}\in\mathbb{R}\wedge\mathbf{\hat{n}}\in S^2 \} U(2) =\{\exp(ix)I+ \exp(i\theta\mathbf{\hat{n}}\cdot\mathbf{\sigma}) \,:\,\{x,\theta\}\in\mathbb{R}\wedge\mathbf{\hat{n}}\in S^2 \} U(2)\simeq \langle SU(2)\cup U(1)\rangle \langle A\rangle A","['linear-algebra', 'group-theory', 'lie-groups']"
78,Show that a positive operator is also hermitian,Show that a positive operator is also hermitian,,"I'm having a little difficulty with this. Given some positive operator $A$, show that it is also hermitian. (A positive operator is defined as $\langle Ax,x\rangle\ge 0$ for all $x \in V$ where $V$ is some vector space.) Here's what I have so far. We can construct $A = B + iC$ where $B,C$ are hermitian operators $B = (A + A^*)/2$, $C = (-iA + iA^*)/2$ where $^*$ is the conjugate transpose. I'm trying to show that $B$ and $C$ are diagonalizable by the same vectors, and that the eigenvalues of $C$ are $0$. I'm not sure how to do this though.","I'm having a little difficulty with this. Given some positive operator $A$, show that it is also hermitian. (A positive operator is defined as $\langle Ax,x\rangle\ge 0$ for all $x \in V$ where $V$ is some vector space.) Here's what I have so far. We can construct $A = B + iC$ where $B,C$ are hermitian operators $B = (A + A^*)/2$, $C = (-iA + iA^*)/2$ where $^*$ is the conjugate transpose. I'm trying to show that $B$ and $C$ are diagonalizable by the same vectors, and that the eigenvalues of $C$ are $0$. I'm not sure how to do this though.",,['linear-algebra']
79,Finding the Jordan canonical form of this upper triangular $3\times3$ matrix,Finding the Jordan canonical form of this upper triangular  matrix,3\times3,"I am supposed to find the Jordan canonical form of a couple of matrices, but I was absent for a few lectures. \begin{bmatrix}  1 & 1 & 0 \\  0 & 1 & 2 \\ 0 & 0 & 3 \end{bmatrix} Since this is an upper triangular matrix, its eigenvalues are the diagonal entries. Hence $\lambda_{1,2}=1$ and $\lambda_3 = 3$, with corresponding eigenvectors $(1,2,2)$ and $(1,0,0)$. Now what? I do not know how to proceed, nor what it means that my matrix is built up by Jordan blocks.","I am supposed to find the Jordan canonical form of a couple of matrices, but I was absent for a few lectures. \begin{bmatrix}  1 & 1 & 0 \\  0 & 1 & 2 \\ 0 & 0 & 3 \end{bmatrix} Since this is an upper triangular matrix, its eigenvalues are the diagonal entries. Hence $\lambda_{1,2}=1$ and $\lambda_3 = 3$, with corresponding eigenvectors $(1,2,2)$ and $(1,0,0)$. Now what? I do not know how to proceed, nor what it means that my matrix is built up by Jordan blocks.",,"['linear-algebra', 'matrices', 'jordan-normal-form']"
80,$A$ and $A^2$ have same characteristic polynomial,and  have same characteristic polynomial,A A^2,"Is it possible to have a non-identity $2 \times 2$ diagonalizable, invertible, complex matrix $A$ s.t characteristics polynomials of $A$ and $A^2$ are the same? I am not getting any hint even how to create one. I can start with two different eigenvalues but for this, we won't have the same characteristic poly. I was also trying to play with $$     \begin{pmatrix}     0 & i  \\     i & 0  \\    \end{pmatrix} $$ Does not help.","Is it possible to have a non-identity diagonalizable, invertible, complex matrix s.t characteristics polynomials of and are the same? I am not getting any hint even how to create one. I can start with two different eigenvalues but for this, we won't have the same characteristic poly. I was also trying to play with Does not help.","2 \times 2 A A A^2 
    \begin{pmatrix}
    0 & i  \\
    i & 0  \\
   \end{pmatrix}
","['linear-algebra', 'matrices', 'complex-numbers', 'diagonalization', 'characteristic-functions']"
81,does a rectangular matrix have an inverse?,does a rectangular matrix have an inverse?,,"I know all square matrices have easily to identify inverses, but does that continue on with rectangular matrices?","I know all square matrices have easily to identify inverses, but does that continue on with rectangular matrices?",,['linear-algebra']
82,Are linear combinations of powers of a matrix unique?,Are linear combinations of powers of a matrix unique?,,"One can see from the Cayley-Hamilton Theorem that for a $n\times n$ matrix, we can write any power of the matrix as a linear combination of lesser powers and the identity matrix, say if $A\neq cI_n$, $c\in \Bbb{C}$ is a given matrix, it can be written as a linear combination of $I_n,A^{-1},A,A^2,\cdots,A^{n-1}$. Is this representation unique? If so, is it under special cases? As an example let's consider this: Let $A\neq cI_n$ , $c\in \Bbb{C}$ be a $3\times 3$ matrix over $\Bbb{C}$ and $A^3=k_3A^2+k_2A+k_1I_3=m_3A^2+m_2A+m_1I_3$ for $k_i,m_i\in \Bbb{C}$. Does it hold that $k_i=m_i \forall i=1,2,3?$ If we take a formalistic approach I guess we can say that the equivalence above consitutes a system of $3$ equations with $6$ variables ($k_i,m_i$). That is, we evaluate the two linear combinations and by matrix equivalence we demand that all entries are equal: $a_{ij}=b_{ij}=c_{ij}$. But in general, unless the matrix $A$ has a ""special"" form, that will have infinite solutions. Is this-overly simplistic-approach correct? Should I try to find counter examples perhaps or the question has some trivial answer that I overlook?","One can see from the Cayley-Hamilton Theorem that for a $n\times n$ matrix, we can write any power of the matrix as a linear combination of lesser powers and the identity matrix, say if $A\neq cI_n$, $c\in \Bbb{C}$ is a given matrix, it can be written as a linear combination of $I_n,A^{-1},A,A^2,\cdots,A^{n-1}$. Is this representation unique? If so, is it under special cases? As an example let's consider this: Let $A\neq cI_n$ , $c\in \Bbb{C}$ be a $3\times 3$ matrix over $\Bbb{C}$ and $A^3=k_3A^2+k_2A+k_1I_3=m_3A^2+m_2A+m_1I_3$ for $k_i,m_i\in \Bbb{C}$. Does it hold that $k_i=m_i \forall i=1,2,3?$ If we take a formalistic approach I guess we can say that the equivalence above consitutes a system of $3$ equations with $6$ variables ($k_i,m_i$). That is, we evaluate the two linear combinations and by matrix equivalence we demand that all entries are equal: $a_{ij}=b_{ij}=c_{ij}$. But in general, unless the matrix $A$ has a ""special"" form, that will have infinite solutions. Is this-overly simplistic-approach correct? Should I try to find counter examples perhaps or the question has some trivial answer that I overlook?",,"['linear-algebra', 'matrices']"
83,Prove that $\det(A^2 + A + xI) = x$,Prove that,\det(A^2 + A + xI) = x,Let $x > 0$ and $A \in \mathbb R^{2 \times 2}$ satisfy $\det(A^2 + xI) = 0$. Prove that $$\det(A^2 + A + xI) = x$$ I have tried something with characteristic polynomial and eigenvalues but it did not work. Can you give me a hint to solve this problem?,Let $x > 0$ and $A \in \mathbb R^{2 \times 2}$ satisfy $\det(A^2 + xI) = 0$. Prove that $$\det(A^2 + A + xI) = x$$ I have tried something with characteristic polynomial and eigenvalues but it did not work. Can you give me a hint to solve this problem?,,"['linear-algebra', 'matrices', 'eigenvalues-eigenvectors', 'determinant']"
84,How to prove that $A^2$ is a symmetric matrix?,How to prove that  is a symmetric matrix?,A^2,"Conjecture 1 : Let $A$ be a real matrix such that $A^5=A A^T A A^T A$.  Then $A^2$ is a symmetric matrix. (here $A^T$ denotes the transpose of a matrix A). I guess that the following is also true : Conjecture 2 : If $A^{2n+1}=AA^TAA^T\cdots AA^TA$ then    $A^n $ is symmetric. PS: This second conjecture has been shown to be false when $A$ is invertible, see Robert Israel's answer below. But I still think that it's true when $A$ is not invertible. The post if matrix such $AA^T=A^2$ then $A$ is symmetric? solves the $n=1$ case.","Conjecture 1 : Let $A$ be a real matrix such that $A^5=A A^T A A^T A$.  Then $A^2$ is a symmetric matrix. (here $A^T$ denotes the transpose of a matrix A). I guess that the following is also true : Conjecture 2 : If $A^{2n+1}=AA^TAA^T\cdots AA^TA$ then    $A^n $ is symmetric. PS: This second conjecture has been shown to be false when $A$ is invertible, see Robert Israel's answer below. But I still think that it's true when $A$ is not invertible. The post if matrix such $AA^T=A^2$ then $A$ is symmetric? solves the $n=1$ case.",,"['linear-algebra', 'matrices', 'symmetric-matrices']"
85,Efficient computation of the minimum distance of a binary linear code,Efficient computation of the minimum distance of a binary linear code,,"I need to find parameters $n$, $k$ and $d$ of a binary linear code from its Generator Matrix. How can I find parameter $d$ efficiently? I know the method that compute all the codewords and take minimum non-zero weight code will be the minimum distance. but it's an exponential time algorithm. Is there any efficient algorithm to do the same?","I need to find parameters $n$, $k$ and $d$ of a binary linear code from its Generator Matrix. How can I find parameter $d$ efficiently? I know the method that compute all the codewords and take minimum non-zero weight code will be the minimum distance. but it's an exponential time algorithm. Is there any efficient algorithm to do the same?",,"['linear-algebra', 'combinatorics', 'algorithms', 'computational-complexity', 'coding-theory']"
86,Eigenvalues of tridiagonal symmetric matrix with diagonal entries $2$ and subdiagonal entries $1$,Eigenvalues of tridiagonal symmetric matrix with diagonal entries  and subdiagonal entries,2 1,"Let $A$ be a square matrix with all diagonal entries equal to $2$ , all entries directly above or below the main diagonal equal to $1$ , and all other entries equal to $0$ . Show that every eigenvalue of $A$ is a real number strictly between $0$ and $4$ . Attempt at solution: Since $A$ is real and symmetric, we already know that its eigenvalues are real numbers. Since the entries in the diagonal of $A$ are all positive (all $2$ ), $A$ is positive definite iff the determinants of all the upper left-hand corners of $A$ are positive. I think this can be proven via induction (showing that each time the dimension goes up, the determinant goes up too) Since A is symmetric and positive definite, eigenvalues are positive, i.e. greater than $0$ . But I can't get the upper bound of $4$ . Any help would be appreciated. Thank you.","Let be a square matrix with all diagonal entries equal to , all entries directly above or below the main diagonal equal to , and all other entries equal to . Show that every eigenvalue of is a real number strictly between and . Attempt at solution: Since is real and symmetric, we already know that its eigenvalues are real numbers. Since the entries in the diagonal of are all positive (all ), is positive definite iff the determinants of all the upper left-hand corners of are positive. I think this can be proven via induction (showing that each time the dimension goes up, the determinant goes up too) Since A is symmetric and positive definite, eigenvalues are positive, i.e. greater than . But I can't get the upper bound of . Any help would be appreciated. Thank you.",A 2 1 0 A 0 4 A A 2 A A 0 4,"['linear-algebra', 'matrices', 'eigenvalues-eigenvectors', 'tridiagonal-matrices', 'toeplitz-matrices']"
87,Question about basis and finite dimensional vector space,Question about basis and finite dimensional vector space,,"I have seen the statement ""Every finite dimensional vector space has a basis."" ( Here on page 5) I'm confused about what this tells me. It seems to tell me nothing: by definition, the dimension of a vector space is the number of elements in a basis of it. Then saying a vector space is finite dimensional is the same as saying that it has a basis. Or are there any other definitions of dimension than the number of basis elements?","I have seen the statement ""Every finite dimensional vector space has a basis."" ( Here on page 5) I'm confused about what this tells me. It seems to tell me nothing: by definition, the dimension of a vector space is the number of elements in a basis of it. Then saying a vector space is finite dimensional is the same as saying that it has a basis. Or are there any other definitions of dimension than the number of basis elements?",,"['linear-algebra', 'vector-spaces', 'definition']"
88,Categorification of characteristic polynomial,Categorification of characteristic polynomial,,"It's probably just my applied background talking, but I'm puzzled by characteristic polynomials of matrices. Useful things like that are usually closely connected to some nice functor or homomorphism, but I can't think of anything off the top of my head. So my question is: Is there a categorification of characteristic polynomial?","It's probably just my applied background talking, but I'm puzzled by characteristic polynomials of matrices. Useful things like that are usually closely connected to some nice functor or homomorphism, but I can't think of anything off the top of my head. So my question is: Is there a categorification of characteristic polynomial?",,"['linear-algebra', 'soft-question']"
89,What are the finite subgroups of $SU_2(C)$?,What are the finite subgroups of ?,SU_2(C),"Is there any reference which classifies the finite subgroups of $SU_2(C)$ up to conjugacy ? What I know is that lifting the finite subgroups of $SO_3(R)$ by the map $SU_2(C) \rightarrow PSU_2(C)$ gives rise to the groups : cyclic groups of even order, dicyclic groups, binary tetrahedral/octahedral/icosahedral groups. But I dont know if they are the only one.","Is there any reference which classifies the finite subgroups of $SU_2(C)$ up to conjugacy ? What I know is that lifting the finite subgroups of $SO_3(R)$ by the map $SU_2(C) \rightarrow PSU_2(C)$ gives rise to the groups : cyclic groups of even order, dicyclic groups, binary tetrahedral/octahedral/icosahedral groups. But I dont know if they are the only one.",,"['linear-algebra', 'group-theory', 'finite-groups']"
90,What's wrong with manipulating this algebraic equation? and why does a manipulated system of equations have a different solution than the original?,What's wrong with manipulating this algebraic equation? and why does a manipulated system of equations have a different solution than the original?,,"I'll give an example for my first question: $x^2 + x + 1 = 0$ Clearly $x = 0$ and $x = 1$ aren't solutions, so first we can safely divide by $x$ : $x + 1 + 1/x = 0$ By subtracting $1/x$ from both sides we get: $x + 1 = -1/x$ By plugging the value $x + 1$ back we get: $x^2 - 1/x = 0$ Multiplying by $x$ and adding $1$ to both sides: $x^3 = 1$ Which $x = 1$ is clearly a solution to, unlike the original equation. I have a problem with this, all manipulations did not include dividing by zero or any non-defined operations, all what was done is expressing $x$ by a different way so why does it change the final solution? When we divided the equation by $x$ the solution shouldn't change which means the first equation holds so why isn't the second one compatible with the first? On another note, I have a different question that is slightly related to this one, an example for this one is: We have the system of equations $S$ which is: $x + y + z = 1 \quad (L1)$ $x + y - z = 1/2 \quad (L2)$ $x - y + z = -4 \quad (L3)$ Then we transform this system into $S'$ by manipulating equations together: $2x + 2y = 3 \quad (L1 + L2)$ $2y - 2z = 6 \quad (L2 - L3)$ $2x + 2z = -3 \quad (L1 + L3)$ The solution for $S$ (which is $(-3/2, 5/2, 0)$ ) also doesn't satisfy $S'$ , which is counterintuitive to me because that's the kind of transformations we're taught then why does it change the solution? For both questions are those like general phenomenons or are they just special cases? What are the names of topics concerned with the act of transforming equations like the examples above?","I'll give an example for my first question: Clearly and aren't solutions, so first we can safely divide by : By subtracting from both sides we get: By plugging the value back we get: Multiplying by and adding to both sides: Which is clearly a solution to, unlike the original equation. I have a problem with this, all manipulations did not include dividing by zero or any non-defined operations, all what was done is expressing by a different way so why does it change the final solution? When we divided the equation by the solution shouldn't change which means the first equation holds so why isn't the second one compatible with the first? On another note, I have a different question that is slightly related to this one, an example for this one is: We have the system of equations which is: Then we transform this system into by manipulating equations together: The solution for (which is ) also doesn't satisfy , which is counterintuitive to me because that's the kind of transformations we're taught then why does it change the solution? For both questions are those like general phenomenons or are they just special cases? What are the names of topics concerned with the act of transforming equations like the examples above?","x^2 + x + 1 = 0 x = 0 x = 1 x x + 1 + 1/x = 0 1/x x + 1 = -1/x x + 1 x^2 - 1/x = 0 x 1 x^3 = 1 x = 1 x x S x + y + z = 1 \quad (L1) x + y - z = 1/2 \quad (L2) x - y + z = -4 \quad (L3) S' 2x + 2y = 3 \quad (L1 + L2) 2y - 2z = 6 \quad (L2 - L3) 2x + 2z = -3 \quad (L1 + L3) S (-3/2, 5/2, 0) S'","['linear-algebra', 'polynomials', 'linear-transformations', 'systems-of-equations', 'algebraic-equations']"
91,Transpose matrix dual map,Transpose matrix dual map,,"How do I see that the representing matrix of the dual map $f^*$ between finite-dimensional dual spaces is given by the transpose of the representing matrix of $f$ ? Here I want to assume that the matrix $f^*$ is represented with respect to the dual basis. Apparently this result is very well-known, but I would like to see proof of this.","How do I see that the representing matrix of the dual map between finite-dimensional dual spaces is given by the transpose of the representing matrix of ? Here I want to assume that the matrix is represented with respect to the dual basis. Apparently this result is very well-known, but I would like to see proof of this.",f^* f f^*,['linear-algebra']
92,"Showing AB=0 does not imply either A,B=0, but that singular","Showing AB=0 does not imply either A,B=0, but that singular",,"Ex. 8.5 - Mathematical Methods for Physics and Engineering (Riley) By considering the matrices   $$ A = \left( \begin{matrix}         1 & 0 \\         0 & 0 \\         \end{matrix} \right)  \text{ ,  } B = \left( \begin{matrix}         0 & 0 \\         3 & 4 \\         \end{matrix} \right)  $$   show that $AB = 0$ does not imply that either $A$ or $B$ is the zero matrix, but that it does imply that at least one of them is singular. So, my reasoning was the following: It's not difficult to compute $AB = \left( \begin{matrix}         0 & 0 \\         0 & 0 \\         \end{matrix} \right) $, in fact it's really even implied in the question. So, assume that $A, B$ are each non-singular - i.e. they are invertible. Thus, $A^{-1}AB=B$, and $ABB^{-1}=A$. But $AB$ is a zero matrix, so $A=B=0$. Thus proven that the initial assumption $A, B$ are non-singular is false. Is my reasoning correct? I ask because the 'hints and answers' said simply ""Use the property of the determinant of a matrix product."" While I don't expect there to be only one proof, it's a tad disconcerting for it to so flatly suggest a different method - I'm not nearly confident enough in my ability to disregard it.","Ex. 8.5 - Mathematical Methods for Physics and Engineering (Riley) By considering the matrices   $$ A = \left( \begin{matrix}         1 & 0 \\         0 & 0 \\         \end{matrix} \right)  \text{ ,  } B = \left( \begin{matrix}         0 & 0 \\         3 & 4 \\         \end{matrix} \right)  $$   show that $AB = 0$ does not imply that either $A$ or $B$ is the zero matrix, but that it does imply that at least one of them is singular. So, my reasoning was the following: It's not difficult to compute $AB = \left( \begin{matrix}         0 & 0 \\         0 & 0 \\         \end{matrix} \right) $, in fact it's really even implied in the question. So, assume that $A, B$ are each non-singular - i.e. they are invertible. Thus, $A^{-1}AB=B$, and $ABB^{-1}=A$. But $AB$ is a zero matrix, so $A=B=0$. Thus proven that the initial assumption $A, B$ are non-singular is false. Is my reasoning correct? I ask because the 'hints and answers' said simply ""Use the property of the determinant of a matrix product."" While I don't expect there to be only one proof, it's a tad disconcerting for it to so flatly suggest a different method - I'm not nearly confident enough in my ability to disregard it.",,"['linear-algebra', 'matrices', 'proof-verification']"
93,Optimization problem (in linear algebra course!),Optimization problem (in linear algebra course!),,"Let $a_1, a_2, \ldots, a_n$ be real numbers such that $a_1 + \cdots + a_n = 0$ and $a_1^2 + \cdots +a_n^2 = 1$.  What is the maximum value of $a_1a_2 + a_2a_3 + \cdots + a_{n - 1}a_n  + a_na_1$? I'd like to emphasize that this was found in a random linear algebra exercise sheet so one might expect that there exists a clever solution based on matrix manipulation... Personally I tried to conceive the problem geometrically and analytically. Notably $n=1$ has no meaning, $n=2$ gives $-1$, $n=3$ gives $-1/2$. This did not reveal much about the general case except for the fact that Lagrangian (system of partial derivatives and all that jazz) seems to imply that ANY combination satisfying the constraints gives the same value (value I'm trying to maximize) - but this needs some further checking. Back to the linear algebra I see traces of matrices, but I need to somewhat simply express $A$ and $B$ (see below) in terms of one another before anything useful can be done... $$A = \operatorname{diag}(a_1,a_2,\ldots,a_{n-1},a_n);$$ $$B = \operatorname{diag}(a_2,a_3,\ldots,a_{n-1},a_n,a_1);$$ P.S. The problem was originally taken from this very forum but it's quite and old post and I don't seem to be able to leave a comment there.","Let $a_1, a_2, \ldots, a_n$ be real numbers such that $a_1 + \cdots + a_n = 0$ and $a_1^2 + \cdots +a_n^2 = 1$.  What is the maximum value of $a_1a_2 + a_2a_3 + \cdots + a_{n - 1}a_n  + a_na_1$? I'd like to emphasize that this was found in a random linear algebra exercise sheet so one might expect that there exists a clever solution based on matrix manipulation... Personally I tried to conceive the problem geometrically and analytically. Notably $n=1$ has no meaning, $n=2$ gives $-1$, $n=3$ gives $-1/2$. This did not reveal much about the general case except for the fact that Lagrangian (system of partial derivatives and all that jazz) seems to imply that ANY combination satisfying the constraints gives the same value (value I'm trying to maximize) - but this needs some further checking. Back to the linear algebra I see traces of matrices, but I need to somewhat simply express $A$ and $B$ (see below) in terms of one another before anything useful can be done... $$A = \operatorname{diag}(a_1,a_2,\ldots,a_{n-1},a_n);$$ $$B = \operatorname{diag}(a_2,a_3,\ldots,a_{n-1},a_n,a_1);$$ P.S. The problem was originally taken from this very forum but it's quite and old post and I don't seem to be able to leave a comment there.",,"['linear-algebra', 'optimization']"
94,Definition of Affine Independence in Brondsted's Convex Polytopes?,Definition of Affine Independence in Brondsted's Convex Polytopes?,,"At one point in the book (An Introduction to Convex Polytopes, by Arne Brondsted) a definition of affine independence is given as follows, An n-family $(x_{1},...,x_{n})$ of points from $\mathbb{R}^d$ is said to be affinely independent if a linear combination $\lambda_{1} x_{1} + ... + \lambda_{n} x_{n}$ with $\lambda_{1} + ... + \lambda_{n} = 0$ can only have the value zero vector when $\lambda_{1}=...=\lambda_{n}=0$. It is my hunch that affine independence is analogous to linear independence in that, a set of vectors is (affinely/linearly) independent if none of the vectors is an (affine/linear) combination of the others. If this is the case, then what does the condition $\lambda_{1} +... +\lambda_{n} = 0$ have to do with anything? Shouldn't it be that the linear combination $\lambda_{1}x_{1} +...+\lambda_{n}x_{n}$, with $\lambda_{1} + ... +\lambda_{n} =1$ can only have the value zero vector when $\lambda_{1}=...=\lambda_{n}=0$?","At one point in the book (An Introduction to Convex Polytopes, by Arne Brondsted) a definition of affine independence is given as follows, An n-family $(x_{1},...,x_{n})$ of points from $\mathbb{R}^d$ is said to be affinely independent if a linear combination $\lambda_{1} x_{1} + ... + \lambda_{n} x_{n}$ with $\lambda_{1} + ... + \lambda_{n} = 0$ can only have the value zero vector when $\lambda_{1}=...=\lambda_{n}=0$. It is my hunch that affine independence is analogous to linear independence in that, a set of vectors is (affinely/linearly) independent if none of the vectors is an (affine/linear) combination of the others. If this is the case, then what does the condition $\lambda_{1} +... +\lambda_{n} = 0$ have to do with anything? Shouldn't it be that the linear combination $\lambda_{1}x_{1} +...+\lambda_{n}x_{n}$, with $\lambda_{1} + ... +\lambda_{n} =1$ can only have the value zero vector when $\lambda_{1}=...=\lambda_{n}=0$?",,"['linear-algebra', 'affine-geometry']"
95,Motivating linear algebra for economics students?,Motivating linear algebra for economics students?,,"I'm a tutor for the introductory linear algebra course at my school; this course is required for most upper division economics classes, so a lot of my tutees are economics majors. This is a typical linear algebra course that focuses on things like linear dependence, subspaces, eigenvalues, etc. and does not spend time on ""practical applications"".  As a result, a lot of the economics students have no idea why they should be taking the course.  Since I don't know the first thing about economics, I also have no idea why they should be taking the course. Is it possible to convince economics students that linear algebra is important for their field?  More precisely: Are there any motivating examples where linear algebra is used in economics?","I'm a tutor for the introductory linear algebra course at my school; this course is required for most upper division economics classes, so a lot of my tutees are economics majors. This is a typical linear algebra course that focuses on things like linear dependence, subspaces, eigenvalues, etc. and does not spend time on ""practical applications"".  As a result, a lot of the economics students have no idea why they should be taking the course.  Since I don't know the first thing about economics, I also have no idea why they should be taking the course. Is it possible to convince economics students that linear algebra is important for their field?  More precisely: Are there any motivating examples where linear algebra is used in economics?",,"['linear-algebra', 'education', 'economics']"
96,True of False: Every 3-dimensional subspace of $ \Bbb R^{2 \times 2}$ contains at least one invertible matrix. [duplicate],True of False: Every 3-dimensional subspace of  contains at least one invertible matrix. [duplicate], \Bbb R^{2 \times 2},"This question already has answers here : Max dimension of a subspace of singular $n\times n$ matrices (2 answers) Closed 7 years ago . The true or false question states: ""True of False: Every 3-dimensional subspace of $ \Bbb R^{2 \times 2}$ contains at least one invertible matrix."" Here the $ \Bbb R^{2 \times 2}$ represents the space of all two by two matrices. It seems like this is true, but I am not sure how to prove or disprove the statement. (If such is true, then it's easy to see that every 3-dimensional subspace of $ \Bbb R^{2 \times 2}$ contains infinitely many invertible matrices)","This question already has answers here : Max dimension of a subspace of singular $n\times n$ matrices (2 answers) Closed 7 years ago . The true or false question states: ""True of False: Every 3-dimensional subspace of $ \Bbb R^{2 \times 2}$ contains at least one invertible matrix."" Here the $ \Bbb R^{2 \times 2}$ represents the space of all two by two matrices. It seems like this is true, but I am not sure how to prove or disprove the statement. (If such is true, then it's easy to see that every 3-dimensional subspace of $ \Bbb R^{2 \times 2}$ contains infinitely many invertible matrices)",,['linear-algebra']
97,Proof of transpose property of matrix exponential,Proof of transpose property of matrix exponential,,"Using the fact that the matrix transpose distributes over infinite sums to show that $e^{(A^T)} = (e^A)^T$.  I feel like this is really trivial, but I don't know quite how to prove this.  How would I go about proving it?","Using the fact that the matrix transpose distributes over infinite sums to show that $e^{(A^T)} = (e^A)^T$.  I feel like this is really trivial, but I don't know quite how to prove this.  How would I go about proving it?",,"['linear-algebra', 'matrices', 'exponential-function', 'matrix-equations', 'matrix-calculus']"
98,Show that $A=0 \iff \operatorname{tr}(A)=0$ where $A= M_1+ \cdots +M_{\ell}$. [duplicate],Show that  where . [duplicate],A=0 \iff \operatorname{tr}(A)=0 A= M_1+ \cdots +M_{\ell},"This question already has an answer here : A Representation Theory Problem in Putnam Competition (1 answer) Closed 5 years ago . Let $G=\{M_1, M_2, \ldots ,M_{\ell}\} \subset \mathcal{M}_n(\mathbb{R})$, such that $G$ forms a group for the usual matrix multiplication. Denote $A= M_1+ \cdots +M_{\ell}$. Show that  $$A=0 \iff \operatorname{tr}(A)=0$$ I am totally stuck here, if someone has any ideas, please share it. Thank you in advance.","This question already has an answer here : A Representation Theory Problem in Putnam Competition (1 answer) Closed 5 years ago . Let $G=\{M_1, M_2, \ldots ,M_{\ell}\} \subset \mathcal{M}_n(\mathbb{R})$, such that $G$ forms a group for the usual matrix multiplication. Denote $A= M_1+ \cdots +M_{\ell}$. Show that  $$A=0 \iff \operatorname{tr}(A)=0$$ I am totally stuck here, if someone has any ideas, please share it. Thank you in advance.",,['linear-algebra']
99,Lagrange basis functions as bases of Polynomials Space,Lagrange basis functions as bases of Polynomials Space,,"Suppose $L$ be a Vector Space of Polynomials of $x$ of degree $\leq n-1$ with coefficients in the field $\mathbb{K}$. Define $$g_i(x) :=\prod _ {{j=1},{j\neq i}}^n \frac{x-a_j}{a_i-a_j}$$ Show that the polynomials $g_1(x), g_2(x),...,g_n(x)$ form a basis of L. Furthermore, show that coordinates of polynomial $f$ in this basis are $\{f(a_1),f(a_2),...,f(a_n)\}.$ To show that the polynomials are the bases, I need to show that they span $L$ and that they are linearly independent. I thought showing that any element in the set $\{1,x,x^2,...,x^{n-1}\}$ belongs to the span of $\{g_1(x), g_2(x),...,g_n(x)\}$ would be enough to show the $g_1(x), g_2(x),...,g_n(x)$ spans $L.$ But I don't know how to do this! Also, linear independence seems to be tougher!","Suppose $L$ be a Vector Space of Polynomials of $x$ of degree $\leq n-1$ with coefficients in the field $\mathbb{K}$. Define $$g_i(x) :=\prod _ {{j=1},{j\neq i}}^n \frac{x-a_j}{a_i-a_j}$$ Show that the polynomials $g_1(x), g_2(x),...,g_n(x)$ form a basis of L. Furthermore, show that coordinates of polynomial $f$ in this basis are $\{f(a_1),f(a_2),...,f(a_n)\}.$ To show that the polynomials are the bases, I need to show that they span $L$ and that they are linearly independent. I thought showing that any element in the set $\{1,x,x^2,...,x^{n-1}\}$ belongs to the span of $\{g_1(x), g_2(x),...,g_n(x)\}$ would be enough to show the $g_1(x), g_2(x),...,g_n(x)$ spans $L.$ But I don't know how to do this! Also, linear independence seems to be tougher!",,"['linear-algebra', 'polynomials']"
