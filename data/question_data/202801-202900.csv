,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,Nontrivial 'classical' examples of contracting differential transformations on compact Riemannian manifolds,Nontrivial 'classical' examples of contracting differential transformations on compact Riemannian manifolds,,"Let $M$ be a Riemannian manifold. A differentiable map $f:M\to M$ is called contracting (self-made definition) if the linear map $D_x f:T_x M \to T_x M$ has the operator norm bounded by $1$ w.r.t. the norms on $T_x M$ induced by Riemannian metric on it for all $x\in M$ . The only examples that I am aware of are either isometries or constant maps. I wonder if there are any relatively nontrivial, natural, classical examples of contracting maps on compact Riemannian manifolds. (Please don't worry too much about the words ""natural"" and ""classical"" too much...)","Let be a Riemannian manifold. A differentiable map is called contracting (self-made definition) if the linear map has the operator norm bounded by w.r.t. the norms on induced by Riemannian metric on it for all . The only examples that I am aware of are either isometries or constant maps. I wonder if there are any relatively nontrivial, natural, classical examples of contracting maps on compact Riemannian manifolds. (Please don't worry too much about the words ""natural"" and ""classical"" too much...)",M f:M\to M D_x f:T_x M \to T_x M 1 T_x M x\in M,"['differential-geometry', 'riemannian-geometry']"
1,"Some problems in Cheeger's ""A lower bound for the smallest eigenvalue of the Laplacian""","Some problems in Cheeger's ""A lower bound for the smallest eigenvalue of the Laplacian""",,"Pictures below is from Cheeger's "" A lower bound for the smallest eigenvalue of the Laplacian "" $f$ is the eigenfunction  of smallest eigenvalue  of Laplacian on $M$ , where $M$ is Riemannian manifold with $\partial M =\varnothing $ . 1, I don't know why $V(M_1)\le V(M_2)$ means that $h_1 \ge h$ . From the first below picture, the definition of $h_1$ and $h$ are different. I don't know how to get $h_1 \ge h$ ? 2, $M_1$ is a part of $M$ , in my view, $\dim M_1 = \dim M$ . There should be not any submanifold $A$ of $M$ such that $\partial A = M_1$ . 3, What is the critical levels ?  Is it the set of critical points ? Why the regions of $M_1$ lying between the critical levels of $f^2$ have a natural product strcture ? I can't see it. In fact, I don't know the image of level surface and orthogonal trajectories. PS: I feel my problem is too much to it is hard to answer. If so, is there any book contain the Cheeger's paper and not hard to read ?","Pictures below is from Cheeger's "" A lower bound for the smallest eigenvalue of the Laplacian "" is the eigenfunction  of smallest eigenvalue  of Laplacian on , where is Riemannian manifold with . 1, I don't know why means that . From the first below picture, the definition of and are different. I don't know how to get ? 2, is a part of , in my view, . There should be not any submanifold of such that . 3, What is the critical levels ?  Is it the set of critical points ? Why the regions of lying between the critical levels of have a natural product strcture ? I can't see it. In fact, I don't know the image of level surface and orthogonal trajectories. PS: I feel my problem is too much to it is hard to answer. If so, is there any book contain the Cheeger's paper and not hard to read ?",f M M \partial M =\varnothing  V(M_1)\le V(M_2) h_1 \ge h h_1 h h_1 \ge h M_1 M \dim M_1 = \dim M A M \partial A = M_1 M_1 f^2,"['differential-geometry', 'partial-differential-equations', 'eigenfunctions']"
2,Natural group action on mapping torus,Natural group action on mapping torus,,"Let $ (F,g) $ be a Riemannian manifold. Let $ G:=Iso(F,g) $ be the isometry group. Let $ M $ be the mapping torus of some isometry of $ F $ . So we have a bundle $$ F \to M \to S^1 $$ $ M $ has Riemannian cover $ F \times \mathbb{R} $ and there is natural action of $ G \times \mathbb{R} $ on $ F \times \mathbb{R} $ . If the mapping torus is trivial $ M\cong  F \times S^1 $ then this action on the cover descends to a natural action on the mapping torus. What if the mapping torus is nontrivial? When is there a natural action of $ G \times \mathbb{R} $ on $ M $ ? I am especially interested in the case where $ F $ is Riemannian homogeneous and this action on the mapping torus is transitive. I was inspired to ask this by a claim in this question https://mathoverflow.net/questions/410547/exact-condition-for-smooth-homogeneous-to-imply-riemannian-homogeneous and a similar claim in this question https://mathoverflow.net/questions/413409/mapping-torus-of-orientation-reversing-isometry-of-the-sphere that there is a natural action of the group $ O_{n+1}(\mathbb{R}) \times \mathbb{R} $ on the mapping torus of the antipodal map on $ S^n $ .",Let be a Riemannian manifold. Let be the isometry group. Let be the mapping torus of some isometry of . So we have a bundle has Riemannian cover and there is natural action of on . If the mapping torus is trivial then this action on the cover descends to a natural action on the mapping torus. What if the mapping torus is nontrivial? When is there a natural action of on ? I am especially interested in the case where is Riemannian homogeneous and this action on the mapping torus is transitive. I was inspired to ask this by a claim in this question https://mathoverflow.net/questions/410547/exact-condition-for-smooth-homogeneous-to-imply-riemannian-homogeneous and a similar claim in this question https://mathoverflow.net/questions/413409/mapping-torus-of-orientation-reversing-isometry-of-the-sphere that there is a natural action of the group on the mapping torus of the antipodal map on .," (F,g)   G:=Iso(F,g)   M   F  
F \to M \to S^1
  M   F \times \mathbb{R}   G \times \mathbb{R}   F \times \mathbb{R}   M\cong  F \times S^1   G \times \mathbb{R}   M   F   O_{n+1}(\mathbb{R}) \times \mathbb{R}   S^n ","['differential-geometry', 'riemannian-geometry', 'lie-groups', 'smooth-manifolds', 'geometric-topology']"
3,Understanding integration along fibers,Understanding integration along fibers,,"I understand the definition of integration of a differential form along fibers as it is stated in Wikipedia article as follows: Let $\pi :E \rightarrow B$ be a fiber bundle over a manifold with compact oriented fibers. If $\alpha$ is a $k$ -form on $E$ , then for tangent vectors $w_i$ 's at $b$ , let $ (\pi _{*}\alpha )_{b}(w_{1},... ,w_{k-m})=\int _{\pi ^{-1}(b)}\beta  $ where $\beta$ is the induced top-form on the fiber $\pi^{-1}(b)$ ; i.e., an $m$ -form given by: with $\widetilde{w_i}$ lifts of $w_i$ to $E$ , $$ \beta(v_1, \dots, v_m) = \alpha(v_1, \dots, v_m, \widetilde{w_1}, \dots, \widetilde{w_{k-m}}). $$ However, I don't intuitively understand why it is defined like this and don't know why it is called integration along fibers ?","I understand the definition of integration of a differential form along fibers as it is stated in Wikipedia article as follows: Let be a fiber bundle over a manifold with compact oriented fibers. If is a -form on , then for tangent vectors 's at , let where is the induced top-form on the fiber ; i.e., an -form given by: with lifts of to , However, I don't intuitively understand why it is defined like this and don't know why it is called integration along fibers ?","\pi :E \rightarrow B \alpha k E w_i b  (\pi _{*}\alpha )_{b}(w_{1},... ,w_{k-m})=\int _{\pi ^{-1}(b)}\beta   \beta \pi^{-1}(b) m \widetilde{w_i} w_i E 
\beta(v_1, \dots, v_m) = \alpha(v_1, \dots, v_m, \widetilde{w_1}, \dots, \widetilde{w_{k-m}}).
","['differential-geometry', 'differential-forms']"
4,"If $M$ is a $m$-dimensional smooth manifold, what is the rank of $\Omega^{m}(M)$ as a $\mathcal{C}^{\infty}(M)$-module?","If  is a -dimensional smooth manifold, what is the rank of  as a -module?",M m \Omega^{m}(M) \mathcal{C}^{\infty}(M),"It is clear to me that if $(U,\mathtt{x})$ is a chart of $M$ , then $\Omega^{m}(U)$ is a free $\mathcal{C}^{\infty}(U)$ -module of rank one. Is $\Omega^{m}(M)$ a free $\mathcal{C}^{\infty}(M)$ -module of rank one? More generally, is ${m}\choose{k}$ the rank of $\Omega^{k}(M)$ as a $\mathcal{C}^{\infty}(M)$ - module for $0\leq k \leq m$ ? I would also appreciate it if you could give me some reference text where I can look this up.","It is clear to me that if is a chart of , then is a free -module of rank one. Is a free -module of rank one? More generally, is the rank of as a - module for ? I would also appreciate it if you could give me some reference text where I can look this up.","(U,\mathtt{x}) M \Omega^{m}(U) \mathcal{C}^{\infty}(U) \Omega^{m}(M) \mathcal{C}^{\infty}(M) {m}\choose{k} \Omega^{k}(M) \mathcal{C}^{\infty}(M) 0\leq k \leq m","['differential-geometry', 'smooth-manifolds', 'differential-forms']"
5,Proving that equivalent atlases form an equivalence relation?,Proving that equivalent atlases form an equivalence relation?,,"I know that this question was already asked here , but I'm afraid I cannot really follow the answers (which are given by the OP her- or himself). So, here is a definition of equivalent atlases: Two atlases $\mathcal A$ and $\mathcal B$ on $M$ are called equivalent if $\mathcal A \cup \mathcal B$ is an atlas on $\mathcal M$ . And here a definition of atlas : Let $M$ be a second countable Hausdorff topological space. An $n$ - dimensional smooth atlas on $M$ is a collection of maps $$\mathcal A = \left\{ \left(\varphi_i, U_i\right) \mid i\in A\right\}, \quad \varphi_i: U_i\rightarrow \varphi_i(U_i)\subset \mathbb R^n,$$ such that all $U_i \subset M$ are open, all $\varphi_i$ are homeomorphisms, and $\{U_i, i\in I\}$ is an open covering of $\mathcal M$ $\varphi_i\circ \varphi_j^{-1}: \varphi_j\left(U_i\cap U_j\right)\rightarrow \varphi_i\left( U_i\cap U_j\right)$ are smooth for all $i, j\in I$ . Let $\mathcal A = \{(\varphi_i, U_i)\mid i\in A\}$ , $\mathcal B = \{(\psi_i, V_i)\mid i\in B\}$ and $\mathcal C = \{(\chi_i, W_i)\mid i\in C\}$ be atlases on the same set $M$ . Now, if $\mathcal A$ is an atlas, then obviously, $\mathcal A\cup \mathcal A = \mathcal A$ is also an atlas. $\mathcal A\cup \mathcal B$ is an atlas if and only if $\mathcal B \cup \mathcal A$ is an atlas. The tricky part is proving transitivity. Let $\mathcal A \cup \mathcal B$ be an atlas (i.e. the transition functions $\varphi_i\circ \psi_j^{-1}: \psi_j\left(U_i \cap V_j\right)\rightarrow\varphi_i\left(U_i\cap V_j\right)$ are smooth $\forall i\in A$ , $j\in B$ ), and let $\mathcal B\cup \mathcal C$ be an atlas (i.e. the transition functions $\psi_j\circ \chi_k^{-1}: \chi_k\left(V_j \cap W_k\right)\rightarrow\psi_j\left(V_j\cap W_k\right)$ are smooth $\forall j\in B$ , $k\in C$ ). For $\mathcal A\cup \mathcal C$ to be an atlas, we need to show that $$\varphi_i\circ \chi_k^{-1}: \chi_k\left(U_i \cap W_k\right)\rightarrow \varphi_i\left(U_i \cap W_k\right)\quad \text{smooth}\ \forall i\in A, k\in C.$$ My idea to prove this was sth like this: $$\varphi_i\circ \chi_{k}^{-1} = (\varphi_i\circ\psi_j^{-1})\circ \left(\psi_j \circ \chi_k^{-1}\right) \forall i\in A, j\in B, k\in C.$$ However, this raises the question of the well-definedness of the RHS. The codomain of $\chi_k^{-1}$ is $W_k$ , whereas the domain of $\psi_j$ is $V_j$ . How can we resolve this?","I know that this question was already asked here , but I'm afraid I cannot really follow the answers (which are given by the OP her- or himself). So, here is a definition of equivalent atlases: Two atlases and on are called equivalent if is an atlas on . And here a definition of atlas : Let be a second countable Hausdorff topological space. An - dimensional smooth atlas on is a collection of maps such that all are open, all are homeomorphisms, and is an open covering of are smooth for all . Let , and be atlases on the same set . Now, if is an atlas, then obviously, is also an atlas. is an atlas if and only if is an atlas. The tricky part is proving transitivity. Let be an atlas (i.e. the transition functions are smooth , ), and let be an atlas (i.e. the transition functions are smooth , ). For to be an atlas, we need to show that My idea to prove this was sth like this: However, this raises the question of the well-definedness of the RHS. The codomain of is , whereas the domain of is . How can we resolve this?","\mathcal A \mathcal B M \mathcal A \cup \mathcal B \mathcal M M n M \mathcal A = \left\{ \left(\varphi_i, U_i\right) \mid i\in A\right\}, \quad \varphi_i: U_i\rightarrow \varphi_i(U_i)\subset \mathbb R^n, U_i \subset M \varphi_i \{U_i, i\in I\} \mathcal M \varphi_i\circ \varphi_j^{-1}: \varphi_j\left(U_i\cap U_j\right)\rightarrow \varphi_i\left( U_i\cap U_j\right) i, j\in I \mathcal A = \{(\varphi_i, U_i)\mid i\in A\} \mathcal B = \{(\psi_i, V_i)\mid i\in B\} \mathcal C = \{(\chi_i, W_i)\mid i\in C\} M \mathcal A \mathcal A\cup \mathcal A = \mathcal A \mathcal A\cup \mathcal B \mathcal B \cup \mathcal A \mathcal A \cup \mathcal B \varphi_i\circ \psi_j^{-1}: \psi_j\left(U_i \cap V_j\right)\rightarrow\varphi_i\left(U_i\cap V_j\right) \forall i\in A j\in B \mathcal B\cup \mathcal C \psi_j\circ \chi_k^{-1}: \chi_k\left(V_j \cap W_k\right)\rightarrow\psi_j\left(V_j\cap W_k\right) \forall j\in B k\in C \mathcal A\cup \mathcal C \varphi_i\circ \chi_k^{-1}: \chi_k\left(U_i \cap W_k\right)\rightarrow \varphi_i\left(U_i \cap W_k\right)\quad \text{smooth}\ \forall i\in A, k\in C. \varphi_i\circ \chi_{k}^{-1} = (\varphi_i\circ\psi_j^{-1})\circ \left(\psi_j \circ \chi_k^{-1}\right) \forall i\in A, j\in B, k\in C. \chi_k^{-1} W_k \psi_j V_j","['differential-geometry', 'manifolds']"
6,"Definition of Developing Map of $(G,X)$-Manifold in Wikipedia",Definition of Developing Map of -Manifold in Wikipedia,"(G,X)","I was reading Wikipedia about $(G,X)$ -manifolds, and I do not understand well the developing map part. https://en.wikipedia.org/wiki/(G,X)-manifold#Developing_map Here is what Wikipedia says. Let $M$ be a connected $(G,X)$ -manifold, and let $\pi : \tilde{M} \to M$ be the universal covering. A developing map $\varphi : \tilde{M} \to X$ is defined as follows. Fix $p \in \tilde{M}$ . Let $q \in \tilde{M}$ be given. Fix a chart $\varphi : U \to X$ near $p$ . Consider a path $\gamma$ from $p$ to $q$ . We may use analytic continuation along $\gamma$ to extend $\varphi$ so that its domain includes $q$ (maybe $\gamma$ ?). Since $\tilde{M}$ is simply connected, $\varphi(q)$ does not depend on the choice of $\gamma$ . (The monodromy theorem ensures the well-definedness of $\varphi : \tilde{M} \to X$ .) At the step 3, why does the analytic continuation exist? How do we extend $\varphi$ along $\gamma$ ? Thank you.","I was reading Wikipedia about -manifolds, and I do not understand well the developing map part. https://en.wikipedia.org/wiki/(G,X)-manifold#Developing_map Here is what Wikipedia says. Let be a connected -manifold, and let be the universal covering. A developing map is defined as follows. Fix . Let be given. Fix a chart near . Consider a path from to . We may use analytic continuation along to extend so that its domain includes (maybe ?). Since is simply connected, does not depend on the choice of . (The monodromy theorem ensures the well-definedness of .) At the step 3, why does the analytic continuation exist? How do we extend along ? Thank you.","(G,X) M (G,X) \pi : \tilde{M} \to M \varphi : \tilde{M} \to X p \in \tilde{M} q \in \tilde{M} \varphi : U \to X p \gamma p q \gamma \varphi q \gamma \tilde{M} \varphi(q) \gamma \varphi : \tilde{M} \to X \varphi \gamma","['differential-geometry', 'manifolds', 'holonomy']"
7,What is differential probability? [closed],What is differential probability? [closed],,"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 2 years ago . Improve this question I am trying to understand the following statement (taken from a context related to volumetric image rendering): Density denotes the differential probability that a ray interacts with the volumetric “medium” of the scene at a particular point What does differential probability mean? (especially that density in this context is defined at any point in 3D space)","Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 2 years ago . Improve this question I am trying to understand the following statement (taken from a context related to volumetric image rendering): Density denotes the differential probability that a ray interacts with the volumetric “medium” of the scene at a particular point What does differential probability mean? (especially that density in this context is defined at any point in 3D space)",,"['probability', 'differential-geometry', 'density-function', 'differential']"
8,Number of line of curvature meets at one point,Number of line of curvature meets at one point,,"I am now consider the surface given by $$f=(x,y,x^3-3xy^2)$$ And I have been asked to prove that there are three line of curvature meet at origan point. I can compute that \begin{align*} 		f_x&=(1,0,3x^2-3y^2)\\ 		f_y&=(0,1,-6xy)\\ 		f_x\times f_y&=(-3(x^2-y^2),6xy,1)\\ 		|f_x\times f_y|&=\sqrt{9(x^2-y^2)^2+36x^2y^2+1}=\sqrt{9(x^2+y^2)^2+1}\\ 		n&=\frac{f_x\times f_y}{|f_x\times f_y|}=\frac{(-3(x^2-y^2),6xy,1)}{\sqrt{9(x^2+y^2)^2+1}}\\ 		f_{xx}&=(0,0,6x)\\ 		f_{xy}&=(0,0,-6y)\\ 		f_{yy}&=(0,0,-6x)\\ 		E&=<f_x,f_x>=1+9(x^2-y^2)^2\\ 		F&=<f_x,f_y>=-18(x^2-y^2)xy\\ 		G&=<f_y,f_y>=1+36x^2y^2\\ 		L&=<n,f_{xx}>=\frac{6x}{\sqrt{9(x^2+y^2)^2+1}}\\ 		M&=<n,f_{xy}>=\frac{-6y}{\sqrt{9(x^2+y^2)^2+1}}\\ 		N&=<n,f_{yy}>=\frac{-6x}{\sqrt{9(x^2+y^2)^2+1}}.\\ 	\end{align*} When $x=y=0$ , we have that $$(g)=\begin{pmatrix} 		E & F\\ 		F & G 	\end{pmatrix}=\begin{pmatrix} 		1 & 0\\ 		0 & 1 	\end{pmatrix}\mbox{ and }(b)=\begin{pmatrix} 		L & M\\ 		M & N 	\end{pmatrix}=\begin{pmatrix} 		0 & 0\\ 		0 & 0 	\end{pmatrix}.$$ Hence the shape operator $$(s)=(g^{-1})(b)=\begin{pmatrix} 		0 & 0\\ 		0 & 0 	\end{pmatrix}.$$ So the principal curvatures at point $x=y=0$ is that $k_1=k_2=0$ . I do not know how to continuous and I think I can not even calculate the principal direction at the origan and how can I know how the number of lines of curvature. But I have observed that this surface is mirror symmetrical to $(x,z)-$ plane. Does it have any relations? I have also calculated that a line of curvature $f\circ \gamma$ with $\gamma=\begin{pmatrix} \gamma_1\\ \gamma_2 \end{pmatrix}$ should fullfill the following equation $$\begin{vmatrix} \gamma_2^{\prime 2} & -\gamma_1^\prime \gamma_2^\prime & \gamma_2^{\prime 2}\\ E & F & G\\ L & M & N \end{vmatrix}=0$$ but how can I know a line of curvature cross the origan point?","I am now consider the surface given by And I have been asked to prove that there are three line of curvature meet at origan point. I can compute that When , we have that Hence the shape operator So the principal curvatures at point is that . I do not know how to continuous and I think I can not even calculate the principal direction at the origan and how can I know how the number of lines of curvature. But I have observed that this surface is mirror symmetrical to plane. Does it have any relations? I have also calculated that a line of curvature with should fullfill the following equation but how can I know a line of curvature cross the origan point?","f=(x,y,x^3-3xy^2) \begin{align*}
		f_x&=(1,0,3x^2-3y^2)\\
		f_y&=(0,1,-6xy)\\
		f_x\times f_y&=(-3(x^2-y^2),6xy,1)\\
		|f_x\times f_y|&=\sqrt{9(x^2-y^2)^2+36x^2y^2+1}=\sqrt{9(x^2+y^2)^2+1}\\
		n&=\frac{f_x\times f_y}{|f_x\times f_y|}=\frac{(-3(x^2-y^2),6xy,1)}{\sqrt{9(x^2+y^2)^2+1}}\\
		f_{xx}&=(0,0,6x)\\
		f_{xy}&=(0,0,-6y)\\
		f_{yy}&=(0,0,-6x)\\
		E&=<f_x,f_x>=1+9(x^2-y^2)^2\\
		F&=<f_x,f_y>=-18(x^2-y^2)xy\\
		G&=<f_y,f_y>=1+36x^2y^2\\
		L&=<n,f_{xx}>=\frac{6x}{\sqrt{9(x^2+y^2)^2+1}}\\
		M&=<n,f_{xy}>=\frac{-6y}{\sqrt{9(x^2+y^2)^2+1}}\\
		N&=<n,f_{yy}>=\frac{-6x}{\sqrt{9(x^2+y^2)^2+1}}.\\
	\end{align*} x=y=0 (g)=\begin{pmatrix}
		E & F\\
		F & G
	\end{pmatrix}=\begin{pmatrix}
		1 & 0\\
		0 & 1
	\end{pmatrix}\mbox{ and }(b)=\begin{pmatrix}
		L & M\\
		M & N
	\end{pmatrix}=\begin{pmatrix}
		0 & 0\\
		0 & 0
	\end{pmatrix}. (s)=(g^{-1})(b)=\begin{pmatrix}
		0 & 0\\
		0 & 0
	\end{pmatrix}. x=y=0 k_1=k_2=0 (x,z)- f\circ \gamma \gamma=\begin{pmatrix}
\gamma_1\\
\gamma_2
\end{pmatrix} \begin{vmatrix}
\gamma_2^{\prime 2} & -\gamma_1^\prime \gamma_2^\prime & \gamma_2^{\prime 2}\\
E & F & G\\
L & M & N \end{vmatrix}=0",['differential-geometry']
9,Proving that Randers norm is a Minkowski norm,Proving that Randers norm is a Minkowski norm,,"I am struggling to follow the proof that the Randers norm is a Minkowski norm from ""Lectures on Finsler Geometry"" by Zhongmin Shen. A Minkowski norm on finite dimensional vector space $V$ is a function $F:V\to[0,\infty)$ which has the following properties: $F$ is $C^{\infty}$ on $V\setminus\{0\}$ $F(\gamma y) = \gamma F(y)$ , for all $\gamma>0$ and $y\in V$ For any $y\in V\setminus \{0\}$ , the symmetric bilinear form $g_y$ on $V$ is positive definite, where $$\mathbf{g}_y (u,v):= \frac{1}{2}\frac{\partial^2}{\partial s \partial t}\bigg[ F^2(y+su+tv)\bigg] \biggr\rvert_{s=t=0}$$ A Randers norm on $V$ is defined as $R(y):=\alpha(y)+\beta(y)$ where $\alpha$ is the Euclidean norm and $\beta$ is a linear form. I am confused about how to show that the Randers norm satisfies the Minkowski norm's third condition. I think my confusion stems from unfamiliarity with Einstein tensor notation. The text's proof proceeds by fixing a basis $\{\mathbf{b}_i\}_{i=1}^n$ for $V$ and computing $$g_{ij}:= \mathbf{g}_y (\mathbf{b}_i\mathbf{b}_j)=\frac{1}{2}[F^2]_{y^{i}y^{j}}(y)$$ to be $$g_{ij} = \frac{F}{\alpha}\left(a_{ij}-\frac{y_i}{\alpha}\frac{y_j}{\alpha}\right)+\left(\frac{y_i}{\alpha}+b_i\right)\left(\frac{y_i}{\alpha}+b_j\right)$$ Any attempt to walk through/clarify this step is appreciated. Also, insight into why $\Vert\beta\Vert<1$ ensures the above is positive definite is desired. I gathered from context that when the index is in the upper position, we have a row vector instead of a column vector, but I'm missing a lot on how to actually perform the computations.","I am struggling to follow the proof that the Randers norm is a Minkowski norm from ""Lectures on Finsler Geometry"" by Zhongmin Shen. A Minkowski norm on finite dimensional vector space is a function which has the following properties: is on , for all and For any , the symmetric bilinear form on is positive definite, where A Randers norm on is defined as where is the Euclidean norm and is a linear form. I am confused about how to show that the Randers norm satisfies the Minkowski norm's third condition. I think my confusion stems from unfamiliarity with Einstein tensor notation. The text's proof proceeds by fixing a basis for and computing to be Any attempt to walk through/clarify this step is appreciated. Also, insight into why ensures the above is positive definite is desired. I gathered from context that when the index is in the upper position, we have a row vector instead of a column vector, but I'm missing a lot on how to actually perform the computations.","V F:V\to[0,\infty) F C^{\infty} V\setminus\{0\} F(\gamma y) = \gamma F(y) \gamma>0 y\in V y\in V\setminus \{0\} g_y V \mathbf{g}_y (u,v):= \frac{1}{2}\frac{\partial^2}{\partial s \partial t}\bigg[ F^2(y+su+tv)\bigg] \biggr\rvert_{s=t=0} V R(y):=\alpha(y)+\beta(y) \alpha \beta \{\mathbf{b}_i\}_{i=1}^n V g_{ij}:= \mathbf{g}_y (\mathbf{b}_i\mathbf{b}_j)=\frac{1}{2}[F^2]_{y^{i}y^{j}}(y) g_{ij} = \frac{F}{\alpha}\left(a_{ij}-\frac{y_i}{\alpha}\frac{y_j}{\alpha}\right)+\left(\frac{y_i}{\alpha}+b_i\right)\left(\frac{y_i}{\alpha}+b_j\right) \Vert\beta\Vert<1","['differential-geometry', 'metric-spaces', 'normed-spaces', 'finsler-geometry']"
10,Confusion with what the metric gives when mapping a surface to the complex plane,Confusion with what the metric gives when mapping a surface to the complex plane,,"Tristan Needham Visual Differential Geometry,pg-32 In the beginning, I understood the metric as the factors by which the length of displacement on surface and on the plane relate. But, in the book the following formula is given and suggests to me that separation vectors on the surface are related to separation vectors in the plane: $$ d \hat{s} = \lambda(z, \gamma)dz$$ With, $dz = e^{i \gamma} ds$ My doubt is that the separation vector on the surface is existing in the tangent plane at the base point $\hat{z}$ and is three dimensional, so how could it possibly be related to the complex separation vector $dz$ existing in the flat plane?","Tristan Needham Visual Differential Geometry,pg-32 In the beginning, I understood the metric as the factors by which the length of displacement on surface and on the plane relate. But, in the book the following formula is given and suggests to me that separation vectors on the surface are related to separation vectors in the plane: With, My doubt is that the separation vector on the surface is existing in the tangent plane at the base point and is three dimensional, so how could it possibly be related to the complex separation vector existing in the flat plane?"," d \hat{s} = \lambda(z, \gamma)dz dz = e^{i \gamma} ds \hat{z} dz",['differential-geometry']
11,A doubt about a proof of chain rule for smooth functions between smooth manifolds,A doubt about a proof of chain rule for smooth functions between smooth manifolds,,"I'm reading Theorem 1.1 in this lecture notes. Theorem 1.1 (Chain Rule for Manifolds). Suppose $f: X \rightarrow Y$ and $g: Y \rightarrow Z$ are smooth maps of manifolds. Then: $$ \mathrm{d}(g \circ f)_{x}=(\mathrm{d} g)_{f(x)} \circ(\mathrm{d} f)_{x} $$ Proof. If $\varphi$ is a local parameterisation of $x, \psi$ is a local parameterisation of $y=f(x)$ , and $\eta$ is a local parameterisation of $z=g(f(x))$ , then this is evident from: $$ g \circ f=\left[\eta \circ\left(\eta^{-1} \circ g \circ \psi\right) \circ \psi^{-1}\right] \circ\left[\psi \circ\left(\psi^{-1} \circ f \circ \varphi\right) \circ \varphi^{-1}\right] $$ and the usual chain rule from multivariate analysis (as we can differentiate the RHS as before using the usual chain rule). In the proof, $g \circ f = \Psi \circ \Phi$ with \begin{align} \Psi &:= \eta \circ\left(\eta^{-1} \circ g \circ \psi\right) \circ \psi^{-1} \\ \Phi &:= \psi \circ\left(\psi^{-1} \circ f \circ \varphi\right) \circ \varphi^{-1}. \end{align} The composition of smooth maps is also smooth, so $\Psi, \Phi$ are smooth. However, $\operatorname{dom} (\Psi) = \operatorname{dom} (\psi^{-1})$ which is open in $Y$ but not necessarily open in its ambient Euclidean space. The same situation holds for $\Phi$ . So $\Psi, \Phi$ are not differentiable in the usual sense, so we can not apply the chain rule on $\Psi \circ \Phi$ . However, the author said ""...we can differentiate the RHS as before using the usual chain rule..."". Could you please elaborate on my confusion?","I'm reading Theorem 1.1 in this lecture notes. Theorem 1.1 (Chain Rule for Manifolds). Suppose and are smooth maps of manifolds. Then: Proof. If is a local parameterisation of is a local parameterisation of , and is a local parameterisation of , then this is evident from: and the usual chain rule from multivariate analysis (as we can differentiate the RHS as before using the usual chain rule). In the proof, with The composition of smooth maps is also smooth, so are smooth. However, which is open in but not necessarily open in its ambient Euclidean space. The same situation holds for . So are not differentiable in the usual sense, so we can not apply the chain rule on . However, the author said ""...we can differentiate the RHS as before using the usual chain rule..."". Could you please elaborate on my confusion?","f: X \rightarrow Y g: Y \rightarrow Z 
\mathrm{d}(g \circ f)_{x}=(\mathrm{d} g)_{f(x)} \circ(\mathrm{d} f)_{x}
 \varphi x, \psi y=f(x) \eta z=g(f(x)) 
g \circ f=\left[\eta \circ\left(\eta^{-1} \circ g \circ \psi\right) \circ \psi^{-1}\right] \circ\left[\psi \circ\left(\psi^{-1} \circ f \circ \varphi\right) \circ \varphi^{-1}\right]
 g \circ f = \Psi \circ \Phi \begin{align}
\Psi &:= \eta \circ\left(\eta^{-1} \circ g \circ \psi\right) \circ \psi^{-1} \\
\Phi &:= \psi \circ\left(\psi^{-1} \circ f \circ \varphi\right) \circ \varphi^{-1}.
\end{align} \Psi, \Phi \operatorname{dom} (\Psi) = \operatorname{dom} (\psi^{-1}) Y \Phi \Psi, \Phi \Psi \circ \Phi","['differential-geometry', 'proof-explanation', 'smooth-manifolds', 'chain-rule']"
12,"""Second"" Lie derivative?","""Second"" Lie derivative?",,"Many of us are familiar with the standard definition of the Lie derivative of some smooth function $\varphi \in \Omega^{0}(X)$ as $$ \frac{d}{dt}(f_{t}^{*}\varphi) \big|_{t=0} =: L_{V}\varphi, $$ where $f_{t}$ is the flow generated by the vector field $V$ . My question is given this , can we compute $$ \frac{d^{2}}{dt^{2}}(f_{t}^{*}\varphi) \big|_{t=0} $$ in terms of $L_{V}\varphi$ ? My naive hunch is that this would simply be $L_{V}(L_{V}\varphi)$ , but I'm cautious about this and haven't found a convincing way of proving it. Is this true? Moreover, I wonder if whatever we compute this ""second derivative"" to be, should it hold not only for $\varphi \in \Omega^{0}(X)$ but for any $\omega \in \Omega^{k}(X)$ , $0 \leq k \leq \mathrm{dim}(X)$ ? That would be even nicer. Thanks in advance to anyone who thinks a bit about this and shares any ideas.","Many of us are familiar with the standard definition of the Lie derivative of some smooth function as where is the flow generated by the vector field . My question is given this , can we compute in terms of ? My naive hunch is that this would simply be , but I'm cautious about this and haven't found a convincing way of proving it. Is this true? Moreover, I wonder if whatever we compute this ""second derivative"" to be, should it hold not only for but for any , ? That would be even nicer. Thanks in advance to anyone who thinks a bit about this and shares any ideas.","\varphi \in \Omega^{0}(X) 
\frac{d}{dt}(f_{t}^{*}\varphi) \big|_{t=0} =: L_{V}\varphi,
 f_{t} V 
\frac{d^{2}}{dt^{2}}(f_{t}^{*}\varphi) \big|_{t=0}
 L_{V}\varphi L_{V}(L_{V}\varphi) \varphi \in \Omega^{0}(X) \omega \in \Omega^{k}(X) 0 \leq k \leq \mathrm{dim}(X)","['differential-geometry', 'differential-topology', 'lie-derivative']"
13,Obtaining the Dolbeault operator on the pullbak of the holomorphic tangent bundle.,Obtaining the Dolbeault operator on the pullbak of the holomorphic tangent bundle.,,"I have ran into a question while reading this paper by Witten. The question is mostly mathematical, so I thought it would be better posed here instead of on Physics SE. Let $X$ be a Kahler manifold, $\Sigma$ a Riemann surface and $\Phi:\Sigma\to X$ be a smooth map. Let $TX=T^{1,0}X\oplus T^{0,1}X$ be the complexified tangent bundle of $X$ . I am interested in understanding what the Dolbeault operator on $\Phi^{*}(T^{1,0}X)$ looks like. My first attempt was to assume that the Dolbeault operator on $\Phi^{*}(T^{1,0}X)$ should be the pullback of the Dolbeault operator on the holomorphic tangent bundle $T^{1,0}X$ , which itself should be obtained from the splitting of the Hermitian connection $\nabla$ on $X$ into its $(0,1)$ -form part. I think I mostly understand how $\nabla$ works (it is similar to the Levi-Civita connection, with which I am somewhat familiar), and from there I seem to be able to understand the splitting, $\nabla s =\partial s + \bar{\partial}s$ . My issue is that $\bar{\partial}$ seems to be zero on $T^{1,0}X$ . In particular, we can locally write a section of the holomorphic tangent bundle as $s=v^{i}\partial_{z^{i}}$ with holomorphic coefficient functions, then: $$ \nabla_{\partial_{z}^{i}} (s) = \left( \frac{\partial v^{j}}{\partial z^{i}} + v^{k}\Gamma^{j}_{ik} \right) \partial_{z^{j}} $$ Then as far as I can tell we have: $$ \nabla s = \left( \frac{\partial v^{j}}{\partial z^{i}} + v^{k}\Gamma^{j}_{ik} \right) dz^{j}\otimes \partial_{z^{i}} \in\Omega^{1,0}(X)\otimes T^{1,0}X $$ i.e. $\nabla s$ is a $(1,0)$ form-valued section, and thus $\bar{\partial}s=0$ . This also fits with the intuitive idea that $\bar{\partial}$ of a holomorphic vector field should vanish. I have good reason to believe that $\bar{\partial}$ on $\Phi^{*}(T^{1,0}X)$ is not trivial, so it seems like this is the wrong way to obtain this operator. My next guess would be to pull back $\nabla$ via $\Phi$ , and then separate the result into holomorphic and anti-holomorphic parts, but I am not so sure about this. I would appreciate any help understanding how to correctly obtain/define $\bar{\partial}$ on $\Phi^{*}(T^{1,0}X)$ , as well as any verification that what I have done above is correct. EDIT: As far as I know the pullback of a holomorphic vector bundle by a non-holomorphic map needs not be holomorphic, so I don't even see why $\bar{\partial}$ makes sense on $\Phi^{*}(T^{1,0}X)$ .","I have ran into a question while reading this paper by Witten. The question is mostly mathematical, so I thought it would be better posed here instead of on Physics SE. Let be a Kahler manifold, a Riemann surface and be a smooth map. Let be the complexified tangent bundle of . I am interested in understanding what the Dolbeault operator on looks like. My first attempt was to assume that the Dolbeault operator on should be the pullback of the Dolbeault operator on the holomorphic tangent bundle , which itself should be obtained from the splitting of the Hermitian connection on into its -form part. I think I mostly understand how works (it is similar to the Levi-Civita connection, with which I am somewhat familiar), and from there I seem to be able to understand the splitting, . My issue is that seems to be zero on . In particular, we can locally write a section of the holomorphic tangent bundle as with holomorphic coefficient functions, then: Then as far as I can tell we have: i.e. is a form-valued section, and thus . This also fits with the intuitive idea that of a holomorphic vector field should vanish. I have good reason to believe that on is not trivial, so it seems like this is the wrong way to obtain this operator. My next guess would be to pull back via , and then separate the result into holomorphic and anti-holomorphic parts, but I am not so sure about this. I would appreciate any help understanding how to correctly obtain/define on , as well as any verification that what I have done above is correct. EDIT: As far as I know the pullback of a holomorphic vector bundle by a non-holomorphic map needs not be holomorphic, so I don't even see why makes sense on .","X \Sigma \Phi:\Sigma\to X TX=T^{1,0}X\oplus T^{0,1}X X \Phi^{*}(T^{1,0}X) \Phi^{*}(T^{1,0}X) T^{1,0}X \nabla X (0,1) \nabla \nabla s =\partial s + \bar{\partial}s \bar{\partial} T^{1,0}X s=v^{i}\partial_{z^{i}} 
\nabla_{\partial_{z}^{i}}
(s)
=
\left(
\frac{\partial v^{j}}{\partial z^{i}}
+
v^{k}\Gamma^{j}_{ik}
\right)
\partial_{z^{j}}
 
\nabla s
=
\left(
\frac{\partial v^{j}}{\partial z^{i}}
+
v^{k}\Gamma^{j}_{ik}
\right)
dz^{j}\otimes \partial_{z^{i}}
\in\Omega^{1,0}(X)\otimes T^{1,0}X
 \nabla s (1,0) \bar{\partial}s=0 \bar{\partial} \bar{\partial} \Phi^{*}(T^{1,0}X) \nabla \Phi \bar{\partial} \Phi^{*}(T^{1,0}X) \bar{\partial} \Phi^{*}(T^{1,0}X)","['differential-geometry', 'complex-geometry', 'vector-bundles', 'holomorphic-bundles']"
14,Prove vector area formula by stokes's theorem,Prove vector area formula by stokes's theorem,,"wiki says the following two formulas are equivalent. ${\mathbf {S}}=\int d{\mathbf {S}}$ ${\displaystyle \mathbf {S} ={\frac {1}{2}}\oint _{\partial S}{\vec {r}}\times d{\vec {r}}}$ I am learning exterior derivative and the generalized stokes' theorem ${\displaystyle \int _{\partial \Omega }\omega =\int _{\Omega }d\omega \,}$ , but when I applied $d(r \wedge dr) = dr \wedge dr - r \wedge ddr= dr \wedge dr + 0 = 0 \Rightarrow S = 0$ where did I do wrong? Any help is appreciated.","wiki says the following two formulas are equivalent. I am learning exterior derivative and the generalized stokes' theorem , but when I applied where did I do wrong? Any help is appreciated.","{\mathbf {S}}=\int d{\mathbf {S}} {\displaystyle \mathbf {S} ={\frac {1}{2}}\oint _{\partial S}{\vec {r}}\times d{\vec {r}}} {\displaystyle \int _{\partial \Omega }\omega =\int _{\Omega }d\omega \,} d(r \wedge dr) = dr \wedge dr - r \wedge ddr= dr \wedge dr + 0 = 0 \Rightarrow S = 0","['differential-geometry', 'vector-analysis', 'exterior-algebra', 'stokes-theorem']"
15,Classify the embedding of Lie groups: $U(1)$ in $SU(2)$ versus $U(1)$ in $SO(3)$,Classify the embedding of Lie groups:  in  versus  in,U(1) SU(2) U(1) SO(3),"I am interested in knowing the way to embed a Lie group to another Lie group. For example, we can embed $$U(1) \subset SU(2) \tag{1}$$ also $$U(1) \subset SO(3). \tag{2}$$ Here they are all regarded as some Lie groups. But in terms of differentiable manifolds, $SU(2) \cong S^3$ as a 3-sphere and $SO(3)  \cong RP^3$ as a real protective space both in real 3-dimensions. Now, my question is that how do we classify the way of their embedding? Here I must specify a way to define what exactly is a classification. But I am not certain what is the correct math definition. What I can prescribe is that there should be some identification of deformations between different embedding. As long as the embedding map can be continuous deformed to each other (maybe in the sense of homeomorphic), then the embedding maps are identified. Question: Is there a concrete math definition of such classification of the Lie group embedding identified via continuous deformations or homeomorphic)? My take is that to classify the embedding of Lie groups: $U(1)  \cong S^1 \subset SU(2) \cong S^3$ , since they are all Lie groups so their identities must be the same identity group element $1$ . Different embedding of $U(1)$ must intersect at the identity point on $SU(2)$ . Next we can continuously deform such $U(1)$ on $SU(2)$ . For those maps which cannot be continuous deformed, I expect that they are classified by the homotopy class $$ [U(1), SU(2)]=[S^1,S^3]=\pi_1(S^3)=0. $$ Thus there is only one way of embedding of $U(1) \subset SU(2)$ in terms of homotopy or homeomorphic. To classify the embedding of Lie groups: $U(1)  \cong S^1 \subset SO(3) \cong RP^3$ , since they are all Lie groups so their identities must be the same identity group element $1$ . Different embedding of $U(1)$ must intersect at the identity point on $SO(3)$ . Next we can continuously deform such $U(1)$ on $SO(3)$ . For those maps which cannot be continuous deformed, I expect that they are classified by the homotopy class $$ [U(1), SO(3)]=[S^1,RP^3]=\pi_1(RP^3)=\mathbf{Z}/2. $$ Thus there are two ways of embedding of $U(1) \subset SO(3)$ in terms of homotopy or homeomorphic. In terms of the lift map, we can see that the nontrivial $\mathbf{Z}/2$ class of the embedding $[U(1), SO(3)]$ is the obstruction to lift the $U(1) \to SO(3)$ to $U(1) \to SU(2)$ . And a lift diagram is like: $$ \begin{array}{ccc}   &  & SU(2)\\           &\nearrow &           \downarrow\\   U(1) & \longrightarrow & SO(3) \end{array}. $$ Please correct me and point toward the good way to think about the classifications of the embedding of Lie groups.","I am interested in knowing the way to embed a Lie group to another Lie group. For example, we can embed also Here they are all regarded as some Lie groups. But in terms of differentiable manifolds, as a 3-sphere and as a real protective space both in real 3-dimensions. Now, my question is that how do we classify the way of their embedding? Here I must specify a way to define what exactly is a classification. But I am not certain what is the correct math definition. What I can prescribe is that there should be some identification of deformations between different embedding. As long as the embedding map can be continuous deformed to each other (maybe in the sense of homeomorphic), then the embedding maps are identified. Question: Is there a concrete math definition of such classification of the Lie group embedding identified via continuous deformations or homeomorphic)? My take is that to classify the embedding of Lie groups: , since they are all Lie groups so their identities must be the same identity group element . Different embedding of must intersect at the identity point on . Next we can continuously deform such on . For those maps which cannot be continuous deformed, I expect that they are classified by the homotopy class Thus there is only one way of embedding of in terms of homotopy or homeomorphic. To classify the embedding of Lie groups: , since they are all Lie groups so their identities must be the same identity group element . Different embedding of must intersect at the identity point on . Next we can continuously deform such on . For those maps which cannot be continuous deformed, I expect that they are classified by the homotopy class Thus there are two ways of embedding of in terms of homotopy or homeomorphic. In terms of the lift map, we can see that the nontrivial class of the embedding is the obstruction to lift the to . And a lift diagram is like: Please correct me and point toward the good way to think about the classifications of the embedding of Lie groups.","U(1) \subset SU(2) \tag{1} U(1) \subset SO(3). \tag{2} SU(2) \cong S^3 SO(3)  \cong RP^3 U(1)  \cong S^1 \subset SU(2) \cong S^3 1 U(1) SU(2) U(1) SU(2) 
[U(1), SU(2)]=[S^1,S^3]=\pi_1(S^3)=0.
 U(1) \subset SU(2) U(1)  \cong S^1 \subset SO(3) \cong RP^3 1 U(1) SO(3) U(1) SO(3) 
[U(1), SO(3)]=[S^1,RP^3]=\pi_1(RP^3)=\mathbf{Z}/2.
 U(1) \subset SO(3) \mathbf{Z}/2 [U(1), SO(3)] U(1) \to SO(3) U(1) \to SU(2) 
\begin{array}{ccc}
  &  & SU(2)\\
          &\nearrow &           \downarrow\\
  U(1) & \longrightarrow & SO(3)
\end{array}.
","['general-topology', 'differential-geometry', 'algebraic-topology', 'lie-groups', 'homotopy-theory']"
16,How to do Taylor expansion of a differential form intrinsically?,How to do Taylor expansion of a differential form intrinsically?,,"Let $(M,g)$ be a closed Riemannian manifold with Levi-Civita connection $\nabla$ , let $\alpha$ be a n-form on $M$ , let $P$ be a point in $M$ . My questions are (1) how could we do Taylor expansion of $\alpha$ at P in an intrinsic way? (2) Suppose $\alpha$ is parallel with $\nabla \alpha=0$ , what will happen to that Taylor expansion? Here is some thoughts of this question: (1) We could choose some local coordinates at a neighborhood of P and choose a trivialization and a basis of the n-form bundle, then the Taylor expansion will be the expansion for the coefficients using these basis, but it doesn't look like a ""good"" definition as it depends on the trivialization. (2) My guess is a Taylor expansion of $\alpha$ at $P$ would be the following: for $Q$ lies in a neighborhood of $P$ , we pick a curve $\gamma(t)$ such that $\gamma(0)=P$ and $\gamma(1)=Q$ , then the Taylor expansion of $\alpha$ at $Q$ would be $$\alpha(Q)=\alpha|_P+\nabla_{\gamma'(0)}\alpha|_P+\nabla_{\gamma'(0)}\nabla_{\gamma'(0)}\alpha|_P+\cdots.$$ Here $\alpha(Q)$ is understood as using parallel transport of $\alpha|_Q$ to the fiber of n-forms at $P$ using the Levi-Civita connection. But I don't know how to prove it. But one thing confuses me, suppose $\nabla\alpha=0$ , then the Taylor series seems to be vanishes immediately? But the above Taylor series doesn't reflects any information of $\alpha$ in a neighborhood of $P$ even we know what $\alpha|_P$ is.","Let be a closed Riemannian manifold with Levi-Civita connection , let be a n-form on , let be a point in . My questions are (1) how could we do Taylor expansion of at P in an intrinsic way? (2) Suppose is parallel with , what will happen to that Taylor expansion? Here is some thoughts of this question: (1) We could choose some local coordinates at a neighborhood of P and choose a trivialization and a basis of the n-form bundle, then the Taylor expansion will be the expansion for the coefficients using these basis, but it doesn't look like a ""good"" definition as it depends on the trivialization. (2) My guess is a Taylor expansion of at would be the following: for lies in a neighborhood of , we pick a curve such that and , then the Taylor expansion of at would be Here is understood as using parallel transport of to the fiber of n-forms at using the Levi-Civita connection. But I don't know how to prove it. But one thing confuses me, suppose , then the Taylor series seems to be vanishes immediately? But the above Taylor series doesn't reflects any information of in a neighborhood of even we know what is.","(M,g) \nabla \alpha M P M \alpha \alpha \nabla \alpha=0 \alpha P Q P \gamma(t) \gamma(0)=P \gamma(1)=Q \alpha Q \alpha(Q)=\alpha|_P+\nabla_{\gamma'(0)}\alpha|_P+\nabla_{\gamma'(0)}\nabla_{\gamma'(0)}\alpha|_P+\cdots. \alpha(Q) \alpha|_Q P \nabla\alpha=0 \alpha P \alpha|_P","['differential-geometry', 'riemannian-geometry']"
17,On the integral of Frobenius norm of Jacobian,On the integral of Frobenius norm of Jacobian,,"Let $g : S \to \mathbb{R}^d$ be injective and smooth where $S \subseteq \mathbb{R}^k$ is a $k$ -dim regular smooth compact manifold with $d \geq k$ . I'de considering the term $$ A = \int_S \lVert J_g(x) \rVert_F \, dx $$ where $J_g$ is Jacobian matrix of $g$ and $\lVert \cdot \rVert_F$ is Frobenius norm. If $S$ is an one-dimensional interval on $\mathbb{R}$ , then $A$ is a the length of the curve given by $g(S)$ . I have following questions: What would be a physical meaning of $A$ ? Does increasing the $k$ -dimensional area of $g(S)$ increases $A$ , and vice versa? If 2 is true, how to simply prove? Hopefully my question is not so stupid one.","Let be injective and smooth where is a -dim regular smooth compact manifold with . I'de considering the term where is Jacobian matrix of and is Frobenius norm. If is an one-dimensional interval on , then is a the length of the curve given by . I have following questions: What would be a physical meaning of ? Does increasing the -dimensional area of increases , and vice versa? If 2 is true, how to simply prove? Hopefully my question is not so stupid one.","g : S \to \mathbb{R}^d S \subseteq \mathbb{R}^k k d \geq k 
A = \int_S \lVert J_g(x) \rVert_F \, dx
 J_g g \lVert \cdot \rVert_F S \mathbb{R} A g(S) A k g(S) A","['differential-geometry', 'jacobian']"
18,"""Parametric"" versus ""Non-parametric"" hypersurface","""Parametric"" versus ""Non-parametric"" hypersurface",,"What exactly are the meanings of the terms ""parametric hypersurface"" and ""non-parametric hypersurface""? My initial guess was ""parametric"" referred to a hypersurface that is a global graph (e.g. the graph of a parabola in $\mathbb{R}^{2}$ or the graph of the tangent function in $(-\pi/2,\pi/2) \times \mathbb{R}$ ) --- graphs are explicitly ""parametrized"" ---, whereas ""non-parametric"" referred to hypersurfaces that might not be graphs everywhere, like the circle --- which do not have a ""canonical parametrization.""  However, I fear it's the exact opposite --- and I don't understand the reasoning behind the terminology. The terminology seems to be taken for granted in differential geometry papers (e.g. this and this ).","What exactly are the meanings of the terms ""parametric hypersurface"" and ""non-parametric hypersurface""? My initial guess was ""parametric"" referred to a hypersurface that is a global graph (e.g. the graph of a parabola in or the graph of the tangent function in ) --- graphs are explicitly ""parametrized"" ---, whereas ""non-parametric"" referred to hypersurfaces that might not be graphs everywhere, like the circle --- which do not have a ""canonical parametrization.""  However, I fear it's the exact opposite --- and I don't understand the reasoning behind the terminology. The terminology seems to be taken for granted in differential geometry papers (e.g. this and this ).","\mathbb{R}^{2} (-\pi/2,\pi/2) \times \mathbb{R}",['differential-geometry']
19,Covariant derivative on principal bundle,Covariant derivative on principal bundle,,"I know that there exists a connection on a principal bundle and via parallel transport it is possible to define a a covariant derivative on the associated bundle. However, can we also define a covariant derivative on the principal bundle. I.e. something that can differentiate a section along a vector field? Or do we need a linear structure like the one in a vector bundle to 'take derivatives'?","I know that there exists a connection on a principal bundle and via parallel transport it is possible to define a a covariant derivative on the associated bundle. However, can we also define a covariant derivative on the principal bundle. I.e. something that can differentiate a section along a vector field? Or do we need a linear structure like the one in a vector bundle to 'take derivatives'?",,"['differential-geometry', 'differential-topology', 'connections', 'principal-bundles']"
20,Why inner variations are diffeomorphisms?,Why inner variations are diffeomorphisms?,,"Let $\Omega \subset \mathbb{R}^n $ be an open, bounded, connected domain with smooth boundary. Let $\eta \in C_c^{\infty}(\Omega,\mathbb{R}^n)$ be a smooth compactly supported $n$ -tuple of real valued functions. Define $\phi(x)=x+\epsilon\, \eta(x)$ . How to prove that for sufficiently small $\epsilon$ , $\phi$ is a diffeomorphism $\Omega \to \Omega$ ? I can see why $\phi(\Omega)\subseteq \Omega$ for small $\epsilon$ , but I don't see why $\phi|_{\Omega}$ is injective, or why $\phi(\Omega)\supseteq \Omega$ . I do see why $d\phi$ is invertible for small $\epsilon$ . Edit: Here is some progress: (Is there a more elementary way, which do not use the fact the Jacobian is null-Lagrangian?). Assume $\phi$ is injective (see comment below). First, we have $$ \phi(\bar \Omega) \subseteq \overline{\phi( \Omega)} \subseteq \bar \Omega. $$ The injectivity of $\phi$ implies $$ \text{Vol}(\phi(\bar \Omega))=\int_{\bar \Omega} J\phi\stackrel{(1)}{=}\int_{\bar \Omega} \text{Id}=\text{Vol}(\bar \Omega), $$ where equality $(1)$ follows, since the Jacobian is null-Lagrangian. Thus, we have $$ \phi(\bar \Omega) \subseteq \bar \Omega, \,\,\, \text{Vol}(\phi(\bar \Omega))=\text{Vol}(\bar \Omega). $$ This implies that $\phi(\bar \Omega)=\bar \Omega$ . Thus, $\phi:\bar \Omega \to \bar \Omega$ is bijective, hence by the inverse function theorem, it is a diffeomorphism. Is there a direct way to see $\phi:\bar \Omega \to \bar \Omega$ is surjective?","Let be an open, bounded, connected domain with smooth boundary. Let be a smooth compactly supported -tuple of real valued functions. Define . How to prove that for sufficiently small , is a diffeomorphism ? I can see why for small , but I don't see why is injective, or why . I do see why is invertible for small . Edit: Here is some progress: (Is there a more elementary way, which do not use the fact the Jacobian is null-Lagrangian?). Assume is injective (see comment below). First, we have The injectivity of implies where equality follows, since the Jacobian is null-Lagrangian. Thus, we have This implies that . Thus, is bijective, hence by the inverse function theorem, it is a diffeomorphism. Is there a direct way to see is surjective?","\Omega \subset \mathbb{R}^n  \eta \in C_c^{\infty}(\Omega,\mathbb{R}^n) n \phi(x)=x+\epsilon\, \eta(x) \epsilon \phi \Omega \to \Omega \phi(\Omega)\subseteq \Omega \epsilon \phi|_{\Omega} \phi(\Omega)\supseteq \Omega d\phi \epsilon \phi 
\phi(\bar \Omega) \subseteq \overline{\phi( \Omega)} \subseteq \bar \Omega.
 \phi 
\text{Vol}(\phi(\bar \Omega))=\int_{\bar \Omega} J\phi\stackrel{(1)}{=}\int_{\bar \Omega} \text{Id}=\text{Vol}(\bar \Omega),
 (1) 
\phi(\bar \Omega) \subseteq \bar \Omega, \,\,\, \text{Vol}(\phi(\bar \Omega))=\text{Vol}(\bar \Omega).
 \phi(\bar \Omega)=\bar \Omega \phi:\bar \Omega \to \bar \Omega \phi:\bar \Omega \to \bar \Omega","['differential-geometry', 'euclidean-geometry', 'differential-topology', 'smooth-manifolds', 'calculus-of-variations']"
21,Sectional curvature of Hadamard manifolds vanishes along certain planes if exponential map preserves norm,Sectional curvature of Hadamard manifolds vanishes along certain planes if exponential map preserves norm,,"I've been trying to solve the following exercise: Let $M$ be a Hadamard manifold (simply connected, complete and with sectional curvature $K \leq 0$ ). Show that: i) Let $p \in M, v, w \in T_pM$ linearly independent, $\gamma_v$ the geodesic with initial condition $v$ , and $E_w$ the parallel vector field along $\gamma_v$ with $E_w(0) = w$ . If $\|\mathrm{d}(\exp_p)_v(w)\| = \|w\|$ , then $K(\gamma_v'(t), E_w(t)) = 0$ for all $0 \leq t \leq 1$ . ii) Every metric ball $B_r(p)$ in $M$ , $r \geq 0$ , is strictly convex, i.e every geodesic segment connecting two points of $B_r(p)$ is contained in $B_r(p)$ . But I haven't had any good ideas so far. I know the exponential map is a global diffeomorphism in this case and I think maybe some solution could come from using Jacobi fields/variations but I couldn't think of anything concrete along those lines. I'd appreciate any help. Thanks in advance! EDIT : Since Hadamard manifolds don't have any conjugate points, I see now how i) is a straightforward consequence of Rauch's comparison theorem. I'm still stuck on ii) though and I realize now I should've split it into two posts, so I'm going to ask it in another post.","I've been trying to solve the following exercise: Let be a Hadamard manifold (simply connected, complete and with sectional curvature ). Show that: i) Let linearly independent, the geodesic with initial condition , and the parallel vector field along with . If , then for all . ii) Every metric ball in , , is strictly convex, i.e every geodesic segment connecting two points of is contained in . But I haven't had any good ideas so far. I know the exponential map is a global diffeomorphism in this case and I think maybe some solution could come from using Jacobi fields/variations but I couldn't think of anything concrete along those lines. I'd appreciate any help. Thanks in advance! EDIT : Since Hadamard manifolds don't have any conjugate points, I see now how i) is a straightforward consequence of Rauch's comparison theorem. I'm still stuck on ii) though and I realize now I should've split it into two posts, so I'm going to ask it in another post.","M K \leq 0 p \in M, v, w \in T_pM \gamma_v v E_w \gamma_v E_w(0) = w \|\mathrm{d}(\exp_p)_v(w)\| = \|w\| K(\gamma_v'(t), E_w(t)) = 0 0 \leq t \leq 1 B_r(p) M r \geq 0 B_r(p) B_r(p)","['differential-geometry', 'manifolds', 'riemannian-geometry', 'smooth-manifolds']"
22,Second fundamental form of an immersion,Second fundamental form of an immersion,,"I cannot quite replicate the following calculation from Aubin's Some Nonlinear Problems in Riemannian Geometry (page 349). The setting is as follows. Let $(M^n,g)$ and $(\tilde{M}^m,\tilde{g})$ be two $C^\infty$ Riemannian manifolds and let $f:M\rightarrow\tilde{M}$ be a smooth immersion. Let $\nabla$ be the Levi-Civita connection of $(M,g)$ and let $\tilde{\nabla}$ be the Levi-Civita connection of $(\tilde{M},\tilde{g})$ . Now let $\{x^i\}_{i=1}^n$ be local coordinates in a neighbourhood of $P\in M$ and let $\{y^\alpha\}_{\alpha=1}^m$ be local coordinates in a neighbourhood of $f(P)$ in $\tilde{M}$ . Say $f$ is injective on a neighbourhood $\Omega$ of $P$ in $M$ . Let $Y$ be a vector field on $\Omega$ and extend $\tilde{Y}=f_*Y$ to a neighbourhood of $f(P)$ . For $X$ in $T_x\Omega$ , let $\tilde{X}=f_*X$ . Finally, verify that $\tilde{\nabla}_{\tilde{X}}\tilde{Y}$ is well defined and define the second fundamental form $\alpha_x$ of $f$ at $x\in \Omega$ by $$\alpha_x(X,Y)=\tilde{\nabla}_{\tilde{X}}\tilde{Y}-f_*(\nabla_X Y)$$ The book now claims that in coordinates we have $$\alpha_x(X,Y)=\left[\partial^2_{ij}f^\gamma(x)-\Gamma_{ij}^k\partial_k f^\gamma+\tilde{\Gamma}_{\alpha\beta}^\gamma(f(x))\partial_if^\alpha(x)\partial_jf^\beta(x)\right]X^iY^j\frac{\partial}{\partial y^\gamma}$$ where $\nabla_{\frac{\partial}{\partial x^i}}\frac{\partial}{\partial x^j}=\Gamma_{ij}^k\frac{\partial}{\partial x^k}$ and $\tilde{\nabla}_{\frac{\partial}{\partial y^\alpha}}\frac{\partial}{\partial y^\beta}=\tilde{\Gamma}_{\alpha\beta}^\gamma\frac{\partial}{\partial y^\gamma}$ . I can easily calculate the coordinate expression of $f_*(\nabla_X Y)$ . Indeed I get $$f_*(\nabla_X Y)=(X^i\partial_i Y^j\partial_jf^\gamma+X^iY^j\Gamma_{ij}^k\partial_kf^\gamma)\frac{\partial}{\partial y^\gamma}$$ However, when I try to calculate $\tilde{\nabla}_{\tilde{X}}\tilde{Y}$ I run into trouble, since at some point I have to calculate $$\tilde{\nabla}_{\partial_if^\alpha\frac{\partial}{\partial y^\alpha}}\left(Y^j\partial_j f^\beta\frac{\partial}{\partial y^\beta}\right)=\color{red}{\left(\partial_if^\alpha\frac{\partial}{\partial y^\alpha}\right)\left(Y^j\partial_j f^\beta\right)\frac{\partial}{\partial y^\beta}}+Y^j\partial_if^\alpha\partial_j f^\beta\tilde{\nabla}_{\frac{\partial}{\partial y^\alpha}}\frac{\partial}{\partial y^\beta}$$ and the red summand seems syntactically wrong. Formally, $$\left(\partial_if^\alpha\frac{\partial}{\partial y^\alpha}\right)\left(Y^j\partial_j f^\beta\right)=\frac{\partial}{\partial x^i}\left(Y^j\partial_j f^\beta\right)=\partial_iY^j\partial_j f^\beta+Y^j\partial_{ij}^2f^\beta$$ which gives the correct answer, but I do not quite understand what is going on in this formal calculation. What am I missing? Moreover, what does the second partial $\partial_{ij}^2f^\beta$ of a map between manifolds mean exactly?","I cannot quite replicate the following calculation from Aubin's Some Nonlinear Problems in Riemannian Geometry (page 349). The setting is as follows. Let and be two Riemannian manifolds and let be a smooth immersion. Let be the Levi-Civita connection of and let be the Levi-Civita connection of . Now let be local coordinates in a neighbourhood of and let be local coordinates in a neighbourhood of in . Say is injective on a neighbourhood of in . Let be a vector field on and extend to a neighbourhood of . For in , let . Finally, verify that is well defined and define the second fundamental form of at by The book now claims that in coordinates we have where and . I can easily calculate the coordinate expression of . Indeed I get However, when I try to calculate I run into trouble, since at some point I have to calculate and the red summand seems syntactically wrong. Formally, which gives the correct answer, but I do not quite understand what is going on in this formal calculation. What am I missing? Moreover, what does the second partial of a map between manifolds mean exactly?","(M^n,g) (\tilde{M}^m,\tilde{g}) C^\infty f:M\rightarrow\tilde{M} \nabla (M,g) \tilde{\nabla} (\tilde{M},\tilde{g}) \{x^i\}_{i=1}^n P\in M \{y^\alpha\}_{\alpha=1}^m f(P) \tilde{M} f \Omega P M Y \Omega \tilde{Y}=f_*Y f(P) X T_x\Omega \tilde{X}=f_*X \tilde{\nabla}_{\tilde{X}}\tilde{Y} \alpha_x f x\in \Omega \alpha_x(X,Y)=\tilde{\nabla}_{\tilde{X}}\tilde{Y}-f_*(\nabla_X Y) \alpha_x(X,Y)=\left[\partial^2_{ij}f^\gamma(x)-\Gamma_{ij}^k\partial_k f^\gamma+\tilde{\Gamma}_{\alpha\beta}^\gamma(f(x))\partial_if^\alpha(x)\partial_jf^\beta(x)\right]X^iY^j\frac{\partial}{\partial y^\gamma} \nabla_{\frac{\partial}{\partial x^i}}\frac{\partial}{\partial x^j}=\Gamma_{ij}^k\frac{\partial}{\partial x^k} \tilde{\nabla}_{\frac{\partial}{\partial y^\alpha}}\frac{\partial}{\partial y^\beta}=\tilde{\Gamma}_{\alpha\beta}^\gamma\frac{\partial}{\partial y^\gamma} f_*(\nabla_X Y) f_*(\nabla_X Y)=(X^i\partial_i Y^j\partial_jf^\gamma+X^iY^j\Gamma_{ij}^k\partial_kf^\gamma)\frac{\partial}{\partial y^\gamma} \tilde{\nabla}_{\tilde{X}}\tilde{Y} \tilde{\nabla}_{\partial_if^\alpha\frac{\partial}{\partial y^\alpha}}\left(Y^j\partial_j f^\beta\frac{\partial}{\partial y^\beta}\right)=\color{red}{\left(\partial_if^\alpha\frac{\partial}{\partial y^\alpha}\right)\left(Y^j\partial_j f^\beta\right)\frac{\partial}{\partial y^\beta}}+Y^j\partial_if^\alpha\partial_j f^\beta\tilde{\nabla}_{\frac{\partial}{\partial y^\alpha}}\frac{\partial}{\partial y^\beta} \left(\partial_if^\alpha\frac{\partial}{\partial y^\alpha}\right)\left(Y^j\partial_j f^\beta\right)=\frac{\partial}{\partial x^i}\left(Y^j\partial_j f^\beta\right)=\partial_iY^j\partial_j f^\beta+Y^j\partial_{ij}^2f^\beta \partial_{ij}^2f^\beta","['differential-geometry', 'riemannian-geometry', 'connections', 'submanifold']"
23,Alternative proof of problem 16-21 of Lee's ISM: $\iota_X\mathsf{dVol}=\star(X^\flat)$,Alternative proof of problem 16-21 of Lee's ISM:,\iota_X\mathsf{dVol}=\star(X^\flat),"This is the problem 16-21 of Lee's Introduction to Smooth manifolds: Problem 16-21: Let $(M,g)$ be an oriented Riemannian manifold. Show $\iota_X\mathsf{dVol}_g=\star(X^\flat)$ for a vector field $X$ . I know a proof of that as follows: suppose $v_1,v_2,\dots,v_n$ be an orthonormal basis at $p\in M$ then $$\iota_X\mathsf{dVol}_g(v_2,\dots,v_n)=\mathsf{dVol}_g(X,v_2,\dots,v_n)=g(X,v_1)$$ on the other hand and using $X=X^iv_i$ and $X^\flat=g_{ij}X^iE^j$ $$\star(X^\flat)(v_2,\dots,v_n)=\star(g_{ij}X^iE^j)(v_2,\dots,v_n)=g_{ij}X^i\star E^j(v_2,\dots,v_n)=\sum_jg_{ij}X^i \mathsf{dVol}_g(v_2,\dots,v_j,\dots,v_n)=g_{i1}X^i \mathsf{dVol}_g(v_1,\dots,v_n)=g(X,v_1). $$ I messed up a bit in the last two equalities. Anyway I want to know is there another proof and global (basis-free) proof? I think one can prove this just by using properties of interior multiplication and Hodge star. i.e. suppose $\eta$ be an arbitrary $n-1$ -form then $$\langle \iota_X\mathsf{dVol}_g,\eta\rangle = \langle \mathsf{dVol}_g,X^\flat\wedge\eta\rangle=??= \langle \star(X^\flat),\eta\rangle $$ How to fill the above gap?",This is the problem 16-21 of Lee's Introduction to Smooth manifolds: Problem 16-21: Let be an oriented Riemannian manifold. Show for a vector field . I know a proof of that as follows: suppose be an orthonormal basis at then on the other hand and using and I messed up a bit in the last two equalities. Anyway I want to know is there another proof and global (basis-free) proof? I think one can prove this just by using properties of interior multiplication and Hodge star. i.e. suppose be an arbitrary -form then How to fill the above gap?,"(M,g) \iota_X\mathsf{dVol}_g=\star(X^\flat) X v_1,v_2,\dots,v_n p\in M \iota_X\mathsf{dVol}_g(v_2,\dots,v_n)=\mathsf{dVol}_g(X,v_2,\dots,v_n)=g(X,v_1) X=X^iv_i X^\flat=g_{ij}X^iE^j \star(X^\flat)(v_2,\dots,v_n)=\star(g_{ij}X^iE^j)(v_2,\dots,v_n)=g_{ij}X^i\star E^j(v_2,\dots,v_n)=\sum_jg_{ij}X^i \mathsf{dVol}_g(v_2,\dots,v_j,\dots,v_n)=g_{i1}X^i \mathsf{dVol}_g(v_1,\dots,v_n)=g(X,v_1).  \eta n-1 \langle \iota_X\mathsf{dVol}_g,\eta\rangle = \langle \mathsf{dVol}_g,X^\flat\wedge\eta\rangle=??= \langle \star(X^\flat),\eta\rangle ","['differential-geometry', 'riemannian-geometry', 'differential-forms']"
24,How to define an angular form with bounded support?,How to define an angular form with bounded support?,,"In my smooth manifold course, I am asked to define a differential $1$ -form $\tau$ on $M = \mathbb{R}^{3} \setminus S^{1}$ with support bounded in $\mathbb{R}^3$ such that $\int_{\gamma} \tau$ can represent the signed crossing number of any closed curve $\gamma : S^{1} \to M$ about the disc $D^{2}$ . I believe that $\tau$ must be related to $\omega = \mathrm{d}(\arctan\frac{z}{\sqrt{x^{2}+y^{2}}-1})$ , but how can we modify $\omega$ to obtain a globally smooth $1$ -form with bounded support? It seems that $\int_{\gamma} \tau$ gives rise to linking numbers (see this post and this post ).","In my smooth manifold course, I am asked to define a differential -form on with support bounded in such that can represent the signed crossing number of any closed curve about the disc . I believe that must be related to , but how can we modify to obtain a globally smooth -form with bounded support? It seems that gives rise to linking numbers (see this post and this post ).",1 \tau M = \mathbb{R}^{3} \setminus S^{1} \mathbb{R}^3 \int_{\gamma} \tau \gamma : S^{1} \to M D^{2} \tau \omega = \mathrm{d}(\arctan\frac{z}{\sqrt{x^{2}+y^{2}}-1}) \omega 1 \int_{\gamma} \tau,"['differential-geometry', 'algebraic-topology', 'smooth-manifolds', 'differential-forms', 'knot-theory']"
25,"The hairy ball theorem, from Brouwer's fixed point.","The hairy ball theorem, from Brouwer's fixed point.",,"EDIT : The question is now the following. I know this statement of the hairy ball theorem : Theorem : Let $n \geq 3$ be an odd number, and $f:\mathbb{S}^{n-1} \rightarrow \mathbb{R}^n$ be a continuous map such that $\langle f(x),x \rangle = 0$ for every $x \in \mathbb{S}^{n-1}$ . Then there exists $x_0 \in \mathbb{S}^{n-1}$ such that $f(x_0)=0$ . In the linked paper provided by C.F.G., there is this version of hairy ball theorem : Theorem : For any continuous map $v : B \rightarrow R^n$ such that $\langle v(x), x \rangle \leq 0$ for all $x \in \mathbb{S}^{n-1}$ , there exists some $z \in B$ such that $v(z)=0$ . To be honest, I don't really see how to go from one of these versions to the other : how the even dimension is replaced, in the second version, by the hypothesis $\langle v(x), x \rangle \leq 0$ ? Is someone could explain how these two statements are linked, that would be great ! $$-------------------------------------$$ Original question : My question is rather simple today : is there an easy proof of hairy ball theorem that uses Brouwer fixed point theorem ? I know that these two results are similar in some ways, and I know that they have proofs that rely on the same kind of arguments (I saw Milnor proofs for these two results), but my concern is : let's suppose that you know that Brouwer's fixed point theorem is true ; is there a way to deduce the hairy ball theorem with only elementary steps from there ? I guess there may be an obstruction, due to the fact that Brouwer theorem is true in every dimension, whereas the hairy ball theorem is not. If someone knows a short proof, or has a reference, it would be really nice. Thanks !","EDIT : The question is now the following. I know this statement of the hairy ball theorem : Theorem : Let be an odd number, and be a continuous map such that for every . Then there exists such that . In the linked paper provided by C.F.G., there is this version of hairy ball theorem : Theorem : For any continuous map such that for all , there exists some such that . To be honest, I don't really see how to go from one of these versions to the other : how the even dimension is replaced, in the second version, by the hypothesis ? Is someone could explain how these two statements are linked, that would be great ! Original question : My question is rather simple today : is there an easy proof of hairy ball theorem that uses Brouwer fixed point theorem ? I know that these two results are similar in some ways, and I know that they have proofs that rely on the same kind of arguments (I saw Milnor proofs for these two results), but my concern is : let's suppose that you know that Brouwer's fixed point theorem is true ; is there a way to deduce the hairy ball theorem with only elementary steps from there ? I guess there may be an obstruction, due to the fact that Brouwer theorem is true in every dimension, whereas the hairy ball theorem is not. If someone knows a short proof, or has a reference, it would be really nice. Thanks !","n \geq 3 f:\mathbb{S}^{n-1} \rightarrow \mathbb{R}^n \langle f(x),x \rangle = 0 x \in \mathbb{S}^{n-1} x_0 \in \mathbb{S}^{n-1} f(x_0)=0 v : B \rightarrow R^n \langle v(x), x \rangle \leq 0 x \in \mathbb{S}^{n-1} z \in B v(z)=0 \langle v(x), x \rangle \leq 0 -------------------------------------","['general-topology', 'differential-geometry', 'reference-request', 'differential-topology']"
26,Relating the traditional definition of a vector field in terms of function with the differential geometry definition involving fibers and bundles,Relating the traditional definition of a vector field in terms of function with the differential geometry definition involving fibers and bundles,,"My first exposure to the concept of vector fields was in highschool physics courses, which had a simple intuitive idea of being a function which aassociates a point in a given region in space (domain where function is defined) with an arrow pointing in some direction.  For example, I can give the famous electric field of point charge centered at origin as vector field: $$ \vec{E}(r) = \hat{r} \frac{kq}{r^2} $$ This is fine. Recently, I came with a more mathematically sophisiticated definition when going through some lectures on the mathematical side of general relativity. The following definition is given: A smooth vector field $\chi$ is a smooth map that is a section of the map $TM \xrightarrow{\pi} M$ where $M$ is a topological manifold and $TM$ is the tangent bundle of that manifold. Satifying the law that: $\pi \circ  \chi = id_M$ Source 44:45 , By prof. Frederic D Schuller After some few seconds, the professor explains the above definition using the following picture: He draws on the circle what I know traditionally as a vector field, and then according to length of the arrow of the tangent vector he associates points on the tangent bundle. For example, the bottom most point on the circle has a vector of zero length attached to it, so, on the tangent bundle he choses the point with a height zero above that point. Note: The prof said that it doesn't matter how we depict the tangent bundle i.e: orient the tangent lines because the tangent bundle in itself has only a structure of a set and nothing else (Maybe I am misinterpretting this point he had mentioned here , please correct me if I am wrong) This leads me to the following questions: How exactly does this 'smooth' vector field formalism coincide with the idea of vector fields as function? What exactly is the significance of this formalism?","My first exposure to the concept of vector fields was in highschool physics courses, which had a simple intuitive idea of being a function which aassociates a point in a given region in space (domain where function is defined) with an arrow pointing in some direction.  For example, I can give the famous electric field of point charge centered at origin as vector field: This is fine. Recently, I came with a more mathematically sophisiticated definition when going through some lectures on the mathematical side of general relativity. The following definition is given: A smooth vector field is a smooth map that is a section of the map where is a topological manifold and is the tangent bundle of that manifold. Satifying the law that: Source 44:45 , By prof. Frederic D Schuller After some few seconds, the professor explains the above definition using the following picture: He draws on the circle what I know traditionally as a vector field, and then according to length of the arrow of the tangent vector he associates points on the tangent bundle. For example, the bottom most point on the circle has a vector of zero length attached to it, so, on the tangent bundle he choses the point with a height zero above that point. Note: The prof said that it doesn't matter how we depict the tangent bundle i.e: orient the tangent lines because the tangent bundle in itself has only a structure of a set and nothing else (Maybe I am misinterpretting this point he had mentioned here , please correct me if I am wrong) This leads me to the following questions: How exactly does this 'smooth' vector field formalism coincide with the idea of vector fields as function? What exactly is the significance of this formalism?", \vec{E}(r) = \hat{r} \frac{kq}{r^2}  \chi TM \xrightarrow{\pi} M M TM \pi \circ  \chi = id_M,"['differential-geometry', 'vector-bundles', 'vector-fields']"
27,Prove or disprove $SO(4n) \supseteq \frac{(Sp(1)\times Sp(n))}{\mathbb{Z}_2}$?,Prove or disprove ?,SO(4n) \supseteq \frac{(Sp(1)\times Sp(n))}{\mathbb{Z}_2},"I suspect that this is true $$ \boxed{SO(4n) \supseteq \frac{(Sp(1)\times Sp(n))}{\mathbb{Z}_2}.} $$ How do we prove it? When $n=1$ , we have $$ SO(4) \supseteq \frac{(SU(2)\times Sp(1))}{\mathbb{Z}_2}=  \frac{(SU(2)\times SU(2))}{\mathbb{Z}_2}=\frac{Spin(3) \times Spin(3)}{\mathbb{Z}_2} =\frac{Spin(4)}{\mathbb{Z}_2}. $$ which is true by isomorphism. When $n=2$ , we have $$ SO(8) \supseteq \frac{(SU(2)\times Sp(2))}{\mathbb{Z}_2}=  \frac{(SU(2)\times Spin(5))}{\mathbb{Z}_2}=\frac{Spin(3) \times Spin(5)}{\mathbb{Z}_2}. $$ But in contrast only $$ {Spin(8)} \supseteq \frac{Spin(3) \times Spin(5)}{\mathbb{Z}_2}, $$ which is true. How to prove that general $n$ , it is all true? It is better even to show the explicit embedding. p.s. Here I use the symplectic group $Sp(1)=SU(2)$ and $Sp(2)=Spin(5)$ . We can see that a generic $SU(2)$ group can be represented by a rank-2 unitary matrix satisfies $$V^\dagger V =\mathbb{I}.$$ Then we can write such a complex $V = \begin{pmatrix} a &  b\\ - b^* & a~*  \end{pmatrix}$ . It can be checked that it obeys $Sp(1)$ condition $$ V^T \begin{pmatrix} 0&  1\\ -1 & 0 \end{pmatrix} V = \begin{pmatrix} 0&  1\\ -1 & 0 \end{pmatrix}. $$","I suspect that this is true How do we prove it? When , we have which is true by isomorphism. When , we have But in contrast only which is true. How to prove that general , it is all true? It is better even to show the explicit embedding. p.s. Here I use the symplectic group and . We can see that a generic group can be represented by a rank-2 unitary matrix satisfies Then we can write such a complex . It can be checked that it obeys condition","
\boxed{SO(4n) \supseteq \frac{(Sp(1)\times Sp(n))}{\mathbb{Z}_2}.}
 n=1 
SO(4) \supseteq \frac{(SU(2)\times Sp(1))}{\mathbb{Z}_2}=  \frac{(SU(2)\times SU(2))}{\mathbb{Z}_2}=\frac{Spin(3) \times Spin(3)}{\mathbb{Z}_2}
=\frac{Spin(4)}{\mathbb{Z}_2}.
 n=2 
SO(8) \supseteq \frac{(SU(2)\times Sp(2))}{\mathbb{Z}_2}=  \frac{(SU(2)\times Spin(5))}{\mathbb{Z}_2}=\frac{Spin(3) \times Spin(5)}{\mathbb{Z}_2}.
 
{Spin(8)} \supseteq \frac{Spin(3) \times Spin(5)}{\mathbb{Z}_2},
 n Sp(1)=SU(2) Sp(2)=Spin(5) SU(2) V^\dagger V =\mathbb{I}. V = \begin{pmatrix} a &  b\\ - b^* & a~* 
\end{pmatrix} Sp(1) 
V^T \begin{pmatrix} 0&  1\\ -1 & 0 \end{pmatrix} V = \begin{pmatrix} 0&  1\\ -1 & 0 \end{pmatrix}.
","['group-theory', 'differential-geometry', 'lie-groups', 'symplectic-geometry', 'spin-geometry']"
28,"$\frac{d}{ds} \langle T, T \rangle = 2\langle \nabla_S T, T \rangle$?",?,"\frac{d}{ds} \langle T, T \rangle = 2\langle \nabla_S T, T \rangle","Our setting is $(M, g)$ , a Riemannian manifold. Let $\Gamma(s,t) \subset M$ be a variation about curve $\gamma(t) = \Gamma(0, t)$ (Let us say that our domain of $\Gamma$ is $(a_0, a_1) \times (b_0, b_1) \subset \mathbb R^2$ , and $(a_0, a_1)$ contains $0$ .) Define $$T = \partial_t \Gamma; S = \partial_s \Gamma.$$ My textbook says: \begin{equation*} \frac{d}{ds} \langle T, T \rangle = 2\langle \nabla_S T, T \rangle.\end{equation*} If I treat $\frac{\partial}{\partial s}$ as a tangent vector $S$ (or a vector field), then everything makes sense. However, I have a trouble understanding why $\frac{d}{ds}$ is a tangent vector at $T_p M$ , where $p = \Gamma(s_0,t_0)$ for some $s_0, t_0$ . Note that $\langle T, T \rangle$ is a function $(a_0, a_1) \times (b_0, b_1) \rightarrow \mathbb R$ , so it can be treated it as a function from $\mathbb R^2$ to $\mathbb R$ . I am merely taking a partial differentiation w.r.t. $s$ , and it has nothing to do with tangent vector at $T_p M$ . How do I resolve this?","Our setting is , a Riemannian manifold. Let be a variation about curve (Let us say that our domain of is , and contains .) Define My textbook says: If I treat as a tangent vector (or a vector field), then everything makes sense. However, I have a trouble understanding why is a tangent vector at , where for some . Note that is a function , so it can be treated it as a function from to . I am merely taking a partial differentiation w.r.t. , and it has nothing to do with tangent vector at . How do I resolve this?","(M, g) \Gamma(s,t) \subset M \gamma(t) = \Gamma(0, t) \Gamma (a_0, a_1) \times (b_0, b_1) \subset \mathbb R^2 (a_0, a_1) 0 T = \partial_t \Gamma; S = \partial_s \Gamma. \begin{equation*}
\frac{d}{ds} \langle T, T \rangle = 2\langle \nabla_S T, T \rangle.\end{equation*} \frac{\partial}{\partial s} S \frac{d}{ds} T_p M p = \Gamma(s_0,t_0) s_0, t_0 \langle T, T \rangle (a_0, a_1) \times (b_0, b_1) \rightarrow \mathbb R \mathbb R^2 \mathbb R s T_p M",['differential-geometry']
29,Globally hyperbolic Lorentzian manifold,Globally hyperbolic Lorentzian manifold,,"I am currently trying to work through properties of globally hyperbolic Lorentzian manifolds and there are some things which aren't clear to me: I have the following definition of globally hyperbolic: Let $(M,g)$ be a connected time-oriented Lorentzian manifold. Then $(M,g)$ is called globally hyperbolic, if one of the following equivalent conditions hold: There exists a Cauchy hypersurface in $M$ . There exists a smooth spacelike Cauchy hypersurface in $M$ . $M$ is isometric to $\mathbb{R} \times S$ with metric \begin{align*} - Rdt^2 +\sigma_t \end{align*} where $R$ is a smooth positive function, $(S, \sigma_t)$ is a Riemannian manifold, $\sigma_t$ depending smoothly on $t$ . Moreover, $\{t\} \times S$ is a Cauchy hypersurface in $M$ for each $t \in \mathbb{R}$ . $M$ satisfies the strong causality condition and for all $p,q \in M$ s.t. $p<q$ , it holds that $J^+(p) \cap J^-(q)$ is compact. where a Cauchy hypersurface is defined as: $S$ is called a Cauchy hypersurface, if it is met exactly one by exery inextendible timelike curve $\gamma$ in $M$ . First question : what does ""smooth Cauchy hypersurface"" mean? In the sources I use, this is never defined. Second question : As far as I understand, a Cauchy hypersurface is not necessarily spacelike, right? And if we look at the Cauchy hypersurface from 3. this one is spacelike, because we have a Riemannian metric, right?","I am currently trying to work through properties of globally hyperbolic Lorentzian manifolds and there are some things which aren't clear to me: I have the following definition of globally hyperbolic: Let be a connected time-oriented Lorentzian manifold. Then is called globally hyperbolic, if one of the following equivalent conditions hold: There exists a Cauchy hypersurface in . There exists a smooth spacelike Cauchy hypersurface in . is isometric to with metric where is a smooth positive function, is a Riemannian manifold, depending smoothly on . Moreover, is a Cauchy hypersurface in for each . satisfies the strong causality condition and for all s.t. , it holds that is compact. where a Cauchy hypersurface is defined as: is called a Cauchy hypersurface, if it is met exactly one by exery inextendible timelike curve in . First question : what does ""smooth Cauchy hypersurface"" mean? In the sources I use, this is never defined. Second question : As far as I understand, a Cauchy hypersurface is not necessarily spacelike, right? And if we look at the Cauchy hypersurface from 3. this one is spacelike, because we have a Riemannian metric, right?","(M,g) (M,g) M M M \mathbb{R} \times S \begin{align*}
- Rdt^2 +\sigma_t
\end{align*} R (S, \sigma_t) \sigma_t t \{t\} \times S M t \in \mathbb{R} M p,q \in M p<q J^+(p) \cap J^-(q) S \gamma M","['differential-geometry', 'riemannian-geometry', 'semi-riemannian-geometry']"
30,Derivative of sandwich product of Geometric Algebra rotor and vector wrt that rotor.,Derivative of sandwich product of Geometric Algebra rotor and vector wrt that rotor.,,"How do I find the derivative (differential?) of the sandwich product of a rotor, with respect to the rotor? Namely, given a rotor $R$ , and vector $v$ , what is $\frac{\text{d}}{\text{d}R}(Rv\tilde{R})$ ? I've seen how this is done using matrix representations of quaternions, but I'd like to know how to do this staying in Geometric Algebra.  I'd like the answer as a (multi?)vector.","How do I find the derivative (differential?) of the sandwich product of a rotor, with respect to the rotor? Namely, given a rotor , and vector , what is ? I've seen how this is done using matrix representations of quaternions, but I'd like to know how to do this staying in Geometric Algebra.  I'd like the answer as a (multi?)vector.",R v \frac{\text{d}}{\text{d}R}(Rv\tilde{R}),"['differential-geometry', 'geometric-algebras', 'exterior-derivative']"
31,Does $\lim_n v_n(f) = v(f)$ for $v\in T_pM$,Does  for,\lim_n v_n(f) = v(f) v\in T_pM,"Let $v \in T_pM$ for some smooth manifold,assume $v_n \in T_pM$ also. Assume further that $\lim_n v_n = v$ . Do we have the identity that $$v_n(f) \to v(f)$$ for $f\in C^\infty(M)$ (I was tring to provd the relation  which comes from Lee's smooth manifold book page 231: $$\left.\frac{d}{d s}\right|_{s=0} d\left(\theta_{-t_{0}}\right) \circ d\left(\theta_{-s}\right)\left(W_{\theta_{s}}\left(\theta_{t_{0}}(p)\right)\right. =d\left(\theta_{-t_{0}}\right)\left(\left.\frac{d}{d s}\right|_{s=0} d\left(\theta_{-s}\right)\left(W_{\theta_{s}}\left(\theta_{t_{0}}(p)\right)\right)\right)$$ Where $W$ is a vector field over M. that is same to prove(notation may differ but the idea is the same): $$\frac{d}{ds} d\theta_{t_0} (v(s)) = d\theta_{t_0}(\frac{d}{ds} v(s)) \tag{*}$$ I try to do it as follows (using definiton of differential) denote $\Delta v(s) = \frac{v(s) - v(0)}{s}$ write everything in coordinate: $$\lim_s d\theta_{t_0} (\Delta v(s)) = \lim_{s\to 0}\sum d\theta_{t_0} (\Delta v(s))(x^i) \partial_i = \sum \lim_{s\to 0}d\theta_{t_0} (\Delta v(s))(x^i) \partial_i $$ Then $$\sum \lim_{s\to 0}d\theta_{t_0} (\Delta v(s))(x^i) \partial_i = \sum \lim_{s\to 0}[(\Delta v(s))(x^i\circ\theta_{t_0})] \partial_i$$ If we can show $$\lim [\Delta v(s) (f)] = (\lim \Delta v(s)) f$$ then we are done since $$ \sum \lim_{s\to 0}[(\Delta v(s))(x^i\circ\theta_{t_0})] \partial_i =  \sum d\theta_{t_0}[(\lim_{s\to 0})(\Delta v(s))](x^i) \partial_i =\sum d\theta_{t_0}(\frac{d}{ds} v(s))(x^i) \partial_i = RHS$$","Let for some smooth manifold,assume also. Assume further that . Do we have the identity that for (I was tring to provd the relation  which comes from Lee's smooth manifold book page 231: Where is a vector field over M. that is same to prove(notation may differ but the idea is the same): I try to do it as follows (using definiton of differential) denote write everything in coordinate: Then If we can show then we are done since","v \in T_pM v_n \in T_pM \lim_n v_n = v v_n(f) \to v(f) f\in C^\infty(M) \left.\frac{d}{d s}\right|_{s=0} d\left(\theta_{-t_{0}}\right) \circ d\left(\theta_{-s}\right)\left(W_{\theta_{s}}\left(\theta_{t_{0}}(p)\right)\right.
=d\left(\theta_{-t_{0}}\right)\left(\left.\frac{d}{d s}\right|_{s=0} d\left(\theta_{-s}\right)\left(W_{\theta_{s}}\left(\theta_{t_{0}}(p)\right)\right)\right) W \frac{d}{ds} d\theta_{t_0} (v(s)) = d\theta_{t_0}(\frac{d}{ds} v(s)) \tag{*} \Delta v(s) = \frac{v(s) - v(0)}{s} \lim_s d\theta_{t_0} (\Delta v(s)) = \lim_{s\to 0}\sum d\theta_{t_0} (\Delta v(s))(x^i) \partial_i = \sum \lim_{s\to 0}d\theta_{t_0} (\Delta v(s))(x^i) \partial_i  \sum \lim_{s\to 0}d\theta_{t_0} (\Delta v(s))(x^i) \partial_i = \sum \lim_{s\to 0}[(\Delta v(s))(x^i\circ\theta_{t_0})] \partial_i \lim [\Delta v(s) (f)] = (\lim \Delta v(s)) f  \sum \lim_{s\to 0}[(\Delta v(s))(x^i\circ\theta_{t_0})] \partial_i =  \sum d\theta_{t_0}[(\lim_{s\to 0})(\Delta v(s))](x^i) \partial_i =\sum d\theta_{t_0}(\frac{d}{ds} v(s))(x^i) \partial_i = RHS","['differential-geometry', 'smooth-manifolds', 'lie-derivative']"
32,one substitution in Chern's intrinsic proof of Gauss-Bonnet-Chern theorem,one substitution in Chern's intrinsic proof of Gauss-Bonnet-Chern theorem,,"In Chern's proof for Gauss-Bonnet-Chern theorem, he claims that $$ \varepsilon_{i}u_{i_1}u_j\Omega_{ji_2}\theta_{i_3}\cdots\theta_{i_{2p-2k}}\Omega_{i_{2p-2k+1}i_{2p-2k+2}}\cdots\Omega_{i_{2p-1}i_{2p}}=P_k+2(p-k-1)\Sigma_k $$ where $$ P_k=\varepsilon_{i}u_{i_1}^2\Omega_{i_1i_2}\theta_{i_3}\cdots\theta_{i_{2p-2k}}\Omega_{i_{2p-2k+1}i_{2p-2k+2}}\cdots\Omega_{i_{2p-1}i_{2p}} $$ and $$ \Sigma_k=\varepsilon_{i}u_{i_1}u_{i_3}\Omega_{i_3i_2}\theta_{i_3}\cdots\theta_{i_{2p-2k}}\Omega_{i_{2p-2k+1}i_{2p-2k+2}}\cdots\Omega_{i_{2p-1}i_{2p}} $$ By direct computations: $j=i_1$ : we get $P_k$ . $j=i_3,\dots,i_{2p-2k}$ : we get $\Sigma_k$ . My question is: why $$ \sum_{j=i_{2p-2k+1}}^{2p}\varepsilon_{i}u_{i_1}u_j\Omega_{ji_2}\theta_{i_3}\cdots\theta_{i_{2p-2k}}\Omega_{i_{2p-2k+1}i_{2p-2k+2}}\cdots\Omega_{i_{2p-1}i_{2p}}=0? $$ It seems that $$ \varepsilon_{i}u_{i_1}u_{j}\Omega_{ji_2}\theta_{i_3}\cdots\theta_{i_{2p-2k}}\Omega_{i_{2p-2k+1}i_{2p-2k+2}}\cdots\Omega_{i_{2p-1}i_{2p}}\\=\varepsilon_{i}u_{i_1}u_{i_{2p-2k+1}}\Omega_{i_{2p-2k+1}i_2}\theta_{i_3}\cdots\theta_{i_{2p-2k}}\Omega_{i_{2p-2k+1}i_{2p-2k+2}}\cdots\Omega_{i_{2p-1}i_{2p}} $$ for all $j\in\{2p-2k+2,\dots,2p\}$ , then why $$ \varepsilon_{i}u_{i_1}u_{i_{2p-2k+1}}\Omega_{i_{2p-2k+1}i_2}\theta_{i_3}\cdots\theta_{i_{2p-2k}}\Omega_{i_{2p-2k+1}i_{2p-2k+2}}\cdots\Omega_{i_{2p-1}i_{2p}}=0? $$ Any help would be appreciated. The link for Chern's original proof is: https://www.maths.ed.ac.uk/~v1ranick/papers/chern7.pdf","In Chern's proof for Gauss-Bonnet-Chern theorem, he claims that where and By direct computations: : we get . : we get . My question is: why It seems that for all , then why Any help would be appreciated. The link for Chern's original proof is: https://www.maths.ed.ac.uk/~v1ranick/papers/chern7.pdf","
\varepsilon_{i}u_{i_1}u_j\Omega_{ji_2}\theta_{i_3}\cdots\theta_{i_{2p-2k}}\Omega_{i_{2p-2k+1}i_{2p-2k+2}}\cdots\Omega_{i_{2p-1}i_{2p}}=P_k+2(p-k-1)\Sigma_k
 
P_k=\varepsilon_{i}u_{i_1}^2\Omega_{i_1i_2}\theta_{i_3}\cdots\theta_{i_{2p-2k}}\Omega_{i_{2p-2k+1}i_{2p-2k+2}}\cdots\Omega_{i_{2p-1}i_{2p}}
 
\Sigma_k=\varepsilon_{i}u_{i_1}u_{i_3}\Omega_{i_3i_2}\theta_{i_3}\cdots\theta_{i_{2p-2k}}\Omega_{i_{2p-2k+1}i_{2p-2k+2}}\cdots\Omega_{i_{2p-1}i_{2p}}
 j=i_1 P_k j=i_3,\dots,i_{2p-2k} \Sigma_k 
\sum_{j=i_{2p-2k+1}}^{2p}\varepsilon_{i}u_{i_1}u_j\Omega_{ji_2}\theta_{i_3}\cdots\theta_{i_{2p-2k}}\Omega_{i_{2p-2k+1}i_{2p-2k+2}}\cdots\Omega_{i_{2p-1}i_{2p}}=0?
 
\varepsilon_{i}u_{i_1}u_{j}\Omega_{ji_2}\theta_{i_3}\cdots\theta_{i_{2p-2k}}\Omega_{i_{2p-2k+1}i_{2p-2k+2}}\cdots\Omega_{i_{2p-1}i_{2p}}\\=\varepsilon_{i}u_{i_1}u_{i_{2p-2k+1}}\Omega_{i_{2p-2k+1}i_2}\theta_{i_3}\cdots\theta_{i_{2p-2k}}\Omega_{i_{2p-2k+1}i_{2p-2k+2}}\cdots\Omega_{i_{2p-1}i_{2p}}
 j\in\{2p-2k+2,\dots,2p\} 
\varepsilon_{i}u_{i_1}u_{i_{2p-2k+1}}\Omega_{i_{2p-2k+1}i_2}\theta_{i_3}\cdots\theta_{i_{2p-2k}}\Omega_{i_{2p-2k+1}i_{2p-2k+2}}\cdots\Omega_{i_{2p-1}i_{2p}}=0?
","['differential-geometry', 'riemannian-geometry', 'curvature']"
33,Understanding a local definition of the gradient,Understanding a local definition of the gradient,,"In this topic , the tangential gradient on a submanifold $M \subset \mathbb{R}^d$ , embedded by a map $F: M\rightarrow \mathbb{R}^d$ , is defined as $$\nabla^M f = \nabla^i f \frac{\partial F}{\partial x_i} = g^{ij} \frac{\partial f}{\partial x_j} \frac{\partial F}{\partial x_i},$$ where $g_{ij}=\langle\frac{\partial F}{\partial x_i}, \frac{\partial F}{\partial x_j}\rangle$ . I'm stack in understanding the embedding $F$ and how to calculate $\frac{\partial F}{\partial x_i}$ even in the basic example of a sphere ( $d=3$ ). Any hint would really be helpful.","In this topic , the tangential gradient on a submanifold , embedded by a map , is defined as where . I'm stack in understanding the embedding and how to calculate even in the basic example of a sphere ( ). Any hint would really be helpful.","M \subset \mathbb{R}^d F: M\rightarrow \mathbb{R}^d \nabla^M f = \nabla^i f \frac{\partial F}{\partial x_i} = g^{ij} \frac{\partial f}{\partial x_j} \frac{\partial F}{\partial x_i}, g_{ij}=\langle\frac{\partial F}{\partial x_i}, \frac{\partial F}{\partial x_j}\rangle F \frac{\partial F}{\partial x_i} d=3","['differential-geometry', 'riemannian-geometry', 'surfaces']"
34,Contact form in Polar Coordinates,Contact form in Polar Coordinates,,"While going through Etnyres Lectures on Contact Topology (which can be found here ) Example 2.8 two questions came up He uses cylindrcal coordinates $(r,\theta, z)$ to define a 1-form $\alpha_2 = dz +r^2d\theta$ on $\mathbb{R}^3$ and it is not clear to me how this is well defined. I would interpret this as follows: $\theta$ should be a function $\mathbb{R}^3\to\mathbb{R}$ sending a point $p$ to the angle of the projection onto the xy-plane of p with respect to the ray along the x-axis going out to + $\infty$ , $d\theta$ would then be the exterior derivative. However, $\theta$ does not  define a smooth map on all of $\mathbb{R}^3$ because of problems at the origin. My question is then, how should one interpret this definition of $\alpha_2$ . He claims that $\alpha_2$ defines a contact form, that is $\alpha_2\wedge d\alpha_2\neq 0$ , but his computation shows that $$\alpha_2\wedge d\alpha_2 = 2rdr\wedge d\theta\wedge dz$$ which gives the trivial form at the origin since $r(0,0,0) = 0$ . I thought the contact condition implies that the form is non-trivial at every point $p\in\mathbb{R}^3$ , or is it enough that the form is non-trivial at a single point?","While going through Etnyres Lectures on Contact Topology (which can be found here ) Example 2.8 two questions came up He uses cylindrcal coordinates to define a 1-form on and it is not clear to me how this is well defined. I would interpret this as follows: should be a function sending a point to the angle of the projection onto the xy-plane of p with respect to the ray along the x-axis going out to + , would then be the exterior derivative. However, does not  define a smooth map on all of because of problems at the origin. My question is then, how should one interpret this definition of . He claims that defines a contact form, that is , but his computation shows that which gives the trivial form at the origin since . I thought the contact condition implies that the form is non-trivial at every point , or is it enough that the form is non-trivial at a single point?","(r,\theta, z) \alpha_2 = dz +r^2d\theta \mathbb{R}^3 \theta \mathbb{R}^3\to\mathbb{R} p \infty d\theta \theta \mathbb{R}^3 \alpha_2 \alpha_2 \alpha_2\wedge d\alpha_2\neq 0 \alpha_2\wedge d\alpha_2 = 2rdr\wedge d\theta\wedge dz r(0,0,0) = 0 p\in\mathbb{R}^3","['differential-geometry', 'differential-forms', 'contact-geometry']"
35,How to prove that $\phi=f:S\rightarrow \mathbb{S}^2$ is a bijection without using $h$?,How to prove that  is a bijection without using ?,\phi=f:S\rightarrow \mathbb{S}^2 h,"If $f:\mathbb{R}^3\rightarrow \mathbb{R}$ given by $f(a,b,c)=e^{a^2}+e^{b^2}+e^{c^2}$ defining $h:\mathbb{R}\rightarrow \mathbb{R}$ by $h(t)= f(ta, tb, tc)$ , we have that $h'(t)>0$ give us all conditions to define a bijection (diffeomorphism) $\phi=f:S\rightarrow \mathbb{S}^2$ . Would someone explain this bijection please? And also help me to find another way to prove this? ( $S=\{(a, b, c)\in \mathbb{R}^3: f(a, b, c)=a\}\; \text{with}\; a>3$ )","If given by defining by , we have that give us all conditions to define a bijection (diffeomorphism) . Would someone explain this bijection please? And also help me to find another way to prove this? ( )","f:\mathbb{R}^3\rightarrow \mathbb{R} f(a,b,c)=e^{a^2}+e^{b^2}+e^{c^2} h:\mathbb{R}\rightarrow \mathbb{R} h(t)= f(ta, tb, tc) h'(t)>0 \phi=f:S\rightarrow \mathbb{S}^2 S=\{(a, b, c)\in \mathbb{R}^3: f(a, b, c)=a\}\; \text{with}\; a>3","['differential-geometry', 'differential-topology', 'diffeomorphism']"
36,Proving that Gauss map $N: S \rightarrow \mathbb{S}^{2}$ is surjective,Proving that Gauss map  is surjective,N: S \rightarrow \mathbb{S}^{2},"I'm taking a course in elementary differential geometry, and there was the following problem. Let $S \subset \mathbb{R}^{3}$ be a non-empty, compact, and connected surface. Assume that (by the ""Jordan-Brouwer"" theorem) we can talk about ""inside and outside"" components of $S$ . Let $N(p)$ be the outward normal vector at $p \in S$ . Prove that the Gauss map $N: S \rightarrow \mathbb{S}^{2}$ is surjective. Solution: Fix a unit vector $v \in \mathbb{S}^{2},$ and consider the family of planes $$ P_{c}=\left\{x \in \mathbb{R}^{3} \mid\langle x, v\rangle=c\right\}, \quad c \in \mathbb{R} $$ Since $S$ is compact, the function $x \mapsto\langle x, v\rangle$ on $S$ achieves its maximum value at some point $p \in S$ . Define $c_{0}=\langle p, v\rangle$ . Then $S \cap P_{c}$ is empty for all $c>c_{0}$ whereas $S \cap P_{c_{0}}$ is not empty. We claim that $v=N(p)$ . To see this, consider an arbitrary smooth curve $\alpha:(-\epsilon, \epsilon) \rightarrow S$ , with $\alpha(0)=p$ and $\alpha^{\prime}(0)=w \in T_{p} S$ . Since $f(t)=\langle\alpha(t), v\rangle$ is maximised at $t=0$ , we must have $0=f^{\prime}(0)=\langle w, v\rangle$ . Therefore, $v$ is orthogonal to any tangent vector to $S$ at $p$ . In fact, $v$ must be the outward unit normal vector, since all of $S$ lies in the half-space $\langle x, v\rangle \leq\langle p, v\rangle$ . As $v \in \mathbb{S}^{2}$ was arbitrary, the Gauss map $N: S \rightarrow \mathbb{S}^{2}$ must be surjective. I don't understand the line ""In fact, $v$ must be the outward unit normal vector, since all of $S$ lies in the half-space $\langle x, v\rangle \leq\langle p, v\rangle$ ."" Why is this? I can't understand this intuitively, nor can I understand it formally (with an actual proof). Does anyone know why this is?","I'm taking a course in elementary differential geometry, and there was the following problem. Let be a non-empty, compact, and connected surface. Assume that (by the ""Jordan-Brouwer"" theorem) we can talk about ""inside and outside"" components of . Let be the outward normal vector at . Prove that the Gauss map is surjective. Solution: Fix a unit vector and consider the family of planes Since is compact, the function on achieves its maximum value at some point . Define . Then is empty for all whereas is not empty. We claim that . To see this, consider an arbitrary smooth curve , with and . Since is maximised at , we must have . Therefore, is orthogonal to any tangent vector to at . In fact, must be the outward unit normal vector, since all of lies in the half-space . As was arbitrary, the Gauss map must be surjective. I don't understand the line ""In fact, must be the outward unit normal vector, since all of lies in the half-space ."" Why is this? I can't understand this intuitively, nor can I understand it formally (with an actual proof). Does anyone know why this is?","S \subset \mathbb{R}^{3} S N(p) p \in S N: S \rightarrow \mathbb{S}^{2} v \in \mathbb{S}^{2}, 
P_{c}=\left\{x \in \mathbb{R}^{3} \mid\langle x, v\rangle=c\right\}, \quad c \in \mathbb{R}
 S x \mapsto\langle x, v\rangle S p \in S c_{0}=\langle p, v\rangle S \cap P_{c} c>c_{0} S \cap P_{c_{0}} v=N(p) \alpha:(-\epsilon, \epsilon) \rightarrow S \alpha(0)=p \alpha^{\prime}(0)=w \in T_{p} S f(t)=\langle\alpha(t), v\rangle t=0 0=f^{\prime}(0)=\langle w, v\rangle v S p v S \langle x, v\rangle \leq\langle p, v\rangle v \in \mathbb{S}^{2} N: S \rightarrow \mathbb{S}^{2} v S \langle x, v\rangle \leq\langle p, v\rangle","['differential-geometry', 'solution-verification', 'surfaces']"
37,Best books for self-studying differential geometry,Best books for self-studying differential geometry,,Next semester (fall 2021) I am planning on taking a grad-student level differential topology course but I have never studied differential geometry which is a pre-requisite for the course. My plan is to self-study in the summer and this semester so that I do not have to waste a semester taking a differential geometry course which will ruin my schedule. I am looking for a book that covers all of the following topics (ideally) but at least most of them: any suggestions would be welcome.,Next semester (fall 2021) I am planning on taking a grad-student level differential topology course but I have never studied differential geometry which is a pre-requisite for the course. My plan is to self-study in the summer and this semester so that I do not have to waste a semester taking a differential geometry course which will ruin my schedule. I am looking for a book that covers all of the following topics (ideally) but at least most of them: any suggestions would be welcome.,,"['differential-geometry', 'differential-topology', 'book-recommendation']"
38,Kulkarni-Nomizu identity,Kulkarni-Nomizu identity,,"I am trying to prove the identity $\newcommand\KN{\bigcirc \kern-2.5ex\wedge \;}$ $$ g(T,h \KN\, g)=4g({\rm tr}_g T,h) $$ where $h$ is a symmetric tensor of type $(0,2)$ , $T$ is an algebraic curvature tensor and $g$ is a metric for a vector space $V$ . Here, $h \KN\, g$ denotes the Kulkarni-Nomizu product between $h$ and $g$ . In my attempt, I wrote in coordinates the left part of the equality: $$ T_{ijkl}(h_{mp}g_{no}+h_{no}g_{mp}-h_{mo}g_{np}-h_{np}g_{mo})g^{im}g^{jk}g^{ko}g^{lp} $$ using the formula for the inner product of covariant tensors, but I don't get anywhere. Any help?","I am trying to prove the identity where is a symmetric tensor of type , is an algebraic curvature tensor and is a metric for a vector space . Here, denotes the Kulkarni-Nomizu product between and . In my attempt, I wrote in coordinates the left part of the equality: using the formula for the inner product of covariant tensors, but I don't get anywhere. Any help?","\newcommand\KN{\bigcirc
\kern-2.5ex\wedge \;} 
g(T,h \KN\, g)=4g({\rm tr}_g T,h)
 h (0,2) T g V h \KN\, g h g 
T_{ijkl}(h_{mp}g_{no}+h_{no}g_{mp}-h_{mo}g_{np}-h_{np}g_{mo})g^{im}g^{jk}g^{ko}g^{lp}
","['differential-geometry', 'riemannian-geometry', 'tensors', 'curvature']"
39,$x^3+y^3-3xy+C=0$ is a smooth curve for $C\neq0$ or $1$,is a smooth curve for  or,x^3+y^3-3xy+C=0 C\neq0 1,"Show that $x^3+y^3-3xy+C=0$ describes a smooth curve for $C\neq0$ and $C\neq1$ . For $C=0$ , the curve self-intersects at the origin, so cannot be smooth. For $C=1$ , the curve contains an isolated point at $(x,y)=(1,1)$ . However, I'm not sure how to show that the curve is smooth for all other values of $C$ . I guess I'd be done if I could find some smooth parametrisation for the curve (that is injective and has non-zero derivative).","Show that describes a smooth curve for and . For , the curve self-intersects at the origin, so cannot be smooth. For , the curve contains an isolated point at . However, I'm not sure how to show that the curve is smooth for all other values of . I guess I'd be done if I could find some smooth parametrisation for the curve (that is injective and has non-zero derivative).","x^3+y^3-3xy+C=0 C\neq0 C\neq1 C=0 C=1 (x,y)=(1,1) C","['calculus', 'differential-geometry']"
40,Mean curvature of graph over its tangent plane,Mean curvature of graph over its tangent plane,,"Let $S$ be a regular surface in $\mathbb{R}^3$ and $p\in S$ a point on the surface. By the implicit function theorem $S$ can be locally written as a graph of a function, e.g. $V\cap S = \{ (x,y,z) \in \mathbb{R}^3 : (x,y)\in U, z=f(x,y)\}$ for some open neighbourhood $V$ of $p$ , open set $U\subset \mathbb{R}^2$ and some smooth function $f: U \rightarrow \mathbb{R}.$ By choosing local coordinates we can identify $U$ as part of the tangent plane of $S$ at $p$ , furthermore we can set $f^{-1}(p)=(0,0)$ . In this case, the mean curvature at $p$ is given by $H=\frac{f_{xx}\;\;+f_{yy}}{2}$ (average of second derivatives at $p$ ) and principal curvatures are $f_{xx},f_{yy}$ . Is this correct? If it is, how can one describe this more precisely than ""choosing local coordinates...""? If it is not, how could I achieve a similar result (surface as graph over its tangent plane and easy formula of $H$ )? Thank you.","Let be a regular surface in and a point on the surface. By the implicit function theorem can be locally written as a graph of a function, e.g. for some open neighbourhood of , open set and some smooth function By choosing local coordinates we can identify as part of the tangent plane of at , furthermore we can set . In this case, the mean curvature at is given by (average of second derivatives at ) and principal curvatures are . Is this correct? If it is, how can one describe this more precisely than ""choosing local coordinates...""? If it is not, how could I achieve a similar result (surface as graph over its tangent plane and easy formula of )? Thank you.","S \mathbb{R}^3 p\in S S V\cap S = \{ (x,y,z) \in \mathbb{R}^3 : (x,y)\in U, z=f(x,y)\} V p U\subset \mathbb{R}^2 f: U \rightarrow \mathbb{R}. U S p f^{-1}(p)=(0,0) p H=\frac{f_{xx}\;\;+f_{yy}}{2} p f_{xx},f_{yy} H","['differential-geometry', 'surfaces', 'curvature', 'tangent-spaces']"
41,More Examples of Positive Measures on Manifolds,More Examples of Positive Measures on Manifolds,,"Given a smooth manifold $M$ , there are several ways of constructing measures on $M$ . The most common procedure I've seen is by starting with a $(0,2)$ tensor field $T$ on $M$ , and defining for each chart $(U,x)$ , the function $\rho_x := \sqrt{|\det T_{(x), ij}|}$ . These functions then give us a non-negative scalar density, $\rho$ on $M$ . Using this scalar density, we can essentially (chart by chart) ""pull back"" the Lebesgue measure to get a well-defined positive measure $\lambda_{\rho}$ on $M$ . For example if we're on a (pseudo)-Riemannian manifold, we can use the metric tensor to get the usual Riemannian volume measure. If we're on a symplectic manifold, we can use this same recipe with the symplectic form $\omega$ (or equivalently we just take $\left|\frac{\omega^n}{n!}\right|$ ... where we think of a scalar density as a section of an appropriate bundle). Now, my question is, can someone provide me some interesting examples where other types of measures naturally arise; for example are there some other structures which are studied (aside from Riemannian/symplectic, since these are the only two I know) in more advanced areas of geometry/analysis from which a natural notion of a positive measure on a manifold arises. Also, could you provide a (brief) explanation of where such a construction is used/why it is useful. I'm mainly asking to just broaden my perspective. Thanks in advance.","Given a smooth manifold , there are several ways of constructing measures on . The most common procedure I've seen is by starting with a tensor field on , and defining for each chart , the function . These functions then give us a non-negative scalar density, on . Using this scalar density, we can essentially (chart by chart) ""pull back"" the Lebesgue measure to get a well-defined positive measure on . For example if we're on a (pseudo)-Riemannian manifold, we can use the metric tensor to get the usual Riemannian volume measure. If we're on a symplectic manifold, we can use this same recipe with the symplectic form (or equivalently we just take ... where we think of a scalar density as a section of an appropriate bundle). Now, my question is, can someone provide me some interesting examples where other types of measures naturally arise; for example are there some other structures which are studied (aside from Riemannian/symplectic, since these are the only two I know) in more advanced areas of geometry/analysis from which a natural notion of a positive measure on a manifold arises. Also, could you provide a (brief) explanation of where such a construction is used/why it is useful. I'm mainly asking to just broaden my perspective. Thanks in advance.","M M (0,2) T M (U,x) \rho_x := \sqrt{|\det T_{(x), ij}|} \rho M \lambda_{\rho} M \omega \left|\frac{\omega^n}{n!}\right|","['differential-geometry', 'riemannian-geometry', 'volume', 'symplectic-geometry', 'geometric-measure-theory']"
42,Tensor contraction via universal property of the tensor product,Tensor contraction via universal property of the tensor product,,"I'm having a little trouble with the definition of the contraction operation for tensors via the universal property of the tensor product. The construction I've found (for instance, suggested here and outlined here ) goes as follows: suppose we have a commutative ring $R$ (for example, the smooth real valued functions on a manifold) and an $R$ -module $E$ which is finitely generated and projective (for example, the module of smooth sections of the tangent bundle of a smooth manifold), with dual $E^{*}$ . Consider the tensor product $$ \bigotimes_{i=1}^{r}E \otimes \bigotimes_{i=1}^{s}E^{*} $$ and call $\otimes^r_s$ the corresponding canonical multilinear mapping between the cartesian product to the tensor product. To define the contraction $C^\ell_{k}$ between the $\ell$ -th $E$ factor and the $k$ -th $E^{*}$ factor, one defines the (suppossedly) multilinear mapping: $$ f:\prod_{i=1}^{r}E \times \prod_{i=1}^{s}E^{*}\longrightarrow \prod_{i=1}^{r-1}E \times \prod_{i=1}^{s-1}E^{*}  $$ $$ f:(U_1,\ldots,U_r,\omega^1,\ldots,\omega^s)\mapsto \omega^k(U_\ell)(U_1,\ldots,\widehat{U_\ell},\ldots,U_r,\omega_1,\ldots,\widehat{\omega^k},\ldots,\omega_s) $$ where hats mean ommitted arguments, and where (I presume) the product spaces are understood to be $R$ -modules through the usual construction as direct sums/products. Then by the universal property of the tensor product, there exists a unique module homomorphism $$ h:\bigotimes_{i=1}^{r}E \otimes \bigotimes_{i=1}^{s}E^{*}\longrightarrow \prod_{i=1}^{r-1}E \times \prod_{i=1}^{s-1}E^{*} $$ such that $f = h\circ\otimes^r_s$ ; then the contraction would suppossedly be defined as $C^\ell_k:= \otimes^{r-1}_{s-1}\circ h$ , and the universal property should yield: $$ C^\ell_k(U_1\otimes\cdots\otimes U_r\otimes\omega^1\otimes\cdots\otimes\omega^s) = \omega^k(U_\ell)U_1\otimes\cdots,\otimes\widehat{U_\ell}\otimes\cdots\otimes U_r\otimes\omega_1\otimes\cdots\otimes\widehat{\omega^k}\otimes\cdots\otimes\omega_s $$ I just can't get this to work. My questions/problems are: How exactly is $f$ suppossed to be multilinear? If addition and multiplication by elements of $R$ on $\prod_{i=1}^{r-1}E \times \prod_{i=1}^{s-1}E^{*}$ are defined component-wise, then it seems that $f$ would only be multilinear in the two arguments that are being contracted. Even if we admit that $h$ exists and is unique, if we understand that multiplication by elements of $R$ acts componentwise, then we should have: $$ \otimes^{r-1}_{s-1}\left(\omega^k(U_\ell)(U_1,\ldots,\widehat{U_\ell},\ldots,U_r,\omega_1,\ldots,\widehat{\omega^k},\ldots,\omega_s)\right) = \omega^k(U_\ell)^{(r-1)(s-1)}U_1\otimes\cdots,\otimes\widehat{U_\ell}\otimes\cdots\otimes U_r\otimes\omega_1\otimes\cdots\otimes\widehat{\omega^k}\otimes\cdots\otimes\omega_s $$ since the factor $\omega^k(U_\ell)$ multiplies all the components and thus appears $(r-1)(s-1)$ times in the tensor product. What am I missing here?","I'm having a little trouble with the definition of the contraction operation for tensors via the universal property of the tensor product. The construction I've found (for instance, suggested here and outlined here ) goes as follows: suppose we have a commutative ring (for example, the smooth real valued functions on a manifold) and an -module which is finitely generated and projective (for example, the module of smooth sections of the tangent bundle of a smooth manifold), with dual . Consider the tensor product and call the corresponding canonical multilinear mapping between the cartesian product to the tensor product. To define the contraction between the -th factor and the -th factor, one defines the (suppossedly) multilinear mapping: where hats mean ommitted arguments, and where (I presume) the product spaces are understood to be -modules through the usual construction as direct sums/products. Then by the universal property of the tensor product, there exists a unique module homomorphism such that ; then the contraction would suppossedly be defined as , and the universal property should yield: I just can't get this to work. My questions/problems are: How exactly is suppossed to be multilinear? If addition and multiplication by elements of on are defined component-wise, then it seems that would only be multilinear in the two arguments that are being contracted. Even if we admit that exists and is unique, if we understand that multiplication by elements of acts componentwise, then we should have: since the factor multiplies all the components and thus appears times in the tensor product. What am I missing here?","R R E E^{*} 
\bigotimes_{i=1}^{r}E \otimes \bigotimes_{i=1}^{s}E^{*}
 \otimes^r_s C^\ell_{k} \ell E k E^{*} 
f:\prod_{i=1}^{r}E \times \prod_{i=1}^{s}E^{*}\longrightarrow \prod_{i=1}^{r-1}E \times \prod_{i=1}^{s-1}E^{*} 
 
f:(U_1,\ldots,U_r,\omega^1,\ldots,\omega^s)\mapsto \omega^k(U_\ell)(U_1,\ldots,\widehat{U_\ell},\ldots,U_r,\omega_1,\ldots,\widehat{\omega^k},\ldots,\omega_s)
 R 
h:\bigotimes_{i=1}^{r}E \otimes \bigotimes_{i=1}^{s}E^{*}\longrightarrow \prod_{i=1}^{r-1}E \times \prod_{i=1}^{s-1}E^{*}
 f = h\circ\otimes^r_s C^\ell_k:= \otimes^{r-1}_{s-1}\circ h 
C^\ell_k(U_1\otimes\cdots\otimes U_r\otimes\omega^1\otimes\cdots\otimes\omega^s) = \omega^k(U_\ell)U_1\otimes\cdots,\otimes\widehat{U_\ell}\otimes\cdots\otimes U_r\otimes\omega_1\otimes\cdots\otimes\widehat{\omega^k}\otimes\cdots\otimes\omega_s
 f R \prod_{i=1}^{r-1}E \times \prod_{i=1}^{s-1}E^{*} f h R 
\otimes^{r-1}_{s-1}\left(\omega^k(U_\ell)(U_1,\ldots,\widehat{U_\ell},\ldots,U_r,\omega_1,\ldots,\widehat{\omega^k},\ldots,\omega_s)\right) = \omega^k(U_\ell)^{(r-1)(s-1)}U_1\otimes\cdots,\otimes\widehat{U_\ell}\otimes\cdots\otimes U_r\otimes\omega_1\otimes\cdots\otimes\widehat{\omega^k}\otimes\cdots\otimes\omega_s
 \omega^k(U_\ell) (r-1)(s-1)","['differential-geometry', 'modules', 'tensor-products', 'multilinear-algebra', 'contraction-operator']"
43,Pullback of an $n$-form by the inclusion map.,Pullback of an -form by the inclusion map.,n,"Let $\Omega = dx^1 \wedge \ldots \wedge dx^n$ an $n$ -form on $\mathbb R^n$ and $$\alpha = \sum_{j = 1}^n (-1)^{j-1}x^j dx^1 \wedge \ldots \wedge dx^{j-1} \wedge dx^{j+1}\wedge \ldots \wedge dx^n.$$ In order to prove Brouwer fixed point, I've to show that for every smooth function $F: B^n \to \mathbb S^{n-1}$ , with $B^n$ the unit ball in $\mathbb R^n$ , such that $F(x) = x$ for all $x \in \mathbb S^{n-1} \subset B^n$ , we have $d(F^* \circ \iota^*)(\alpha) = 0$ where $\iota: \mathbb S^n \to \mathbb R^n$ is the inclusion map. Since the exterior derivative commute with the pullback, I find $$d(F^* \circ \iota^*)(\alpha) = (F^* \circ \iota^*)(d\alpha)= n (F^* \circ \iota^*)(\Omega).$$ Now here is my problem: As $\Omega$ is an $n$ -form on $\mathbb R^n$ , $\iota ^* \Omega$ is an $n$ -form on $\mathbb S^{n-1}$ which is zero because the dimension $\mathbb S^{n-1}$ is $n-1$ . Is this argument correct ? It seems too easy, I didn't use anywhere the fact $F|_{\mathbb S^{n-1}} = \text{Id}_{\mathbb S^{n-1}}$ . More generally, if we've $N \subset M$ with $\dim N = n < \dim M = m$ , does the pullback by the inclusion map of a $k$ -form (for $k > n$ ) always vanishes ?","Let an -form on and In order to prove Brouwer fixed point, I've to show that for every smooth function , with the unit ball in , such that for all , we have where is the inclusion map. Since the exterior derivative commute with the pullback, I find Now here is my problem: As is an -form on , is an -form on which is zero because the dimension is . Is this argument correct ? It seems too easy, I didn't use anywhere the fact . More generally, if we've with , does the pullback by the inclusion map of a -form (for ) always vanishes ?",\Omega = dx^1 \wedge \ldots \wedge dx^n n \mathbb R^n \alpha = \sum_{j = 1}^n (-1)^{j-1}x^j dx^1 \wedge \ldots \wedge dx^{j-1} \wedge dx^{j+1}\wedge \ldots \wedge dx^n. F: B^n \to \mathbb S^{n-1} B^n \mathbb R^n F(x) = x x \in \mathbb S^{n-1} \subset B^n d(F^* \circ \iota^*)(\alpha) = 0 \iota: \mathbb S^n \to \mathbb R^n d(F^* \circ \iota^*)(\alpha) = (F^* \circ \iota^*)(d\alpha)= n (F^* \circ \iota^*)(\Omega). \Omega n \mathbb R^n \iota ^* \Omega n \mathbb S^{n-1} \mathbb S^{n-1} n-1 F|_{\mathbb S^{n-1}} = \text{Id}_{\mathbb S^{n-1}} N \subset M \dim N = n < \dim M = m k k > n,"['differential-geometry', 'differential-forms', 'pullback']"
44,Transformation law for metric tensor,Transformation law for metric tensor,,"For my topology class I have to calculate the transformation law of the metric tensor $g = g_{ij} \, dx^i \otimes dx^j$ under a coordinate transformation $x \longmapsto y=(x)$ . My approach: $$ g(y) \enspace = \enspace g_{ij}(y) \, \big( dy^i \otimes dy^j \big) \enspace = \enspace g_{ij}(y) \, \bigg( \frac{\partial y^i}{\partial x^k} dx^k \otimes \frac{\partial y^j}{\partial x^{\ell}} dx^{\ell} \bigg) $$ $$= \enspace g_{ij}(y) \, \frac{\partial y^i}{\partial x^k} \, \frac{\partial y^j}{\partial x^{\ell}} \, \big(  dx^k \otimes dx^{\ell}  \big) \quad .$$ ${}$ $\large \textbf{1.)} \enspace$ How do I know how the components $g_{ij}(y)$ transform? ${}$ $\large \textbf{2.)} \enspace$ If I identify the terms $\dfrac{\partial y^i}{\partial x^k}$ with the Jacobian $J^i{}_j$ , then I get $$ g(y) \enspace = \enspace g_{ij}(y) \, J^i{}_k \, J^j{}_{\ell} \, \big(  dx^k \otimes dx^{\ell}  \big) \quad .$$ Is it then true that: $$ g(y) \enspace = \enspace \big( \boldsymbol{\operatorname{J}}^T \cdot \boldsymbol{\operatorname{g}}(y) \cdot \boldsymbol{\operatorname{J}} \big)_{k\ell} \, \big(  dx^k \otimes dx^{\ell}  \big) \quad ? $$ ${}$ $\large \textbf{3.)} \enspace$ If I were to calculate the determinant of $g(y)$ , how would I handle the tensor product in a mathematical rigorous way? Assuming that the equation in 2.) is true, would it just be $$ \det g(y) \enspace = \enspace \det \boldsymbol{\operatorname{J}}^T \cdot \det \boldsymbol{\operatorname{g}}(y) \cdot \det \boldsymbol{\operatorname{J}} \quad ? $$","For my topology class I have to calculate the transformation law of the metric tensor under a coordinate transformation . My approach: How do I know how the components transform? If I identify the terms with the Jacobian , then I get Is it then true that: If I were to calculate the determinant of , how would I handle the tensor product in a mathematical rigorous way? Assuming that the equation in 2.) is true, would it just be","g = g_{ij} \, dx^i \otimes dx^j x \longmapsto y=(x)  g(y) \enspace = \enspace g_{ij}(y) \, \big( dy^i \otimes dy^j \big) \enspace = \enspace g_{ij}(y) \, \bigg( \frac{\partial y^i}{\partial x^k} dx^k \otimes \frac{\partial y^j}{\partial x^{\ell}} dx^{\ell} \bigg)  = \enspace g_{ij}(y) \, \frac{\partial y^i}{\partial x^k} \, \frac{\partial y^j}{\partial x^{\ell}} \, \big(  dx^k \otimes dx^{\ell}  \big) \quad . {} \large \textbf{1.)} \enspace g_{ij}(y) {} \large \textbf{2.)} \enspace \dfrac{\partial y^i}{\partial x^k} J^i{}_j  g(y) \enspace = \enspace g_{ij}(y) \, J^i{}_k \, J^j{}_{\ell} \, \big(  dx^k \otimes dx^{\ell}  \big) \quad .  g(y) \enspace = \enspace \big( \boldsymbol{\operatorname{J}}^T \cdot \boldsymbol{\operatorname{g}}(y) \cdot \boldsymbol{\operatorname{J}} \big)_{k\ell} \, \big(  dx^k \otimes dx^{\ell}  \big) \quad ?  {} \large \textbf{3.)} \enspace g(y)  \det g(y) \enspace = \enspace \det \boldsymbol{\operatorname{J}}^T \cdot \det \boldsymbol{\operatorname{g}}(y) \cdot \det \boldsymbol{\operatorname{J}} \quad ? ","['differential-geometry', 'tensor-products', 'tensors', 'transformation']"
45,Expressing hyperelliptic integrals through series,Expressing hyperelliptic integrals through series,,"Suppose I want to evaluate the following integral, but non-numerically : $$\int_1^2\frac1{\sqrt{x^5-x+1}}\,dx$$ This is, of course, a hyperelliptic integral, which cannot in general be expressed in terms of elementary (or even elliptic) functions. Furthermore, it is quite well-known that the denominator polynomial here is not even solvable in radicals, but just assume we know the roots. (It is far easier to numerically find polynomial roots than to numerically integrate.) Byrd and Friedman have this to say about the matter: For the evaluation of hyperelliptic integrals, one must usually resort to direct numerical integration or to the use of complicated series expansions. This second part has me intrigued. How can I express the integral above using series, hypergeometric or otherwise? Is there a general procedure to derive such a series representation? Note that if there were just two terms in the polynomial, the integrand would be expressible as a binomial series; the integral itself would then be expressible using the $_2F_1$ function. But there are three terms here, so $_2F_1$ cannot be immediately applied. Taylor and Padé expansions are not quite desirable either, since they require much work at extended precision and will not converge over the entire domain if truncated.","Suppose I want to evaluate the following integral, but non-numerically : This is, of course, a hyperelliptic integral, which cannot in general be expressed in terms of elementary (or even elliptic) functions. Furthermore, it is quite well-known that the denominator polynomial here is not even solvable in radicals, but just assume we know the roots. (It is far easier to numerically find polynomial roots than to numerically integrate.) Byrd and Friedman have this to say about the matter: For the evaluation of hyperelliptic integrals, one must usually resort to direct numerical integration or to the use of complicated series expansions. This second part has me intrigued. How can I express the integral above using series, hypergeometric or otherwise? Is there a general procedure to derive such a series representation? Note that if there were just two terms in the polynomial, the integrand would be expressible as a binomial series; the integral itself would then be expressible using the function. But there are three terms here, so cannot be immediately applied. Taylor and Padé expansions are not quite desirable either, since they require much work at extended precision and will not converge over the entire domain if truncated.","\int_1^2\frac1{\sqrt{x^5-x+1}}\,dx _2F_1 _2F_1","['calculus', 'integration', 'differential-geometry', 'definite-integrals', 'hypergeometric-function']"
46,Exercise about vector fields on smooth manifolds,Exercise about vector fields on smooth manifolds,,"Let $V$ be a vector field on $\mathbb{R}^2$ . If $[\frac{\partial}{\partial x},V]=V=[V,\frac{\partial}{\partial y}]$ , determine $V$ . The first way to look at this is by observing vector fields primarily as functions from $\mathfrak{F}(\mathbb{R}^2)$ to $\mathfrak{F}(\mathbb{R}^2)$ . Obviously, if $D=\frac{\partial}{\partial x}+\frac{\partial}{\partial y}$ , then $[D,V]=0$ . Writing it out this way: $$\frac{\partial}{\partial x}(V(f))-V(\frac{\partial}{\partial x}f)=V(f)=V(\frac{\partial}{\partial y}f)-\frac{\partial}{\partial y}(V(f))$$ , we can also see that $$\frac{\partial}{\partial x}(V(f))=V(f+\frac{\partial}{\partial x}f)$$ $$\frac{\partial}{\partial y}(V(f))=V(\frac{\partial}{\partial y}f-f)$$ . Frankly, I don't know what to do with this. Jacobi identity doesn't give me anything new. The second way to look at this is to observe these equations locally, at some point $p\in M$ . Then we'd be able to write $V$ as $V=\alpha \frac{\partial}{\partial x}+\beta \frac{\partial}{\partial y}$ (or at least I hope we can do this?). However, observed locally, given equations become $0=V=0$ , which looks like it's incorrect, because it would mean I have more data than I need. Other idea was to use the fact that we're working in $\mathbb{R}^2$ and that this is a manifold with one chart basically, but this comes down to the previous idea and the result is the same. I feel like $[D,V]=0$ is significant here, but I don't know what to do with it. Also, it seems that my attempt at observing things locally is completely flawed. Can we even use the fact that partial derivatives constitute a basis of tangent space the way I've used it?","Let be a vector field on . If , determine . The first way to look at this is by observing vector fields primarily as functions from to . Obviously, if , then . Writing it out this way: , we can also see that . Frankly, I don't know what to do with this. Jacobi identity doesn't give me anything new. The second way to look at this is to observe these equations locally, at some point . Then we'd be able to write as (or at least I hope we can do this?). However, observed locally, given equations become , which looks like it's incorrect, because it would mean I have more data than I need. Other idea was to use the fact that we're working in and that this is a manifold with one chart basically, but this comes down to the previous idea and the result is the same. I feel like is significant here, but I don't know what to do with it. Also, it seems that my attempt at observing things locally is completely flawed. Can we even use the fact that partial derivatives constitute a basis of tangent space the way I've used it?","V \mathbb{R}^2 [\frac{\partial}{\partial x},V]=V=[V,\frac{\partial}{\partial y}] V \mathfrak{F}(\mathbb{R}^2) \mathfrak{F}(\mathbb{R}^2) D=\frac{\partial}{\partial x}+\frac{\partial}{\partial y} [D,V]=0 \frac{\partial}{\partial x}(V(f))-V(\frac{\partial}{\partial x}f)=V(f)=V(\frac{\partial}{\partial y}f)-\frac{\partial}{\partial y}(V(f)) \frac{\partial}{\partial x}(V(f))=V(f+\frac{\partial}{\partial x}f) \frac{\partial}{\partial y}(V(f))=V(\frac{\partial}{\partial y}f-f) p\in M V V=\alpha \frac{\partial}{\partial x}+\beta \frac{\partial}{\partial y} 0=V=0 \mathbb{R}^2 [D,V]=0","['differential-geometry', 'smooth-manifolds', 'vector-fields', 'tangent-spaces', 'tangent-bundle']"
47,simple calculation with Christoffel symbols on Poincare half-plane,simple calculation with Christoffel symbols on Poincare half-plane,,"Equip $H=\{(x,y):y>0, x,y \in \mathbb{R}\}$ with the metric $$ds^2=\frac{dx^2+dy^2}{y^2}.$$ ( https://en.wikipedia.org/wiki/Poincar%C3%A9_half-plane_model ). I want to show that the sectional curvature $$ K(\partial_i,\partial_j) = \frac{\langle R(\partial_i,\partial_j)\partial_j,\partial_i\rangle}{\det(g)}=\frac{R_{ijji}}{\det(g)}$$ is $-1$ . I know this is a simple calculation, but for some reason I'm off by a sign and it's driving me nuts. Here $g$ is the metric in matrix form: $$g=(g_{ij})= \begin{pmatrix} \frac{1}{y^2} & 0 \\ 0 & \frac{1}{y^2} \end{pmatrix}.$$ Let $\{ \partial_1=\partial/ \partial x,\partial_2=\partial/ \partial y\}$ be the coordinate basis, and just consider the computation of $K(\partial_1,\partial_2).$ So I just need to compute $R_{1221}=g_{1m}R^m_{\ 221}=g_{11}R^1_{\ 221}$ , where $$R^i_{jkl}=\partial_k \Gamma^i_{jl}-\partial_l \Gamma^i_{jk}+\Gamma^p_{jl}\Gamma^i_{pk}-\Gamma^p_{jk}\Gamma^i_{pl}.$$ I already have the Christoffel symbols, namely \begin{equation*} \Gamma^1_{12}=\Gamma^1_{21}=-\frac{1}{y},\\ \Gamma^2_{12}=\Gamma^2_{21}=0,\\ \Gamma^1_{11}=\Gamma^1_{22}=0,\\ \Gamma^2_{11}=\frac{1}{y},\\ \Gamma^2_{22}=-\frac{1}{y}.  \end{equation*} So \begin{align*} R^1_{\ 221} &= \partial_2 \Gamma^1_{21}-\partial_1 \Gamma^1_{22}+\Gamma^p_{21}\Gamma^1_{p2}-\Gamma^p_{22}\Gamma^1_{p1}\\ &= \partial_2 \left( -\frac{1}{y}\right)+\Gamma^1_{21}\Gamma^1_{12}-\Gamma^2_{22}\Gamma^1_{21}\\ &= \frac{1}{y^2}+\left(-\frac{1}{y}\right)^2-\left(-\frac{1}{y}\right)^2\\ &= \frac{1}{y^2}. \end{align*} But this gives $R_{1221}=1/y^4$ , and hence $K(\partial_1,\partial_2)=1$ . Please tell me what I'm doing wrong here or what formula is wrong.","Equip with the metric ( https://en.wikipedia.org/wiki/Poincar%C3%A9_half-plane_model ). I want to show that the sectional curvature is . I know this is a simple calculation, but for some reason I'm off by a sign and it's driving me nuts. Here is the metric in matrix form: Let be the coordinate basis, and just consider the computation of So I just need to compute , where I already have the Christoffel symbols, namely So But this gives , and hence . Please tell me what I'm doing wrong here or what formula is wrong.","H=\{(x,y):y>0, x,y \in \mathbb{R}\} ds^2=\frac{dx^2+dy^2}{y^2}.  K(\partial_i,\partial_j) = \frac{\langle R(\partial_i,\partial_j)\partial_j,\partial_i\rangle}{\det(g)}=\frac{R_{ijji}}{\det(g)} -1 g g=(g_{ij})=
\begin{pmatrix}
\frac{1}{y^2} & 0 \\
0 & \frac{1}{y^2}
\end{pmatrix}. \{ \partial_1=\partial/ \partial x,\partial_2=\partial/ \partial y\} K(\partial_1,\partial_2). R_{1221}=g_{1m}R^m_{\ 221}=g_{11}R^1_{\ 221} R^i_{jkl}=\partial_k \Gamma^i_{jl}-\partial_l \Gamma^i_{jk}+\Gamma^p_{jl}\Gamma^i_{pk}-\Gamma^p_{jk}\Gamma^i_{pl}. \begin{equation*}
\Gamma^1_{12}=\Gamma^1_{21}=-\frac{1}{y},\\
\Gamma^2_{12}=\Gamma^2_{21}=0,\\
\Gamma^1_{11}=\Gamma^1_{22}=0,\\
\Gamma^2_{11}=\frac{1}{y},\\
\Gamma^2_{22}=-\frac{1}{y}. 
\end{equation*} \begin{align*}
R^1_{\ 221} &= \partial_2 \Gamma^1_{21}-\partial_1 \Gamma^1_{22}+\Gamma^p_{21}\Gamma^1_{p2}-\Gamma^p_{22}\Gamma^1_{p1}\\
&= \partial_2 \left( -\frac{1}{y}\right)+\Gamma^1_{21}\Gamma^1_{12}-\Gamma^2_{22}\Gamma^1_{21}\\
&= \frac{1}{y^2}+\left(-\frac{1}{y}\right)^2-\left(-\frac{1}{y}\right)^2\\
&= \frac{1}{y^2}.
\end{align*} R_{1221}=1/y^4 K(\partial_1,\partial_2)=1",['differential-geometry']
48,Do curves with $(\phi\circ\gamma_1)'(0)=(\phi\circ\gamma_2)'(0)$ for a chart $\phi$ also have same derivative with respect to another chart?,Do curves with  for a chart  also have same derivative with respect to another chart?,(\phi\circ\gamma_1)'(0)=(\phi\circ\gamma_2)'(0) \phi,"As discussed in this other question , given a manifold $M$ and a point $p\in M$ , we can define its tangent vectors in $T_p M$ as the set of equivalence classes $[\gamma'(0)]$ defined so that $\gamma_1,\gamma_2\in[\gamma'(0)]$ iff $(\phi\circ\gamma_1)'(0)=(\phi\circ\gamma_2)'(0)$ for all coordinate charts $\phi:U\to\mathbb R^n$ , where $p\in U\subset M$ . In this definition, is it sufficient to ask for the curves to have same derivative with respect to one coordinate chart defined around $p$ ? In other words, given two charts $\phi,\tilde\phi:U\to\mathbb R^n$ defined on some neighbourhood of $p$ , suppose $$(\phi\circ\gamma_1)'(0)=(\phi\circ\gamma_2)'(0).$$ Does this imply that $(\tilde\phi\circ\gamma_1)'(0)=(\tilde\phi\circ\gamma_2)'(0)$ ? From the definition of a smooth manifold, I know that $\tilde\phi\circ\phi^{-1}$ is a homeomorphism between $\phi(U)$ and $\tilde\phi(U)$ . I would therefore expect that if $$\phi(\gamma_1(\epsilon))-\phi(\gamma_2(\epsilon)) = o(\epsilon),$$ then the same should hold replacing $\phi\to\tilde\phi$ . However, I'm not sure what properties of $\tilde\phi\circ\phi^{-1}$ I could use to show this.","As discussed in this other question , given a manifold and a point , we can define its tangent vectors in as the set of equivalence classes defined so that iff for all coordinate charts , where . In this definition, is it sufficient to ask for the curves to have same derivative with respect to one coordinate chart defined around ? In other words, given two charts defined on some neighbourhood of , suppose Does this imply that ? From the definition of a smooth manifold, I know that is a homeomorphism between and . I would therefore expect that if then the same should hold replacing . However, I'm not sure what properties of I could use to show this.","M p\in M T_p M [\gamma'(0)] \gamma_1,\gamma_2\in[\gamma'(0)] (\phi\circ\gamma_1)'(0)=(\phi\circ\gamma_2)'(0) \phi:U\to\mathbb R^n p\in U\subset M p \phi,\tilde\phi:U\to\mathbb R^n p (\phi\circ\gamma_1)'(0)=(\phi\circ\gamma_2)'(0). (\tilde\phi\circ\gamma_1)'(0)=(\tilde\phi\circ\gamma_2)'(0) \tilde\phi\circ\phi^{-1} \phi(U) \tilde\phi(U) \phi(\gamma_1(\epsilon))-\phi(\gamma_2(\epsilon)) = o(\epsilon), \phi\to\tilde\phi \tilde\phi\circ\phi^{-1}","['differential-geometry', 'manifolds', 'smooth-manifolds', 'diffeomorphism', 'tangent-bundle']"
49,Example 11.27 in Lee's Introduction to Smooth Manifolds,Example 11.27 in Lee's Introduction to Smooth Manifolds,,"In Lee's ""Introduction to Smooth Manifolds"" there is the following quoted piece of text: Example 11.27: Let $F : \mathbb{R}^3 \to \mathbb{R}^2$ be the map given by $$(u, v) = F(x, y, z) = (x^2y, y\sin z)$$ and let $\omega \in \mathfrak{X}^*(\mathbb{R}^2) $ be the covector field $$\omega = u dv + v du$$ ... Now my interpretation of this is that the author is defining the function $u = \pi_1 \circ F : \mathbb{R}^3 \to \mathbb{R}$ where $\pi_1 : \mathbb{R}^2 \to \mathbb{R}$ is the standard projection function onto the first coordinate. And similar the author is defining $v = \pi_2 \circ F : \mathbb{R}^3 \to \mathbb{R}$ . So essentially the author is saying that $u$ and $v$ are the first and second component functions of $F$ respectively in my interpretation. But if my interpretation is correct then $du$ and $dv$ are covector fields on $\mathbb{R}^3$ and not on $\mathbb{R}^2$ . What is my mistake here and what is the correct interpretation of this piece of text?","In Lee's ""Introduction to Smooth Manifolds"" there is the following quoted piece of text: Example 11.27: Let be the map given by and let be the covector field ... Now my interpretation of this is that the author is defining the function where is the standard projection function onto the first coordinate. And similar the author is defining . So essentially the author is saying that and are the first and second component functions of respectively in my interpretation. But if my interpretation is correct then and are covector fields on and not on . What is my mistake here and what is the correct interpretation of this piece of text?","F : \mathbb{R}^3 \to \mathbb{R}^2 (u, v) = F(x, y, z) = (x^2y, y\sin z) \omega \in \mathfrak{X}^*(\mathbb{R}^2)  \omega = u dv + v du u = \pi_1 \circ F : \mathbb{R}^3 \to \mathbb{R} \pi_1 : \mathbb{R}^2 \to \mathbb{R} v = \pi_2 \circ F : \mathbb{R}^3 \to \mathbb{R} u v F du dv \mathbb{R}^3 \mathbb{R}^2","['differential-geometry', 'smooth-manifolds']"
50,Questions about the moment map,Questions about the moment map,,"Let $(M,\omega)$ be a symplectic manifold endowed with a hamiltonian action of a torus $T$ . Let $\mu : M \longrightarrow {Lie(T)}^*,$ be a moment map associated to this action. Let $S_M =\bigcap\limits_{m \in M} Stab(m)$ , and $s_m$ be its lie algebra. I have two questions about the moment map: I) Why is the moment map  constant on each connected component of $M^T$ ? II) I know that for each $m \in M $ , the image of tangent map of $\mu $ at m is $ Im(T_m \mu)={(s_m)}^\bot = \lbrace \eta \in Lie(T) \mid  \langle \eta , X \rangle = 0 , \forall X \in s_m \rbrace $ . How does this imply that the image of M by the moment map is an affine space directed by the linear space ${(s_m)}^\bot$ ?","Let be a symplectic manifold endowed with a hamiltonian action of a torus . Let be a moment map associated to this action. Let , and be its lie algebra. I have two questions about the moment map: I) Why is the moment map  constant on each connected component of ? II) I know that for each , the image of tangent map of at m is . How does this imply that the image of M by the moment map is an affine space directed by the linear space ?","(M,\omega) T \mu : M \longrightarrow {Lie(T)}^*, S_M =\bigcap\limits_{m \in M} Stab(m) s_m M^T m \in M  \mu   Im(T_m \mu)={(s_m)}^\bot = \lbrace \eta \in Lie(T) \mid 
\langle \eta , X \rangle = 0 , \forall X \in s_m \rbrace  {(s_m)}^\bot","['differential-geometry', 'lie-groups', 'symplectic-geometry', 'moment-map']"
51,Global contact form defining contact structure,Global contact form defining contact structure,,"Let $M$ be a $(2n+1)$ -manifold and $\xi$ a smooth hyperplane field. It's easy to see that for each point $p$ exists a neighborhood $U$ of $p$ such that $\xi_{|U} = \ker \alpha_{U}$ , where $\alpha_U$ is a smooth 1-form. I have read that if $M$ and $\xi$ are both oriented, we can find $\alpha$ $\textbf{globally}$ defined such that $\xi = \ker \alpha$ in all $M$ . I can't see why, I mean, if $M$ is oriented by a volume form $\omega \in \Omega^{2n+1}(M)$ , without supposing any kind of regularity in $\xi$ I don't know how to construct this globally defined 1-form. This exercise is proposed in $\textit{Introductory Lectures on Contact Geometry}$ by John B. Etnyre. EDITED: In this post have a nice proof of this statement. I am going to introduce another proof based on it without using Riemannian metrics. When we say that $\xi$ is orientable we refer to $TM/\xi$ is the trivial bundle (by definition). We will say that $\xi$ is $\textit{corientable}$ . In fact, since $\xi$ is a field of hyperplanes the previous quotient is a line bundle, that is, a 1-dimensional bundle. Suposse $\xi$ is coorientable. If $TM/\xi$ is trivial, there exists a $\textbf{global}$ section throughout $TM/\xi$ and therefore a global section $\overline{\alpha}$ in $(TM/\xi)^*$ . Consider $\pi : TM \to (TM/\xi)^*$ and the pullback $\alpha := \pi^*\overline{\alpha}$ . In fact, this $\alpha$ satisfies $\ker \alpha = \xi$ by definition, and it's globally defined. Therefore we have proved that $$ \xi \text{ coorientable} \Longrightarrow \alpha \text{ globally defined} $$ Reciprocally, assume $\alpha$ globally defining $\xi = \ker \alpha$ throughout $M$ . Then this $\alpha$ induces a non-zero section in $TM/\xi$ , that's mean, $TM/\xi$ is trivial.","Let be a -manifold and a smooth hyperplane field. It's easy to see that for each point exists a neighborhood of such that , where is a smooth 1-form. I have read that if and are both oriented, we can find defined such that in all . I can't see why, I mean, if is oriented by a volume form , without supposing any kind of regularity in I don't know how to construct this globally defined 1-form. This exercise is proposed in by John B. Etnyre. EDITED: In this post have a nice proof of this statement. I am going to introduce another proof based on it without using Riemannian metrics. When we say that is orientable we refer to is the trivial bundle (by definition). We will say that is . In fact, since is a field of hyperplanes the previous quotient is a line bundle, that is, a 1-dimensional bundle. Suposse is coorientable. If is trivial, there exists a section throughout and therefore a global section in . Consider and the pullback . In fact, this satisfies by definition, and it's globally defined. Therefore we have proved that Reciprocally, assume globally defining throughout . Then this induces a non-zero section in , that's mean, is trivial.","M (2n+1) \xi p U p \xi_{|U} = \ker \alpha_{U} \alpha_U M \xi \alpha \textbf{globally} \xi = \ker \alpha M M \omega \in \Omega^{2n+1}(M) \xi \textit{Introductory Lectures on Contact Geometry} \xi TM/\xi \xi \textit{corientable} \xi \xi TM/\xi \textbf{global} TM/\xi \overline{\alpha} (TM/\xi)^* \pi : TM \to (TM/\xi)^* \alpha := \pi^*\overline{\alpha} \alpha \ker \alpha = \xi 
\xi \text{ coorientable} \Longrightarrow \alpha \text{ globally defined}
 \alpha \xi = \ker \alpha M \alpha TM/\xi TM/\xi","['differential-geometry', 'contact-topology']"
52,Killing vectors in Minkowski Metric,Killing vectors in Minkowski Metric,,"Firstly, I know this is a physics-related problem, and I have posted here , but the physics forum seems so much more empty then this one, so here it goes: I was in the process to find the Killing vectors for the Minkowski Metric and I stumbbled into a material that does a different procedure at the very end of the process, in comparisson to usual books and articles I've seen. The reasoning goes as follows: For example, suppose we have found the Killing vector $$K=x \frac{\partial}{\partial_t} + t\frac{\partial}{\partial_x}$$ The way I would check this is a generator for the boost in the x direction is by acting these vectors onto t and x and check they give, respectively, x and t. The way this material I've found does is , they suddenly deffine: $$ \Lambda=\exp[\lambda(x \frac{\partial}{\partial_t} + t\frac{\partial}{\partial_x})]=\sum_{n=0}^{\infty}\frac{1}{n!}\lambda^n (x \frac{\partial}{\partial_t} + t\frac{\partial}{\partial_x})^n $$ then he proceeds to find the explicit form of the boost: $$ \Lambda t = x\sinh \lambda +t\cosh \lambda $$ $$ \Lambda x = t\sinh \lambda +x\cosh \lambda $$ I understand the steps in this process. What I don't get is where did the motivation for the exponentiation come. What does that mean? It seems to me to have something to do with the application of the diffeomorfisms, but I'm not sure. Also, would this be a more correct way to proceed then what I've done?I really would appreciate any comments on this, as well as reccomended material.","Firstly, I know this is a physics-related problem, and I have posted here , but the physics forum seems so much more empty then this one, so here it goes: I was in the process to find the Killing vectors for the Minkowski Metric and I stumbbled into a material that does a different procedure at the very end of the process, in comparisson to usual books and articles I've seen. The reasoning goes as follows: For example, suppose we have found the Killing vector The way I would check this is a generator for the boost in the x direction is by acting these vectors onto t and x and check they give, respectively, x and t. The way this material I've found does is , they suddenly deffine: then he proceeds to find the explicit form of the boost: I understand the steps in this process. What I don't get is where did the motivation for the exponentiation come. What does that mean? It seems to me to have something to do with the application of the diffeomorfisms, but I'm not sure. Also, would this be a more correct way to proceed then what I've done?I really would appreciate any comments on this, as well as reccomended material.","K=x \frac{\partial}{\partial_t} + t\frac{\partial}{\partial_x} 
\Lambda=\exp[\lambda(x \frac{\partial}{\partial_t} + t\frac{\partial}{\partial_x})]=\sum_{n=0}^{\infty}\frac{1}{n!}\lambda^n (x \frac{\partial}{\partial_t} + t\frac{\partial}{\partial_x})^n
 
\Lambda t = x\sinh \lambda +t\cosh \lambda
 
\Lambda x = t\sinh \lambda +x\cosh \lambda
","['differential-geometry', 'general-relativity', 'tangent-spaces', 'special-relativity']"
53,a question about decomposition of the cotangent space on a complex manifold M (from Griffiths and Harris),a question about decomposition of the cotangent space on a complex manifold M (from Griffiths and Harris),,"Recently, I am reading Griffiths and Harris' book 'principles of algebraic geometry '. In chapter 0, section DeRham and Dolbeault Cohomology . they mention that By linear algebra, the decomposition $$T^*_{\mathbb{C},z}(M)=T^{*'}_z(M)\bigoplus T^{*''}_z(M)$$ of the cotangent space of a complex manifold M at each point $p\in M$ gives a decomposition $$\wedge^nT^*_{\mathbb{C},z}(M)=\bigoplus_{p+q=n} (\wedge^p T^{*'}_z(M)\bigotimes \wedge^q T^{*''}_z(M))$$ My question is what is $\bigotimes$ means here? Does it means tensor product? If so, why? In my mind, it should be $\wedge$ here i.e we have $$\wedge^nT^*_{\mathbb{C},z}(M)=\bigoplus_{p+q=n} (\wedge^p T^{*'}_z(M)\wedge\wedge^q T^{*''}_z(M))$$ For example, when n=2 (we assume $dim_{\mathbb{C}}M \ge 2$ ). The decomposition here should be $$\wedge^2T^*_{\mathbb{C},z}(M)=\wedge^2 T^{*'}_z(M)\bigoplus \wedge^{1,1} T^{*'}_z(M)\bigoplus \wedge^2 T^{*''}_z(M))$$ where $$\wedge^{1,1} T^{*'}_z(M)=T^{*'}_z(M)\wedge T^{*''}_z(M)=\{f(z)dz_i\wedge dz_{\bar{j}}\} $$ not $$\wedge^{1,1} T^{*'}_z(M)=T^{*'}_z(M)\bigotimes T^{*''}_z(M)=\{f(z)dz_i\bigotimes dz_{\bar{j}}\} $$ I see many other book use the same symbol as Griffiths and Harris' book (for example 'Complex geometry; an introduction' by Daniel Huybrechts). Can anyone tell what's happening here. Thank you so much.","Recently, I am reading Griffiths and Harris' book 'principles of algebraic geometry '. In chapter 0, section DeRham and Dolbeault Cohomology . they mention that By linear algebra, the decomposition of the cotangent space of a complex manifold M at each point gives a decomposition My question is what is means here? Does it means tensor product? If so, why? In my mind, it should be here i.e we have For example, when n=2 (we assume ). The decomposition here should be where not I see many other book use the same symbol as Griffiths and Harris' book (for example 'Complex geometry; an introduction' by Daniel Huybrechts). Can anyone tell what's happening here. Thank you so much.","T^*_{\mathbb{C},z}(M)=T^{*'}_z(M)\bigoplus T^{*''}_z(M) p\in M \wedge^nT^*_{\mathbb{C},z}(M)=\bigoplus_{p+q=n} (\wedge^p T^{*'}_z(M)\bigotimes \wedge^q T^{*''}_z(M)) \bigotimes \wedge \wedge^nT^*_{\mathbb{C},z}(M)=\bigoplus_{p+q=n} (\wedge^p T^{*'}_z(M)\wedge\wedge^q T^{*''}_z(M)) dim_{\mathbb{C}}M \ge 2 \wedge^2T^*_{\mathbb{C},z}(M)=\wedge^2 T^{*'}_z(M)\bigoplus \wedge^{1,1} T^{*'}_z(M)\bigoplus \wedge^2 T^{*''}_z(M)) \wedge^{1,1} T^{*'}_z(M)=T^{*'}_z(M)\wedge T^{*''}_z(M)=\{f(z)dz_i\wedge dz_{\bar{j}}\}  \wedge^{1,1} T^{*'}_z(M)=T^{*'}_z(M)\bigotimes T^{*''}_z(M)=\{f(z)dz_i\bigotimes dz_{\bar{j}}\} ","['differential-geometry', 'complex-geometry']"
54,How to consider an integral of the form $\int\limits_{a<u_{1}<...<u_{n}<b}dX_{u_{1}}\otimes...\otimes dX_{u_{n}}\in (\mathbb R^{d})^{\otimes n}$,How to consider an integral of the form,\int\limits_{a<u_{1}<...<u_{n}<b}dX_{u_{1}}\otimes...\otimes dX_{u_{n}}\in (\mathbb R^{d})^{\otimes n},"Let $X: [a,b] \to \mathbb R^{d}$ be some smooth path. I am having trouble understanding what is actually being expressed in the integral of the form $$\int\limits_{a<u_{1}<...<u_{n}<b}dX_{u_{1}}\otimes...\otimes dX_{u_{n}}\in (\mathbb R^{d})^{\otimes n}.$$ Is it just riemann integration in every ""component"" of the tensor? Any intuition behind the actual expression is much appreciated. EDIT: For greater context, I am looking at the definition of a signature of a path from Differential Equations driven by Rough Paths by Terry Lyons on page 30. Here the signature is denoted as $\textbf{X}$ , where $$\textbf{X}=(1,\textbf{X}^{1},...,\textbf{X}^{n},....)$$ such that $$\textbf{X}^{n}=\int\limits_{a<u_{1}<...<u_{n}<b}dX_{u_{1}}\otimes...\otimes dX_{u_{n}}\in (\mathbb R^{d})^{\otimes n}$$","Let be some smooth path. I am having trouble understanding what is actually being expressed in the integral of the form Is it just riemann integration in every ""component"" of the tensor? Any intuition behind the actual expression is much appreciated. EDIT: For greater context, I am looking at the definition of a signature of a path from Differential Equations driven by Rough Paths by Terry Lyons on page 30. Here the signature is denoted as , where such that","X: [a,b] \to \mathbb R^{d} \int\limits_{a<u_{1}<...<u_{n}<b}dX_{u_{1}}\otimes...\otimes dX_{u_{n}}\in (\mathbb R^{d})^{\otimes n}. \textbf{X} \textbf{X}=(1,\textbf{X}^{1},...,\textbf{X}^{n},....) \textbf{X}^{n}=\int\limits_{a<u_{1}<...<u_{n}<b}dX_{u_{1}}\otimes...\otimes dX_{u_{n}}\in (\mathbb R^{d})^{\otimes n}","['real-analysis', 'integration', 'differential-geometry', 'stochastic-processes', 'tensor-products']"
55,"Riemann volume forms: Absolute value of the determinant of the metric, or not?","Riemann volume forms: Absolute value of the determinant of the metric, or not?",,"When using differential geometry in the context of General Relativity, you are usually taught that the general invariant scalar volume element reads $$ \mathrm{d}V=\sqrt{|g|}\mathrm{d}x^n $$ for an n-dimensional Riemannian manifold. With $g$ being the determinant of the metric tensor. Cf. for example [1,2] and also the Wikipedia article on volume forms [3]. Now, my question is: Is it just a convention that we use the $\textit{absolute value}$ of the determinant when defining the volume form? In the Wikipedia article [3] it is mentioned that volume forms are in general not unique. Does this imply that we could equally say that $\mathrm{d}V=\sqrt{g}\mathrm{d}x^n$ is the volume form of choice? PS: Note that this would imply for the Minkowski metric ( $g=\mathrm{diag}(-1,1,1,1)$ ), that in one incident that volume form would be real and in the other imaginary. See for example this article [4] and the consequences for the complex structure of Quantum Mechanics. [1] http://www.blau.itp.unibe.ch/newlecturesGR.pdf Eq. 4.47 and 4.51 [2] https://ned.ipac.caltech.edu/level5/March01/Carroll3/Carroll2.html Eq. 2.48 [3] https://en.wikipedia.org/wiki/Volume_form#Riemannian_volume_form [4] Lindgren, Liukkon 2019: https://www.nature.com/articles/s41598-019-56357-3","When using differential geometry in the context of General Relativity, you are usually taught that the general invariant scalar volume element reads for an n-dimensional Riemannian manifold. With being the determinant of the metric tensor. Cf. for example [1,2] and also the Wikipedia article on volume forms [3]. Now, my question is: Is it just a convention that we use the of the determinant when defining the volume form? In the Wikipedia article [3] it is mentioned that volume forms are in general not unique. Does this imply that we could equally say that is the volume form of choice? PS: Note that this would imply for the Minkowski metric ( ), that in one incident that volume form would be real and in the other imaginary. See for example this article [4] and the consequences for the complex structure of Quantum Mechanics. [1] http://www.blau.itp.unibe.ch/newlecturesGR.pdf Eq. 4.47 and 4.51 [2] https://ned.ipac.caltech.edu/level5/March01/Carroll3/Carroll2.html Eq. 2.48 [3] https://en.wikipedia.org/wiki/Volume_form#Riemannian_volume_form [4] Lindgren, Liukkon 2019: https://www.nature.com/articles/s41598-019-56357-3"," \mathrm{d}V=\sqrt{|g|}\mathrm{d}x^n  g \textit{absolute value} \mathrm{d}V=\sqrt{g}\mathrm{d}x^n g=\mathrm{diag}(-1,1,1,1)","['differential-geometry', 'riemannian-geometry']"
56,Pairing higher forms of a Lie group with the universal enveloping algebra,Pairing higher forms of a Lie group with the universal enveloping algebra,,"For a (compact) Lie group $G$ we have a pairing between its cotangent space $T^*$ and its Lie algebra $\frak{g}$ . What happens for higher forms? Do we have a pairing between $\Lambda(T^*)$ , the exterior algebra of $T^*$ , and some subalgebra of the universal enveloping algebra $U(\frak{g})$ ?","For a (compact) Lie group we have a pairing between its cotangent space and its Lie algebra . What happens for higher forms? Do we have a pairing between , the exterior algebra of , and some subalgebra of the universal enveloping algebra ?",G T^* \frak{g} \Lambda(T^*) T^* U(\frak{g}),"['differential-geometry', 'algebraic-geometry', 'lie-groups', 'lie-algebras', 'differential-graded-algebras']"
57,Proving the decomposition is independent of the choice of chart.,Proving the decomposition is independent of the choice of chart.,,"Let $M$ be a manifold with boundary, and let $p\in \partial M.$ So there is a decomposition of $T_pM$ as follows: For a chart $\phi : U\to \tilde{U} \subset \mathbb{H}^n,$ with coordinate functions $(x^1,\dots, x^n),$ we say that $X\in T_pM$ points into $M$ if $Xx^n > 0,$ out of $M$ if $Xx^n<0,$ and parallel to $\partial M$ if $Xx^n = 0.$ How do I prove this decomposition is independent of the choice of chart? And that the set of vectors that point parallel to $\partial M$ form an $n-1$ -dimensional subspace of $T_pM?$ Can someone please help me with this? Thank you","Let be a manifold with boundary, and let So there is a decomposition of as follows: For a chart with coordinate functions we say that points into if out of if and parallel to if How do I prove this decomposition is independent of the choice of chart? And that the set of vectors that point parallel to form an -dimensional subspace of Can someone please help me with this? Thank you","M p\in \partial M. T_pM \phi : U\to \tilde{U} \subset \mathbb{H}^n, (x^1,\dots, x^n), X\in T_pM M Xx^n > 0, M Xx^n<0, \partial M Xx^n = 0. \partial M n-1 T_pM?","['differential-geometry', 'manifolds', 'smooth-manifolds', 'submanifold']"
58,"How to show that $T_{(1,0)}\mathbb S^1 \cong \operatorname{span}(\{e_2\})$?",How to show that ?,"T_{(1,0)}\mathbb S^1 \cong \operatorname{span}(\{e_2\})","I want to show that $T_{(1,0)}\mathbb S^1 \cong \operatorname{span}(\{e_2\})$ using the stereographic chart and using the definition that $T_xM$ is the set of velocity vectors $v$ where each vector $v$ is the equivalence class of curves that goes through point $x$ and tangent to each other. I got so far the following: Since $\varphi:U\to\mathbb{R}$ is given by $\varphi(x,y)=\frac{x}{1-y}$ and $v=\frac{d}{dt}(\varphi\circ \gamma)(t)\Big|_{t=0}$ for some $\gamma:I\to \mathbb S^1$ with $\gamma(0)=x=(1,0)$ , we can compute that \begin{align} v& =\frac{d}{dt}(\varphi\circ \gamma)(t)\Big|_{t=0}\\ &=\frac{d}{dt}\Big(\frac{x(t)}{1-y(t)}\Big)\Big|_{t=0}\\ &=\frac{x^{\prime}(t)(1-y(t))-x(t)(-y^{\prime}(t))}{(1-y(t))^2}\Big|_{t=0}\\ &=\frac{x^{\prime}(0)(1-y(0))+x(0)y^{\prime}(0))}{(1-y(0))^2}\\ &=x^{\prime}(0)+y^{\prime}(0). \end{align} I don't know how to interprete that and how to actually show that $T_{(0,0)}\mathbb S^1$ should be a span of $e_2$ . I know that if $i:\mathbb S^1\to\mathbb{R}^2$ is an inclusion, then $$di_x:T_x \mathbb S^1\to T_{i(x)}\mathbb{R}^2\text{ is   injective}.$$ So, we need somehow show $di_x(v)=\operatorname{span}(\{e_2\})$ . What should I do?","I want to show that using the stereographic chart and using the definition that is the set of velocity vectors where each vector is the equivalence class of curves that goes through point and tangent to each other. I got so far the following: Since is given by and for some with , we can compute that I don't know how to interprete that and how to actually show that should be a span of . I know that if is an inclusion, then So, we need somehow show . What should I do?","T_{(1,0)}\mathbb S^1 \cong \operatorname{span}(\{e_2\}) T_xM v v x \varphi:U\to\mathbb{R} \varphi(x,y)=\frac{x}{1-y} v=\frac{d}{dt}(\varphi\circ \gamma)(t)\Big|_{t=0} \gamma:I\to \mathbb S^1 \gamma(0)=x=(1,0) \begin{align}
v& =\frac{d}{dt}(\varphi\circ \gamma)(t)\Big|_{t=0}\\
&=\frac{d}{dt}\Big(\frac{x(t)}{1-y(t)}\Big)\Big|_{t=0}\\
&=\frac{x^{\prime}(t)(1-y(t))-x(t)(-y^{\prime}(t))}{(1-y(t))^2}\Big|_{t=0}\\
&=\frac{x^{\prime}(0)(1-y(0))+x(0)y^{\prime}(0))}{(1-y(0))^2}\\
&=x^{\prime}(0)+y^{\prime}(0).
\end{align} T_{(0,0)}\mathbb S^1 e_2 i:\mathbb S^1\to\mathbb{R}^2 di_x:T_x \mathbb S^1\to T_{i(x)}\mathbb{R}^2\text{ is 
 injective}. di_x(v)=\operatorname{span}(\{e_2\})","['differential-geometry', 'manifolds', 'tangent-spaces']"
59,Is the group of translations on a spherical topology tied to the group of rotations of that sphere?,Is the group of translations on a spherical topology tied to the group of rotations of that sphere?,,"Is the group of translations on a spherical topology tied to the group of rotations of that sphere? More concretely, suppose we define some set of orthonormal basis $e^{i}$ where i runs from 1 to n on a sphere $S^{n}$ (yes, I realize n=1, 3, and 7 are the only parellizable cases). For example suppose we consider $S^{3}$ , we can then translate some point (consider perhaps a vector) around on the three-sphere. However, we could have just rotated the sphere itself and achieved the same thing. Now I get that this all seems rather evident but I'm interested in something a bit more. We can say that the principle bundle of (oriented now) orthonormal frames is $P_{OF}(S^{3})=S^{3}\times SO(4)$ (which is a trivial bundle). Due to a special isomorphism we know $S^{3}=SU(2)$ , so we have $P_{OF}(S^{3})=SU(2)\times SO(4)$ . Back to our orthonormal basis, each $e^{i}$ can be chosen to identify with a generator of the Lie algebra $su(2)$ . Of course the lie algebra of $su(2)$ is the same as that of $so(3)$ (the former being the double cover of the latter). Contrast this with the topology $\mathbb{R}^{3}$ , whose principle oriented orthonormal frame bundle is $P_{OF}(\mathbb{R}^{3})=\mathbb{R}^{3}\times SO(3)$ . In this latter case, the two (frame translations and rotations) are entirely divorced from one another, yet in the former a translation on the three sphere is representable by rotations of the three-sphere. I find myself wondering if the two are “soldered” in some fashion? It seems like a translation on the surface of the sphere is equivalent to a rotation in the fiber of the frame bundle of that sphere. For instance, if we consider the 1-forms dual to our basis, we call these the soldering forms. Are these then tied to our rotations in the fibers? Put another way, are the horizontal and vertical sub-bundles tied to one another? can someone please elaborate?","Is the group of translations on a spherical topology tied to the group of rotations of that sphere? More concretely, suppose we define some set of orthonormal basis where i runs from 1 to n on a sphere (yes, I realize n=1, 3, and 7 are the only parellizable cases). For example suppose we consider , we can then translate some point (consider perhaps a vector) around on the three-sphere. However, we could have just rotated the sphere itself and achieved the same thing. Now I get that this all seems rather evident but I'm interested in something a bit more. We can say that the principle bundle of (oriented now) orthonormal frames is (which is a trivial bundle). Due to a special isomorphism we know , so we have . Back to our orthonormal basis, each can be chosen to identify with a generator of the Lie algebra . Of course the lie algebra of is the same as that of (the former being the double cover of the latter). Contrast this with the topology , whose principle oriented orthonormal frame bundle is . In this latter case, the two (frame translations and rotations) are entirely divorced from one another, yet in the former a translation on the three sphere is representable by rotations of the three-sphere. I find myself wondering if the two are “soldered” in some fashion? It seems like a translation on the surface of the sphere is equivalent to a rotation in the fiber of the frame bundle of that sphere. For instance, if we consider the 1-forms dual to our basis, we call these the soldering forms. Are these then tied to our rotations in the fibers? Put another way, are the horizontal and vertical sub-bundles tied to one another? can someone please elaborate?",e^{i} S^{n} S^{3} P_{OF}(S^{3})=S^{3}\times SO(4) S^{3}=SU(2) P_{OF}(S^{3})=SU(2)\times SO(4) e^{i} su(2) su(2) so(3) \mathbb{R}^{3} P_{OF}(\mathbb{R}^{3})=\mathbb{R}^{3}\times SO(3),"['differential-geometry', 'lie-groups', 'principal-bundles']"
60,Reality check on Schwarzschild computations,Reality check on Schwarzschild computations,,"TL;DR: I have something Ricci-flat which is not turning out to be scalar-flat and this is absurd. Consider $P_I\times_r \Bbb S^2$ , where $P_I  =\{(t,r) \in \Bbb R^2 \mid r>2m\}$ , with $m>0$ , and the Lorentzian metric $g^{P_I} =-h(r)\,{\rm d}t^2+h(r)^{-1}\,{\rm d}r^2$ , where $h(r) = 1-2mr^{-1}$ . The metric on $P_I\times_r \Bbb S^2$ is $$g=-h(r)\,{\rm d}t^2+h(r)^{-1}\,{\rm d}r^2 + r^2\,{\rm d}\Omega^2,$$ where ${\rm d}\Omega^2$ is the standard round metric on $\Bbb S^2$ . I have computed that the Gaussian curvature of the half-plane $P_I$ is given by $K^{P_I}(t,r) = -h''(r)/2 = 2mr^{-3}$ . Hence $${\rm Ric}^{P_I} = -\frac{h''(r)}{2} g^{P_I} \quad \mbox{and}\quad {\rm s}^{P_I} = -h''(r) = 4mr^{-3},$$ where ${\rm s}$ stands for scalar curvature. It's not hard to check that $\nabla r = h(r)\partial_r$ and ${\rm Hess}\,r = (h'(r)/2) g^{P_I}$ . Great. Now in O'Neill's Semi-Riemannian Geometry book we have that for an arbitrary $B\times_\phi F$ , the formulas hold: ${\rm Ric}(X,Y) = {\rm Ric}^B(X,Y) - \dfrac{(\dim F)}{\phi}\,{\rm Hess}\,\phi$ for horizontal $X,Y$ ; ${\rm Ric}(X,Y) = 0$ for horizontal $X$ and vertical $Y$ . ${\rm Ric}(V,W) = {\rm Ric}^F(V,W) - g(V,W) \phi^\#$ for vertical $V$ , $W$ , where $$\phi^\# = \frac{\triangle \phi}{\phi} + (\dim F-1)\frac{g(\nabla\phi,\nabla\phi)}{\phi^2}.$$ This is corollary 43 in page 211. Now, the Schwarzschild black hole is supposed to be Ricci-flat, right? Sure, take $\phi = r$ , so $\triangle r = h'(r)$ and thus $r^\# = r^{-2}$ . For horizontal vectors we have $$-\frac{h''(r)}{2}g^{P_I}_{ij} - \frac{2}{r} \frac{h'(r)}{2}g^{P_I}_{ij} = 0$$ for all $i,j \in \{t,r\}$ , by plugging in $h(r) = 1-2mr^{-1}$ . Similarly, $r^\# = r^{-2}$ cancels the warping factor in ${\rm d}\Omega^2$ and since the Gaussian curvature of $\Bbb S^2$ is $1$ , ${\rm Ric}$ vanishes on pairs of vertical vectors as well. Awesome, right? Now look at exercise 13a) in page 214: $$ {\rm s} = {\rm s}^B + \frac{{\rm s}^F}{\phi^2} - 2\dim F\frac{\triangle \phi}{\phi} - \dim F(\dim F-1)\frac{g(\nabla \phi, \nabla \phi)}{\phi^2}.$$ We have $\dim F = 2$ , so: $${\rm s} = -h''(r) + r^{-2} - 4r^{-1} h'(r) - 2r^{-2}h(r).$$ But for $h(r) = 1-2mr^{-1}$ , we have $h'(r) = 2mr^{-2}$ and $h''(r) = -4mr^{-3}$ , leading to: $$4mr^{-3} + r^{-2}-8mr^{-3}-2r^{-2} + 4mr^{-3} = {\color{red}{-r^{-2} \neq 0}}.$$ What gives?","TL;DR: I have something Ricci-flat which is not turning out to be scalar-flat and this is absurd. Consider , where , with , and the Lorentzian metric , where . The metric on is where is the standard round metric on . I have computed that the Gaussian curvature of the half-plane is given by . Hence where stands for scalar curvature. It's not hard to check that and . Great. Now in O'Neill's Semi-Riemannian Geometry book we have that for an arbitrary , the formulas hold: for horizontal ; for horizontal and vertical . for vertical , , where This is corollary 43 in page 211. Now, the Schwarzschild black hole is supposed to be Ricci-flat, right? Sure, take , so and thus . For horizontal vectors we have for all , by plugging in . Similarly, cancels the warping factor in and since the Gaussian curvature of is , vanishes on pairs of vertical vectors as well. Awesome, right? Now look at exercise 13a) in page 214: We have , so: But for , we have and , leading to: What gives?","P_I\times_r \Bbb S^2 P_I  =\{(t,r) \in \Bbb R^2 \mid r>2m\} m>0 g^{P_I} =-h(r)\,{\rm d}t^2+h(r)^{-1}\,{\rm d}r^2 h(r) = 1-2mr^{-1} P_I\times_r \Bbb S^2 g=-h(r)\,{\rm d}t^2+h(r)^{-1}\,{\rm d}r^2 + r^2\,{\rm d}\Omega^2, {\rm d}\Omega^2 \Bbb S^2 P_I K^{P_I}(t,r) = -h''(r)/2 = 2mr^{-3} {\rm Ric}^{P_I} = -\frac{h''(r)}{2} g^{P_I} \quad \mbox{and}\quad {\rm s}^{P_I} = -h''(r) = 4mr^{-3}, {\rm s} \nabla r = h(r)\partial_r {\rm Hess}\,r = (h'(r)/2) g^{P_I} B\times_\phi F {\rm Ric}(X,Y) = {\rm Ric}^B(X,Y) - \dfrac{(\dim F)}{\phi}\,{\rm Hess}\,\phi X,Y {\rm Ric}(X,Y) = 0 X Y {\rm Ric}(V,W) = {\rm Ric}^F(V,W) - g(V,W) \phi^\# V W \phi^\# = \frac{\triangle \phi}{\phi} + (\dim F-1)\frac{g(\nabla\phi,\nabla\phi)}{\phi^2}. \phi = r \triangle r = h'(r) r^\# = r^{-2} -\frac{h''(r)}{2}g^{P_I}_{ij} - \frac{2}{r} \frac{h'(r)}{2}g^{P_I}_{ij} = 0 i,j \in \{t,r\} h(r) = 1-2mr^{-1} r^\# = r^{-2} {\rm d}\Omega^2 \Bbb S^2 1 {\rm Ric}  {\rm s} = {\rm s}^B + \frac{{\rm s}^F}{\phi^2} - 2\dim F\frac{\triangle \phi}{\phi} - \dim F(\dim F-1)\frac{g(\nabla \phi, \nabla \phi)}{\phi^2}. \dim F = 2 {\rm s} = -h''(r) + r^{-2} - 4r^{-1} h'(r) - 2r^{-2}h(r). h(r) = 1-2mr^{-1} h'(r) = 2mr^{-2} h''(r) = -4mr^{-3} 4mr^{-3} + r^{-2}-8mr^{-3}-2r^{-2} + 4mr^{-3} = {\color{red}{-r^{-2} \neq 0}}.","['differential-geometry', 'riemannian-geometry', 'general-relativity', 'semi-riemannian-geometry']"
61,Orientability of a submanifold which is a preimage of a submanifold,Orientability of a submanifold which is a preimage of a submanifold,,"I am reading Milnor & Stacheff, Characteristic Classes, Chapter 18. There is a short review of smooth manifolds, and there is a following statement: Suppose $f:M\to N$ is a smooth map between smooth manifolds, and suppose $f$ is transverse to a submanifold $Y\subset N$ (so that $f^{-1}(Y)$ is a submanifold of $M$ ). If $\nu^k$ is the normal bundle of $Y$ in $N$ , then the induce bundle over $f^{-1}(Y)$ from $\nu^k$ by $f$ can be idenfied with the normal bundle of $f^{-1}(Y)$ in $M$ . In particular, if $\nu^k$ is an oriented vector bundle, and if $M$ is an oriented manifold, then $f^{-1}(Y)$ is also an oriented manifold. I see that (assuming that $M,N$ are Riemannian) the normal bundle of $f^{-1}(Y)$ can be identified with the induced bundle $f^*\nu^k$ , but I can't see how the last statement follows. Am I missing a elementary theorem or something?","I am reading Milnor & Stacheff, Characteristic Classes, Chapter 18. There is a short review of smooth manifolds, and there is a following statement: Suppose is a smooth map between smooth manifolds, and suppose is transverse to a submanifold (so that is a submanifold of ). If is the normal bundle of in , then the induce bundle over from by can be idenfied with the normal bundle of in . In particular, if is an oriented vector bundle, and if is an oriented manifold, then is also an oriented manifold. I see that (assuming that are Riemannian) the normal bundle of can be identified with the induced bundle , but I can't see how the last statement follows. Am I missing a elementary theorem or something?","f:M\to N f Y\subset N f^{-1}(Y) M \nu^k Y N f^{-1}(Y) \nu^k f f^{-1}(Y) M \nu^k M f^{-1}(Y) M,N f^{-1}(Y) f^*\nu^k","['differential-geometry', 'riemannian-geometry', 'smooth-manifolds', 'vector-bundles', 'transversality']"
62,"If $T_t$ is the flow generated by the autonomous velocity $v$ and $\left.v\right|_{\partial\Omega}=0$, then $T_t(\partial\Omega)=\partial\Omega$","If  is the flow generated by the autonomous velocity  and , then",T_t v \left.v\right|_{\partial\Omega}=0 T_t(\partial\Omega)=\partial\Omega,"Let $d\in\mathbb N$ and $v\in C_c^\infty(\mathbb R^d,\mathbb R^d)$ . We know that, for any $\tau>0$ , there is a unique solution $X^x\in C^0([0,\tau],\mathbb R^d)$ of \begin{align}X'(t)&=v(X(t))\tag1&\text{for all }t\in[0,\tau],\\X(0)&=x\end{align} for all $x\in\mathbb R^d$ . It is easy to show that $$T_t(x):=X^x(t)\;\;\;\text{for }t\in[0,\tau]$$ is a $C^1$ -diffeomorphism from $\mathbb R^d$ onto $\mathbb R^d$ . Now let $\Omega\subseteq\mathbb R^d$ . How can we show that, if $\left.v\right|_{\partial\Omega}=0$ , then $$T_t(\partial\Omega)=\partial\Omega\tag2$$ for all $t\in[0,\tau]$ ? if $\Omega$ is closed or open, then $$T_t(\Omega)=\Omega\tag3$$ for all $t\in[0,\tau]$ ? It's clear to me that any homeomorphism maps boundary (interior) points to boundary (interior) points. I guess we need to use this somehow. EDIT : From the comments it is clear that $(2)$ holds, since it should generally hold that if $B$ is any subset of $\mathbb R^d$ with $\left.v\right|_B=0$ , then $T_t(x)=x$ for all $x\in B$ . But how can we prove $(3)$ ? EDIT 2 : If $f$ is any homeomorphism between topological spaces $E_1$ and $E_2$ and $B_1\subseteq E_1$ , then we know that $f(B_1^\circ)=f(B_1)^\circ$ , $f(\partial B_1)=\partial f(B_1)$ and $f(\overline{B_1})=\overline{f(B_1)}$ . If $B_1$ os open, then $B_1=B_1^\circ$ and if $B_1$ is closed, then $B_1=\overline{B_1}$ . I think we need to use this for $(3)$ . EDIT 3 : Let $x\in\Omega^\circ$ . Then there is a $\varepsilon>0$ with $B_\varepsilon(x)\subseteq\Omega$ . Maybe we can at least show that there is a $t\in[0,\tau]$ (sufficiently small) such that $\left\|X^x(s)-x\right\|<\varepsilon$ for all $s\in[0,t]$ . Then it would follow that $$T_s(\Omega^\circ)\subseteq\Omega^\circ\;\;\;\text{for all }s\in[0,t].\tag4$$ From pure intuition, for a sufficiently small $t$ , the velocity should not be able to move the point $x$ outside of the ball $B_\varepsilon(x)$ . So, $(4)$ should hold. (How would we need to argue that it must even be an equality? This seems trivial, by bijectivity though.)","Let and . We know that, for any , there is a unique solution of for all . It is easy to show that is a -diffeomorphism from onto . Now let . How can we show that, if , then for all ? if is closed or open, then for all ? It's clear to me that any homeomorphism maps boundary (interior) points to boundary (interior) points. I guess we need to use this somehow. EDIT : From the comments it is clear that holds, since it should generally hold that if is any subset of with , then for all . But how can we prove ? EDIT 2 : If is any homeomorphism between topological spaces and and , then we know that , and . If os open, then and if is closed, then . I think we need to use this for . EDIT 3 : Let . Then there is a with . Maybe we can at least show that there is a (sufficiently small) such that for all . Then it would follow that From pure intuition, for a sufficiently small , the velocity should not be able to move the point outside of the ball . So, should hold. (How would we need to argue that it must even be an equality? This seems trivial, by bijectivity though.)","d\in\mathbb N v\in C_c^\infty(\mathbb R^d,\mathbb R^d) \tau>0 X^x\in C^0([0,\tau],\mathbb R^d) \begin{align}X'(t)&=v(X(t))\tag1&\text{for all }t\in[0,\tau],\\X(0)&=x\end{align} x\in\mathbb R^d T_t(x):=X^x(t)\;\;\;\text{for }t\in[0,\tau] C^1 \mathbb R^d \mathbb R^d \Omega\subseteq\mathbb R^d \left.v\right|_{\partial\Omega}=0 T_t(\partial\Omega)=\partial\Omega\tag2 t\in[0,\tau] \Omega T_t(\Omega)=\Omega\tag3 t\in[0,\tau] (2) B \mathbb R^d \left.v\right|_B=0 T_t(x)=x x\in B (3) f E_1 E_2 B_1\subseteq E_1 f(B_1^\circ)=f(B_1)^\circ f(\partial B_1)=\partial f(B_1) f(\overline{B_1})=\overline{f(B_1)} B_1 B_1=B_1^\circ B_1 B_1=\overline{B_1} (3) x\in\Omega^\circ \varepsilon>0 B_\varepsilon(x)\subseteq\Omega t\in[0,\tau] \left\|X^x(s)-x\right\|<\varepsilon s\in[0,t] T_s(\Omega^\circ)\subseteq\Omega^\circ\;\;\;\text{for all }s\in[0,t].\tag4 t x B_\varepsilon(x) (4)","['general-topology', 'differential-geometry', 'numerical-methods', 'differential-topology', 'diffeomorphism']"
63,Defining the derivative of a vector field component,Defining the derivative of a vector field component,,"I'm reading 'Core Principles of Special and General Relativity' by Luscombe, specifically the introductory section on problems with defining usual notion of differentiation for tensor fields. I'll quote the relevant part: The second way (to see whether the partial derivative of a tensor is a tensor) is to look at the definition of derivative, $$\frac{\partial T^i}{\partial x^j}=\lim_{dx^j\to 0}\frac{T^i(x+dx^j)-T^i(x)}{dx^j}$$ The numerator is not in general a vector! We're comparing (subtracting) vectors from different points, yet the transformation properties of tensors are defined at a point . Since the equation above is a notational mess, here's my attempt to interpret it: $$\bigg(\frac{\partial T^i}{\partial x^j}\bigg)_p=\partial_j(T^i\circ x^{-1})(x(p))=\lim_{h\to 0}\frac{(T^i\circ x^{-1})(x(p)+[0,\ldots,h,\ldots,0])-(T^i\circ x^{-1})(x(p))}{h}$$ where $[0,\ldots,h,\ldots,0]\in\mathbb{R}^n$ has $h$ as its $j$ -th coordinate. Is my above interpretation correct? If so, what's the issue with defining the derivative of a vector field component in this way?","I'm reading 'Core Principles of Special and General Relativity' by Luscombe, specifically the introductory section on problems with defining usual notion of differentiation for tensor fields. I'll quote the relevant part: The second way (to see whether the partial derivative of a tensor is a tensor) is to look at the definition of derivative, The numerator is not in general a vector! We're comparing (subtracting) vectors from different points, yet the transformation properties of tensors are defined at a point . Since the equation above is a notational mess, here's my attempt to interpret it: where has as its -th coordinate. Is my above interpretation correct? If so, what's the issue with defining the derivative of a vector field component in this way?","\frac{\partial T^i}{\partial x^j}=\lim_{dx^j\to 0}\frac{T^i(x+dx^j)-T^i(x)}{dx^j} \bigg(\frac{\partial T^i}{\partial x^j}\bigg)_p=\partial_j(T^i\circ x^{-1})(x(p))=\lim_{h\to 0}\frac{(T^i\circ x^{-1})(x(p)+[0,\ldots,h,\ldots,0])-(T^i\circ x^{-1})(x(p))}{h} [0,\ldots,h,\ldots,0]\in\mathbb{R}^n h j","['differential-geometry', 'vector-fields']"
64,Smoothest possible path interpolation given 2-D positions and directions?,Smoothest possible path interpolation given 2-D positions and directions?,,"Assume I have a set of $n$ points $\{x_1,...,x_n \} \in \mathbb{R^2}$ and a set of corresponding directions (or angles) $\{d_1,...,d_n \}$ . It is my goal to find the smoothest, continuous path (edit: I define smoothness as the smallest overall curvature) which passes through all $n$ points while honoring all $n$ directions. The solution may be provided in increments, for example through splines between a pairs of successive points. I have illustrated what I mean below. Do you have any suggestions on how to tackle this issue?","Assume I have a set of points and a set of corresponding directions (or angles) . It is my goal to find the smoothest, continuous path (edit: I define smoothness as the smallest overall curvature) which passes through all points while honoring all directions. The solution may be provided in increments, for example through splines between a pairs of successive points. I have illustrated what I mean below. Do you have any suggestions on how to tackle this issue?","n \{x_1,...,x_n \} \in \mathbb{R^2} \{d_1,...,d_n \} n n","['geometry', 'differential-geometry', 'interpolation']"
65,Maximum Principle for Minimal Surface Equation with Dirichlet Boundary Condition,Maximum Principle for Minimal Surface Equation with Dirichlet Boundary Condition,,"I'm an undergraduate student and I'm currently reading a classical paper for my final project for the course differential geometry on the Bernstein problem of minimal surfaces, namely, the paper: Bombieri, Enrico, E. De Giorgi, and Enrico Giusti, "" Minimal Cones and the Bernstein Problem "" Inventiones Mathematicae 7.3 (1969): 243-268. In equation \eqref{1}, the authors considered the folowing Dirichlet problem for the minimal surface equation: $$ \begin{cases}     \sum_{i=1}^{n} \left( D_i \left( \dfrac{D_i f}{\sqrt{1+\vert D f \vert^2}} \right) \right) = 0, \qquad f\in C^2(B_R), \\     f=f_1\quad  \text{in} \quad \partial B_R \end{cases}\label{1}\tag{25} $$ where $B_R$ is the unit ball in $\mathbb{R}^8$ . ( NOT in three-dimensional Euclidean space ) We have known the existence and uniqueness of the solution of such a boundary problem. Denote its solution by $f^{(R)}(x)$ . Similarly, we consider the same boundary problem with function $f_2$ on $\partial B_R$ . With some assumptions and calculations mentioned in the paper before, we have obtained that $$ f_1(x) \leq f^{(R)}(x) \leq f_2(x) $$ on the boundary $\partial(B_R \cap D_1)$ . Here comes my question which has been puzzling me for a long time: The authors claims that: ""by the well-known maximum principle for solutions of the Dirichlet problem and equation \eqref{2} and \eqref{3}"" (listed below), we obtained that $$ f_1(x) \leq f^{(R)}(x) \leq f_2(x) \, \text{for} \,  x \in \bar{B}_R\cap\bar{D_1}. $$ I'm confused at the ""maximum principle mentioned there. I have learned the strong maximum principle and the Hopf maximum principle for Laplacian equations (with corresponding boundary conditions), but I have no idea how to apply these here. Or, it there any maximum principle stated for the minimal surface equation in the above contexts? I tried but find no reference for such a theorem. (For example, the book on elliptic PDEs by David Gilbarg, et.al). Moreover, I have no idea on the role played by the equation \eqref{2} and \eqref{3}. P.S. I list here equations \eqref{2} and \eqref{3}: $$ \int_{D_{1}} \sum_{i=1}^{8} \frac{\partial f_{1}}{\partial x_{i}} \frac{\partial \varphi}{\partial x_{i}}\left(1+\left|D f_{1}\right|^{2}\right)^{-\frac{1}{2}} dx \leq 0\label{2}\tag{23} $$ and $$ \int_{D_{1}} \sum_{i=1}^{8} \frac{\partial f_{2}}{\partial x_{i}} \frac{\partial \varphi}{\partial x_{i}}\left(1+\left|D f_{2}\right|^{2}\right)^{-\frac{1}{2}} dx \geq 0\label{3}\tag{24} $$ where the region $D_1$ is defined as $D_1 = \{ x \in \mathbb{R}^8 \vert 0 \leq v \leq u \}$ , $u=\left(x_{1}^{2}+\cdots+x_{4}^{2}\right)^{\frac{1}{2}}$ and $v=\left(x_{5}^{2}+\cdots+x_{8}^{2}\right)^{\frac{1}{2}}$ . Thank you in advance! It is my first time to ask a question on MSE, and I am sincerely sorry for any possible mistakes and rudeness in this question. Thank you!","I'm an undergraduate student and I'm currently reading a classical paper for my final project for the course differential geometry on the Bernstein problem of minimal surfaces, namely, the paper: Bombieri, Enrico, E. De Giorgi, and Enrico Giusti, "" Minimal Cones and the Bernstein Problem "" Inventiones Mathematicae 7.3 (1969): 243-268. In equation \eqref{1}, the authors considered the folowing Dirichlet problem for the minimal surface equation: where is the unit ball in . ( NOT in three-dimensional Euclidean space ) We have known the existence and uniqueness of the solution of such a boundary problem. Denote its solution by . Similarly, we consider the same boundary problem with function on . With some assumptions and calculations mentioned in the paper before, we have obtained that on the boundary . Here comes my question which has been puzzling me for a long time: The authors claims that: ""by the well-known maximum principle for solutions of the Dirichlet problem and equation \eqref{2} and \eqref{3}"" (listed below), we obtained that I'm confused at the ""maximum principle mentioned there. I have learned the strong maximum principle and the Hopf maximum principle for Laplacian equations (with corresponding boundary conditions), but I have no idea how to apply these here. Or, it there any maximum principle stated for the minimal surface equation in the above contexts? I tried but find no reference for such a theorem. (For example, the book on elliptic PDEs by David Gilbarg, et.al). Moreover, I have no idea on the role played by the equation \eqref{2} and \eqref{3}. P.S. I list here equations \eqref{2} and \eqref{3}: and where the region is defined as , and . Thank you in advance! It is my first time to ask a question on MSE, and I am sincerely sorry for any possible mistakes and rudeness in this question. Thank you!","
\begin{cases}
    \sum_{i=1}^{n} \left( D_i \left( \dfrac{D_i f}{\sqrt{1+\vert D f \vert^2}} \right) \right) = 0, \qquad f\in C^2(B_R), \\
    f=f_1\quad  \text{in} \quad \partial B_R
\end{cases}\label{1}\tag{25}
 B_R \mathbb{R}^8 f^{(R)}(x) f_2 \partial B_R 
f_1(x) \leq f^{(R)}(x) \leq f_2(x)
 \partial(B_R \cap D_1) 
f_1(x) \leq f^{(R)}(x) \leq f_2(x) \, \text{for} \,  x \in \bar{B}_R\cap\bar{D_1}.
 
\int_{D_{1}} \sum_{i=1}^{8} \frac{\partial f_{1}}{\partial x_{i}} \frac{\partial \varphi}{\partial x_{i}}\left(1+\left|D f_{1}\right|^{2}\right)^{-\frac{1}{2}} dx \leq 0\label{2}\tag{23}
 
\int_{D_{1}} \sum_{i=1}^{8} \frac{\partial f_{2}}{\partial x_{i}} \frac{\partial \varphi}{\partial x_{i}}\left(1+\left|D f_{2}\right|^{2}\right)^{-\frac{1}{2}} dx \geq 0\label{3}\tag{24}
 D_1 D_1 = \{ x \in \mathbb{R}^8 \vert 0 \leq v \leq u \} u=\left(x_{1}^{2}+\cdots+x_{4}^{2}\right)^{\frac{1}{2}} v=\left(x_{5}^{2}+\cdots+x_{8}^{2}\right)^{\frac{1}{2}}","['differential-geometry', 'partial-differential-equations', 'elliptic-equations', 'minimal-surfaces']"
66,Tangents imply secants,Tangents imply secants,,"I am stuck with proving a limit which I think should be immediate... I will explain the problem and comment one of my attempts.  Let $x: [0, \infty) \to \mathbb{R}^n$ be a differentiable arc with $\lim\limits_{t \to \infty} x(t) = x_0 \in \mathbb{R}^n$ , $x(t) \not = x_0, \dot{x}(t) \not = 0, t \geq 0$ , of which we know the following limit of tangents exists: $$ \lim\limits_{t \to \infty} \frac{ \dot{x} (t) }{|| \dot{x} (t) ||} ,$$ say it tends to a certain $L \in S^1$ . Now I want to prove $ \lim\limits_{t \to \infty} \frac{ x(t) - x_0}{ || x(t) - x_0 ||}= -L$ (this can be graphically seen to make sense). My best attempt of a proof was just trying to apply the limit definitions, but I can not get the result... The problem arises when I have: $$ \left| \frac{x(t)- x_0}{ || x(t)- x_0 ||} - \frac{x(t)- x(s)}{ || x(t)- x(s)||} \right| + \left| \frac{x(t)- x(s)}{ || x(t)- x (s) ||} - \frac{\dot{x} (t)}{ ||\dot{x} (t)||}\right| + \left| \frac{\dot{x} (t)}{ ||\dot{x} (t)||}  - L \right|,$$ since the last term is smaller than $\varepsilon /3$ if $t >M_1 >0$ , the second term is smaller than $\varepsilon/3$ if $s \in (t- \delta, t + \delta)$ , but the first term is smaller than $\varepsilon/3$ if $s > M_2 (t)$ , i.e., the constant $M_2$ depends on $t$ , so I can not make the three summands ""small"" at the same time. I have tried to prove that $M_2$ does not depend on $t$ but I have not achieved it, I think it can't be done. So, any help or suggestion is appreciated, thanks in advance!","I am stuck with proving a limit which I think should be immediate... I will explain the problem and comment one of my attempts.  Let be a differentiable arc with , , of which we know the following limit of tangents exists: say it tends to a certain . Now I want to prove (this can be graphically seen to make sense). My best attempt of a proof was just trying to apply the limit definitions, but I can not get the result... The problem arises when I have: since the last term is smaller than if , the second term is smaller than if , but the first term is smaller than if , i.e., the constant depends on , so I can not make the three summands ""small"" at the same time. I have tried to prove that does not depend on but I have not achieved it, I think it can't be done. So, any help or suggestion is appreciated, thanks in advance!","x: [0, \infty) \to \mathbb{R}^n \lim\limits_{t \to \infty} x(t) = x_0 \in \mathbb{R}^n x(t) \not = x_0, \dot{x}(t) \not = 0, t \geq 0  \lim\limits_{t \to \infty} \frac{ \dot{x} (t) }{|| \dot{x} (t) ||} , L \in S^1  \lim\limits_{t \to \infty} \frac{ x(t) - x_0}{ || x(t) - x_0 ||}= -L  \left| \frac{x(t)- x_0}{ || x(t)- x_0 ||} - \frac{x(t)- x(s)}{ || x(t)- x(s)||} \right| + \left| \frac{x(t)- x(s)}{ || x(t)- x (s) ||} - \frac{\dot{x} (t)}{ ||\dot{x} (t)||}\right| + \left| \frac{\dot{x} (t)}{ ||\dot{x} (t)||}  - L \right|, \varepsilon /3 t >M_1 >0 \varepsilon/3 s \in (t- \delta, t + \delta) \varepsilon/3 s > M_2 (t) M_2 t M_2 t",['differential-geometry']
67,Composition of local diffeomorphisms is a local diffeomorphism,Composition of local diffeomorphisms is a local diffeomorphism,,"Let $F: M\rightarrow N$ , $G:N\rightarrow P$ be local diffeomorphisms, where $M,N,P$ are smooth manifolds. I would like to show that $G\circ F: M\rightarrow P$ is a local diffeomorphism. My attempt: Let $x\in M$ . Since $F:M\rightarrow N$ is a local diffeomorphism, there exists an open set $U$ of $x$ such that $F(U)$ is open in $N$ and $F|_U: U\rightarrow F(U)$ is a diffeomorphism. Similarly, since $F(x)\in N$ , there exists a neighborhood $V$ of $F(x)$ such that $G(V)$ is open in $P$ and $G|_V: V \rightarrow G(V)$ is a diffeomorphism. I thought of considering the set: $F|_U^{-1}(F(U)\cap V)$ , since this set is open in $U$ .However, I have not gotten far. May I have hints? Please do not use immersions","Let , be local diffeomorphisms, where are smooth manifolds. I would like to show that is a local diffeomorphism. My attempt: Let . Since is a local diffeomorphism, there exists an open set of such that is open in and is a diffeomorphism. Similarly, since , there exists a neighborhood of such that is open in and is a diffeomorphism. I thought of considering the set: , since this set is open in .However, I have not gotten far. May I have hints? Please do not use immersions","F: M\rightarrow N G:N\rightarrow P M,N,P G\circ F: M\rightarrow P x\in M F:M\rightarrow N U x F(U) N F|_U: U\rightarrow F(U) F(x)\in N V F(x) G(V) P G|_V: V \rightarrow G(V) F|_U^{-1}(F(U)\cap V) U","['differential-geometry', 'smooth-manifolds']"
68,How to show that the space of Kähler metrics is simply connected?,How to show that the space of Kähler metrics is simply connected?,,I wanted to find the proof showing that the space of Kähler metrics on a given Kähler manifold $M$ is simply connected. Any reference containing such topological results on the space of Kähler metrics would be highly appreciated.,I wanted to find the proof showing that the space of Kähler metrics on a given Kähler manifold is simply connected. Any reference containing such topological results on the space of Kähler metrics would be highly appreciated.,M,"['differential-geometry', 'differential-topology', 'kahler-manifolds']"
69,Intuition behind Willmore energy,Intuition behind Willmore energy,,"I read about Willmore Energy is a quantitative measure of how much a given surface deviates from a round sphere. Also, I heard that it says that things in nature tends to change their shape in such a way that they use the least energy to survive. And it ends up to be a sphere by using Willmore energy. But how does this link to this formula? $$W=\int_SH^2dA-\int_SKdA$$ I mean, what does this has to do with all of the real-life interpretations?","I read about Willmore Energy is a quantitative measure of how much a given surface deviates from a round sphere. Also, I heard that it says that things in nature tends to change their shape in such a way that they use the least energy to survive. And it ends up to be a sphere by using Willmore energy. But how does this link to this formula? I mean, what does this has to do with all of the real-life interpretations?",W=\int_SH^2dA-\int_SKdA,['differential-geometry']
70,Proving $R_{ij;m}=g^{kl} R_{ikjl;m}$.,Proving .,R_{ij;m}=g^{kl} R_{ikjl;m},"In the coordinate $\{x^i\}$ , the Riemann curvature tensor can be written as $$ R=R_{ijkl}\,dx^i\otimes dx^j\otimes dx^k \otimes dx^l $$ and the Ricci curvature can be written as $$\text{Ric}=R_{ij}\,dx^i\otimes dx^j$$ where $R_{ij}=g^{kl} R_{kilj}\,$ .   Prove that $$R_{ij;m}=g^{kl} R_{ikjl;m}$$ where $$\theta_{i_1i_2\cdots i_s;m}=\left(D_{\partial_m}\theta\right)(\partial_{i_1},\cdots,\partial_{i_s})\,.$$ My attempt I got a hint to prove $$R_{jk;m} = (g^{il}R_{ijkl})_{;m} =  g^{il}{}_{;m}R_{ijkl} + g^{il}R_{ijkl;m} = g^{il}R_{ijkl;m}\tag{1}$$ and $$g^{il}{}_{;m}=0 \,.\tag{2}$$ As for $(2)$ , I am not sure what the meaning of $g^{il}{}_{;m}$ is. I thought that $$ g^{il}{}_{;m}=\left(D_{\partial_m}\,\check g\right)(dx^i,dx^l) $$ where $\check g(\alpha,\beta)=g(\alpha^\sharp, \beta^\sharp)$ is the dual metric in which $\sharp$ denotes the musical isomorphism . Am I right? I can prove that $Dg=0$ and hence $g_{ij;k}=0$ . I hope to use this to prove $(2)$ , and I tried to take advantage of $g^{il}g_{lj}=\delta^i_j$ . And then I guess that $$(g^{il}g_{lj})_{;m}=g^{il}{}_{;m}g_{lj}+g^{il}g_{lj;m}$$ but I don't know whether this is right, and I don't know how to get $(1)$ . They are similar and seem like leibniz rule. Could we conclude that? Any help would be highly appreciated!","In the coordinate , the Riemann curvature tensor can be written as and the Ricci curvature can be written as where .   Prove that where My attempt I got a hint to prove and As for , I am not sure what the meaning of is. I thought that where is the dual metric in which denotes the musical isomorphism . Am I right? I can prove that and hence . I hope to use this to prove , and I tried to take advantage of . And then I guess that but I don't know whether this is right, and I don't know how to get . They are similar and seem like leibniz rule. Could we conclude that? Any help would be highly appreciated!","\{x^i\} 
R=R_{ijkl}\,dx^i\otimes dx^j\otimes dx^k \otimes dx^l
 \text{Ric}=R_{ij}\,dx^i\otimes dx^j R_{ij}=g^{kl} R_{kilj}\, R_{ij;m}=g^{kl} R_{ikjl;m} \theta_{i_1i_2\cdots i_s;m}=\left(D_{\partial_m}\theta\right)(\partial_{i_1},\cdots,\partial_{i_s})\,. R_{jk;m} = (g^{il}R_{ijkl})_{;m} =  g^{il}{}_{;m}R_{ijkl} + g^{il}R_{ijkl;m} = g^{il}R_{ijkl;m}\tag{1} g^{il}{}_{;m}=0 \,.\tag{2} (2) g^{il}{}_{;m} 
g^{il}{}_{;m}=\left(D_{\partial_m}\,\check g\right)(dx^i,dx^l)
 \check g(\alpha,\beta)=g(\alpha^\sharp, \beta^\sharp) \sharp Dg=0 g_{ij;k}=0 (2) g^{il}g_{lj}=\delta^i_j (g^{il}g_{lj})_{;m}=g^{il}{}_{;m}g_{lj}+g^{il}g_{lj;m} (1)","['differential-geometry', 'manifolds', 'riemannian-geometry', 'curvature']"
71,Faà di Bruno's formula for $C^k$ Banach-valued functions.,Faà di Bruno's formula for  Banach-valued functions.,C^k,"Let $X,Y,Z$ be Banach spaces, $f:X\to Y,g: Y\to Z$ be two functions of class $C^k$ , which means that $f^{(k)}(x)$ exists as a $k$ -linear form $\mathcal B^k(X;Y)$ and similarly for $g^{(k)}$ . Is the a Faà di Bruno-like formula for computing the $k$ -linear form $(g\circ f)^{(k)}(x)\in \mathcal B^k(X;Z)$ ? The usual Faà di Bruno's formula already looks horrible enough for real-valued $f,g$ . I can't imagine how complicated its Banach-valued would be but I am sure someone must have thought about it. If anyone know where I can look for such a formula I would be very grateful. Alternatively, I would be satisfied with a proof of the following statement: For $f\in C^k(X;Y)$ and $g\in C^k(Y;Z)$ , it is the case that $g\circ f\in C^k(X;Z)$ . It would be a direct consequence of the Banach version of Faà di Bruno's formula (if there is one, which I'm quite sure there is). The statement seems simple enough and I tried proving it using induction. However, applying chain rule twice in the case $k=2$ already looks horrible and I'm not sure what's the correct way to prove it. Maybe I should do an induction on some tree-like structures but I'm not sure of the details.","Let be Banach spaces, be two functions of class , which means that exists as a -linear form and similarly for . Is the a Faà di Bruno-like formula for computing the -linear form ? The usual Faà di Bruno's formula already looks horrible enough for real-valued . I can't imagine how complicated its Banach-valued would be but I am sure someone must have thought about it. If anyone know where I can look for such a formula I would be very grateful. Alternatively, I would be satisfied with a proof of the following statement: For and , it is the case that . It would be a direct consequence of the Banach version of Faà di Bruno's formula (if there is one, which I'm quite sure there is). The statement seems simple enough and I tried proving it using induction. However, applying chain rule twice in the case already looks horrible and I'm not sure what's the correct way to prove it. Maybe I should do an induction on some tree-like structures but I'm not sure of the details.","X,Y,Z f:X\to Y,g: Y\to Z C^k f^{(k)}(x) k \mathcal B^k(X;Y) g^{(k)} k (g\circ f)^{(k)}(x)\in \mathcal B^k(X;Z) f,g f\in C^k(X;Y) g\in C^k(Y;Z) g\circ f\in C^k(X;Z) k=2","['real-analysis', 'functional-analysis', 'differential-geometry', 'partial-differential-equations', 'dynamical-systems']"
72,"What is the ""full"" topological requirements for a (classical) spacetime?","What is the ""full"" topological requirements for a (classical) spacetime?",,"The model for (classical) spacetimes are, fundamentally, topological Manifolds. I know that this isn't the complete structure (because in fact we need to realize what is lorentz manifolds and so on...). But, the thing is, topological manifolds are the cement of spacetime definition and there's a lot of equivalent ways to use the ""requirements"" (paracompactness, hausdorff condition, etc...) and define properly what kind of structure we need to impose on they. The fact is that I wish to know all the ""hierarchical path"" to spacetime manifolds. I will explain. The requirements are then: $(\mathcal{M}, \tau)$ , a topological space, of course. Then, the next structures are listed in the following, but I don't know, properly, how is the hierachy between then (which one implies the other and so on...). First Countable Second-Countable Connected Path-Connected Separable Hausdorff Compact Paracompact Metrizable Metric Space I know that we use all of these to define, properly, a spacetime, but I don't know what is the hierarchy between them. Please fell free to be redundant, because the kind of answer that I'm looking for is something like: A spacetime $\mathfrak{M}$ is a topological manifold $(\mathcal{M}, \tau)$ , which is compact, paracompact, metrizable and so on.... So, how can I use the requirements listed above, from $1$ to $11$ , to bake the ""full"" definition of a spacetime? $$ * * * $$ Also there's another concept that I don't know how to fit in between the requirements $1$ to $10$ Normal (or Regular)","The model for (classical) spacetimes are, fundamentally, topological Manifolds. I know that this isn't the complete structure (because in fact we need to realize what is lorentz manifolds and so on...). But, the thing is, topological manifolds are the cement of spacetime definition and there's a lot of equivalent ways to use the ""requirements"" (paracompactness, hausdorff condition, etc...) and define properly what kind of structure we need to impose on they. The fact is that I wish to know all the ""hierarchical path"" to spacetime manifolds. I will explain. The requirements are then: , a topological space, of course. Then, the next structures are listed in the following, but I don't know, properly, how is the hierachy between then (which one implies the other and so on...). First Countable Second-Countable Connected Path-Connected Separable Hausdorff Compact Paracompact Metrizable Metric Space I know that we use all of these to define, properly, a spacetime, but I don't know what is the hierarchy between them. Please fell free to be redundant, because the kind of answer that I'm looking for is something like: A spacetime is a topological manifold , which is compact, paracompact, metrizable and so on.... So, how can I use the requirements listed above, from to , to bake the ""full"" definition of a spacetime? Also there's another concept that I don't know how to fit in between the requirements to Normal (or Regular)","(\mathcal{M}, \tau) \mathfrak{M} (\mathcal{M}, \tau) 1 11  * * *  1 10","['general-topology', 'differential-geometry', 'manifolds', 'differential-topology', 'general-relativity']"
73,Killing vector fields are affine,Killing vector fields are affine,,"Let $(M, g)$ be a Riemannian manifold and let $X$ be a smooth vector field on $M$ . We say that $X$ is affine if $L_X \nabla = 0$ , where $\nabla$ is the Riemannian connection on $M$ . How do we prove that Killing vector fields are affine? I know that if $X$ is a Killing vector field, then $L_X g = 0$ , which is equivalent to saying that for all other smooth vector fields $Y$ and $Z$ , $$g(\nabla_Y X, Z) + g(Y, \nabla_Z X) = 0. $$ We can also say that the flow of $X$ preserves $g$ , that is, $(\phi_X^t)^*g = g$ , where $\phi_X^t$ is the flow of $X$ at time $t$ . However, I don't know how to relate this to the Lie derivative of the connection. I also read about a formula relating the coefficients of $L_X \nabla$ with the curvature tensor (in local coordinates). However, I would like to avoid using it.","Let be a Riemannian manifold and let be a smooth vector field on . We say that is affine if , where is the Riemannian connection on . How do we prove that Killing vector fields are affine? I know that if is a Killing vector field, then , which is equivalent to saying that for all other smooth vector fields and , We can also say that the flow of preserves , that is, , where is the flow of at time . However, I don't know how to relate this to the Lie derivative of the connection. I also read about a formula relating the coefficients of with the curvature tensor (in local coordinates). However, I would like to avoid using it.","(M, g) X M X L_X \nabla = 0 \nabla M X L_X g = 0 Y Z g(\nabla_Y X, Z) + g(Y, \nabla_Z X) = 0.  X g (\phi_X^t)^*g = g \phi_X^t X t L_X \nabla","['differential-geometry', 'riemannian-geometry']"
74,Direct proof that closed 1-form on $\mathbb{R}^2$ is exact,Direct proof that closed 1-form on  is exact,\mathbb{R}^2,"Let $\omega = a_1\cdot dx_1 + a_2 \cdot dx_2 \in \Omega^1 \mathbb{R}^2$ . Show that if $d\omega = 0$ , then $$f(x) = f(x_1,x_2) := x_1 \cdot \int_0^1 a_1(tx)dt + x_2 \cdot \int_0^1 a_2(tx)dt$$ $x \in \mathbb{R}^2$ defines a function $f \in \Omega^0 \mathbb{R}^2 = C^{\infty} (\mathbb{R}^2,\mathbb{R})$ with $df = \omega$ . I tried to derive this f, but I have some problems with dealing with the map h. We defined the derivative of f like this: For a chart (U,h,V) around p with coordinates $(x_1,..,x_n)$ in V we get: $$df(p) = \sum_{i=1}^n \frac{\partial (f\circ h^{-1})}{\partial x_i} (h(p)) \cdot dx_i(p)$$ I did this so far: Using this definition we get for our f: $$df(x) = \frac{(f\circ h^{-1})}{\partial x_1} (h(x)) \cdot dx_1(p) + \frac{(f\circ h^{-1})}{\partial x_2} (h(x)) \cdot dx_2(x)$$ Using the chain rule we get $$= (\frac{\partial f}{\partial x_1}\circ h^{-1}) (h(x)) \cdot \frac{\partial h^{-1}}{\partial x_1}(h(x)) \cdot dx_1(x) + (\frac{\partial f}{\partial x_2}\circ h^{-1}) (h(x)) \cdot \frac{\partial h^{-1}}{\partial x_2}(h(x)) \cdot dx_2(x)$$ $$= (\frac{\partial f}{\partial x_1} (x)) \cdot \frac{\partial h^{-1}}{\partial x_1}(h(x)) \cdot dx_1(x) + (\frac{\partial f}{\partial x_2}(x) \cdot \frac{\partial h^{-1}}{\partial x_2}(h(x)) \cdot dx_2(x)$$ $$= ((\int_0^1 a_1(tx)dt) \cdot \frac{\partial h^{-1}}{\partial x_1}(h(x)) \cdot dx_1(x) + ((\int_0^1 a_2(tx)d) \cdot \frac{\partial h^{-1}}{\partial x_2}(h(x)) \cdot dx_2(x)$$ but I am a bit lost at this point, since I would have to use the indefinite integral of the $a_i$ and I don't see how I could end up with $\omega$ in the end. Also, I am confused by the $\frac{\partial h^{-1}}{\partial x_i}(h(x))$ . How could I do anything with them? It is just some map I don't know anything particular about.","Let . Show that if , then defines a function with . I tried to derive this f, but I have some problems with dealing with the map h. We defined the derivative of f like this: For a chart (U,h,V) around p with coordinates in V we get: I did this so far: Using this definition we get for our f: Using the chain rule we get but I am a bit lost at this point, since I would have to use the indefinite integral of the and I don't see how I could end up with in the end. Also, I am confused by the . How could I do anything with them? It is just some map I don't know anything particular about.","\omega = a_1\cdot dx_1 + a_2 \cdot dx_2 \in \Omega^1 \mathbb{R}^2 d\omega = 0 f(x) = f(x_1,x_2) := x_1 \cdot \int_0^1 a_1(tx)dt + x_2 \cdot \int_0^1 a_2(tx)dt x \in \mathbb{R}^2 f \in \Omega^0 \mathbb{R}^2 = C^{\infty} (\mathbb{R}^2,\mathbb{R}) df = \omega (x_1,..,x_n) df(p) = \sum_{i=1}^n \frac{\partial (f\circ h^{-1})}{\partial x_i} (h(p)) \cdot dx_i(p) df(x) = \frac{(f\circ h^{-1})}{\partial x_1} (h(x)) \cdot dx_1(p) + \frac{(f\circ h^{-1})}{\partial x_2} (h(x)) \cdot dx_2(x) = (\frac{\partial f}{\partial x_1}\circ h^{-1}) (h(x)) \cdot \frac{\partial h^{-1}}{\partial x_1}(h(x)) \cdot dx_1(x) + (\frac{\partial f}{\partial x_2}\circ h^{-1}) (h(x)) \cdot \frac{\partial h^{-1}}{\partial x_2}(h(x)) \cdot dx_2(x) = (\frac{\partial f}{\partial x_1} (x)) \cdot \frac{\partial h^{-1}}{\partial x_1}(h(x)) \cdot dx_1(x) + (\frac{\partial f}{\partial x_2}(x) \cdot \frac{\partial h^{-1}}{\partial x_2}(h(x)) \cdot dx_2(x) = ((\int_0^1 a_1(tx)dt) \cdot \frac{\partial h^{-1}}{\partial x_1}(h(x)) \cdot dx_1(x) + ((\int_0^1 a_2(tx)d) \cdot \frac{\partial h^{-1}}{\partial x_2}(h(x)) \cdot dx_2(x) a_i \omega \frac{\partial h^{-1}}{\partial x_i}(h(x))","['differential-geometry', 'manifolds', 'differential-forms']"
75,Find an isometry from a plane to another in $R^3$,Find an isometry from a plane to another in,R^3,"Question If P is the plane through $(1/2, -1, 0)$ orthogonal to $(0, 1, 0)$ find an isometry F = TC such that F(P) is the plane through $(1, -2, 1)$ orthogonal to $(1, 0, -1)$ . This is an exercise in O'neill's Elementary Differential Geometry. I found the given plane P: y = -1 and F(P): x-z= $0$ But that's all I did. How should I approach this problem?",Question If P is the plane through orthogonal to find an isometry F = TC such that F(P) is the plane through orthogonal to . This is an exercise in O'neill's Elementary Differential Geometry. I found the given plane P: y = -1 and F(P): x-z= But that's all I did. How should I approach this problem?,"(1/2, -1, 0) (0, 1, 0) (1, -2, 1) (1, 0, -1) 0",['differential-geometry']
76,volume of an infinite dimensional ball.,volume of an infinite dimensional ball.,,"I am was doing some problems from my text book and one of them says "" let the volume of of the nth dimensional ball be denoted by $vol(B^n(R))$ where $R$ is the radius of the ball. Show that $\lim_{n\to\infty} vol(B^n(R))$ "" I have seen explanations of this, but all of them use the gamma function. I was wondering if there was any way to do it without using it. So far I have shown that $vol(B^n(R)=vol(B^{n-2}(R))\frac{2\pi R^2}{n}$ . Using this I was thinking you could keep iterating and you would get $n(n-2)(n-4)$ in the denominator and so on which is less than $n!$ and whatever is above will not depend on n so we could treat it as a constant, thus you would have something like $\frac{T}{n!}<\frac{T}{n(n-2)(n-4)..}$ so then by taking limits the RHS crushes the left hand side so they both go to 0. Is this right ? I am not sure if this is even rigorous enough. Any help is appreciated :). Thank you.","I am was doing some problems from my text book and one of them says "" let the volume of of the nth dimensional ball be denoted by where is the radius of the ball. Show that "" I have seen explanations of this, but all of them use the gamma function. I was wondering if there was any way to do it without using it. So far I have shown that . Using this I was thinking you could keep iterating and you would get in the denominator and so on which is less than and whatever is above will not depend on n so we could treat it as a constant, thus you would have something like so then by taking limits the RHS crushes the left hand side so they both go to 0. Is this right ? I am not sure if this is even rigorous enough. Any help is appreciated :). Thank you.",vol(B^n(R)) R \lim_{n\to\infty} vol(B^n(R)) vol(B^n(R)=vol(B^{n-2}(R))\frac{2\pi R^2}{n} n(n-2)(n-4) n! \frac{T}{n!}<\frac{T}{n(n-2)(n-4)..},"['real-analysis', 'linear-algebra', 'integration', 'differential-geometry', 'vector-analysis']"
77,$\nabla_a \nabla_b v^c$ abstract index notation,abstract index notation,\nabla_a \nabla_b v^c,"I am having some trouble with the equivalence between abstract index notation (AIN) and standard tensor component notation (TCN, for short). Let us consider a covariant derivative $\nabla$ . In TCN, we can define it, for a given basis of tangent vectors $e_i$ , by the relation $$ \nabla_i e_j = \Gamma_{ij}^k e_k\,, $$ where $\nabla_i=\nabla_{e_i}$ . Then, for any vector field $v=v^i e_i$ , we have $$ \nabla_i v = (\partial_iv^j + \Gamma^j_{ik}v^k)e_j \implies (\nabla_i v)^j=\partial_iv^j + \Gamma^j_{ik}v^k $$ since $\nabla_i(v^j e_j)=(\nabla_i v^j) e_j + v^j(\nabla_i e_j)$ and $\nabla_i v^j=\partial_i v^j$ . In AIN, this translates to $$ \nabla_a v^b = \partial_a v^b + \Gamma_{ac}^b v^c\,, $$ where however $a, b,c,\ldots$ are just abstract indices and not components with respect to a specific basis. Let us now consider the second derivative. In TCN, we have $$ \nabla_i \nabla_j v = \nabla_i((\partial_j v^k+ \Gamma^k_{jl}v^l)e_k)=(\partial_i(\partial_j v^k+ \Gamma^k_{jl}v^l)+(\partial_j v^m+ \Gamma^m_{jl}v^l)\Gamma_{im}^k)e_k\,. $$ Therefore, $$ (\nabla_i \nabla_j v)^k=\partial_i \partial_j v^k + v^l \partial_i \Gamma^k_{jl}+\Gamma^k_{jl}\partial_i v^l + \Gamma_{il}^k \partial_j v^l+ \Gamma^k_{im}\Gamma^{m}_{jl}v^l\,. $$ On the other hand, in AIN, $$ \nabla_a \nabla_b v^c = \partial_a \nabla_b v^c + \Gamma^c_{ad}\nabla_b v^d-\Gamma_{ab}^d \nabla_d v^c\,, $$ because we need to treat the lower $b$ index in $\nabla_b$ according to its covariant nature, and therefore $$ \nabla_a \nabla_b v^c = \partial_a \partial_b v^c + v^d\partial_a\Gamma^c_{bd} + \Gamma^c_{bd}\partial_a v^d + \Gamma^c_{ad}\partial_b v^d + \Gamma^c_{ad}\Gamma^{d}_{be}v^e-\Gamma_{ab}^d(\partial_d v^c + \Gamma_{de}^c v^e)\,. $$ But the last term is not there in TCN! Do I have to conclude that somehow these two notations are not really equivalent? (I always assumed they were...) The unwanted piece cancels in the calculation of the commutator $[\nabla_a,\nabla_b]v^c$ , provided the connection is symmetric, so this is not really an issue in GR, but still I would like to understand what goes wrong.","I am having some trouble with the equivalence between abstract index notation (AIN) and standard tensor component notation (TCN, for short). Let us consider a covariant derivative . In TCN, we can define it, for a given basis of tangent vectors , by the relation where . Then, for any vector field , we have since and . In AIN, this translates to where however are just abstract indices and not components with respect to a specific basis. Let us now consider the second derivative. In TCN, we have Therefore, On the other hand, in AIN, because we need to treat the lower index in according to its covariant nature, and therefore But the last term is not there in TCN! Do I have to conclude that somehow these two notations are not really equivalent? (I always assumed they were...) The unwanted piece cancels in the calculation of the commutator , provided the connection is symmetric, so this is not really an issue in GR, but still I would like to understand what goes wrong.","\nabla e_i 
\nabla_i e_j = \Gamma_{ij}^k e_k\,,
 \nabla_i=\nabla_{e_i} v=v^i e_i 
\nabla_i v = (\partial_iv^j + \Gamma^j_{ik}v^k)e_j \implies (\nabla_i v)^j=\partial_iv^j + \Gamma^j_{ik}v^k
 \nabla_i(v^j e_j)=(\nabla_i v^j) e_j + v^j(\nabla_i e_j) \nabla_i v^j=\partial_i v^j 
\nabla_a v^b = \partial_a v^b + \Gamma_{ac}^b v^c\,,
 a, b,c,\ldots 
\nabla_i \nabla_j v = \nabla_i((\partial_j v^k+ \Gamma^k_{jl}v^l)e_k)=(\partial_i(\partial_j v^k+ \Gamma^k_{jl}v^l)+(\partial_j v^m+ \Gamma^m_{jl}v^l)\Gamma_{im}^k)e_k\,.
 
(\nabla_i \nabla_j v)^k=\partial_i \partial_j v^k + v^l \partial_i \Gamma^k_{jl}+\Gamma^k_{jl}\partial_i v^l + \Gamma_{il}^k \partial_j v^l+ \Gamma^k_{im}\Gamma^{m}_{jl}v^l\,.
 
\nabla_a \nabla_b v^c = \partial_a \nabla_b v^c + \Gamma^c_{ad}\nabla_b v^d-\Gamma_{ab}^d \nabla_d v^c\,,
 b \nabla_b 
\nabla_a \nabla_b v^c = \partial_a \partial_b v^c + v^d\partial_a\Gamma^c_{bd} + \Gamma^c_{bd}\partial_a v^d + \Gamma^c_{ad}\partial_b v^d + \Gamma^c_{ad}\Gamma^{d}_{be}v^e-\Gamma_{ab}^d(\partial_d v^c + \Gamma_{de}^c v^e)\,.
 [\nabla_a,\nabla_b]v^c","['differential-geometry', 'tensors', 'connections', 'index-notation']"
78,Computing the differential of the coadjoint representation of a Lie group at origin ( $ad^*$),Computing the differential of the coadjoint representation of a Lie group at origin ( ),ad^*,"Let $G$ be a Lie group and $Ad^*: \mathfrak{g} \rightarrow GL(\mathfrak{g}^*)$ , $[Ad^*(g)(\xi)](x) \stackrel{def}{=} \xi(Ad(g^{-1})x)$ , the coadjoint representation. I am trying to compute $ad^*: \mathfrak{g} \rightarrow \mathfrak{gl}(\mathfrak{g}^*)$ , $ad^* = d_eAd^*$ . Supposedly, it is $ad^*(x)(\xi)(y)=-\xi([x,y]_G)$ . I know that $ad(x)(y)=[x,y]_G, \forall x,y \in \mathfrak{g}$ . Intuitively, it seems like what is happening is that the differential of $Ad^*$ ""passes over"" each 1-form $\xi$ and simply differentiates $Ad(a^{-1})$ . Since $Ad(a^{-1})=(Ad(a))^{-1} = (i \circ Ad)(a)$ , where $i$ is the inverse in $GL(\mathfrak{g})$ , and $d_ei=-id$ , the result would follow. But I can't see how to prove this ""passing over"" rigorously. Also, $[\cdot, \cdot]_G$ can be seen as the linear term of the function $\nu(x,y)$ which makes it that $exp(x)exp(y)=exp(\nu(x,y))$ on a neighborhood of $0_{\mathfrak{g}}$ . Based on this and the fact that the diagram: $$ \require{AMScd} \begin{CD} \mathfrak{g} @>{ad}>> \mathfrak{gl}(\mathfrak{g})\\ @VV{exp_G}V @VV{exp_{GL(\mathfrak{g})}}V \\ G @>{Ad}>> GL(\mathfrak{g}) \end{CD} $$ commutes, it is shown that $ad(x)(y)=[x,y]_G$ . (A more general proposition holds, namely that if $\Phi:G \rightarrow H$ is a lie group homeomorphism, then the diagram: $$ \require{AMScd} \begin{CD} \mathfrak{g} @>{d_e\Phi}>> \mathfrak{h}\\ @VV{exp_G}V @VV{exp_H}V \\ G @>{\Phi}>> \mathfrak{h} \end{CD} $$ commutes.) That is, it suffices to calculate $Ad$ and then use the theorem to get to $ad$ . However, the argument begins by exploiting the definition of $Ad(a) = d_e(b \rightarrow aba^{-1})$ in the sense that $Ad(exp(x))(y)$ can be seen as $\gamma'(0)$ for $\gamma(t)=exp(x)exp(ty)exp(x)^{-1}$ and proceeds from there. I tried reproducing the argument for $Ad^*: G \rightarrow GL(\mathfrak{g}^*)$ and $ad^*: \mathfrak{g} \rightarrow \mathfrak{gl}(\mathfrak{g}^*)$ instead of $Ad: G \rightarrow GL(\mathfrak{g})$ and $ad: \mathfrak{g} \rightarrow \mathfrak{gl}(\mathfrak{g})$ respectively ( $Ad^*$ and $ad^*$ are also Lie group homeomorphisms), but I can't find a way to exploit the definition of $Ad^*$ in the same way. Thank you for your time.","Let be a Lie group and , , the coadjoint representation. I am trying to compute , . Supposedly, it is . I know that . Intuitively, it seems like what is happening is that the differential of ""passes over"" each 1-form and simply differentiates . Since , where is the inverse in , and , the result would follow. But I can't see how to prove this ""passing over"" rigorously. Also, can be seen as the linear term of the function which makes it that on a neighborhood of . Based on this and the fact that the diagram: commutes, it is shown that . (A more general proposition holds, namely that if is a lie group homeomorphism, then the diagram: commutes.) That is, it suffices to calculate and then use the theorem to get to . However, the argument begins by exploiting the definition of in the sense that can be seen as for and proceeds from there. I tried reproducing the argument for and instead of and respectively ( and are also Lie group homeomorphisms), but I can't find a way to exploit the definition of in the same way. Thank you for your time.","G Ad^*: \mathfrak{g} \rightarrow GL(\mathfrak{g}^*) [Ad^*(g)(\xi)](x) \stackrel{def}{=} \xi(Ad(g^{-1})x) ad^*: \mathfrak{g} \rightarrow \mathfrak{gl}(\mathfrak{g}^*) ad^* = d_eAd^* ad^*(x)(\xi)(y)=-\xi([x,y]_G) ad(x)(y)=[x,y]_G, \forall x,y \in \mathfrak{g} Ad^* \xi Ad(a^{-1}) Ad(a^{-1})=(Ad(a))^{-1} = (i \circ Ad)(a) i GL(\mathfrak{g}) d_ei=-id [\cdot, \cdot]_G \nu(x,y) exp(x)exp(y)=exp(\nu(x,y)) 0_{\mathfrak{g}} 
\require{AMScd}
\begin{CD}
\mathfrak{g} @>{ad}>> \mathfrak{gl}(\mathfrak{g})\\
@VV{exp_G}V @VV{exp_{GL(\mathfrak{g})}}V \\
G @>{Ad}>> GL(\mathfrak{g})
\end{CD}
 ad(x)(y)=[x,y]_G \Phi:G \rightarrow H 
\require{AMScd}
\begin{CD}
\mathfrak{g} @>{d_e\Phi}>> \mathfrak{h}\\
@VV{exp_G}V @VV{exp_H}V \\
G @>{\Phi}>> \mathfrak{h}
\end{CD}
 Ad ad Ad(a) = d_e(b \rightarrow aba^{-1}) Ad(exp(x))(y) \gamma'(0) \gamma(t)=exp(x)exp(ty)exp(x)^{-1} Ad^*: G \rightarrow GL(\mathfrak{g}^*) ad^*: \mathfrak{g} \rightarrow \mathfrak{gl}(\mathfrak{g}^*) Ad: G \rightarrow GL(\mathfrak{g}) ad: \mathfrak{g} \rightarrow \mathfrak{gl}(\mathfrak{g}) Ad^* ad^* Ad^*","['differential-geometry', 'representation-theory', 'lie-groups', 'lie-algebras']"
79,Hypersurface-orthogonality and static spacetimes,Hypersurface-orthogonality and static spacetimes,,"I am currently learning about general relativity using these notes and I am confused by the notion of hypersurface orthogonality of a vector field. We define a spacetime to be stationary if there exists a timelike Killing vector field K. In this case we construct a system of coordinates $(t, x^i)$ where $x^i$ are coords on a hypersurface which is nowhere tangent to $K^a$ , $K = \frac{\partial}{\partial t}$ and the metric takes the form $$ ds^2 = g_{00}(x^k)dt^2 + 2g_{0i}(x^k)dtdx^i + g_{ij}(x^k). \label{eq:metric}$$ If in addition the Killing vector $K^a$ is hypersurface-orthogonal then we say the spacetime is static. We can choose the hypersurface in the construction of the above coords to be orthogonal to $K^a$ , and so the metric has $g_{0i} = 0$ in these coordinates. My issue here is that in my head I picture that for any smooth covector field there should be some hypersurface which is orthogonal to it. This would mean that any vector field is hypersurface orthogonal and hence any stationary spacetime is automatically static, but this is clearly false. Why is it that some vector fields are not hypersurface-orthogonal? The standard example of a stationary but not static spacetime is the Kerr metric. In Boyer-Lindquist coordinates, the metric is in the above form with $g_{0i} \neq 0$ (to be precise there is a $dtd\varphi$ cross term). This means that when lowering the index on $K^a = \left(\frac{\partial}{\partial t}\right)^a$ we get a $d\varphi$ component, ensuring that $K_a$ is not orthogonal to the constant $t$ surfaces. But why does this mean that Kerr is not static, since surely there could be a different foliation by hypersurfaces which are orthogonal to $K$ ?","I am currently learning about general relativity using these notes and I am confused by the notion of hypersurface orthogonality of a vector field. We define a spacetime to be stationary if there exists a timelike Killing vector field K. In this case we construct a system of coordinates where are coords on a hypersurface which is nowhere tangent to , and the metric takes the form If in addition the Killing vector is hypersurface-orthogonal then we say the spacetime is static. We can choose the hypersurface in the construction of the above coords to be orthogonal to , and so the metric has in these coordinates. My issue here is that in my head I picture that for any smooth covector field there should be some hypersurface which is orthogonal to it. This would mean that any vector field is hypersurface orthogonal and hence any stationary spacetime is automatically static, but this is clearly false. Why is it that some vector fields are not hypersurface-orthogonal? The standard example of a stationary but not static spacetime is the Kerr metric. In Boyer-Lindquist coordinates, the metric is in the above form with (to be precise there is a cross term). This means that when lowering the index on we get a component, ensuring that is not orthogonal to the constant surfaces. But why does this mean that Kerr is not static, since surely there could be a different foliation by hypersurfaces which are orthogonal to ?","(t, x^i) x^i K^a K = \frac{\partial}{\partial t}  ds^2 = g_{00}(x^k)dt^2 + 2g_{0i}(x^k)dtdx^i + g_{ij}(x^k). \label{eq:metric} K^a K^a g_{0i} = 0 g_{0i} \neq 0 dtd\varphi K^a = \left(\frac{\partial}{\partial t}\right)^a d\varphi K_a t K","['differential-geometry', 'mathematical-physics', 'general-relativity']"
80,Intersection of the zero sets of two multivariate polynomials,Intersection of the zero sets of two multivariate polynomials,,"Suppose now I have two multivariate polynomials, say $p_1: \mathbb{R}^n\to\mathbb{R}$ and $p_2: \mathbb{R}^n\to\mathbb{R}$ , $n\ge 2$ . Suppose they are both irreducible, and $A$ , $B$ are the zero sets of $p_1$ and $p_2$ respectively. Suppose "" $A$ and $B$ has a common part"". This description is not mathematically rigorous, but I cannot find a better way to describe it. So let me give some examples to explain it. For example, in 2D, $A$ and $B$ represent curves. Then "" $A$ and $B$ has a common part"" means that they share a common sub-arc. In 3D, then they share a common subsurface and so on. I am wondering under this assumption, can we say that $A$ must be equal to $B$ ? Or can we say that $p_1$ and $p_2$ are the same up to a scale? For $n=2$ . I think the answer is positive. If we have two curves which are given by the zero sets of the two polynomials, and the two curves share a sub-arc. Then the two irreducible polynomials must be the same up to a scale by Bezout's theorem. But I do not know what will happen if $n\ge 3$ . I guess we will still have the conclusion but I do not know how to show it.","Suppose now I have two multivariate polynomials, say and , . Suppose they are both irreducible, and , are the zero sets of and respectively. Suppose "" and has a common part"". This description is not mathematically rigorous, but I cannot find a better way to describe it. So let me give some examples to explain it. For example, in 2D, and represent curves. Then "" and has a common part"" means that they share a common sub-arc. In 3D, then they share a common subsurface and so on. I am wondering under this assumption, can we say that must be equal to ? Or can we say that and are the same up to a scale? For . I think the answer is positive. If we have two curves which are given by the zero sets of the two polynomials, and the two curves share a sub-arc. Then the two irreducible polynomials must be the same up to a scale by Bezout's theorem. But I do not know what will happen if . I guess we will still have the conclusion but I do not know how to show it.",p_1: \mathbb{R}^n\to\mathbb{R} p_2: \mathbb{R}^n\to\mathbb{R} n\ge 2 A B p_1 p_2 A B A B A B A B p_1 p_2 n=2 n\ge 3,"['differential-geometry', 'algebraic-geometry', 'polynomials', 'roots', 'irreducible-polynomials']"
81,Parallel vector fields orthonormal at a point implies manifold is euclidean,Parallel vector fields orthonormal at a point implies manifold is euclidean,,"Let $(N, g)$ be a complete Riemannian $n$ -manifold such that there is a compact set $K$ such that $N\setminus K$ is diffeomorphic to $\mathbb{R}^n \setminus D^n$ . Suppose there exist $n$ parallel vector fields $V_1, ..., V_n$ that are orthonormal at a point. I want to prove that the manifold is globally isometric to $\mathbb{R}^n$ with the euclidean metric. First part of the proof: As $\forall X \in \mathfrak{X}(N)$ $$ 		\nabla_X \langle V_i, V_j \rangle = \langle \nabla_X V_i, V_j \rangle +\langle V_i, \nabla_X V_j \rangle = 0 		$$ $\langle V_i, V_j \rangle$ is constant.         Moreover, as it is equal to $\delta_{ij}$ a a point, $\{V_i\}$ are orthogonal everywhere by continuity.         Now, as the Levi-Civita connection is torsion-free $$ 		[V_i, V_j] = \nabla_{V_i} V_j - \nabla_{V_j} V_i = 0 		$$ and hence they are integrable locally to a chart which is an isometry because of the equation $\langle V_i, V_j \rangle = \delta_{ij}$ . Missing part: The goal is to use Killing-Hopf theorem to prove the following lemma Lemma: Let $(M,g)$ be a complete flat Riemannian manifold such that there is a compact set $K \subset M$ with $M\setminus K$ diffeomeorphic to $\mathbb{R}^n \setminus D^n$ . Then $(M,g)$ is isometric to Euclidean space. but I am not really sure how to do this. Does anybody have a hint? (In case someone is wondering I need this to give a rigorous proof of the positive mass rigidity in the positive mass theorem).","Let be a complete Riemannian -manifold such that there is a compact set such that is diffeomorphic to . Suppose there exist parallel vector fields that are orthonormal at a point. I want to prove that the manifold is globally isometric to with the euclidean metric. First part of the proof: As is constant.         Moreover, as it is equal to a a point, are orthogonal everywhere by continuity.         Now, as the Levi-Civita connection is torsion-free and hence they are integrable locally to a chart which is an isometry because of the equation . Missing part: The goal is to use Killing-Hopf theorem to prove the following lemma Lemma: Let be a complete flat Riemannian manifold such that there is a compact set with diffeomeorphic to . Then is isometric to Euclidean space. but I am not really sure how to do this. Does anybody have a hint? (In case someone is wondering I need this to give a rigorous proof of the positive mass rigidity in the positive mass theorem).","(N, g) n K N\setminus K \mathbb{R}^n \setminus D^n n V_1, ..., V_n \mathbb{R}^n \forall X \in \mathfrak{X}(N) 
		\nabla_X \langle V_i, V_j \rangle = \langle \nabla_X V_i, V_j \rangle +\langle V_i, \nabla_X V_j \rangle = 0
		 \langle V_i, V_j \rangle \delta_{ij} \{V_i\} 
		[V_i, V_j] = \nabla_{V_i} V_j - \nabla_{V_j} V_i = 0
		 \langle V_i, V_j \rangle = \delta_{ij} (M,g) K \subset M M\setminus K \mathbb{R}^n \setminus D^n (M,g)","['differential-geometry', 'riemannian-geometry']"
82,Outward normal vector on a curve,Outward normal vector on a curve,,"Given a simple closed curve $\gamma:[a,b]\subset\mathbb{R}\to\mathbb{R^2}, \gamma(t) = (x(t), y(t))$ , $x(t)$ and $y(t)$ continuous in $\mathbb{R}$ and a point $P(x, y) = \gamma(t)$ for some $t \in [a,b]$ , it is known that two normal unit vectors exist for $P$ , which can be found by solving: $ \begin{equation} \begin{cases} \mathbf{n}\cdot\gamma'(t)=0\\ ||\mathbf{n}||=1 \end{cases} \end{equation} $ Let the solutions be $\mathbf{n_1}$ and $\mathbf{n_2}$ with $\mathbf{n_1} = -\mathbf{n_2}$ . Can one algebraically decide which of these unit vectors points outwards, i.e., without having to resort to plotting the curve and analyzing it?","Given a simple closed curve , and continuous in and a point for some , it is known that two normal unit vectors exist for , which can be found by solving: Let the solutions be and with . Can one algebraically decide which of these unit vectors points outwards, i.e., without having to resort to plotting the curve and analyzing it?","\gamma:[a,b]\subset\mathbb{R}\to\mathbb{R^2}, \gamma(t) = (x(t), y(t)) x(t) y(t) \mathbb{R} P(x, y) = \gamma(t) t \in [a,b] P 
\begin{equation}
\begin{cases}
\mathbf{n}\cdot\gamma'(t)=0\\
||\mathbf{n}||=1
\end{cases}
\end{equation}
 \mathbf{n_1} \mathbf{n_2} \mathbf{n_1} = -\mathbf{n_2}","['differential-geometry', 'vector-analysis']"
83,Volumes and surface areas for the $d-$sphere $S^d$ rather than the $d-$ball.,Volumes and surface areas for the sphere  rather than the ball.,d- S^d d-,"I've been struggling with the concept of volume and surface area for the $d-$ sphere $S^d$ . For concreteness, I can give the $d-$ sphere of radius $R$ an extrinsic definition by embedding it in $(d+1)-$ dimensional Euclidean space: $$S^d = \{x\in \mathbb{R}^{d+1}: |x|=R\}$$ (although I can't see how one defines this intrinsically, any suggestions?) $S^d$ is a d-dimensional manifold without boundary. My confusion begins from here: Does it make sense to talk about the volume of $S^d$ given that it's merely a spherical shell and isn't ""solid"" in the conventional sense? Presumably, it's volume should be zero when viewed as an embedding in $R^{d+1}$ ? Note: I stress, I'm not asking for the volume of the region in $\mathbb{R}^{d+1}$ enclosed by this spherical surface, but rather the volume of the shell $S^d$ itself. What is the precise definition for the volume of a manifold without boundary? Does it make sense to speak of a surface area of $S^d$ given that it has no boundary itself? What is the precise definition for the surface area of a manifold without boundary? Note, I can see both concepts being totally clear for the $(d+1)-$ ball because: The $(d+1)-$ ball is ""solid"" in $(d+1)-$ dimensional space and has a boundary $S^d$ , so the concept of volume is intuitively clear to me here. The surface area of the $(d+1)-$ ball is equally easy to visualise given that it possesses a boundary, and one would get the usual ""surface area of a sphere"". But, surely, the surface area of $B^{d+1}$ and the surface area of $S^{d}$ cannot be defined in the same way, whilst also giving the same answer? I just can't make sense of this question when I consider $S^d$ as a manifold in it's own right, rather than through embedding. For instance, the volume of the closed unit disk $B^2$ in two dimensions is clearly $V(B^2)=\pi R^2$ , and the surface area is corresponding the circumference of the boundary $S^1$ : $A(B^2) = 2\pi R$ . My question is, what are $V(S^1)$ and $A(S^1)$ exactly? Is $V(S^1) = A(B^2)$ ? If there are issues with the small dimensionality here, one can ask the same question for $B^3$ and $S^2$ . I appreciate that this question is very muddled, so please fire away for any clarifications. It has very been painful to search the internet for answers to this.","I've been struggling with the concept of volume and surface area for the sphere . For concreteness, I can give the sphere of radius an extrinsic definition by embedding it in dimensional Euclidean space: (although I can't see how one defines this intrinsically, any suggestions?) is a d-dimensional manifold without boundary. My confusion begins from here: Does it make sense to talk about the volume of given that it's merely a spherical shell and isn't ""solid"" in the conventional sense? Presumably, it's volume should be zero when viewed as an embedding in ? Note: I stress, I'm not asking for the volume of the region in enclosed by this spherical surface, but rather the volume of the shell itself. What is the precise definition for the volume of a manifold without boundary? Does it make sense to speak of a surface area of given that it has no boundary itself? What is the precise definition for the surface area of a manifold without boundary? Note, I can see both concepts being totally clear for the ball because: The ball is ""solid"" in dimensional space and has a boundary , so the concept of volume is intuitively clear to me here. The surface area of the ball is equally easy to visualise given that it possesses a boundary, and one would get the usual ""surface area of a sphere"". But, surely, the surface area of and the surface area of cannot be defined in the same way, whilst also giving the same answer? I just can't make sense of this question when I consider as a manifold in it's own right, rather than through embedding. For instance, the volume of the closed unit disk in two dimensions is clearly , and the surface area is corresponding the circumference of the boundary : . My question is, what are and exactly? Is ? If there are issues with the small dimensionality here, one can ask the same question for and . I appreciate that this question is very muddled, so please fire away for any clarifications. It has very been painful to search the internet for answers to this.",d- S^d d- R (d+1)- S^d = \{x\in \mathbb{R}^{d+1}: |x|=R\} S^d S^d R^{d+1} \mathbb{R}^{d+1} S^d S^d (d+1)- (d+1)- (d+1)- S^d (d+1)- B^{d+1} S^{d} S^d B^2 V(B^2)=\pi R^2 S^1 A(B^2) = 2\pi R V(S^1) A(S^1) V(S^1) = A(B^2) B^3 S^2,"['geometry', 'differential-geometry', 'spheres']"
84,Proof from John Lee on the equivalence of topological boundary and manifold boundary for regular domains,Proof from John Lee on the equivalence of topological boundary and manifold boundary for regular domains,,"I am having difficulty following the conclusion of the proof below from John Lee's Introduction to Smooth Manifolds. Here a regular domain in $M$ is a properly embedded codimension- $0$ submanifolds with boundary. Here $F$ is the inclusion map from $D \hookrightarrow M$ , which is a smooth embedding. I understand the proof before the final sentence. But how do we conclude from here that every neighborhood of $p$ intersects both $D$ and $M\backslash D$ ?","I am having difficulty following the conclusion of the proof below from John Lee's Introduction to Smooth Manifolds. Here a regular domain in is a properly embedded codimension- submanifolds with boundary. Here is the inclusion map from , which is a smooth embedding. I understand the proof before the final sentence. But how do we conclude from here that every neighborhood of intersects both and ?",M 0 F D \hookrightarrow M p D M\backslash D,"['general-topology', 'differential-geometry', 'manifolds', 'smooth-manifolds']"
85,Tangent and cotangent bundle as an associated bundle,Tangent and cotangent bundle as an associated bundle,,"In a book I read the isomorphisms below were mentioned without any explanation. Is there any intuitive way to see these identities hold? Let $M$ be a smooth $n$ -manifold, $F(M)$ a frame bundle of $M$ , and $\lambda_{0}$ the standard action of $\operatorname{GL}(n, \mathbb{R})$ on $\mathbb{R}^{n}$ . Then $$\begin{align} F(M) \times_{\lambda_{0}} \mathbb{R}^{n} &\cong TM \\  F(M) \times_{\lambda^{*}_{0}} \mathbb{R}^{n} &\cong T^{*}M \end{align} $$ where $\lambda^{*}_{0}$ denotes the dual representation to $\lambda_{0}$ .","In a book I read the isomorphisms below were mentioned without any explanation. Is there any intuitive way to see these identities hold? Let be a smooth -manifold, a frame bundle of , and the standard action of on . Then where denotes the dual representation to .","M n F(M) M \lambda_{0} \operatorname{GL}(n, \mathbb{R}) \mathbb{R}^{n} \begin{align} F(M) \times_{\lambda_{0}} \mathbb{R}^{n} &\cong TM \\ 
F(M) \times_{\lambda^{*}_{0}} \mathbb{R}^{n} &\cong T^{*}M \end{align}  \lambda^{*}_{0} \lambda_{0}","['differential-geometry', 'representation-theory', 'fiber-bundles', 'principal-bundles', 'tangent-bundle']"
86,Characterization of tangent space independent of parametrization,Characterization of tangent space independent of parametrization,,"Let $\mathcal{M}$ be a smooth $n$ -dimensional submanifold of $\mathbb{R}^{n + d}$ ; $p \in \mathcal{M}$ ; $c : (-\varepsilon, \varepsilon) \rightarrow \mathbb{R}^{n + d}$ a differentiable curve with $c ((-\varepsilon, \varepsilon)) \subset \mathcal{M}$ and $c (0) = p$ ; $T_p \mathcal{M} = \operatorname{range} (D f_u)$ the tangent space at $p$ , where $f$ is a parametrization of $\mathcal{M}$ with $f (u) = p$ . Show that $T_p \mathcal{M}$ doesn't depend on the chosen parametrization $f$ . My attempt: Let $f : U \rightarrow \mathbb{R}^{n + d}$ and $\tilde{f} : \widetilde{U} \rightarrow \mathbb{R}^{n + d}$ both parametrize $\mathcal{M}$ around $f (u) = p = \tilde{f} (\widetilde{u})$ . We have $\operatorname{range} (D f_u) = T_p \mathcal{M} = \operatorname{range} (D \tilde{f}_{\tilde{u}})$ and $\operatorname{rank} (D f_u) = \operatorname{rank} (D \tilde{f}_{\tilde{u}}) = n$ . Hence, there is $\alpha \in \mathbb{R}^n$ such that $v = D f_u \alpha$ iff there is $\widetilde{\alpha} \in \mathbb{R}^n$ such that $v = D \tilde{f}_{\tilde{u}} \widetilde{\alpha}$ for $v \in T_p \mathcal{M}$ , and the characterization of $T_p \mathcal{M}$ is thus independent of the parametrization. This seems too simple indeed and I'm not sure whether I'm really proving the claim. Is there anything that I'm missing?","Let be a smooth -dimensional submanifold of ; ; a differentiable curve with and ; the tangent space at , where is a parametrization of with . Show that doesn't depend on the chosen parametrization . My attempt: Let and both parametrize around . We have and . Hence, there is such that iff there is such that for , and the characterization of is thus independent of the parametrization. This seems too simple indeed and I'm not sure whether I'm really proving the claim. Is there anything that I'm missing?","\mathcal{M} n \mathbb{R}^{n + d} p \in \mathcal{M} c : (-\varepsilon, \varepsilon) \rightarrow \mathbb{R}^{n + d} c ((-\varepsilon, \varepsilon)) \subset \mathcal{M} c (0) = p T_p \mathcal{M} = \operatorname{range} (D f_u) p f \mathcal{M} f (u) = p T_p \mathcal{M} f f : U \rightarrow \mathbb{R}^{n + d} \tilde{f} : \widetilde{U} \rightarrow \mathbb{R}^{n + d} \mathcal{M} f (u) = p = \tilde{f} (\widetilde{u}) \operatorname{range} (D f_u) = T_p \mathcal{M} = \operatorname{range} (D \tilde{f}_{\tilde{u}}) \operatorname{rank} (D f_u) = \operatorname{rank} (D \tilde{f}_{\tilde{u}}) = n \alpha \in \mathbb{R}^n v = D f_u \alpha \widetilde{\alpha} \in \mathbb{R}^n v = D \tilde{f}_{\tilde{u}} \widetilde{\alpha} v \in T_p \mathcal{M} T_p \mathcal{M}","['differential-geometry', 'riemannian-geometry']"
87,"Without invoking Riesz's representation theorem, why do Hodge duals exist?","Without invoking Riesz's representation theorem, why do Hodge duals exist?",,"Working through Peter Szekeres's Course in Modern Mathematical Physics and I'm running into some confusion in its development of the Hodge dual. In particular, I'm not clear on what justification the book is using for its existence. Choose a basis and let $E$ be its volume element. Let $A\in\Lambda^p(V)$ let the map $f_A:\Lambda^{n-p}(V)\to\mathbb R$ be defined by $A\land B=f_A(B)E$ . It's easy to check that $f_A$ is a linear functional. Szekeres claims on page 223: ...as the inner product $(\cdot,\cdot)$ on $\Lambda^{n-p}(V)$ is non-singular there exists a unique $(n-p)$ -vector $\ast A$ such that $f_A(B)=(\ast A,B)$ ... Ignoring the ""non-singular"" part, this claim is clearly immediately true by the Riesz representation theorem. However, this bit of algebra/analysis isn't introduced in the text until way later. What I'm interested in is what justification is being used here . What the book defines as being non-singular is the following property of inner products on page 127: If $u\cdot v=0$ for all $v\in V$ then $u=0$ . Am I missing something obvious? I don't see the chain of reasoning here.","Working through Peter Szekeres's Course in Modern Mathematical Physics and I'm running into some confusion in its development of the Hodge dual. In particular, I'm not clear on what justification the book is using for its existence. Choose a basis and let be its volume element. Let let the map be defined by . It's easy to check that is a linear functional. Szekeres claims on page 223: ...as the inner product on is non-singular there exists a unique -vector such that ... Ignoring the ""non-singular"" part, this claim is clearly immediately true by the Riesz representation theorem. However, this bit of algebra/analysis isn't introduced in the text until way later. What I'm interested in is what justification is being used here . What the book defines as being non-singular is the following property of inner products on page 127: If for all then . Am I missing something obvious? I don't see the chain of reasoning here.","E A\in\Lambda^p(V) f_A:\Lambda^{n-p}(V)\to\mathbb R A\land B=f_A(B)E f_A (\cdot,\cdot) \Lambda^{n-p}(V) (n-p) \ast A f_A(B)=(\ast A,B) u\cdot v=0 v\in V u=0","['linear-algebra', 'differential-geometry', 'exterior-algebra']"
88,Frame bundle is parallelizable - Kobayashi,Frame bundle is parallelizable - Kobayashi,,"Let $M$ be a Riemannian manifold, and let $L(M)$ be the associated frame bundle. At the end of page 40 of Kobayashi's book , as I understand, it is stated that: There exsits $n^2$ connection forms $\omega_j^i$ on $L(M)$ which are nowhere vanishing. And hence together with the $n$ canonical/solder forms $\theta_i$ , they give $L(M)$ an absolute parallelism. I could prove that the $n$ solder forms are nowhere vanishing. My question is, what are the Kobayashi's $n^2$ connection forms on $L(M)$ ? Are they related to the Levi-Civita connection forms on $M$ ? (which can be $0$ everywhere if I choose a flat manifold?). And how do we prove that they are nowhere vanishing (if it is not direct from the definition).","Let be a Riemannian manifold, and let be the associated frame bundle. At the end of page 40 of Kobayashi's book , as I understand, it is stated that: There exsits connection forms on which are nowhere vanishing. And hence together with the canonical/solder forms , they give an absolute parallelism. I could prove that the solder forms are nowhere vanishing. My question is, what are the Kobayashi's connection forms on ? Are they related to the Levi-Civita connection forms on ? (which can be everywhere if I choose a flat manifold?). And how do we prove that they are nowhere vanishing (if it is not direct from the definition).",M L(M) n^2 \omega_j^i L(M) n \theta_i L(M) n n^2 L(M) M 0,['differential-geometry']
89,Integral of Poisson Bracket vanishes,Integral of Poisson Bracket vanishes,,"This arose when reading a text on linear PDE. It is a step in an argument that I am not sure how to justify. Let $(M, g)$ be a smooth compact Riemannian manifold and $p(x, \xi) = |\xi|_g= \sqrt{g^{ij}(x) \xi ^i \xi ^j}$ be a smooth function on $T^*M \setminus 0$ and $\omega = d\xi \wedge dx$ the canonical symplectic form. Suppose that $ b \in C^\infty (T^*M \setminus 0)$ and fix $\lambda > 0$ . Is it necessarily true that $$\int_{p < \lambda}\{p, b \} dx d\xi = 0$$ Here $\{p, b\}$ denotes the Poisson bracket and I presume that $dx d\xi = \omega ^n$ . I tried to integrate by parts but it didn't seem to work.",This arose when reading a text on linear PDE. It is a step in an argument that I am not sure how to justify. Let be a smooth compact Riemannian manifold and be a smooth function on and the canonical symplectic form. Suppose that and fix . Is it necessarily true that Here denotes the Poisson bracket and I presume that . I tried to integrate by parts but it didn't seem to work.,"(M, g) p(x, \xi) = |\xi|_g= \sqrt{g^{ij}(x) \xi ^i \xi ^j} T^*M \setminus 0 \omega = d\xi \wedge dx  b \in C^\infty (T^*M \setminus 0) \lambda > 0 \int_{p < \lambda}\{p, b \} dx d\xi = 0 \{p, b\} dx d\xi = \omega ^n","['differential-geometry', 'symplectic-geometry']"
90,$\ker \alpha$ is involutive if and only if $\alpha \wedge d\alpha = 0$,is involutive if and only if,\ker \alpha \alpha \wedge d\alpha = 0,Let $M$ be a manifold and $\alpha \in \Omega^1(M)$ is a nowhere-vanishing one-form. I have to show that $\ker \alpha$ is involutive if and only if $\alpha \wedge d\alpha = 0$ but I'm having some trouble to prove this. I found this question - Why is $\ker\omega$ integrable iff $\omega\wedge d\omega=0$? - but I'm having a hard time proving this in the case when $M$ is a manifold of an arbitrary dimension.,Let be a manifold and is a nowhere-vanishing one-form. I have to show that is involutive if and only if but I'm having some trouble to prove this. I found this question - Why is $\ker\omega$ integrable iff $\omega\wedge d\omega=0$? - but I'm having a hard time proving this in the case when is a manifold of an arbitrary dimension.,M \alpha \in \Omega^1(M) \ker \alpha \alpha \wedge d\alpha = 0 M,"['differential-geometry', 'exterior-algebra']"
91,Why is the curvature of the connection $\bar{z_1}dz_1 + \bar{z_2}dz_2$ on the Hopf fibration not exact?,Why is the curvature of the connection  on the Hopf fibration not exact?,\bar{z_1}dz_1 + \bar{z_2}dz_2,"Let $\pi : S^3 \to S^2$ be the Hopf fibration, where we take $S^3 \subset \mathbb{C}^2$ , $S^2 = \mathbb{C}\mathbb{P}^1$ , and $\pi(z_1, z_2) = [z_1 : z_2]$ . This is a principal $U(1)$ -bundle. The form $\omega = \bar{z_1}dz_1 + \bar{z_2}dz_2$ on $S^3$ defines a connection on this principal bundle. Its curvature is given by $K = d\bar{z_1}\wedge dz_1 + d\bar{z_2}\wedge dz_2$ . Since the Lie algebra of $U(1)$ is isomorphic to $\mathbb{R}$ and the adjoint representation is trivial, $K$ descends to a standard $2$ -form on $S^2$ . I want to show this gives a non-trivial element of $H^2_{dR}(S^2)$ . I have tried to compute the pullback along $\pi$ of the Fubini-Study area form on $\mathbb{C}\mathbb{P}^1$ to see whether this is proportional to $K$ , but I didn't succeed. Does someone know how to do this? (A solution using a different approach would also be appreciated.)","Let be the Hopf fibration, where we take , , and . This is a principal -bundle. The form on defines a connection on this principal bundle. Its curvature is given by . Since the Lie algebra of is isomorphic to and the adjoint representation is trivial, descends to a standard -form on . I want to show this gives a non-trivial element of . I have tried to compute the pullback along of the Fubini-Study area form on to see whether this is proportional to , but I didn't succeed. Does someone know how to do this? (A solution using a different approach would also be appreciated.)","\pi : S^3 \to S^2 S^3 \subset \mathbb{C}^2 S^2 = \mathbb{C}\mathbb{P}^1 \pi(z_1, z_2) = [z_1 : z_2] U(1) \omega = \bar{z_1}dz_1 + \bar{z_2}dz_2 S^3 K = d\bar{z_1}\wedge dz_1 + d\bar{z_2}\wedge dz_2 U(1) \mathbb{R} K 2 S^2 H^2_{dR}(S^2) \pi \mathbb{C}\mathbb{P}^1 K","['differential-geometry', 'principal-bundles', 'hopf-fibration']"
92,Geodesics in the Hyperbolic Plane,Geodesics in the Hyperbolic Plane,,"In my the class the definition given for a curve $c(t)$ to be a geodesic is that $c''(t)$ is orthogonal to the surface at the point $c(t)$ . The hyperbolic plane is the upper half plane with the metric: \begin{equation} ds^2 = \frac{1}{y^2}(dx^2 + dy^2) \end{equation} I have a fundamental misunderstanding somewhere, because I cannot think why the following curve is not a geodesic in $\mathbb{H}$ : $c(t) = (t, t)$ for $t > 0$ . This curve has second derivative as zero, but we have learnt the geodesics of $\mathbb{H}$ and this is not one of them.  Where am I reasoning this incorrectly?","In my the class the definition given for a curve to be a geodesic is that is orthogonal to the surface at the point . The hyperbolic plane is the upper half plane with the metric: I have a fundamental misunderstanding somewhere, because I cannot think why the following curve is not a geodesic in : for . This curve has second derivative as zero, but we have learnt the geodesics of and this is not one of them.  Where am I reasoning this incorrectly?","c(t) c''(t) c(t) \begin{equation}
ds^2 = \frac{1}{y^2}(dx^2 + dy^2)
\end{equation} \mathbb{H} c(t) = (t, t) t > 0 \mathbb{H}","['geometry', 'differential-geometry', 'hyperbolic-geometry', 'geodesic']"
93,Proof that divergence is independent of choice of coordinate system,Proof that divergence is independent of choice of coordinate system,,"I am attempting to prove that divergence of a smooth vector field $X$ over a $n$ -dimensional Riemannian manifold $(M,g)$ is invariant under change of coordinates (or invariant of the choice of frame). I am stuck in the proof and here is my attempt. I am starting from the formula given in [p. 33, 1]. So, we have for $$X=X^i \frac{\partial}{\partial x^i}$$ the divergence is $$ \text{div}\left( X^i \frac{\partial}{\partial x^i}\right) =  \frac{1}{\sqrt{\det g_{ij}} }\frac{\partial}{\partial x^i}\big( X^i \sqrt{\det g_{ij}}\big). $$ As a first step I am trying to prove that the RHS of that formula is the same for two different orthonormal frames. So, I assume an orthonormal  frame $\{E_i,E_2,...,E_n\}$ with dual frame $\{e^1,e^2,...,e^n\}$ . First of all in this frame we have $$X^i = e^i(X)$$ and due to orthonormality we obtain $$ \det g_{ij} = 1.$$ I shall use $\text{div}(X)_{Ee}$ to emphasize the dependence on the frame. So we obtain $$\text{div}(X)_{Ee} = E_i(e^i(X))$$ Now I consider another orthonormal frame $\{F_i,F_2,...,F_n\}$ with dual frame $\{f^1,f^2,...,f^n\}$ .  For this frame we have $$\text{div}(X)_{Ff} = F_i(f^i(X))$$ Now, to prove invariance under the two frames, I need to prove $$\text{div}(X)_{Ff} = \text{div}(X)_{Ee}$$ I expand the second frame in terms of the first as $$\begin{eqnarray} F_i &=& F_i^l E_l \\ f^i &=& f^i_m e^m\end{eqnarray}$$ Then we have $$\begin{eqnarray}  \text{div}(X)_{Ff} &=& F_i(f^i(X))  \nonumber \\ &=& F_i^lE_l(f^i_m e^m(X)) \nonumber \\ &=& F_i^lE_l(f^i_m) e^m(X) + F_i^lf^i_m E_l(e^m(X))   \\ &=& F_i^lE_l(f^i_m) e^m(X) + \delta^l_m E_l(e^m(X)) \\ &=& F_i^lE_l(f^i_m) e^m(X) + \text{div}(X)_{Ee} \end{eqnarray}$$ Where the third step is from Leibniz's rule and the fourth step is because the matrices $[f_m^i]$ and $[F_i^l]$ are inverses of each other. So essentially I need to prove $$F_i^lE_l(f^i_m) e^m(X) = 0$$ and I am not able to do this, can someone help? [1] Lee, John M. Introduction to Riemannian manifolds. Vol. 176. Springer, 2018. P.S. I did go through Divergence of a smooth vector field and that answer uses the covariant derivative. I am attempting to avoid using it.","I am attempting to prove that divergence of a smooth vector field over a -dimensional Riemannian manifold is invariant under change of coordinates (or invariant of the choice of frame). I am stuck in the proof and here is my attempt. I am starting from the formula given in [p. 33, 1]. So, we have for the divergence is As a first step I am trying to prove that the RHS of that formula is the same for two different orthonormal frames. So, I assume an orthonormal  frame with dual frame . First of all in this frame we have and due to orthonormality we obtain I shall use to emphasize the dependence on the frame. So we obtain Now I consider another orthonormal frame with dual frame .  For this frame we have Now, to prove invariance under the two frames, I need to prove I expand the second frame in terms of the first as Then we have Where the third step is from Leibniz's rule and the fourth step is because the matrices and are inverses of each other. So essentially I need to prove and I am not able to do this, can someone help? [1] Lee, John M. Introduction to Riemannian manifolds. Vol. 176. Springer, 2018. P.S. I did go through Divergence of a smooth vector field and that answer uses the covariant derivative. I am attempting to avoid using it.","X n (M,g) X=X^i \frac{\partial}{\partial x^i} 
\text{div}\left( X^i \frac{\partial}{\partial x^i}\right)
= 
\frac{1}{\sqrt{\det g_{ij}} }\frac{\partial}{\partial x^i}\big( X^i \sqrt{\det g_{ij}}\big).
 \{E_i,E_2,...,E_n\} \{e^1,e^2,...,e^n\} X^i = e^i(X)  \det g_{ij} = 1. \text{div}(X)_{Ee} \text{div}(X)_{Ee} = E_i(e^i(X)) \{F_i,F_2,...,F_n\} \{f^1,f^2,...,f^n\} \text{div}(X)_{Ff} = F_i(f^i(X)) \text{div}(X)_{Ff} = \text{div}(X)_{Ee} \begin{eqnarray} F_i &=& F_i^l E_l \\
f^i &=& f^i_m e^m\end{eqnarray} \begin{eqnarray} 
\text{div}(X)_{Ff} &=& F_i(f^i(X))  \nonumber \\
&=& F_i^lE_l(f^i_m e^m(X)) \nonumber \\
&=& F_i^lE_l(f^i_m) e^m(X) + F_i^lf^i_m E_l(e^m(X))   \\
&=& F_i^lE_l(f^i_m) e^m(X) + \delta^l_m E_l(e^m(X)) \\
&=& F_i^lE_l(f^i_m) e^m(X) + \text{div}(X)_{Ee}
\end{eqnarray} [f_m^i] [F_i^l] F_i^lE_l(f^i_m) e^m(X) = 0","['differential-geometry', 'riemannian-geometry', 'smooth-manifolds', 'divergence-operator']"
94,Smooth function $y$ that satisfies $dy|_p=w$ for covector $w$.,Smooth function  that satisfies  for covector .,y dy|_p=w w,"If $M$ is a smooth manifold, $w\in T^*_pM$ , then is it possible to find a smooth function $y:M\to \mathbb{R}$ such that $dy|_p=w$ . If it is, is there an easy way to give $y$ explicitly? I have just learned about covectors and I am not really understanding them overly well.","If is a smooth manifold, , then is it possible to find a smooth function such that . If it is, is there an easy way to give explicitly? I have just learned about covectors and I am not really understanding them overly well.",M w\in T^*_pM y:M\to \mathbb{R} dy|_p=w y,"['differential-geometry', 'differential-topology', 'co-tangent-space']"
95,On the definition of a connection 1-form,On the definition of a connection 1-form,,"Let $P\to M$ be a principal $G$ -bundle. I'm reading Mathematical Gauge Theory by Mark Hamilton and he defines a connection $1$ -form as $A\in \Omega^1(P,\mathfrak{g})$ satisfying (i) $({\rm R}_g)^*A = {\rm Ad}_{g^{-1}}\circ A$ for all $g\in G$ and (ii) $A(X^\#)=X$ for all $X\in \mathfrak{g}$ . Here ${\rm R}_g(p)=pg$ is the action map of $g$ and $X^\#\in \mathfrak{X}(P)$ is the action field of $X$ . While condition (ii) seems very natural to me and I can sort of convince myself of needing $g^{-1}$ instead of $g$ on the right side of (i) (maybe because $G$ acts on $P$ by the right?), I'm not sure how to understand condition (i). What does it mean?","Let be a principal -bundle. I'm reading Mathematical Gauge Theory by Mark Hamilton and he defines a connection -form as satisfying (i) for all and (ii) for all . Here is the action map of and is the action field of . While condition (ii) seems very natural to me and I can sort of convince myself of needing instead of on the right side of (i) (maybe because acts on by the right?), I'm not sure how to understand condition (i). What does it mean?","P\to M G 1 A\in \Omega^1(P,\mathfrak{g}) ({\rm R}_g)^*A = {\rm Ad}_{g^{-1}}\circ A g\in G A(X^\#)=X X\in \mathfrak{g} {\rm R}_g(p)=pg g X^\#\in \mathfrak{X}(P) X g^{-1} g G P","['differential-geometry', 'mathematical-physics', 'principal-bundles', 'gauge-theory']"
96,"Lie group homomorphism induces Lie algebra homomorphism, proof with Ad and ad","Lie group homomorphism induces Lie algebra homomorphism, proof with Ad and ad",,"I want to show that if $\phi:G\to H$ is a Lie group homomorphism, then $d\phi:\mathfrak{g}\to\mathfrak{h}$ is a Lie algebra homomorphism. The proof given in the classroom is the following: let $x \in G$ . Since $\phi$ is a Lie group homomorphism, we have that $$\phi(xyx^{-1}) = \phi(x) \phi(y) \phi(x)^{-1} \tag{$1$}$$ for all $y \in G$ . Differentiating $(\ast)$ with respect to $y$ at $y = 1$ in the direction of $Y \in \mathfrak{g}$ gives us $$d\phi(\mathrm{Ad}(x) Y) = \mathrm{Ad}(\phi(x)) d\phi(Y). \tag{$2$}$$ Differentiating $(2)$ with respect to $x$ at $x = 1$ in the direction of $X \in \mathfrak{g}$ , we obtain $$d\phi(\mathrm{ad}(X) Y) = \mathrm{ad}(d\phi(X)) d\phi(Y) \tag{3}$$ and so: $$d\phi([X, Y]_{\mathfrak{g}}) = [d\phi(X), d\phi(Y)]_{\mathfrak{h}}. \tag{4}$$ $(1)\implies(2)$ , because, if $C_z:G\to G$ is $C_z(g)=zgz^{-1}$ , (i use same notation for $C_z:H\to H$ if $z\in H$ ), (1) is: $\phi \circ C_x=C_\phi(x)\circ\phi$ , differentiating wrt $e\in G$ , $e=1_G$ , and evaluated in $Y\in\mathfrak{g}$ , i have (2). $(3)\implies (4)$ is obvious. But i can't prove that $(2)\implies (3)$ . Can someone show me the calculation for $(2)\implies(3)$ please? Any help is appreciated and sorry for the bad English.","I want to show that if is a Lie group homomorphism, then is a Lie algebra homomorphism. The proof given in the classroom is the following: let . Since is a Lie group homomorphism, we have that for all . Differentiating with respect to at in the direction of gives us Differentiating with respect to at in the direction of , we obtain and so: , because, if is , (i use same notation for if ), (1) is: , differentiating wrt , , and evaluated in , i have (2). is obvious. But i can't prove that . Can someone show me the calculation for please? Any help is appreciated and sorry for the bad English.","\phi:G\to H d\phi:\mathfrak{g}\to\mathfrak{h} x \in G \phi \phi(xyx^{-1}) = \phi(x) \phi(y) \phi(x)^{-1} \tag{1} y \in G (\ast) y y = 1 Y \in \mathfrak{g} d\phi(\mathrm{Ad}(x) Y) = \mathrm{Ad}(\phi(x)) d\phi(Y). \tag{2} (2) x x = 1 X \in \mathfrak{g} d\phi(\mathrm{ad}(X) Y) = \mathrm{ad}(d\phi(X)) d\phi(Y) \tag{3} d\phi([X, Y]_{\mathfrak{g}}) = [d\phi(X), d\phi(Y)]_{\mathfrak{h}}. \tag{4} (1)\implies(2) C_z:G\to G C_z(g)=zgz^{-1} C_z:H\to H z\in H \phi \circ C_x=C_\phi(x)\circ\phi e\in G e=1_G Y\in\mathfrak{g} (3)\implies (4) (2)\implies (3) (2)\implies(3)","['differential-geometry', 'lie-groups', 'lie-algebras']"
97,How to prove that Christoffel symbols are not components of a tensor,How to prove that Christoffel symbols are not components of a tensor,,"I know how to prove it by the fact they don't respect the usual change of coordinates, but I want to prove it using that a tensor must be $\mathcal{C}^\infty$ -multilinear in all its components.  Here is my wrong proof, someone can help me finding the mistake? Call $\Gamma:=\Gamma_{ij}^kdx^idx^j\frac{\partial}{\partial x^k}$ , then I want to prove that $\Gamma$ is not $\mathcal{C}^\infty$ -multilinear, i.e. if $X,Y\in\mathcal{T}(M)$ , $w\in\mathcal{T}^*(M)$ and $f,g,h\in\mathcal{C}^\infty(M)$ , then it doesn't hold that $\Gamma(fX,gY,hw)=fgh\Gamma(X,Y,w)$ . But we have: $\Gamma(fX,gY,hw)=\Gamma_{ij}^kdx^i(fX^s\frac{\partial}{\partial x^s})dx^j(gY^m\frac{\partial}{\partial x^m})\frac{\partial}{\partial x^k}(hw_tdx^t)=fgh\Gamma_{ij}^kdx^i(X^s\frac{\partial}{\partial x^s})dx^j(Y^m\frac{\partial}{\partial x^m})\frac{\partial}{\partial x^k}(w_tdx^t)=fgh\Gamma(X,Y,X)$ So where's the mistake? Thank you all.","I know how to prove it by the fact they don't respect the usual change of coordinates, but I want to prove it using that a tensor must be -multilinear in all its components.  Here is my wrong proof, someone can help me finding the mistake? Call , then I want to prove that is not -multilinear, i.e. if , and , then it doesn't hold that . But we have: So where's the mistake? Thank you all.","\mathcal{C}^\infty \Gamma:=\Gamma_{ij}^kdx^idx^j\frac{\partial}{\partial x^k} \Gamma \mathcal{C}^\infty X,Y\in\mathcal{T}(M) w\in\mathcal{T}^*(M) f,g,h\in\mathcal{C}^\infty(M) \Gamma(fX,gY,hw)=fgh\Gamma(X,Y,w) \Gamma(fX,gY,hw)=\Gamma_{ij}^kdx^i(fX^s\frac{\partial}{\partial x^s})dx^j(gY^m\frac{\partial}{\partial x^m})\frac{\partial}{\partial x^k}(hw_tdx^t)=fgh\Gamma_{ij}^kdx^i(X^s\frac{\partial}{\partial x^s})dx^j(Y^m\frac{\partial}{\partial x^m})\frac{\partial}{\partial x^k}(w_tdx^t)=fgh\Gamma(X,Y,X)","['differential-geometry', 'differential']"
98,Interior product of a exterior derivative in jet bundles,Interior product of a exterior derivative in jet bundles,,"Let $(E,\pi,M)$ be a bundle which coordinates is given by $(x^i,u^\alpha)$ and $J^1\pi$ the first jet associated with this bundle which coordinates is given by $(x^i,u^\alpha,u^\alpha_i)$ Given a vector  vector field $\xi=\xi^i\frac{\partial }{\partial x^i}+\xi^\alpha\frac{\partial }{\partial u^\alpha}$ on $E$ its , its first lift is given by $$\xi^1=\xi^i\frac{\partial }{\partial x^i}+\xi^\alpha\frac{\partial }{\partial u^\alpha}+(\frac{d \xi^\alpha }{d x_i}-u^\alpha _j\frac{d \xi^j }{d x_i})\frac{\partial }{\partial u^\alpha _i}$$ and Lagrangian density on $\pi$ is a function $L \in C^\infty(J^1\pi)$ .  The corresponding Lagrangian is the m-form $\mathcal{L}=L\Omega \in \Lambda^{m}_0 \pi_1 $ where $\Lambda^{m}_0 \pi_1 $ is the space of  m-forms on $J^1\pi$ horizontal over $M$ . Now in this paper https://www.icmat.es/Thesis/CMartinezCampos.pdf on page 56 it has $$i_{\xi^1}d\mathcal{L}=\xi^1(L)d^mx-d\mathcal{L}\wedge i_{\xi^1}d^mx \tag{1}$$ where $i_{\xi^1}$ denotes the interior product and $d$ de exterior derivative. I am not seeing why this formula is correct. For example if we have this bundle $(R^2,\pi,R)$ and he first jet $J^1\pi$ associated with this bundle which coordinates is given by $(x,y,z)$ then $\mathcal{L}$ would be of the form $$\mathcal{L}=Ldx$$ and $$\xi^1=\xi^x\frac{\partial }{\partial x}+\xi^y\frac{\partial }{\partial y}+\xi^z\frac{\partial }{\partial z}$$ and so $$i_{\xi^1}d\mathcal{L}=i_{\xi^1}d(Ldx)=i_{\xi^1}\Bigg(\frac{\partial L}{\partial y}dy\wedge dx+\frac{\partial L}{\partial z}dz\wedge dx\Bigg)=$$ $$=\frac{\partial L}{\partial y}dy(\xi^1)\wedge dx-\frac{\partial L}{\partial y}dy\wedge dx(\xi^1)+\frac{\partial L}{\partial z}dz(\xi^1)\wedge dx)-\frac{\partial L}{\partial z}dz\wedge dx(\xi^1)$$ $$=\xi^y\frac{\partial L}{\partial y}dx-\xi^x\frac{\partial L}{\partial y}dy+\xi^z\frac{\partial L}{\partial z}dx-\xi^x\frac{\partial L}{\partial z}dz\tag{2}$$ Can anyone explain me how expression $(1)$ and $(2)$ are related","Let be a bundle which coordinates is given by and the first jet associated with this bundle which coordinates is given by Given a vector  vector field on its , its first lift is given by and Lagrangian density on is a function .  The corresponding Lagrangian is the m-form where is the space of  m-forms on horizontal over . Now in this paper https://www.icmat.es/Thesis/CMartinezCampos.pdf on page 56 it has where denotes the interior product and de exterior derivative. I am not seeing why this formula is correct. For example if we have this bundle and he first jet associated with this bundle which coordinates is given by then would be of the form and and so Can anyone explain me how expression and are related","(E,\pi,M) (x^i,u^\alpha) J^1\pi (x^i,u^\alpha,u^\alpha_i) \xi=\xi^i\frac{\partial }{\partial x^i}+\xi^\alpha\frac{\partial }{\partial u^\alpha} E \xi^1=\xi^i\frac{\partial }{\partial x^i}+\xi^\alpha\frac{\partial }{\partial u^\alpha}+(\frac{d \xi^\alpha }{d x_i}-u^\alpha _j\frac{d \xi^j }{d x_i})\frac{\partial }{\partial u^\alpha _i} \pi L \in C^\infty(J^1\pi) \mathcal{L}=L\Omega \in \Lambda^{m}_0 \pi_1  \Lambda^{m}_0 \pi_1  J^1\pi M i_{\xi^1}d\mathcal{L}=\xi^1(L)d^mx-d\mathcal{L}\wedge i_{\xi^1}d^mx \tag{1} i_{\xi^1} d (R^2,\pi,R) J^1\pi (x,y,z) \mathcal{L} \mathcal{L}=Ldx \xi^1=\xi^x\frac{\partial }{\partial x}+\xi^y\frac{\partial }{\partial y}+\xi^z\frac{\partial }{\partial z} i_{\xi^1}d\mathcal{L}=i_{\xi^1}d(Ldx)=i_{\xi^1}\Bigg(\frac{\partial L}{\partial y}dy\wedge dx+\frac{\partial L}{\partial z}dz\wedge dx\Bigg)= =\frac{\partial L}{\partial y}dy(\xi^1)\wedge dx-\frac{\partial L}{\partial y}dy\wedge dx(\xi^1)+\frac{\partial L}{\partial z}dz(\xi^1)\wedge dx)-\frac{\partial L}{\partial z}dz\wedge dx(\xi^1) =\xi^y\frac{\partial L}{\partial y}dx-\xi^x\frac{\partial L}{\partial y}dy+\xi^z\frac{\partial L}{\partial z}dx-\xi^x\frac{\partial L}{\partial z}dz\tag{2} (1) (2)","['differential-geometry', 'jet-bundles']"
99,Examples of simple non-parallelizable smooth manifolds,Examples of simple non-parallelizable smooth manifolds,,"I'm looking for examples of simple non-parallelizable smooth manifolds and honestly just general insight into the concept of a manifold being parallelizable. $S^2$ would be parallelizable, right? At each point on $S^2$ , you can have two vectors meet orthogonally that are tangent to $S^2$ , and would thus span the tangent space at that point. It seems to me that if you have an $n$ dimensional smooth manifold, that if you can find $n$ linearly independent vector fields, (so that at each point on the manifold, $V_1(x),....,V_n(x)$ are linearly independent)$, Then your manifold would be parallelizable. General insight and comments greatly appreciated!","I'm looking for examples of simple non-parallelizable smooth manifolds and honestly just general insight into the concept of a manifold being parallelizable. would be parallelizable, right? At each point on , you can have two vectors meet orthogonally that are tangent to , and would thus span the tangent space at that point. It seems to me that if you have an dimensional smooth manifold, that if you can find linearly independent vector fields, (so that at each point on the manifold, are linearly independent)$, Then your manifold would be parallelizable. General insight and comments greatly appreciated!","S^2 S^2 S^2 n n V_1(x),....,V_n(x)","['differential-geometry', 'soft-question']"
