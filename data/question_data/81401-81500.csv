,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,"Show that $A=(a_{ij})_{1\le i,j\le5}$ is positive definite, where $a_{ij}=\frac1{n_i+n_j+1}$","Show that  is positive definite, where","A=(a_{ij})_{1\le i,j\le5} a_{ij}=\frac1{n_i+n_j+1}","Problem Let $A=(a_{ij})_{1\le i,j\le5}$ be a matrix such that $$a_{ij}=\frac1{n_i+n_j+1}\,,$$ where $n_i,n_j\in\Bbb{N}$ . Show that $A$ is positive definite if $n_1>n_2>n_3>n_4>n_5$ or, $n_1<n_2<n_3<n_4<n_5$ . As far as my knowledge is concerned, I have following two equivalent criteria for a matrix $A$ to be positive definite. all eigenvalues are positive. there is a matrix $B$ such that $A=B^tB$ . But nothing is helping me here. Any hint how to solve this!! Thank you.","Problem Let be a matrix such that where . Show that is positive definite if or, . As far as my knowledge is concerned, I have following two equivalent criteria for a matrix to be positive definite. all eigenvalues are positive. there is a matrix such that . But nothing is helping me here. Any hint how to solve this!! Thank you.","A=(a_{ij})_{1\le i,j\le5} a_{ij}=\frac1{n_i+n_j+1}\,, n_i,n_j\in\Bbb{N} A n_1>n_2>n_3>n_4>n_5 n_1<n_2<n_3<n_4<n_5 A B A=B^tB",['matrices']
1,An expression subtly different from the one in Sherman-Morrison formula,An expression subtly different from the one in Sherman-Morrison formula,,"Let $0\ne x\in\mathbb{R},\,\mathbf{y} \in \mathbb{R}^{n},\,\mathbf{1}=(1,1, \cdots, 1)^{T} \in \mathbb{R}^{n}$ and assume that $\Sigma=x I_{n}+\mathbf{y} \mathbf{1}^{T}+\mathbf{1} \mathbf{y}^{T}$ is positive definite. Prove that $$ \Sigma^{-1}-\frac{\Sigma^{-1} \mathbf{1 1}^{T} \Sigma^{-1}}{\mathbf{1}^{T} \Sigma^{-1} \mathbf{1}}=\frac{1}{x}\left(I_{n}-\frac{1}{n} \mathbf{1 1}^{T}\right). $$",Let and assume that is positive definite. Prove that,"0\ne x\in\mathbb{R},\,\mathbf{y} \in \mathbb{R}^{n},\,\mathbf{1}=(1,1, \cdots, 1)^{T} \in \mathbb{R}^{n} \Sigma=x I_{n}+\mathbf{y} \mathbf{1}^{T}+\mathbf{1} \mathbf{y}^{T} 
\Sigma^{-1}-\frac{\Sigma^{-1} \mathbf{1 1}^{T} \Sigma^{-1}}{\mathbf{1}^{T} \Sigma^{-1} \mathbf{1}}=\frac{1}{x}\left(I_{n}-\frac{1}{n} \mathbf{1 1}^{T}\right).
","['linear-algebra', 'matrices', 'positive-definite', 'symmetric-matrices']"
2,Does $\sum_i A_i=I$ with $A_i$ positive imply $\{A_i\}_i$ are mutually diagonalisable?,Does  with  positive imply  are mutually diagonalisable?,\sum_i A_i=I A_i \{A_i\}_i,"As discussed in this other question , if $A$ and $B$ are matrices such that $A+B=I$ , then trivially they commute, and thus if they are both diagonalisable they are also mutually diagonalisable. The same argument doesn't, however, apply when summing more than two such matrices. Suppose then that $$\sum_{i=1}^n A_i = I.$$ The case of $A_i\ge0$ is the one I'm most interested about, but if positivity turns out to not be relevant for this, as it might very well be the case, feel free to weaken this constraint (to maybe consider Hermitian, normal, or just diagonalisable matrices). If $\sum_i A_i=I$ then I can say that, for example, $[A_1,A_2+...+A_n]=0$ , and thus $A_1$ and $\sum_{i>1} A_i$ are mutually diagonalisable. But then I cannot iterate the argument by splitting $A_2$ from $A_3+...+A_n$ , as now they sum to a diagonal matrix (in their common eigenbasis), but not to the identity. So does the result about mutual diagonalisability only work for $n=2$ ? A counterexample of three or more non-mutually-diagonalisable matrices summing to the identity would be a good answer.","As discussed in this other question , if and are matrices such that , then trivially they commute, and thus if they are both diagonalisable they are also mutually diagonalisable. The same argument doesn't, however, apply when summing more than two such matrices. Suppose then that The case of is the one I'm most interested about, but if positivity turns out to not be relevant for this, as it might very well be the case, feel free to weaken this constraint (to maybe consider Hermitian, normal, or just diagonalisable matrices). If then I can say that, for example, , and thus and are mutually diagonalisable. But then I cannot iterate the argument by splitting from , as now they sum to a diagonal matrix (in their common eigenbasis), but not to the identity. So does the result about mutual diagonalisability only work for ? A counterexample of three or more non-mutually-diagonalisable matrices summing to the identity would be a good answer.","A B A+B=I \sum_{i=1}^n A_i = I. A_i\ge0 \sum_i A_i=I [A_1,A_2+...+A_n]=0 A_1 \sum_{i>1} A_i A_2 A_3+...+A_n n=2","['linear-algebra', 'matrices', 'eigenvalues-eigenvectors', 'positive-definite']"
3,Why supremum not max in definition of matrix norm?,Why supremum not max in definition of matrix norm?,,"I know the definitions of max and sup. But, I don't see why would we use sup in this definition for example, rather than max. This is definition of vector-induced norm of a matrix.","I know the definitions of max and sup. But, I don't see why would we use sup in this definition for example, rather than max. This is definition of vector-induced norm of a matrix.",,"['linear-algebra', 'matrices', 'vector-spaces', 'inner-products']"
4,Derivative of Frobenius norm of tensor product in component-free notation,Derivative of Frobenius norm of tensor product in component-free notation,,"I need to take a derivate with respect to $w$ of the following function: $$ f(x,w)=\left\lVert x  \otimes w\right\rVert _{F}, $$ where $x \in \mathbb{R^{n}}$ , $w \in \mathbb{R^{n}}$ , $\otimes$ is a tensor (outer) product and $\left\lVert\right\rVert _{F}$ - Frobenius norm . I managed to do this calculation in index notation (i.e. in coordinates): $\frac{\partial f}{\partial w_k}= \frac{\partial(\left\lVert x  \otimes w\right\rVert _{F})}{\partial(x \otimes w)_{ij}} \cdot \frac{\partial(x \otimes w)_{ij}}{\partial w_k},$ where Einstein summation convention has been used. $\frac{\partial(\left\lVert x  \otimes w\right\rVert _{F})}{\partial(x \otimes w)_{ij}} = \frac{1}{\left(\sum_{mn} |(x \otimes w)_{mn}|^2\right)^{1/2}}\cdot (x \otimes w)_{ij}$ $\frac{\partial(x \otimes w)_{ij}}{\partial w_k}= \frac{\partial(x_i w_j)}{\partial w_k} = x_i \delta_{jk},$ where $\delta_{jk}$ is Kronecker delta . All together combined: $$ \frac{\partial f}{\partial w_k}= \frac{1}{\left(\sum_{mn} |a_{mn}|^2\right)^{1/2}}\cdot a_{ij}\cdot x_i \delta_{jk},\quad \mbox{with} \quad a_{ij} \equiv(x \otimes w)_{ij}=x_iw_j. $$ We are left with a vector since there is summation over $i$ and $j$ . On the other hand the last term in product has three indices so it corresponds to a tensor in non index notation. Could someone do this calculation in component-free notation (i.e. without using coordinates = index notation)?","I need to take a derivate with respect to of the following function: where , , is a tensor (outer) product and - Frobenius norm . I managed to do this calculation in index notation (i.e. in coordinates): where Einstein summation convention has been used. where is Kronecker delta . All together combined: We are left with a vector since there is summation over and . On the other hand the last term in product has three indices so it corresponds to a tensor in non index notation. Could someone do this calculation in component-free notation (i.e. without using coordinates = index notation)?","w 
f(x,w)=\left\lVert x  \otimes w\right\rVert _{F},
 x \in \mathbb{R^{n}} w \in \mathbb{R^{n}} \otimes \left\lVert\right\rVert _{F} \frac{\partial f}{\partial w_k}= \frac{\partial(\left\lVert x  \otimes w\right\rVert _{F})}{\partial(x \otimes w)_{ij}} \cdot \frac{\partial(x \otimes w)_{ij}}{\partial w_k}, \frac{\partial(\left\lVert x  \otimes w\right\rVert _{F})}{\partial(x \otimes w)_{ij}} = \frac{1}{\left(\sum_{mn} |(x \otimes w)_{mn}|^2\right)^{1/2}}\cdot (x \otimes w)_{ij} \frac{\partial(x \otimes w)_{ij}}{\partial w_k}= \frac{\partial(x_i w_j)}{\partial w_k} = x_i \delta_{jk}, \delta_{jk} 
\frac{\partial f}{\partial w_k}= \frac{1}{\left(\sum_{mn} |a_{mn}|^2\right)^{1/2}}\cdot a_{ij}\cdot x_i \delta_{jk},\quad \mbox{with} \quad a_{ij} \equiv(x \otimes w)_{ij}=x_iw_j.
 i j","['linear-algebra', 'matrices', 'tensor-products', 'matrix-calculus']"
5,matrices with huge numbers as components,matrices with huge numbers as components,,"If $A=\begin{bmatrix}  10^{30}+5& 10^{20}+4 &10^{20}+6 \\  10^{4}+2 & 10^{8}+7 &10^{10}+2n \\   10^{4}+8&10^{6}+4  &10^{15}+9  \end{bmatrix}$ for all $n\in \mathbb{N},$ Then $(a)\;\;A$ is invertible for all $n\in \mathbb{N}$ $(b)\;\;A$ is not  invertible for all $n\in \mathbb{N}$ $(c)\;\;A$ may or may not be invertible depending on the values of $n\in \mathbb{N}$ $(d)\;$ Data Insufficient What I try If $A$ is invertiable, then $\det(A)\neq 0$ $$A=\begin{vmatrix}  10^{30}+5& 10^{20}+4 &10^{20}+6 \\  10^{4}+2 & 10^{8}+7 &10^{10}+2n \\   10^{4}+8&10^{6}+4  &10^{15}+9  \end{vmatrix}$$ Expanding along $1^\mathrm{st}$ row $\displaystyle A=\bigg(10^{30}+5\bigg)\bigg[\bigg(10^8+7\bigg)\bigg(10^{15}+9\bigg)-\bigg(10^6+4\bigg)\bigg(10^{10}+2n\bigg)\bigg]-\bigg(10^{20}+4\bigg)\bigg[\bigg(10)^4+2\bigg)\bigg(10^{15}+9\bigg)-\bigg(10^{10}+2n\bigg)\bigg(10^8+4\bigg)\bigg]+\bigg(10^{20}+6\bigg)\bigg[\bigg(10^4+2\bigg)\bigg(10^6+4\bigg)-\bigg(10^8+7\bigg)\bigg(10^4+8\bigg)\bigg]$ How do I simplify such a huge calculation? Please help me.","If for all Then is invertible for all is not  invertible for all may or may not be invertible depending on the values of Data Insufficient What I try If is invertiable, then Expanding along row How do I simplify such a huge calculation? Please help me.","A=\begin{bmatrix}
 10^{30}+5& 10^{20}+4 &10^{20}+6 \\ 
10^{4}+2 & 10^{8}+7 &10^{10}+2n \\ 
 10^{4}+8&10^{6}+4  &10^{15}+9 
\end{bmatrix} n\in \mathbb{N}, (a)\;\;A n\in \mathbb{N} (b)\;\;A n\in \mathbb{N} (c)\;\;A n\in \mathbb{N} (d)\; A \det(A)\neq 0 A=\begin{vmatrix}
 10^{30}+5& 10^{20}+4 &10^{20}+6 \\ 
10^{4}+2 & 10^{8}+7 &10^{10}+2n \\ 
 10^{4}+8&10^{6}+4  &10^{15}+9 
\end{vmatrix} 1^\mathrm{st} \displaystyle A=\bigg(10^{30}+5\bigg)\bigg[\bigg(10^8+7\bigg)\bigg(10^{15}+9\bigg)-\bigg(10^6+4\bigg)\bigg(10^{10}+2n\bigg)\bigg]-\bigg(10^{20}+4\bigg)\bigg[\bigg(10)^4+2\bigg)\bigg(10^{15}+9\bigg)-\bigg(10^{10}+2n\bigg)\bigg(10^8+4\bigg)\bigg]+\bigg(10^{20}+6\bigg)\bigg[\bigg(10^4+2\bigg)\bigg(10^6+4\bigg)-\bigg(10^8+7\bigg)\bigg(10^4+8\bigg)\bigg]",['matrices']
6,A diagonalizable matrix's proof,A diagonalizable matrix's proof,,"I came a cross the following question today at class If a matrix has $n$ eigenvalues and it is known all of them are different from each other then $A$ is a diagonalizable matrix to best of my knowledge, if the eigenvalues are different from each other (and let's say you got $n$ of those), you can make $n$ bases vectors using those eigenvalues, but I'd love to see a proper proof :) cheers","I came a cross the following question today at class If a matrix has eigenvalues and it is known all of them are different from each other then is a diagonalizable matrix to best of my knowledge, if the eigenvalues are different from each other (and let's say you got of those), you can make bases vectors using those eigenvalues, but I'd love to see a proper proof :) cheers",n A n n,"['matrices', 'eigenvalues-eigenvectors', 'diagonalization']"
7,Diagonalization of very large (but very simple) sparse matrix,Diagonalization of very large (but very simple) sparse matrix,,"I have a $10^5 \times 10^5$ matrix and I need its smallest eigenvalue (not the the smallest in absolute value, but actually the lowest) and the associated eigenvector (I know the eigenvalue to be non-degenerate). The matrix is huge, but it has several nice properties: It is symmetric. The density is extremely low: the proportion of non-zero entries is much less than $0.1\%$ . In each row there are only (maximum) $15$ non-zero entries. There are only $10$ different values among the entries. I'd like to find a very efficient way to compute the eigenvalue and the eigenvector. Standard diagonalization techniques are too costful, and even Lanczos algorithm is not entirely useful in this case.","I have a matrix and I need its smallest eigenvalue (not the the smallest in absolute value, but actually the lowest) and the associated eigenvector (I know the eigenvalue to be non-degenerate). The matrix is huge, but it has several nice properties: It is symmetric. The density is extremely low: the proportion of non-zero entries is much less than . In each row there are only (maximum) non-zero entries. There are only different values among the entries. I'd like to find a very efficient way to compute the eigenvalue and the eigenvector. Standard diagonalization techniques are too costful, and even Lanczos algorithm is not entirely useful in this case.",10^5 \times 10^5 0.1\% 15 10,"['linear-algebra', 'matrices', 'numerical-linear-algebra', 'diagonalization', 'sparse-matrices']"
8,What is the derivative of kernel function?,What is the derivative of kernel function?,,"The heat kernel function is as follow： $$f(x_i,x_j)= \exp\left(-\frac{||x_i-x_j||^2_2}{\sigma}\right)$$ where $x_i$ and $x_j$ are two column vectors of matrix $X$ . $\sigma$ is a nonzero constant. What is the derivative of kernel function? What about the second partial derivative? ################################################### In addition, we know that $L=D-S$ , where $L$ is a Laplacian matrix. $D$ is a degree matrix. $S$ is an affinity matrix. Here, $S$ can be composed by the above $f(x_i,x_j)$ . Therefore, what is the derivative of $Tr(QLQ^T)$ with respect to $X$ ? where $Q \in \mathbb{R}^{d\times n}$ is a constant matrix, $L \in \mathbb{R}^{n\times n}$ is a variable that is related to $X$ . Nobody?","The heat kernel function is as follow： where and are two column vectors of matrix . is a nonzero constant. What is the derivative of kernel function? What about the second partial derivative? ################################################### In addition, we know that , where is a Laplacian matrix. is a degree matrix. is an affinity matrix. Here, can be composed by the above . Therefore, what is the derivative of with respect to ? where is a constant matrix, is a variable that is related to . Nobody?","f(x_i,x_j)= \exp\left(-\frac{||x_i-x_j||^2_2}{\sigma}\right) x_i x_j X \sigma L=D-S L D S S f(x_i,x_j) Tr(QLQ^T) X Q \in \mathbb{R}^{d\times n} L \in \mathbb{R}^{n\times n} X","['real-analysis', 'matrices', 'derivatives', 'graph-theory', 'normed-spaces']"
9,Show that following determinant is divisible by $\lambda^2$ and find the other factor.,Show that following determinant is divisible by  and find the other factor.,\lambda^2,"Show that $\begin{vmatrix} a^2+\lambda &ab &ac \\  ab & b^2+\lambda & bc \\ ac & bc & c^2+\lambda \end{vmatrix}=0$ is divisible by $\lambda^2$ and find the other factor. My attempt is as follows:- $$R_1\rightarrow R_1+R_2+R_3$$ $$\begin{vmatrix} a(a+b+c)+\lambda &b(a+b+c)+\lambda &c(a+b+c)+\lambda \\  ab & b^2+\lambda & bc \\ ac & bc & c^2+\lambda \end{vmatrix}=0$$ $$C_1\rightarrow C_1-\dfrac{a}{b}C_2$$ $$C_2\rightarrow C_2-\dfrac{b}{c}C_3$$ $$\begin{vmatrix} \lambda-\dfrac{a\lambda}{b}&\lambda-\dfrac{b\lambda}{c} &c(a+b+c)+\lambda \\  -\lambda & \lambda & bc \\ 0 & -\lambda & c^2+\lambda \end{vmatrix}=0$$ Taking $\lambda^2$ common $$\lambda^2\begin{vmatrix} 1-\dfrac{a}{b}&1-\dfrac{b}{c} &c(a+b+c)+\lambda \\  -1 & 1 & bc \\ 0 & -1 & c^2+\lambda \end{vmatrix}=0 $$ $$\dfrac{\lambda^2}{bc}\begin{vmatrix} b-a&c-b &c(a+b+c)+\lambda \\  -b & c & bc \\ 0 & -c & c^2+\lambda \end{vmatrix}=0 $$ $$R_1\rightarrow R_1-R_3$$ $$\dfrac{\lambda^2}{bc}\begin{vmatrix} b-a&2c-b &ca+bc \\  -b & c & bc \\ 0 & -c & c^2+\lambda \end{vmatrix}=0$$ $$R_1\rightarrow R_1-R_2$$ $$\dfrac{\lambda^2}{bc}\begin{vmatrix} 2b-a&c-b &ca \\  -b & c & bc \\ 0 & -c & c^2+\lambda \end{vmatrix}=0$$ Now expanding it $$\dfrac{\lambda^2}{bc}\left(c(2b^2c-abc+abc)+(c^2+\lambda)(2bc-ac+bc-b^2)\right)=0$$ $$\dfrac{\lambda^2}{bc}\left(2b^2c^2+(c^2+\lambda)(3bc-ac-b^2)\right)=0$$ $$\dfrac{\lambda^2}{bc}\left(2b^2c^2+3bc^3-ac^3-b^2c^2+3bc\lambda-\lambda ac-\lambda b^2\right)=0$$ $$\dfrac{\lambda^2}{bc}\left(b^2c^2+3bc^3-ac^3+3bc\lambda-\lambda ac-\lambda b^2\right)=0$$ $$\dfrac{\lambda^2}{bc}\left(c^2(b^2+3bc-ac\right)+\lambda(3bc-ac-b^2)=0$$ So another factor seems to be $\dfrac{1}{bc}\left(c^2(b^2+3bc-ac)+\lambda\left(3bc-ac-b^2\right)\right)$ But actual answer is $a^2+b^2+c^2+\lambda$ . I tried to find my mistake, but everything seems correct. What am I missing here? Please help me in this.","Show that is divisible by and find the other factor. My attempt is as follows:- Taking common Now expanding it So another factor seems to be But actual answer is . I tried to find my mistake, but everything seems correct. What am I missing here? Please help me in this.","\begin{vmatrix}
a^2+\lambda &ab &ac \\ 
ab & b^2+\lambda & bc \\
ac & bc & c^2+\lambda
\end{vmatrix}=0 \lambda^2 R_1\rightarrow R_1+R_2+R_3 \begin{vmatrix}
a(a+b+c)+\lambda &b(a+b+c)+\lambda &c(a+b+c)+\lambda \\ 
ab & b^2+\lambda & bc \\
ac & bc & c^2+\lambda
\end{vmatrix}=0 C_1\rightarrow C_1-\dfrac{a}{b}C_2 C_2\rightarrow C_2-\dfrac{b}{c}C_3 \begin{vmatrix}
\lambda-\dfrac{a\lambda}{b}&\lambda-\dfrac{b\lambda}{c} &c(a+b+c)+\lambda \\ 
-\lambda & \lambda & bc \\
0 & -\lambda & c^2+\lambda
\end{vmatrix}=0 \lambda^2 \lambda^2\begin{vmatrix}
1-\dfrac{a}{b}&1-\dfrac{b}{c} &c(a+b+c)+\lambda \\ 
-1 & 1 & bc \\
0 & -1 & c^2+\lambda
\end{vmatrix}=0
 \dfrac{\lambda^2}{bc}\begin{vmatrix}
b-a&c-b &c(a+b+c)+\lambda \\ 
-b & c & bc \\
0 & -c & c^2+\lambda
\end{vmatrix}=0
 R_1\rightarrow R_1-R_3 \dfrac{\lambda^2}{bc}\begin{vmatrix}
b-a&2c-b &ca+bc \\ 
-b & c & bc \\
0 & -c & c^2+\lambda
\end{vmatrix}=0 R_1\rightarrow R_1-R_2 \dfrac{\lambda^2}{bc}\begin{vmatrix}
2b-a&c-b &ca \\ 
-b & c & bc \\
0 & -c & c^2+\lambda
\end{vmatrix}=0 \dfrac{\lambda^2}{bc}\left(c(2b^2c-abc+abc)+(c^2+\lambda)(2bc-ac+bc-b^2)\right)=0 \dfrac{\lambda^2}{bc}\left(2b^2c^2+(c^2+\lambda)(3bc-ac-b^2)\right)=0 \dfrac{\lambda^2}{bc}\left(2b^2c^2+3bc^3-ac^3-b^2c^2+3bc\lambda-\lambda ac-\lambda b^2\right)=0 \dfrac{\lambda^2}{bc}\left(b^2c^2+3bc^3-ac^3+3bc\lambda-\lambda ac-\lambda b^2\right)=0 \dfrac{\lambda^2}{bc}\left(c^2(b^2+3bc-ac\right)+\lambda(3bc-ac-b^2)=0 \dfrac{1}{bc}\left(c^2(b^2+3bc-ac)+\lambda\left(3bc-ac-b^2\right)\right) a^2+b^2+c^2+\lambda","['matrices', 'determinant']"
10,proof: $A-A(A+B)^{-1}A=B-B(A+B)^{-1}B$ [duplicate],proof:  [duplicate],A-A(A+B)^{-1}A=B-B(A+B)^{-1}B,This question already has an answer here : Matrix equation: $A−A(A + B)^{−1}A = B−B(A + B)^{−1}B$ (1 answer) Closed 4 years ago . I have to prove that $$A-A(A+B)^{-1}A=B-B(A+B)^{-1}B$$ and I don't know how to start. The only thing that is known is $A+B$ is nonsingular. Can someone help me to prove this?,This question already has an answer here : Matrix equation: $A−A(A + B)^{−1}A = B−B(A + B)^{−1}B$ (1 answer) Closed 4 years ago . I have to prove that and I don't know how to start. The only thing that is known is is nonsingular. Can someone help me to prove this?,A-A(A+B)^{-1}A=B-B(A+B)^{-1}B A+B,"['linear-algebra', 'matrices']"
11,expected operator norm of random symmetric matrices,expected operator norm of random symmetric matrices,,"The following is an easy corollary from noncommutative Khintchine's inequality (see, e.g., Vershynin's high-dimensional probability book, Theorem 6.5.1). Let $A$ be an $n\times n$ symmetric random matrix whose entries on and above the diagonal are independent, mean zero random variables. Then $$ \mathbb{E}\|A\| \lesssim \sqrt{\log n}\ \mathbb{E}\max_i \| A_i\|_2 $$ where $\|A\|$ denotes the operator norm of $A$ and $A_i$ denotes the $i$ -th row of $A$ . Question : Is $\sqrt{\log n}$ necessary in the bound above? Vershynin's book claims that it is necessary (Exercise 6.5.4) but I am unable to find an example. The bound seems quite loose to me, actually, and it is already loose for diagonal matrices and Wigner matrices. I looked up the literature, and for entries that are gaussians (with different variances) the bound above is definitely loose, as it is known that when $A_{ij}\sim N(0,b_{ij}^2)$ (due to van Handel and others) we have $$ \mathbb{E}\|A\| \lesssim \max_i\sqrt{\sum_j b_{ij}^2} + (\max_{i,j} b_{ij})\sqrt{\log n}. $$ So the hope of finding a tight example is not to have Gaussian entries, and I don't have a clue for this. Usually I think the $\sqrt{\log n}$ factor would come from the maximum of $n$ gaussians in tightness examples.","The following is an easy corollary from noncommutative Khintchine's inequality (see, e.g., Vershynin's high-dimensional probability book, Theorem 6.5.1). Let be an symmetric random matrix whose entries on and above the diagonal are independent, mean zero random variables. Then where denotes the operator norm of and denotes the -th row of . Question : Is necessary in the bound above? Vershynin's book claims that it is necessary (Exercise 6.5.4) but I am unable to find an example. The bound seems quite loose to me, actually, and it is already loose for diagonal matrices and Wigner matrices. I looked up the literature, and for entries that are gaussians (with different variances) the bound above is definitely loose, as it is known that when (due to van Handel and others) we have So the hope of finding a tight example is not to have Gaussian entries, and I don't have a clue for this. Usually I think the factor would come from the maximum of gaussians in tightness examples.","A n\times n 
\mathbb{E}\|A\| \lesssim \sqrt{\log n}\ \mathbb{E}\max_i \| A_i\|_2
 \|A\| A A_i i A \sqrt{\log n} A_{ij}\sim N(0,b_{ij}^2) 
\mathbb{E}\|A\| \lesssim \max_i\sqrt{\sum_j b_{ij}^2} + (\max_{i,j} b_{ij})\sqrt{\log n}.
 \sqrt{\log n} n","['matrices', 'expected-value', 'random-matrices', 'spectral-norm']"
12,"Let $A$ be a matrix. If $A^{n-1} \neq \bf{0}$ but $A^{n} = \bf{0}$, what does it say about the matrix $A$?","Let  be a matrix. If  but , what does it say about the matrix ?",A A^{n-1} \neq \bf{0} A^{n} = \bf{0} A,"Since the question doesn't say find all matrices $A$ , I will only produce one example for both: a. $A = \begin{bmatrix}0 & 1\\0&0\end{bmatrix}$ b. $A = \begin{bmatrix}0 & 0 &0\\1&0 & 0\\0&1&0\end{bmatrix}$ from Find a matrix so that $A^2$ not equal to 0 but $A^3$ is [Strang P78 2.4.23] Just based off of these two, I would conjecture that Matrices $A$ satisfying $A^{n-1} \neq \bf{0}$ but $A^{n} = 0$ must be such that they are of dimension $n$ However, there is a matrix $A \in \mathbb{R}^{3\times3}$ that satisfies $A\neq \bf{0}$ but $A^2= \bf{0}$ , namely $$\begin{bmatrix}0&0&0\\1&0&0\\0&0&0\end{bmatrix}$$ The next thing I could think of was: Matrices $A$ satisfying $A^{n-1} \neq \bf{0}$ but $A^{n} = 0$ must be such that they have a dimension of at least $n$ But I don't know if that's it or not... Question(s): What was I meant to conjecture?","Since the question doesn't say find all matrices , I will only produce one example for both: a. b. from Find a matrix so that $A^2$ not equal to 0 but $A^3$ is [Strang P78 2.4.23] Just based off of these two, I would conjecture that Matrices satisfying but must be such that they are of dimension However, there is a matrix that satisfies but , namely The next thing I could think of was: Matrices satisfying but must be such that they have a dimension of at least But I don't know if that's it or not... Question(s): What was I meant to conjecture?",A A = \begin{bmatrix}0 & 1\\0&0\end{bmatrix} A = \begin{bmatrix}0 & 0 &0\\1&0 & 0\\0&1&0\end{bmatrix} A A^{n-1} \neq \bf{0} A^{n} = 0 n A \in \mathbb{R}^{3\times3} A\neq \bf{0} A^2= \bf{0} \begin{bmatrix}0&0&0\\1&0&0\\0&0&0\end{bmatrix} A A^{n-1} \neq \bf{0} A^{n} = 0 n,"['linear-algebra', 'matrices', 'proof-writing']"
13,Prove or disprove $\exp(AB)=\exp(BA)$ where $A$ and $B$ are square matrices,Prove or disprove  where  and  are square matrices,\exp(AB)=\exp(BA) A B,"Is there a necessary and sufficient condition for    exp(AB) = exp(BA) where $A,B\in\mathbb{C}^{n\times n}$ Notice that AB does not have to equal to BA. What I can prove now is that det(exp(AB))=det(exp(BA)) since tr(AB)=tr(AB) implies exp(tr(AB))=exp(tr(BA)), which is sufficient that det(exp(AB)) = det(exp(BA)). Note that exp(tr(A))=det(exp(A)). This is a corollary of Jacobi's formula. Thanks a lot!! Jacobi's formula On the Necessary and Sufficient Condition for a Set of Matrices to Commute and Some Further Linked Results","Is there a necessary and sufficient condition for    exp(AB) = exp(BA) where Notice that AB does not have to equal to BA. What I can prove now is that det(exp(AB))=det(exp(BA)) since tr(AB)=tr(AB) implies exp(tr(AB))=exp(tr(BA)), which is sufficient that det(exp(AB)) = det(exp(BA)). Note that exp(tr(A))=det(exp(A)). This is a corollary of Jacobi's formula. Thanks a lot!! Jacobi's formula On the Necessary and Sufficient Condition for a Set of Matrices to Commute and Some Further Linked Results","A,B\in\mathbb{C}^{n\times n}","['linear-algebra', 'matrices', 'lie-algebras', 'matrix-calculus', 'matrix-exponential']"
14,"What does ""Random Rotation Matrix"" really mean? How to generate it?","What does ""Random Rotation Matrix"" really mean? How to generate it?",,"I got the term from this paper[1] and I don't understand what it means. It said the ""Random Rotation Matrix"" can be generated following ""Haar Distribution""[2]. I only know the output is a matrix which contains random number but has a property which is the ""Rotation"" itself but I don't know what the ""Rotation"" means. I don't know what it means with the Haar distribution too. Then I read the paper[2] about a method for generating this ""Random Rotation Matrix""[1] which is suggested by the first paper[1] and it seems so mathematical that it's hard for me to understand it. I research through the internet, books, and papers then I still don't understand what does ""Rotation Matrix"" mean and how to generate it because of those sources like telling different things. So my question is: What does ""Rotation Matrix""[1] really mean? How does it look like? Can you give me an example of it? What is ""Rotation Center""[1]? Maybe it's coordinate in the matrix where it becomes the centre point of the rotation? What is ""Haar Distribution""[1]? I only know about uniform or normal distribution lol. Is it related to ""Haar Measure""? Are there other good sources which discuss generating this ""Random Rotation Matrix""[1]? Are there tips and trick to understanding those really hard and very mathematical paper to know how to generate this ""Random Rotation Matrix""[1]? [1]: K. Chen and L. Liu. A random rotation perturbation approach to privacy preserving data classification. Technical Report, 2005. [2]: STEWART, G. The efficient generation of random orthogonal matrices with an application to condition estimation. SIAM Journal on Numerical Analysis 17 (1980). Disclaimer: I'm a computer science student who doesn't good at math. So I need an answer which more understandable by engineer and of course it needs to be codeable. Sorry if the scope of this question is too wide or doesn't specific. Maybe the answer doesn't need to be the solution, I mean it can be just another sources which I can learn to answer my questions.","I got the term from this paper[1] and I don't understand what it means. It said the ""Random Rotation Matrix"" can be generated following ""Haar Distribution""[2]. I only know the output is a matrix which contains random number but has a property which is the ""Rotation"" itself but I don't know what the ""Rotation"" means. I don't know what it means with the Haar distribution too. Then I read the paper[2] about a method for generating this ""Random Rotation Matrix""[1] which is suggested by the first paper[1] and it seems so mathematical that it's hard for me to understand it. I research through the internet, books, and papers then I still don't understand what does ""Rotation Matrix"" mean and how to generate it because of those sources like telling different things. So my question is: What does ""Rotation Matrix""[1] really mean? How does it look like? Can you give me an example of it? What is ""Rotation Center""[1]? Maybe it's coordinate in the matrix where it becomes the centre point of the rotation? What is ""Haar Distribution""[1]? I only know about uniform or normal distribution lol. Is it related to ""Haar Measure""? Are there other good sources which discuss generating this ""Random Rotation Matrix""[1]? Are there tips and trick to understanding those really hard and very mathematical paper to know how to generate this ""Random Rotation Matrix""[1]? [1]: K. Chen and L. Liu. A random rotation perturbation approach to privacy preserving data classification. Technical Report, 2005. [2]: STEWART, G. The efficient generation of random orthogonal matrices with an application to condition estimation. SIAM Journal on Numerical Analysis 17 (1980). Disclaimer: I'm a computer science student who doesn't good at math. So I need an answer which more understandable by engineer and of course it needs to be codeable. Sorry if the scope of this question is too wide or doesn't specific. Maybe the answer doesn't need to be the solution, I mean it can be just another sources which I can learn to answer my questions.",,"['matrices', 'rotations', 'random']"
15,Show that $x_2 = 0$ without solving the linear system explicitly.,Show that  without solving the linear system explicitly.,x_2 = 0,"Given the following matrix equation: $$Ax = B$$ where $$A=\begin{pmatrix} 6 & -9 & 31 & 5 & -2\\ 3 & 20 & 42 & 2 & 0 \\ 26 & 9 & 92 & 20 & -10 \\ 1 & 2 & 81 & 4 & 22 \\ 4 & -1 & 3 & 1 & -10 \end{pmatrix}, x = \begin{pmatrix} x_1 \\ x_2 \\ x_3 \\ x_4 \\ x_5 \end{pmatrix} \quad \text{and}\quad B= \begin{pmatrix} 1 \\ 0 \\ 5 \\ -11 \\ 5 \end{pmatrix}.$$ Question: If the linear system has a unique solution, show that $x_2 = 0$ without solving the linear system explicitly. I have been puzzling on this problem for few days. I attempted to solve the linear system but it is not allowed in this question. Any hint is appreciated.","Given the following matrix equation: where Question: If the linear system has a unique solution, show that without solving the linear system explicitly. I have been puzzling on this problem for few days. I attempted to solve the linear system but it is not allowed in this question. Any hint is appreciated.","Ax = B A=\begin{pmatrix}
6 & -9 & 31 & 5 & -2\\
3 & 20 & 42 & 2 & 0 \\
26 & 9 & 92 & 20 & -10 \\
1 & 2 & 81 & 4 & 22 \\
4 & -1 & 3 & 1 & -10
\end{pmatrix}, x = \begin{pmatrix}
x_1 \\
x_2 \\
x_3 \\
x_4 \\
x_5
\end{pmatrix} \quad \text{and}\quad B= \begin{pmatrix}
1 \\
0 \\
5 \\
-11 \\
5
\end{pmatrix}. x_2 = 0","['linear-algebra', 'matrices', 'systems-of-equations']"
16,"If $ABA = B$ and $BAB = A$ and $A$ is invertible, then $A^4 = I$","If  and  and  is invertible, then",ABA = B BAB = A A A^4 = I,"Let $A$ and $B$ be square matrices of the same order so that $ABA = B$ and $BAB = A$ . If $A$ is invertible, prove that $A^4 = I$ . I already proved that $A^2=B^2$ . How can I prove $A^4=I$ ?","Let and be square matrices of the same order so that and . If is invertible, prove that . I already proved that . How can I prove ?",A B ABA = B BAB = A A A^4 = I A^2=B^2 A^4=I,"['matrices', 'matrix-equations', 'projection-matrices']"
17,Is $M_0(\mathbb F)$ a valid construct?,Is  a valid construct?,M_0(\mathbb F),"Or, is there a $\text{dim}\ 0$ matrix ring? I assume you can define $M_0(\mathbb F) := \{[]\}$ and $\mathbf I_0 := []$ , but I couldn't find any confirmation on this.","Or, is there a matrix ring? I assume you can define and , but I couldn't find any confirmation on this.",\text{dim}\ 0 M_0(\mathbb F) := \{[]\} \mathbf I_0 := [],"['linear-algebra', 'matrices']"
18,Flaw in a proof of $\det AB=\det A\det B$?,Flaw in a proof of ?,\det AB=\det A\det B,"Since the elementary row operations, namely row exchanging, multiplying a scalar to a row, and subtracting a row from another row, doesn't affect to the result of the determinant, we only consider the upper triangular matrices. And for an upper triangular matrix $A$ , $\det A$ is just a product of its diagonal entries. And if we multiply two upper triangular matrices, $A,B$ , we have $$AB=\left[\begin{array}{} a_{11}&\dots&\dots&\dots\\ 0&a_{22}&\dots&\dots\\ 0&0&\ddots& \vdots&\\ 0&0&\dots&a_{nn} \end{array}\right]\left[\begin{array}{} b_{11}&\dots&\dots&\dots\\ 0&b_{22}&\dots&\dots\\ 0&0&\ddots& \vdots&\\ 0&0&\dots&b_{nn} \end{array}\right]\\ =\left[\begin{array}{} a_{11}b_{11}&\dots&\dots&\dots\\ 0&a_{22}b_{22}&\dots&\dots\\ 0&0&\ddots& \vdots&\\ 0&0&\dots&a_{nn}b_{nn} \end{array}\right].$$ So $\det AB=\det A \det B.$ I feel this should prove the equality. Any flaw in this reasoning? EDIT: In fact, multiplying scalar to a row does affect to the result. May approaching this direction a dead end?","Since the elementary row operations, namely row exchanging, multiplying a scalar to a row, and subtracting a row from another row, doesn't affect to the result of the determinant, we only consider the upper triangular matrices. And for an upper triangular matrix , is just a product of its diagonal entries. And if we multiply two upper triangular matrices, , we have So I feel this should prove the equality. Any flaw in this reasoning? EDIT: In fact, multiplying scalar to a row does affect to the result. May approaching this direction a dead end?","A \det A A,B AB=\left[\begin{array}{}
a_{11}&\dots&\dots&\dots\\
0&a_{22}&\dots&\dots\\
0&0&\ddots& \vdots&\\
0&0&\dots&a_{nn}
\end{array}\right]\left[\begin{array}{}
b_{11}&\dots&\dots&\dots\\
0&b_{22}&\dots&\dots\\
0&0&\ddots& \vdots&\\
0&0&\dots&b_{nn}
\end{array}\right]\\
=\left[\begin{array}{}
a_{11}b_{11}&\dots&\dots&\dots\\
0&a_{22}b_{22}&\dots&\dots\\
0&0&\ddots& \vdots&\\
0&0&\dots&a_{nn}b_{nn}
\end{array}\right]. \det AB=\det A \det B.","['linear-algebra', 'matrices', 'proof-verification', 'determinant']"
19,"$\operatorname{Tr}(A^2) \geq-2$, if $A\in SL(2,R)$",", if","\operatorname{Tr}(A^2) \geq-2 A\in SL(2,R)","I saw as a hint to an exercise that if $A\in SL(2,R)$ then $$\operatorname{Tr}(A^2)\geq-2.$$ I did the exercise with this hint, but I can't prove why this is true. Also, is there a similar inequality to $A\in SL(3,R)$ ?","I saw as a hint to an exercise that if then I did the exercise with this hint, but I can't prove why this is true. Also, is there a similar inequality to ?","A\in SL(2,R) \operatorname{Tr}(A^2)\geq-2. A\in SL(3,R)","['matrices', 'trace']"
20,The order of a $ 2 \times 2 $ matrix mod $ p $,The order of a  matrix mod, 2 \times 2   p ,"This is a question I found in an old textbook in group theory: We are asked to prove the order of any matrix $ A $ in the group of $ 2 \times 2 $ invertible matrices over $ F_p $ where $ p $ is a prime, $ A \in \text{GL}(2,F_p) $ , divides either $ p^2-1 $ or $ p^2-p $ . I know that the order of this group is $ (p^2-p) (p^2-1) $ thus the order of the matrix must divide it by Lagrange, but the question asks for something stronger. This is obviously true for diagonal matrices and matrices that have one zero. What about a general invertible $ 2 \times 2 $ matrix? How to show its order must divide one of the two factors given? I thought about using the Sylow theorems but nothing comes to mind. Perhaps group actions? The solution eludes me and I would appreciate help on this.","This is a question I found in an old textbook in group theory: We are asked to prove the order of any matrix in the group of invertible matrices over where is a prime, , divides either or . I know that the order of this group is thus the order of the matrix must divide it by Lagrange, but the question asks for something stronger. This is obviously true for diagonal matrices and matrices that have one zero. What about a general invertible matrix? How to show its order must divide one of the two factors given? I thought about using the Sylow theorems but nothing comes to mind. Perhaps group actions? The solution eludes me and I would appreciate help on this."," A   2 \times 2   F_p   p   A \in \text{GL}(2,F_p)   p^2-1   p^2-p   (p^2-p) (p^2-1)   2 \times 2 ","['matrices', 'group-theory', 'finite-groups', 'finite-fields']"
21,What are the implications when matrix's lowest eigenvalue is equal to 0?,What are the implications when matrix's lowest eigenvalue is equal to 0?,,I have a task to solve where only eigenvalues are given and I need to calculate a matrix condition number. The formula for it requires division by the lowest eigenvalue (which is zero). In such case the condition number cannot be calculated. Can equation system be solved if the lowest matrix eigenvalue is 0?,I have a task to solve where only eigenvalues are given and I need to calculate a matrix condition number. The formula for it requires division by the lowest eigenvalue (which is zero). In such case the condition number cannot be calculated. Can equation system be solved if the lowest matrix eigenvalue is 0?,,"['linear-algebra', 'matrices', 'eigenvalues-eigenvectors']"
22,Matrix Equation $B^3+C^3=\begin{pmatrix}-1 & 1\\ 0 & -2\end{pmatrix}$,Matrix Equation,B^3+C^3=\begin{pmatrix}-1 & 1\\ 0 & -2\end{pmatrix},"How can I solve in $\mathcal{M}_{2}(\mathbb{Z})$ the equation $$B^3+C^3=\begin{pmatrix}-1 & 1\\ 0 & -2\end{pmatrix}?$$ I try to use $$X^2-Tr(X)X+det X\cdot I_2=0_2$$ but there are $B$ and $C$ , I don't still obtain anything. thanks.","How can I solve in the equation I try to use but there are and , I don't still obtain anything. thanks.",\mathcal{M}_{2}(\mathbb{Z}) B^3+C^3=\begin{pmatrix}-1 & 1\\ 0 & -2\end{pmatrix}? X^2-Tr(X)X+det X\cdot I_2=0_2 B C,"['linear-algebra', 'matrices', 'matrix-equations']"
23,Eigenvalues of the Jacobian matrix of a quadratic change of variables,Eigenvalues of the Jacobian matrix of a quadratic change of variables,,"Consider the set of coordinates $x_i$ and $y_i$ for $i = \pm 1, \pm 2 \dots \pm N$ and $N \geq 2$ . Consider the change of coordinates from $\mathbf{x}$ to $\mathbf{y}$ defined by $$ y_i(\mathbf{x}) = c_i + x_i \sum_{j \neq i} c_j x_{-j} \tag{1} $$ where the $c_i$ are any strictly positive real numbers constrained by $\sum_{i} c_i = K$ for a constant $K$ . Let $\mathbf{J}(\mathbf{x}) = \partial \mathbf{y}/\partial \mathbf{x}$ be the Jacobian matrix of the transformation. Let $\mathbf{1}$ be a vector of length $2N$ consisting of all $1$ 's. Conjecture 1: The constant $K$ is an eigenvalue of $\mathbf{J}(\mathbf{1})$ . Conjecture 2: Assuming Conjecture $1$ is true, any vector $\mathbf{v}$ satisfying $\mathbf{J}(\mathbf{1}) \mathbf{v} = K \mathbf{v}$ has the property that $v_{j} + v_{-j} = 0$ , where $v_{\ell}$ is the entry of $\mathbf{v}$ corresponding to coordinate $x_\ell$ . I've verified both conjectures by explicit (computer-assisted) calculation for $N = 2, 3, 4$ . One observation is that Eq. $(1)$ can be written as \begin{align} y_i &= c_i + x_i \sum_{j} c_j x_{-j} - c_i x_{i} x_{-i} \\   &= c_i + \alpha x_i - c_i x_{i} x_{-i} \end{align} where $\alpha$ is a constant that doesn't depend on $i$ . In particular, swapping $i \to -i$ shows that $y_i$ and $y_{-i}$ both (sort of) only depend on $x_i$ and $x_{-i}$ which is suggestive of Conjecture $2$ . It really seems like there should be a simple reason for the conjectures but I can't see it.","Consider the set of coordinates and for and . Consider the change of coordinates from to defined by where the are any strictly positive real numbers constrained by for a constant . Let be the Jacobian matrix of the transformation. Let be a vector of length consisting of all 's. Conjecture 1: The constant is an eigenvalue of . Conjecture 2: Assuming Conjecture is true, any vector satisfying has the property that , where is the entry of corresponding to coordinate . I've verified both conjectures by explicit (computer-assisted) calculation for . One observation is that Eq. can be written as where is a constant that doesn't depend on . In particular, swapping shows that and both (sort of) only depend on and which is suggestive of Conjecture . It really seems like there should be a simple reason for the conjectures but I can't see it.","x_i y_i i = \pm 1, \pm 2 \dots \pm N N \geq 2 \mathbf{x} \mathbf{y} 
y_i(\mathbf{x}) = c_i + x_i \sum_{j \neq i} c_j x_{-j} \tag{1}
 c_i \sum_{i} c_i = K K \mathbf{J}(\mathbf{x}) = \partial \mathbf{y}/\partial \mathbf{x} \mathbf{1} 2N 1 K \mathbf{J}(\mathbf{1}) 1 \mathbf{v} \mathbf{J}(\mathbf{1}) \mathbf{v} = K \mathbf{v} v_{j} + v_{-j} = 0 v_{\ell} \mathbf{v} x_\ell N = 2, 3, 4 (1) \begin{align}
y_i &= c_i + x_i \sum_{j} c_j x_{-j} - c_i x_{i} x_{-i} \\ 
 &= c_i + \alpha x_i - c_i x_{i} x_{-i}
\end{align} \alpha i i \to -i y_i y_{-i} x_i x_{-i} 2","['matrices', 'eigenvalues-eigenvectors', 'nonlinear-system', 'jacobian']"
24,Relationship between singular values of $A$ and eigenvalues of $B:= \begin{bmatrix} 0 & A \\ A^\ast & 0 \end{bmatrix}$,Relationship between singular values of  and eigenvalues of,A B:= \begin{bmatrix} 0 & A \\ A^\ast & 0 \end{bmatrix},"Let $$B:= \begin{bmatrix} O_m & A \\ A^\ast & O_n \end{bmatrix}$$ where $A$ is an $m \times n$ matrix. Find the relationship between the two: Singular values and singular vectors of $A$ . Eigenvalues and eigenvectors of $B$ . Try Let $|\lambda_1| \ge \cdots \ge |\lambda_{m+n}|$ and $x_1, \cdots, x_{m+n}$ are eigenvalues and corresponding eigenvectors of $B$ . (Note that eigenvalues are real, since $B$ is Hermitian. Let $\eta_1 \ge \cdots \ge \eta_{k} \ge 0$ and $u_1, \cdots, u_m$ and $v_1, \cdots, v_n$ be singular values and corresponding left- and right- singular vectors of $A$ , with $k := \min\{m,n\}$ . Note that $B^2 = B^\ast B= \begin{bmatrix} A A^\ast & 0 \\ 0 & A^\ast A \end{bmatrix}$ thus $B^2 x_i = B \lambda_i x_i  = \lambda_i^2 x_i, \forall  i$ . Therefore, $$ B^2x_i = \begin{bmatrix} AA^\ast x_i^{(1)} \\ A^\ast A x_i^{(2)} \end{bmatrix} = \begin{bmatrix} \lambda_i^2 x_i^{(1)} \\ \lambda_i^2 x_i^{(2)} \end{bmatrix} $$ where $x_i := [\left(x_i^{(1)}\right)^T_m | \left(x_i^{(2)} \right)^T_n]^T_{m+n}$ . I have noticed that $x_i^{(2)}$ , $\lambda_i^2$ are eigenpairs of $A^\ast A$ , thus by the definition of singular value , $|\lambda_i|$ are the singular values of $A$ . By the way, from SVD of $A = U \Sigma V^\ast$ , we have $$ A^\ast A V = V\Sigma^2_{n \times n} \Leftrightarrow A^\ast A v_j = \eta_j^2 v_j \forall j=1,\cdots, k \\[7pt] A A^\ast U = U\Sigma^2_{m \times m} \Leftrightarrow A A^\ast u_l = \eta_l^2 u_l \forall l=1,\cdots, k $$ Question Since $B$ has $(m+n)$ different(possibly same) $\lambda_i$ (i.e. $i = 1, \cdots, m+n)$ , from $$ B^2x_i = \begin{bmatrix} AA^\ast x_i^{(1)} \\ A^\ast A x_i^{(2)} \end{bmatrix} = \begin{bmatrix} \lambda_i^2 x_i^{(1)} \\ \lambda_i^2 x_i^{(2)} \end{bmatrix} $$ we have $(m+n)$ formulas for $x_i^{(1)}$ , $x_i^{(2)}$ , i.e. $$ AA^\ast x_i^{(1)}  =  \lambda_i^2 x_i^{(1)} \\ A^\ast A x_i^{(2)}  =  \lambda_i^2 x_i^{(2)} \\ $$ for $i = 1,\cdots, m+n$ , but for $$ A^\ast A V = V(\Sigma^T\Sigma)_{n \times n} \Leftrightarrow A^\ast A v_j = \eta_j^2 v_j \forall j=1,\cdots, k \\[7pt] A A^\ast U = U(\Sigma\Sigma^T)_{m \times m} \Leftrightarrow A A^\ast u_l = \eta_l^2 u_l \forall l=1,\cdots, k $$ we only have $k$ formulas. So I'm stuck at specifying the relationship. Is there anyone to help solving the problem?","Let where is an matrix. Find the relationship between the two: Singular values and singular vectors of . Eigenvalues and eigenvectors of . Try Let and are eigenvalues and corresponding eigenvectors of . (Note that eigenvalues are real, since is Hermitian. Let and and be singular values and corresponding left- and right- singular vectors of , with . Note that thus . Therefore, where . I have noticed that , are eigenpairs of , thus by the definition of singular value , are the singular values of . By the way, from SVD of , we have Question Since has different(possibly same) (i.e. , from we have formulas for , , i.e. for , but for we only have formulas. So I'm stuck at specifying the relationship. Is there anyone to help solving the problem?","B:= \begin{bmatrix} O_m & A \\ A^\ast & O_n \end{bmatrix} A m \times n A B |\lambda_1| \ge \cdots \ge |\lambda_{m+n}| x_1, \cdots, x_{m+n} B B \eta_1 \ge \cdots \ge \eta_{k} \ge 0 u_1, \cdots, u_m v_1, \cdots, v_n A k := \min\{m,n\} B^2 = B^\ast B= \begin{bmatrix} A A^\ast & 0 \\ 0 & A^\ast A \end{bmatrix} B^2 x_i = B \lambda_i x_i  = \lambda_i^2 x_i, \forall  i 
B^2x_i = \begin{bmatrix} AA^\ast x_i^{(1)} \\ A^\ast A x_i^{(2)} \end{bmatrix} = \begin{bmatrix} \lambda_i^2 x_i^{(1)} \\ \lambda_i^2 x_i^{(2)} \end{bmatrix}
 x_i := [\left(x_i^{(1)}\right)^T_m | \left(x_i^{(2)} \right)^T_n]^T_{m+n} x_i^{(2)} \lambda_i^2 A^\ast A |\lambda_i| A A = U \Sigma V^\ast 
A^\ast A V = V\Sigma^2_{n \times n} \Leftrightarrow A^\ast A v_j = \eta_j^2 v_j \forall j=1,\cdots, k \\[7pt]
A A^\ast U = U\Sigma^2_{m \times m} \Leftrightarrow A A^\ast u_l = \eta_l^2 u_l \forall l=1,\cdots, k
 B (m+n) \lambda_i i = 1, \cdots, m+n) 
B^2x_i = \begin{bmatrix} AA^\ast x_i^{(1)} \\ A^\ast A x_i^{(2)} \end{bmatrix} = \begin{bmatrix} \lambda_i^2 x_i^{(1)} \\ \lambda_i^2 x_i^{(2)} \end{bmatrix}
 (m+n) x_i^{(1)} x_i^{(2)} 
AA^\ast x_i^{(1)}  =  \lambda_i^2 x_i^{(1)} \\
A^\ast A x_i^{(2)}  =  \lambda_i^2 x_i^{(2)} \\
 i = 1,\cdots, m+n 
A^\ast A V = V(\Sigma^T\Sigma)_{n \times n} \Leftrightarrow A^\ast A v_j = \eta_j^2 v_j \forall j=1,\cdots, k \\[7pt]
A A^\ast U = U(\Sigma\Sigma^T)_{m \times m} \Leftrightarrow A A^\ast u_l = \eta_l^2 u_l \forall l=1,\cdots, k
 k","['linear-algebra', 'matrices', 'eigenvalues-eigenvectors', 'block-matrices', 'singular-values']"
25,How to prove $\det(I_m+AA^t)=\det(I_n+A^tA)$? [closed],How to prove ? [closed],\det(I_m+AA^t)=\det(I_n+A^tA),"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 5 years ago . Improve this question Let $A \in M_{m \times n}({\Bbb F})$ , prove $$ \det\left(I_m + AA^t\right) = \det\left(I_n + A^tA\right) $$ I don't need the full answer, maybe a hint. I've tried using Sylvester's identity but I can't solve either way. I tried to replicate the proof from here , but I wondered if there were other proofs, that is why I asked for a hint, this is not homework.","Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 5 years ago . Improve this question Let , prove I don't need the full answer, maybe a hint. I've tried using Sylvester's identity but I can't solve either way. I tried to replicate the proof from here , but I wondered if there were other proofs, that is why I asked for a hint, this is not homework.",A \in M_{m \times n}({\Bbb F})  \det\left(I_m + AA^t\right) = \det\left(I_n + A^tA\right) ,"['linear-algebra', 'matrices', 'determinant']"
26,"If $x = 2$ is a root of $\det\left[\begin{smallmatrix}x&-6&-1\\2&-3x&x-3\\-3&2x&x+2\end{smallmatrix}\right]=0$, find other two roots","If  is a root of , find other two roots",x = 2 \det\left[\begin{smallmatrix}x&-6&-1\\2&-3x&x-3\\-3&2x&x+2\end{smallmatrix}\right]=0,"If $x = 2$ is a root of equation $$ \begin{vmatrix} x & -6 & -1 \\ 2 & -3x & x-3\\ -3 & 2x & x+2  \end{vmatrix} = 0 $$ Then find the other two roots. I solved it and got a cubic equation, and then I divided it by $(x-2)$ to get the other two roots. But this is a long method to do. Please help me with some shorter approach to this question.","If is a root of equation Then find the other two roots. I solved it and got a cubic equation, and then I divided it by to get the other two roots. But this is a long method to do. Please help me with some shorter approach to this question.","x = 2  \begin{vmatrix}
x & -6 & -1 \\
2 & -3x & x-3\\
-3 & 2x & x+2 
\end{vmatrix} = 0  (x-2)","['matrices', 'determinant', 'matrix-equations']"
27,Integral involving matrix exponential,Integral involving matrix exponential,,"Is there any way to simplify the integral $$ I = \int_{t_1}^{t_2}e^{\Lambda t} A e^{\Lambda t}\,dt $$ knowing that A is symmetric and Λ is a diagonal matrix?",Is there any way to simplify the integral knowing that A is symmetric and Λ is a diagonal matrix?,"
I = \int_{t_1}^{t_2}e^{\Lambda t} A e^{\Lambda t}\,dt
","['integration', 'matrices', 'symmetric-matrices', 'matrix-exponential']"
28,Why does the space of real $N \times N$ rank-$k$ matrices form a manifold?,Why does the space of real  rank- matrices form a manifold?,N \times N k,"In this paper , Uwe Helmke and Mark Shayman claim that it follows because the rank- $k$ matrices with signature $I_{p} - I_{q}$ are an orbit of a group action of $\mbox{GL}(N)$ , but I am not sure what fact is being referenced here.","In this paper , Uwe Helmke and Mark Shayman claim that it follows because the rank- matrices with signature are an orbit of a group action of , but I am not sure what fact is being referenced here.",k I_{p} - I_{q} \mbox{GL}(N),"['linear-algebra', 'matrices', 'differential-geometry', 'manifolds', 'lie-groups']"
29,Characterisation of permutation matrices,Characterisation of permutation matrices,,"$\newcommand\mat{\mathbf}$ A permutation matrix is a matrix whose columns are a permutation of the columns of the identity matrix $\mat I$ . In other words, a permutation matrix is a matrix $\mat P$ with precisely one $1$ per row/column and zeros everywhere else. A few easy observations about permutation matrices are: $\mat P^{-1} = \mat P^\mathsf{T}$ (orthogonality) $\mat P\mat 1 = \mat P^\mathsf{T}\mat1= \mat 1$ (doubly stochastic), where $\mat 1 = (1,\dots,1)$ is the all-ones vector Eigenvalues are $e^{2i\pi k/n}$ for $k=1,\dots,n$ , where $n$ is the least positive integer such that $\mat P^n = \mat I$ . But I don't think these three properties suffice to characterise permutation matrices, and the latter two aren't too nice to work with anyway. Is there a nice set of equations one can work with which completely capture the behaviour of permutation matrices?","A permutation matrix is a matrix whose columns are a permutation of the columns of the identity matrix . In other words, a permutation matrix is a matrix with precisely one per row/column and zeros everywhere else. A few easy observations about permutation matrices are: (orthogonality) (doubly stochastic), where is the all-ones vector Eigenvalues are for , where is the least positive integer such that . But I don't think these three properties suffice to characterise permutation matrices, and the latter two aren't too nice to work with anyway. Is there a nice set of equations one can work with which completely capture the behaviour of permutation matrices?","\newcommand\mat{\mathbf} \mat I \mat P 1 \mat P^{-1} = \mat P^\mathsf{T} \mat P\mat 1 = \mat P^\mathsf{T}\mat1= \mat 1 \mat 1 = (1,\dots,1) e^{2i\pi k/n} k=1,\dots,n n \mat P^n = \mat I","['linear-algebra', 'abstract-algebra', 'matrices', 'permutations', 'permutation-matrices']"
30,"Is there an injective homomorphism from $S_4$ to $GL(2,C)$",Is there an injective homomorphism from  to,"S_4 GL(2,C)","Is there an injective homomorphism from $S_4$ to $GL(2,C)$ ? My attempt : If such an injective homomorphism exists, then $S_4$ is isomorphic to a subgroup $A$ of $GL(2,C)$ . $A$ must contain nine elements of order $2$ ; eight elements of order $3$ and six elements of order $4$ .","Is there an injective homomorphism from to ? My attempt : If such an injective homomorphism exists, then is isomorphic to a subgroup of . must contain nine elements of order ; eight elements of order and six elements of order .","S_4 GL(2,C) S_4 A GL(2,C) A 2 3 4","['abstract-algebra', 'matrices', 'group-theory', 'representation-theory', 'group-homomorphism']"
31,Are matrices of different sizes associative?,Are matrices of different sizes associative?,,"I have three matrices, respectively A, B, C. Matrices A and B are both 3x3 matrices, and C is a 3x1 matrix. Is ABC associative? In general if the product (AB)C and A(BC) is well defined then does associativity always hold?","I have three matrices, respectively A, B, C. Matrices A and B are both 3x3 matrices, and C is a 3x1 matrix. Is ABC associative? In general if the product (AB)C and A(BC) is well defined then does associativity always hold?",,['linear-algebra']
32,Inverse of tridiagonal Toeplitz matrix,Inverse of tridiagonal Toeplitz matrix,,"Consider  the following tridiagonal Toeplitz matrix. Let $n$ be even. $${A_{n \times n}} = \left[ {\begin{array}{*{20}{c}} {0}&{1}&{}&{}&{}\\ {1}&{0}&{1}&{}&{}\\ {}&{1}&{\ddots}&{\ddots}&{}\\ {}&{}&{\ddots}&{\ddots}&{1}\\ {}&{}&{}&{1}&{0} \end{array}} \right]$$ What is the inverse $A^{-1}$ ? Clearly, $A^{-1}$ is symmetric. I look for a proof of the following conjecture that $A^{-1}$ is given as follows: If $A_{i, j}^{-1}$ such that $j$ is odd and $i =1+j + 2 m$ with $m\in \cal N_0$ , then $A_{i, j}^{-1} = (-1)^m$ . From which follows by symmetry: If $A_{i, j}^{-1}$ such that $j$ is even and $i =-1+j - 2 m$ with $m\in \cal N_0$ , then $A_{i, j}^{-1} = (-1)^m$ . All other $A_{i, j}^{-1} = 0$ . Here is an example, computed with Matlab, for $n=10$ which shows the structure: $${A_{10 \times 10}^{-1}} = \left[ {\begin{array}{*{20}{r}} 0 &     1 &     0 &    -1 &      0 &     1 &     0 &    -1 &      0 &     1 \\      1 &     0 &     0 &     0 &     0 &     0 &     0 &     0 &     0 &     0 \\      0 &     0 &     0 &     1 &     0 &    -1 &     0 &     1 &     0 &    -1  \\     -1 &      0 &     1 &     0 &     0 &     0 &     0 &     0 &     0 &     0 \\      0 &     0 &     0 &     0 &     0 &     1 &     0 &    -1 &      0 &     1 \\      1 &     0 &    -1 &     0 &     1 &     0 &     0 &     0 &     0 &     0 \\      0 &     0 &     0 &     0 &     0 &     0 &     0 &     1 &     0 &    -1  \\     -1 &     0 &     1 &     0 &    -1 &     0 &     1 &     0 &     0 &     0 \\      0 &     0 &     0 &     0 &     0 &     0 &     0 &     0 &     0 &     1 \\      1 &     0 &    -1 &     0 &     1 &     0 &    -1 &     0 &     1 &     0  \end{array}} \right]$$","Consider  the following tridiagonal Toeplitz matrix. Let be even. What is the inverse ? Clearly, is symmetric. I look for a proof of the following conjecture that is given as follows: If such that is odd and with , then . From which follows by symmetry: If such that is even and with , then . All other . Here is an example, computed with Matlab, for which shows the structure:","n {A_{n \times n}} = \left[ {\begin{array}{*{20}{c}}
{0}&{1}&{}&{}&{}\\
{1}&{0}&{1}&{}&{}\\
{}&{1}&{\ddots}&{\ddots}&{}\\
{}&{}&{\ddots}&{\ddots}&{1}\\
{}&{}&{}&{1}&{0}
\end{array}} \right] A^{-1} A^{-1} A^{-1} A_{i, j}^{-1} j i =1+j + 2 m m\in \cal N_0 A_{i, j}^{-1} = (-1)^m A_{i, j}^{-1} j i =-1+j - 2 m m\in \cal N_0 A_{i, j}^{-1} = (-1)^m A_{i, j}^{-1} = 0 n=10 {A_{10 \times 10}^{-1}} = \left[ {\begin{array}{*{20}{r}}
0 &     1 &     0 &    -1 &      0 &     1 &     0 &    -1 &      0 &     1 \\
     1 &     0 &     0 &     0 &     0 &     0 &     0 &     0 &     0 &     0 \\
     0 &     0 &     0 &     1 &     0 &    -1 &     0 &     1 &     0 &    -1  \\
    -1 &      0 &     1 &     0 &     0 &     0 &     0 &     0 &     0 &     0 \\
     0 &     0 &     0 &     0 &     0 &     1 &     0 &    -1 &      0 &     1 \\
     1 &     0 &    -1 &     0 &     1 &     0 &     0 &     0 &     0 &     0 \\
     0 &     0 &     0 &     0 &     0 &     0 &     0 &     1 &     0 &    -1  \\
    -1 &     0 &     1 &     0 &    -1 &     0 &     1 &     0 &     0 &     0 \\
     0 &     0 &     0 &     0 &     0 &     0 &     0 &     0 &     0 &     1 \\
     1 &     0 &    -1 &     0 &     1 &     0 &    -1 &     0 &     1 &     0 
\end{array}} \right]","['matrices', 'inverse', 'symmetric-matrices', 'tridiagonal-matrices', 'toeplitz-matrices']"
33,What is n-dimensional DFT as a linear transformation matrix look like? How it can be expressed as a matrix multiplication?,What is n-dimensional DFT as a linear transformation matrix look like? How it can be expressed as a matrix multiplication?,,"In the above picture, the Discrete Fourier Transform, for the case of 1-dimension is expressed as a matrix multiplication of the 1-D vector of length $N$ with a DFT matrix of dimension $N\times N$ . I want a similar thing for a general case of n-dimensional DFT. That is, let $x$ be an n-dimensional object, say $N_1\times N_2\times...\times N_n$ . I want to take a n-dimensional DFT of this object, by expressing as a product with a DFT matrix as in the case of 1-dimension example in the above figure. What kind of object would the DFT matrix (if i can call it a matrix) be? That defintely not a matrix product but something generalized to handle higher dimensions. Please provide me some theory and directions, or if it already exists, please point me to a reference. PS : I don't know anything about Tensors, so Ia m not sure if tensors would help.","In the above picture, the Discrete Fourier Transform, for the case of 1-dimension is expressed as a matrix multiplication of the 1-D vector of length with a DFT matrix of dimension . I want a similar thing for a general case of n-dimensional DFT. That is, let be an n-dimensional object, say . I want to take a n-dimensional DFT of this object, by expressing as a product with a DFT matrix as in the case of 1-dimension example in the above figure. What kind of object would the DFT matrix (if i can call it a matrix) be? That defintely not a matrix product but something generalized to handle higher dimensions. Please provide me some theory and directions, or if it already exists, please point me to a reference. PS : I don't know anything about Tensors, so Ia m not sure if tensors would help.",N N\times N x N_1\times N_2\times...\times N_n,"['linear-algebra', 'matrices', 'fourier-analysis', 'linear-transformations', 'matrix-equations']"
34,Differentiability of matrix square root operator,Differentiability of matrix square root operator,,"I'm interested to know if the map: $$S:\mathcal{L}(\mathbb{R}^n)\rightarrow\mathcal{L}(\mathbb{R}^n)$$ $$S(T)=\sqrt{T^*T}$$ Is differentiable. I'm aware $T\mapsto T^* T$ is differentiable, and I know that since $T^*T$ is positive self-adjoint, the square root exists, but not sure where to go from here... My ideia was that, matrix-wise, choosing the correct basis, $T^*T$ is diagonal, then the map would be equivalent to taking the square root component-wise, which should be differentiable, though I don't know if that's a valid way to justify it...","I'm interested to know if the map: Is differentiable. I'm aware is differentiable, and I know that since is positive self-adjoint, the square root exists, but not sure where to go from here... My ideia was that, matrix-wise, choosing the correct basis, is diagonal, then the map would be equivalent to taking the square root component-wise, which should be differentiable, though I don't know if that's a valid way to justify it...",S:\mathcal{L}(\mathbb{R}^n)\rightarrow\mathcal{L}(\mathbb{R}^n) S(T)=\sqrt{T^*T} T\mapsto T^* T T^*T T^*T,"['real-analysis', 'matrices', 'linear-transformations']"
35,Eigenvalues of A are also eigenvalues of T,Eigenvalues of A are also eigenvalues of T,,"Let $V$ be the set of all $n\times n$ matrices over a field $F$ . Let $A$ be a fixed element of $V$ . Define a linear operator $T$ on $V$ by $T(B)=AB$ . I am trying to show that if $\lambda$ is an eigenvalue of $A$ , then $\lambda$ is also an eigenvalue of $T$ . So suppose $Av=\lambda v$ for some $v\neq 0$ in $V$ and $\lambda\in F$ . So I'd like to prove the existence of a matrix $B$ such that $T(B)=AB=\lambda A$ , or equivalently, show that $T-\lambda I_V$ is not invertible (or injective or surjective). But I am not sure how to proceed from here. What can I do?","Let be the set of all matrices over a field . Let be a fixed element of . Define a linear operator on by . I am trying to show that if is an eigenvalue of , then is also an eigenvalue of . So suppose for some in and . So I'd like to prove the existence of a matrix such that , or equivalently, show that is not invertible (or injective or surjective). But I am not sure how to proceed from here. What can I do?",V n\times n F A V T V T(B)=AB \lambda A \lambda T Av=\lambda v v\neq 0 V \lambda\in F B T(B)=AB=\lambda A T-\lambda I_V,"['linear-algebra', 'matrices', 'eigenvalues-eigenvectors', 'linear-transformations']"
36,Rank of Matrix Determination,Rank of Matrix Determination,,$X=(I+ab^T)A(I+ba^T)$; $A$ is symmetric and positive definite matrix of $n \times n$. $I$ is the Identity matrix of $n \times n$. $a$ and $b$ are vectors of $n \times 1$. $a.b \neq -1$ and $a.b \neq 0$ $a$ is not parallel to $Ab$ How do we show that $X-A$ is a rank $2$ matrix ? Efforts: $$X-A= ab^T A + Aba^T+ ab^T A ba^T$$ Hence each of the terms are having rank $1$. so the sum of all the terms can have rank $\leq 3$ But I am not getting how it can be exactly of rank $2$..,$X=(I+ab^T)A(I+ba^T)$; $A$ is symmetric and positive definite matrix of $n \times n$. $I$ is the Identity matrix of $n \times n$. $a$ and $b$ are vectors of $n \times 1$. $a.b \neq -1$ and $a.b \neq 0$ $a$ is not parallel to $Ab$ How do we show that $X-A$ is a rank $2$ matrix ? Efforts: $$X-A= ab^T A + Aba^T+ ab^T A ba^T$$ Hence each of the terms are having rank $1$. so the sum of all the terms can have rank $\leq 3$ But I am not getting how it can be exactly of rank $2$..,,"['linear-algebra', 'matrices', 'matrix-rank']"
37,Does representation irreducibility ensure non-zero determinant?,Does representation irreducibility ensure non-zero determinant?,,"If a set of matrix representation $\{M(g)\}$ for a group $G$ is irreducible, what can we say about their determinant for every $g\in G$? Are they all of non-zero determinant? Thank you very much! Cheers, Collin P.S.: I'm a physics graduate student. So please use as little math terminology as possible, I would really appreciate that!","If a set of matrix representation $\{M(g)\}$ for a group $G$ is irreducible, what can we say about their determinant for every $g\in G$? Are they all of non-zero determinant? Thank you very much! Cheers, Collin P.S.: I'm a physics graduate student. So please use as little math terminology as possible, I would really appreciate that!",,"['matrices', 'group-theory', 'finite-groups', 'determinant']"
38,Proving similarity of matrices using characteristic polynomial,Proving similarity of matrices using characteristic polynomial,,"Let $\ A $ be a real matrix of $\ 3 \times 3 $ with the characteristic polynom: $\ p(t) = t^3 + 2t^2 -3t $ Prove: the matrix $\ A^2 + A - 2I $ is similar to $\ D = \begin{bmatrix} 0 & 0 & 0 \\ 0 &4 & 0 \\ 0 & 0 & - 2 \end{bmatrix} $ $\ p(t) = t^3 +2t^2 -3t = t(t^2 + 2t -3) = t(t-1)(t+3) $ then $\ A $ has 3 eigenvalues $\ 0,1,-3 $ and is similar to $\ M =  \begin{bmatrix} 1 & 0 & 0 \\ 0 & -3 & 0 \\ 0 & 0 & 0  \end{bmatrix}$ also,  $\ A^2 + A -2I = (A + 2I)(A-I) $ I can see that if I place $\ M$ instead so $\ (M+2I)(M-I) = D$ yet this is far from proving.","Let $\ A $ be a real matrix of $\ 3 \times 3 $ with the characteristic polynom: $\ p(t) = t^3 + 2t^2 -3t $ Prove: the matrix $\ A^2 + A - 2I $ is similar to $\ D = \begin{bmatrix} 0 & 0 & 0 \\ 0 &4 & 0 \\ 0 & 0 & - 2 \end{bmatrix} $ $\ p(t) = t^3 +2t^2 -3t = t(t^2 + 2t -3) = t(t-1)(t+3) $ then $\ A $ has 3 eigenvalues $\ 0,1,-3 $ and is similar to $\ M =  \begin{bmatrix} 1 & 0 & 0 \\ 0 & -3 & 0 \\ 0 & 0 & 0  \end{bmatrix}$ also,  $\ A^2 + A -2I = (A + 2I)(A-I) $ I can see that if I place $\ M$ instead so $\ (M+2I)(M-I) = D$ yet this is far from proving.",,"['linear-algebra', 'matrices']"
39,Group of matrices which share a fixed eigenvector is noncommutative,Group of matrices which share a fixed eigenvector is noncommutative,,"Let $n\in\mathbb{N}$, $n>1$, $v\in\mathbb{R}^n$. Let $X=\{A\in GL_n(\mathbb{R})\mid Av=\lambda v\text{ for some }\lambda\in \mathbb{R}\}$. Then $X$ is a noncommutative group. It is easy to show, that $X$ is a group. But how do I show non commutativity? I can definitely construct counterexamples, but I think there should be a general argument that works for an arbitrary $v$.","Let $n\in\mathbb{N}$, $n>1$, $v\in\mathbb{R}^n$. Let $X=\{A\in GL_n(\mathbb{R})\mid Av=\lambda v\text{ for some }\lambda\in \mathbb{R}\}$. Then $X$ is a noncommutative group. It is easy to show, that $X$ is a group. But how do I show non commutativity? I can definitely construct counterexamples, but I think there should be a general argument that works for an arbitrary $v$.",,"['linear-algebra', 'matrices', 'eigenvalues-eigenvectors', 'abelian-groups']"
40,Matrix vector form. Is this in the correct form?,Matrix vector form. Is this in the correct form?,,I have this question: Write the linear system   $$\begin{array}{rcr}-2x_1+x_2-4x_3 & = & 1 \\ x_1-2x_2 & = & -3 \\ x_1+x_2-4x_3 & = & 0 \end{array}$$   in the matrix-vector form $A\mathbf{x}=\mathbf{b}$. Is this what they want? $$  x_1* \begin{bmatrix}   -2 \\ 1 \\ 1 \end{bmatrix} + x_2* \begin{bmatrix}   1 \\ -2 \\ 1 \end{bmatrix} + x_3* \begin{bmatrix}   -4 \\ 0 \\ -4 \end{bmatrix} = \begin{bmatrix}   1 \\ -3 \\ 0 \end{bmatrix} $$,I have this question: Write the linear system   $$\begin{array}{rcr}-2x_1+x_2-4x_3 & = & 1 \\ x_1-2x_2 & = & -3 \\ x_1+x_2-4x_3 & = & 0 \end{array}$$   in the matrix-vector form $A\mathbf{x}=\mathbf{b}$. Is this what they want? $$  x_1* \begin{bmatrix}   -2 \\ 1 \\ 1 \end{bmatrix} + x_2* \begin{bmatrix}   1 \\ -2 \\ 1 \end{bmatrix} + x_3* \begin{bmatrix}   -4 \\ 0 \\ -4 \end{bmatrix} = \begin{bmatrix}   1 \\ -3 \\ 0 \end{bmatrix} $$,,"['linear-algebra', 'matrices', 'systems-of-equations']"
41,How can I find the general form of an orthogonal matrix?,How can I find the general form of an orthogonal matrix?,,"I know that the general form of orthogonal matrices is $$\begin{pmatrix} \cos \theta & \sin \theta \\ -\sin \theta & \cos \theta \end{pmatrix}$$ since they are all rotation matrices but how do I prove it? I have done the reverse, i.e., for such a rotation matrix proven that it is orthogonal: $$\vec r^\prime . \vec r^\prime = \sum_{p=1}^n r_p r_p = \sum_{p=1}^n (\sum_{j=1}^n R_{pj} r_j) (\sum_{k=1}^n R_{pk} r_k) = \sum_{pjk} R_{pj} R_{pk} r_j r_k = \sum_{i} r_i r_i = \vec r . \vec r$$ which is only possible if $\sum_{p} R_{pj} R_{pk}$ = $\delta_{jk}$ which defines an orthogonal matrix. Does just reversing this process work for finding the general form? If not, what's the correct method?","I know that the general form of orthogonal matrices is $$\begin{pmatrix} \cos \theta & \sin \theta \\ -\sin \theta & \cos \theta \end{pmatrix}$$ since they are all rotation matrices but how do I prove it? I have done the reverse, i.e., for such a rotation matrix proven that it is orthogonal: $$\vec r^\prime . \vec r^\prime = \sum_{p=1}^n r_p r_p = \sum_{p=1}^n (\sum_{j=1}^n R_{pj} r_j) (\sum_{k=1}^n R_{pk} r_k) = \sum_{pjk} R_{pj} R_{pk} r_j r_k = \sum_{i} r_i r_i = \vec r . \vec r$$ which is only possible if $\sum_{p} R_{pj} R_{pk}$ = $\delta_{jk}$ which defines an orthogonal matrix. Does just reversing this process work for finding the general form? If not, what's the correct method?",,['linear-algebra']
42,"How to answer matrices problems involving ""meet"" and ""join""?","How to answer matrices problems involving ""meet"" and ""join""?",,"Problem: Given the matrix below find the meet and join of A and B. \begin{bmatrix}     1    & 0  & 1 \\     1    & 1  & 0 \\       0    & 0  & 1 \\ \end{bmatrix} \begin{bmatrix}     0    & 1  & 1 \\     1    & 0  & 1 \\       1    & 0  & 1 \\ \end{bmatrix} How to answer this kind of questions? I have learned the basics to advance math and know how to answer matrices involving problems, but I am new to this ""meet"" and ""join"" type of questions. Any help would be appreciated.","Problem: Given the matrix below find the meet and join of A and B. \begin{bmatrix}     1    & 0  & 1 \\     1    & 1  & 0 \\       0    & 0  & 1 \\ \end{bmatrix} \begin{bmatrix}     0    & 1  & 1 \\     1    & 0  & 1 \\       1    & 0  & 1 \\ \end{bmatrix} How to answer this kind of questions? I have learned the basics to advance math and know how to answer matrices involving problems, but I am new to this ""meet"" and ""join"" type of questions. Any help would be appreciated.",,"['matrices', 'matrix-equations']"
43,Inverse of the Jordan block matrix,Inverse of the Jordan block matrix,,There is the Jordan block matrix $J_\lambda(n):=\begin{pmatrix}  \lambda & 1 & & & \\  & \lambda & 1 \\ & & ... & ... \\ & & & \lambda & 1 \\ & & & & \lambda  \end{pmatrix} \in \mathbb{C^{n \times n}}$ How to find the inverse of this matrix? I tried with the Gauss Jordan Elimination and got $J_\lambda(n)^{-1} = \begin{pmatrix}  \frac{1}{\lambda} & 0 & & & \\  & \frac{1}{\lambda} & 0 \\ & & ... & ... \\ & & & \frac{1}{\lambda} & 0 \\ & & & & \frac{1}{\lambda} \end{pmatrix}$ But i don't know if this works.,There is the Jordan block matrix $J_\lambda(n):=\begin{pmatrix}  \lambda & 1 & & & \\  & \lambda & 1 \\ & & ... & ... \\ & & & \lambda & 1 \\ & & & & \lambda  \end{pmatrix} \in \mathbb{C^{n \times n}}$ How to find the inverse of this matrix? I tried with the Gauss Jordan Elimination and got $J_\lambda(n)^{-1} = \begin{pmatrix}  \frac{1}{\lambda} & 0 & & & \\  & \frac{1}{\lambda} & 0 \\ & & ... & ... \\ & & & \frac{1}{\lambda} & 0 \\ & & & & \frac{1}{\lambda} \end{pmatrix}$ But i don't know if this works.,,"['matrices', 'inverse', 'jordan-normal-form']"
44,"Does $e^A=e^B$ imply $[A,B]=0$?",Does  imply ?,"e^A=e^B [A,B]=0","Let $A$ and $B$ be two matrices, and suppose that $\exp(A)=\exp(B)$. Does this imply that $[A, B]=0$? BCH 's formula shows a clear relation between $\exp(A)\exp(-B)$ and $[A, B]$, but the implication is far from obvious. However, I haven't been able to find a counterexample for this fact. Is this a known result? While, as was pointed out in one answer , this is not true in the general case, or even in the case where we restrict ourselves to diagonalizable matrices, the argument below (*) leads me to think that it may be true when restricting to normal matrices (that is, to unitarily diagonalizable ones). Is this argument correct? Is this a known result, and/or can it be shown in a more direct way? (*) $U$ is diagonalizable and non-degenerate. If $U$ is diagonalizable and non-degenerate, then we can write $$U=\sum_{k=1}^N \lambda_k P_k,$$ where $\lambda_k$ are the eigenvalues of $A$ and $P_k$ projectors over the corresponding eigenvectors. If $U$ to be normal, we also have $P_j P_k=\delta_{jk}$. Then I can write the set of logarithms of $U$ as: $$\log U = \sum_{k=1}^N (\operatorname{Log}\lambda_k+2\pi i\nu_k)P_k$$ with $\nu_k\in\mathbb Z$. From the above, it seems that indeed all logarithms of $U$ must commute with each other (because trivially $[P_j, P_k]=0$). $U$ diagonalizable and possibly degenerate. If $U$ is diagonalizable but non necessarily non-degenerate, we can write it as $$U=\sum_k\lambda_k\sum_{j=1}^{d_k}P_{kj},$$ where $d_k$ is the dimension of the $k$-th degenerate eigenspace, $\sum_j P_{kj}=P_k$, $\sum_k P_k=\mathbb 1$, $P_{ij}P_{kl}=\delta_{ik}\delta_{jl}$, and it is to be noted that one can choose any such basis of projectors for each degenerate eigenspace. The logarithms of $U$ are then written as $$ \log U = \sum_{jk} \ell_{\nu_{k}}(\lambda_j) P_{jk}, $$ where we used the notation $\ell_j(\lambda)\equiv\operatorname{Log}(\lambda)+2\pi i j$ to denote the set of logarithms of a given $\lambda$. The set of logarithms of $U$ is then characterised by the possible choices of $\{\nu_k\}_k$ and $P_{jk}$.","Let $A$ and $B$ be two matrices, and suppose that $\exp(A)=\exp(B)$. Does this imply that $[A, B]=0$? BCH 's formula shows a clear relation between $\exp(A)\exp(-B)$ and $[A, B]$, but the implication is far from obvious. However, I haven't been able to find a counterexample for this fact. Is this a known result? While, as was pointed out in one answer , this is not true in the general case, or even in the case where we restrict ourselves to diagonalizable matrices, the argument below (*) leads me to think that it may be true when restricting to normal matrices (that is, to unitarily diagonalizable ones). Is this argument correct? Is this a known result, and/or can it be shown in a more direct way? (*) $U$ is diagonalizable and non-degenerate. If $U$ is diagonalizable and non-degenerate, then we can write $$U=\sum_{k=1}^N \lambda_k P_k,$$ where $\lambda_k$ are the eigenvalues of $A$ and $P_k$ projectors over the corresponding eigenvectors. If $U$ to be normal, we also have $P_j P_k=\delta_{jk}$. Then I can write the set of logarithms of $U$ as: $$\log U = \sum_{k=1}^N (\operatorname{Log}\lambda_k+2\pi i\nu_k)P_k$$ with $\nu_k\in\mathbb Z$. From the above, it seems that indeed all logarithms of $U$ must commute with each other (because trivially $[P_j, P_k]=0$). $U$ diagonalizable and possibly degenerate. If $U$ is diagonalizable but non necessarily non-degenerate, we can write it as $$U=\sum_k\lambda_k\sum_{j=1}^{d_k}P_{kj},$$ where $d_k$ is the dimension of the $k$-th degenerate eigenspace, $\sum_j P_{kj}=P_k$, $\sum_k P_k=\mathbb 1$, $P_{ij}P_{kl}=\delta_{ik}\delta_{jl}$, and it is to be noted that one can choose any such basis of projectors for each degenerate eigenspace. The logarithms of $U$ are then written as $$ \log U = \sum_{jk} \ell_{\nu_{k}}(\lambda_j) P_{jk}, $$ where we used the notation $\ell_j(\lambda)\equiv\operatorname{Log}(\lambda)+2\pi i j$ to denote the set of logarithms of a given $\lambda$. The set of logarithms of $U$ is then characterised by the possible choices of $\{\nu_k\}_k$ and $P_{jk}$.",,"['matrices', 'eigenvalues-eigenvectors', 'matrix-exponential']"
45,orthogonal matrix with negative determinant,orthogonal matrix with negative determinant,,"Question : Assume that $A$ is a $n\times n$ orthogonal matrix such that $det(A)<0$. Prove that there exists a nonzero vector $x\in\mathbb{R}^n$ such that $Ax=-x$. Since $A$ is orthogonal and $det(A)<0$, its determinant is $-1$. Please help, thanks a lot!","Question : Assume that $A$ is a $n\times n$ orthogonal matrix such that $det(A)<0$. Prove that there exists a nonzero vector $x\in\mathbb{R}^n$ such that $Ax=-x$. Since $A$ is orthogonal and $det(A)<0$, its determinant is $-1$. Please help, thanks a lot!",,"['linear-algebra', 'matrices']"
46,Proof by induction. Two matrices are equal.,Proof by induction. Two matrices are equal.,,I think i have this proof figured out but I just want a second opinion. We have matrices $A=B$. $$A=\begin{pmatrix}a&1\\0&a\end{pmatrix}^n$$   $$B=\begin{pmatrix}a^n&n a^{n-1}\\0&a^n\end{pmatrix}$$ My base case I used $n=1$ and I was able to get them to equal. Next with my induction step I set $n=k$ and assumed that the base cases holds. So then I let $n=k+1$ and I got matrix $A$ to look nice. However when i did this process with matrix $B$ it did not equal matrix $A$. So with this i continued to solve through and i have gotten them almost identical but i feel like i am missing a step. Thank you!,I think i have this proof figured out but I just want a second opinion. We have matrices $A=B$. $$A=\begin{pmatrix}a&1\\0&a\end{pmatrix}^n$$   $$B=\begin{pmatrix}a^n&n a^{n-1}\\0&a^n\end{pmatrix}$$ My base case I used $n=1$ and I was able to get them to equal. Next with my induction step I set $n=k$ and assumed that the base cases holds. So then I let $n=k+1$ and I got matrix $A$ to look nice. However when i did this process with matrix $B$ it did not equal matrix $A$. So with this i continued to solve through and i have gotten them almost identical but i feel like i am missing a step. Thank you!,,"['matrices', 'proof-verification', 'induction']"
47,Simpler proof that $\det A\neq 0$ if $A$ is invertible?,Simpler proof that  if  is invertible?,\det A\neq 0 A,"One excercise asked me to ""Prove that the determinant of an inversible matrix can't be 0"" . I couldn't remember the proof the teacher gave and I didn't want to ""cheat"" because I'm practising for an exam, so after some thinking I came up with this. I'd like to know if someone has a simpler and/or shorter proof than mine, and if someone can find any mistakes in my proof. Oh, and if someone can suggest a way to shorten it or maybe use more symbols and less natural language. I lost the sheet of the proof the teacher provided, and Googling/searching books I found nothing (only proof that $\det{(A ^{-1})} = 1/\det A$). Maybe I didn't word it properly because English is not my native language. Proof First, proof that the determinant of an elementary matrix $[E]$ can't be 0: An elementary matrix $[E]$ is defined as a matrix obtained by applying a single row operation to the identity matrix. $|I|=1$, so: $|E|=1$ if it comes from adding a row to another one $|E|=-1$ if it comes from swapping two rows $|E|=k/k\in\Bbb R \land k≠0$ if it comes from multiplying a row by $k$ $|E|≠0$ Now, let's say we have a square matrix $[A]_{n\times n}$ that is invertible. To obtain the inverse of $[A]$ we have to apply row operations to the augmented matrix $[A|I]$ until we get $[I|A^{-1}]$. Let's say $E_1*E_2*...E_n$ are the elementary matrixes we premultiply our initial augmented matrix by to get the identity and the inverse. Then we get: $$E_1*E_2*...E_n * A = I$$ and $$E_1*E_2*...E_n * I = A^{-1}$$ Let's take the first one, and apply determinant to both sides of the equality $$|E_1*E_2*...E_n * A| = |I|$$ $$|E_1*E_2*...E_n|*|A| = 1$$ Now let's suppose $|A|$ could be $0$. We would get: $$|E_1*E_2*...E_n|*0 = 1$$ $$0 = 1$$ Which is absurd, and thus the determinant of an invertible matrix can't be 0 .","One excercise asked me to ""Prove that the determinant of an inversible matrix can't be 0"" . I couldn't remember the proof the teacher gave and I didn't want to ""cheat"" because I'm practising for an exam, so after some thinking I came up with this. I'd like to know if someone has a simpler and/or shorter proof than mine, and if someone can find any mistakes in my proof. Oh, and if someone can suggest a way to shorten it or maybe use more symbols and less natural language. I lost the sheet of the proof the teacher provided, and Googling/searching books I found nothing (only proof that $\det{(A ^{-1})} = 1/\det A$). Maybe I didn't word it properly because English is not my native language. Proof First, proof that the determinant of an elementary matrix $[E]$ can't be 0: An elementary matrix $[E]$ is defined as a matrix obtained by applying a single row operation to the identity matrix. $|I|=1$, so: $|E|=1$ if it comes from adding a row to another one $|E|=-1$ if it comes from swapping two rows $|E|=k/k\in\Bbb R \land k≠0$ if it comes from multiplying a row by $k$ $|E|≠0$ Now, let's say we have a square matrix $[A]_{n\times n}$ that is invertible. To obtain the inverse of $[A]$ we have to apply row operations to the augmented matrix $[A|I]$ until we get $[I|A^{-1}]$. Let's say $E_1*E_2*...E_n$ are the elementary matrixes we premultiply our initial augmented matrix by to get the identity and the inverse. Then we get: $$E_1*E_2*...E_n * A = I$$ and $$E_1*E_2*...E_n * I = A^{-1}$$ Let's take the first one, and apply determinant to both sides of the equality $$|E_1*E_2*...E_n * A| = |I|$$ $$|E_1*E_2*...E_n|*|A| = 1$$ Now let's suppose $|A|$ could be $0$. We would get: $$|E_1*E_2*...E_n|*0 = 1$$ $$0 = 1$$ Which is absurd, and thus the determinant of an invertible matrix can't be 0 .",,"['linear-algebra', 'matrices', 'proof-verification', 'proof-writing']"
48,How to prove the complement $P^\perp$ of a projection matrix $P$ have relation $I-P=P^\perp$,How to prove the complement  of a projection matrix  have relation,P^\perp P I-P=P^\perp,"I want to know how to prove that for a projection matrix $P$ and its complement matrix $P^\perp$. We have $$I-P=P^\perp$$ I do know the intuition that $P$ and $P^\perp$ project a vector into two different subspaces. But can we prove it in a algebra way? P can be represented by $$P=U_1U_1^T;P^\perp=U_2U_2^T$$, in which $$\begin{bmatrix}U_1&U_2\end{bmatrix}$$ is an orthogonal matrix.","I want to know how to prove that for a projection matrix $P$ and its complement matrix $P^\perp$. We have $$I-P=P^\perp$$ I do know the intuition that $P$ and $P^\perp$ project a vector into two different subspaces. But can we prove it in a algebra way? P can be represented by $$P=U_1U_1^T;P^\perp=U_2U_2^T$$, in which $$\begin{bmatrix}U_1&U_2\end{bmatrix}$$ is an orthogonal matrix.",,"['linear-algebra', 'matrices', 'orthogonal-matrices', 'projection-matrices']"
49,Matrix inverse $A^{-1}$ as linear combination of the powers of $A$? [closed],Matrix inverse  as linear combination of the powers of ? [closed],A^{-1} A,"Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 6 years ago . Improve this question Let $A \in \mathbb{R}^{n \times n}$ be any invertible matrix. Can $A^{-1}$ always be expressed as a linear combination of the powers of $A$, i.e. $$A^{-1}=\sum_{i=0}^\infty c_iA^i\,,$$ for an appropriate choice of coefficients $\{c_i\}_{i=0}^\infty$?","Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 6 years ago . Improve this question Let $A \in \mathbb{R}^{n \times n}$ be any invertible matrix. Can $A^{-1}$ always be expressed as a linear combination of the powers of $A$, i.e. $$A^{-1}=\sum_{i=0}^\infty c_iA^i\,,$$ for an appropriate choice of coefficients $\{c_i\}_{i=0}^\infty$?",,"['linear-algebra', 'matrices', 'power-series', 'inverse', 'matrix-calculus']"
50,Determine what condition the $n\times n$ matrix $A$ must fulfill so that the matrix $I_n - A$ has an inverse and this equals $I_n + A$.,Determine what condition the  matrix  must fulfill so that the matrix  has an inverse and this equals .,n\times n A I_n - A I_n + A,Determine what condition the $n\times n$ matrix $A$ must fulfill so that the matrix $I_n - A$ has an inverse and this equals $I_n + A$. I tried to apply it to a $2\times 2$ matrix obtaining the following result: $A = \begin{pmatrix}     a & b\\     c & d   \end{pmatrix}; $ $(I_n-A)^{-1} = \frac{1}{(1-a)(1-d)-bc} * \begin{pmatrix}     1-d & b\\     c & 1-a   \end{pmatrix} = \begin{pmatrix}     1+a & b\\     c & 1+d   \end{pmatrix} $,Determine what condition the $n\times n$ matrix $A$ must fulfill so that the matrix $I_n - A$ has an inverse and this equals $I_n + A$. I tried to apply it to a $2\times 2$ matrix obtaining the following result: $A = \begin{pmatrix}     a & b\\     c & d   \end{pmatrix}; $ $(I_n-A)^{-1} = \frac{1}{(1-a)(1-d)-bc} * \begin{pmatrix}     1-d & b\\     c & 1-a   \end{pmatrix} = \begin{pmatrix}     1+a & b\\     c & 1+d   \end{pmatrix} $,,"['linear-algebra', 'matrices']"
51,Most usual notation for extracted matrix,Most usual notation for extracted matrix,,"Let $$A = \pmatrix{1&2&3\\4&5&6\\7&8&9}.$$ What is the most usual notation and english-math vocabulary term for ""the extracted matrix obtained by removing the 3rd line and 2nd column"" , i.e. $$\pmatrix{1&3\\4&6}$$ Would you note it $A_{3,2}$, ${A^{(3,2)}}$, etc. ? I know there's probably no consensus, but what's the most usual notation you find in textbooks? Example: This is useful when computing a determinant: $$\det(A) = 7  \det A_{3,1} - 8 \det A_{3,2} + 9 \det A_{3,3}. $$","Let $$A = \pmatrix{1&2&3\\4&5&6\\7&8&9}.$$ What is the most usual notation and english-math vocabulary term for ""the extracted matrix obtained by removing the 3rd line and 2nd column"" , i.e. $$\pmatrix{1&3\\4&6}$$ Would you note it $A_{3,2}$, ${A^{(3,2)}}$, etc. ? I know there's probably no consensus, but what's the most usual notation you find in textbooks? Example: This is useful when computing a determinant: $$\det(A) = 7  \det A_{3,1} - 8 \det A_{3,2} + 9 \det A_{3,3}. $$",,"['matrices', 'notation', 'terminology']"
52,Computing the product $A_1 A_2 \cdots A_n$ with $A_n =\left(\displaystyle\begin{smallmatrix} n^2&1\\-1&n^2\end{smallmatrix}\right)$,Computing the product  with,A_1 A_2 \cdots A_n A_n =\left(\displaystyle\begin{smallmatrix} n^2&1\\-1&n^2\end{smallmatrix}\right),"Let $n \in \Bbb N$ and consider the square matrix $$A_n =\begin{pmatrix} n^2 & 1\\-1 & n^2\end{pmatrix}.$$ Prove that there are sequence, $x_n,y_n$ such that    $$A_1A_2\cdots A_n =\begin{pmatrix} x_n&y_n\\-y_n& x_n\end{pmatrix}.$$ Find the explicit expression of $x_n$ and $y_n$. What can we say about the convergence of  $x_n$ and $y_n$? I have shown the existence of  $x_n$ and $y_n$ by induction and it turn out after identification that they satisfy the relations,  $$ x_{n+1} =(n+1)^2x_n-y_n, \qquad\qquad y_{n+1} = x_n +(n+1)^2y_n$$ Can someone help to solve this recursive formula in other to get into the two last questions? Is there a more elegant way to overcome this problem?","Let $n \in \Bbb N$ and consider the square matrix $$A_n =\begin{pmatrix} n^2 & 1\\-1 & n^2\end{pmatrix}.$$ Prove that there are sequence, $x_n,y_n$ such that    $$A_1A_2\cdots A_n =\begin{pmatrix} x_n&y_n\\-y_n& x_n\end{pmatrix}.$$ Find the explicit expression of $x_n$ and $y_n$. What can we say about the convergence of  $x_n$ and $y_n$? I have shown the existence of  $x_n$ and $y_n$ by induction and it turn out after identification that they satisfy the relations,  $$ x_{n+1} =(n+1)^2x_n-y_n, \qquad\qquad y_{n+1} = x_n +(n+1)^2y_n$$ Can someone help to solve this recursive formula in other to get into the two last questions? Is there a more elegant way to overcome this problem?",,"['sequences-and-series', 'matrices']"
53,If $A$ is $2\times 2$ skew orthogonal with $A^TA=-I$ and $\det A=1$ then $\operatorname{tr} A=0$,If  is  skew orthogonal with  and  then,A 2\times 2 A^TA=-I \det A=1 \operatorname{tr} A=0,"$A$ is skew orthogonal if $A^TA=-I$ in $\mathbb{Z_p}$ for $p>2$. The general form of $2\times 2$ characteristic polynomial: $x²-(\operatorname{tr} A)x+\det A$. It is given: $\det A=\pm 1$ If $\det A=-1$ then $\operatorname{tr} A= 0,1,-1$. But if $\det A=1$, then $\operatorname{tr} A$ must be $0$ only. What is the reason behind this?","$A$ is skew orthogonal if $A^TA=-I$ in $\mathbb{Z_p}$ for $p>2$. The general form of $2\times 2$ characteristic polynomial: $x²-(\operatorname{tr} A)x+\det A$. It is given: $\det A=\pm 1$ If $\det A=-1$ then $\operatorname{tr} A= 0,1,-1$. But if $\det A=1$, then $\operatorname{tr} A$ must be $0$ only. What is the reason behind this?",,['linear-algebra']
54,Spectral decomposition of some special matrix,Spectral decomposition of some special matrix,,"Let $A_{n\times n} = aI+bJ$, where $I$ is the identity matrix and $J$ is the matrix of all ones. Is it possible to find the expression of $A^{1/2}$ such that $A^{1/2}A^{1/2} = A$? In particular $A = I_{n\times n} - \frac{(1-\alpha)}{n+\alpha(2-n)}J_{n\times n}$, where $0<\alpha<1$.","Let $A_{n\times n} = aI+bJ$, where $I$ is the identity matrix and $J$ is the matrix of all ones. Is it possible to find the expression of $A^{1/2}$ such that $A^{1/2}A^{1/2} = A$? In particular $A = I_{n\times n} - \frac{(1-\alpha)}{n+\alpha(2-n)}J_{n\times n}$, where $0<\alpha<1$.",,['matrices']
55,"If $Ax \in \langle x \rangle=\{ax:a\in \mathbb{R}\}$ for every vector $x$, then $A$ is square and diagonal.","If  for every vector , then  is square and diagonal.",Ax \in \langle x \rangle=\{ax:a\in \mathbb{R}\} x A,"I'm asked to prove or disprove the title statement. I'm looking for verification/critique of my proof. This is false. This statement is claiming that if every vector $x$ is an eigenvector for $A$ corresponding to a real eigenvalue, then $A$ must be square and diagonal. Symbolically, $\forall x \exists a\in\mathbb{R}$ s.t. $(A-aI)x=0\Rightarrow A$ is square and diagonal. It is true that $A$ is square, as $A-aI$ is only defined for square $A$. However, it is not required that $A$ be diagonal. Consider, for counterexample, $A_{2\times2}=J_{2\times2}$, the $2\times2$ matrix of all 1's, and let $x=[x_1,x_2]^T$. Solving the system of equations $(J-aI)x=0$, we get that $-ax_1+ax_2=0$. Thus, $A_{2\times2}=J_{2\times2}$ has every vector $x$ as an eigenvector corresponding to the eigenvalue 0.","I'm asked to prove or disprove the title statement. I'm looking for verification/critique of my proof. This is false. This statement is claiming that if every vector $x$ is an eigenvector for $A$ corresponding to a real eigenvalue, then $A$ must be square and diagonal. Symbolically, $\forall x \exists a\in\mathbb{R}$ s.t. $(A-aI)x=0\Rightarrow A$ is square and diagonal. It is true that $A$ is square, as $A-aI$ is only defined for square $A$. However, it is not required that $A$ be diagonal. Consider, for counterexample, $A_{2\times2}=J_{2\times2}$, the $2\times2$ matrix of all 1's, and let $x=[x_1,x_2]^T$. Solving the system of equations $(J-aI)x=0$, we get that $-ax_1+ax_2=0$. Thus, $A_{2\times2}=J_{2\times2}$ has every vector $x$ as an eigenvector corresponding to the eigenvalue 0.",,"['linear-algebra', 'matrices', 'proof-verification', 'proof-writing']"
56,Why does an orthogonal matrix have a transpose that equals its inverse?,Why does an orthogonal matrix have a transpose that equals its inverse?,,Wikipedia says the following: How does it follow from the fact that an orthogonal matrix whose columns are orthonormal that the transpose of the matrix is its inverse?,Wikipedia says the following: How does it follow from the fact that an orthogonal matrix whose columns are orthonormal that the transpose of the matrix is its inverse?,,"['linear-algebra', 'matrices', 'orthogonality']"
57,Compute $A^n$ where $A^2+bA+cI=0$,Compute  where,A^n A^2+bA+cI=0,"Let $A$ be a complex matrix such that $$A^2+bA+cI=0,$$ where $I$ is the identity matrix and $b,c\in \mathbb{C}$. I am interested in finding a formula for $A^n$ in terms of $A$ and $I$. The binomial formula  is not giving an answer I think. Maybe using $A^2=-bA-cI$, then $A^3=-bA^2-cA$, etc. But the computation becomes so complicated to find a formula by induction.","Let $A$ be a complex matrix such that $$A^2+bA+cI=0,$$ where $I$ is the identity matrix and $b,c\in \mathbb{C}$. I am interested in finding a formula for $A^n$ in terms of $A$ and $I$. The binomial formula  is not giving an answer I think. Maybe using $A^2=-bA-cI$, then $A^3=-bA^2-cA$, etc. But the computation becomes so complicated to find a formula by induction.",,"['linear-algebra', 'matrices', 'matrix-equations']"
58,How to extend the matrix with determinant 1 to keep it,How to extend the matrix with determinant 1 to keep it,,"Lets consider 2x2 integer matrix with determinant equal 1: $$\left( \begin{array}{cc}  a & b \\  c & d \\ \end{array} \right)$$ I am working on the following: How to extend this to 3x3 matrix in order to get another matrix with determinant 1: $$\left( \begin{array}{ccc}  a & b & e \\  c & d & f \\  i & h & g \\ \end{array} \right)$$ And also is there any $a,b,c,d$ for which this extension is unique. I even have no idea how to start solving this.  I have discovered the following so far on the web, but not sure how to use this: Integer matrices with determinant equal to $1$ https://mathoverflow.net/questions/24131/is-the-semigroup-of-nonnegative-integer-matrices-with-determinant-1-finitely-gen EDITED: Actually I am looking for general algorithm, how to construct all 3x3 matricies from 2x2 matrix with determinant 1. EDITED 2: Some samples of such matricies: $$\left( \begin{array}{ccc}  1 & 1 & 1 \\  -1 & 0 & 1 \\  -1 & 0 & 2 \\ \end{array} \right) $$ $$\left( \begin{array}{ccc}  1 & 1 & 1 \\  1 & 2 & 3 \\  2 & 5 & 9 \\ \end{array} \right)$$ $$\left( \begin{array}{ccc}  1 & 1 & 1 \\  -6 & -5 & -4 \\  9 & 5 & 2 \\ \end{array} \right) $$","Lets consider 2x2 integer matrix with determinant equal 1: $$\left( \begin{array}{cc}  a & b \\  c & d \\ \end{array} \right)$$ I am working on the following: How to extend this to 3x3 matrix in order to get another matrix with determinant 1: $$\left( \begin{array}{ccc}  a & b & e \\  c & d & f \\  i & h & g \\ \end{array} \right)$$ And also is there any $a,b,c,d$ for which this extension is unique. I even have no idea how to start solving this.  I have discovered the following so far on the web, but not sure how to use this: Integer matrices with determinant equal to $1$ https://mathoverflow.net/questions/24131/is-the-semigroup-of-nonnegative-integer-matrices-with-determinant-1-finitely-gen EDITED: Actually I am looking for general algorithm, how to construct all 3x3 matricies from 2x2 matrix with determinant 1. EDITED 2: Some samples of such matricies: $$\left( \begin{array}{ccc}  1 & 1 & 1 \\  -1 & 0 & 1 \\  -1 & 0 & 2 \\ \end{array} \right) $$ $$\left( \begin{array}{ccc}  1 & 1 & 1 \\  1 & 2 & 3 \\  2 & 5 & 9 \\ \end{array} \right)$$ $$\left( \begin{array}{ccc}  1 & 1 & 1 \\  -6 & -5 & -4 \\  9 & 5 & 2 \\ \end{array} \right) $$",,"['matrices', 'number-theory', 'determinant', 'diophantine-equations']"
59,is taking the transpose of a matrix a continuous operation [closed],is taking the transpose of a matrix a continuous operation [closed],,Closed . This question needs details or clarity . It is not currently accepting answers. Want to improve this question? Add details and clarify the problem by editing this post . Closed 6 years ago . Improve this question Suppose $\{A_i\}_{i=1}^{\infty}$ is a sequence of matrices with $\lim_{i\to\infty}A_i= B$. Then does  $\lim_{i\to \infty}A_i^\top= B^\top$ if we measure distances between matrices with a 2-norm?,Closed . This question needs details or clarity . It is not currently accepting answers. Want to improve this question? Add details and clarify the problem by editing this post . Closed 6 years ago . Improve this question Suppose $\{A_i\}_{i=1}^{\infty}$ is a sequence of matrices with $\lim_{i\to\infty}A_i= B$. Then does  $\lim_{i\to \infty}A_i^\top= B^\top$ if we measure distances between matrices with a 2-norm?,,"['linear-algebra', 'matrices', 'continuity']"
60,Prove $\det(1+tA)=1+t\cdot tr(A)+O(t^2)$,Prove,\det(1+tA)=1+t\cdot tr(A)+O(t^2),"I need help proving $\det(1+tA)=1+t\cdot \operatorname{tr}(A)+O(t^2)$ I'm not really sure where to start due to the $(1+tA)$, the $1$ is throwing me off.","I need help proving $\det(1+tA)=1+t\cdot \operatorname{tr}(A)+O(t^2)$ I'm not really sure where to start due to the $(1+tA)$, the $1$ is throwing me off.",,"['linear-algebra', 'matrices', 'determinant', 'trace']"
61,What matrix corresponds to differentiation?,What matrix corresponds to differentiation?,,"Today, in my linear algebra class, we discussed how differentiation of polynomials of degree at most $4$ can be defined using the following matrix $$\begin{bmatrix} 0 & 1 & 0 & 0 & 0 \\ 0 & 0 & 2 & 0 & 0 \\ 0 & 0 & 0 & 3 & 0 \\ 0 & 0 & 0 & 0 & 4\end{bmatrix}$$ since differentiation is a linear transformation. My question is the following Is there a way to represent differentiation using a matrix when we consider the space of all differentiable functions with domain and codomain $\mathbb{R}$? Note: I've studied set theory, so if the matrix has uncountably many entries, that's fine. Axiom of choice is fine as well.","Today, in my linear algebra class, we discussed how differentiation of polynomials of degree at most $4$ can be defined using the following matrix $$\begin{bmatrix} 0 & 1 & 0 & 0 & 0 \\ 0 & 0 & 2 & 0 & 0 \\ 0 & 0 & 0 & 3 & 0 \\ 0 & 0 & 0 & 0 & 4\end{bmatrix}$$ since differentiation is a linear transformation. My question is the following Is there a way to represent differentiation using a matrix when we consider the space of all differentiable functions with domain and codomain $\mathbb{R}$? Note: I've studied set theory, so if the matrix has uncountably many entries, that's fine. Axiom of choice is fine as well.",,"['linear-algebra', 'matrices', 'functional-analysis', 'derivatives', 'linear-transformations']"
62,singluar values unchanged when multiplied with an orthogonal matrix?,singluar values unchanged when multiplied with an orthogonal matrix?,,"revisiting singular values of ""rotated"" matrix Maybe I've since forgotten - or maybe I never bothered to ask myself - but Since $A^T$ has the same singular values as $A$ and $K$ is orthogonal, conclude that [        $A^\top K$        ] indeed has the same singular values. Why does this hold? Why can we say that multiplying by an orthogonal matrix retains the singular values?","revisiting singular values of ""rotated"" matrix Maybe I've since forgotten - or maybe I never bothered to ask myself - but Since $A^T$ has the same singular values as $A$ and $K$ is orthogonal, conclude that [        $A^\top K$        ] indeed has the same singular values. Why does this hold? Why can we say that multiplying by an orthogonal matrix retains the singular values?",,"['linear-algebra', 'matrices', 'matrix-decomposition', 'svd', 'singular-values']"
63,Matrix condition number and loss of accuracy,Matrix condition number and loss of accuracy,,"There are quite a few sources online that say something along the lines of : ""As a rule of thumb, if the condition number $\kappa(A)=10^k$ then you may lose up to $k$ digits of accuracy on top of what would be lost to the numerical method due to loss of precision from arithmetic methods."" What's the reason for this?","There are quite a few sources online that say something along the lines of : ""As a rule of thumb, if the condition number $\kappa(A)=10^k$ then you may lose up to $k$ digits of accuracy on top of what would be lost to the numerical method due to loss of precision from arithmetic methods."" What's the reason for this?",,"['linear-algebra', 'matrices', 'numerical-linear-algebra', 'condition-number']"
64,Guarantee of matrix inverse for $A'A$,Guarantee of matrix inverse for,A'A,"Suppose I have a matrix $A$. Let $A'$ be the transpose of A. Is there any guarantee that $A'A$ has an inverse? You can find this formula at Adaptive Neuro-Fuzzy Inference System (ANFIS) theory in ""LSE Recursive"" part. Thank you.","Suppose I have a matrix $A$. Let $A'$ be the transpose of A. Is there any guarantee that $A'A$ has an inverse? You can find this formula at Adaptive Neuro-Fuzzy Inference System (ANFIS) theory in ""LSE Recursive"" part. Thank you.",,"['linear-algebra', 'matrices', 'inverse', 'least-squares']"
65,Which matrices commute with A symmetric positive-definite?,Which matrices commute with A symmetric positive-definite?,,"I have a symmetric positive-definite matrix $A\in R_{n\times n}$. Its eigenvectors $e_i$ are an orthonormal basis of $R_n$. Are the $n^2$ matrices $[e_i\,e_j^T]$ a basis of $R_{n\times n}$? I have noticed an interesting property: $[e_i\,e_j^T]$ commute with $A$ iff $\lambda_i=\lambda_j$ If $A$ does not commute with $[e_a\,e_b^T]$ and $[e_c\,e_d^T]$, can it commute with $\left( [e_a\,e_b^T]+[e_c\,e_d^T] \right)$? Would the matrices $[\bar{e}_i\,\bar{e}_j^T]$ (eigenvectors with the same eigenvalue) generate the space of all matrices that commute with $A$?","I have a symmetric positive-definite matrix $A\in R_{n\times n}$. Its eigenvectors $e_i$ are an orthonormal basis of $R_n$. Are the $n^2$ matrices $[e_i\,e_j^T]$ a basis of $R_{n\times n}$? I have noticed an interesting property: $[e_i\,e_j^T]$ commute with $A$ iff $\lambda_i=\lambda_j$ If $A$ does not commute with $[e_a\,e_b^T]$ and $[e_c\,e_d^T]$, can it commute with $\left( [e_a\,e_b^T]+[e_c\,e_d^T] \right)$? Would the matrices $[\bar{e}_i\,\bar{e}_j^T]$ (eigenvectors with the same eigenvalue) generate the space of all matrices that commute with $A$?",,"['linear-algebra', 'matrices', 'eigenvalues-eigenvectors']"
66,vector notation – for every element bigger $0$,vector notation – for every element bigger,0,"I was wondering what the correct notation would be for the following: a vector $x$ for which every element smaller $0$ is equal to $0$. so, for example: $x = [1, 2, -1, 3, 4, -2]$, accordingly, $x' = [1, 2, 0, 3, 4, 0]$ so how would you write: $x$ where $x_1,\ldots,x_n = 0$ if $x_1,\ldots,x_n < 0$ more concisely / correctly?","I was wondering what the correct notation would be for the following: a vector $x$ for which every element smaller $0$ is equal to $0$. so, for example: $x = [1, 2, -1, 3, 4, -2]$, accordingly, $x' = [1, 2, 0, 3, 4, 0]$ so how would you write: $x$ where $x_1,\ldots,x_n = 0$ if $x_1,\ldots,x_n < 0$ more concisely / correctly?",,"['matrices', 'notation', 'vectors']"
67,distance SO(3) rotation matrix,distance SO(3) rotation matrix,,"According to M. Moakher's Means and averaging in the group of rotations and I. Sharf's Arithmetic and geometric solutions for average rigid-body rotation the distance between two rotation matrices is $$\| R_1 - R_2 \|_{\text{F}}$$ where $\| \cdot \|_{\text{F}}$ denotes the Frobenious norm. Does it mean $\left\| R_1 - R_2 \right\|_{\text{F}}$ or $\left\|  R_1^T R_2 \right\|_{\text{F}}$ ? $\left\| R_1 - R_2 \right\|_{\text{F}}$ does not make sense as $(R_1-R_2) \notin$ SO (3). $\left\|  R_1^T R_2 \right\|_{\text{F}}$ is also strange: suppose $R_1=R_2$ , the distance is $3$ . I find out that  the distance between two rotation matrices is less than $3$ . Such metric is against my intuition (the distance between two identical elements is largest!).","According to M. Moakher's Means and averaging in the group of rotations and I. Sharf's Arithmetic and geometric solutions for average rigid-body rotation the distance between two rotation matrices is where denotes the Frobenious norm. Does it mean or ? does not make sense as SO (3). is also strange: suppose , the distance is . I find out that  the distance between two rotation matrices is less than . Such metric is against my intuition (the distance between two identical elements is largest!).",\| R_1 - R_2 \|_{\text{F}} \| \cdot \|_{\text{F}} \left\| R_1 - R_2 \right\|_{\text{F}} \left\|  R_1^T R_2 \right\|_{\text{F}} \left\| R_1 - R_2 \right\|_{\text{F}} (R_1-R_2) \notin \left\|  R_1^T R_2 \right\|_{\text{F}} R_1=R_2 3 3,"['matrices', 'lie-groups', 'orthogonal-matrices']"
68,Finding the eigenvalues and a basis for the eigenspaces of a $3\times3$ matrix.,Finding the eigenvalues and a basis for the eigenspaces of a  matrix.,3\times3,"For the  matrix $A \in M_{3\times3}(\mathbb{R})$ below, I need to find the eigenvalues and a basis for the corresponding eigenspaces: $$\begin{bmatrix}\ 1 & -3 & 3 \\            3 & -5 & 3 \\ 6 & -6 & 4 \\ \end{bmatrix}$$ I have tried to use the formula $\det(I\lambda - A) = 0$ but I ended up with equation $\lambda^3 - 12\lambda - 16 = 0$, of which I can't seem to find the solutions to.","For the  matrix $A \in M_{3\times3}(\mathbb{R})$ below, I need to find the eigenvalues and a basis for the corresponding eigenspaces: $$\begin{bmatrix}\ 1 & -3 & 3 \\            3 & -5 & 3 \\ 6 & -6 & 4 \\ \end{bmatrix}$$ I have tried to use the formula $\det(I\lambda - A) = 0$ but I ended up with equation $\lambda^3 - 12\lambda - 16 = 0$, of which I can't seem to find the solutions to.",,"['linear-algebra', 'matrices', 'eigenvalues-eigenvectors']"
69,An $n\times n$ matrix $A$ such that $PA^T = AP$,An  matrix  such that,n\times n A PA^T = AP,"Suppose that I have an $n\times n$ real matrix $A$ such that $PA^T = AP$ for every invertible matrix $P$. Does this imply that $A$ is a multiple of the identity matrix (i.e., $A= c\, I_{n\times n}$ for some $c\in \mathbb{R}$.) ?","Suppose that I have an $n\times n$ real matrix $A$ such that $PA^T = AP$ for every invertible matrix $P$. Does this imply that $A$ is a multiple of the identity matrix (i.e., $A= c\, I_{n\times n}$ for some $c\in \mathbb{R}$.) ?",,"['linear-algebra', 'matrices']"
70,Writing columns of a matrix as linear combinations of other columns,Writing columns of a matrix as linear combinations of other columns,,Let A be the matrix: $$\begin{pmatrix} 1&2&3&2&1&0\\2&4&5&3&3&1\\1&2&2&1&2&1 \end{pmatrix}$$ What is the best way to write the fifth and sixth columns of the matrix as linear combinations of the first and third columns?,Let A be the matrix: $$\begin{pmatrix} 1&2&3&2&1&0\\2&4&5&3&3&1\\1&2&2&1&2&1 \end{pmatrix}$$ What is the best way to write the fifth and sixth columns of the matrix as linear combinations of the first and third columns?,,"['linear-algebra', 'matrices', 'vector-spaces']"
71,The diagonalizable matrices are not dense in the square real matrices,The diagonalizable matrices are not dense in the square real matrices,,Suppose that $n \ge 2$. How to prove that the set $\mathcal D \subset M_n(\mathbb R)$ of the diagonalizable real matrices is not dense in $M_n(\mathbb R)$?,Suppose that $n \ge 2$. How to prove that the set $\mathcal D \subset M_n(\mathbb R)$ of the diagonalizable real matrices is not dense in $M_n(\mathbb R)$?,,"['general-topology', 'matrices', 'diagonalization']"
72,A geometric matrix inequality,A geometric matrix inequality,,"Let $0 < \sigma_i \in \mathbb{R}$ ($i=1 \dots d$). Is it true that $$ c \sum_{i=1}^d (\sigma_i-1)^2 \le \sqrt{\sum_{i=1}^d (\sigma_i^2-1)^2} \tag{1}$$ for some $c>0$ which does not depend on the $\sigma_i$. (I actually suspect this holds for $c=1$). Motivation: This inequality is equivalent to the following $$c \|\sqrt{A^TA}-I\|^2 \le \|A^TA-I\|, \tag{2}$$ where $A$ is a $d \times d$ real matrix. (The equivalence is obtained by considering SVD). (Inequality $(2)$ comes from comparing different ways to measure deviation of a linear transformation from being an isometry).","Let $0 < \sigma_i \in \mathbb{R}$ ($i=1 \dots d$). Is it true that $$ c \sum_{i=1}^d (\sigma_i-1)^2 \le \sqrt{\sum_{i=1}^d (\sigma_i^2-1)^2} \tag{1}$$ for some $c>0$ which does not depend on the $\sigma_i$. (I actually suspect this holds for $c=1$). Motivation: This inequality is equivalent to the following $$c \|\sqrt{A^TA}-I\|^2 \le \|A^TA-I\|, \tag{2}$$ where $A$ is a $d \times d$ real matrix. (The equivalence is obtained by considering SVD). (Inequality $(2)$ comes from comparing different ways to measure deviation of a linear transformation from being an isometry).",,"['linear-algebra', 'matrices', 'inequality', 'matrix-calculus']"
73,Inverse of integer matrix with determinant $\pm 1$,Inverse of integer matrix with determinant,\pm 1,Suppose we have a matrix $A = (a_{ij}) \in \operatorname{GL}_n(\mathbb{R})$ with $a_{ij} \in \mathbb{Z}$. I need to show that $A^{-1}$ has entries in $\mathbb{Z}$ if and only if $\det(A) = \pm 1$.,Suppose we have a matrix $A = (a_{ij}) \in \operatorname{GL}_n(\mathbb{R})$ with $a_{ij} \in \mathbb{Z}$. I need to show that $A^{-1}$ has entries in $\mathbb{Z}$ if and only if $\det(A) = \pm 1$.,,"['matrices', 'determinant']"
74,$A^{2016}-2A^{3016}+A=0$,,A^{2016}-2A^{3016}+A=0,"I'm learning linear algebra and need help with the following problem: Let $A = \begin{pmatrix}-2 & 4 & 3\\0 & 0 & 0\\-1 & 5 & 2\end{pmatrix} \in M_{3x3}(\mathbb{R})$. Show that $A^{2016}-2A^{3016}+A=0$. I guess this is a direct application of the Cayley-Hamilton theorem which states that every matrix satisfies its own characteristic equation. The characteristic polynomial of $A$ is  $p_{A}(\lambda) = \lambda - \lambda^{3}$ (I skipped the easy computation of the determinant to save me some time). Hence, by the Cayley-Hamilton theorem $$A - A^{3} = 0 \tag{*}$$ How should I make use of $(*)$ and continue from here to prove the identity? I thought I could write $(*)$ as $A = A^{3}$ and then appropriately multiply both sides of latter equality but I got stuck. I'm also interested to know if there are other methods to solve this problem. EDIT: As A.G. demonstrated, the identity is not true. It would be true for odd powers, e.g. $A^{2017}-2A^{3017}+A=0$. This is an unfortunate typo from my teacher's notes. I apologize to the users who gave answers prior to this edit.","I'm learning linear algebra and need help with the following problem: Let $A = \begin{pmatrix}-2 & 4 & 3\\0 & 0 & 0\\-1 & 5 & 2\end{pmatrix} \in M_{3x3}(\mathbb{R})$. Show that $A^{2016}-2A^{3016}+A=0$. I guess this is a direct application of the Cayley-Hamilton theorem which states that every matrix satisfies its own characteristic equation. The characteristic polynomial of $A$ is  $p_{A}(\lambda) = \lambda - \lambda^{3}$ (I skipped the easy computation of the determinant to save me some time). Hence, by the Cayley-Hamilton theorem $$A - A^{3} = 0 \tag{*}$$ How should I make use of $(*)$ and continue from here to prove the identity? I thought I could write $(*)$ as $A = A^{3}$ and then appropriately multiply both sides of latter equality but I got stuck. I'm also interested to know if there are other methods to solve this problem. EDIT: As A.G. demonstrated, the identity is not true. It would be true for odd powers, e.g. $A^{2017}-2A^{3017}+A=0$. This is an unfortunate typo from my teacher's notes. I apologize to the users who gave answers prior to this edit.",,['linear-algebra']
75,Prove that a doubly stochastic matrix is a square matrix,Prove that a doubly stochastic matrix is a square matrix,,A matrix denoted by $(a_{ij})_{m \times n}$  is said to be doubly stochastic if: $$ \sum_{i}{a_{ij}} = \sum_{j}{a_{ij}} = 1 $$ I am trying to prove that such a matrix is a square matrix. I thought of multiplying or adding 2 of such matrices together but this leads to nowhere. I also wrote down down a generalised doubly stochastic matrix in this form: $$ \begin{bmatrix}a_{11} & a_{12} & \cdots & a_{1n}\\a_{21} & a_{22} & \cdots & a_{2n} \\ \vdots & \vdots & \vdots & \vdots \\ a_{m1} & a_{m2} & \cdots & a_{mn} \end{bmatrix} $$ I can't see any relationship between the different rows and columns from this form. Could anyone please give me some hints?,A matrix denoted by $(a_{ij})_{m \times n}$  is said to be doubly stochastic if: $$ \sum_{i}{a_{ij}} = \sum_{j}{a_{ij}} = 1 $$ I am trying to prove that such a matrix is a square matrix. I thought of multiplying or adding 2 of such matrices together but this leads to nowhere. I also wrote down down a generalised doubly stochastic matrix in this form: $$ \begin{bmatrix}a_{11} & a_{12} & \cdots & a_{1n}\\a_{21} & a_{22} & \cdots & a_{2n} \\ \vdots & \vdots & \vdots & \vdots \\ a_{m1} & a_{m2} & \cdots & a_{mn} \end{bmatrix} $$ I can't see any relationship between the different rows and columns from this form. Could anyone please give me some hints?,,"['linear-algebra', 'matrices']"
76,"If matrix A commutes with B, and B commutes with C, and B is non invertible, is it true that A commutes with C?","If matrix A commutes with B, and B commutes with C, and B is non invertible, is it true that A commutes with C?",,"Original question asked: If matrix $A$ commutes with $B$, and $B$ commutes with $C$, then does matrix $A$ commute with $C$? This can easily be disproven by taking $B = I$ and looking at some matrices $A$ and $C$ that doesn't satisfy the condition. However, I thought: if $B$ is given to be non-invertible, is the case true? If so, why? If not, can you provide an example? Also, if not, are there any stronger conditions that would make the case true (for example, $A$ and $C$ has to be both non-invertible)?","Original question asked: If matrix $A$ commutes with $B$, and $B$ commutes with $C$, then does matrix $A$ commute with $C$? This can easily be disproven by taking $B = I$ and looking at some matrices $A$ and $C$ that doesn't satisfy the condition. However, I thought: if $B$ is given to be non-invertible, is the case true? If so, why? If not, can you provide an example? Also, if not, are there any stronger conditions that would make the case true (for example, $A$ and $C$ has to be both non-invertible)?",,['matrices']
77,Find a value such that linear system has no solution,Find a value such that linear system has no solution,,"Okay, so this is probably an elementary level question, but I am going to ask anyways since I cannot figure out what to do next. Given the system of equations: x + 2y - 5z = 1 x + y + 4z = 1 4x + 10y + kz = 5 Find a k such that there is no solution to the system.  How I started on the problem: From looking at the first two equation I figured out that y=9z . I am not sure what to do with this information. My reasoning could be wrong but I feel that there would be many k such that there would be no solution and only one that there would be a solution. Am I wrong in my reasoning? I also know that if I could get a row in the matrix that was untrue there would be no solution.","Okay, so this is probably an elementary level question, but I am going to ask anyways since I cannot figure out what to do next. Given the system of equations: x + 2y - 5z = 1 x + y + 4z = 1 4x + 10y + kz = 5 Find a k such that there is no solution to the system.  How I started on the problem: From looking at the first two equation I figured out that y=9z . I am not sure what to do with this information. My reasoning could be wrong but I feel that there would be many k such that there would be no solution and only one that there would be a solution. Am I wrong in my reasoning? I also know that if I could get a row in the matrix that was untrue there would be no solution.",,"['linear-algebra', 'matrices', 'systems-of-equations']"
78,How to extract the main diagonal of a matrix?,How to extract the main diagonal of a matrix?,,"I know my question is very silly, but I cannot figure it out. I have a $m \times m$ matrix $A$. I want to create vector $B$ such that its elements are the diagonal elements of matrix $A$. i.e. $i=j$, there for size of vector $B$ is gonna be $1 \times m$. Can you help me how to right in math in a correct way? Thanks.","I know my question is very silly, but I cannot figure it out. I have a $m \times m$ matrix $A$. I want to create vector $B$ such that its elements are the diagonal elements of matrix $A$. i.e. $i=j$, there for size of vector $B$ is gonna be $1 \times m$. Can you help me how to right in math in a correct way? Thanks.",,"['linear-algebra', 'matrices']"
79,Proving that $p = \inf\{\|Ax-b\|: x\in\mathbb{R}^n\}$ is attained,Proving that  is attained,p = \inf\{\|Ax-b\|: x\in\mathbb{R}^n\},"I'm trying to solve the following problem in my textbook: Let $A$ be an $m \times n$ matrix of unspecified rank, $b\in\mathbb{R}^n$ and let $p =\inf\{\|Ax-b\|: x\in\mathbb{R}^n\}$ (the norm is abitrary on $\mathbb{R}^n$ ). Show that this infimum is attained (meaning, proving the existence of an $x$ for which $\|Ax-b\| = p$ ). I'm having a lot of trouble figuring out how to exactly prove this. In the problems chapter, I have been introduced to ""Least-squares problems"", which is the first thing I thought of. The problem is that the least-square method uses a specific norm (problem uses an abitrary) and also assumes that the rank is $n$ , and thereby $n\le m$ (problem has unspecified rank). Another thought was to introduce $b'$ , with property $\|b'-b\|=p$ , and then prove that a solution to $Ax=b'$ exists — but as far as I know, that would again depend on the actual rank of $A$ , and I'm not too sure that this is the correct way to proceed. Would appreciate any hints on how one might tackle on such a proof?","I'm trying to solve the following problem in my textbook: Let be an matrix of unspecified rank, and let (the norm is abitrary on ). Show that this infimum is attained (meaning, proving the existence of an for which ). I'm having a lot of trouble figuring out how to exactly prove this. In the problems chapter, I have been introduced to ""Least-squares problems"", which is the first thing I thought of. The problem is that the least-square method uses a specific norm (problem uses an abitrary) and also assumes that the rank is , and thereby (problem has unspecified rank). Another thought was to introduce , with property , and then prove that a solution to exists — but as far as I know, that would again depend on the actual rank of , and I'm not too sure that this is the correct way to proceed. Would appreciate any hints on how one might tackle on such a proof?",A m \times n b\in\mathbb{R}^n p =\inf\{\|Ax-b\|: x\in\mathbb{R}^n\} \mathbb{R}^n x \|Ax-b\| = p n n\le m b' \|b'-b\|=p Ax=b' A,"['linear-algebra', 'matrices', 'matrix-equations']"
80,"What is the rank of $\alpha I + \beta P$ with $P^2=P$ and $\alpha,\beta>0$?",What is the rank of  with  and ?,"\alpha I + \beta P P^2=P \alpha,\beta>0","I have been working with projection matrices lately  and I have one additional question: Is it true a statement that linear combination of identity matrix $I$ of rank $n$ and any projection matrix $P$ of rank less than $n$ where coefficients are positive i.e. matrix $A =  {\alpha}I+{\beta}P$ ,    with $  {\alpha}>0, {\beta}>0 ,$ has always rank $n$ ? If so how to prove it? If so the same is true of course for powers $A^m$ .","I have been working with projection matrices lately  and I have one additional question: Is it true a statement that linear combination of identity matrix of rank and any projection matrix of rank less than where coefficients are positive i.e. matrix ,    with has always rank ? If so how to prove it? If so the same is true of course for powers .","I n P n A =  {\alpha}I+{\beta}P   {\alpha}>0, {\beta}>0 , n A^m","['linear-algebra', 'matrices', 'projection-matrices']"
81,Is matrix with entries $a_{ij} = x_ix_j$ positive semidefinite?,Is matrix with entries  positive semidefinite?,a_{ij} = x_ix_j,I wonder if I can proof that the square matrix with entries $a_{ij} = x_ix_j$ for a given vector $x$ is positive semidefinite. I hope it is because this matrix is somehow related to this question where I asked if mean square error is convex function in linear regression. I actually calculated the Hessian matrix and obtained the one I give you (multiplied by 2). The problem is that I don't know how to proof that it is positive semidefinite so that I can show that mean square error is convex. Please give a proof or a counterexample.,I wonder if I can proof that the square matrix with entries $a_{ij} = x_ix_j$ for a given vector $x$ is positive semidefinite. I hope it is because this matrix is somehow related to this question where I asked if mean square error is convex function in linear regression. I actually calculated the Hessian matrix and obtained the one I give you (multiplied by 2). The problem is that I don't know how to proof that it is positive semidefinite so that I can show that mean square error is convex. Please give a proof or a counterexample.,,"['real-analysis', 'linear-algebra', 'matrices', 'convex-analysis', 'hessian-matrix']"
82,General form of a matrix that is both centrosymmetric and orthogonal,General form of a matrix that is both centrosymmetric and orthogonal,,"A centrosymmetric matrix is of the form $JAJ=A$ where $J$ is the counter identity matrix, i.e. $$J=         \begin{bmatrix}         0 & 0 & 1 \\         0 & 1 & 0 \\         1 & 0 & 0 \\         \end{bmatrix} $$And an orthogonal matrix is $A$ such that $A^{-1}=A^T$ The only such matrix I could think of having these property is the identity matrix $$I=         \begin{bmatrix}         1 & 0 & 0 \\         0 & 1 & 0 \\         0 & 0 & 1 \\         \end{bmatrix} $$Is there a general form that A takes that has these properties?","A centrosymmetric matrix is of the form $JAJ=A$ where $J$ is the counter identity matrix, i.e. $$J=         \begin{bmatrix}         0 & 0 & 1 \\         0 & 1 & 0 \\         1 & 0 & 0 \\         \end{bmatrix} $$And an orthogonal matrix is $A$ such that $A^{-1}=A^T$ The only such matrix I could think of having these property is the identity matrix $$I=         \begin{bmatrix}         1 & 0 & 0 \\         0 & 1 & 0 \\         0 & 0 & 1 \\         \end{bmatrix} $$Is there a general form that A takes that has these properties?",,"['linear-algebra', 'matrices']"
83,n mode product and kronecker product relation,n mode product and kronecker product relation,,"I am reading tensor decompositions and applications , by Tamara Kolda. There she mentions a property of $\mathscr{Y} = \mathscr{X} \times_1 A^{(1)} \times_2 A^{(2)} \ldots \times_N A^{(N)} \iff$ $ Y_{(n)} = A^{(n)}X_{(n)}(A^{(N)}\otimes \ldots \otimes A^{(n+1)} \otimes A^{(n-1)} \otimes \ldots \otimes A^{(1)})^T$ Where $\mathscr{X},\mathscr{Y}$ are tensors, and $A^{(i)}$ is a matrix. $X_{(n)}$ is a mode $n$ matricized tensor $\mathscr{X}$. Here $\otimes$ is a kronecker product and $\times_i$ represents $i$ mode  product. I am unable to find/do a proof of it. I tried it with examples, and obviously it worked. What is the proof of it, and what is the intuitive reasoning behind the statement? For the definitions of these things, refer the hyperlink above. This question has been asked before and is left unanswered.","I am reading tensor decompositions and applications , by Tamara Kolda. There she mentions a property of $\mathscr{Y} = \mathscr{X} \times_1 A^{(1)} \times_2 A^{(2)} \ldots \times_N A^{(N)} \iff$ $ Y_{(n)} = A^{(n)}X_{(n)}(A^{(N)}\otimes \ldots \otimes A^{(n+1)} \otimes A^{(n-1)} \otimes \ldots \otimes A^{(1)})^T$ Where $\mathscr{X},\mathscr{Y}$ are tensors, and $A^{(i)}$ is a matrix. $X_{(n)}$ is a mode $n$ matricized tensor $\mathscr{X}$. Here $\otimes$ is a kronecker product and $\times_i$ represents $i$ mode  product. I am unable to find/do a proof of it. I tried it with examples, and obviously it worked. What is the proof of it, and what is the intuitive reasoning behind the statement? For the definitions of these things, refer the hyperlink above. This question has been asked before and is left unanswered.",,"['matrices', 'tensors', 'kronecker-product', 'tensor-decomposition']"
84,Trying to show $\Vert A (\sum_{j=1}^n v_j)\Vert_2^2 = \sum_{j=1}^n\lambda_j^2$?,Trying to show ?,\Vert A (\sum_{j=1}^n v_j)\Vert_2^2 = \sum_{j=1}^n\lambda_j^2,"I have been reading through some lecture notes and I think the author must be mistaken in a particular statement he makes. Suppose $A$ is a symmetric positive definite matrix. Then there exists an orthonormal basis $v_1, \dots, v_n \in \mathbb{R}^n$ of eigenvectors with eigenvalues $0 < \lambda_1 \le \dots \le \lambda_n < \infty$. As the eigenvectors are orthonormal this means that we can write: $$\Vert A (\sum_{j=1}^n v_j)\Vert_2^2 = \sum_{j=1}^n\lambda_j^2$$ I don't think this is correct? I have $$\Vert A (\sum_{j=1}^n v_j)\Vert_2^2 = \Vert \sum_{j=1}^n \lambda_j v_j \Vert_2^2.$$ Taking the $2$ norm gives $$\sum_{i=1}^n |\sum_{j=1}^n \lambda_j v_{j_i}|^2,$$ and I don't think this can be reduced further?","I have been reading through some lecture notes and I think the author must be mistaken in a particular statement he makes. Suppose $A$ is a symmetric positive definite matrix. Then there exists an orthonormal basis $v_1, \dots, v_n \in \mathbb{R}^n$ of eigenvectors with eigenvalues $0 < \lambda_1 \le \dots \le \lambda_n < \infty$. As the eigenvectors are orthonormal this means that we can write: $$\Vert A (\sum_{j=1}^n v_j)\Vert_2^2 = \sum_{j=1}^n\lambda_j^2$$ I don't think this is correct? I have $$\Vert A (\sum_{j=1}^n v_j)\Vert_2^2 = \Vert \sum_{j=1}^n \lambda_j v_j \Vert_2^2.$$ Taking the $2$ norm gives $$\sum_{i=1}^n |\sum_{j=1}^n \lambda_j v_{j_i}|^2,$$ and I don't think this can be reduced further?",,"['matrices', 'eigenvalues-eigenvectors', 'normed-spaces', 'orthogonality']"
85,Determinant of a linear transformation $T.$,Determinant of a linear transformation,T.,Let $M$ be the real vector space of $2\times 3$ matrices with real entries. Let $T:M \rightarrow M$ be defined by  $$T(\begin{bmatrix}     x_{1} & x_{2} & x_{3}  \\     x_{4} & x_{5} & x_{6}  \\ \end{bmatrix})=\begin{bmatrix}     x_{6} & x_{4} & x_{1}  \\     x_{3} & x_{5} & x_{2}  \\ \end{bmatrix}$$ Then the determinant of $T$ is $A. 1.$ $B. 2.$ $C.-1.$ $D.0.$ Now one method is as usual  firstly find the matrix of the transformation $T$ with respect to usual basis of $W$ and then find the determinant of the matrix thus obtained which come as $-1.$ But i want some short trick that gives the determinant in less time. One more thing is that matrix of $T$ with respect to usual basis is an Orthogonal matrix. Please give some suggestion. Thank you.,Let $M$ be the real vector space of $2\times 3$ matrices with real entries. Let $T:M \rightarrow M$ be defined by  $$T(\begin{bmatrix}     x_{1} & x_{2} & x_{3}  \\     x_{4} & x_{5} & x_{6}  \\ \end{bmatrix})=\begin{bmatrix}     x_{6} & x_{4} & x_{1}  \\     x_{3} & x_{5} & x_{2}  \\ \end{bmatrix}$$ Then the determinant of $T$ is $A. 1.$ $B. 2.$ $C.-1.$ $D.0.$ Now one method is as usual  firstly find the matrix of the transformation $T$ with respect to usual basis of $W$ and then find the determinant of the matrix thus obtained which come as $-1.$ But i want some short trick that gives the determinant in less time. One more thing is that matrix of $T$ with respect to usual basis is an Orthogonal matrix. Please give some suggestion. Thank you.,,"['linear-algebra', 'matrices']"
86,Find the eigenvalues of a symmetric matrix,Find the eigenvalues of a symmetric matrix,,"Find the eigenvalues of a $3 \times 3$ symmetric matrix with $1$ on the main diagonal and  $\frac{1}{\sqrt 3}$ off the main diagonal. Since each row on addition give the same value, one of the three eigenvalue is $1+\frac{2}{\sqrt 3}$. Is there an easy way to find the other two values without using the formula $\det(A-\lambda I_3) = 0$.","Find the eigenvalues of a $3 \times 3$ symmetric matrix with $1$ on the main diagonal and  $\frac{1}{\sqrt 3}$ off the main diagonal. Since each row on addition give the same value, one of the three eigenvalue is $1+\frac{2}{\sqrt 3}$. Is there an easy way to find the other two values without using the formula $\det(A-\lambda I_3) = 0$.",,"['matrices', 'eigenvalues-eigenvectors', 'symmetric-matrices', 'circulant-matrices']"
87,Solutions to Binary Equations,Solutions to Binary Equations,,"Let $A \in \mathrm{Mat}(m,n,\{0,1\})$ (i.e. $m \times n$ matrices with entries in $\{0,1\}$ ) and $x,y\in \{0,1\}^n$. We will denote the $i$-th row of $A$ as $\mathrm{row}_i(A)\in \{0,1\}^n$. Define, $$  z_i :=       \begin{cases}        1~: & (x-y)\cdot \mathrm{row}_i(A) = \sum_{j=1}^n x_j\\        0~: & \text{otherwise}\\ \end{cases} $$ where $x_j$ is the $j$-th component of $x$ and $\cdot$ denotes the dot product. Is there an algorithm to determine all $x,y$ given a vector $z=(z_1,...,z_m)$? Example: Let $$A=         \begin{bmatrix}         0 & 0 & 1\\         1 & 1 & 0\\         1 & 0 & 1\\         \end{bmatrix} $$ and $z=(1, 0, 1)$. Then $x=(0,0,1)$ and $y=(0,1,0)$ would satisfy the conditions described.","Let $A \in \mathrm{Mat}(m,n,\{0,1\})$ (i.e. $m \times n$ matrices with entries in $\{0,1\}$ ) and $x,y\in \{0,1\}^n$. We will denote the $i$-th row of $A$ as $\mathrm{row}_i(A)\in \{0,1\}^n$. Define, $$  z_i :=       \begin{cases}        1~: & (x-y)\cdot \mathrm{row}_i(A) = \sum_{j=1}^n x_j\\        0~: & \text{otherwise}\\ \end{cases} $$ where $x_j$ is the $j$-th component of $x$ and $\cdot$ denotes the dot product. Is there an algorithm to determine all $x,y$ given a vector $z=(z_1,...,z_m)$? Example: Let $$A=         \begin{bmatrix}         0 & 0 & 1\\         1 & 1 & 0\\         1 & 0 & 1\\         \end{bmatrix} $$ and $z=(1, 0, 1)$. Then $x=(0,0,1)$ and $y=(0,1,0)$ would satisfy the conditions described.",,"['combinatorics', 'matrices']"
88,Inverse of a matrix and a scalar,Inverse of a matrix and a scalar,,I'm asked o find $$\det((ad-bc)^{-1}\begin{bmatrix}a & b \\ c& d \end{bmatrix})$$ what I did was : This would equal to $$\det (\begin{bmatrix}\cfrac{1}{(ad-bc)} & 0 \\ 0 & \cfrac{1}{(ad-bc)} \end{bmatrix}\begin{bmatrix} a & b \\ c& d\end{bmatrix})$$ and this would also equal to $$\det(\begin{bmatrix}\cfrac{1}{(ad-bc)} & 0 \\ 0 & \cfrac{1}{(ad-bc)} \end{bmatrix})\det(\begin{bmatrix}a & b \\c &d  \end{bmatrix})$$ When calculating this : $$(ad-bc)^{2}(ad-bc)^{-1}$$ is what you end up with so the answer would be $$(ad-bc)^{}$$ but the textbook says that the correct answer is  $$\cfrac{1}{(ad-bc)}$$ How come? What did I do wrong here? I couldn't understand I simply used $$\det(AB) = \det(A)\det(B)$$,I'm asked o find $$\det((ad-bc)^{-1}\begin{bmatrix}a & b \\ c& d \end{bmatrix})$$ what I did was : This would equal to $$\det (\begin{bmatrix}\cfrac{1}{(ad-bc)} & 0 \\ 0 & \cfrac{1}{(ad-bc)} \end{bmatrix}\begin{bmatrix} a & b \\ c& d\end{bmatrix})$$ and this would also equal to $$\det(\begin{bmatrix}\cfrac{1}{(ad-bc)} & 0 \\ 0 & \cfrac{1}{(ad-bc)} \end{bmatrix})\det(\begin{bmatrix}a & b \\c &d  \end{bmatrix})$$ When calculating this : $$(ad-bc)^{2}(ad-bc)^{-1}$$ is what you end up with so the answer would be $$(ad-bc)^{}$$ but the textbook says that the correct answer is  $$\cfrac{1}{(ad-bc)}$$ How come? What did I do wrong here? I couldn't understand I simply used $$\det(AB) = \det(A)\det(B)$$,,['linear-algebra']
89,Prove an equality of complex matrixes,Prove an equality of complex matrixes,,"If $A \in M_2(\mathbb{C})$ a matrix so that $$\det\left(A^2 + A +  I_2\right)=\det\left(A^2 - A + I_2\right)=3 \tag1$$ then $$A^2\left(A^2 + I_2\right)=2I_2. \tag2$$ I tried to use Cayley-Hamilton theorem, without success. I think one step might be to prove $A$ is invertible. Meanwhile I found (2) is equivalent to: $\left(A^2 -I_2\right)\left(A^2 +2I_2\right)=O_2. \tag3$","If $A \in M_2(\mathbb{C})$ a matrix so that $$\det\left(A^2 + A +  I_2\right)=\det\left(A^2 - A + I_2\right)=3 \tag1$$ then $$A^2\left(A^2 + I_2\right)=2I_2. \tag2$$ I tried to use Cayley-Hamilton theorem, without success. I think one step might be to prove $A$ is invertible. Meanwhile I found (2) is equivalent to: $\left(A^2 -I_2\right)\left(A^2 +2I_2\right)=O_2. \tag3$",,['matrices']
90,"From Halmos' ""Finite-Dimensional Vector Spaces"": Similar matrices and transformations paradox","From Halmos' ""Finite-Dimensional Vector Spaces"": Similar matrices and transformations paradox",,"From section 47 called Similarity: (In the following I will represent matrices like $[A]$ and linear transforms as $A$ and also sorry if I am not rigorous enough) Halmos proves that when we have one linear transformation $T:V\longrightarrow V$ with matrix $[B]$ in a basis $X$ (vectors $\ \vec x_1, \vec x_2, .., \vec x_n$)  and matrix $[C]$ in basis $Y$ (vectors $\ \vec y_1, \vec y_2, .., \vec y_n$) and the two bases are related by $[A]x_i=y_i$, then the two matrices are related by $[C]=[A]^{-1}[B][A]$. He also proves that when we have two linear transformations $B$ and$C$ and $[B]=(β_{ij})$ is a matrix and the two transformations are defined as $B\vec x_j= \sum_{i=0}^nβ_{ij}\vec x_i$ and $C\vec y_j= \sum_{i=0}^nβ_{ij}\vec y_i$, the two transformations are related by $C=ABA^{-1} $. While I have proved these things, I can't intuitively(geometrically) understand why the difference in the relation of the transformations with the relation with the matrices , since a matrix is a way to express the transformation on a coordinates system(please correct me if I am wrong as I am not a mathematician). Also, which of the two relations are used when somebody deals with change of basis? Trying to understand these concepts through a rotation matrix $[A]$ and a projection matrix $[B]$, I figured out that in the first case(relation between matrices), the matrix $[C]$ is presented this way in order to again project to the same plane as $[B]$ did but it just has to have different matrix elements in order for it to work in the new basis $Y$ and I suppose that is why Halmos calls the two matrices similar. But, I can't figure out such an intuitive and geometrical explanation or example of how the second case with the relation between linear transformations works and thus, I can't explain why the two transformations are called similar. EDIT: I understood why $[C]=[A]^{-1}[B][A]$ but I didn't understand why $C=ABA^{-1} $ and why this difference between the two relation exists.","From section 47 called Similarity: (In the following I will represent matrices like $[A]$ and linear transforms as $A$ and also sorry if I am not rigorous enough) Halmos proves that when we have one linear transformation $T:V\longrightarrow V$ with matrix $[B]$ in a basis $X$ (vectors $\ \vec x_1, \vec x_2, .., \vec x_n$)  and matrix $[C]$ in basis $Y$ (vectors $\ \vec y_1, \vec y_2, .., \vec y_n$) and the two bases are related by $[A]x_i=y_i$, then the two matrices are related by $[C]=[A]^{-1}[B][A]$. He also proves that when we have two linear transformations $B$ and$C$ and $[B]=(β_{ij})$ is a matrix and the two transformations are defined as $B\vec x_j= \sum_{i=0}^nβ_{ij}\vec x_i$ and $C\vec y_j= \sum_{i=0}^nβ_{ij}\vec y_i$, the two transformations are related by $C=ABA^{-1} $. While I have proved these things, I can't intuitively(geometrically) understand why the difference in the relation of the transformations with the relation with the matrices , since a matrix is a way to express the transformation on a coordinates system(please correct me if I am wrong as I am not a mathematician). Also, which of the two relations are used when somebody deals with change of basis? Trying to understand these concepts through a rotation matrix $[A]$ and a projection matrix $[B]$, I figured out that in the first case(relation between matrices), the matrix $[C]$ is presented this way in order to again project to the same plane as $[B]$ did but it just has to have different matrix elements in order for it to work in the new basis $Y$ and I suppose that is why Halmos calls the two matrices similar. But, I can't figure out such an intuitive and geometrical explanation or example of how the second case with the relation between linear transformations works and thus, I can't explain why the two transformations are called similar. EDIT: I understood why $[C]=[A]^{-1}[B][A]$ but I didn't understand why $C=ABA^{-1} $ and why this difference between the two relation exists.",,"['linear-algebra', 'matrices', 'linear-transformations', 'change-of-basis']"
91,Eigenvectors are unique up to a scalar,Eigenvectors are unique up to a scalar,,"If $ A $ is a matrix with eigenvector $ v $ corresponding to the eigenvalue $ \lambda, $ can we prove that $ v $ is unique up to $ \lambda, $ that is if $ v $ and $ v' $ are eigenvectors corresponding to $ \lambda, $ then $ v = Cv' $ for some constant $ C. $","If $ A $ is a matrix with eigenvector $ v $ corresponding to the eigenvalue $ \lambda, $ can we prove that $ v $ is unique up to $ \lambda, $ that is if $ v $ and $ v' $ are eigenvectors corresponding to $ \lambda, $ then $ v = Cv' $ for some constant $ C. $",,"['linear-algebra', 'matrices']"
92,What exactly does a rotation preserve?,What exactly does a rotation preserve?,,"I understand a rotation should preserve length and angle and hence the dot product. Since anything that preserves the dot product is a linear transformation, then a rotation can be represented by a matrix. Let's only consider real matrices . However, there is something else that a rotation preserves and I am here to ask what it is. A linear transformation that preserves length must be orthogonal, however, an orthogonal matrix is a rotation if its determinant is $1$. That means a rotation is a special case of orthogonal matrix. So what makes a rotation special from orthogonal matrices? What else does a rotation preserve in addition to length and angle ? Anyone can provide a prove that a matrix is a rotation iff it is an orthogonal matrix with determinant $1$? Thank you!","I understand a rotation should preserve length and angle and hence the dot product. Since anything that preserves the dot product is a linear transformation, then a rotation can be represented by a matrix. Let's only consider real matrices . However, there is something else that a rotation preserves and I am here to ask what it is. A linear transformation that preserves length must be orthogonal, however, an orthogonal matrix is a rotation if its determinant is $1$. That means a rotation is a special case of orthogonal matrix. So what makes a rotation special from orthogonal matrices? What else does a rotation preserve in addition to length and angle ? Anyone can provide a prove that a matrix is a rotation iff it is an orthogonal matrix with determinant $1$? Thank you!",,"['linear-algebra', 'matrices', 'orthogonal-matrices']"
93,Does there exist $A$ of infinite order in $\{ A \in GL_2(\mathbb{R}) : A^T = A^{-1} \}$? [duplicate],Does there exist  of infinite order in ? [duplicate],A \{ A \in GL_2(\mathbb{R}) : A^T = A^{-1} \},"This question already has an answer here : Matrix Group induction proof and order of elements question (1 answer) Closed 7 years ago . Does there or does there not exist $A$ of infinite order in $\{ A \in GL_2(\mathbb{R}) : A^T = A^{-1} \}$? I know that elements of the form $$A=\begin{bmatrix}\cos(\theta) & \sin(\theta)\\\pm\sin(\theta) & \pm\cos(\theta)\end{bmatrix}$$ are in this group, but do not know how to calculate their order.","This question already has an answer here : Matrix Group induction proof and order of elements question (1 answer) Closed 7 years ago . Does there or does there not exist $A$ of infinite order in $\{ A \in GL_2(\mathbb{R}) : A^T = A^{-1} \}$? I know that elements of the form $$A=\begin{bmatrix}\cos(\theta) & \sin(\theta)\\\pm\sin(\theta) & \pm\cos(\theta)\end{bmatrix}$$ are in this group, but do not know how to calculate their order.",,"['matrices', 'group-theory', 'infinite-groups']"
94,Solution of $A^\top M A=M$ for all $M$ positive-definite,Solution of  for all  positive-definite,A^\top M A=M M,"I am trying to find all matrices $A$ such that for all positive-definite matrices $M$, $A^\top M A=M$. $I$ and $-I$ are obvious solutions. I can't find out it there are other such matrices and if so, how  to find them. Necessarily, $A$ is invertible since $\det(A)^2\det(M)=\det(M)\neq 0$ and more precisely, $\det A=\pm 1$. But that certainly not sufficient.","I am trying to find all matrices $A$ such that for all positive-definite matrices $M$, $A^\top M A=M$. $I$ and $-I$ are obvious solutions. I can't find out it there are other such matrices and if so, how  to find them. Necessarily, $A$ is invertible since $\det(A)^2\det(M)=\det(M)\neq 0$ and more precisely, $\det A=\pm 1$. But that certainly not sufficient.",,"['linear-algebra', 'matrices', 'matrix-equations']"
95,Continuity of matrix product with respect to matrix norm?,Continuity of matrix product with respect to matrix norm?,,"I'm trying to teach myself about ordinary differential equations with an old script and I'm struggling with this problem: Show that the matrix product is continuous with respect to the matrix norm. That is, if $A_j → A$ and $B_j → B$ we have $A_j\cdot B_j → AB$. My problem is that I don't even understand what the limit of $A_j\cdot B_j$ has to do with the matrix norm. Any help would be greatly appreciated!","I'm trying to teach myself about ordinary differential equations with an old script and I'm struggling with this problem: Show that the matrix product is continuous with respect to the matrix norm. That is, if $A_j → A$ and $B_j → B$ we have $A_j\cdot B_j → AB$. My problem is that I don't even understand what the limit of $A_j\cdot B_j$ has to do with the matrix norm. Any help would be greatly appreciated!",,"['matrices', 'continuity', 'normed-spaces']"
96,"Determine whether $w$ is in the $Span\{v_1, v_2, v_3\}$",Determine whether  is in the,"w Span\{v_1, v_2, v_3\}","my question is how to determine whether a vector $w$ is in the $span\{v_1, v_2, v_3\}$. In this case: $w = \begin{bmatrix}         9  \\         6  \\         1  \\           9  \\         \end{bmatrix} $ and  $v_1 = \begin{bmatrix} 1\\2\\-1\\1 \end{bmatrix}$ ,  $v_2 = \begin{bmatrix} 2\\-1\\1\\0 \end{bmatrix}$ ,  $v_3 = \begin{bmatrix} 1\\2\\0\\3 \end{bmatrix}$ My understanding so far is that I must see if $w$ can be written as a linear combination of $v_1, v_2,$ and $v_3$. To do this, I row reduced the augmented matrix of the 4 vectors to the identity matrix $I_4$ $$I_4 = \left[\begin{array}{ccc|c} 1 & 0 & 0 & 0\\0 & 1 & 0 & 0\\0 & 0 & 1 & 0\\0 & 0 & 0 & 1 \end{array}\right]$$ but from here I am confused. I'm just trying to better understand $span$, both in a problem like this and in a conceptual sense.","my question is how to determine whether a vector $w$ is in the $span\{v_1, v_2, v_3\}$. In this case: $w = \begin{bmatrix}         9  \\         6  \\         1  \\           9  \\         \end{bmatrix} $ and  $v_1 = \begin{bmatrix} 1\\2\\-1\\1 \end{bmatrix}$ ,  $v_2 = \begin{bmatrix} 2\\-1\\1\\0 \end{bmatrix}$ ,  $v_3 = \begin{bmatrix} 1\\2\\0\\3 \end{bmatrix}$ My understanding so far is that I must see if $w$ can be written as a linear combination of $v_1, v_2,$ and $v_3$. To do this, I row reduced the augmented matrix of the 4 vectors to the identity matrix $I_4$ $$I_4 = \left[\begin{array}{ccc|c} 1 & 0 & 0 & 0\\0 & 1 & 0 & 0\\0 & 0 & 1 & 0\\0 & 0 & 0 & 1 \end{array}\right]$$ but from here I am confused. I'm just trying to better understand $span$, both in a problem like this and in a conceptual sense.",,"['linear-algebra', 'matrices']"
97,Determinant of $P_n$,Determinant of,P_n,"I am preparing for an exam on linear algebra within few days, so I am in desperate need for a solution for the following question: Question: Let $P_n$, $n\ge2$, be the $n\times n$ matrix whose entries are all $1's$, except for $0's$ directly below the main diagonal. For instance $P_3=\begin{bmatrix}1&1&1\\0&1&1\\1&0&1\end{bmatrix}$ and $P_4=\begin{bmatrix}1&1&1&1\\0&1&1&1\\1&0&1&1\\1&1&0&1\end{bmatrix}$. Find the determinant of $P_n$. My attempt Note that $P_{n-1}$ is the minor of the entry $[a_{11}]$ of $P_n$. By observation, one could conjecture that $det(P_n)=1$ as all the cofactors vanish. (I've tried $P_2$ to $P_5$ and it seems valid.) So is there a formal way to proof such cofactors sum up to zero?","I am preparing for an exam on linear algebra within few days, so I am in desperate need for a solution for the following question: Question: Let $P_n$, $n\ge2$, be the $n\times n$ matrix whose entries are all $1's$, except for $0's$ directly below the main diagonal. For instance $P_3=\begin{bmatrix}1&1&1\\0&1&1\\1&0&1\end{bmatrix}$ and $P_4=\begin{bmatrix}1&1&1&1\\0&1&1&1\\1&0&1&1\\1&1&0&1\end{bmatrix}$. Find the determinant of $P_n$. My attempt Note that $P_{n-1}$ is the minor of the entry $[a_{11}]$ of $P_n$. By observation, one could conjecture that $det(P_n)=1$ as all the cofactors vanish. (I've tried $P_2$ to $P_5$ and it seems valid.) So is there a formal way to proof such cofactors sum up to zero?",,"['linear-algebra', 'matrices', 'determinant']"
98,Is there a diagonalizable matrix $A \neq B$ such that $e^A = e^B$?,Is there a diagonalizable matrix  such that ?,A \neq B e^A = e^B,"Is there a diagonalizable matrix $A \neq B$ such that $e^A = e^B$? My initial thought was that if this is possible, then $$ e^A e^{-A} = e^B e^{-A} $$ so $$ I = e^B e^{-A}. $$ This means the statement is true if there exists a nonzero matrix $C$ such that $e^C = I$. But is this possible? My intuition tells me no, but how can it be proved?","Is there a diagonalizable matrix $A \neq B$ such that $e^A = e^B$? My initial thought was that if this is possible, then $$ e^A e^{-A} = e^B e^{-A} $$ so $$ I = e^B e^{-A}. $$ This means the statement is true if there exists a nonzero matrix $C$ such that $e^C = I$. But is this possible? My intuition tells me no, but how can it be proved?",,"['linear-algebra', 'matrices', 'matrix-exponential']"
99,Find the Matrix $A^{482}$ in terms of $A$,Find the Matrix  in terms of,A^{482} A,"Given $$A=\begin{bmatrix} -4 & 3\\  -7 & 5 \end{bmatrix}$$ Find $A^{482}$ in terms of $A$ I tried using Characteristic equation of $A$ which is $$|\lambda I-A|=0$$ which gives $$A^2=A-I$$ so $$A^4=A^2A^2=(A-I)^2=A^2-2A+I=-A$$ so $$A^4=-A$$ but $482$ is neither multiple of $4$ nor Power of $2$, How can I proceed ?","Given $$A=\begin{bmatrix} -4 & 3\\  -7 & 5 \end{bmatrix}$$ Find $A^{482}$ in terms of $A$ I tried using Characteristic equation of $A$ which is $$|\lambda I-A|=0$$ which gives $$A^2=A-I$$ so $$A^4=A^2A^2=(A-I)^2=A^2-2A+I=-A$$ so $$A^4=-A$$ but $482$ is neither multiple of $4$ nor Power of $2$, How can I proceed ?",,"['matrices', 'eigenvalues-eigenvectors']"
