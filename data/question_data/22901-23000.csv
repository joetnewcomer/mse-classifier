,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,Why is the condition number of a matrix given by these eigenvalues?,Why is the condition number of a matrix given by these eigenvalues?,,"${\bf A}$ is $n \times n$ matrix. I want to know why the condition number of ${\bf A}$ is given by: $$\frac{\rm{max}(\lambda_i)}{\rm{min}(\lambda_j)},$$ where $\lambda$ are eigenvalues of ${\bf A}$ . Would you please give me a proof? This equation come from the page 80 in ""Deep Learning"" written by Ian Goodfellow.","is matrix. I want to know why the condition number of is given by: where are eigenvalues of . Would you please give me a proof? This equation come from the page 80 in ""Deep Learning"" written by Ian Goodfellow.","{\bf A} n \times n {\bf A} \frac{\rm{max}(\lambda_i)}{\rm{min}(\lambda_j)}, \lambda {\bf A}","['linear-algebra', 'matrices', 'numerical-linear-algebra', 'condition-number']"
1,Is left shift operator compact?,Is left shift operator compact?,,"Let $T:l^2 \to l^2$ be $T(a_1,a_2,...)=(a_2,a_3,...)$. Is this linear operator compact. If yes, how to prove it? If no, please give an example. I want to show that if $(a_2,a_3,...)$ has a cluster point or not. I think it is sufficient to show that if $(a_1,a_2,...)\in l^2$ , does it have a cluster point?","Let $T:l^2 \to l^2$ be $T(a_1,a_2,...)=(a_2,a_3,...)$. Is this linear operator compact. If yes, how to prove it? If no, please give an example. I want to show that if $(a_2,a_3,...)$ has a cluster point or not. I think it is sufficient to show that if $(a_1,a_2,...)\in l^2$ , does it have a cluster point?",,"['linear-algebra', 'linear-transformations', 'hilbert-spaces']"
2,"For every matrix $A$, can one find a matrix $B \ne O$ such that $\det(A+B)=\det(A)+\det(B)$?","For every matrix , can one find a matrix  such that ?",A B \ne O \det(A+B)=\det(A)+\det(B),"Since we can't say that $\det(A+B)=\det(A)+\det(B)$ for every matrix $A$ and $B$ , we can ask a different question. Clearly, for every matrix $A$ we have $\det(A+O)=\det(A)+\det(O)$ . So can we find for every matrix $A$ a matrix $B\neq O$ such that $\det(A+B)=\det(A)+\det(B)$ ? If $A$ is singular we get $\det(A-A)=\det(A)+\det(-A)$ and we can find many other ways to obtain our desired result. How do we deal with the case of $A$ being invertible?","Since we can't say that for every matrix and , we can ask a different question. Clearly, for every matrix we have . So can we find for every matrix a matrix such that ? If is singular we get and we can find many other ways to obtain our desired result. How do we deal with the case of being invertible?",\det(A+B)=\det(A)+\det(B) A B A \det(A+O)=\det(A)+\det(O) A B\neq O \det(A+B)=\det(A)+\det(B) A \det(A-A)=\det(A)+\det(-A) A,"['linear-algebra', 'matrices', 'determinant']"
3,Yet another matrix equation [closed],Yet another matrix equation [closed],,"Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 6 years ago . Improve this question Solve the following matrix equation for $X$. $$\left[\begin{array}{cc} 5 &-8\cr 8 &1 \end{array}\right] X + \left[\begin{array}{cc} 6 &6\cr 3 &5 \end{array}\right] = \left[\begin{array}{cc} -1 &4\cr -3 &-1 \end{array}\right] X$$ Please give me some hint to do this question. Thanks.","Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 6 years ago . Improve this question Solve the following matrix equation for $X$. $$\left[\begin{array}{cc} 5 &-8\cr 8 &1 \end{array}\right] X + \left[\begin{array}{cc} 6 &6\cr 3 &5 \end{array}\right] = \left[\begin{array}{cc} -1 &4\cr -3 &-1 \end{array}\right] X$$ Please give me some hint to do this question. Thanks.",,"['linear-algebra', 'matrices', 'systems-of-equations', 'matrix-equations']"
4,"If $A$ and $B$ are invertible matrices, is $A+B$ invertible too?","If  and  are invertible matrices, is  invertible too?",A B A+B,"Assume that $A$ and $B$ are invertible matrices, is $A+B$ (given that $A+B\neq0$) an invertible matrix too? And if so how do I prove it?","Assume that $A$ and $B$ are invertible matrices, is $A+B$ (given that $A+B\neq0$) an invertible matrix too? And if so how do I prove it?",,"['linear-algebra', 'matrices']"
5,Prove an inequality using the Cauchy -Schwarz Inequality [duplicate],Prove an inequality using the Cauchy -Schwarz Inequality [duplicate],,"This question already has answers here : Prove QM-AM inequality (5 answers) Closed 8 years ago . I want to prove that for all $n\in\mathbb{N}$ and $x_i\in\mathbb{R}$, we have $$(x_1+\cdots+x_n)^2\leq n(x_1^2+\cdots+x_n^2).$$ At first I wanted to do a proof by induction, but it has turned out to be much more complicated than I imagined.  Now I'm thinking Cauchy-Schwarz, but I can't seem to write it in the correct form to get the result I need. Any hints or help would be much appreciated.  The Cauchy-Schwarz inequality as we are using it in my class is in vector form, i.e., $$|\langle u,v\rangle|\le\|u\| \|v\|.$$","This question already has answers here : Prove QM-AM inequality (5 answers) Closed 8 years ago . I want to prove that for all $n\in\mathbb{N}$ and $x_i\in\mathbb{R}$, we have $$(x_1+\cdots+x_n)^2\leq n(x_1^2+\cdots+x_n^2).$$ At first I wanted to do a proof by induction, but it has turned out to be much more complicated than I imagined.  Now I'm thinking Cauchy-Schwarz, but I can't seem to write it in the correct form to get the result I need. Any hints or help would be much appreciated.  The Cauchy-Schwarz inequality as we are using it in my class is in vector form, i.e., $$|\langle u,v\rangle|\le\|u\| \|v\|.$$",,"['linear-algebra', 'inequality']"
6,Find area of ellipse $5x^2 -6xy +5y^2=8$,Find area of ellipse,5x^2 -6xy +5y^2=8,"Find the  area of ellipse whose  equation in the $xy$ - plane  is given by $5x^2 -6xy +5y^2=8$ My attempt  : I know  that area   of ellipse $ = \pi a b$ ,where $a$ is  semi-major axis and $b$ is  semi minor axis Now  if we make  matrix $\begin{bmatrix} 5 & -3 \\-3& 5\end{bmatrix}$ Here  eigenvalue $\lambda_1=  8 ,\lambda_2=2$ That  is area of ellipse $ = \pi \frac{1}{\sqrt\lambda_2} \frac{1}{\sqrt\lambda_1}= \pi \frac{1}{2\sqrt 2}\frac{1}{\sqrt2}$ Is its  correct ?","Find the  area of ellipse whose  equation in the - plane  is given by My attempt  : I know  that area   of ellipse ,where is  semi-major axis and is  semi minor axis Now  if we make  matrix Here  eigenvalue That  is area of ellipse Is its  correct ?","xy 5x^2 -6xy +5y^2=8  = \pi a b a b \begin{bmatrix} 5 & -3 \\-3& 5\end{bmatrix} \lambda_1=  8 ,\lambda_2=2  = \pi \frac{1}{\sqrt\lambda_2} \frac{1}{\sqrt\lambda_1}= \pi \frac{1}{2\sqrt 2}\frac{1}{\sqrt2}","['linear-algebra', 'geometry']"
7,"How to find the ""most upright"" orthogonal vector to another vector?","How to find the ""most upright"" orthogonal vector to another vector?",,"Given an arbitrary vector, I can find any orthogonal vector by solving $ax + by + cz = 0$. I want to find the ""most upright"" orthogonal (unit) vector, where $z$ is maximized. There must be a straightforward closed form solution to this, right?","Given an arbitrary vector, I can find any orthogonal vector by solving $ax + by + cz = 0$. I want to find the ""most upright"" orthogonal (unit) vector, where $z$ is maximized. There must be a straightforward closed form solution to this, right?",,"['linear-algebra', 'vectors']"
8,Proof of Uncountable Basis for $\mathbb{N} \to \mathbb{R}$ over $\mathbb{R}$,Proof of Uncountable Basis for  over,\mathbb{N} \to \mathbb{R} \mathbb{R},Does anyone have a simple proof of the uncountability of bases of the vector space of all functions $f : \mathbb{N} \to \mathbb{R}$. I have seen a proof which uses the determinant of the Vandermonde matrix to show the linear independence of functions of the form $f_c=c^n$ but I believe that there might be a simpler one that doesn't require the use of matrices. I have attached a link of another proof that I found online but I find the use of the limit unsettling. https://minhyongkim.wordpress.com/2013/10/23/a-vector-space-of-uncountable-dimension/,Does anyone have a simple proof of the uncountability of bases of the vector space of all functions $f : \mathbb{N} \to \mathbb{R}$. I have seen a proof which uses the determinant of the Vandermonde matrix to show the linear independence of functions of the form $f_c=c^n$ but I believe that there might be a simpler one that doesn't require the use of matrices. I have attached a link of another proof that I found online but I find the use of the limit unsettling. https://minhyongkim.wordpress.com/2013/10/23/a-vector-space-of-uncountable-dimension/,,"['linear-algebra', 'set-theory']"
9,Linear transformation on the vector space of complex numbers over the reals that isn't a linear transformation on $\mathbb{C}^1$.,Linear transformation on the vector space of complex numbers over the reals that isn't a linear transformation on .,\mathbb{C}^1,"I am having a little trouble with the following question (that is from the Linear Transformations chapter in Hoffman's / Kunze Linear Algebra): Let $\mathbb{V}$ be the set of all complex numbers regarded as a vector space over the field of real numbers (usual operations). Find a function from $\mathbb{V}$ into $\mathbb{V}$ which is a linear transformation on the above vector space, but which is not a linear transformation on $\mathbb{C}^1$, i.e., which is not complex linear. $\mathbb{C}^1$ is the set of all complex numbers regarded as a vector space over the field of complex numbers right? If then, isn't all linear transformations in $\mathbb{V}$ a linear transformation on $\mathbb{C}^1$? (Because $\mathbb{R}\subset\mathbb{C}$) I think I'm missing something fundamental here. Any help is welcome. Thanks!","I am having a little trouble with the following question (that is from the Linear Transformations chapter in Hoffman's / Kunze Linear Algebra): Let $\mathbb{V}$ be the set of all complex numbers regarded as a vector space over the field of real numbers (usual operations). Find a function from $\mathbb{V}$ into $\mathbb{V}$ which is a linear transformation on the above vector space, but which is not a linear transformation on $\mathbb{C}^1$, i.e., which is not complex linear. $\mathbb{C}^1$ is the set of all complex numbers regarded as a vector space over the field of complex numbers right? If then, isn't all linear transformations in $\mathbb{V}$ a linear transformation on $\mathbb{C}^1$? (Because $\mathbb{R}\subset\mathbb{C}$) I think I'm missing something fundamental here. Any help is welcome. Thanks!",,"['linear-algebra', 'linear-transformations']"
10,Why isn't removing zero rows an elementary operation?,Why isn't removing zero rows an elementary operation?,,"My prof taught us that during Gaussian Elimination, we can perform three elementary operations to transform the matrix: 1) Multiple both sides of a row by a non-zero constant 2) Add or subtract rows 3) Interchanging rows In addition to those, why isn't removing zero rows an elementary operation? It doesn't affect the system in any way. Define zero rows to be a row with no leading variables. For example isn't $\begin{bmatrix}a & b & k\\c & d & m\end{bmatrix} \rightarrow  \begin{bmatrix}a & b & k\\c & d & m\\0 & 0 & 0\end{bmatrix}$","My prof taught us that during Gaussian Elimination, we can perform three elementary operations to transform the matrix: 1) Multiple both sides of a row by a non-zero constant 2) Add or subtract rows 3) Interchanging rows In addition to those, why isn't removing zero rows an elementary operation? It doesn't affect the system in any way. Define zero rows to be a row with no leading variables. For example isn't $\begin{bmatrix}a & b & k\\c & d & m\end{bmatrix} \rightarrow  \begin{bmatrix}a & b & k\\c & d & m\\0 & 0 & 0\end{bmatrix}$",,"['linear-algebra', 'matrices', 'gaussian-elimination']"
11,Relationships between $\det(A+B)$ and $A+B$,Relationships between  and,\det(A+B) A+B,When computing $\det(A+B)$ we notice that there is no relation between $\det A + \det B$. However does the $\det(A+B)$ have any relation to the matrices $A+B$ as they stand?,When computing $\det(A+B)$ we notice that there is no relation between $\det A + \det B$. However does the $\det(A+B)$ have any relation to the matrices $A+B$ as they stand?,,"['linear-algebra', 'matrices', 'determinant']"
12,Calculating number of non-zero elements in a lower triangular matrix,Calculating number of non-zero elements in a lower triangular matrix,,"Given a lower triangular matrix $M$ of size $m$ by $n$, is there an equation for the number of elements in this matrix that can be non-zero?  What if the matrix is strictly lower triangular?","Given a lower triangular matrix $M$ of size $m$ by $n$, is there an equation for the number of elements in this matrix that can be non-zero?  What if the matrix is strictly lower triangular?",,"['linear-algebra', 'matrices']"
13,Projection matrices,Projection matrices,,I have found these two apparently contradicting remarks about projection matrices: A matrix $P$ is idempotent if $PP = P$ . An idempotent matrix that is also Hermitian is called a projection matrix. $P$ is a projector if $PP = P$ . Projectors are always positive which implies that they are always Hermitian. Which of both is correct? Is a matrix $P$ that verifies $PP=P$ always Hermitian?,I have found these two apparently contradicting remarks about projection matrices: A matrix is idempotent if . An idempotent matrix that is also Hermitian is called a projection matrix. is a projector if . Projectors are always positive which implies that they are always Hermitian. Which of both is correct? Is a matrix that verifies always Hermitian?,P PP = P P PP = P P PP=P,"['linear-algebra', 'matrices', 'projection', 'projection-matrices', 'idempotents']"
14,Are all symmetric matrices with positive eigenvalues a product of a matrix and its transpose?,Are all symmetric matrices with positive eigenvalues a product of a matrix and its transpose?,,"Given an $m \times n$ matrix $A$, it's easy to show that the matrix products $B = A^TA$ and $C = AA^T$ are both symmetric. I was wondering if any symmetric matrix with positive eigenvalues could be expressed as the product of some matrix and its transpose? Note: It's obvious that if the eigenvalues are not all positive then this cannot be the case, as $A^TA$ and $AA^T$ both have to be positive semidefinite. However this is all I can think of regarding this problem.","Given an $m \times n$ matrix $A$, it's easy to show that the matrix products $B = A^TA$ and $C = AA^T$ are both symmetric. I was wondering if any symmetric matrix with positive eigenvalues could be expressed as the product of some matrix and its transpose? Note: It's obvious that if the eigenvalues are not all positive then this cannot be the case, as $A^TA$ and $AA^T$ both have to be positive semidefinite. However this is all I can think of regarding this problem.",,"['linear-algebra', 'eigenvalues-eigenvectors']"
15,if $AB\neq 0$ for any non zero matrix $B$ then $A$ is invertible,if  for any non zero matrix  then  is invertible,AB\neq 0 B A,"Question is to check that : If $A$ is an $n\times n$ matrix over a field $F$ and $AB\neq 0$ for any non zero matrix $B_{n\times n}$ over $F$ then, $A$ is invertible. This does make some sense to me but i am not sure how to prove this. As $AB\neq 0$ for any $B$ , in particular, we have $A.A\neq 0$ i.e., $A^2\neq 0$ for similar reasons we see that $A^n\neq 0$ for any positive integer $n$ So, $A$ is not nilpotent... I see that this is just nilpotent... I am stuck to prove that $A$ is invertible. I do have some thoughts inbetween but nothing gives me simple way to conclude final result. please help me to see this by giving some hints (I am sure this must be very easy) Thank you","Question is to check that : If $A$ is an $n\times n$ matrix over a field $F$ and $AB\neq 0$ for any non zero matrix $B_{n\times n}$ over $F$ then, $A$ is invertible. This does make some sense to me but i am not sure how to prove this. As $AB\neq 0$ for any $B$ , in particular, we have $A.A\neq 0$ i.e., $A^2\neq 0$ for similar reasons we see that $A^n\neq 0$ for any positive integer $n$ So, $A$ is not nilpotent... I see that this is just nilpotent... I am stuck to prove that $A$ is invertible. I do have some thoughts inbetween but nothing gives me simple way to conclude final result. please help me to see this by giving some hints (I am sure this must be very easy) Thank you",,['linear-algebra']
16,Why should I care that a matrix is diagonalizable? [duplicate],Why should I care that a matrix is diagonalizable? [duplicate],,"This question already has answers here : A matrix is diagonalizable, so what? (5 answers) Closed last year . I was talking to someone about diagonalizability of matrices and they asked me why we even care. For one thing, we can easily raise the matrix to the $n$ -th power. If $A=PDP^{-1}$ then, $A^n = P D^n P^{-1}$ and the diagonal matrix $D$ is very easy to raise to the $n$ -th power (just raise all the diagonals). Ok, he said - so just a computational benefit. Also, you can find the determinant by multiplying the numbers on the diagonal. Ok, again a computational benefit since you could find it through other means too. I realized that I can't think of a fundamental benefit one gets from being able to diagonalize a matrix that extends beyond computational efficiency. For invertibility, there are many things you get that you just can't do otherwise. Is there anything similar one gets for diagonalizability that I'm not aware of? Note another similar question: A matrix is diagonalizable, so what? that asks for a solid perception of diagonalizability. I believe this one is slightly different since it focuses on any utility barring numerical efficiency.","This question already has answers here : A matrix is diagonalizable, so what? (5 answers) Closed last year . I was talking to someone about diagonalizability of matrices and they asked me why we even care. For one thing, we can easily raise the matrix to the -th power. If then, and the diagonal matrix is very easy to raise to the -th power (just raise all the diagonals). Ok, he said - so just a computational benefit. Also, you can find the determinant by multiplying the numbers on the diagonal. Ok, again a computational benefit since you could find it through other means too. I realized that I can't think of a fundamental benefit one gets from being able to diagonalize a matrix that extends beyond computational efficiency. For invertibility, there are many things you get that you just can't do otherwise. Is there anything similar one gets for diagonalizability that I'm not aware of? Note another similar question: A matrix is diagonalizable, so what? that asks for a solid perception of diagonalizability. I believe this one is slightly different since it focuses on any utility barring numerical efficiency.",n A=PDP^{-1} A^n = P D^n P^{-1} D n,"['linear-algebra', 'matrices', 'diagonalization']"
17,What is the connection between the invertibility of a matrix the kernel of the matrix,What is the connection between the invertibility of a matrix the kernel of the matrix,,"If the kernel of a matrix A only has the zero vector (ker(A) = {0}) Then why is the matrix A considered invertible because of that? I know that if ker(A) = 0, then there is only one unique solution of the matrix( assuming that the matrix is made up of a system of linear equations) but what does this tell me about whether or not the matrix is singular? Can someone explain the logic behind this?","If the kernel of a matrix A only has the zero vector (ker(A) = {0}) Then why is the matrix A considered invertible because of that? I know that if ker(A) = 0, then there is only one unique solution of the matrix( assuming that the matrix is made up of a system of linear equations) but what does this tell me about whether or not the matrix is singular? Can someone explain the logic behind this?",,"['linear-algebra', 'matrices']"
18,Rank nullity theorem -bijection,Rank nullity theorem -bijection,,Let $T:\Bbb R^n\to \Bbb R^n$ be a linear transformation. Which of the following statement implies that $T$ is bijective? a) $\operatorname{Null}(T)=n$ b) $\operatorname{Rank}(T)=\operatorname{Null}(T)=n$ c) $\operatorname{Rank}(T)+\operatorname{Null}(T)=n$ d) $\operatorname{Rank}(T)-\operatorname{Null}(T)=n$ I think that since $T$ is one one n onto... Nullity will be zero... So option a) and b) are incorrect.. And c) and d) will be correct.. But the answer is option d) . I am not able to understand why option c) is incorrect.,Let be a linear transformation. Which of the following statement implies that is bijective? a) b) c) d) I think that since is one one n onto... Nullity will be zero... So option a) and b) are incorrect.. And c) and d) will be correct.. But the answer is option d) . I am not able to understand why option c) is incorrect.,T:\Bbb R^n\to \Bbb R^n T \operatorname{Null}(T)=n \operatorname{Rank}(T)=\operatorname{Null}(T)=n \operatorname{Rank}(T)+\operatorname{Null}(T)=n \operatorname{Rank}(T)-\operatorname{Null}(T)=n T,['linear-algebra']
19,"How to show that the set of functions $1,x,x^2,x^3...$ is linearly independent? [closed]",How to show that the set of functions  is linearly independent? [closed],"1,x,x^2,x^3...","Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 9 years ago . Improve this question Show that the set of functions $1,x,x^2,x^3...x^n...$ is linearly independent on any interval $[a,b]$. If $$c_1+xc_2+x^2c_3+x^3c_4...=0$$ we should show $$c_i=0,\quad i=1,2, \ldots$$ how could I start? My second question: is it linearly independent on $C[0,1]$?","Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 9 years ago . Improve this question Show that the set of functions $1,x,x^2,x^3...x^n...$ is linearly independent on any interval $[a,b]$. If $$c_1+xc_2+x^2c_3+x^3c_4...=0$$ we should show $$c_i=0,\quad i=1,2, \ldots$$ how could I start? My second question: is it linearly independent on $C[0,1]$?",,"['linear-algebra', 'functional-analysis']"
20,Find the $2013$th power of a given $3\times 3$ matrix,Find the th power of a given  matrix,2013 3\times 3,Question from my linear algebra homework I'm struggling with: Let $D = \begin{bmatrix} -2 & 5 & 4 \\-1 & 0 & 0 \\0 & 4 & 3 \end{bmatrix}$ We are asked: Find $D^5+3D^2-D+I$ Find $D^{2013}$ Write $D^{-1}$ as a polynomial of $D$ I solved questions 1) and 3) but can't solve 2)...,Question from my linear algebra homework I'm struggling with: Let $D = \begin{bmatrix} -2 & 5 & 4 \\-1 & 0 & 0 \\0 & 4 & 3 \end{bmatrix}$ We are asked: Find $D^5+3D^2-D+I$ Find $D^{2013}$ Write $D^{-1}$ as a polynomial of $D$ I solved questions 1) and 3) but can't solve 2)...,,"['linear-algebra', 'matrices']"
21,Condition number of a product of two matrices,Condition number of a product of two matrices,,"Given two square matrices $A$ and $B$, is the following inequality $$\operatorname{cond}(AB) \leq \operatorname{cond}(A)\operatorname{cond}(B),$$ where $\operatorname {cond}$ is the condition number, true? Is this still true for rectangular matrices? I know this is true: $$||AB|| \leq ||A|| \cdot ||B||$$ The definition of condition number of matrix is as follows: $$\operatorname{cond}(A)=||A|| \cdot ||A^{-1}||$$","Given two square matrices $A$ and $B$, is the following inequality $$\operatorname{cond}(AB) \leq \operatorname{cond}(A)\operatorname{cond}(B),$$ where $\operatorname {cond}$ is the condition number, true? Is this still true for rectangular matrices? I know this is true: $$||AB|| \leq ||A|| \cdot ||B||$$ The definition of condition number of matrix is as follows: $$\operatorname{cond}(A)=||A|| \cdot ||A^{-1}||$$",,"['linear-algebra', 'matrices', 'normed-spaces', 'condition-number']"
22,Given a matrix $A$ find a matrix $C$ such that $C^3$=$A$,Given a matrix  find a matrix  such that =,A C C^3 A,"This is a question I had on a test, we were told not to use brute-force and figure out a smart way to solve the problem. We have a matrix $A =$ $\displaystyle\begin{bmatrix} 2 & 3\\  3 & 2 \end{bmatrix}$ . Find a matrix $C$ such that $C^{3}$ = $A$ . What is the 'smart not brute-force' way to solve this, without picking numbers, looking for patterns and so on? it was in eigenvalues section"" in the end?","This is a question I had on a test, we were told not to use brute-force and figure out a smart way to solve the problem. We have a matrix . Find a matrix such that = . What is the 'smart not brute-force' way to solve this, without picking numbers, looking for patterns and so on? it was in eigenvalues section"" in the end?","A = \displaystyle\begin{bmatrix}
2 & 3\\ 
3 & 2
\end{bmatrix} C C^{3} A",[]
23,Show that a transformation is linear if and only if its restriction to subspaces of dimension 2 is linear.,Show that a transformation is linear if and only if its restriction to subspaces of dimension 2 is linear.,,"Let $V$ be a vector space over a field $\mathbb{K}$ with $\dim_\mathbb{K} \geq 3$ . Show that a transformation $T : V \rightarrow V $ is linear if and only if the restriction of $T$ to each subspace of dimension $2$ of $V$ is linear. (->) If $T$ is linear in $V$ then it's clear that it's also linear in any subspace of $V$ . (<-) Suppose $\dim_\mathbb{K} = n \geq 3$ and that $T$ is linear in any subspace of dimension $2$ of $V$ . Let $\{b_1,b_2, \cdots, b_n\} \subset V$ be a basis for $V$ . Now consider the following subspaces of $V$ : $$ W_i = \text{span}(\{b_i, b_{i+1}\}) $$ Now let $v = \big(\sum_{i=1}^n \alpha_i \cdot b_i\big) \in V$ . Therefore: $$ v = \sum_{i=1}^n \alpha_i \cdot b_i = \underbrace{(\alpha_1 b_1 + \alpha_2 b_2)}_{\in W_1} + \underbrace{(\alpha_3 b_3 + \alpha_4 b_4)}_{\in W_3} + \cdots + \underbrace{(\alpha_{n-1} b_{n-1} + \alpha_n b_n)}_{\in W_{n-1}} $$ And from that it follows that if $n$ is even, then $$ V = W_1 \oplus W_3 \oplus \cdots \oplus W_{n-1} $$ and if $n$ is odd, then: $$ V = W_1 \oplus W_3 \oplus \cdots \oplus W_{n-2} \oplus \text{span}(\{b_n\}) $$ It's clear to see that the sum is direct since $W_i \cap W_{i+2} = \{0\}$ . Now I need to prove the linearity of $T$ in $V$ , so let $v = \sum_{i=1}^n \alpha_i \cdot b_i$ , $u = \sum_{i=1}^n \beta_i \cdot b_i$ and $\lambda \in \mathbb{K}$ . So it remains to prove that $T(u+v) = T(u) + T(v)$ and $T(\lambda \cdot u) = \lambda \cdot T(u)$ . $$ \begin{align*} T(u+v) = T\big( \sum_{i=1}^n \alpha_i \cdot b_i + \sum_{i=1}^n \beta_i \cdot b_i \big) =  T\big( \sum_{i=1}^n (\alpha_i + \beta_i) \cdot b_i \big) = \cdots \end{align*} $$ And now I'm stuck because for me ""the restriction of $T$ to each subspace of dimension $2$ of $V$ is linear"" means is that $T$ is going to be linear in each of those $W_i$ that I've defined. That means that if $w = \alpha b_i + \beta b_{i+1} \in W_i$ then $T(w) = \alpha \cdot T(b_i) + \beta \cdot T(b_{i+1})$ . But that does not implies that $$ T(w_1 + w_3 + \cdots + w_{n-1}) = T(w_1) + T(w_3) + \cdots + T(w_{n-1}) $$ where $w_i \in W_i$ . Any help is highly appreciated. Thanks!","Let be a vector space over a field with . Show that a transformation is linear if and only if the restriction of to each subspace of dimension of is linear. (->) If is linear in then it's clear that it's also linear in any subspace of . (<-) Suppose and that is linear in any subspace of dimension of . Let be a basis for . Now consider the following subspaces of : Now let . Therefore: And from that it follows that if is even, then and if is odd, then: It's clear to see that the sum is direct since . Now I need to prove the linearity of in , so let , and . So it remains to prove that and . And now I'm stuck because for me ""the restriction of to each subspace of dimension of is linear"" means is that is going to be linear in each of those that I've defined. That means that if then . But that does not implies that where . Any help is highly appreciated. Thanks!","V \mathbb{K} \dim_\mathbb{K} \geq 3 T : V \rightarrow V  T 2 V T V V \dim_\mathbb{K} = n \geq 3 T 2 V \{b_1,b_2, \cdots, b_n\} \subset V V V 
W_i = \text{span}(\{b_i, b_{i+1}\})
 v = \big(\sum_{i=1}^n \alpha_i \cdot b_i\big) \in V 
v = \sum_{i=1}^n \alpha_i \cdot b_i = \underbrace{(\alpha_1 b_1 + \alpha_2 b_2)}_{\in W_1} + \underbrace{(\alpha_3 b_3 + \alpha_4 b_4)}_{\in W_3} + \cdots + \underbrace{(\alpha_{n-1} b_{n-1} + \alpha_n b_n)}_{\in W_{n-1}}
 n 
V = W_1 \oplus W_3 \oplus \cdots \oplus W_{n-1}
 n 
V = W_1 \oplus W_3 \oplus \cdots \oplus W_{n-2} \oplus \text{span}(\{b_n\})
 W_i \cap W_{i+2} = \{0\} T V v = \sum_{i=1}^n \alpha_i \cdot b_i u = \sum_{i=1}^n \beta_i \cdot b_i \lambda \in \mathbb{K} T(u+v) = T(u) + T(v) T(\lambda \cdot u) = \lambda \cdot T(u) 
\begin{align*}
T(u+v) = T\big( \sum_{i=1}^n \alpha_i \cdot b_i + \sum_{i=1}^n \beta_i \cdot b_i \big) = 
T\big( \sum_{i=1}^n (\alpha_i + \beta_i) \cdot b_i \big) = \cdots
\end{align*}
 T 2 V T W_i w = \alpha b_i + \beta b_{i+1} \in W_i T(w) = \alpha \cdot T(b_i) + \beta \cdot T(b_{i+1}) 
T(w_1 + w_3 + \cdots + w_{n-1}) = T(w_1) + T(w_3) + \cdots + T(w_{n-1})
 w_i \in W_i","['linear-algebra', 'linear-transformations', 'solution-verification']"
24,Why is the eigenvalue equation $ Ax = \lambda x $ nonlinear?,Why is the eigenvalue equation  nonlinear?, Ax = \lambda x ,I read in a book that the eigenvalue equation is nonlinear but I can't see why that is form just looking at it... $$ Ax = \lambda x $$ looks a lot similar to $$ Ax = b $$ and if the matrix $A$ represents a linear transformation of $x$ then I don't see why $Ax = \lambda x$ would be nonlinear...,I read in a book that the eigenvalue equation is nonlinear but I can't see why that is form just looking at it... looks a lot similar to and if the matrix represents a linear transformation of then I don't see why would be nonlinear...,"
Ax = \lambda x
 
Ax = b
 A x Ax = \lambda x","['linear-algebra', 'eigenvalues-eigenvectors', 'linear-transformations']"
25,How do I find out that the following two matrices are similar?,How do I find out that the following two matrices are similar?,,How do I find out that the following two matrices are similar? $N = \begin{pmatrix}  0 & 1 & 0 & 0 \\ 0 & 0 & 0 & 0 \\ 0 & 0 & 0 & 0 \\ 0 & 0 & 0 & 0  \end{pmatrix}$ and $M= \begin{pmatrix}  0 & 0 & 0 & 0 \\ 0 & 0 & 0 & 0 \\ 0 & 0 & 0 & 1 \\ 0 & 0 & 0 & 0  \end{pmatrix}$ I initially tried to think of the left multiplication of a matrix $P$ as a row operation and tried $P= \begin{pmatrix}  0 & 0 & 1 & 0 \\ 0 & 1 & 0 & 0 \\ 1 & 0 & 0 & 0 \\ 0 & 0 & 0 & 1  \end{pmatrix}$ such that $PN = \begin{pmatrix}  0 & 0 & 0 & 0 \\ 0 & 0 & 0 & 0 \\ 0 & 1 & 0 & 0 \\ 0 & 0 & 0 & 0  \end{pmatrix}$ but then $PNP^{-1} \neq M$ . My linear algebra is a bit rusty. Is there a more elaborate way to do this?,How do I find out that the following two matrices are similar? and I initially tried to think of the left multiplication of a matrix as a row operation and tried such that but then . My linear algebra is a bit rusty. Is there a more elaborate way to do this?,"N =
\begin{pmatrix} 
0 & 1 & 0 & 0 \\
0 & 0 & 0 & 0 \\
0 & 0 & 0 & 0 \\
0 & 0 & 0 & 0 
\end{pmatrix} M=
\begin{pmatrix} 
0 & 0 & 0 & 0 \\
0 & 0 & 0 & 0 \\
0 & 0 & 0 & 1 \\
0 & 0 & 0 & 0 
\end{pmatrix} P P=
\begin{pmatrix} 
0 & 0 & 1 & 0 \\
0 & 1 & 0 & 0 \\
1 & 0 & 0 & 0 \\
0 & 0 & 0 & 1 
\end{pmatrix} PN = \begin{pmatrix} 
0 & 0 & 0 & 0 \\
0 & 0 & 0 & 0 \\
0 & 1 & 0 & 0 \\
0 & 0 & 0 & 0 
\end{pmatrix} PNP^{-1} \neq M","['linear-algebra', 'matrices', 'similar-matrices']"
26,Why does multiplying by $\textbf{A}^T$ make a previously unsolvable linear system solvable,Why does multiplying by  make a previously unsolvable linear system solvable,\textbf{A}^T,"Consider for instance the linear system: $$\left( \begin{array}{cc}  1 & 2 \\  3 & 4 \\  5 & 6 \\ \end{array} \right).\left( \begin{array}{c}  x \\  y \\ \end{array} \right)=\left( \begin{array}{c}  1 \\  2 \\  4 \\ \end{array} \right)$$ This is over determined and thus has no solution. Yet, by simply multiplying both sides by $\textbf{A}^T$: $$\left( \begin{array}{ccc}  1 & 3 & 5 \\  2 & 4 & 6 \\ \end{array} \right).\left( \begin{array}{cc}  1 & 2 \\  3 & 4 \\  5 & 6 \\ \end{array} \right).\left( \begin{array}{c}  x \\  y \\ \end{array} \right)=\left( \begin{array}{ccc}  1 & 3 & 5 \\  2 & 4 & 6 \\ \end{array} \right).\left( \begin{array}{c}  1 \\  2 \\  4 \\ \end{array} \right)$$ We find that the system now has a unique solution, which is the (x,y) that minimizes the squared error. Now I understand the derivation of why multiplying by the transpose helps to find the pseudoinverse which then helps to perform OLS regression, but my question is perhaps a bit more fundamental. How can multiplying both sides of an equation by a matrix change a system which previously had no solutions into one that has a unique solution? This seems to against what I assumed that the solutions to $\textbf{A}x = \textbf{B}$ were the same as the solutions to $\textbf{P}\textbf{A}x = \textbf{P}\textbf{B}$.","Consider for instance the linear system: $$\left( \begin{array}{cc}  1 & 2 \\  3 & 4 \\  5 & 6 \\ \end{array} \right).\left( \begin{array}{c}  x \\  y \\ \end{array} \right)=\left( \begin{array}{c}  1 \\  2 \\  4 \\ \end{array} \right)$$ This is over determined and thus has no solution. Yet, by simply multiplying both sides by $\textbf{A}^T$: $$\left( \begin{array}{ccc}  1 & 3 & 5 \\  2 & 4 & 6 \\ \end{array} \right).\left( \begin{array}{cc}  1 & 2 \\  3 & 4 \\  5 & 6 \\ \end{array} \right).\left( \begin{array}{c}  x \\  y \\ \end{array} \right)=\left( \begin{array}{ccc}  1 & 3 & 5 \\  2 & 4 & 6 \\ \end{array} \right).\left( \begin{array}{c}  1 \\  2 \\  4 \\ \end{array} \right)$$ We find that the system now has a unique solution, which is the (x,y) that minimizes the squared error. Now I understand the derivation of why multiplying by the transpose helps to find the pseudoinverse which then helps to perform OLS regression, but my question is perhaps a bit more fundamental. How can multiplying both sides of an equation by a matrix change a system which previously had no solutions into one that has a unique solution? This seems to against what I assumed that the solutions to $\textbf{A}x = \textbf{B}$ were the same as the solutions to $\textbf{P}\textbf{A}x = \textbf{P}\textbf{B}$.",,['linear-algebra']
27,$|\text{det}(A)| = 1$ implies $A$ is orthogonal,implies  is orthogonal,|\text{det}(A)| = 1 A,"I know that $A$ orthogonal $\Rightarrow$ |det($A$)| = 1. Now I need to prove or disprove the reversed statement: $$ |\det(A)| = 1 \Rightarrow A \,\text{ is orthogonal} $$ This is what I'm currently trying: $$ |\det(A)| = 1 \Rightarrow \det(A)^2 = 1 \Rightarrow \det(AA^t) = 1 $$ But I'm unsure whether this implies, that $AA^t = E_n$. Any help is welcome at this point. Maybe the statement isn't even true.","I know that $A$ orthogonal $\Rightarrow$ |det($A$)| = 1. Now I need to prove or disprove the reversed statement: $$ |\det(A)| = 1 \Rightarrow A \,\text{ is orthogonal} $$ This is what I'm currently trying: $$ |\det(A)| = 1 \Rightarrow \det(A)^2 = 1 \Rightarrow \det(AA^t) = 1 $$ But I'm unsure whether this implies, that $AA^t = E_n$. Any help is welcome at this point. Maybe the statement isn't even true.",,"['linear-algebra', 'proof-writing', 'determinant', 'orthogonal-matrices', 'transpose']"
28,Prove ${\rm tr}\ (AA^T)={\rm tr}\ (A^TA)$ for any Matrix $A$,Prove  for any Matrix,{\rm tr}\ (AA^T)={\rm tr}\ (A^TA) A,"Prove ${\rm tr}\ (AA^T)={\rm tr}\ (A^TA)$ for any Matrix $A$ I know that each are always well defined and I have proved that, but I am struggling to write up a solid proof to equate them. I know they're equal. I tried to show that the main diagonal elements were the same but if I say that $A$ is $n\times m$ then  $$(AA^T)_{ii} = (a_{11}^2+\dots +a_{1m}^2) + \dots + (a_{n1}^2+\dots +a_{nm}^2)$$ and $$(A^TA)_{ii} = (a_{11}^2+\dots +a_{n1}^2) + \dots + (a_{1m}^2+\dots +a_{nm}^2)$$","Prove ${\rm tr}\ (AA^T)={\rm tr}\ (A^TA)$ for any Matrix $A$ I know that each are always well defined and I have proved that, but I am struggling to write up a solid proof to equate them. I know they're equal. I tried to show that the main diagonal elements were the same but if I say that $A$ is $n\times m$ then  $$(AA^T)_{ii} = (a_{11}^2+\dots +a_{1m}^2) + \dots + (a_{n1}^2+\dots +a_{nm}^2)$$ and $$(A^TA)_{ii} = (a_{11}^2+\dots +a_{n1}^2) + \dots + (a_{1m}^2+\dots +a_{nm}^2)$$",,"['linear-algebra', 'trace']"
29,"If $A^T=-A$, then A is not invertible","If , then A is not invertible",A^T=-A,"Let $n \in \mathbb{N}$ be odd and $A \in$Mat$(n,\mathbb{R})$ with $A^T=-A$. Show that $A$ is not invertible. I have no idea how to start this...","Let $n \in \mathbb{N}$ be odd and $A \in$Mat$(n,\mathbb{R})$ with $A^T=-A$. Show that $A$ is not invertible. I have no idea how to start this...",,"['linear-algebra', 'matrices', 'eigenvalues-eigenvectors', 'determinant']"
30,Why does the Fourier series of $x$ not seem to give the right value?,Why does the Fourier series of  not seem to give the right value?,x,"I'm reading a lecture about Fourier series , and it says that you can represent any continuous function as Fourier series. There's a given example: Let $f(x) = x$. $f(x) \approx \sum_{n=1}^\infty \frac{2(-1)^{n+1}}{n}\sin(nx).$ So I can understand that $5 = f(5) = \sum_{n=1}^\infty \frac{2(-1)^{n+1}}{n}\sin(5n)$ According to wolframalpha , the sum of this series is $\approx -1.283$ What is wrong here ..?","I'm reading a lecture about Fourier series , and it says that you can represent any continuous function as Fourier series. There's a given example: Let $f(x) = x$. $f(x) \approx \sum_{n=1}^\infty \frac{2(-1)^{n+1}}{n}\sin(nx).$ So I can understand that $5 = f(5) = \sum_{n=1}^\infty \frac{2(-1)^{n+1}}{n}\sin(5n)$ According to wolframalpha , the sum of this series is $\approx -1.283$ What is wrong here ..?",,"['linear-algebra', 'fourier-series']"
31,How to determine the rank and determinant of $A$?,How to determine the rank and determinant of ?,A,let $A$ be $$A_{a} = \begin{pmatrix} a & 1 & 1 & 1 \\ 1 & a & 1 & 1\\ 1 & 1 & a & 1\\ 1 & 1 & 1 & a \end{pmatrix}$$ How can I calculate the rank of $A$ by the Gauss' methode and $\det A$?,let $A$ be $$A_{a} = \begin{pmatrix} a & 1 & 1 & 1 \\ 1 & a & 1 & 1\\ 1 & 1 & a & 1\\ 1 & 1 & 1 & a \end{pmatrix}$$ How can I calculate the rank of $A$ by the Gauss' methode and $\det A$?,,['linear-algebra']
32,"Proving that if $A^n = 0$, then $I - A$ is invertible and $(I - A)^{-1} = I + A + \cdots + A^{n-1}$ [duplicate]","Proving that if , then  is invertible and  [duplicate]",A^n = 0 I - A (I - A)^{-1} = I + A + \cdots + A^{n-1},"This question already has answers here : Prove that $A+I$ is invertible if $A$ is nilpotent [duplicate] (4 answers) Closed 11 years ago . Let $A$ be a squared matrix, and suppose there exists an $n\in \Bbb N$ in a way that $A^n=0$. Show that $I-A$ is invertible and that $(I-A)^{-1}=I+A+\cdots+A^{n-1}$ I don't have a clue where to start from.","This question already has answers here : Prove that $A+I$ is invertible if $A$ is nilpotent [duplicate] (4 answers) Closed 11 years ago . Let $A$ be a squared matrix, and suppose there exists an $n\in \Bbb N$ in a way that $A^n=0$. Show that $I-A$ is invertible and that $(I-A)^{-1}=I+A+\cdots+A^{n-1}$ I don't have a clue where to start from.",,"['linear-algebra', 'matrices']"
33,Prove: The set of all polynomials p with p(2) = p(3) is a vector space,Prove: The set of all polynomials p with p(2) = p(3) is a vector space,,"Prove that this set is a vector space (by proving that it is a subspace of a known vector space). The set of all polynomials p with p(2) = p(3). I understand I need to satisfy, vector addition, scalar multiplication and show that it is non empty. I'm new to this concept so not even sure how to start. Do i maybe use P(2)-P(3)=0 instead? My concern is I'm not sure what two polynomials I need to add to prove vector addition ; proving scalar multiplication seems okay though. Thankyou also a follow up question; if I prove something is a subspace of a known vector space does this imply the subspace is a vector space. Or does that subspace has to span the entire vector space first? how would i prove this in this case?","Prove that this set is a vector space (by proving that it is a subspace of a known vector space). The set of all polynomials p with p(2) = p(3). I understand I need to satisfy, vector addition, scalar multiplication and show that it is non empty. I'm new to this concept so not even sure how to start. Do i maybe use P(2)-P(3)=0 instead? My concern is I'm not sure what two polynomials I need to add to prove vector addition ; proving scalar multiplication seems okay though. Thankyou also a follow up question; if I prove something is a subspace of a known vector space does this imply the subspace is a vector space. Or does that subspace has to span the entire vector space first? how would i prove this in this case?",,['linear-algebra']
34,Calculate the determinant of $A-5I$,Calculate the determinant of,A-5I,"Question Let $ A =  \begin{bmatrix} 1 & 2 & 3 & 4 & 5 \\ 6 & 7 & 8 & 9 & 10 \\ 11 & 12 & 13 & 14 & 15 \\ 16 & 17 & 18 & 19 & 20 \\ 21& 22 & 23 & 24 & 25 \end{bmatrix} $. Calculate the determinant of $A-5I$. My approach the nullity of $A$ is $3$, so the algebraic multiplicity of $\lambda = 0$ is $3$, i.e. $\lambda_1 = \lambda_2 = \lambda_3 = 0.$ Now trace($A$) = $\lambda_4 + \lambda_5 = 1+6+13+19+25 = 65$ Then det($A-5I$) = $(\lambda_1-5)(\lambda_2-5)(\lambda_3-5)(\lambda_4-5)(\lambda_5-5)=(-5)^3(\lambda_4\lambda_5 - 5 \times 65 + 25)$ We need to calculate the value of $\lambda_4 \lambda_5$, which includes sum of lots of determinant of $2 \times 2$ matrices. Is there any fast way to calculate the determinant?","Question Let $ A =  \begin{bmatrix} 1 & 2 & 3 & 4 & 5 \\ 6 & 7 & 8 & 9 & 10 \\ 11 & 12 & 13 & 14 & 15 \\ 16 & 17 & 18 & 19 & 20 \\ 21& 22 & 23 & 24 & 25 \end{bmatrix} $. Calculate the determinant of $A-5I$. My approach the nullity of $A$ is $3$, so the algebraic multiplicity of $\lambda = 0$ is $3$, i.e. $\lambda_1 = \lambda_2 = \lambda_3 = 0.$ Now trace($A$) = $\lambda_4 + \lambda_5 = 1+6+13+19+25 = 65$ Then det($A-5I$) = $(\lambda_1-5)(\lambda_2-5)(\lambda_3-5)(\lambda_4-5)(\lambda_5-5)=(-5)^3(\lambda_4\lambda_5 - 5 \times 65 + 25)$ We need to calculate the value of $\lambda_4 \lambda_5$, which includes sum of lots of determinant of $2 \times 2$ matrices. Is there any fast way to calculate the determinant?",,"['linear-algebra', 'determinant']"
35,Show $\pmatrix{A & B\\ C & I}$ is nonsingular given that $A-BC$ is nonsingular.,Show  is nonsingular given that  is nonsingular.,\pmatrix{A & B\\ C & I} A-BC,"I'm having some trouble showing that the block matrix $$D = \pmatrix{A & B\\ C & I}$$ is nonsingular, given that $A-BC$ is nonsingular. I have gotten close with the following by saying let $$T = \pmatrix{(A-BC)^{-1} & -B(A-BC)^{-1}\\-C(A-BC)^{-1} & A(A-BC)^{-1}}.$$ (Basically, I'm trying to extend the formula for the inverse of a $2\times2$ real matrix to block matrices.) Then $$\begin{align}DT &= \pmatrix{A & B\\ C & I}\pmatrix{(A-BC)^{-1} & -B(A-BC)^{-1}\\-C(A-BC)^{-1} & A(A-BC)^{-1}} \\ &= \pmatrix{A(A-BC)^{-1}-BC(A-BC)^{-1} & -AB(A-BC)^{-1}+BA(A-BC)^{-1}\\ C(A-BC)^{-1}-C(A-BC)^{-1} & -CB(A-BC)^{-1} + A(A-BC)^{-1}}\\ &=\pmatrix{(A-BC)(A-BC)^{-1} & (BA - AB)(A-BC)^{-1}\\ O & (A - BC)(A-BC)^{-1}}\\ &=\pmatrix{I & (BA - AB)(A-BC)^{-1}\\ O & I}.\end{align}$$ I manage to get almost everything, except the top-right block. I want it to be $O$, but I'm not sure what information I have that makes it equal $O$. I don't necessarily know that $A$ and $B$ commute, i.e., $AB = BA$, which would imply the top-right block would zero out. Can anyone see if I made a mistake? If there is no mistake, can anyone give a hint on what to do to try and zero out the top-right block?","I'm having some trouble showing that the block matrix $$D = \pmatrix{A & B\\ C & I}$$ is nonsingular, given that $A-BC$ is nonsingular. I have gotten close with the following by saying let $$T = \pmatrix{(A-BC)^{-1} & -B(A-BC)^{-1}\\-C(A-BC)^{-1} & A(A-BC)^{-1}}.$$ (Basically, I'm trying to extend the formula for the inverse of a $2\times2$ real matrix to block matrices.) Then $$\begin{align}DT &= \pmatrix{A & B\\ C & I}\pmatrix{(A-BC)^{-1} & -B(A-BC)^{-1}\\-C(A-BC)^{-1} & A(A-BC)^{-1}} \\ &= \pmatrix{A(A-BC)^{-1}-BC(A-BC)^{-1} & -AB(A-BC)^{-1}+BA(A-BC)^{-1}\\ C(A-BC)^{-1}-C(A-BC)^{-1} & -CB(A-BC)^{-1} + A(A-BC)^{-1}}\\ &=\pmatrix{(A-BC)(A-BC)^{-1} & (BA - AB)(A-BC)^{-1}\\ O & (A - BC)(A-BC)^{-1}}\\ &=\pmatrix{I & (BA - AB)(A-BC)^{-1}\\ O & I}.\end{align}$$ I manage to get almost everything, except the top-right block. I want it to be $O$, but I'm not sure what information I have that makes it equal $O$. I don't necessarily know that $A$ and $B$ commute, i.e., $AB = BA$, which would imply the top-right block would zero out. Can anyone see if I made a mistake? If there is no mistake, can anyone give a hint on what to do to try and zero out the top-right block?",,"['linear-algebra', 'matrices']"
36,Minimum of the given expression,Minimum of the given expression,,"For all real numbers $a$ and $b$ find the minimum of the following expression. $$(a-b)^2 + (2-a-b)^2 + (2a-3b)^2$$ I tried expressing the entire expression in terms of a single function of $a$ and $b$. For example, if the entire expression reduces to $(a-2b)^2+(a-2b)+5$ then its minimum can be easily found. But nothing seems to get this expression in such a form, because of the third unsymmetric square. Since there are two variables here we can also not use differentiation. Can you please provide hints on how to solve this?","For all real numbers $a$ and $b$ find the minimum of the following expression. $$(a-b)^2 + (2-a-b)^2 + (2a-3b)^2$$ I tried expressing the entire expression in terms of a single function of $a$ and $b$. For example, if the entire expression reduces to $(a-2b)^2+(a-2b)+5$ then its minimum can be easily found. But nothing seems to get this expression in such a form, because of the third unsymmetric square. Since there are two variables here we can also not use differentiation. Can you please provide hints on how to solve this?",,"['linear-algebra', 'algebra-precalculus', 'optimization', 'maxima-minima', 'quadratic-programming']"
37,Why is tensor product of linear maps defined as $(S\otimes T)(v\otimes w)=S(v)\otimes T(w)$?,Why is tensor product of linear maps defined as ?,(S\otimes T)(v\otimes w)=S(v)\otimes T(w),"In my understanding, the definition of tensor product of linear maps cannot be directly derived from the definition of tensor product of vector spaces (or modules), since it's not clear what is the domain, range, and the map of the result product. Originally I thought this is just a definition. But then I learned that if we represent $S$ and $T$ by matrices, then the matrix describing the tensor product $S \otimes T$ is the Kronecker product of the two matrices. However, Kronecker product of matrices can be developed totally without the concept of tensor product of linear maps. For example, we can find basis of the matrix spaces (say $V$ and $W$). Then we can get natural basis for $V \otimes W$. And we can easily get the definition of Kronecker product from these basis. So my question is: is this coincidence just a coincidence or there is some deep reason behind the definition so that it has to be defined like this. I can't even see why the domain of $S \otimes T$ should be $V\otimes W$, given $S$ and $T$ are linear maps over $V$ and $W$. (I know that the definition of tensor product of linear maps is very natural, and I can't imagine other definitions of it. I just thought there must be a formal reason that the definition has to be that. )","In my understanding, the definition of tensor product of linear maps cannot be directly derived from the definition of tensor product of vector spaces (or modules), since it's not clear what is the domain, range, and the map of the result product. Originally I thought this is just a definition. But then I learned that if we represent $S$ and $T$ by matrices, then the matrix describing the tensor product $S \otimes T$ is the Kronecker product of the two matrices. However, Kronecker product of matrices can be developed totally without the concept of tensor product of linear maps. For example, we can find basis of the matrix spaces (say $V$ and $W$). Then we can get natural basis for $V \otimes W$. And we can easily get the definition of Kronecker product from these basis. So my question is: is this coincidence just a coincidence or there is some deep reason behind the definition so that it has to be defined like this. I can't even see why the domain of $S \otimes T$ should be $V\otimes W$, given $S$ and $T$ are linear maps over $V$ and $W$. (I know that the definition of tensor product of linear maps is very natural, and I can't imagine other definitions of it. I just thought there must be a formal reason that the definition has to be that. )",,"['linear-algebra', 'abstract-algebra', 'tensor-products']"
38,Every Matrix a linear transformation?,Every Matrix a linear transformation?,,Every finite-dimensional linear map can be represented by a matrix. But what about the opposite: Does every matrix correspond to a linear map?,Every finite-dimensional linear map can be represented by a matrix. But what about the opposite: Does every matrix correspond to a linear map?,,"['linear-algebra', 'matrices']"
39,2-norm vs operator norm,2-norm vs operator norm,,"I have read that we define the ""2-norm"" of a matrix as $$\max_i \,{|\sigma_i|},$$ which I have also heard called the ""operator norm"" (here $\sigma_i$ are the singular values). Also we have the norms $$\|A\| = \left( \sum_{i,j}|a_{ij}|^q \right)^{1/q}$$ for every $q\geq 1$. Do we refer to these as $\|A\|_q$? (For $q=2$, I have heard this referred to as the ""Frobenius norm"".) If we do refer to them as $\|A\|_q$, then how can we reconcile the two meanings of the term ""two-norm""? Subquestion: How can we bound the values of $\|A\|$ for $q=1$ and $q=2$ in terms of each other?","I have read that we define the ""2-norm"" of a matrix as $$\max_i \,{|\sigma_i|},$$ which I have also heard called the ""operator norm"" (here $\sigma_i$ are the singular values). Also we have the norms $$\|A\| = \left( \sum_{i,j}|a_{ij}|^q \right)^{1/q}$$ for every $q\geq 1$. Do we refer to these as $\|A\|_q$? (For $q=2$, I have heard this referred to as the ""Frobenius norm"".) If we do refer to them as $\|A\|_q$, then how can we reconcile the two meanings of the term ""two-norm""? Subquestion: How can we bound the values of $\|A\|$ for $q=1$ and $q=2$ in terms of each other?",,"['linear-algebra', 'matrices', 'normed-spaces', 'matrix-norms']"
40,This set of matrices is open,This set of matrices is open,,"I'm trying to prove that the set of the matrices whose eigenvalues have non-zero real part is an open subset of $M^n$, the set of square matrices with order $n$ which is identify with $\mathbb R^{n^2}$. I don't know even how to begin, this question is really different to me, I don't how to use these topological concepts in relation with the eigenvalues of matrices. I really need help. Thanks in advance","I'm trying to prove that the set of the matrices whose eigenvalues have non-zero real part is an open subset of $M^n$, the set of square matrices with order $n$ which is identify with $\mathbb R^{n^2}$. I don't know even how to begin, this question is really different to me, I don't how to use these topological concepts in relation with the eigenvalues of matrices. I really need help. Thanks in advance",,"['linear-algebra', 'general-topology']"
41,How to show that linear map is invertible?,How to show that linear map is invertible?,,"a) Let  $L:V \to V$ be a linear map such that $L^2 + 2L + I = 0$, show that $L$ is invertible. b) Let $L:V \to V$ be a linear map such that $L^3 = 0$, show that $I - L$ is invertible. Here, $I$ is identity mapping. For first part, I know that $L^2 + 2L + I = (L+I)^2 = 0$, if $v\in V$ then $(L+I)^2 v = (L+I)(L(v) + v)) = 0$ so $L(v) + v$ is in null space of $L+I$, from here how do I show that $0$ is only in null space of $L$. I don't need exact solution. Hints would suffice.","a) Let  $L:V \to V$ be a linear map such that $L^2 + 2L + I = 0$, show that $L$ is invertible. b) Let $L:V \to V$ be a linear map such that $L^3 = 0$, show that $I - L$ is invertible. Here, $I$ is identity mapping. For first part, I know that $L^2 + 2L + I = (L+I)^2 = 0$, if $v\in V$ then $(L+I)^2 v = (L+I)(L(v) + v)) = 0$ so $L(v) + v$ is in null space of $L+I$, from here how do I show that $0$ is only in null space of $L$. I don't need exact solution. Hints would suffice.",,['linear-algebra']
42,Uniqueness of Hermitian inner product,Uniqueness of Hermitian inner product,,"Let V be an irreducible representation of a finite group G.How to show that up to scalars,there is a unique Hermitian inner product on V preserved by G. i know of how to get an inner product. but i have no idea on the uniqueness part. i think i have to use schur's lemma in some way","Let V be an irreducible representation of a finite group G.How to show that up to scalars,there is a unique Hermitian inner product on V preserved by G. i know of how to get an inner product. but i have no idea on the uniqueness part. i think i have to use schur's lemma in some way",,"['linear-algebra', 'representation-theory']"
43,Ellipse inscribed in an irregular quadrilateral,Ellipse inscribed in an irregular quadrilateral,,"I want to obtain the ellipse inscribed in the irregular quadrilateral (no parallel sides) defined by the four points A, B, C, D. I summarize the ideas given in the comments and answers: The is not an unique ellipse inside the given quadrilateral. For the unit square, there are infinite ellipses inscribed into it, with different eccentricities You cannot transform the unit square into a irregular quadrilateral using linear transformations, as those transform only two vectors into other two vectors. In this case we need to transform 4 vectors. As shown in this figure: Increasing the eccentricity, decreases the area. So the problem can be reduced to obtain the maximum area ellipse inscribed into the quadrilateral.","I want to obtain the ellipse inscribed in the irregular quadrilateral (no parallel sides) defined by the four points A, B, C, D. I summarize the ideas given in the comments and answers: The is not an unique ellipse inside the given quadrilateral. For the unit square, there are infinite ellipses inscribed into it, with different eccentricities You cannot transform the unit square into a irregular quadrilateral using linear transformations, as those transform only two vectors into other two vectors. In this case we need to transform 4 vectors. As shown in this figure: Increasing the eccentricity, decreases the area. So the problem can be reduced to obtain the maximum area ellipse inscribed into the quadrilateral.",,"['linear-algebra', 'geometry', 'conic-sections']"
44,How to determine which of the following transformations are linear transformations?,How to determine which of the following transformations are linear transformations?,,"Determine which of the following transformations are linear transformations A. The transformation $T_1$ defined by $T_1(x_1,x_2,x_3)=(x_1,0,x_3)$ B. The transformation $T_2$ defined by $T_2(x_1,x_2)=(2x_1−3x_2,x_1+4,5x_2)$ . C. The transformation $T_3$ defined by $T_3(x_1,x_2,x3)=(x_1,x_2,−x_3)$ D. The transformation $T_4$ defined by $T_4(x_1,x_2,x3)=(1,x_2,x_3)$ E. The transformation $T_5$ defined by $T_5(x_1,x_2)=(4x_1−2x_2,3|x_2|)$ . I believe that it could be A and E. How can I determine this? If someone could show me one I could figure out the rest.",Determine which of the following transformations are linear transformations A. The transformation defined by B. The transformation defined by . C. The transformation defined by D. The transformation defined by E. The transformation defined by . I believe that it could be A and E. How can I determine this? If someone could show me one I could figure out the rest.,"T_1 T_1(x_1,x_2,x_3)=(x_1,0,x_3) T_2 T_2(x_1,x_2)=(2x_1−3x_2,x_1+4,5x_2) T_3 T_3(x_1,x_2,x3)=(x_1,x_2,−x_3) T_4 T_4(x_1,x_2,x3)=(1,x_2,x_3) T_5 T_5(x_1,x_2)=(4x_1−2x_2,3|x_2|)","['linear-algebra', 'linear-transformations']"
45,Show that a matrix with (I) a row of zeros and (II) a column of zeros cannot be invertible (respectively),Show that a matrix with (I) a row of zeros and (II) a column of zeros cannot be invertible (respectively),,"Show that a matrix with a row of zeros cannot be invertible. Show that a matrix with a column of zeros cannot be invertible. What I tried: I tried to show that a matrix $A \in M_n (\mathbb{R})$ such that $(A)_{ij} = 0 \forall j \in \mathbb{N}, j\leq n$ and then show that $A A^{-1} \neq I$ but I got stuck.","Show that a matrix with a row of zeros cannot be invertible. Show that a matrix with a column of zeros cannot be invertible. What I tried: I tried to show that a matrix $A \in M_n (\mathbb{R})$ such that $(A)_{ij} = 0 \forall j \in \mathbb{N}, j\leq n$ and then show that $A A^{-1} \neq I$ but I got stuck.",,"['linear-algebra', 'matrices']"
46,Trace and the coefficients of the characteristic polynomial of a matrix,Trace and the coefficients of the characteristic polynomial of a matrix,,"Let $A\in M(\mathbb F)_{n \times n}$ Prove that the trace of $A$ is minus the coefficient of $\lambda ^{n-1}$ in the characteristic polynomial of $A$ . I had several ideas to approach this problem - the first one is to develop the characteristic polynomial through the Leibniz or Laplace formula, and from there to show that the contribution to the coefficient of $\lambda ^{n-1}$ is in fact minus the trace of A, but every time i tried it's a dead end. Another approach is to use induction on a similar matrix to ( $\lambda I-A$ ) from an upper triangular form, which has the eigenvalues of A on its diagonal, and of course the same determinant and trace, to show that for every choice of n this statement holds. I think my proof doesn't hold for all fields, so any thought on the matter will be much appreciated, or an explanation to why this statement is true.","Let Prove that the trace of is minus the coefficient of in the characteristic polynomial of . I had several ideas to approach this problem - the first one is to develop the characteristic polynomial through the Leibniz or Laplace formula, and from there to show that the contribution to the coefficient of is in fact minus the trace of A, but every time i tried it's a dead end. Another approach is to use induction on a similar matrix to ( ) from an upper triangular form, which has the eigenvalues of A on its diagonal, and of course the same determinant and trace, to show that for every choice of n this statement holds. I think my proof doesn't hold for all fields, so any thought on the matter will be much appreciated, or an explanation to why this statement is true.",A\in M(\mathbb F)_{n \times n} A \lambda ^{n-1} A \lambda ^{n-1} \lambda I-A,"['linear-algebra', 'matrices', 'eigenvalues-eigenvectors', 'trace', 'characteristic-polynomial']"
47,Having trouble understanding generalized complex numbers,Having trouble understanding generalized complex numbers,,"I'm reading a paper on the generalized complex numbers, but I have trouble in some of its fundamental properties. I have searched wiki but it left me none the wiser. Please see the image below, in which I have underlined the ""dual numbers"" with red because that's where I feel especially confused. My problems are: 1).The definition of the imaginary unit $i$ as $i^2=iq+p$ doesn't make me comfortable. Because I think it is a recursive one. Is there any other way to define $i$ equivalently but without recursion? 2).Why are all ""dual numbers"" located on the parabola: $p+\frac14q^2=0$? How to show it is equivalent to the definition ""$i^2=0$""? I have tried to derive it but I come to nothing. I don't think it should be very hard but I just fail do it. Could anyone give me some explanation or hint that is not beyond my limit? (I'm a freshman and so far I have learned a little elementary calculus and linear algebra) Thanks in advance!","I'm reading a paper on the generalized complex numbers, but I have trouble in some of its fundamental properties. I have searched wiki but it left me none the wiser. Please see the image below, in which I have underlined the ""dual numbers"" with red because that's where I feel especially confused. My problems are: 1).The definition of the imaginary unit $i$ as $i^2=iq+p$ doesn't make me comfortable. Because I think it is a recursive one. Is there any other way to define $i$ equivalently but without recursion? 2).Why are all ""dual numbers"" located on the parabola: $p+\frac14q^2=0$? How to show it is equivalent to the definition ""$i^2=0$""? I have tried to derive it but I come to nothing. I don't think it should be very hard but I just fail do it. Could anyone give me some explanation or hint that is not beyond my limit? (I'm a freshman and so far I have learned a little elementary calculus and linear algebra) Thanks in advance!",,"['linear-algebra', 'complex-analysis', 'complex-numbers', 'exterior-algebra']"
48,Linear Algebra: What do vector spaces represent?,Linear Algebra: What do vector spaces represent?,,"I understand what a vector can represent, but I still don't understand what a vector space represents. I understand that you can add two vectors and that becomes a vector space. What else can you do with them? What can you apply it to? I am taking a linear algebra course right now and understand the calculation steps, but not really what these tools are for. I hope that adds some context to my strange and probably vague question. Thanks!","I understand what a vector can represent, but I still don't understand what a vector space represents. I understand that you can add two vectors and that becomes a vector space. What else can you do with them? What can you apply it to? I am taking a linear algebra course right now and understand the calculation steps, but not really what these tools are for. I hope that adds some context to my strange and probably vague question. Thanks!",,"['linear-algebra', 'vector-spaces']"
49,Eigenvalue of $F^2$ and$ F$,Eigenvalue of  and,F^2  F,"Let $F$ be a linear operator on vector space $V$ over $\mathbb{C}$ and let $\lambda \in \mathbb{C}$ Show that if $\lambda^2$ is eigenvalue of linear operator $F^2$ then at least one of $\lambda,-\lambda$ is eigenvalue of $F$. What I tried to do was to write simply: $\exists v \ F^2(v)=\lambda^2v \Rightarrow \exists v \ F(F(v))=\lambda^2v \Rightarrow \exists v \ F(F(v))=\lambda\cdot\lambda\cdot v \ \vee F(F(v))=(-\lambda)\cdot(-\lambda)\cdot v$ But I am stuck here. Thought of applying $F^{-1}$ from both sides, but we don't know if such an operator exists and besides, it would lead me to nothing, I presume. I will appreciate any help.","Let $F$ be a linear operator on vector space $V$ over $\mathbb{C}$ and let $\lambda \in \mathbb{C}$ Show that if $\lambda^2$ is eigenvalue of linear operator $F^2$ then at least one of $\lambda,-\lambda$ is eigenvalue of $F$. What I tried to do was to write simply: $\exists v \ F^2(v)=\lambda^2v \Rightarrow \exists v \ F(F(v))=\lambda^2v \Rightarrow \exists v \ F(F(v))=\lambda\cdot\lambda\cdot v \ \vee F(F(v))=(-\lambda)\cdot(-\lambda)\cdot v$ But I am stuck here. Thought of applying $F^{-1}$ from both sides, but we don't know if such an operator exists and besides, it would lead me to nothing, I presume. I will appreciate any help.",,['linear-algebra']
50,what happens to rank of matrices when singular matrix multiply by non-singular matrix??,what happens to rank of matrices when singular matrix multiply by non-singular matrix??,,"I need to find the prove for any rectangular matrix $A$ and any non-singular matrices $P$, $Q$ of appropriate sizes, rank($PAQ$)= rank($A$) I know that when singular matrix multiply by non-singular the result will be singular matrix. However, It is not relevant to the rank of the matrix. If A is singular with rank 2, why rank($PAQ$)= 2 not any numbers.","I need to find the prove for any rectangular matrix $A$ and any non-singular matrices $P$, $Q$ of appropriate sizes, rank($PAQ$)= rank($A$) I know that when singular matrix multiply by non-singular the result will be singular matrix. However, It is not relevant to the rank of the matrix. If A is singular with rank 2, why rank($PAQ$)= 2 not any numbers.",,"['linear-algebra', 'matrices']"
51,Are there infinitely many $A\in \mathbb{C}^{2 \times 2}$ satisfying $A^3 = A$?,Are there infinitely many  satisfying ?,A\in \mathbb{C}^{2 \times 2} A^3 = A,Let $A$ be a $2 × 2$-matrix with complex entries. The number of $2 × 2$-matrices $A$ with complex entries satisfying the equation $A^3 = A$ is infinite. Is the above statement true? I know that $0$ and I are two solutions. But are there any more solutions?,Let $A$ be a $2 × 2$-matrix with complex entries. The number of $2 × 2$-matrices $A$ with complex entries satisfying the equation $A^3 = A$ is infinite. Is the above statement true? I know that $0$ and I are two solutions. But are there any more solutions?,,['linear-algebra']
52,Angle between two vectors?,Angle between two vectors?,,"I have been taught that the angle between two vectors is supposed to be their inner product. However, the book I'm reading states: Recall that the angle between two vectors $u	= (u_0,\ldots,u_{n−1})$ and $v = (v_0,\ldots, v_{n−1})$ in $\mathbb{C}^n$ (the complex plane) is just a scaling factor times their inner product . What is a ""scaling factor""?","I have been taught that the angle between two vectors is supposed to be their inner product. However, the book I'm reading states: Recall that the angle between two vectors $u	= (u_0,\ldots,u_{n−1})$ and $v = (v_0,\ldots, v_{n−1})$ in $\mathbb{C}^n$ (the complex plane) is just a scaling factor times their inner product . What is a ""scaling factor""?",,['linear-algebra']
53,Is the rank of the sum of two positive semi-definite matrices larger than their individual ranks?,Is the rank of the sum of two positive semi-definite matrices larger than their individual ranks?,,"How to prove $\operatorname{rank}(X+Y) \geq \min(\operatorname{rank}(X),\operatorname{rank}(Y))$, where $X$ and $Y$ are both positive semi-definite matrices?","How to prove $\operatorname{rank}(X+Y) \geq \min(\operatorname{rank}(X),\operatorname{rank}(Y))$, where $X$ and $Y$ are both positive semi-definite matrices?",,"['linear-algebra', 'matrices']"
54,"What definition of ""nearly orthogonal"" would result in ""In a 10,000-dimensional space there are millions of nearly orthogonal vectors""?","What definition of ""nearly orthogonal"" would result in ""In a 10,000-dimensional space there are millions of nearly orthogonal vectors""?",,"Quanta Magazine's April 13, 2023 A New Approach to Computation Reimagines Artificial Intelligence starts with: By imbuing enormous vectors with semantic meaning, we can get machines to reason more abstractly — and efficiently — than before. Later on, during the explanation are the paragraphs: The vectors must be distinct. This distinctness can be quantified by a property called orthogonality, which means to be at right angles. In 3D space, there are three vectors that are orthogonal to each other: One in the x direction, another in the y and a third in the z. In 10,000-dimensional space, there are 10,000 such mutually orthogonal vectors. But if we allow vectors to be nearly orthogonal, the number of such distinct vectors in a high-dimensional space explodes. In a 10,000-dimensional space there are millions of nearly orthogonal vectors. I remember reading previous questions here with high dimensions and dot products are discussed and seeing comments about how easy it is to get very small or even zero dot products in high dimensions, but I've never worked outside of one, two and three dimensional problems. Question: What definition of ""nearly orthogonal"" would result in ""In a 10,000-dimensional space there are millions of nearly orthogonal vectors""? Would it be for example dot product 1 smaller than some number like 0.1? 1 of the presumably normalized vectors","Quanta Magazine's April 13, 2023 A New Approach to Computation Reimagines Artificial Intelligence starts with: By imbuing enormous vectors with semantic meaning, we can get machines to reason more abstractly — and efficiently — than before. Later on, during the explanation are the paragraphs: The vectors must be distinct. This distinctness can be quantified by a property called orthogonality, which means to be at right angles. In 3D space, there are three vectors that are orthogonal to each other: One in the x direction, another in the y and a third in the z. In 10,000-dimensional space, there are 10,000 such mutually orthogonal vectors. But if we allow vectors to be nearly orthogonal, the number of such distinct vectors in a high-dimensional space explodes. In a 10,000-dimensional space there are millions of nearly orthogonal vectors. I remember reading previous questions here with high dimensions and dot products are discussed and seeing comments about how easy it is to get very small or even zero dot products in high dimensions, but I've never worked outside of one, two and three dimensional problems. Question: What definition of ""nearly orthogonal"" would result in ""In a 10,000-dimensional space there are millions of nearly orthogonal vectors""? Would it be for example dot product 1 smaller than some number like 0.1? 1 of the presumably normalized vectors",,"['linear-algebra', 'geometry', 'inner-products']"
55,$\operatorname{rank}(AB-BA)=1$ implies $AB-BA$ is nilpotent,implies  is nilpotent,\operatorname{rank}(AB-BA)=1 AB-BA,"Let $A$ and $B$ be $n \times n$ complex matrices. Then $\operatorname{rank}(AB-BA)=1$ implies $AB-BA$ is nilpotent. I have seen proof here that $\operatorname{rank}(AB-BA)=1$ implies $A$ and $B$ are simultaneously triangularisable, which implies that $AB-BA$ is nilpotent. I was wondering if there is an easier way to show the result. (I could show $A$ and $B$ share a common eigenvector but that does not imply $AB-BA$ is nilpotent.)","Let and be complex matrices. Then implies is nilpotent. I have seen proof here that implies and are simultaneously triangularisable, which implies that is nilpotent. I was wondering if there is an easier way to show the result. (I could show and share a common eigenvector but that does not imply is nilpotent.)",A B n \times n \operatorname{rank}(AB-BA)=1 AB-BA \operatorname{rank}(AB-BA)=1 A B AB-BA A B AB-BA,"['linear-algebra', 'matrices', 'eigenvalues-eigenvectors', 'matrix-rank']"
56,A matrix raised to a high power ($87$),A matrix raised to a high power (),87,"So, I have this matrix: $$\pmatrix {0&0&0&-1\\1&0&0&0\\0&1&0&0\\0&0&1&0}^{87}$$ My teacher never discussed eigenvalues. So, I do not know what they are and there must be another way to do this (without multiplying the matrix $87$ times). Thanks for your help.","So, I have this matrix: $$\pmatrix {0&0&0&-1\\1&0&0&0\\0&1&0&0\\0&0&1&0}^{87}$$ My teacher never discussed eigenvalues. So, I do not know what they are and there must be another way to do this (without multiplying the matrix $87$ times). Thanks for your help.",,"['linear-algebra', 'matrices']"
57,Calculate inner product of two vectors,Calculate inner product of two vectors,,"I know that the title is vague, at least for what I am asking.  Anyway, this is an exercise that I've been struggling with for the past two hours, it seems relatively simple on a first glance, and I've tried a method which proved to be wrong. I'll go through what I've tried, but before I do I will briefly provide an overview of the exercise: Let $ e_1, e_2, e_3$ be a basis in space $\mathbb R$ $ u = e_1 + 2e_2 +3e_3 $   and $ v = 2e_1  - 3e_2 + 4e_3 $ $ |e_1| = 3, |e_2| = 4, |e_3| = 2$ $ e_1  \cdot e_2 = -6 $ $ e_1 \cdot e_3 = 4  $ $ e_2 \cdot e_3 = 4$ Calculate $ u \cdot v $ so this is what I've tried doing: From $u \cdot v = |u|  |v|  \cos \theta$ I tried plugging the data we are given above into the forumla: $ \frac{u \cdot v}{|u||v|} = \cos\theta $ so one example would be: $ \frac{e_1 \cdot e_2}{|e_1| |e_2|} = \cos\theta $ $ \frac{ -6}{3 \times 4} = \frac{-1}{2}$ so $\cos\theta = -0.5$ of course the other two give me $\cos\theta = 0.5$ I then, individually, get the length of vector u and vector v by using pythagoras: $ |u| = \sqrt{1^2 + 2^2 + 3^3 } = \sqrt{14} $ On a first glance, this does not seem right... applying the same 'logic' to $ |v| = \ldots = \sqrt{29} $ I then plug the data into the aforementioned formula ($u \cdot v = |u| |v| \cos\theta$ ) and basically get gibberish, it does not make any sense. I am doing something completely wrong, that I know, I am just not sure how to use the provided data to get an answer that is $ = 0 $ I'd sincerely love to get an answer as I've been on this exercise for over two hours to no avail. EDIT: Thank you all! It does make sense now that I look at it, and I somehow tried to over-complicate things (which is actually a real struggle of mine). However, Thanks to your help, I managed to solve it.","I know that the title is vague, at least for what I am asking.  Anyway, this is an exercise that I've been struggling with for the past two hours, it seems relatively simple on a first glance, and I've tried a method which proved to be wrong. I'll go through what I've tried, but before I do I will briefly provide an overview of the exercise: Let $ e_1, e_2, e_3$ be a basis in space $\mathbb R$ $ u = e_1 + 2e_2 +3e_3 $   and $ v = 2e_1  - 3e_2 + 4e_3 $ $ |e_1| = 3, |e_2| = 4, |e_3| = 2$ $ e_1  \cdot e_2 = -6 $ $ e_1 \cdot e_3 = 4  $ $ e_2 \cdot e_3 = 4$ Calculate $ u \cdot v $ so this is what I've tried doing: From $u \cdot v = |u|  |v|  \cos \theta$ I tried plugging the data we are given above into the forumla: $ \frac{u \cdot v}{|u||v|} = \cos\theta $ so one example would be: $ \frac{e_1 \cdot e_2}{|e_1| |e_2|} = \cos\theta $ $ \frac{ -6}{3 \times 4} = \frac{-1}{2}$ so $\cos\theta = -0.5$ of course the other two give me $\cos\theta = 0.5$ I then, individually, get the length of vector u and vector v by using pythagoras: $ |u| = \sqrt{1^2 + 2^2 + 3^3 } = \sqrt{14} $ On a first glance, this does not seem right... applying the same 'logic' to $ |v| = \ldots = \sqrt{29} $ I then plug the data into the aforementioned formula ($u \cdot v = |u| |v| \cos\theta$ ) and basically get gibberish, it does not make any sense. I am doing something completely wrong, that I know, I am just not sure how to use the provided data to get an answer that is $ = 0 $ I'd sincerely love to get an answer as I've been on this exercise for over two hours to no avail. EDIT: Thank you all! It does make sense now that I look at it, and I somehow tried to over-complicate things (which is actually a real struggle of mine). However, Thanks to your help, I managed to solve it.",,"['linear-algebra', 'inner-products']"
58,Can we show that all $2 \times 2$ matrices are sums of matrices with determinant 1?,Can we show that all  matrices are sums of matrices with determinant 1?,2 \times 2,"I came across a paper on the Sums of 2-by-2 Matrices with Determinant One . In the paper, which I have conveniently indicated here for reference, the author claims, but without proof, that a $2 \times 2$ is a sum of elements of the special linear group, $SL_2(\mathbb{F})$, whose elements, $U$, are also $2 \times 2$ matrices, such that $|U|=1$. I was thinking of proving this by either technique. Let $A= \begin{bmatrix}a & b\\c & d\end{bmatrix}$, and $a, b, c, d \in \mathbb{F}$. Technique 1. Consider the following 4 matrices with determinant 1: $M_1= \begin{bmatrix}e & 0\\0 & 1/e\end{bmatrix}$, $M_2 = \begin{bmatrix}0 & -f\\1/f & 0\end{bmatrix}$, $M_3 = \begin{bmatrix}1/h & 0\\0 & h\end{bmatrix}$, and $M_4 = \begin{bmatrix}0 & 1/g\\-g & 0\end{bmatrix}$. We show that that $\sum\limits_{i=1}^4 M_i = A$. Thus, we have $e + 1/h = a$, $1/e + h = d$, $-f + 1/g = b$, $1/f - g = c$. Technique 2. Consider $\{U_i\}_{i=1} ^\infty \in SL_2(\mathbb{F})$. Show that the sum of a countable number of $U_i$s is $A$. The problem I have here is that I don't know how to proceed from here. I don't know if either of these will be considered correct, though. I hope someone could help me out here. Thanks.","I came across a paper on the Sums of 2-by-2 Matrices with Determinant One . In the paper, which I have conveniently indicated here for reference, the author claims, but without proof, that a $2 \times 2$ is a sum of elements of the special linear group, $SL_2(\mathbb{F})$, whose elements, $U$, are also $2 \times 2$ matrices, such that $|U|=1$. I was thinking of proving this by either technique. Let $A= \begin{bmatrix}a & b\\c & d\end{bmatrix}$, and $a, b, c, d \in \mathbb{F}$. Technique 1. Consider the following 4 matrices with determinant 1: $M_1= \begin{bmatrix}e & 0\\0 & 1/e\end{bmatrix}$, $M_2 = \begin{bmatrix}0 & -f\\1/f & 0\end{bmatrix}$, $M_3 = \begin{bmatrix}1/h & 0\\0 & h\end{bmatrix}$, and $M_4 = \begin{bmatrix}0 & 1/g\\-g & 0\end{bmatrix}$. We show that that $\sum\limits_{i=1}^4 M_i = A$. Thus, we have $e + 1/h = a$, $1/e + h = d$, $-f + 1/g = b$, $1/f - g = c$. Technique 2. Consider $\{U_i\}_{i=1} ^\infty \in SL_2(\mathbb{F})$. Show that the sum of a countable number of $U_i$s is $A$. The problem I have here is that I don't know how to proceed from here. I don't know if either of these will be considered correct, though. I hope someone could help me out here. Thanks.",,"['linear-algebra', 'matrices']"
59,"If $A$ and $B$ are $n\times n$ complex matrices , then $AB-BA=I$ is impossible. [duplicate]","If  and  are  complex matrices , then  is impossible. [duplicate]",A B n\times n AB-BA=I,"This question already has answers here : $AB-BA=I$ having no solutions (6 answers) Closed 7 years ago . If $A$ and $B$ are $n\times n$ complex matrices , then $AB-BA=I$ is   impossible I understand this by any example. But how can one explain it generally?","This question already has answers here : $AB-BA=I$ having no solutions (6 answers) Closed 7 years ago . If $A$ and $B$ are $n\times n$ complex matrices , then $AB-BA=I$ is   impossible I understand this by any example. But how can one explain it generally?",,"['linear-algebra', 'matrices', 'linear-transformations']"
60,"Existence of a ""basis"" for the symmetric positive definite matrices","Existence of a ""basis"" for the symmetric positive definite matrices",,"Let $P_{\text{sym}}$ denote the convex cone of the symmetric positive definite real matrices (of size $n \times n$). Question: Is there a finite subset $B$ of matrices in $P_{\text{sym}}$ such that every matrix in $P_{\text{sym}}$ is a (non-zero) conic combination of elements of $B$? Note: It is not possible for such an expression to be unique ; Assuming otherwise, we have a finite set $\{P_1,...,P_k \} \subseteq P_{\text{sym}}$ such that any matrix $P \in P_{\text{sym}}$ can be expressed uniquely as: $P=\sum_{i=1}^k a_iP_i$ where all $a_i \ge 0$ and $(a_1,...,a_k)\neq \bar 0$. This would imply $P_{\text{sym}}$ is homeomorphic to the set $D_k:= \{(a_1,...,a_k) \in \mathbb{R}^k | a_i \ge 0 \}$. (via the map $(a_1,...,a_k) \to \sum_{i=1}^k a_iP_i$). However, it is known that $P_{\text{sym}}$ is homeomorphic to $\mathbb{R}^{n(n+1)/2}$, so $\mathbb{R}^{n(n+1)/2} \cong D_k$. For $k \neq \frac{n(n+1)}{2}$ this is impossible, since this would imply an open subset of $\mathbb{R}^k$ is homeomorphic to an open subset of $\mathbb{R}^{n(n+1)/2}$, contradicting the invariance of domain. For $k=\frac{n(n+1)}{2}$, this is also impossible; Invariance of domain implies the only subsets of $\mathbb{R}^n$ which are homeomorphic to it are open, and $D_k$ is not open.","Let $P_{\text{sym}}$ denote the convex cone of the symmetric positive definite real matrices (of size $n \times n$). Question: Is there a finite subset $B$ of matrices in $P_{\text{sym}}$ such that every matrix in $P_{\text{sym}}$ is a (non-zero) conic combination of elements of $B$? Note: It is not possible for such an expression to be unique ; Assuming otherwise, we have a finite set $\{P_1,...,P_k \} \subseteq P_{\text{sym}}$ such that any matrix $P \in P_{\text{sym}}$ can be expressed uniquely as: $P=\sum_{i=1}^k a_iP_i$ where all $a_i \ge 0$ and $(a_1,...,a_k)\neq \bar 0$. This would imply $P_{\text{sym}}$ is homeomorphic to the set $D_k:= \{(a_1,...,a_k) \in \mathbb{R}^k | a_i \ge 0 \}$. (via the map $(a_1,...,a_k) \to \sum_{i=1}^k a_iP_i$). However, it is known that $P_{\text{sym}}$ is homeomorphic to $\mathbb{R}^{n(n+1)/2}$, so $\mathbb{R}^{n(n+1)/2} \cong D_k$. For $k \neq \frac{n(n+1)}{2}$ this is impossible, since this would imply an open subset of $\mathbb{R}^k$ is homeomorphic to an open subset of $\mathbb{R}^{n(n+1)/2}$, contradicting the invariance of domain. For $k=\frac{n(n+1)}{2}$, this is also impossible; Invariance of domain implies the only subsets of $\mathbb{R}^n$ which are homeomorphic to it are open, and $D_k$ is not open.",,"['linear-algebra', 'matrices', 'positive-definite']"
61,Show that the set of Hermitian matrices forms a real vector space,Show that the set of Hermitian matrices forms a real vector space,,"How can I ""show that the Hermitian Matrices form a real Vector Space""? I'm assuming this means the set of all Hermitian matrices. I understand how a hermitian matrix containing complex numbers can be closed under scalar multiplication by multiplying it by $i$ , but how can it be closed under addition? Wouldn't adding two complex matrices just produce another complex matrix in most instances? Also, how can I find a basis for this space?","How can I ""show that the Hermitian Matrices form a real Vector Space""? I'm assuming this means the set of all Hermitian matrices. I understand how a hermitian matrix containing complex numbers can be closed under scalar multiplication by multiplying it by , but how can it be closed under addition? Wouldn't adding two complex matrices just produce another complex matrix in most instances? Also, how can I find a basis for this space?",i,"['linear-algebra', 'matrices', 'vector-spaces', 'hermitian-matrices']"
62,How do I find the matrix with respect to a different basis?,How do I find the matrix with respect to a different basis?,,"I tried to solve this question but the answer is totally different, can you explain how to solve it","I tried to solve this question but the answer is totally different, can you explain how to solve it",,"['linear-algebra', 'matrices', 'transformation']"
63,Why does a singular matrix imply that it does not have a solution?,Why does a singular matrix imply that it does not have a solution?,,"(Trying to learn linear algebra over here) The augmented matrix in question: $$\begin{bmatrix}0 & 1 &5 & -4\\1 & 4 & 3 & 2\\2 & 7 & 1 & -2\end{bmatrix}$$ So I tried to solve the matrix above but I couldn't. I decided to see what happened when I pushed it through Numpy (Python): numpy.linalg.linalg.LinAlgError: Singular matrix So I went back to the definition for a singular matrix: A square matrix that is not invertible is called singular or   degenerate. The book simply says it is inconsistent. So, now I'm wondering: How does a singular matrix relate to it not having a solution?","(Trying to learn linear algebra over here) The augmented matrix in question: $$\begin{bmatrix}0 & 1 &5 & -4\\1 & 4 & 3 & 2\\2 & 7 & 1 & -2\end{bmatrix}$$ So I tried to solve the matrix above but I couldn't. I decided to see what happened when I pushed it through Numpy (Python): numpy.linalg.linalg.LinAlgError: Singular matrix So I went back to the definition for a singular matrix: A square matrix that is not invertible is called singular or   degenerate. The book simply says it is inconsistent. So, now I'm wondering: How does a singular matrix relate to it not having a solution?",,"['linear-algebra', 'matrices']"
64,What are these math symbols?,What are these math symbols?,,I'm studying linear algebra and all of a sudden the symbol $\dot{+}$ appears. For example:  $a*(v \dot{+} w) = a*v \dot{+} a*w$ Any idea what it might be? Also two more symbols. they are on top of $0$ in equations: - and ~. For example $u \dot{+} w = \bar{0}$ and $\tilde{0}=\bar{0} + \tilde{0} = \bar{0}$. Sorry if those questions are stupid but let's just say that linear algebra is not my favourite subject that I take in university 😝,I'm studying linear algebra and all of a sudden the symbol $\dot{+}$ appears. For example:  $a*(v \dot{+} w) = a*v \dot{+} a*w$ Any idea what it might be? Also two more symbols. they are on top of $0$ in equations: - and ~. For example $u \dot{+} w = \bar{0}$ and $\tilde{0}=\bar{0} + \tilde{0} = \bar{0}$. Sorry if those questions are stupid but let's just say that linear algebra is not my favourite subject that I take in university 😝,,['linear-algebra']
65,Show: $(x+y)^4 \leq 8(x^4 + y^4)$,Show:,(x+y)^4 \leq 8(x^4 + y^4),"I wish to show the following statement: $ \forall x,y \in \mathbb{R} $ $$ (x+y)^4 \leq 8(x^4 + y^4)  $$ What is the scope for generalisaion? Edit: Apparently the above inequality can be shown using the Cauchy-Schwarz inequality. Could someone please elaborate,  stating the vectors you are using in the Cauchy-Schwarz inequality: $\ \ \forall \ \ v,w \in V, $ an inner product space, $$|\langle v,w\rangle|^2 \leq \langle v,v \rangle \cdot \langle w,w \rangle$$ where $\langle v,w\rangle$ is an inner product.","I wish to show the following statement: $ \forall x,y \in \mathbb{R} $ $$ (x+y)^4 \leq 8(x^4 + y^4)  $$ What is the scope for generalisaion? Edit: Apparently the above inequality can be shown using the Cauchy-Schwarz inequality. Could someone please elaborate,  stating the vectors you are using in the Cauchy-Schwarz inequality: $\ \ \forall \ \ v,w \in V, $ an inner product space, $$|\langle v,w\rangle|^2 \leq \langle v,v \rangle \cdot \langle w,w \rangle$$ where $\langle v,w\rangle$ is an inner product.",,"['linear-algebra', 'algebra-precalculus', 'inequality', 'inner-products']"
66,Proof that $e^x$ is the eigenvector of the derivative operator,Proof that  is the eigenvector of the derivative operator,e^x,I remember hearing my professor talk about how $e^x$ shows up in all our differential equations because it is the eigenvector for the derivative operator. Can someone explain and prove this to me? I have taken Linear algebra and a course on ODEs and a little bit of PDEs. EDIT: Specifically I am wondering: know how you take a matrix that represents a linear operator and subtract lambda off the diagonals and then solve for the eigenvalues and eigenvectors? Is there a similar proof that results in e^x?,I remember hearing my professor talk about how $e^x$ shows up in all our differential equations because it is the eigenvector for the derivative operator. Can someone explain and prove this to me? I have taken Linear algebra and a course on ODEs and a little bit of PDEs. EDIT: Specifically I am wondering: know how you take a matrix that represents a linear operator and subtract lambda off the diagonals and then solve for the eigenvalues and eigenvectors? Is there a similar proof that results in e^x?,,"['linear-algebra', 'eigenvalues-eigenvectors', 'differential']"
67,Vector space decomposed by operator $T$ such that $T^2=I$.,Vector space decomposed by operator  such that .,T T^2=I,"Let $T$ be a linear operator on a finite dimensional vector space $V$ such that $T^2$ is the identity operator. Prove that for any $v\in V$, $v-T(v)$ is either $0_V$ or is an eigenvector with eigenvalue $-1$. Prove that $V=V(1)\oplus V(-1)$. *$V(\lambda)$ denotes eigenspace I have no problem showing that $v-T(v)$ is either $0_V$ or is an eigenvector with eigenvalue $-1$. From this we see that $v\in V(1)$ or $v-T(v)\in V(-1)$ for every $v\in V$. Also, no problem showing that $V(1)$ and $V(-1)$ are independent. However, I am stuck on showing that $V=V(1)+V(-1)$. So far: We know that either $v\in V(1)$ or $v\notin V(1)$. If the former, then $v=v+0\in V(1)+V(-1)$. If $v\notin V(1)$, then we know that $v-T(v)\in V(-1)$. Then we could write $$ v = T(v)+v-T(v), $$ and if $T(v)\in V(1)$, then we are done. But I haven't been able to prove nor disprove $T(v)\in V(1)$. Am I on the right track? I feel like I am missing something completely obvious. Somehow I think I should use that $T$ is invertible, and hence has trivial kernel, but I do not see how. And I would love to see different solutions for this problem, if they exist.","Let $T$ be a linear operator on a finite dimensional vector space $V$ such that $T^2$ is the identity operator. Prove that for any $v\in V$, $v-T(v)$ is either $0_V$ or is an eigenvector with eigenvalue $-1$. Prove that $V=V(1)\oplus V(-1)$. *$V(\lambda)$ denotes eigenspace I have no problem showing that $v-T(v)$ is either $0_V$ or is an eigenvector with eigenvalue $-1$. From this we see that $v\in V(1)$ or $v-T(v)\in V(-1)$ for every $v\in V$. Also, no problem showing that $V(1)$ and $V(-1)$ are independent. However, I am stuck on showing that $V=V(1)+V(-1)$. So far: We know that either $v\in V(1)$ or $v\notin V(1)$. If the former, then $v=v+0\in V(1)+V(-1)$. If $v\notin V(1)$, then we know that $v-T(v)\in V(-1)$. Then we could write $$ v = T(v)+v-T(v), $$ and if $T(v)\in V(1)$, then we are done. But I haven't been able to prove nor disprove $T(v)\in V(1)$. Am I on the right track? I feel like I am missing something completely obvious. Somehow I think I should use that $T$ is invertible, and hence has trivial kernel, but I do not see how. And I would love to see different solutions for this problem, if they exist.",,['linear-algebra']
68,Prove that the integral operator has no eigenvalues [closed],Prove that the integral operator has no eigenvalues [closed],,"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 3 years ago . Improve this question Let $V$ be the vector space of all real valued continuous functions. Prove that the linear operator $\displaystyle\int_{0}^{x}f(t)dt$ has no eigenvalues.","Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 3 years ago . Improve this question Let $V$ be the vector space of all real valued continuous functions. Prove that the linear operator $\displaystyle\int_{0}^{x}f(t)dt$ has no eigenvalues.",,['linear-algebra']
69,Determinant of symmetric matrix,Determinant of symmetric matrix,,"Given the following matrix, is there a way to compute the determinant other than using laplace till there're $3\times3$ determinants? \begin{pmatrix}   2 & 1 &1 &1&1 \\   1 & 2 & 1& 1 &1\\   1& 1 & 2 & 1  &1\\  1&1 &1 &2&1\\  1&1&1&1&-2  \end{pmatrix}","Given the following matrix, is there a way to compute the determinant other than using laplace till there're $3\times3$ determinants? \begin{pmatrix}   2 & 1 &1 &1&1 \\   1 & 2 & 1& 1 &1\\   1& 1 & 2 & 1  &1\\  1&1 &1 &2&1\\  1&1&1&1&-2  \end{pmatrix}",,"['linear-algebra', 'matrices', 'determinant']"
70,Image of function definition notation,Image of function definition notation,,"In my Linear Algebra and Geometry textbook, it defines the image of a linear transformation $T$ as: $$\operatorname{Im}\, (T) := \{\; w \in W : \; w=Tv \;\;\text{ for some } v \in V \} $$ As far as I can see, this is just the same as: $$\operatorname{Im} \, (T) := \{ \;Tv \in W : \;v \in V\}$$ Is there any difference in these definitions? If not, why is the first one used?","In my Linear Algebra and Geometry textbook, it defines the image of a linear transformation $T$ as: $$\operatorname{Im}\, (T) := \{\; w \in W : \; w=Tv \;\;\text{ for some } v \in V \} $$ As far as I can see, this is just the same as: $$\operatorname{Im} \, (T) := \{ \;Tv \in W : \;v \in V\}$$ Is there any difference in these definitions? If not, why is the first one used?",,"['linear-algebra', 'functions', 'notation', 'transformation']"
71,Is the product of two positive semidefinite matrices positive semidefinite?,Is the product of two positive semidefinite matrices positive semidefinite?,,"If $X$ and $W$ are real, square, symmetric, positive semidefinite matrices of the same dimension, does $XW + WX$ have to be positive semidefinite? This is not homework.","If $X$ and $W$ are real, square, symmetric, positive semidefinite matrices of the same dimension, does $XW + WX$ have to be positive semidefinite? This is not homework.",,"['linear-algebra', 'matrices']"
72,Power a Matrix (Without calculating),Power a Matrix (Without calculating),,"So, I'm reading a linear algebra book (Fraleigh's - self learning) and one of the exercises about Matrix product is to power the matrix: $$   \left( \begin{matrix}   0 & 1\\ -1 & 1  \end{matrix} \right)  $$ to the 2001st power. But can't see how without calculating. Any tip? I have the ""feeling"" that given that 1 only changes on its sig then its the same the power of 3 that the power of 2001 but I'm not sure. I'm correct?","So, I'm reading a linear algebra book (Fraleigh's - self learning) and one of the exercises about Matrix product is to power the matrix: $$   \left( \begin{matrix}   0 & 1\\ -1 & 1  \end{matrix} \right)  $$ to the 2001st power. But can't see how without calculating. Any tip? I have the ""feeling"" that given that 1 only changes on its sig then its the same the power of 3 that the power of 2001 but I'm not sure. I'm correct?",,"['linear-algebra', 'matrices']"
73,Example of a non-linear isometry?,Example of a non-linear isometry?,,Is there a simple example of an isometry between normed vector spaces that is not an affine map?,Is there a simple example of an isometry between normed vector spaces that is not an affine map?,,['linear-algebra']
74,$A^{T}A$ positive definite then A is invertible?,positive definite then A is invertible?,A^{T}A,"Say if $A$ is an $n \times  n$ matrix, why is it that if $A^{T}A$ is positive definite, the matrix $A$ is then invertible? All I know is $A^{T}A$ gives a symmetric matrix but what does $A^{T}A$ is positive definite tell or imply or hint about the matrix $A$ itself that leads to the fact that it will be invertible?","Say if $A$ is an $n \times  n$ matrix, why is it that if $A^{T}A$ is positive definite, the matrix $A$ is then invertible? All I know is $A^{T}A$ gives a symmetric matrix but what does $A^{T}A$ is positive definite tell or imply or hint about the matrix $A$ itself that leads to the fact that it will be invertible?",,"['linear-algebra', 'matrices']"
75,Linear Algebra: different determinant answers,Linear Algebra: different determinant answers,,"I'm having a problem verifying my answer to this question: Solve for x: $$\left| \begin{array}{cc} x+3 & 2 \\ 1 & x+2 \end{array} \right| = 0$$ I get: $(x+3)(x+2)-2=0$ $(x+3)(x+2)=2$ thus: $x+3=2$ and    $x+2=2$ $x=-1$ and $x=0$ The book says that $x=-1$ and $x=-4$ is the correct answer. I tried doing it a different way by expanding and got totally different answers: $x^2+5x=-4$ $x(x+5)=-4$ $x=-4$ and $x=-9$ What is going on here?","I'm having a problem verifying my answer to this question: Solve for x: $$\left| \begin{array}{cc} x+3 & 2 \\ 1 & x+2 \end{array} \right| = 0$$ I get: $(x+3)(x+2)-2=0$ $(x+3)(x+2)=2$ thus: $x+3=2$ and    $x+2=2$ $x=-1$ and $x=0$ The book says that $x=-1$ and $x=-4$ is the correct answer. I tried doing it a different way by expanding and got totally different answers: $x^2+5x=-4$ $x(x+5)=-4$ $x=-4$ and $x=-9$ What is going on here?",,['linear-algebra']
76,Does anyone know an interesting introductory topic involving vector spaces over the rationals,Does anyone know an interesting introductory topic involving vector spaces over the rationals,,"Many introductory books on vector spaces mention that the scalars need not be reals, and might even have sections discussing complex vector spaces or vector spaces over the integers mod 2.  I have never seen any such book mention that all of the theory goes through as well if one restricts the scalars to be just rational numbers.  Perhaps this is because there is a dearth of interesting problems about such vector spaces accessible at this level that couldn't simply be discussed in the context of real scalars. I wonder if there is an interesting introductory-level problem or topic about vector spaces that would be most naturally conducted by allowing rational number scalars.  Does anyone know of such, perhaps one with a number-theoretic aspect? (By introductory:  I envision a first course on linear algebra, including non-math majors.  They would be seeing vector spaces (and that level of abstraction) for the first time.  Perhaps they would be seeing matrix multiplication for the first time.  Usually, in my experience, such courses primarily use the real numbers as scalars.)","Many introductory books on vector spaces mention that the scalars need not be reals, and might even have sections discussing complex vector spaces or vector spaces over the integers mod 2.  I have never seen any such book mention that all of the theory goes through as well if one restricts the scalars to be just rational numbers.  Perhaps this is because there is a dearth of interesting problems about such vector spaces accessible at this level that couldn't simply be discussed in the context of real scalars. I wonder if there is an interesting introductory-level problem or topic about vector spaces that would be most naturally conducted by allowing rational number scalars.  Does anyone know of such, perhaps one with a number-theoretic aspect? (By introductory:  I envision a first course on linear algebra, including non-math majors.  They would be seeing vector spaces (and that level of abstraction) for the first time.  Perhaps they would be seeing matrix multiplication for the first time.  Usually, in my experience, such courses primarily use the real numbers as scalars.)",,['linear-algebra']
77,Show that the characteristic polynomial is the same as the minimal polynomial,Show that the characteristic polynomial is the same as the minimal polynomial,,"Let $$A =\begin{pmatrix}0 & 0 & c \\1 & 0 & b \\ 0& 1 & a\end{pmatrix}$$ Show that the characteristic and minimal polynomials of $A$ are the same. I have already computated the characteristic polynomial $$p_A(x)=x^3-ax^2-bx-c$$ and I know from here that if I could show that the eigenspaces of $A$ all have dimension $1$ , I would be done. The problem is that solving for the eigenvalues of this (very general) cubic equation is difficult (albeit possible), meaning it would be difficult to find bases for the eigenspaces. A hint would be appreciated.","Let Show that the characteristic and minimal polynomials of are the same. I have already computated the characteristic polynomial and I know from here that if I could show that the eigenspaces of all have dimension , I would be done. The problem is that solving for the eigenvalues of this (very general) cubic equation is difficult (albeit possible), meaning it would be difficult to find bases for the eigenspaces. A hint would be appreciated.",A =\begin{pmatrix}0 & 0 & c \\1 & 0 & b \\ 0& 1 & a\end{pmatrix} A p_A(x)=x^3-ax^2-bx-c A 1,"['linear-algebra', 'matrices', 'eigenvalues-eigenvectors', 'jordan-normal-form', 'minimal-polynomials']"
78,How many principal minors a 3x3 matrix can have?,How many principal minors a 3x3 matrix can have?,,"I'm reading the book Polarimetric Radar Imaging: From basics to applications On page 75, there's this claim: $$T_3 = \begin{bmatrix} 	2A_0 & C-jD & H+jG\\ 	C+jD & B_0+B & E+jF\\ 	H-jG & E-jF & B_0-B 	\end{bmatrix}$$  As the coherency $T_3$ matrix in such a case is a rank 1 Hermitian matrix, it follows that its nine principal minors are zero, with: $$\bbox[yellow]{2A_0(B_0+B)-C^2-D^2=0}\\ 	\bbox[yellow]{2A_0(B_0-B)-G^2-H^2=0}\\ 	-2A_0E+CH-DG=0\\ 	\bbox[yellow]{B_0^2-B^2-E^2-F^2=0}\\ 	C(B_0-B)-EH-GF=0\\ 	-D(B_0-B)+FH-GE=0\\ 	2A_0F-CG-DH=0\\ 	-G(B_0+B)+FC-ED=0\\ 	H(B_0+B)-CE-DF=0$$ I am wondering because I know that a $3\times 3$ matrix has nine $2\times 2$ submatrices and so nine minors, but only 3 of them (the highlighted ones are principal minors)?!! The others are only submatrices (not principals) and so the other six equations are resulted from $\Re{|A_{({1,3},{1,2})}|}=0$,$\Im{|A_{({1,3},{1,2})}|}=0$,$\Re{|A_{({2,3},{1,2})}|}=0$,$\Im{|A_{({2,3},{1,2})}|}=0$,$\Re{|A_{({2,3},{1,3})}|}=0$ , $\Im{|A_{({2,3},{1,3})}|}=0$ Knowing that when a $3\times 3$ matrix is of rank 1, the determinant of all of its $2\times 2$ submatrices ($2\times 2$ minors) should be zero not just the principal ones.","I'm reading the book Polarimetric Radar Imaging: From basics to applications On page 75, there's this claim: $$T_3 = \begin{bmatrix} 	2A_0 & C-jD & H+jG\\ 	C+jD & B_0+B & E+jF\\ 	H-jG & E-jF & B_0-B 	\end{bmatrix}$$  As the coherency $T_3$ matrix in such a case is a rank 1 Hermitian matrix, it follows that its nine principal minors are zero, with: $$\bbox[yellow]{2A_0(B_0+B)-C^2-D^2=0}\\ 	\bbox[yellow]{2A_0(B_0-B)-G^2-H^2=0}\\ 	-2A_0E+CH-DG=0\\ 	\bbox[yellow]{B_0^2-B^2-E^2-F^2=0}\\ 	C(B_0-B)-EH-GF=0\\ 	-D(B_0-B)+FH-GE=0\\ 	2A_0F-CG-DH=0\\ 	-G(B_0+B)+FC-ED=0\\ 	H(B_0+B)-CE-DF=0$$ I am wondering because I know that a $3\times 3$ matrix has nine $2\times 2$ submatrices and so nine minors, but only 3 of them (the highlighted ones are principal minors)?!! The others are only submatrices (not principals) and so the other six equations are resulted from $\Re{|A_{({1,3},{1,2})}|}=0$,$\Im{|A_{({1,3},{1,2})}|}=0$,$\Re{|A_{({2,3},{1,2})}|}=0$,$\Im{|A_{({2,3},{1,2})}|}=0$,$\Re{|A_{({2,3},{1,3})}|}=0$ , $\Im{|A_{({2,3},{1,3})}|}=0$ Knowing that when a $3\times 3$ matrix is of rank 1, the determinant of all of its $2\times 2$ submatrices ($2\times 2$ minors) should be zero not just the principal ones.",,"['linear-algebra', 'determinant', 'matrix-rank']"
79,Prove $\ker(T^n)=\ker(T^{n+1})$ and $\operatorname{range}(T^n)=\operatorname{range}(T^{n+1}).$,Prove  and,\ker(T^n)=\ker(T^{n+1}) \operatorname{range}(T^n)=\operatorname{range}(T^{n+1}).,"Let $V$ be an $n$-dimensional vector space over a field $F$ and $T$ an operator on $V.$ Prove $\ker(T^n)=\ker(T^{n+1})$ and $\operatorname{range}(T^n)=\operatorname{range}(T^{n+1}).$ Suppose $v \in \ker(T^n).$ Then $T^n(v) = 0,$ implying that $T^{n+1}(v)=T(T^n(v))=T(0)=0.$ Thus $v \in \ker(T^{n+1})$ and so $\ker(T^n) \subset \ker(T^{n+1}).$ Question: How do I prove  $\ker(T^n) \supset \ker(T^{n+1})?$ I would also like some hints on proving $\operatorname{range}(T^n)=\operatorname{range}(T^{n+1}).$","Let $V$ be an $n$-dimensional vector space over a field $F$ and $T$ an operator on $V.$ Prove $\ker(T^n)=\ker(T^{n+1})$ and $\operatorname{range}(T^n)=\operatorname{range}(T^{n+1}).$ Suppose $v \in \ker(T^n).$ Then $T^n(v) = 0,$ implying that $T^{n+1}(v)=T(T^n(v))=T(0)=0.$ Thus $v \in \ker(T^{n+1})$ and so $\ker(T^n) \subset \ker(T^{n+1}).$ Question: How do I prove  $\ker(T^n) \supset \ker(T^{n+1})?$ I would also like some hints on proving $\operatorname{range}(T^n)=\operatorname{range}(T^{n+1}).$",,['linear-algebra']
80,Does a symmetric matrix with all entries $0$ or $1$ and with diagonal $0$ have integer eigenvalues?,Does a symmetric matrix with all entries  or  and with diagonal  have integer eigenvalues?,0 1 0,"Suppose $A$ is an $n×n$ symmetric matrix with all entries $0$ or $1$, and with diagonal $0$. Are all of the eigenvalues of $A$ integers? It works for all the cases I have tried so far.","Suppose $A$ is an $n×n$ symmetric matrix with all entries $0$ or $1$, and with diagonal $0$. Are all of the eigenvalues of $A$ integers? It works for all the cases I have tried so far.",,"['linear-algebra', 'matrices', 'eigenvalues-eigenvectors']"
81,Trace of a matrix $A$ with $A^2=I$,Trace of a matrix  with,A A^2=I,Let $A$ be a complex-value square matrix with $A^2=I$ identity. Then is the trace of $A$ a real value?,Let $A$ be a complex-value square matrix with $A^2=I$ identity. Then is the trace of $A$ a real value?,,['linear-algebra']
82,"Proof of triangle inequality for $d(x,y)=\sqrt{\lvert x-y\rvert}$",Proof of triangle inequality for,"d(x,y)=\sqrt{\lvert x-y\rvert}","There is this problem that says: show that $d(x,y)=\sqrt{\lvert x-y\rvert}$ is a distance function on $\mathbb{R}$, and I am unable to proof the triangle inequality for this? any suggestion I look forward to.  thanks.","There is this problem that says: show that $d(x,y)=\sqrt{\lvert x-y\rvert}$ is a distance function on $\mathbb{R}$, and I am unable to proof the triangle inequality for this? any suggestion I look forward to.  thanks.",,"['linear-algebra', 'inequality', 'metric-spaces']"
83,Determinant of a special $n\times n$ matrix [duplicate],Determinant of a special  matrix [duplicate],n\times n,"This question already has answers here : Determinant of a rank $1$ update of a scalar matrix, or characteristic polynomial of a rank $1$ matrix (2 answers) Closed 9 years ago . Compute the determinant of the nun matrix: $$ \begin{pmatrix} 2 &  1  & \ldots & 1 \\ 1  &  2 & \ldots & 1\\ \vdots & \vdots & \ddots & \vdots\\ 1  &   1       &\ldots & 2 \end{pmatrix} $$ For $n=2$, I have$$ \begin{pmatrix} 2 &  1   \\ 1  &  2  \end{pmatrix} $$ Then $det = 3$. For $n=3$, we have $$ \begin{pmatrix} 2 &  1 & 1\\ 1  &  2 & 1\\ 1 & 1 & 2 \\ \end{pmatrix} $$ Then $det = 4$. For $n=4$ again we have $$ \begin{pmatrix} 2 &  1  & 1 & 1 \\ 1  &  2 & 1 & 1\\ 1 & 1 & 2 & 1\\ 1  &   1  & 1 & 2 \end{pmatrix} $$ Then $det = 5$ How can I prove that the determinant of nun matrix is $n+1$.","This question already has answers here : Determinant of a rank $1$ update of a scalar matrix, or characteristic polynomial of a rank $1$ matrix (2 answers) Closed 9 years ago . Compute the determinant of the nun matrix: $$ \begin{pmatrix} 2 &  1  & \ldots & 1 \\ 1  &  2 & \ldots & 1\\ \vdots & \vdots & \ddots & \vdots\\ 1  &   1       &\ldots & 2 \end{pmatrix} $$ For $n=2$, I have$$ \begin{pmatrix} 2 &  1   \\ 1  &  2  \end{pmatrix} $$ Then $det = 3$. For $n=3$, we have $$ \begin{pmatrix} 2 &  1 & 1\\ 1  &  2 & 1\\ 1 & 1 & 2 \\ \end{pmatrix} $$ Then $det = 4$. For $n=4$ again we have $$ \begin{pmatrix} 2 &  1  & 1 & 1 \\ 1  &  2 & 1 & 1\\ 1 & 1 & 2 & 1\\ 1  &   1  & 1 & 2 \end{pmatrix} $$ Then $det = 5$ How can I prove that the determinant of nun matrix is $n+1$.",,"['linear-algebra', 'matrices', 'determinant']"
84,Is this a linear transformation?,Is this a linear transformation?,,Let $T$ be a transformation from $P_2$ to $P_2$ (where $P_2$ is the space of all polynomials with degree less than or equal to $2$) $$T(f(t)) = f''(t)f(t)$$ I'm tempted to say that this is not a linear transformation because $$T(f(t) + g(t)) = (f''(t) + g''(t))(f(t) + g(t))$$ Which does not equal $$T(f(t)) + T(g(t))$$ But I'm not sure if I did that correctly...,Let $T$ be a transformation from $P_2$ to $P_2$ (where $P_2$ is the space of all polynomials with degree less than or equal to $2$) $$T(f(t)) = f''(t)f(t)$$ I'm tempted to say that this is not a linear transformation because $$T(f(t) + g(t)) = (f''(t) + g''(t))(f(t) + g(t))$$ Which does not equal $$T(f(t)) + T(g(t))$$ But I'm not sure if I did that correctly...,,['linear-algebra']
85,Finite multiplicative group of matrices with sum of traces equal to zero.,Finite multiplicative group of matrices with sum of traces equal to zero.,,"Let  $G=\{M_1,M_2,...,M_k\}$ be a finite set such that $ M_i\in M_n(\mathbb R)$ and $(G,\;\cdot\:)$ is a group with matrix multiplication. If $\sum _{i=1}^k \operatorname{tr} (M_i)=0$ then how to prove $\sum _{i=1}^k M_i=0$ ?","Let  $G=\{M_1,M_2,...,M_k\}$ be a finite set such that $ M_i\in M_n(\mathbb R)$ and $(G,\;\cdot\:)$ is a group with matrix multiplication. If $\sum _{i=1}^k \operatorname{tr} (M_i)=0$ then how to prove $\sum _{i=1}^k M_i=0$ ?",,"['linear-algebra', 'matrices', 'group-theory', 'contest-math']"
86,Prove that if $A$ is invertible matrix then $AA^T$ and $A^T A$ are also invertible.,Prove that if  is invertible matrix then  and  are also invertible.,A AA^T A^T A,"I want to know how to prove this question: Prove that if $A$ is invertible matrix then $AA^T$ and $A^T A$ are also invertible. My attempt: Since $A$ is invertible we have that $AA^{-1} = I$ and if we denote $B = A^{-1}$, we have that $AB=I$ so if we take the transpose of both sides then we have $(AB)^T = I^T = I$, but this is where I get my problem since the transpose, inverse of $B$ isn't equal to $A$.","I want to know how to prove this question: Prove that if $A$ is invertible matrix then $AA^T$ and $A^T A$ are also invertible. My attempt: Since $A$ is invertible we have that $AA^{-1} = I$ and if we denote $B = A^{-1}$, we have that $AB=I$ so if we take the transpose of both sides then we have $(AB)^T = I^T = I$, but this is where I get my problem since the transpose, inverse of $B$ isn't equal to $A$.",,['linear-algebra']
87,Is the determinant of a matrix lower when all its elements are lower?,Is the determinant of a matrix lower when all its elements are lower?,,"Problem Consider a generic matrix $A$, we are going to think of a simple case by taking into consideration a $3 \times 3$ matrix: $$ A = \begin{pmatrix} a_{1,1} & a_{1,2} & a_{1,3}\\ a_{2,1} & a_{2,2} & a_{2,3}\\ a_{3,1} & a_{3,2} & a_{3,3}\\ \end{pmatrix} $$ Consider now having $A'$ as: $$ A' = \begin{pmatrix} a'_{1,1} & a'_{1,2} & a'_{1,3}\\ a'_{2,1} & a'_{2,2} & a'_{2,3}\\ a'_{3,1} & a'_{3,2} & a'_{3,3}\\ \end{pmatrix} $$ The following holds: $$a'_{i,j} \leq a_{i,j}$$ Question I would like to know if the following: $$|A'| \leq |A|$$ If it holds, can you prove it? Another problem What if we considered: $$ a_{i,j} \leq 1, a'_{i,j} \leq 1 $$ Considering also that $A$ is a stochastic matrix? This does not mean that both $A$ and $A'$ are stochastic. I am considering $A$ stochastic and $A'$ obtained as a reduced version of $A$ so that $A'$ is not stochastic but its values are all between 0 and 1.","Problem Consider a generic matrix $A$, we are going to think of a simple case by taking into consideration a $3 \times 3$ matrix: $$ A = \begin{pmatrix} a_{1,1} & a_{1,2} & a_{1,3}\\ a_{2,1} & a_{2,2} & a_{2,3}\\ a_{3,1} & a_{3,2} & a_{3,3}\\ \end{pmatrix} $$ Consider now having $A'$ as: $$ A' = \begin{pmatrix} a'_{1,1} & a'_{1,2} & a'_{1,3}\\ a'_{2,1} & a'_{2,2} & a'_{2,3}\\ a'_{3,1} & a'_{3,2} & a'_{3,3}\\ \end{pmatrix} $$ The following holds: $$a'_{i,j} \leq a_{i,j}$$ Question I would like to know if the following: $$|A'| \leq |A|$$ If it holds, can you prove it? Another problem What if we considered: $$ a_{i,j} \leq 1, a'_{i,j} \leq 1 $$ Considering also that $A$ is a stochastic matrix? This does not mean that both $A$ and $A'$ are stochastic. I am considering $A$ stochastic and $A'$ obtained as a reduced version of $A$ so that $A'$ is not stochastic but its values are all between 0 and 1.",,"['linear-algebra', 'matrices', 'determinant']"
88,"Using linear algebra, how is the Binet formula (for finding the nth Fibonacci number) derived?","Using linear algebra, how is the Binet formula (for finding the nth Fibonacci number) derived?",,"If possible, please refrain from any type of proof besides linear algebra. So, using the recursion formula $F_{n+1} = F_{n-1} + F_n$, for $n\gt 1$, and where $F_0 = 0$ and $F_1 = 1$, and the Fibonacci matrix, derive the golden ratio and ultimately the Binet formula.","If possible, please refrain from any type of proof besides linear algebra. So, using the recursion formula $F_{n+1} = F_{n-1} + F_n$, for $n\gt 1$, and where $F_0 = 0$ and $F_1 = 1$, and the Fibonacci matrix, derive the golden ratio and ultimately the Binet formula.",,"['linear-algebra', 'fibonacci-numbers']"
89,"How does one prove that if $f$ and $g$ are linear functionals on $V$ such that $h=fg$ is also a linear functional, then either $f=0$ or $g=0?$","How does one prove that if  and  are linear functionals on  such that  is also a linear functional, then either  or",f g V h=fg f=0 g=0?,"I am self-studying Hoffman and Kunze's book Linear Algebra . This is the exercise 13 from page 106. Let $\mathbb{F}$ be a subfield of the field of complex numbers and let $V$ be any vector space over $\mathbb{F}.$ Suppose that $f$ and $g$ are linear functionals on $V$ such that the function $h$ defined by $h(v)=f(v)g(v)$ is also a linear functional on $V$. Prove that either $f=0$ or $g=0.$ I was able to show that $h=0$. Therefore $V=\operatorname {Ker} (f)\cup \operatorname{Ker}(g)$. I am assuming that $f\neq 0$ and I would like to show that $\operatorname {Ker} (f)\subset \operatorname {Ker} (g)$, but I wasn't able to acomplish that. I would appreciate your help.","I am self-studying Hoffman and Kunze's book Linear Algebra . This is the exercise 13 from page 106. Let $\mathbb{F}$ be a subfield of the field of complex numbers and let $V$ be any vector space over $\mathbb{F}.$ Suppose that $f$ and $g$ are linear functionals on $V$ such that the function $h$ defined by $h(v)=f(v)g(v)$ is also a linear functional on $V$. Prove that either $f=0$ or $g=0.$ I was able to show that $h=0$. Therefore $V=\operatorname {Ker} (f)\cup \operatorname{Ker}(g)$. I am assuming that $f\neq 0$ and I would like to show that $\operatorname {Ker} (f)\subset \operatorname {Ker} (g)$, but I wasn't able to acomplish that. I would appreciate your help.",,[]
90,If $A\in M_{n}(\mathbb{C})$ is a Hermitian matrix  so $I-iA$ is Invertible,If  is a Hermitian matrix  so  is Invertible,A\in M_{n}(\mathbb{C}) I-iA,I'd love your help with this. Let $A\in M_{n}(\mathbb{C})$ be a Hermitian matrix $(A^{*}=A)$. How can I prove that $I-iA$ is Invertible? Thank you,I'd love your help with this. Let $A\in M_{n}(\mathbb{C})$ be a Hermitian matrix $(A^{*}=A)$. How can I prove that $I-iA$ is Invertible? Thank you,,['linear-algebra']
91,Best way to exactly solve a linear system (300x300) with integer coefficients,Best way to exactly solve a linear system (300x300) with integer coefficients,,"I want to solve system of linear equations of size 300x300 (300 variables & 300 equations) exactly (not with floating point, aka dgesl , but with fractions in answer ). All coefficients in this system are integer (e.g. 32 bit), there is only 1 solution to it. There are non-zero constant terms in right column (b). A*x = b where A is matrix of coefficients, b is vector of constant terms. answer is x vector, given in rational numbers (fractions of pairs of very long integers). The matrix A is dense (general case), but it can have up to 60 % of zero coefficients. What is the best way (fastest algorithm for x86/x86_64) to solve such system? Thanks! PS typical answer of such systems have integers in fraction up to 50-80 digits long, so, please don't suggest anything based only on float/doubles. They have no needed precision.","I want to solve system of linear equations of size 300x300 (300 variables & 300 equations) exactly (not with floating point, aka dgesl , but with fractions in answer ). All coefficients in this system are integer (e.g. 32 bit), there is only 1 solution to it. There are non-zero constant terms in right column (b). A*x = b where A is matrix of coefficients, b is vector of constant terms. answer is x vector, given in rational numbers (fractions of pairs of very long integers). The matrix A is dense (general case), but it can have up to 60 % of zero coefficients. What is the best way (fastest algorithm for x86/x86_64) to solve such system? Thanks! PS typical answer of such systems have integers in fraction up to 50-80 digits long, so, please don't suggest anything based only on float/doubles. They have no needed precision.",,"['linear-algebra', 'matrices', 'fractions']"
92,Is there any relation between the eigenvalues of possibly non-Hermitian matrix A and those of exp(A)?,Is there any relation between the eigenvalues of possibly non-Hermitian matrix A and those of exp(A)?,,"Is there any relation between the eigenvalues of possibly non-Hermitian matrix A and those of exp(A)? For hermitian matrices, they are just exponentials of the corresponding values. But in general, any relation? Thanks.","Is there any relation between the eigenvalues of possibly non-Hermitian matrix A and those of exp(A)? For hermitian matrices, they are just exponentials of the corresponding values. But in general, any relation? Thanks.",,['linear-algebra']
93,"Eigenvalues and ""Eigenvectors"" of Linear Transformations","Eigenvalues and ""Eigenvectors"" of Linear Transformations",,"I am so lost on this question I am not sure even where to start.  I am not looking for an answer but more of a turn in the right direction. Find all the eigenvalues and eigenfunctions of the following linear transformation:  $T(f(x)) = f(-x)$ from $\mathbf{P}_2$ to $\mathbf{P}_2$.  IS T diagonalizable? All I know for sure is that the standard basis is $1$, $x$, $x^2$, and if this is an eigenbasis for $T$ then $T$ is diagonalizable. Thanks for any assistance!","I am so lost on this question I am not sure even where to start.  I am not looking for an answer but more of a turn in the right direction. Find all the eigenvalues and eigenfunctions of the following linear transformation:  $T(f(x)) = f(-x)$ from $\mathbf{P}_2$ to $\mathbf{P}_2$.  IS T diagonalizable? All I know for sure is that the standard basis is $1$, $x$, $x^2$, and if this is an eigenbasis for $T$ then $T$ is diagonalizable. Thanks for any assistance!",,"['linear-algebra', 'eigenvalues-eigenvectors']"
94,Using a block matrix to show that $A$ and $B$ commute,Using a block matrix to show that  and  commute,A B,"I'm studying for a PhD qualifying exam in linear algebra and I wanted to ask about the following problem: Let $A$ and $B$ be invertible $n\times n$ matrices. Let $M$ be the matrix \begin{bmatrix}     A & B \\     B^{-1} & A^{-1} \\     \end{bmatrix} Assume that $M$ has rank $n$ . Prove that $A$ and $B$ commute. Here's what I got so far. We know that $$0 = \det (M) = \det(A) \det \left( A^{-1} - B^{-1} A^{-1} B \right)$$ since $M$ is rank deficient. Since $A$ is invertible, $\det(A)\neq 0 $ , so $\det(A^{-1}-B^{-1}A^{-1}B) = 0$ . I would like to somehow conclude that $A^{-1}-B^{-1}A^{-1}B = 0$ , which would give the desired result. But I only have that this matrix is singular, not necessarily $0$ . Does anyone have any suggestions for what I can try?","I'm studying for a PhD qualifying exam in linear algebra and I wanted to ask about the following problem: Let and be invertible matrices. Let be the matrix Assume that has rank . Prove that and commute. Here's what I got so far. We know that since is rank deficient. Since is invertible, , so . I would like to somehow conclude that , which would give the desired result. But I only have that this matrix is singular, not necessarily . Does anyone have any suggestions for what I can try?","A B n\times n M \begin{bmatrix}
    A & B \\
    B^{-1} & A^{-1} \\
    \end{bmatrix} M n A B 0 = \det (M) = \det(A) \det \left( A^{-1} - B^{-1} A^{-1} B \right) M A \det(A)\neq 0  \det(A^{-1}-B^{-1}A^{-1}B) = 0 A^{-1}-B^{-1}A^{-1}B = 0 0","['linear-algebra', 'matrices', 'matrix-rank', 'block-matrices']"
95,A linear map $T: \mathbb{R^3 \to \mathbb{R^3}}$ has a two dimensional invariant subspace.,A linear map  has a two dimensional invariant subspace.,T: \mathbb{R^3 \to \mathbb{R^3}},Let $T: \mathbb{R^3 \to \mathbb{R^3}}$ be an $\mathbb{R}$ -linear map. Then I want to show that $T$ has a $2$ dimensional invariant subspace of $\mathbb{R^3}.$ I considered all possible minimal polynomial of $T$ and applying canonical forms I found some obvious $2$ dimensional invariant subspaces. I stuck when the minimal polynomial is of the form $(X-a)^3$ for some real number $a.$ In this situation since the minimal polynomial and the characteristic polynomial coincides $T$ has a cyclic vector. But I can't complete it further. I need some help. Thanks.,Let be an -linear map. Then I want to show that has a dimensional invariant subspace of I considered all possible minimal polynomial of and applying canonical forms I found some obvious dimensional invariant subspaces. I stuck when the minimal polynomial is of the form for some real number In this situation since the minimal polynomial and the characteristic polynomial coincides has a cyclic vector. But I can't complete it further. I need some help. Thanks.,T: \mathbb{R^3 \to \mathbb{R^3}} \mathbb{R} T 2 \mathbb{R^3}. T 2 (X-a)^3 a. T,"['linear-algebra', 'abstract-algebra', 'matrices', 'linear-transformations', 'jordan-normal-form']"
96,How did Einstein integrate $\frac{\partial \tau}{\partial x'}+\frac{v}{c^2-v^2}\frac{\partial \tau}{\partial t}=0$?,How did Einstein integrate ?,\frac{\partial \tau}{\partial x'}+\frac{v}{c^2-v^2}\frac{\partial \tau}{\partial t}=0,"In his paper ""On the Electrodynamics of Moving Bodies"", Einstein writes the equation $$\dfrac{\partial \tau}{\partial x'}+\dfrac{v}{c^2-v^2}\dfrac{\partial \tau}{\partial t}=0$$ where $\tau=\tau(x',y,z,t)$ is a linear function (i.e. $\tau=Ax'+By+Cz+Dt$ ) $x'=x-vt$ $\dfrac{\partial \tau}{\partial y}=0$ (i.e. $B=0$ ) $\dfrac{\partial \tau}{\partial z}=0$ (i.e. $C=0$ ) $c$ is a constant $x,x',y,z,t,v$ are variables and he derives that $$\tau=a\left(t-\dfrac{v}{c^2-v^2}x'\right)$$ where $a=a(v)$ Could someone please walk me through step-by-step how he derived this? I am not very familiar with integrals invovling partial derivatives, so I would be appreciative if any answers are quite explicit. Additionally, if I modified the question to say that $\tau$ was an affine function (i.e. $\tau=Ax'+By+Cz+Dt+E$ ), would it make any difference to the result (I suspect it wouldn't)?","In his paper ""On the Electrodynamics of Moving Bodies"", Einstein writes the equation where is a linear function (i.e. ) (i.e. ) (i.e. ) is a constant are variables and he derives that where Could someone please walk me through step-by-step how he derived this? I am not very familiar with integrals invovling partial derivatives, so I would be appreciative if any answers are quite explicit. Additionally, if I modified the question to say that was an affine function (i.e. ), would it make any difference to the result (I suspect it wouldn't)?","\dfrac{\partial \tau}{\partial x'}+\dfrac{v}{c^2-v^2}\dfrac{\partial \tau}{\partial t}=0 \tau=\tau(x',y,z,t) \tau=Ax'+By+Cz+Dt x'=x-vt \dfrac{\partial \tau}{\partial y}=0 B=0 \dfrac{\partial \tau}{\partial z}=0 C=0 c x,x',y,z,t,v \tau=a\left(t-\dfrac{v}{c^2-v^2}x'\right) a=a(v) \tau \tau=Ax'+By+Cz+Dt+E","['linear-algebra', 'partial-differential-equations', 'linear-transformations', 'partial-derivative']"
97,Why do similar matrices represent the same linear transformation? [duplicate],Why do similar matrices represent the same linear transformation? [duplicate],,"This question already has answers here : Similar Matrices and Linear Transformations (2 answers) Closed 5 years ago . I don’t understand the theorem: $A$ and $B$ are similar if and only if they represent the same linear transformation. I know one direction ""If $A$ and $B$ represent the same linear transformation then they are similar"", but why is the other direction also true?  (I did only basic linear algebra so could you please give an easy proof if possible! Or any intuitions for this theorem! Thank you!)","This question already has answers here : Similar Matrices and Linear Transformations (2 answers) Closed 5 years ago . I don’t understand the theorem: and are similar if and only if they represent the same linear transformation. I know one direction ""If and represent the same linear transformation then they are similar"", but why is the other direction also true?  (I did only basic linear algebra so could you please give an easy proof if possible! Or any intuitions for this theorem! Thank you!)",A B A B,['linear-algebra']
98,"How many distinct triples of non-negative $\lbrace{x,y,z\mid x,y,z\in \mathbb Z\rbrace}$ satisfy $2x+y+z=16$?",How many distinct triples of non-negative  satisfy ?,"\lbrace{x,y,z\mid x,y,z\in \mathbb Z\rbrace} 2x+y+z=16","Consider $$2x+y+z=16$$ how many distinct, non-negative triples of $\lbrace{x,y,z|x,y,z\in \mathbb Z\rbrace}$ are there that satisfy the equation? I assumed that in this question, the role of combinatorics play a vital role, so I thought of it in this order: if the triples did not have to be distinct, there would be $16$ options the first time a number is chosen, $16$ the second time a number is chosen, and $16$ the third time a number is chosen, hence there are $16^3$ permutations where repetition is allowed. Then, when repetition is not allowed, the first time a number is chosen there $16$ options, then there are $15$ options, and then $14$, but from here not another number is chosen, so assuming there are $16!$ permutations is senseless. Therefore the remaining $13!$ permutations need to be discounted, hence there are $\displaystyle \frac{16!}{(16-3)!}$ permutations when repetition is not allowed. So does this imply there are $3360$ possible triples? This is more or less where I encountered the flaw in my logic: When the first number is chosen, there aren't actually $15$ remaining numbers to choose from. Because say for example you choose $y$ to be $16$, then both $x$ and $z$ must be $0$, i.e. there is actually only one choice once the first number has been chosen, and one choice once the second number has been chosen. But, what if we let $x=9$? Well, this wouldn't work at all as when $x=9$, $2x=18$, which means there are no non-negative values of $y,z$ that can satisfy the equation, hence $x$ actually has its own range; $0 \leq x \leq 8$ More or less here I kind of set aside the combinatorics approach and assumed a very basic approach: listing values. Given that $x$ has the smallest range of each of the variables, I let $x$ be the independent variable and $y,z$ be the dependant variables: \begin{array}{|c|c|c|c|c|c|c|c|c|c|} \hline x & 0 & 1 & 2 & 3 & 4 & 5 & 6 & 7 & 8\\\hline   y & 0\leq y \leq 16 & 0 \leq y \leq 14 & 0 \leq y \leq 12 & 0 \leq y \leq 10 & 0 \leq y \leq 8 & 0 \leq y \leq 6 & 0 \leq y \leq 4 & 0 \leq y \leq 2 & y=0 \\\hline   z & 0\leq z \leq 16 & 0 \leq z \leq 14 & 0 \leq z \leq 12 & 0 \leq z \leq 10 & 0 \leq z \leq 8 & 0 \leq z \leq 6 & 0 \leq z \leq 4 & 0 \leq z \leq 2 & z=0\\\hline   T & 17 & 15 & 13 & 11 & 9 & 7 & 5 & 3 & 1\\\hline \end{array} (Note; a prerequisite in each entry is that $y+z=16-2x$. Also; the $T$ in the bottom row signifies the total number of permutations provided all the conditions, including the prerequisite, are met) From the table, the total number of distinct, non-negative triples of $\lbrace{x,y,z|x,y,z\in\mathbb Z\rbrace}$ that satisfy the equation $2x+y+z=16$ is a mere $81$. From all of what you've just read, I have only two questions: the first of which is obviously is this answer correct, and the second of which is, arguably more obviously, how can what I've written be expressed mathematically provided it is correct? Any responses are very appreciated, thank you.","Consider $$2x+y+z=16$$ how many distinct, non-negative triples of $\lbrace{x,y,z|x,y,z\in \mathbb Z\rbrace}$ are there that satisfy the equation? I assumed that in this question, the role of combinatorics play a vital role, so I thought of it in this order: if the triples did not have to be distinct, there would be $16$ options the first time a number is chosen, $16$ the second time a number is chosen, and $16$ the third time a number is chosen, hence there are $16^3$ permutations where repetition is allowed. Then, when repetition is not allowed, the first time a number is chosen there $16$ options, then there are $15$ options, and then $14$, but from here not another number is chosen, so assuming there are $16!$ permutations is senseless. Therefore the remaining $13!$ permutations need to be discounted, hence there are $\displaystyle \frac{16!}{(16-3)!}$ permutations when repetition is not allowed. So does this imply there are $3360$ possible triples? This is more or less where I encountered the flaw in my logic: When the first number is chosen, there aren't actually $15$ remaining numbers to choose from. Because say for example you choose $y$ to be $16$, then both $x$ and $z$ must be $0$, i.e. there is actually only one choice once the first number has been chosen, and one choice once the second number has been chosen. But, what if we let $x=9$? Well, this wouldn't work at all as when $x=9$, $2x=18$, which means there are no non-negative values of $y,z$ that can satisfy the equation, hence $x$ actually has its own range; $0 \leq x \leq 8$ More or less here I kind of set aside the combinatorics approach and assumed a very basic approach: listing values. Given that $x$ has the smallest range of each of the variables, I let $x$ be the independent variable and $y,z$ be the dependant variables: \begin{array}{|c|c|c|c|c|c|c|c|c|c|} \hline x & 0 & 1 & 2 & 3 & 4 & 5 & 6 & 7 & 8\\\hline   y & 0\leq y \leq 16 & 0 \leq y \leq 14 & 0 \leq y \leq 12 & 0 \leq y \leq 10 & 0 \leq y \leq 8 & 0 \leq y \leq 6 & 0 \leq y \leq 4 & 0 \leq y \leq 2 & y=0 \\\hline   z & 0\leq z \leq 16 & 0 \leq z \leq 14 & 0 \leq z \leq 12 & 0 \leq z \leq 10 & 0 \leq z \leq 8 & 0 \leq z \leq 6 & 0 \leq z \leq 4 & 0 \leq z \leq 2 & z=0\\\hline   T & 17 & 15 & 13 & 11 & 9 & 7 & 5 & 3 & 1\\\hline \end{array} (Note; a prerequisite in each entry is that $y+z=16-2x$. Also; the $T$ in the bottom row signifies the total number of permutations provided all the conditions, including the prerequisite, are met) From the table, the total number of distinct, non-negative triples of $\lbrace{x,y,z|x,y,z\in\mathbb Z\rbrace}$ that satisfy the equation $2x+y+z=16$ is a mere $81$. From all of what you've just read, I have only two questions: the first of which is obviously is this answer correct, and the second of which is, arguably more obviously, how can what I've written be expressed mathematically provided it is correct? Any responses are very appreciated, thank you.",,"['linear-algebra', 'combinatorics', 'permutations']"
99,Parametric representation of orthogonal matrices,Parametric representation of orthogonal matrices,,"It is a known fact that if $X$ is a skew-symmetric matrix, then $e^X$ is an orthogonal matrix. Is it also true the opposite, ie that any orthogonal matrix admits a representation like $e^X$ for some $X$ skew-symmetric? If not, is there a way to parametrize the space of the orthogonal matrices? With this I mean, a function $f$ that takes some parameters $\Theta$ and returns a matrix $O$ such that: $f(\Theta)=O$ is orthogonal If $O$ is orthogonal, there exists a set of parameters $\Theta$ s.t. $f(\Theta)=O$? Does the topic get easier if we focus on orthonormal matrices?","It is a known fact that if $X$ is a skew-symmetric matrix, then $e^X$ is an orthogonal matrix. Is it also true the opposite, ie that any orthogonal matrix admits a representation like $e^X$ for some $X$ skew-symmetric? If not, is there a way to parametrize the space of the orthogonal matrices? With this I mean, a function $f$ that takes some parameters $\Theta$ and returns a matrix $O$ such that: $f(\Theta)=O$ is orthogonal If $O$ is orthogonal, there exists a set of parameters $\Theta$ s.t. $f(\Theta)=O$? Does the topic get easier if we focus on orthonormal matrices?",,"['linear-algebra', 'matrices', 'orthogonal-matrices', 'matrix-exponential']"
