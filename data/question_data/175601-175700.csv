,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,Finding a scalar potential given its set of stationary points,Finding a scalar potential given its set of stationary points,,"Given a set of points in $\mathbb{R}^3$ $$x^\star(t), y^\star(t), z^\star(t)\text{,}\quad 0\le t\le 1$$ I am looking for a scalar potential $V(x, y): \mathbb{R^2} \rightarrow \mathbb{R}$ , whose partial derivatives satisfy the following conditions, \begin{align*} \dfrac{\partial V}{\partial x}(x^\star(t),y^\star(t)) &= 0\\ \dfrac{\partial V}{\partial y}(x^\star(t),y^\star(t))&=z^\star(t)\text{,}\\ \forall 0\le t\le 1\text{.} \end{align*} Any idea on how to find $V$ ? Less confusing formulation Given a curve $x^\star, y(x^\star)$ in $\mathbb{R}^2$ , I am looking for a scalar potential $V(x, y)$ having a specific gradient along that curve, i.e. \begin{align*} \dfrac{\partial V}{\partial x}(x^\star,y(x^\star)) &= 0\\ \dfrac{\partial V}{\partial y}(x^\star,y(x^\star))&=z(x^\star)\text{,}\\ \end{align*} where $z: \mathbb{R} \rightarrow \mathbb{R}$ is a given function. Context This problem arises when one tries to derive the potential elastic energy of an elastic structure with two degrees of freedom $x$ and $y$ given its equilibrium path. The equilibrium path is the set of all the static equilibrium states the structure can be in when a force acts on it. This path can be represented by a curve in a three-dimension space: two dimensions for the degrees of freedom ( $x$ and $y$ ), and a third dimension for the force $z$ acting on one of the degree of freedom ( $y$ in this case). Each triplet of points $(x^\star(t), y^\star(t), z^\star(t))$ corresponds to a static configuration of the structure. When on tries to find the equilibrium path of an elastic structure whose elastic energy is given by $V(x, y)$ , the classic approach consists of forming the total potential energy $\Pi$ defined as $$\Pi = V(x, y) - zy\text{,}$$ where $z$ represents the external force acting on the degree of freedom $y$ . We consider the case where no external force acts on the degree of freedom $x$ . The static equilibrium states ( $x^\star$ , $y^\star$ , $z^\star$ ) are the stationary points of the potential $\Pi$ (w.r.t the variables $x$ and $y$ ), which leads to the following set of equations \begin{equation*} 0=\dfrac{\partial V}{\partial x}(x^\star, y^\star),\quad 0=\dfrac{\partial V}{\partial y}(x^\star, y^\star) - z^\star\text{.} \end{equation*}","Given a set of points in I am looking for a scalar potential , whose partial derivatives satisfy the following conditions, Any idea on how to find ? Less confusing formulation Given a curve in , I am looking for a scalar potential having a specific gradient along that curve, i.e. where is a given function. Context This problem arises when one tries to derive the potential elastic energy of an elastic structure with two degrees of freedom and given its equilibrium path. The equilibrium path is the set of all the static equilibrium states the structure can be in when a force acts on it. This path can be represented by a curve in a three-dimension space: two dimensions for the degrees of freedom ( and ), and a third dimension for the force acting on one of the degree of freedom ( in this case). Each triplet of points corresponds to a static configuration of the structure. When on tries to find the equilibrium path of an elastic structure whose elastic energy is given by , the classic approach consists of forming the total potential energy defined as where represents the external force acting on the degree of freedom . We consider the case where no external force acts on the degree of freedom . The static equilibrium states ( , , ) are the stationary points of the potential (w.r.t the variables and ), which leads to the following set of equations","\mathbb{R}^3 x^\star(t), y^\star(t), z^\star(t)\text{,}\quad 0\le t\le 1 V(x, y): \mathbb{R^2} \rightarrow \mathbb{R} \begin{align*}
\dfrac{\partial V}{\partial x}(x^\star(t),y^\star(t)) &= 0\\
\dfrac{\partial V}{\partial y}(x^\star(t),y^\star(t))&=z^\star(t)\text{,}\\
\forall 0\le t\le 1\text{.}
\end{align*} V x^\star, y(x^\star) \mathbb{R}^2 V(x, y) \begin{align*}
\dfrac{\partial V}{\partial x}(x^\star,y(x^\star)) &= 0\\
\dfrac{\partial V}{\partial y}(x^\star,y(x^\star))&=z(x^\star)\text{,}\\
\end{align*} z: \mathbb{R} \rightarrow \mathbb{R} x y x y z y (x^\star(t), y^\star(t), z^\star(t)) V(x, y) \Pi \Pi = V(x, y) - zy\text{,} z y x x^\star y^\star z^\star \Pi x y \begin{equation*}
0=\dfrac{\partial V}{\partial x}(x^\star, y^\star),\quad
0=\dfrac{\partial V}{\partial y}(x^\star, y^\star) - z^\star\text{.}
\end{equation*}","['calculus', 'multivariable-calculus', 'indefinite-integrals', 'lagrange-multiplier', 'multiple-integral']"
1,Why do cyclic inequality questions usually have three variables?,Why do cyclic inequality questions usually have three variables?,,"This is probably more of a meta- or soft- question , but I thought I'd ask anyway. The question is the title: Why do nearly all posts involving cyclic sums involve exactly three variables? With only one or two variables, especially if the two variables have a constraint, perhaps the problem could be reduced to a single-variable calculus question. I guess I'm wondering if with four or more, aside from the obvious chore of writing out all the terms, if there is some inherent lack of symmetry or 'niceness' present in the three-variable case that often rules them out. As an analogy, which is somewhat of a stretch, the cross-product is really only used in three dimensions. It exists in $\mathbb{R}^7$ , and is rotationally invariant, but doesn't satisfy the Jacobi identity : there's something special about $\mathbb{R}^3$ specifically. I was wondering if there was a similar symmetry or property involving cyclic sums in $n$ variables that is only present when $n=3$ .","This is probably more of a meta- or soft- question , but I thought I'd ask anyway. The question is the title: Why do nearly all posts involving cyclic sums involve exactly three variables? With only one or two variables, especially if the two variables have a constraint, perhaps the problem could be reduced to a single-variable calculus question. I guess I'm wondering if with four or more, aside from the obvious chore of writing out all the terms, if there is some inherent lack of symmetry or 'niceness' present in the three-variable case that often rules them out. As an analogy, which is somewhat of a stretch, the cross-product is really only used in three dimensions. It exists in , and is rotationally invariant, but doesn't satisfy the Jacobi identity : there's something special about specifically. I was wondering if there was a similar symmetry or property involving cyclic sums in variables that is only present when .",\mathbb{R}^7 \mathbb{R}^3 n n=3,"['multivariable-calculus', 'inequality', 'optimization', 'soft-question', 'contest-math']"
2,Finding integration bounds for the volume between ellipsoid and plane,Finding integration bounds for the volume between ellipsoid and plane,,"Let $S$ be the region between $x+z=1$ , and ellipsoid $x^{2}+2y^{2}+z^{2}=1$ ( $S$ is above $x+z=1$ ). Find the volume of $S$ . I'm having trouble to find the bound for $x,y$ , and $z$ . At first I tried to identify the bound of $z$ as follows: $1-x\le z\le \sqrt{1-x^{2}-2y^{2}}\quad$ and for $x$ is $0\le x\le1\quad$ . But I struggled to find the bound for $y$ . My idea  for this is to find the ellipse equation on $x+z=1$ plane to substitute $z=1-x$ to the ellipsoid equation to get $(2x-1)^{2}+(2y)^{2}=1$ . So I get: $-\sqrt{-(x-\frac{1}{2})^{2}+\frac{1}{4}}\le y\le\sqrt{-(x-\frac{1}{2})^{2}+\frac{1}{4}}$ Is this right or am I doing it wrong?","Let be the region between , and ellipsoid ( is above ). Find the volume of . I'm having trouble to find the bound for , and . At first I tried to identify the bound of as follows: and for is . But I struggled to find the bound for . My idea  for this is to find the ellipse equation on plane to substitute to the ellipsoid equation to get . So I get: Is this right or am I doing it wrong?","S x+z=1 x^{2}+2y^{2}+z^{2}=1 S x+z=1 S x,y z z 1-x\le z\le \sqrt{1-x^{2}-2y^{2}}\quad x 0\le x\le1\quad y x+z=1 z=1-x (2x-1)^{2}+(2y)^{2}=1 -\sqrt{-(x-\frac{1}{2})^{2}+\frac{1}{4}}\le y\le\sqrt{-(x-\frac{1}{2})^{2}+\frac{1}{4}}","['calculus', 'integration', 'multivariable-calculus', 'analytic-geometry', 'multiple-integral']"
3,Prove that $f(\mathbf{x}) = \mathbf{c}^T\mathbf{x} - \sum_{i=1}^{m} \log(b_i - \mathbf{a}_i^T\mathbf{x})$ is convex,Prove that  is convex,f(\mathbf{x}) = \mathbf{c}^T\mathbf{x} - \sum_{i=1}^{m} \log(b_i - \mathbf{a}_i^T\mathbf{x}),"Let's consider $f \colon \mathbb{R}^n \to \mathbb{R}$ , $$f(\mathbf{x}) = \mathbf{c}^T\mathbf{x} - \sum_{i=1}^{m} \log(b_i - \mathbf{a}_i^T\mathbf{x}), $$ with $\mathbf{x} \in \mathbb{R}^n$ , $\mathbf{c} \in \mathbb{R}^n$ , $\mathbf{b} \in \mathbb{R}^m$ , $\mathbf{a}_i \in \mathbb{R}^n$ , for $i = 1,\ldots,m$ , so $\mathbf{A} \in \mathbb{R}^{n\times m}$ . $\mathbf{dom}f$ contains the $\mathbf{x}$ for which the arguments of the logarithms are positive. (Note: I have also seen $f$ written as, $f(\mathbf{x}) = \mathbf{c}^T\mathbf{x} - \operatorname{sum}(\log(\mathbf{b} - \mathbf{A}\mathbf{x}) )$ .) I have to prove that $f$ is convex (most likely using the second derivative theorem). I am not sure how to proceed. I have read that: $$ \nabla \biggl( \sum_{i=1}^{m} \log(b_i - \mathbf{a}_i^T\mathbf{x}) \biggr) = \sum_{i=1}^{m} \frac{1}{b_i - \mathbf{a}_i^T\mathbf{x}}\mathbf{a}_i $$ But I don't fully understand it.","Let's consider , with , , , , for , so . contains the for which the arguments of the logarithms are positive. (Note: I have also seen written as, .) I have to prove that is convex (most likely using the second derivative theorem). I am not sure how to proceed. I have read that: But I don't fully understand it.","f \colon \mathbb{R}^n \to \mathbb{R} f(\mathbf{x}) = \mathbf{c}^T\mathbf{x} - \sum_{i=1}^{m} \log(b_i - \mathbf{a}_i^T\mathbf{x}),  \mathbf{x} \in \mathbb{R}^n \mathbf{c} \in \mathbb{R}^n \mathbf{b} \in \mathbb{R}^m \mathbf{a}_i \in \mathbb{R}^n i = 1,\ldots,m \mathbf{A} \in \mathbb{R}^{n\times m} \mathbf{dom}f \mathbf{x} f f(\mathbf{x}) = \mathbf{c}^T\mathbf{x} - \operatorname{sum}(\log(\mathbf{b} - \mathbf{A}\mathbf{x}) ) f  \nabla \biggl( \sum_{i=1}^{m} \log(b_i - \mathbf{a}_i^T\mathbf{x}) \biggr) = \sum_{i=1}^{m} \frac{1}{b_i - \mathbf{a}_i^T\mathbf{x}}\mathbf{a}_i ","['calculus', 'multivariable-calculus', 'derivatives', 'convex-analysis']"
4,Anti-derivative of $ |x^2-1|$,Anti-derivative of, |x^2-1|,"I ran into a problem of finding the anti-derivative of a function which involves the absolute value function. Here is the problem: Question: If $v(t) = \langle|t^2-1|,4t-3\rangle$ is the velocity vector, find $s(t)$ the position vector. Given that $s(0) = \langle 1,1\rangle$ . My answer: $s(t) = \displaystyle \int_{0}^t v(x)dx$ . Do we split this into $2$ cases ? $0 \le t \le 1$ and $t \ge 1$ ? To be quite honest here my elementary differential geometry is quite rusty. I would appreciate to see a full analysis on this. Thanks. WY.","I ran into a problem of finding the anti-derivative of a function which involves the absolute value function. Here is the problem: Question: If is the velocity vector, find the position vector. Given that . My answer: . Do we split this into cases ? and ? To be quite honest here my elementary differential geometry is quite rusty. I would appreciate to see a full analysis on this. Thanks. WY.","v(t) = \langle|t^2-1|,4t-3\rangle s(t) s(0) = \langle 1,1\rangle s(t) = \displaystyle \int_{0}^t v(x)dx 2 0 \le t \le 1 t \ge 1",['multivariable-calculus']
5,"If $f(x,t) = g(u)$, how do I relate $\iint{fdxdt}$ to $\int{gdu}$?","If , how do I relate  to ?","f(x,t) = g(u) \iint{fdxdt} \int{gdu}","I'm learning about Fourier Transforms in the context of travelling waves on a dispersive medium, and my textbook sort of handwaves a simplification in which $$ \textbf{F}(k,\omega) = \int_{-\infty}^{\infty}\int_{-\infty}^{\infty}f(x,t)e^{-i(kx-\omega t)}dxdt $$ becomes $$ \textbf{F}(k) = \int_{-\infty}^{\infty}f(x,0)e^{-ikx}dx $$ Which, in conjunction with a dispersion relation $\omega =\omega (k)$ , allows the Fourier series for the travelling wave to be expressed as a single integral with respect to k. In trying to wrap my head around the subtleties of why this works, I ran into a more general problem that I think is at the crux of my confusion: if $f(x,t)=g(u)$ where $u$ is a function of $x$ and $t$ , is there a way to relate $\iint{fdxdt}$ to $\int{gdu}$ ? Of course $du=\frac{\partial u}{\partial x}dx + \frac{\partial u}{\partial t}dt$ , but I have no clue how to use that to turn a single integral into two. I was also having trouble finding anything about this online since I don't know if there's a name for this simplification. Many thanks in advance for any and all help. Edit: I've managed to grasp a intuitive understanding for the Fourier simplification, but I'm still confused on how to mathematically justify it. Essentially, if you have a formula for the waveform at a specific time, then you can do a spatial FT to get the wavenumbers k that comprise it. Then, if you know the dispersion relation, essentially knowing the wavespeed associated with each wavenumber, then you can imagine pushing all the component waves of the spatial FT at their associated wavespeed, giving $$ f(x,t)=\frac{1}{2\pi} \int_{-\infty}^\infty \textbf{F}(k)e^{i(kx-\omega (k)t)}dk $$ With $\textbf{F}(k)$ being what I had written above. While I have a clear enough picture in my head as to why this works, I'm still having trouble justifying it algebraically. Hopefully the following picture makes my question clear:","I'm learning about Fourier Transforms in the context of travelling waves on a dispersive medium, and my textbook sort of handwaves a simplification in which becomes Which, in conjunction with a dispersion relation , allows the Fourier series for the travelling wave to be expressed as a single integral with respect to k. In trying to wrap my head around the subtleties of why this works, I ran into a more general problem that I think is at the crux of my confusion: if where is a function of and , is there a way to relate to ? Of course , but I have no clue how to use that to turn a single integral into two. I was also having trouble finding anything about this online since I don't know if there's a name for this simplification. Many thanks in advance for any and all help. Edit: I've managed to grasp a intuitive understanding for the Fourier simplification, but I'm still confused on how to mathematically justify it. Essentially, if you have a formula for the waveform at a specific time, then you can do a spatial FT to get the wavenumbers k that comprise it. Then, if you know the dispersion relation, essentially knowing the wavespeed associated with each wavenumber, then you can imagine pushing all the component waves of the spatial FT at their associated wavespeed, giving With being what I had written above. While I have a clear enough picture in my head as to why this works, I'm still having trouble justifying it algebraically. Hopefully the following picture makes my question clear:"," \textbf{F}(k,\omega) = \int_{-\infty}^{\infty}\int_{-\infty}^{\infty}f(x,t)e^{-i(kx-\omega t)}dxdt   \textbf{F}(k) = \int_{-\infty}^{\infty}f(x,0)e^{-ikx}dx  \omega =\omega (k) f(x,t)=g(u) u x t \iint{fdxdt} \int{gdu} du=\frac{\partial u}{\partial x}dx + \frac{\partial u}{\partial t}dt  f(x,t)=\frac{1}{2\pi} \int_{-\infty}^\infty \textbf{F}(k)e^{i(kx-\omega (k)t)}dk  \textbf{F}(k)","['multivariable-calculus', 'fourier-transform']"
6,Geometric interpretation of the Hessian,Geometric interpretation of the Hessian,,"Assume we have a smooth function $f:\mathbb{R}^2 \to \mathbb{R}$ . We may then form the differential of $f$ , denoted by $Df$ , given by the row vector $$ Df=\Big[\frac{\partial f}{\partial x_1},\frac{\partial f}{\partial x_2}\Big] $$ This quantity is a $1$ -form, i.e. for every $p \in \mathbb{R}^2$ , $Df(p):\mathbb{R}^2 \to \mathbb{R}$ is a linear map. The action of this object is easy to visualize: It is simply the linearization at a point $p$ of the given function, ie. the $Df(p)$ takes in a vector $v \in \mathbb{R}^2$ , and spits out the directional derivative of $f$ in the direction of $v$ . We may then also form the Hessian, $$ Hf = \begin{bmatrix}\frac{\partial f}{\partial^2 x_1} & \frac{\partial f}{\partial x_1\partial x_2} \\ \frac{\partial f}{\partial x_1x_2} & \frac{\partial f}{\partial^2x_2}\end{bmatrix} $$ The action of this matrix is more difficult for me to understand. According to my understanding, this object should be interpreted as a $2$ -form, i.e., for each point $p \in \mathbb{R}^2$ it eats two vectors $v_1,v_2 \in \mathbb{R}^2$ and spits out a number. However, I am wondering what the geometric interpretation is of these two vectors - in the case of the differential of $f$ , it was clear that the vector it ate was to be interpreted as the direction of the directional derivative. What is the geometric intuition behind the two vectors that the Hessian takes in as argument?","Assume we have a smooth function . We may then form the differential of , denoted by , given by the row vector This quantity is a -form, i.e. for every , is a linear map. The action of this object is easy to visualize: It is simply the linearization at a point of the given function, ie. the takes in a vector , and spits out the directional derivative of in the direction of . We may then also form the Hessian, The action of this matrix is more difficult for me to understand. According to my understanding, this object should be interpreted as a -form, i.e., for each point it eats two vectors and spits out a number. However, I am wondering what the geometric interpretation is of these two vectors - in the case of the differential of , it was clear that the vector it ate was to be interpreted as the direction of the directional derivative. What is the geometric intuition behind the two vectors that the Hessian takes in as argument?","f:\mathbb{R}^2 \to \mathbb{R} f Df 
Df=\Big[\frac{\partial f}{\partial x_1},\frac{\partial f}{\partial x_2}\Big]
 1 p \in \mathbb{R}^2 Df(p):\mathbb{R}^2 \to \mathbb{R} p Df(p) v \in \mathbb{R}^2 f v 
Hf = \begin{bmatrix}\frac{\partial f}{\partial^2 x_1} & \frac{\partial f}{\partial x_1\partial x_2} \\ \frac{\partial f}{\partial x_1x_2} & \frac{\partial f}{\partial^2x_2}\end{bmatrix}
 2 p \in \mathbb{R}^2 v_1,v_2 \in \mathbb{R}^2 f","['multivariable-calculus', 'tensors', 'quadratic-forms', 'hessian-matrix', 'geometric-interpretation']"
7,Universal substitution or Feynman trick to solve this integral,Universal substitution or Feynman trick to solve this integral,,"I started with an integral $ \int_{0}^{2\pi} \sqrt{2[\sin^2(t) + 16\cos^2(t) - 4\sin(t)\cos(t)]} \,dt $ And I simplified it to $ \int_{0}^{2\pi} \sqrt{17 + 15\cos(2t) - 4\sin(2t)} \, dt$ My question: I know this can be simplified with some sort of substitution that cancels the $\sin$ and $\cos$ with a $u$ -sub, but I do not know how. I saw it online, with no explanation (see the first answer: find length of curve of intersection ). I think this has an exact elementary solution, if you use $\tan\left(\frac x2\right)$ substitution and possibly Feynman's trick if necessary.","I started with an integral And I simplified it to My question: I know this can be simplified with some sort of substitution that cancels the and with a -sub, but I do not know how. I saw it online, with no explanation (see the first answer: find length of curve of intersection ). I think this has an exact elementary solution, if you use substitution and possibly Feynman's trick if necessary."," \int_{0}^{2\pi} \sqrt{2[\sin^2(t) + 16\cos^2(t) - 4\sin(t)\cos(t)]} \,dt   \int_{0}^{2\pi} \sqrt{17 + 15\cos(2t) - 4\sin(2t)} \, dt \sin \cos u \tan\left(\frac x2\right)","['integration', 'multivariable-calculus', 'definite-integrals', 'parametric']"
8,Why do these results not contradict Green's Theorem?,Why do these results not contradict Green's Theorem?,,"I have a vector field $\;\underline{v}(x,y) = \left(\dfrac{-y}{x^{2}+4y^{2}}, \dfrac{x}{x^{2}+4y^{2}}\right)$ $\text{curl }\underline{v} = 0$ and thus the vector field is conservative. I have a surface, $\Omega$ , with $\Omega:=\left\{(x,y)\in \mathbf{R}^2: x^2+4y^2=4\right\}$ . A parameterisation of the border of $\Omega$ , $\partial\Omega$ is $\underline{r}(t) = \big(2\cos(t), \sin(t)\big)$ , for $t \in [0, 2\pi]$ The integral $\oint_{\partial\Omega}\underline{v}\cdot d\underline{r} = \pi$ However, by Green's Theorem, $\oint_{\partial\Omega}\underline{v}\cdot d\underline{r} = \iint_{\Omega}\textbf{curl}\,\underline{v}\text{ }dxdy$ . Since curl $\underline{v} = 0$ , and the curve is closed, $\oint_{\partial\Omega}\underline{v}\cdot d\underline{r} = 0$ Why are these two results not contradictory? Edit: I think that this might be because $\underline{v}(0,0)$ is undefined, however I am not sure what difference that would make, nor how to formalise why this is not a contradiction.","I have a vector field and thus the vector field is conservative. I have a surface, , with . A parameterisation of the border of , is , for The integral However, by Green's Theorem, . Since curl , and the curve is closed, Why are these two results not contradictory? Edit: I think that this might be because is undefined, however I am not sure what difference that would make, nor how to formalise why this is not a contradiction.","\;\underline{v}(x,y) = \left(\dfrac{-y}{x^{2}+4y^{2}}, \dfrac{x}{x^{2}+4y^{2}}\right) \text{curl }\underline{v} = 0 \Omega \Omega:=\left\{(x,y)\in \mathbf{R}^2: x^2+4y^2=4\right\} \Omega \partial\Omega \underline{r}(t) = \big(2\cos(t), \sin(t)\big) t \in [0, 2\pi] \oint_{\partial\Omega}\underline{v}\cdot d\underline{r} = \pi \oint_{\partial\Omega}\underline{v}\cdot d\underline{r} = \iint_{\Omega}\textbf{curl}\,\underline{v}\text{ }dxdy \underline{v} = 0 \oint_{\partial\Omega}\underline{v}\cdot d\underline{r} = 0 \underline{v}(0,0)","['calculus', 'multivariable-calculus', 'vector-fields', 'line-integrals', 'curl']"
9,Invertability of derivative of a bijection,Invertability of derivative of a bijection,,"I am working on the following problem: Let $f(x)$ a differentiable bijection $\mathbb{R}^n \rightarrow \mathbb{R}^n$ , and its inverse $f^{-1}$ also differentiable. Then $f'$ is invertible for $ \forall x \in \mathbb{R}^n$ . I want to get the solution myself (so I am not asking for it), but I am confused on this example: If $f(x)=x$ , then it is differentiable, and it is its own inverse, so again, differentiable, and it is a bijection from $\mathbb{R} \rightarrow \mathbb{R}$ , but the derivative is $f'(x)=1$ is not invertible because it is not injective. Maybe this problem only makes sense for $n\geq2$ , but I don't see why because multivariable theory is built on top of single-variable. Am I missing something obvious?","I am working on the following problem: Let a differentiable bijection , and its inverse also differentiable. Then is invertible for . I want to get the solution myself (so I am not asking for it), but I am confused on this example: If , then it is differentiable, and it is its own inverse, so again, differentiable, and it is a bijection from , but the derivative is is not invertible because it is not injective. Maybe this problem only makes sense for , but I don't see why because multivariable theory is built on top of single-variable. Am I missing something obvious?",f(x) \mathbb{R}^n \rightarrow \mathbb{R}^n f^{-1} f'  \forall x \in \mathbb{R}^n f(x)=x \mathbb{R} \rightarrow \mathbb{R} f'(x)=1 n\geq2,"['multivariable-calculus', 'functions', 'derivatives', 'inverse']"
10,Double integral whose answer doesn't make any sense,Double integral whose answer doesn't make any sense,,"SOLVED: READ BOTTOM How do I calculate the following double integral? I attempted multiple times and got the same answer (both by integrating with respect to y first and by integrating with respect to x, using IBP). I keep getting $\ln({\frac{3}{\sqrt{5}}}$ ); the answer is given as $\ln({\frac{7\sqrt{6}}{12})}$ . What am I doing wrong? The original question is $$ \iint_R \frac{x}{(2+xy)^2}dA, R={(x,y): 0 \le x \le 5, 1 \le y \le 2}$$ My steps: $$ \int_{x = 0}^5 \int_{y = 1}^2 \frac{x}{(2+xy)^2} dydx $$ Solving first integral: $$ x \int_{y = 1}^2 (2+xy)^{-2} dy$$ $$ u = (2+xy) $$ $$ \frac{du}{dy} = x $$ $$ dy = \frac{du}{x} $$ $$ \int_{y = 1}^2 u^{-2} du$$ $$ = -[u^{-1}] \Big|_{y=1}^2 $$ $$ = -[(2+xy)^{-1}] \Big|_{y=1}^2 $$ $$ = - (2+2x)^{-1} + (2+x)^{-1} $$ Now evaluate the x-integral: $$ \int_{x=0}^5 [(2+x)^{-1} - (2+2x)^{-1}] dx $$ $$ = \int_{x=0}^5 (2+x)^{-1} dx - \int_{x=0}^5 (2+2x)^{-1} dx $$ $$ = \ln{|2+x|} \Big|_{x=0}^4 - \frac{\ln{|2+2x|}}{2} \Big|_{x=0}^4 $$ $$ = [\ln{6} - \ln{2}] - [\frac{\ln{10}}{2} - \frac{\ln{2}}{2}] $$ $$ = \ln{3} - \frac{\ln{\sqrt{10}}}{\ln{\sqrt{2}}} $$ $$ = \ln{\frac{3}{\frac{\sqrt{10}}{\sqrt{2}}}} $$ $$ = \ln{\frac{3{\sqrt{2}}}{\sqrt{10}}} $$ $$ = \ln{\frac{3}{\sqrt{5}}} $$ And this is how I arrived at $ \ln({\frac{3}{\sqrt{5}})} $ ! Let me know if I made any mistakes. Thank you! :) Just realized I somehow swapped the bounds from $\int_{x=0}^5$ to $\int_{x=0}^4$ halfway through solving. Oops","SOLVED: READ BOTTOM How do I calculate the following double integral? I attempted multiple times and got the same answer (both by integrating with respect to y first and by integrating with respect to x, using IBP). I keep getting ); the answer is given as . What am I doing wrong? The original question is My steps: Solving first integral: Now evaluate the x-integral: And this is how I arrived at ! Let me know if I made any mistakes. Thank you! :) Just realized I somehow swapped the bounds from to halfway through solving. Oops","\ln({\frac{3}{\sqrt{5}}} \ln({\frac{7\sqrt{6}}{12})}  \iint_R \frac{x}{(2+xy)^2}dA, R={(x,y): 0 \le x \le 5, 1 \le y \le 2}  \int_{x = 0}^5 \int_{y = 1}^2 \frac{x}{(2+xy)^2} dydx   x \int_{y = 1}^2 (2+xy)^{-2} dy  u = (2+xy)   \frac{du}{dy} = x   dy = \frac{du}{x}   \int_{y = 1}^2 u^{-2} du  = -[u^{-1}] \Big|_{y=1}^2   = -[(2+xy)^{-1}] \Big|_{y=1}^2   = - (2+2x)^{-1} + (2+x)^{-1}   \int_{x=0}^5 [(2+x)^{-1} - (2+2x)^{-1}] dx   = \int_{x=0}^5 (2+x)^{-1} dx - \int_{x=0}^5 (2+2x)^{-1} dx   = \ln{|2+x|} \Big|_{x=0}^4 - \frac{\ln{|2+2x|}}{2} \Big|_{x=0}^4   = [\ln{6} - \ln{2}] - [\frac{\ln{10}}{2} - \frac{\ln{2}}{2}]   = \ln{3} - \frac{\ln{\sqrt{10}}}{\ln{\sqrt{2}}}   = \ln{\frac{3}{\frac{\sqrt{10}}{\sqrt{2}}}}   = \ln{\frac{3{\sqrt{2}}}{\sqrt{10}}}   = \ln{\frac{3}{\sqrt{5}}}   \ln({\frac{3}{\sqrt{5}})}  \int_{x=0}^5 \int_{x=0}^4","['integration', 'multivariable-calculus', 'definite-integrals']"
11,Is this notation appropriate for multiple integrals of this form?,Is this notation appropriate for multiple integrals of this form?,,"I am trying to write the following integral in an easier notation $$ \int_0^\infty\ldots \int_0^\infty f(x_1,\dots,x_k)\,dx_1\ldots dx_k $$ For example $$ \int_0^\infty\ldots \int_0^\infty \prod_{i=3}^k e^{-x_i} \,dx_3\ldots dx_k $$ Is the notation $$ \int_{\mathbb{R}_{+}^k} f(x_1,\dots,x_k)\,dx_1\ldots dx_k $$ correct? Also, is it appropriate to write $dx_1\ldots dx_k$ in a simpler form?","I am trying to write the following integral in an easier notation For example Is the notation correct? Also, is it appropriate to write in a simpler form?","
\int_0^\infty\ldots \int_0^\infty f(x_1,\dots,x_k)\,dx_1\ldots dx_k
 
\int_0^\infty\ldots \int_0^\infty \prod_{i=3}^k e^{-x_i} \,dx_3\ldots dx_k
 
\int_{\mathbb{R}_{+}^k} f(x_1,\dots,x_k)\,dx_1\ldots dx_k
 dx_1\ldots dx_k","['integration', 'multivariable-calculus', 'notation']"
12,"Evaluating $\oint_c x\,dx-y\,dy$ (and others) over unit circle $c$. I'm not sure I understand how those separate derivatives are supposed to work.",Evaluating  (and others) over unit circle . I'm not sure I understand how those separate derivatives are supposed to work.,"\oint_c x\,dx-y\,dy c","The question says: Solve the following line integrals, where $c$ is the circle with radius $1$ centered at the origin, that can be parametrized by $ c : [0,2\pi]\rightarrow \mathbb{R}^2 : t \mapsto c(t) = (\cos(t),\sin(t))$ . (i) $\;\oint_c x\,dx-y\,dy$ (ii) $\;\oint_c x\,dx+x\,dy$ (iii) $\;\oint_c y\,dx$ (iv) $\;\oint_c dy$ I think I'm supposed to use Green's theorem and solve these integrals as a double integral on the region of a circle, and then use polar coordinate substitution? But I'm not sure I understand how those separate derivatives are supposed to work like in "" $x\,\mathrm{d}x-y\,\mathrm{d}y$ "" for example.","The question says: Solve the following line integrals, where is the circle with radius centered at the origin, that can be parametrized by . (i) (ii) (iii) (iv) I think I'm supposed to use Green's theorem and solve these integrals as a double integral on the region of a circle, and then use polar coordinate substitution? But I'm not sure I understand how those separate derivatives are supposed to work like in "" "" for example.","c 1  c : [0,2\pi]\rightarrow \mathbb{R}^2 : t \mapsto c(t) = (\cos(t),\sin(t)) \;\oint_c x\,dx-y\,dy \;\oint_c x\,dx+x\,dy \;\oint_c y\,dx \;\oint_c dy x\,\mathrm{d}x-y\,\mathrm{d}y","['calculus', 'multivariable-calculus', 'line-integrals']"
13,Extrema of a surface $z=f(x;y)$ when $det(H)=0$,Extrema of a surface  when,z=f(x;y) det(H)=0,"I'm given the following problem: $\text{Examine}\ z=f(x;y)=x^4+y^4+18xy-9x^2-9y^2+1\text{ for extrema and saddle points.}$ It is trivial to find $\nabla f=(4x^3+18y-18x; 4y^3+18x-18y)$ and the critical points are $P_0(0,0), P_1(3,-3)\text{ and }P_2(-3,3)$ . Also the determinant of the Hessian matrix is easily found (details not included): $$\text{det}(H)=72(2x^2y^2-3x^2-3y^2)$$ Evaluating $\text{det}(H)$ for $P_1$ and $P_2$ gives positive values so there are extrema in these points. Furthermore $f''_{xx}>0$ so these are both minima. For $P_0$ the determinant is $0$ . In our class it was said that in these cases ""further investigation is needed"", but noone provided me with any information about this ""further investigation"". In internet I also found nothing. I plotted the graph of $z=x^4+y^4+18xy-9x^2-9y^2+1$ in GeoGebra and it turned out that in $P_0$ there is a saddle point. But I want to find out how can one analytically determine what to further do when $\text{det}(H)=0$ When in a single-variable calculus $f'(x)=f''(x)=0$ , I keep differentiating $f$ until I get a non-zero derivative. And if the first non-zero derivative is of odd order (i.e. $f^{(3)}, f^{(5)}$ and so on), I know the function has an inflection point; when it is of even order, there is an extremum, whose kind depends on the sign of this non-zero derivative. And so I'm stuck with this problem. I'm not explicitly asked to find what is there in $P_0(0,0)$ , but I want to learn how to tackle such problems. Any help will be appreciated. Thanks in advance! $\textbf{Edit:}$ The answer by @Robert Z is very helpful, but I find it a bit of a guess-and-check method. And if at $P_0(0,0)$ there was an extremum, it wouldn't work out because we can't check all lines, passing through $P_0$ . Any suggestions for the case where there would be an extremum?","I'm given the following problem: It is trivial to find and the critical points are . Also the determinant of the Hessian matrix is easily found (details not included): Evaluating for and gives positive values so there are extrema in these points. Furthermore so these are both minima. For the determinant is . In our class it was said that in these cases ""further investigation is needed"", but noone provided me with any information about this ""further investigation"". In internet I also found nothing. I plotted the graph of in GeoGebra and it turned out that in there is a saddle point. But I want to find out how can one analytically determine what to further do when When in a single-variable calculus , I keep differentiating until I get a non-zero derivative. And if the first non-zero derivative is of odd order (i.e. and so on), I know the function has an inflection point; when it is of even order, there is an extremum, whose kind depends on the sign of this non-zero derivative. And so I'm stuck with this problem. I'm not explicitly asked to find what is there in , but I want to learn how to tackle such problems. Any help will be appreciated. Thanks in advance! The answer by @Robert Z is very helpful, but I find it a bit of a guess-and-check method. And if at there was an extremum, it wouldn't work out because we can't check all lines, passing through . Any suggestions for the case where there would be an extremum?","\text{Examine}\ z=f(x;y)=x^4+y^4+18xy-9x^2-9y^2+1\text{ for extrema and saddle points.} \nabla f=(4x^3+18y-18x; 4y^3+18x-18y) P_0(0,0), P_1(3,-3)\text{ and }P_2(-3,3) \text{det}(H)=72(2x^2y^2-3x^2-3y^2) \text{det}(H) P_1 P_2 f''_{xx}>0 P_0 0 z=x^4+y^4+18xy-9x^2-9y^2+1 P_0 \text{det}(H)=0 f'(x)=f''(x)=0 f f^{(3)}, f^{(5)} P_0(0,0) \textbf{Edit:} P_0(0,0) P_0","['calculus', 'multivariable-calculus', 'optimization', 'maxima-minima', 'hessian-matrix']"
14,why does contour lines intersect at saddle points?,why does contour lines intersect at saddle points?,,"Consider functions of two variables. A saddle point of $f(x,y)$ is a point in the domain of $f$ (or on the graph of $f$ by an abuse of language) where the gradient is $\bf 0$ , but which is not a local extremum of the function. A classic example is $(0,0)$ for $f(x,y)=y^2-x^2$ . However, there are two types of saddle points. Type I: the graph looks like a saddle. Type II: the graph does not look like a saddle. e.g. $f(x,y)=x^3$ , and $f(x,y)=x^2+y^3$ . I have two questions: Question 1: How to rigorously define saddle points of the above two types? Question 2: A folklore theorem says $P$ is a saddle point of type I if and only if the contour lines (aka level curves) intersect at $P$ . I don't find a proof of such a result, and I'm not sure if the theorem holds as ""if and only if"" or just holds in one direction.","Consider functions of two variables. A saddle point of is a point in the domain of (or on the graph of by an abuse of language) where the gradient is , but which is not a local extremum of the function. A classic example is for . However, there are two types of saddle points. Type I: the graph looks like a saddle. Type II: the graph does not look like a saddle. e.g. , and . I have two questions: Question 1: How to rigorously define saddle points of the above two types? Question 2: A folklore theorem says is a saddle point of type I if and only if the contour lines (aka level curves) intersect at . I don't find a proof of such a result, and I'm not sure if the theorem holds as ""if and only if"" or just holds in one direction.","f(x,y) f f \bf 0 (0,0) f(x,y)=y^2-x^2 f(x,y)=x^3 f(x,y)=x^2+y^3 P P","['calculus', 'multivariable-calculus', 'differential-geometry', 'vector-analysis']"
15,The commutator map,The commutator map,,"I am trying to study the commutator map of a given Lie group $G$ : $$\mu : G\times G\to G,\ \mu(x,y)=[x,y]=xyx^{-1}y^{-1}$$ I am interested in: Its singular points (where the  the differential is not onto). Its fibers, are connected or not? After some long computations, I get: $$D_{(x,y)}\mu(u,v)=D_xR_{yx^{-1}y^{-1}}(u)-D_x\left(L_{xyx^{-1}}\circ R_{x^{-1}y^{-1}}\right)(u)+D_y\left(L_x\circ R_{x^{-1}y^{-1}}\right)(v)-D_y\left(L_{xyx^{-1}y^{-1}}\circ R_{y^{-1}}\right)(v)$$ Where $L_x$ and $R_x$ are the left and right translations of $G$ . Apart from the obvious singular point $(e,e)$ , I don't see how to find the singular points in the cases: $G=SL(2), SU(2), GL(2)$ ? Is there a nice reference where this map is studied in detail? Same question for the map: $$c(x_1,...x_{2g})=[x_1,x_2]...[x_{2g-1},x_{2g}]$$","I am trying to study the commutator map of a given Lie group : I am interested in: Its singular points (where the  the differential is not onto). Its fibers, are connected or not? After some long computations, I get: Where and are the left and right translations of . Apart from the obvious singular point , I don't see how to find the singular points in the cases: ? Is there a nice reference where this map is studied in detail? Same question for the map:","G \mu : G\times G\to G,\ \mu(x,y)=[x,y]=xyx^{-1}y^{-1} D_{(x,y)}\mu(u,v)=D_xR_{yx^{-1}y^{-1}}(u)-D_x\left(L_{xyx^{-1}}\circ R_{x^{-1}y^{-1}}\right)(u)+D_y\left(L_x\circ R_{x^{-1}y^{-1}}\right)(v)-D_y\left(L_{xyx^{-1}y^{-1}}\circ R_{y^{-1}}\right)(v) L_x R_x G (e,e) G=SL(2), SU(2), GL(2) c(x_1,...x_{2g})=[x_1,x_2]...[x_{2g-1},x_{2g}]","['multivariable-calculus', 'lie-groups', 'singularity-theory']"
16,Adjusting bounds of integration after a substitution with mutually dependent variables for a double integral,Adjusting bounds of integration after a substitution with mutually dependent variables for a double integral,,"I really have trouble with figuring out the correct bounds in such cases. Consider the substitution $$ x = function_{1}(u, v),\\y = function_{2}(u, v) $$ for the integral $$ \int_{p}^{q} \int_{j(y)}^{e(y)}f(x, y)dxdy. $$ How am I supposed to proceed? I understand, having searched the internet, that ""each case is different,"" but is there anything that could be thought of as a general rule? Examples I found that were supposed to explain Jacobians quite often (if not always) skip the adjustment of bounds. Of course, I could just plug in the newly introduced functions right away but then the outer integral's bounds depend on the inner integral's iterating variable, which is not supposed to happen. Since ""each case is different,"" I came up with a substitution the explanation of which I would appreciate. Please, do show me every detail of changing the integrals' intervals, the more detail the better. And any visuals will be much appreciated as well! The example: $$ \int_{a}^{b} \int_{1+b-y}^{1+b+y} \frac{1}{1-x^{2}y}dxdy,\\x = \sqrt2\frac{\tan v}{u},\\y = \frac{u^{2}\cos^{2}v}{2}. $$","I really have trouble with figuring out the correct bounds in such cases. Consider the substitution for the integral How am I supposed to proceed? I understand, having searched the internet, that ""each case is different,"" but is there anything that could be thought of as a general rule? Examples I found that were supposed to explain Jacobians quite often (if not always) skip the adjustment of bounds. Of course, I could just plug in the newly introduced functions right away but then the outer integral's bounds depend on the inner integral's iterating variable, which is not supposed to happen. Since ""each case is different,"" I came up with a substitution the explanation of which I would appreciate. Please, do show me every detail of changing the integrals' intervals, the more detail the better. And any visuals will be much appreciated as well! The example:"," x = function_{1}(u, v),\\y = function_{2}(u, v)   \int_{p}^{q} \int_{j(y)}^{e(y)}f(x, y)dxdy.   \int_{a}^{b} \int_{1+b-y}^{1+b+y} \frac{1}{1-x^{2}y}dxdy,\\x = \sqrt2\frac{\tan v}{u},\\y = \frac{u^{2}\cos^{2}v}{2}. ","['integration', 'multivariable-calculus', 'substitution', 'jacobian', 'change-of-variable']"
17,Question on baby Rudin examples 10.12,Question on baby Rudin examples 10.12,,"I have two questions: $(a)$ . Fix $a > 0$ and $b > 0$ and define $\gamma(t) = (a\cos(t),b\sin(t))$ where $(0\leq t \leq 2\pi)$ . So that $\gamma$ is a closed curve in $R^2$ . (Its range is an ellipse.) Then $$\int_\gamma x dy = \int_0^{2\pi} ab \cos^2(t)dt = \pi ab \tag{1}$$ $(b)$ . Let $D$ be the $3$ -call defined by $0 \leq r \leq 1$ , $0 \leq \theta \leq \pi$ and $0 \leq \varphi \leq 2\pi$ . Define $\phi ( r,\theta,\varphi ) = (x,y,z)$ , where $x = r\sin\theta \cos\varphi$ , $y = r\sin\theta \sin\varphi$ , $z = r\cos\theta$ Then $$ J_\phi (r,\theta,\varphi) = \frac{\partial(x,y,z)}{\partial(r,\theta,\varphi)} =  r^2\sin\theta. $$ Hence $$ \int_{\phi} dx \land dy \land dz = \int_D J_\phi\ \color{red}{dr\,d\theta\, d\varphi} = \frac{4\pi}{3}. \tag{2} $$ I don't understand how do we get $(1)$ and $(2)$ . (in the $(2)$ I also don't understand why is $\int_D J_{\phi}$ equal of $\frac{4\pi}{3}$ ). Any help would be appreciated.","I have two questions: . Fix and and define where . So that is a closed curve in . (Its range is an ellipse.) Then . Let be the -call defined by , and . Define , where , , Then Hence I don't understand how do we get and . (in the I also don't understand why is equal of ). Any help would be appreciated.","(a) a > 0 b > 0 \gamma(t) = (a\cos(t),b\sin(t)) (0\leq t \leq 2\pi) \gamma R^2 \int_\gamma x dy = \int_0^{2\pi} ab \cos^2(t)dt = \pi ab \tag{1} (b) D 3 0 \leq r \leq 1 0 \leq \theta \leq \pi 0 \leq \varphi \leq 2\pi \phi ( r,\theta,\varphi ) = (x,y,z) x = r\sin\theta \cos\varphi y = r\sin\theta \sin\varphi z = r\cos\theta 
J_\phi (r,\theta,\varphi) = \frac{\partial(x,y,z)}{\partial(r,\theta,\varphi)} =  r^2\sin\theta.
 
\int_{\phi} dx \land dy \land dz = \int_D J_\phi\ \color{red}{dr\,d\theta\, d\varphi} = \frac{4\pi}{3}. \tag{2}
 (1) (2) (2) \int_D J_{\phi} \frac{4\pi}{3}","['real-analysis', 'integration', 'multivariable-calculus', 'differential-forms']"
18,Doubt in Integration a function on a $k$ - cell.,Doubt in Integration a function on a  - cell.,k,"Suppose $I^k$ is a $k$ -cell in $R^k$ , consisting of all $X = (x_1, ... , x_k)$ such that $a_i \le  x_i \le b_i (i = 1, \cdots , k)$ , $I^j$ is the $j$ -cell in $R^j$ defined by the first $j$ inequalities, and $f$ is a real continuous function on $I^k$ . ( What does real continuous function in this case mean?- A function $f:I^k \to \mathbb{R}$ ? ) Put $f = f_k$ , and define $f_{k-1}$ on $I^{k-1}$ by $$f_{k-1}(x_1, \cdots , x_{k-1}) = \int_{a_k}^{b_k}h(x_1,\cdots, x_{k-1}, x_k) d_{x_k}\cdots (**)$$ . let us introduce the temporary notation $L(f)$ for the integral (**) and $L'(f)$ for the result obtained by carrying out the k integrations in some other order. For every $f \in \mathbb{C}(I^k), L(f) = L'(f)$ Proof : If $h(x) = h_1(x_1)\cdots h_k(x_k)$ ,( I really don't understand how and why are we considering such a function h(x) ) where $h_j \in \mathbb{C}([a_i, b_i])$ , $L(h) = \prod_{i = 1}^k \int_{a_i}^{b_i} h_i(x_i) d_{x_i} = L'(h)$ . ( How are we getting this? ) Put $V = \prod_{i=1}^k(b_i-a_i)$ If $f \in \mathbb{C}(I^k)$ and $\epsilon > 0$ , there exists $g \in \mathbb{A} $ such that $||f - g|| < \epsilon/ V$ , where $||f||$ is defined as max $|f(x)|$ ( $x \in I^k$ ). Then $|L(f-g)| < \epsilon$ , $|L'(f-g)|< \epsilon$ ,(** I don't understand this calculation also**) and since $L(f) - L'(f) = L(f - g) + L'(g - f)$ , we conclude that $L(f) - L'(f)  < 2\epsilon$ .","Suppose is a -cell in , consisting of all such that , is the -cell in defined by the first inequalities, and is a real continuous function on . ( What does real continuous function in this case mean?- A function ? ) Put , and define on by . let us introduce the temporary notation for the integral (**) and for the result obtained by carrying out the k integrations in some other order. For every Proof : If ,( I really don't understand how and why are we considering such a function h(x) ) where , . ( How are we getting this? ) Put If and , there exists such that , where is defined as max ( ). Then , ,(** I don't understand this calculation also**) and since , we conclude that .","I^k k R^k X = (x_1, ... , x_k) a_i \le  x_i \le b_i (i = 1, \cdots , k) I^j j R^j j f I^k f:I^k \to \mathbb{R} f = f_k f_{k-1} I^{k-1} f_{k-1}(x_1, \cdots , x_{k-1}) = \int_{a_k}^{b_k}h(x_1,\cdots, x_{k-1}, x_k) d_{x_k}\cdots (**) L(f) L'(f) f \in \mathbb{C}(I^k), L(f) = L'(f) h(x) = h_1(x_1)\cdots h_k(x_k) h_j \in \mathbb{C}([a_i, b_i]) L(h) = \prod_{i = 1}^k \int_{a_i}^{b_i} h_i(x_i) d_{x_i} = L'(h) V = \prod_{i=1}^k(b_i-a_i) f \in \mathbb{C}(I^k) \epsilon > 0 g \in \mathbb{A}  ||f - g|| < \epsilon/ V ||f|| |f(x)| x \in I^k |L(f-g)| < \epsilon |L'(f-g)|< \epsilon L(f) - L'(f) = L(f - g) + L'(g - f) L(f) - L'(f)  < 2\epsilon","['integration', 'multivariable-calculus']"
19,Coordinate-free proof that dual Minkowski norm is indeed a Minkowski norm on the dual space,Coordinate-free proof that dual Minkowski norm is indeed a Minkowski norm on the dual space,,"Context: let's say that a Minkowski norm on a vector space $V$ is a map $F\colon V\to \Bbb R_{\geq 0}$ such that $F$ is smooth on $V\setminus \{0\}$ , $F$ is positive-homogeneous of degree $1$ , and for every $x\in V\setminus \{0\}$ , the symmetric bilinear form $g_x$ on $V$ defined by $$g_x(v,w) = \frac{1}{2} \frac{\partial^2}{\partial t\partial s}\bigg|_{t=s=0}F^2(x+tv+sw)= \frac{1}{2} D^2(F^2)(x)(v,w)$$ is positive-definite. Now define $F^*\colon V^* \to \Bbb R_{\geq 0}$ by $F^*(\xi) =\max \{\xi(x) \mid F(x)=1\}$ . I want to understand the proof that $F^*$ is a Minkowski norm, in the above sense, on $V^*$ . Everything is clear except the positivity condition. I am following the notes by Matias Dahl (clearly based on the book by Zhongmin Shen). He does a heavy coordinate computation on page 7 that, as I see it, hides the actual idea behind the proof. This is Lemma 3.1.2 on Shen's book. So, I want a proof without coordinates, but I'm struggling to translate what he did there. We are free to use the Legendre transform $\ell\colon V\to V^*$ given by $\ell(x) = g_x(x,\cdot)$ and $\ell(0) = 0$ , and its properties --- in particular that $F = F^*\circ \ell$ . From $F^2 = (F^*)^2 \circ \ell$ , we have that $$D(F^2)(x)(v) = D((F^*)^2)(\ell(x)) \circ D\ell(x)v,\tag{$1$}$$ and thus $$D^2(F^2)(x)(v,w) = D^2((F^*)^2)(\ell(x))(D\ell(x)v,D\ell(x)w) + D((F^*)^2)(\ell(x))((D^2\ell)(x)(v,w)),\tag{$2$}$$ so the proof is concluded once we establish that $$D((F^*)^2)(\ell(x))((D^2\ell)(x)(v,w)) = 0,\tag{$3$}$$ as it will follow that $g = \ell^*(g^*)$ , where $g$ is the assignment $x\mapsto g_x$ on $V\setminus \{0\}$ and $g^*$ is the assignment $\xi \mapsto g^*_\xi$ on $V^*\setminus \{0\}$ induced by $F^*$ . So each $g_x$ being positive-definite implies each $g^*_{\ell(x)}$ positive-definite as well, and we're done as $\ell$ is surjective. I cannot verify that $(3)$ is true. From Dahl's coordinate computation, the homogeneity relation $g_{\lambda x} = g_x$ (for $\lambda > 0$ ) must enter. This implies that $\ell(\lambda x) = \lambda\ell(x)$ (for $\lambda>0$ ), and (1) reads $g_x(x,v) = g^*_{\ell(x)}(\ell(x), D\ell(x)v)$ , while $(D\ell(x)v)w = g_x(v,w)$ , but we only have that $(D^2\ell)(x)(x,\cdot) = 0$ and I don't see how this is enough.","Context: let's say that a Minkowski norm on a vector space is a map such that is smooth on , is positive-homogeneous of degree , and for every , the symmetric bilinear form on defined by is positive-definite. Now define by . I want to understand the proof that is a Minkowski norm, in the above sense, on . Everything is clear except the positivity condition. I am following the notes by Matias Dahl (clearly based on the book by Zhongmin Shen). He does a heavy coordinate computation on page 7 that, as I see it, hides the actual idea behind the proof. This is Lemma 3.1.2 on Shen's book. So, I want a proof without coordinates, but I'm struggling to translate what he did there. We are free to use the Legendre transform given by and , and its properties --- in particular that . From , we have that and thus so the proof is concluded once we establish that as it will follow that , where is the assignment on and is the assignment on induced by . So each being positive-definite implies each positive-definite as well, and we're done as is surjective. I cannot verify that is true. From Dahl's coordinate computation, the homogeneity relation (for ) must enter. This implies that (for ), and (1) reads , while , but we only have that and I don't see how this is enough.","V F\colon V\to \Bbb R_{\geq 0} F V\setminus \{0\} F 1 x\in V\setminus \{0\} g_x V g_x(v,w) = \frac{1}{2} \frac{\partial^2}{\partial t\partial s}\bigg|_{t=s=0}F^2(x+tv+sw)= \frac{1}{2} D^2(F^2)(x)(v,w) F^*\colon V^* \to \Bbb R_{\geq 0} F^*(\xi) =\max \{\xi(x) \mid F(x)=1\} F^* V^* \ell\colon V\to V^* \ell(x) = g_x(x,\cdot) \ell(0) = 0 F = F^*\circ \ell F^2 = (F^*)^2 \circ \ell D(F^2)(x)(v) = D((F^*)^2)(\ell(x)) \circ D\ell(x)v,\tag{1} D^2(F^2)(x)(v,w) = D^2((F^*)^2)(\ell(x))(D\ell(x)v,D\ell(x)w) + D((F^*)^2)(\ell(x))((D^2\ell)(x)(v,w)),\tag{2} D((F^*)^2)(\ell(x))((D^2\ell)(x)(v,w)) = 0,\tag{3} g = \ell^*(g^*) g x\mapsto g_x V\setminus \{0\} g^* \xi \mapsto g^*_\xi V^*\setminus \{0\} F^* g_x g^*_{\ell(x)} \ell (3) g_{\lambda x} = g_x \lambda > 0 \ell(\lambda x) = \lambda\ell(x) \lambda>0 g_x(x,v) = g^*_{\ell(x)}(\ell(x), D\ell(x)v) (D\ell(x)v)w = g_x(v,w) (D^2\ell)(x)(x,\cdot) = 0","['multivariable-calculus', 'differential-geometry', 'normed-spaces', 'duality-theorems', 'finsler-geometry']"
20,Multivariable Chain Rule for Implicit Multivariable Functions?,Multivariable Chain Rule for Implicit Multivariable Functions?,,"I'd like to compute $\frac{\partial x}{\partial z}$ along $S$ at $(x,y,z)$ for $S: \frac{1}{x}+\arctan(y+2z)=1$ . My Approach: I can define $w(x,y,z)=\frac{1}{x}+\arctan(y+2z)$ and find the total differential and so on, i.e., $dw=w_x dx+w_y dy+w_z dz$ (we'd also need to use the fact that $y$ is held constant and $dw=0$ ). How can I use the multivariable chain rule here? I'd like to find $\frac{\partial x}{\partial z}$ using the chain rule, but I'm a little bummed out here because I am only used to using the chain rule for solving equations where, say, $y$ depends on $a,b$ and $a, b$ depend on $t$ (e.g., $\frac{dy}{dt}=\frac{\partial y}{\partial a}\frac{da}{dt}+\frac{\partial y}{\partial b}\frac{db}{dt}$ ).","I'd like to compute along at for . My Approach: I can define and find the total differential and so on, i.e., (we'd also need to use the fact that is held constant and ). How can I use the multivariable chain rule here? I'd like to find using the chain rule, but I'm a little bummed out here because I am only used to using the chain rule for solving equations where, say, depends on and depend on (e.g., ).","\frac{\partial x}{\partial z} S (x,y,z) S: \frac{1}{x}+\arctan(y+2z)=1 w(x,y,z)=\frac{1}{x}+\arctan(y+2z) dw=w_x dx+w_y dy+w_z dz y dw=0 \frac{\partial x}{\partial z} y a,b a, b t \frac{dy}{dt}=\frac{\partial y}{\partial a}\frac{da}{dt}+\frac{\partial y}{\partial b}\frac{db}{dt}","['linear-algebra', 'multivariable-calculus', 'differential', 'chain-rule']"
21,Can a critical point found by Lagrange multipliers method be a saddle point?,Can a critical point found by Lagrange multipliers method be a saddle point?,,I am trying to find the extrema of a function on a particular set. I used the Lagrange multiplier method and found 2 points. Now I want to classify those points. Should I just put them in the function and determine which one is maximum and which one is minimum according to the function value at those 2 points? Or is there a possibility that one of them is actually a saddle point?,I am trying to find the extrema of a function on a particular set. I used the Lagrange multiplier method and found 2 points. Now I want to classify those points. Should I just put them in the function and determine which one is maximum and which one is minimum according to the function value at those 2 points? Or is there a possibility that one of them is actually a saddle point?,,"['multivariable-calculus', 'lagrange-multiplier']"
22,"Show that $\frac{d}{dt}\int_Sf(x,t)=\int_S\frac{\partial f}{\partial t}(x,t)$ for each $t\in I$ when $S$ is a fixed integrable set.",Show that  for each  when  is a fixed integrable set.,"\frac{d}{dt}\int_Sf(x,t)=\int_S\frac{\partial f}{\partial t}(x,t) t\in I S","Let be $S\subseteq\Bbb R^n$ a fixed integrable set and let be $f(x,t)$ scalar function of class $C^1$ defined in $S\times I$ where $I$ is an interval. So I ask to prove that the identity $$ \frac{d}{dt}\int_Sf(x,t)=\int_S\frac{\partial f}{\partial t}(x,t) $$ holds fore each $t\in I$ . So to prove it I advanced the following argumentations. First of all thorugh the linearly if integral (is this correct?) I observed that $$ \frac{d}{dt}\int_Sf(x,t):=\lim_{t\rightarrow t_0}\frac{\int_S f(x,t)-\int_Sf(x,t_0)}{t-t_0}=\lim_{t\rightarrow t_0}\frac{\int_S\big(f(x,t)-f(x,t_0)\big)}{t-t_0}=\lim_{t\rightarrow t_0}\int_S\Biggl(\frac{f(x,t)-f(x,t_0)}{t-t_0}\Biggl) $$ for each $t_0\in I$ and so the statement follows directely showing that $$ \lim_{t\rightarrow t_0}\int_S\Biggl(\frac{f(x,t)-f(x,t_0)}{t-t_0}\Biggl)=\int_S\Biggl(\lim_{t\rightarrow t_0}\frac{f(x,t)-f(x,t_0)}{t-t_0}\Biggl)= $$ but unfortunately I did not able to do this. However by the mean value theorem (is this true?) I know that there must exist $\theta_t\in(0,1)$ such that $$ \frac{\partial f}{\partial t}(x,t_0+\theta_t(t-t_0))=\frac{f(x,t)-f(x,t_0)}{t-t_0} $$ so that $$ \lim_{t\rightarrow t_0}\int_S\Biggl(\frac{f(x,t)-f(x,t_0)}{t-t_0}\Biggl)=\lim_{t\rightarrow t_0}\int_S\frac{\partial f}{\partial t}(x,t_0+\theta_t(t-t_0)) $$ and so I think that the resul follows showing that if $t\rightarrow t_0$ then $\theta_t\rightarrow 0$ but unfortunately I did not able to do this. Finally by the mean value integral theorem we know that for each $t\in I$ exists $\xi_t\in S$ such that $$ \int_S\Biggl(\frac{f(x,t)-f(x,t_0)}{t-t_0}\Biggl)=\frac{f(\xi,t)-f(\xi,t_0)}{t-t_0}\cdot\text{vol}(S) $$ so that $$ \lim_{t\rightarrow t_0}\int_S\Biggl(\frac{f(x,t)-f(x,t_0)}{t-t_0}\Biggl)=\lim_{t\rightarrow t_0}\Biggl(\frac{f(\xi,t)-f(\xi,t_0)}{t-t_0}\cdot\text{vol}(S)\Biggl)=\\ \Biggl(\lim_{t\rightarrow t_0}\frac{f(\xi,t)-f(\xi,t_0)}{t-t_0}\Biggl)\cdot\text{vol}(S)=\frac{\partial f}{\partial t}(\xi,t_0)\cdot\text{vol}(S) $$ so that the statement follows showing that $$ \frac{\partial f}{\partial t}(\xi,t_0)\cdot\text{vol}(S)=\int_S\frac{\partial f}{\partial t}(x,t_0) $$ but again I did not able to do this. So could someone help me, please? I point out I did NOT study Lebesgue integration theory so I courteously ask to do not use it, thanks.","Let be a fixed integrable set and let be scalar function of class defined in where is an interval. So I ask to prove that the identity holds fore each . So to prove it I advanced the following argumentations. First of all thorugh the linearly if integral (is this correct?) I observed that for each and so the statement follows directely showing that but unfortunately I did not able to do this. However by the mean value theorem (is this true?) I know that there must exist such that so that and so I think that the resul follows showing that if then but unfortunately I did not able to do this. Finally by the mean value integral theorem we know that for each exists such that so that so that the statement follows showing that but again I did not able to do this. So could someone help me, please? I point out I did NOT study Lebesgue integration theory so I courteously ask to do not use it, thanks.","S\subseteq\Bbb R^n f(x,t) C^1 S\times I I 
\frac{d}{dt}\int_Sf(x,t)=\int_S\frac{\partial f}{\partial t}(x,t)
 t\in I 
\frac{d}{dt}\int_Sf(x,t):=\lim_{t\rightarrow t_0}\frac{\int_S f(x,t)-\int_Sf(x,t_0)}{t-t_0}=\lim_{t\rightarrow t_0}\frac{\int_S\big(f(x,t)-f(x,t_0)\big)}{t-t_0}=\lim_{t\rightarrow t_0}\int_S\Biggl(\frac{f(x,t)-f(x,t_0)}{t-t_0}\Biggl)
 t_0\in I 
\lim_{t\rightarrow t_0}\int_S\Biggl(\frac{f(x,t)-f(x,t_0)}{t-t_0}\Biggl)=\int_S\Biggl(\lim_{t\rightarrow t_0}\frac{f(x,t)-f(x,t_0)}{t-t_0}\Biggl)=
 \theta_t\in(0,1) 
\frac{\partial f}{\partial t}(x,t_0+\theta_t(t-t_0))=\frac{f(x,t)-f(x,t_0)}{t-t_0}
 
\lim_{t\rightarrow t_0}\int_S\Biggl(\frac{f(x,t)-f(x,t_0)}{t-t_0}\Biggl)=\lim_{t\rightarrow t_0}\int_S\frac{\partial f}{\partial t}(x,t_0+\theta_t(t-t_0))
 t\rightarrow t_0 \theta_t\rightarrow 0 t\in I \xi_t\in S 
\int_S\Biggl(\frac{f(x,t)-f(x,t_0)}{t-t_0}\Biggl)=\frac{f(\xi,t)-f(\xi,t_0)}{t-t_0}\cdot\text{vol}(S)
 
\lim_{t\rightarrow t_0}\int_S\Biggl(\frac{f(x,t)-f(x,t_0)}{t-t_0}\Biggl)=\lim_{t\rightarrow t_0}\Biggl(\frac{f(\xi,t)-f(\xi,t_0)}{t-t_0}\cdot\text{vol}(S)\Biggl)=\\
\Biggl(\lim_{t\rightarrow t_0}\frac{f(\xi,t)-f(\xi,t_0)}{t-t_0}\Biggl)\cdot\text{vol}(S)=\frac{\partial f}{\partial t}(\xi,t_0)\cdot\text{vol}(S)
 
\frac{\partial f}{\partial t}(\xi,t_0)\cdot\text{vol}(S)=\int_S\frac{\partial f}{\partial t}(x,t_0)
","['real-analysis', 'integration', 'multivariable-calculus', 'solution-verification', 'riemann-integration']"
23,How do I check if the normal vector is pointing inside or outside?,How do I check if the normal vector is pointing inside or outside?,,"Lets say I've a sphere $x^2+y^2+z^2=1$ and I need to solve an integral $\iint_S\vec F\cdot\vec ndS$ . while $S$ is the sphere. And I can't use gauss law because $\vec F$ is not continuous at some point inside $S$ . I had $$\vec F=\left(\frac{x}{\sqrt{x^2+y^2+z^2}},\frac{y}{\sqrt{x^2+y^2+z^2}},\frac{z}{\sqrt{x^2+y^2+z^2}}\right).$$ So I went and tried to go for it normally using ball coordinates $$\vec r(\theta,\phi)=(\cos\theta \sin\phi, \sin\theta \sin\phi, \cos\phi).$$ And I found that $$r_{\theta}\times r_{\phi}=(-\cos\theta \sin^2\phi,-\sin\theta \sin^2\phi,-\sin\phi \cos\phi).$$ Now I know that this could be the normal pointing inside the ball or outside of it. In order to try to check, I tried to reach a point $(1,0,0)$ on it, and check if the "" $x$ "" part of the normal is positive or negative in that point. But I got lost trying to find $\phi,\theta$ which satisfy that, and wanted to know if there's any better way to decide in what direction the normal I found is pointing. Any feedback is appreciated, thanks in advance!","Lets say I've a sphere and I need to solve an integral . while is the sphere. And I can't use gauss law because is not continuous at some point inside . I had So I went and tried to go for it normally using ball coordinates And I found that Now I know that this could be the normal pointing inside the ball or outside of it. In order to try to check, I tried to reach a point on it, and check if the "" "" part of the normal is positive or negative in that point. But I got lost trying to find which satisfy that, and wanted to know if there's any better way to decide in what direction the normal I found is pointing. Any feedback is appreciated, thanks in advance!","x^2+y^2+z^2=1 \iint_S\vec F\cdot\vec ndS S \vec F S \vec F=\left(\frac{x}{\sqrt{x^2+y^2+z^2}},\frac{y}{\sqrt{x^2+y^2+z^2}},\frac{z}{\sqrt{x^2+y^2+z^2}}\right). \vec r(\theta,\phi)=(\cos\theta \sin\phi, \sin\theta \sin\phi, \cos\phi). r_{\theta}\times r_{\phi}=(-\cos\theta \sin^2\phi,-\sin\theta \sin^2\phi,-\sin\phi \cos\phi). (1,0,0) x \phi,\theta","['integration', 'multivariable-calculus', 'vector-analysis', 'vector-fields']"
24,"Differentiating vector by matrix, optimization problem","Differentiating vector by matrix, optimization problem",,"I am currently writing a simulation, which includes an loss function at the end for optimization. In order to perform backpropagation I need to calculate some derivatives. My forward pass is the following function: $$ s(\alpha, \beta)=N - \sum_{i=1}^{N}\prod_{j=1}^{E_{max}}1-sigm(\alpha*I_{i,j}+\beta*D_{i,j}) \\ ´\\ \text{with: } N, E_{max} \in \mathbb{N_0}, \\ \alpha, \beta \in \mathbb{R}\\ I, D \in \mathbb{R}^{N\times E_{max}} $$ 1. Is there a more concise way to calculate the row product of a matrix? While applying the chain rule I ran into the following issue: $$ f_{i,j}(\alpha,\beta) :=  sigm(\alpha*I_{i,j}+\beta*D_{i,j})\\ g_{i,j}(f):=sigm(f)\\ m_i(g):= \prod_{j=1}^{E_{max}}1-g_{i,j} \\ s(m):= N-\sum_{i=1}^{N}m_i\\ L(s):= \frac{1}{2}*(y-s)^2 $$ If I want to optimize for $\alpha$ , I need to calculate $\frac{dL}{d\alpha}$ So far I got: $\frac{dL}{ds}=-(y-s)$ $\frac{ds}{dm}=[-1, \dots, -1]$ 2. But how can I get $\frac{dm}{dg}$ ? If I am not mistaken I would have to differentiate a vector function $m$ by a matrix $g$ ? 3. Is there a better way to get the derivative of $s$ directly?","I am currently writing a simulation, which includes an loss function at the end for optimization. In order to perform backpropagation I need to calculate some derivatives. My forward pass is the following function: 1. Is there a more concise way to calculate the row product of a matrix? While applying the chain rule I ran into the following issue: If I want to optimize for , I need to calculate So far I got: 2. But how can I get ? If I am not mistaken I would have to differentiate a vector function by a matrix ? 3. Is there a better way to get the derivative of directly?","
s(\alpha, \beta)=N - \sum_{i=1}^{N}\prod_{j=1}^{E_{max}}1-sigm(\alpha*I_{i,j}+\beta*D_{i,j})
\\ ´\\ \text{with: } N, E_{max} \in \mathbb{N_0}, \\ \alpha, \beta \in \mathbb{R}\\ I, D \in \mathbb{R}^{N\times E_{max}}
 
f_{i,j}(\alpha,\beta) :=  sigm(\alpha*I_{i,j}+\beta*D_{i,j})\\
g_{i,j}(f):=sigm(f)\\
m_i(g):= \prod_{j=1}^{E_{max}}1-g_{i,j} \\
s(m):= N-\sum_{i=1}^{N}m_i\\
L(s):= \frac{1}{2}*(y-s)^2
 \alpha \frac{dL}{d\alpha} \frac{dL}{ds}=-(y-s) \frac{ds}{dm}=[-1, \dots, -1] \frac{dm}{dg} m g s","['multivariable-calculus', 'derivatives', 'optimization', 'matrix-calculus', 'gradient-descent']"
25,Describe a subset of $R^{3}$ and modify the order of an iterated triple integral,Describe a subset of  and modify the order of an iterated triple integral,R^{3},"I have come to a problem in a multivariate calculus book that I am having trouble solving. The problem goes : If $D \subseteq \mathbb{R}^{3}$ and : \begin{equation} \int\int\int_{D} d(x,y,z) = \int_{-1}^{1} \left[ \int_{x^{2}}^{1} \left( \int_{0}^{1-y} dz\right)dy\right]dx \end{equation} then describe $D$ . Rewrite the triple integral as an iterated integral in which $dx$ , $dy$ , and $dz$ appear in each of the following orders (i.) $dz,dx,dy$ , (ii.) $dx,dy,dz$ , (iii.) $dx,dz,dy$ , (iv.) $dy,dz,dx$ , (v.) $dy,dx,dz$ . I'm not sure how to approach this problem. I was able to do previous problems in the book that required changing the order of 2D iterated integrals, but in the 3D case I am having more trouble. Can someone help with this ? Edit : After reading the comments, I have come up with a possible solution. The solution is below : We have for the cross section perpendicular to the z-axis at $z = 0$ : Now suppose we have $E \subset \mathbb{R}^{3}$ s.t. : \begin{equation} \int\int\int_{E} d(x,y,z) = \int_{-1}^{1} \left[ \int_{x^{2}}^{1} \left( \int_{0}^{1} dz \right) dy \right] dx \end{equation} Then $E$ is a parabolic cylinder with height $1$ and base at the plane corresponding to $z = 0$ : \begin{equation} E = \{ (x,y,z) \in \mathbb{R}^{3} \; : \; x \in [-1,1] \text{ and } y \in [x^{2},1] \text{ and } z \in [0,1] \} \end{equation} $D$ is a subset of $E$ . Define plane $P$ : \begin{equation} P = \{ (x,y,z) \in \mathbb{R}^{3} \; : \; y + z = 1 \} \end{equation} Now define region of $\mathbb{R}^{3}$ below $P$ : \begin{equation} M = \{ (x,y,z) \in \mathbb{R}^{3} \; : \; y + z \leq 1 \} \end{equation} We see : \begin{equation} D = \{ (x,y,z) \in \mathbb{R}^{3} \; : \; (x,y,z) \in E \bigcap M \} \end{equation} We see : \begin{equation} y + z \leq 1 \Leftrightarrow y \leq 1 - z \end{equation} We see : \begin{equation} y = x^{2} \Rightarrow x = \pm \sqrt{y} \end{equation} So for plane $z = z_{0}$ we have the cross section of $D$ : So you can peform the integration by adding up slices like those shown above. In this case the order is $[dy,dx,dz]$ or $[dx,dy,dz]$ I believe. Let : \begin{equation} I = \int\int\int_{D} d(x,y,z) \end{equation} Then I think we have for $dy,dx,dz$ : \begin{equation} I = \int_{0}^{1} \left[ \int_{-\sqrt{1-z}}^{\sqrt{1-z}} \left( \int_{0}^{x^{2}} dy \right) dx \right] dz \; \checkmark \end{equation} and : \begin{equation} I = \int_{0}^{1} \left[ \int_{0}^{1-z} \left( \int_{-\sqrt{y}}^{\sqrt{y}} dx \right) dy \right] dz \; \checkmark \end{equation} Now still need $[dz,dx,dy]$ , $[dx,dz,dy]$ , and $[dy,dz,dx]$ . For $[dz,dx,dy]$ and $[dx,dz,dy]$ we will need to sum cross sections that are perpendicular to the y-axis. We can draw the cross-section of $D$ at $y = y_{0}$ as : So we have : \begin{equation} I = \int_{0}^{1} \left[ \int_{-\sqrt{y}}^{\sqrt{y}} \left( \int_{0}^{1-x^{2}} dz \right) dx \right] dy \; \checkmark \end{equation} and : \begin{equation} I = \int_{0}^{1} \left[ \int_{0}^{1-y} \left( \int_{-\sqrt{1-z}}^{\sqrt{1-z}} dx \right) dz \right] dy \; \checkmark \end{equation} Now only need $[dy,dz,dx]$ . We see : \begin{align} y \in [x^{2},1] \text{ and } z \in [0,1-y] 	& \Rightarrow z \in [1-1,1-x^{2}]\\ 						& \Rightarrow z \in [0,1-x^{2}] \end{align} We see : \begin{align} z = 1-x^{2} 	& \Leftrightarrow z - 1 = -x^{2} \\ 		& \Leftrightarrow 1 - z = x^{2} \end{align} So : \begin{equation} y \in [x^{2},1] \Rightarrow y \in [1-z,1] \end{equation} and : \begin{equation} I = \int_{-1}^{1} \left[ \int_{0}^{1-x^{2}} \left( \int_{1-z}^{1} dy \right) dz \right] dx \; \checkmark \end{equation}","I have come to a problem in a multivariate calculus book that I am having trouble solving. The problem goes : If and : then describe . Rewrite the triple integral as an iterated integral in which , , and appear in each of the following orders (i.) , (ii.) , (iii.) , (iv.) , (v.) . I'm not sure how to approach this problem. I was able to do previous problems in the book that required changing the order of 2D iterated integrals, but in the 3D case I am having more trouble. Can someone help with this ? Edit : After reading the comments, I have come up with a possible solution. The solution is below : We have for the cross section perpendicular to the z-axis at : Now suppose we have s.t. : Then is a parabolic cylinder with height and base at the plane corresponding to : is a subset of . Define plane : Now define region of below : We see : We see : We see : So for plane we have the cross section of : So you can peform the integration by adding up slices like those shown above. In this case the order is or I believe. Let : Then I think we have for : and : Now still need , , and . For and we will need to sum cross sections that are perpendicular to the y-axis. We can draw the cross-section of at as : So we have : and : Now only need . We see : We see : So : and :","D \subseteq \mathbb{R}^{3} \begin{equation}
\int\int\int_{D} d(x,y,z) = \int_{-1}^{1} \left[ \int_{x^{2}}^{1} \left( \int_{0}^{1-y} dz\right)dy\right]dx
\end{equation} D dx dy dz dz,dx,dy dx,dy,dz dx,dz,dy dy,dz,dx dy,dx,dz z = 0 E \subset \mathbb{R}^{3} \begin{equation}
\int\int\int_{E} d(x,y,z) = \int_{-1}^{1} \left[ \int_{x^{2}}^{1} \left( \int_{0}^{1} dz \right) dy \right] dx
\end{equation} E 1 z = 0 \begin{equation}
E = \{ (x,y,z) \in \mathbb{R}^{3} \; : \; x \in [-1,1] \text{ and } y \in [x^{2},1] \text{ and } z \in [0,1] \}
\end{equation} D E P \begin{equation}
P = \{ (x,y,z) \in \mathbb{R}^{3} \; : \; y + z = 1 \}
\end{equation} \mathbb{R}^{3} P \begin{equation}
M = \{ (x,y,z) \in \mathbb{R}^{3} \; : \; y + z \leq 1 \}
\end{equation} \begin{equation}
D = \{ (x,y,z) \in \mathbb{R}^{3} \; : \; (x,y,z) \in E \bigcap M \}
\end{equation} \begin{equation}
y + z \leq 1 \Leftrightarrow y \leq 1 - z
\end{equation} \begin{equation}
y = x^{2} \Rightarrow x = \pm \sqrt{y}
\end{equation} z = z_{0} D [dy,dx,dz] [dx,dy,dz] \begin{equation}
I = \int\int\int_{D} d(x,y,z)
\end{equation} dy,dx,dz \begin{equation}
I = \int_{0}^{1} \left[ \int_{-\sqrt{1-z}}^{\sqrt{1-z}} \left( \int_{0}^{x^{2}} dy \right) dx \right] dz \; \checkmark
\end{equation} \begin{equation}
I = \int_{0}^{1} \left[ \int_{0}^{1-z} \left( \int_{-\sqrt{y}}^{\sqrt{y}} dx \right) dy \right] dz \; \checkmark
\end{equation} [dz,dx,dy] [dx,dz,dy] [dy,dz,dx] [dz,dx,dy] [dx,dz,dy] D y = y_{0} \begin{equation}
I = \int_{0}^{1} \left[ \int_{-\sqrt{y}}^{\sqrt{y}} \left( \int_{0}^{1-x^{2}} dz \right) dx \right] dy \; \checkmark
\end{equation} \begin{equation}
I = \int_{0}^{1} \left[ \int_{0}^{1-y} \left( \int_{-\sqrt{1-z}}^{\sqrt{1-z}} dx \right) dz \right] dy \; \checkmark
\end{equation} [dy,dz,dx] \begin{align}
y \in [x^{2},1] \text{ and } z \in [0,1-y] 	& \Rightarrow z \in [1-1,1-x^{2}]\\
						& \Rightarrow z \in [0,1-x^{2}]
\end{align} \begin{align}
z = 1-x^{2} 	& \Leftrightarrow z - 1 = -x^{2} \\
		& \Leftrightarrow 1 - z = x^{2}
\end{align} \begin{equation}
y \in [x^{2},1] \Rightarrow y \in [1-z,1]
\end{equation} \begin{equation}
I = \int_{-1}^{1} \left[ \int_{0}^{1-x^{2}} \left( \int_{1-z}^{1} dy \right) dz \right] dx \; \checkmark
\end{equation}",['multivariable-calculus']
26,Sufficient conditions for partial derivative reciprocal,Sufficient conditions for partial derivative reciprocal,,"I would like to know the sufficient conditions for the following to hold, $$\frac{\partial y}{\partial x}= \frac{1}{\frac{\partial x}{\partial y}}$$ I have intentionally been vague here because I don't want to restrict any answers to specific cases and I don't know enough about multivariable calculus to appreciate what information I need to tell you in order for this to be a straightforward question. I'm hoping this situation is common enough for you to get the jist, but please let me know otherwise. There a bunch of similar questions on this site, and I haven't found one that quite addersses this. This question tells us that it is not always true, but not why. This question also shows it isn't true in general, and apparently we can form some kind of matrix to answer this question, but there is no reference how to actually do this in any other example or what it is called. Also, we have to make sure ""the same variables are being held constant in each partial derivative."", what does this mean? Why is it sufficient? Because the lecturer said so? Before marking this as a duplicate, please dig deep as to whether the question posed here has really been answered.","I would like to know the sufficient conditions for the following to hold, I have intentionally been vague here because I don't want to restrict any answers to specific cases and I don't know enough about multivariable calculus to appreciate what information I need to tell you in order for this to be a straightforward question. I'm hoping this situation is common enough for you to get the jist, but please let me know otherwise. There a bunch of similar questions on this site, and I haven't found one that quite addersses this. This question tells us that it is not always true, but not why. This question also shows it isn't true in general, and apparently we can form some kind of matrix to answer this question, but there is no reference how to actually do this in any other example or what it is called. Also, we have to make sure ""the same variables are being held constant in each partial derivative."", what does this mean? Why is it sufficient? Because the lecturer said so? Before marking this as a duplicate, please dig deep as to whether the question posed here has really been answered.",\frac{\partial y}{\partial x}= \frac{1}{\frac{\partial x}{\partial y}},"['multivariable-calculus', 'partial-derivative']"
27,"Why can we think the divergence and the curl as some kind of ""derivative""?","Why can we think the divergence and the curl as some kind of ""derivative""?",,My question arises from think about why the divergence theorem and the stoke's theorem are considered generalizations of the fundamental theorem of calculus.,My question arises from think about why the divergence theorem and the stoke's theorem are considered generalizations of the fundamental theorem of calculus.,,"['calculus', 'multivariable-calculus', 'vector-analysis']"
28,One-point gradient estimator and Stokes' theorem.,One-point gradient estimator and Stokes' theorem.,,"In bandit convex optimization, we are only given access to zeroth-order oracle of a function but not first-order (gradient) oracle. Hence, people often use some one-point gradient estimator to approximate the gradient of a function at a certain point. More specifically, let $f:\mathbb{R}^d\mapsto \mathbb{R}$ be a function in $\mathbb{R}^d$ , $\mathbb{S}$ be the unit sphere in $\mathbb{R}^d$ , $\mathbb{B}$ be the unit ball in $\mathbb{R}^d$ . For a point $\mathbb{x}\in\mathbb{R}^d$ , if we want to estimate $\nabla f(\mathbf{x})$ , we randomly sample a $\mathbf{u}$ from $\mathbb{S}$ . Let $\delta>0$ be a small positive scalar. We query the zeroth-order oracle and obtain $f(\mathbf{x}+\delta \mathbf{u})$ . We then use $\frac{f(\mathbf{x}+\delta \mathbf{u})d\mathbf{u}}{\delta}$ as an approximation of $\nabla f(\mathbf{x})$ . Lemma 1 in this paper proved that the expectation of the approximation is the gradient of a smoothed version of $f$ , that is let $\hat{f}(\mathbf{x})=\mathbb{E}_{\mathbf{v}\in\mathbb{B}}[f(\mathbf{x}+\delta \mathbf{v})]$ , then $\nabla \hat{f}(x)=\mathbb{E}_{\mathbf{u}\in \mathbb{S}}\big[\frac{f(\mathbf{x}+\delta \mathbf{u})d\mathbf{u}}{\delta}\big]$ . The proof invoked Stokes's theorem to show the following (equation (15) in the paper): \begin{align} \nabla \int_{\delta \mathbb{B}}f(\mathbf{x}+\mathbf{v})\mathrm{d}\mathbf{v}=\int_{\delta \mathbb{S}}f(\mathbf{x}+\mathbf{u})\frac{\mathbf{u}}{\|\mathbf{u}\|}\mathrm{d}\mathbf{u}. \end{align} I'm trying to understand how exactly does the above follow from the Stokes's theorem. Is it that the curl of the vector field defined as $f(\mathbf{x}+\mathbf{u})\frac{\mathbf{u}}{\|\mathbf{u}\|}$ somehow is related to the left-hand-side?","In bandit convex optimization, we are only given access to zeroth-order oracle of a function but not first-order (gradient) oracle. Hence, people often use some one-point gradient estimator to approximate the gradient of a function at a certain point. More specifically, let be a function in , be the unit sphere in , be the unit ball in . For a point , if we want to estimate , we randomly sample a from . Let be a small positive scalar. We query the zeroth-order oracle and obtain . We then use as an approximation of . Lemma 1 in this paper proved that the expectation of the approximation is the gradient of a smoothed version of , that is let , then . The proof invoked Stokes's theorem to show the following (equation (15) in the paper): I'm trying to understand how exactly does the above follow from the Stokes's theorem. Is it that the curl of the vector field defined as somehow is related to the left-hand-side?","f:\mathbb{R}^d\mapsto \mathbb{R} \mathbb{R}^d \mathbb{S} \mathbb{R}^d \mathbb{B} \mathbb{R}^d \mathbb{x}\in\mathbb{R}^d \nabla f(\mathbf{x}) \mathbf{u} \mathbb{S} \delta>0 f(\mathbf{x}+\delta \mathbf{u}) \frac{f(\mathbf{x}+\delta \mathbf{u})d\mathbf{u}}{\delta} \nabla f(\mathbf{x}) f \hat{f}(\mathbf{x})=\mathbb{E}_{\mathbf{v}\in\mathbb{B}}[f(\mathbf{x}+\delta \mathbf{v})] \nabla \hat{f}(x)=\mathbb{E}_{\mathbf{u}\in \mathbb{S}}\big[\frac{f(\mathbf{x}+\delta \mathbf{u})d\mathbf{u}}{\delta}\big] \begin{align}
\nabla \int_{\delta \mathbb{B}}f(\mathbf{x}+\mathbf{v})\mathrm{d}\mathbf{v}=\int_{\delta \mathbb{S}}f(\mathbf{x}+\mathbf{u})\frac{\mathbf{u}}{\|\mathbf{u}\|}\mathrm{d}\mathbf{u}.
\end{align} f(\mathbf{x}+\mathbf{u})\frac{\mathbf{u}}{\|\mathbf{u}\|}","['multivariable-calculus', 'convex-optimization', 'gradient-descent', 'stokes-theorem']"
29,I can't find my error in solving line integral in two different methods,I can't find my error in solving line integral in two different methods,,"I have this problem: given the points $A(-1,-2)$ and $B(2,1)$ the path $T$ from $A$ to $B$ is on the circule $(x-0.5)^{2}+(y+0.5)^{2}=4.5$ caculate: $$\int _{T}\frac{ydx-xdy}{x^{2}+y^{2}}$$ after a short calculate I found out that: $P_{y}=Q_{x}$ and with this following simply connected space $D$ , making $\vec{F}$ conservative vector field: Therefore I can choose a different path between the two points. Now my main error is that I calculate the integrals with two different paths and I've given different results. for the path $C$ on the circle $x^{2}+y^{2}=5$ I've got that: the red path is part of the circule. $$\left\{\begin{matrix} x= \sqrt[]{5}\cos(t) & dx = (-1)\sqrt[]{5}\sin(t)dt \\   y= \sqrt[]{5}\sin(t) & dy = \sqrt[]{5}\cos(t)dt \end{matrix}\right.$$ and by using caculator: $$\int _{T}\frac{ydx-xdy}{x^{2}+y^{2}} = \int _{C}\frac{((\sqrt[]{5}\sin(t)\cdot (-1)\sqrt[]{5}\sin(t))-(\sqrt[]{5}\cos(t)\cdot\sqrt[]{5}\cos(t)))dt}{5}$$ $$=-1\cdot\int _{C}1\cdot dt = -1\cdot\int_{\arccos (\frac{-1}{\sqrt[]{5}})}^{\arccos (\frac{2}{\sqrt[]{5}})} 1 \cdot dt = \frac{\pi }{2}$$ but for the path $E$ wich is the sum of the following paths: $E1$ , $E2$ I'm getting diffrent result. $$E_{1}(t) = (t,-2)\rightarrow\left\{\begin{matrix} dx= dt   \\   dy= 0  \end{matrix}\right., -1\leq t\leq 2$$ $$\int _{E_{1}}\frac{ydx-xdy}{x^{2}+y^{2}} = \int_{-1}^{2}\frac{-2}{t^{2}+4}dt$$ $$E_{2}(t) = (2,t)\rightarrow\left\{\begin{matrix} dx= 0   \\   dy= dt  \end{matrix}\right., -2\leq t\leq 1$$ $$\int _{E_{2}}\frac{ydx-xdy}{x^{2}+y^{2}} = \int_{-2}^{1}\frac{-2}{t^{2}+4}dt$$ in this picture the paths $E_{1}$ and $E_{2}$ are drwan in pink: and I'm getting using caculator that: $$\int _{T}\frac{ydx-xdy}{x^{2}+y^{2}} = \int_{-1}^{2}\frac{-2}{t^{2}+4}dt + \int_{-2}^{1}\frac{-2}{t^{2}+4}dt \approx -2.49$$ so, where is my error?","I have this problem: given the points and the path from to is on the circule caculate: after a short calculate I found out that: and with this following simply connected space , making conservative vector field: Therefore I can choose a different path between the two points. Now my main error is that I calculate the integrals with two different paths and I've given different results. for the path on the circle I've got that: the red path is part of the circule. and by using caculator: but for the path wich is the sum of the following paths: , I'm getting diffrent result. in this picture the paths and are drwan in pink: and I'm getting using caculator that: so, where is my error?","A(-1,-2) B(2,1) T A B (x-0.5)^{2}+(y+0.5)^{2}=4.5 \int _{T}\frac{ydx-xdy}{x^{2}+y^{2}} P_{y}=Q_{x} D \vec{F} C x^{2}+y^{2}=5 \left\{\begin{matrix}
x= \sqrt[]{5}\cos(t) & dx = (-1)\sqrt[]{5}\sin(t)dt \\  
y= \sqrt[]{5}\sin(t) & dy = \sqrt[]{5}\cos(t)dt
\end{matrix}\right. \int _{T}\frac{ydx-xdy}{x^{2}+y^{2}} = \int _{C}\frac{((\sqrt[]{5}\sin(t)\cdot (-1)\sqrt[]{5}\sin(t))-(\sqrt[]{5}\cos(t)\cdot\sqrt[]{5}\cos(t)))dt}{5} =-1\cdot\int _{C}1\cdot dt = -1\cdot\int_{\arccos (\frac{-1}{\sqrt[]{5}})}^{\arccos (\frac{2}{\sqrt[]{5}})} 1 \cdot dt = \frac{\pi }{2} E E1 E2 E_{1}(t) = (t,-2)\rightarrow\left\{\begin{matrix}
dx= dt   \\  
dy= 0 
\end{matrix}\right., -1\leq t\leq 2 \int _{E_{1}}\frac{ydx-xdy}{x^{2}+y^{2}} = \int_{-1}^{2}\frac{-2}{t^{2}+4}dt E_{2}(t) = (2,t)\rightarrow\left\{\begin{matrix}
dx= 0   \\  
dy= dt 
\end{matrix}\right., -2\leq t\leq 1 \int _{E_{2}}\frac{ydx-xdy}{x^{2}+y^{2}} = \int_{-2}^{1}\frac{-2}{t^{2}+4}dt E_{1} E_{2} \int _{T}\frac{ydx-xdy}{x^{2}+y^{2}} = \int_{-1}^{2}\frac{-2}{t^{2}+4}dt + \int_{-2}^{1}\frac{-2}{t^{2}+4}dt \approx -2.49","['integration', 'multivariable-calculus', 'path-connected']"
30,Shared eigen vectors of Hessian,Shared eigen vectors of Hessian,,"Note: this question is still unanswered! (I will provide an answer if I come to one on my own). Background Say we have a continuous twice differentiable function $f:\mathbb{R}^n \rightarrow \mathbb{R}$ . We know that this function has a symmetric, positive definite Hessian matrix, $\nabla^2 f$ . This Hessian can be decomposed as, $$\nabla^2 f = R \Lambda R^T, $$ where $\Lambda: \mathbb{R}^n \rightarrow \mathbb{R}^{n \times n}$ is a diagonal matrix of eigen valuaes and $R:\mathbb{R}^n \rightarrow \mathbb{R}^{n \times n}$ is a matrix of eigen vectors. Note that here $\Lambda$ and $R$ are themselves functions of the arguments to $f$ . Question Given the function $f$ has a Hessian with decomposition $R \Lambda R^T$ , is there a way to compute all functions $g:\mathbb{R}^n \rightarrow \mathbb{R}$ such that, $$\nabla^2 g = R D R^T, $$ where $D$ is a diagonal matrix? Updates: John provided a method for computing $g$ so that its Hessian has the correct decomposition at a chosen point. Unfortunately this does not hold over all of $\mathbb{R}^n$ .","Note: this question is still unanswered! (I will provide an answer if I come to one on my own). Background Say we have a continuous twice differentiable function . We know that this function has a symmetric, positive definite Hessian matrix, . This Hessian can be decomposed as, where is a diagonal matrix of eigen valuaes and is a matrix of eigen vectors. Note that here and are themselves functions of the arguments to . Question Given the function has a Hessian with decomposition , is there a way to compute all functions such that, where is a diagonal matrix? Updates: John provided a method for computing so that its Hessian has the correct decomposition at a chosen point. Unfortunately this does not hold over all of .","f:\mathbb{R}^n \rightarrow \mathbb{R} \nabla^2 f \nabla^2 f = R \Lambda R^T,  \Lambda: \mathbb{R}^n \rightarrow \mathbb{R}^{n \times n} R:\mathbb{R}^n \rightarrow \mathbb{R}^{n \times n} \Lambda R f f R \Lambda R^T g:\mathbb{R}^n \rightarrow \mathbb{R} \nabla^2 g = R D R^T,  D g \mathbb{R}^n","['linear-algebra', 'multivariable-calculus', 'eigenvalues-eigenvectors', 'hessian-matrix']"
31,When do total and partial derivatives commute?,When do total and partial derivatives commute?,,"In Classical Mechanics, the Lagrangian is a function of a generalized coordinate $q$ , the corresponding generalized velocity $\dot{q}$ , and time $t$ . For the Lagrangian, $$\dfrac{d}{dt}\frac{\partial L}{\partial \dot{q}} \neq \frac{\partial \dot{L}}{\partial \dot{q}},$$ that is, the two derivatives do not commute. However, what if $L$ were a function only of $q$ and $t$ ? Then would the total derivative in $t$ and the partial derivative in $q$ always commute? Also, for $L(q, t)$ ,  is $\frac{\partial \dot{L}}{\partial \dot{q}}$ always equal to $\frac{\partial L}{\partial q}$ ?","In Classical Mechanics, the Lagrangian is a function of a generalized coordinate , the corresponding generalized velocity , and time . For the Lagrangian, that is, the two derivatives do not commute. However, what if were a function only of and ? Then would the total derivative in and the partial derivative in always commute? Also, for ,  is always equal to ?","q \dot{q} t \dfrac{d}{dt}\frac{\partial L}{\partial \dot{q}} \neq \frac{\partial \dot{L}}{\partial \dot{q}}, L q t t q L(q, t) \frac{\partial \dot{L}}{\partial \dot{q}} \frac{\partial L}{\partial q}","['multivariable-calculus', 'derivatives', 'partial-derivative']"
32,"Method to compute $\int_{\sqrt{2}}^{2} \int_{1}^{\sqrt{2}} \frac{(\log(\frac{xy}{2}))^2 (x^2+y^2) }{(x^2-y^2)^{2}}\,dx \,dy$",Method to compute,"\int_{\sqrt{2}}^{2} \int_{1}^{\sqrt{2}} \frac{(\log(\frac{xy}{2}))^2 (x^2+y^2) }{(x^2-y^2)^{2}}\,dx \,dy","Can anybody give a hint, how to compute the integral analytically $$ \int_{\sqrt{\,{2}\,}}^{2}\int_{1}^{\sqrt{\,{2}\,}} \log^{2}\left(xy \over 2\right)\, {x^{2} + y^{2} \over \left(\,{x^{2} - y^{2}}\,\right)^{2}} \,\mathrm{d}x\,\mathrm{d}y. $$ Please, I am not looking for computer assisted proofs.","Can anybody give a hint, how to compute the integral analytically Please, I am not looking for computer assisted proofs.","
\int_{\sqrt{\,{2}\,}}^{2}\int_{1}^{\sqrt{\,{2}\,}}
\log^{2}\left(xy \over 2\right)\,
{x^{2} + y^{2} \over \left(\,{x^{2} - y^{2}}\,\right)^{2}}
\,\mathrm{d}x\,\mathrm{d}y.
","['integration', 'multivariable-calculus', 'multiple-integral']"
33,"Prove that $\{f_n\} _{n=1}^{\infty}$ uniformly converges to $ f(x)=\int_{0}^{1}g(x,t)\mathrm{dt}$",Prove that  uniformly converges to,"\{f_n\} _{n=1}^{\infty}  f(x)=\int_{0}^{1}g(x,t)\mathrm{dt}","Let $g:(0,\infty)\times [0,1]\to {\mathbb{R}}$ be continuous with respect to each variable separately  and $$f_n=\frac{1}{n}\sum_{i=1}^{n}g\left(x,\frac{i}{n}\right)$$ How can show that $\{f_n\} _{n=1}^{\infty}$ uniformly converges to $\displaystyle f(x)=\int_{0}^{1}g(x,t)\mathrm {dt}$ on $[m,M]$ , each subset of $(0,\infty)$ .","Let be continuous with respect to each variable separately  and How can show that uniformly converges to on , each subset of .","g:(0,\infty)\times [0,1]\to {\mathbb{R}} f_n=\frac{1}{n}\sum_{i=1}^{n}g\left(x,\frac{i}{n}\right) \{f_n\} _{n=1}^{\infty} \displaystyle f(x)=\int_{0}^{1}g(x,t)\mathrm {dt} [m,M] (0,\infty)","['real-analysis', 'calculus', 'multivariable-calculus', 'uniform-convergence']"
34,Difficulty interpreting high order derivatives in $\mathbb{R}^n$,Difficulty interpreting high order derivatives in,\mathbb{R}^n,"If $f:U\subseteq\mathbb{R}^m\to \mathbb{R}^n$ is differentiable function then its derivative $$ f':U\to M_{n\times m}(\mathbb{R})\simeq\mathcal{L}(\mathbb{R}^m;\mathbb{R}^n) $$ can be seen, for each $x\in U$ , as a linear transformation $f'(x):\mathbb{R}^m\to\mathbb{R}^n$ . If $f$ is of class $\mathcal{C}^2$ then its second order derivative is a function $$ f'':U\to\mathcal{L}(\mathbb{R}^m\times\mathbb{R}^m;\mathbb{R}^n) $$ that carries each $x\in U$ into a bilinear transformation $f''(x):\mathbb{R}^m\times\mathbb{R}^m\to\mathbb{R}^n$ and inductively if $f$ is of class $\mathcal{C}^k$ its $k$ th derivative is a function $$ f^{(k)}:U\to\mathcal{L}(\mathbb{R}^m\times\cdots\times\mathbb{R}^m;\mathbb{R}^n). $$ Maybe I'm lacking some linear algebra background, but using the isomorphism $\mathcal{L}_2(\mathbb{R}^m\times\mathbb{R}^m;\mathbb{R}^n)\simeq\mathcal{L}(\mathbb{R}^m;\mathcal{L}(\mathbb{R}^m;\mathbb{R}^n))$ and its correspondent in the $k$ -linear case I can see that the $k$ th derivative is a $k$ -linear map. But I don't see how to relate (in the case of the second derivative to simplify) the partial second derivatives $\frac{\partial f_i}{\partial x_j\partial x_k}(x)$ to the matrix of $f''(x)$ as a bilinear map. In the case of a bilinear form it would be a $m\times m$ matrix but it's not the case. Can someone please explain me better? P.S.: I have seen this question but I didn't understand his notation in the last part of the answer (and the first part which answers that question I'm ok with).","If is differentiable function then its derivative can be seen, for each , as a linear transformation . If is of class then its second order derivative is a function that carries each into a bilinear transformation and inductively if is of class its th derivative is a function Maybe I'm lacking some linear algebra background, but using the isomorphism and its correspondent in the -linear case I can see that the th derivative is a -linear map. But I don't see how to relate (in the case of the second derivative to simplify) the partial second derivatives to the matrix of as a bilinear map. In the case of a bilinear form it would be a matrix but it's not the case. Can someone please explain me better? P.S.: I have seen this question but I didn't understand his notation in the last part of the answer (and the first part which answers that question I'm ok with).","f:U\subseteq\mathbb{R}^m\to \mathbb{R}^n 
f':U\to M_{n\times m}(\mathbb{R})\simeq\mathcal{L}(\mathbb{R}^m;\mathbb{R}^n)
 x\in U f'(x):\mathbb{R}^m\to\mathbb{R}^n f \mathcal{C}^2 
f'':U\to\mathcal{L}(\mathbb{R}^m\times\mathbb{R}^m;\mathbb{R}^n)
 x\in U f''(x):\mathbb{R}^m\times\mathbb{R}^m\to\mathbb{R}^n f \mathcal{C}^k k 
f^{(k)}:U\to\mathcal{L}(\mathbb{R}^m\times\cdots\times\mathbb{R}^m;\mathbb{R}^n).
 \mathcal{L}_2(\mathbb{R}^m\times\mathbb{R}^m;\mathbb{R}^n)\simeq\mathcal{L}(\mathbb{R}^m;\mathcal{L}(\mathbb{R}^m;\mathbb{R}^n)) k k k \frac{\partial f_i}{\partial x_j\partial x_k}(x) f''(x) m\times m","['real-analysis', 'linear-algebra', 'multivariable-calculus', 'multilinear-algebra']"
35,Computing $\int_Q \frac{xy}{x^2+y^2}dxdy$,Computing,\int_Q \frac{xy}{x^2+y^2}dxdy,"I'm asked to compute the following integral: $$\int_Q \frac{xy}{x^2+y^2}dxdy \qquad Q=[0,1]^2$$ Solution: First I'm going to study if the integral is convergent. To do this, we notice that $$\int_Q \frac{xy}{x^2+y^2}dxdy < \infty \iff \int_S \frac{xy}{x^2+y^2}dxdy<\infty$$ where $S=\{(x,y) \in \mathbb{R}^2 | x\geq0, y\geq 0, x^2+y^2\leq1\}$ and this is clear because the difference between $\int_Q f(x,y)dxdy$ and $\int_S f(x,y)dxdy$ is a proper integral. Computing we have: $$\int_S \frac{xy}{x^2+y^2}dxdy=\lim_{\epsilon \to 0}\int_{\epsilon}^1\int_0^{\frac{\pi}{2}}\rho^3\sin\theta\cos\theta d\theta d\rho=\frac{1}{8}$$ and so I'm granted the convergence. Now I have to compute the real integral, as we have assured that it's convergent. $$\int_Q \frac{xy}{x^2+y^2}dxdy=\int_0^1 \int_0^1 \frac{xy}{x^2+y^2}dxdy= \int_0^1 \frac{y}{2} \int_0^1 \frac{2x}{x^2+y^2}dxdy = \frac{1}{2} \int_0^1 y(log(1+y^2)-log(y^2))dy =$$ $$ = \frac{\log2}{2}-\frac{1}{2}\lim_{\epsilon \to 0}\int_{\epsilon}^1y\log(y^2)dy= \frac{\log2}{2}$$ I checked the result and it's correct, but I'm asking for a review of the process: did I do anything wrong?","I'm asked to compute the following integral: Solution: First I'm going to study if the integral is convergent. To do this, we notice that where and this is clear because the difference between and is a proper integral. Computing we have: and so I'm granted the convergence. Now I have to compute the real integral, as we have assured that it's convergent. I checked the result and it's correct, but I'm asking for a review of the process: did I do anything wrong?","\int_Q \frac{xy}{x^2+y^2}dxdy \qquad Q=[0,1]^2 \int_Q \frac{xy}{x^2+y^2}dxdy < \infty \iff \int_S \frac{xy}{x^2+y^2}dxdy<\infty S=\{(x,y) \in \mathbb{R}^2 | x\geq0, y\geq 0, x^2+y^2\leq1\} \int_Q f(x,y)dxdy \int_S f(x,y)dxdy \int_S \frac{xy}{x^2+y^2}dxdy=\lim_{\epsilon \to 0}\int_{\epsilon}^1\int_0^{\frac{\pi}{2}}\rho^3\sin\theta\cos\theta d\theta d\rho=\frac{1}{8} \int_Q \frac{xy}{x^2+y^2}dxdy=\int_0^1 \int_0^1 \frac{xy}{x^2+y^2}dxdy= \int_0^1 \frac{y}{2} \int_0^1 \frac{2x}{x^2+y^2}dxdy = \frac{1}{2} \int_0^1 y(log(1+y^2)-log(y^2))dy =  = \frac{\log2}{2}-\frac{1}{2}\lim_{\epsilon \to 0}\int_{\epsilon}^1y\log(y^2)dy= \frac{\log2}{2}","['calculus', 'integration', 'multivariable-calculus', 'improper-integrals', 'solution-verification']"
36,Jacobians when integrating over symmetric and antisymmetric matrices,Jacobians when integrating over symmetric and antisymmetric matrices,,"I'm interested in integrals over matrix elements. For integrals over the elements of a symmetric matrix $S$ or an antisymmetric $A$ , how can I find the Jacobian of the transformations $S \rightarrow B^T S B$ or $A \rightarrow B^T A B$ , where B is some real invertible matrix? Below shares my work and my guesses for what the Jacobians should be. Here's a warmup that shows how I personally would find the Jacobian corresponding to a linear transformation of my matrix of interest. Consider an integral over the $d$ by $d$ dimensional matrix $X$ . That is, consider the integral $$\int_{\Gamma} f(X) dX .$$ $f(X)$ is schematic for a function of all the matrix elements. Here, $dX$ is schematic for $\prod_{i,j}^d dX_{ij}$ . $\Gamma$ is some $d^2$ -dimensional integration region. If I were to consider a real, invertible transformation $U=BX$ , one can quickly see by considering the columns of $X$ that $$\int_{\Gamma} f(X) dX = \int_{\Gamma} f(X) \prod_{j=1}^d \Big(\prod_{i=1}^d dX_{ij}\Big) = \int_{B\Gamma} f(B^{-1}X) \prod_{j=1}^d \frac{\prod_{i=1}^d dU_{ij}}{\det(B)} = \int_{B\Gamma} f(B^{-1}X)  \frac{\ dU}{\det(B)^d}.$$ That is, we have a Jacobian not of $\det(B)$ but of $\det(B)^d$ . One can check this makes sense by considering $B$ a constant $c$ times the identity matrix - then $\det(B) = c^d$ , and $\det(B)^d = c^{d^2}$ , which is exactly what one would get by scaling each of the $d^2$ elements of the matrix $X$ by $c$ . Thus, for $U=BX$ , we have $dX \rightarrow \frac{dU}{\det(B)^d}$ . Similarly, we can see that $$\text{for }U=B^TXB\text{, we have }dX \rightarrow \frac{dU}{\det(B)^{2d}}.$$ This can be seen quickly by performing the transformation in two steps and considering the columns and rows of $X$ . However, what if our integration was over a symmetric matrix $S$ ? That is, consider the integral $\int_{\Delta} f(S)dS.$ Here, $dS$ is schematic for $\prod_{i\geq j}^d dS_{ij}$ . Here, $\Delta$ is the integration region; counting the number of independent components of a symmetric matrix gives us that $\Delta$ must be a $\frac{1}{2}d(d+1)$ -dimensional region. Note that the transformation $U=B^TSB$ preserves the symmetry of $S$ : $U$ is also symmetric. Then, by counting the number of independent components, $\frac{1}{2}d(d+1)$ , I'd guess that $$\text{for } B^TSB\text{, we have } dS \stackrel{?}{\rightarrow} \frac{dU}{\det(B)^{d+1}}.$$ Similarly, for an antisymmetric matrix $A$ , we have that the transformation $U=B^T A B$ preserves the asymmetry; $U$ is also antisymmetric. Defining $dA = \prod_{i>j}^d dA_{ij}$ , and noting $A$ has $\frac{1}{2}d(d-1)$ independent components, I'd guess that $$\text{for } B^TAB\text{, we have } dA \stackrel{?}{\rightarrow} \frac{dU}{\det(B)^{d-1}}.$$ Are these guesses correct? How can I formally find the Jacobians of the transformations $U = B^T SB$ and $U=B^T AB$ ?","I'm interested in integrals over matrix elements. For integrals over the elements of a symmetric matrix or an antisymmetric , how can I find the Jacobian of the transformations or , where B is some real invertible matrix? Below shares my work and my guesses for what the Jacobians should be. Here's a warmup that shows how I personally would find the Jacobian corresponding to a linear transformation of my matrix of interest. Consider an integral over the by dimensional matrix . That is, consider the integral is schematic for a function of all the matrix elements. Here, is schematic for . is some -dimensional integration region. If I were to consider a real, invertible transformation , one can quickly see by considering the columns of that That is, we have a Jacobian not of but of . One can check this makes sense by considering a constant times the identity matrix - then , and , which is exactly what one would get by scaling each of the elements of the matrix by . Thus, for , we have . Similarly, we can see that This can be seen quickly by performing the transformation in two steps and considering the columns and rows of . However, what if our integration was over a symmetric matrix ? That is, consider the integral Here, is schematic for . Here, is the integration region; counting the number of independent components of a symmetric matrix gives us that must be a -dimensional region. Note that the transformation preserves the symmetry of : is also symmetric. Then, by counting the number of independent components, , I'd guess that Similarly, for an antisymmetric matrix , we have that the transformation preserves the asymmetry; is also antisymmetric. Defining , and noting has independent components, I'd guess that Are these guesses correct? How can I formally find the Jacobians of the transformations and ?","S A S \rightarrow B^T S B A \rightarrow B^T A B d d X \int_{\Gamma} f(X) dX . f(X) dX \prod_{i,j}^d dX_{ij} \Gamma d^2 U=BX X \int_{\Gamma} f(X) dX = \int_{\Gamma} f(X) \prod_{j=1}^d \Big(\prod_{i=1}^d dX_{ij}\Big) = \int_{B\Gamma} f(B^{-1}X) \prod_{j=1}^d \frac{\prod_{i=1}^d dU_{ij}}{\det(B)} = \int_{B\Gamma} f(B^{-1}X)  \frac{\ dU}{\det(B)^d}. \det(B) \det(B)^d B c \det(B) = c^d \det(B)^d = c^{d^2} d^2 X c U=BX dX \rightarrow \frac{dU}{\det(B)^d} \text{for }U=B^TXB\text{, we have }dX \rightarrow \frac{dU}{\det(B)^{2d}}. X S \int_{\Delta} f(S)dS. dS \prod_{i\geq j}^d dS_{ij} \Delta \Delta \frac{1}{2}d(d+1) U=B^TSB S U \frac{1}{2}d(d+1) \text{for } B^TSB\text{, we have } dS \stackrel{?}{\rightarrow} \frac{dU}{\det(B)^{d+1}}. A U=B^T A B U dA = \prod_{i>j}^d dA_{ij} A \frac{1}{2}d(d-1) \text{for } B^TAB\text{, we have } dA \stackrel{?}{\rightarrow} \frac{dU}{\det(B)^{d-1}}. U = B^T SB U=B^T AB","['linear-algebra', 'multivariable-calculus', 'matrix-calculus', 'jacobian']"
37,Proving (or disproving) multivariable limit existence,Proving (or disproving) multivariable limit existence,,"I've come across a problem I think is impossible, but I could be wrong. It goes Let $$f(x, y) = \begin{cases}0,& \text{ if }y = 0\\ \ \\ y + x\sin\tfrac1y,&\ \text{ otherwise}\end{cases}$$ Show that the limits $\lim\limits_{(x y) \to (0,0)}$ and $\lim\limits_{ y \to 0}$ $\lim\limits_{ x \to 0}$ $f(x,y)$ exist, while $\lim\limits_{ x \to 0}$ $\lim\limits_{ y \to 0}$ $f(x,y)$ does not exist. In the original equation, if you let $y=0$ , then $f(x,0) = 0 + x\sin(1/0)$ , which isn't $0$ because it's undefined, right? So how can we even continue with the problem? And, if we can continue, how can the first limit $\lim\limits_{x y \to (0,0)}$ exist if $xy$ can only map to $0$ , not $(0,0)$ ?","I've come across a problem I think is impossible, but I could be wrong. It goes Let Show that the limits and exist, while does not exist. In the original equation, if you let , then , which isn't because it's undefined, right? So how can we even continue with the problem? And, if we can continue, how can the first limit exist if can only map to , not ?","f(x, y) = \begin{cases}0,& \text{ if }y = 0\\ \ \\ y + x\sin\tfrac1y,&\ \text{ otherwise}\end{cases} \lim\limits_{(x y) \to (0,0)} \lim\limits_{ y \to 0} \lim\limits_{ x \to 0} f(x,y) \lim\limits_{ x \to 0} \lim\limits_{ y \to 0} f(x,y) y=0 f(x,0) = 0 + x\sin(1/0) 0 \lim\limits_{x y \to (0,0)} xy 0 (0,0)","['calculus', 'multivariable-calculus']"
38,Finding this math YouTube channel,Finding this math YouTube channel,,"So last year, by chance I found a YouTube channel while I was looking for videos related to 3D geometry and multivariable calculus. The channel had very few subscribers and most video had views only in the hundreds. The channel had amazing animation and it had animated mascots. A guy wearing funky glasses appears in the beginning of every video and speaks in an animated old TV set. Can anybody help me find that channel please. Thank you. Edit: Apparently no mascots or funky glasses, it's just my memory.","So last year, by chance I found a YouTube channel while I was looking for videos related to 3D geometry and multivariable calculus. The channel had very few subscribers and most video had views only in the hundreds. The channel had amazing animation and it had animated mascots. A guy wearing funky glasses appears in the beginning of every video and speaks in an animated old TV set. Can anybody help me find that channel please. Thank you. Edit: Apparently no mascots or funky glasses, it's just my memory.",,"['multivariable-calculus', 'vectors', 'education', '3d']"
39,Division by $dx$ in multi-variable calculus ....,Division by  in multi-variable calculus ....,dx,"I am stuck on this doubt : Suppose $f=f(x,y,z).$ Hence, $ df= \frac {\partial f}{\partial x}dx + \frac { \partial f}{\partial y}dy + \frac {\partial f}{\partial z}dz.$ Then, is the following equation correct : $$\frac {df}{dx}=\frac {\partial f}{\partial x}+\frac {\partial f}{\partial y}\frac{dy}{dx} + \frac {\partial f}{\partial z}\frac{dz}{dx} \,\,\,\,(*)$$ The reasoning used in obtaining $(*)$ is : ""dividing"" the whole equation by $dx$ . Normally, the $\large \frac {d}{dx}$ operator is used in single variable calculus where only single-variable functions are differentiated wrt $x$ . But it does look a bit awkward (at least to me) when used in multi-variable calculus. Do the expressions $\large \frac {df}{dx}$ , $\large \frac {dy}{dx}$ and $\large \frac {dz}{dx}$ even make any sense when used like this ? I know that ""division"" by $\partial x$ can cause problems in multi-variable calculus. But what about ""division"" by $dx$ . It works fine in single-variable calculus. If $\large \frac {df}{dx}$ makes any sense, then does it mean the ""total"" rate of change of $f$ wrt $x$ if $y$ and $z$ are allowed to change ? Summary : (1) Is division by $dx$ allowed in multi-variable calculus? (2) What does $\frac {df(x,y,z)}{dx}$ mean if answer to (1) is ""yes"" ? Does it mean anything if the answer to $(1)$ is ""no"" ?","I am stuck on this doubt : Suppose Hence, Then, is the following equation correct : The reasoning used in obtaining is : ""dividing"" the whole equation by . Normally, the operator is used in single variable calculus where only single-variable functions are differentiated wrt . But it does look a bit awkward (at least to me) when used in multi-variable calculus. Do the expressions , and even make any sense when used like this ? I know that ""division"" by can cause problems in multi-variable calculus. But what about ""division"" by . It works fine in single-variable calculus. If makes any sense, then does it mean the ""total"" rate of change of wrt if and are allowed to change ? Summary : (1) Is division by allowed in multi-variable calculus? (2) What does mean if answer to (1) is ""yes"" ? Does it mean anything if the answer to is ""no"" ?","f=f(x,y,z).  df= \frac {\partial f}{\partial x}dx + \frac { \partial f}{\partial y}dy + \frac {\partial f}{\partial z}dz. \frac {df}{dx}=\frac {\partial f}{\partial x}+\frac {\partial f}{\partial y}\frac{dy}{dx} + \frac {\partial f}{\partial z}\frac{dz}{dx} \,\,\,\,(*) (*) dx \large \frac {d}{dx} x \large \frac {df}{dx} \large \frac {dy}{dx} \large \frac {dz}{dx} \partial x dx \large \frac {df}{dx} f x y z dx \frac {df(x,y,z)}{dx} (1)","['calculus', 'multivariable-calculus', 'derivatives', 'partial-derivative']"
40,Questions about different formulations of the Taylor expansion terms,Questions about different formulations of the Taylor expansion terms,,"I'm reading Numerical Optimization from Nocedal/Wright, and was playing around the matrix notation of the Taylor expansion, regarding which I would have two questions. We have a 3-times differentiable function $f\,:\,\mathbb{R}^n\rightarrow \mathbb{R}$ , for which the Taylor expansion around the point $a$ , up to the quadratic term, is the following: where $x,\,\,a,\,\,p\,\in\,\mathbb{R}^n$ , and $p$ is the distance between $a$ and $x$ . ( $x = a + p$ ) $$ f\left( x \right) =f\left( a+p \right) \approx f\left( a \right) +\nabla f\left( a \right) ^Tp+\frac{1}{2}p^T\nabla ^2f\left( a \right) p $$ My first question is: Can the quadratic term be expressed the following way? $$ \frac{1}{2}p^T\nabla ^2f\left( a \right) p\,\,\overset{?}{=}\,\,\frac{1}{2}\nabla \left( \nabla f\left( a \right) ^Tp \right) ^Tp $$ (I tried to prove it with mapping matrix indices with each other, but always got lost somewhere.) My second question stands only if the answer to the first is yes. Is it possible to write the third-order term in this manner? $$ \frac{1}{6}\nabla \left( p^T\nabla ^2f\left( a \right) p \right) ^Tp $$ I know that these forms would be of little practical use, I'm just asking out of curiosity. EDIT: I've made some calculations, and it worked out in the case I tried, but I still can't prove it: $$ f\left( a \right) =2a_{1}^{3}a_{2}^{4}+a_{1}^{2} $$ $$ a=\left[ \begin{array}{c} 	a_1\\ 	a_2\\ \end{array} \right] =\left[ \begin{array}{c} 	1\\ 	2\\ \end{array} \right] \,\,\,\,\,\,\,\,\,\,\,\,p=\left[ \begin{array}{c} 	p_1\\ 	p_2\\ \end{array} \right] =\left[ \begin{array}{c} 	2\\ 	3\\ \end{array} \right]  $$ $$ \nabla f\left( a \right) =\left[ \begin{array}{c} 	6a_{1}^{2}a_{2}^{4}+2a_1\\ 	8a_{1}^{3}a_{2}^{3}\\ \end{array} \right]  $$ $$ \nabla ^2f\left( a \right) =\left[ \begin{matrix} 	12a_1a_{2}^{4}+2&		24a_{1}^{2}a_{2}^{3}\\ 	24a_{1}^{2}a_{2}^{3}&		24a_{1}^{3}a_{2}^{2}\\ \end{matrix} \right]  $$ $$ p^T\nabla ^2f\left( a \right) p=\left[ \begin{matrix} 	2&		3\\ \end{matrix} \right] \left[ \begin{matrix} 	194&		192\\ 	192&		96\\ \end{matrix} \right] \left[ \begin{array}{c} 	2\\ 	3\\ \end{array} \right] =3944 $$ $$ \nabla \left( \nabla f\left( a \right) ^Tp \right) ^Tp=\nabla \left( \left[ \begin{matrix} 	6a_{1}^{2}a_{2}^{4}+2a_1&		8a_{1}^{3}a_{2}^{3}\\ \end{matrix} \right] \left[ \begin{array}{c} 	p_1\\ 	p_2\\ \end{array} \right] \right) ^Tp= \\ =\nabla \left( 6a_{1}^{2}a_{2}^{4}p_1+2a_1p_1+8a_{1}^{3}a_{2}^{3}p_2 \right) ^Tp= \\ =\left[ \begin{array}{c} 	12a_1a_{2}^{4}p_1+2p_1+24a_{1}^{2}a_{2}^{3}p_2\\ 	24a_{1}^{2}a_{2}^{3}p_1+24a_{1}^{3}a_{2}^{2}p_2\\ \end{array} \right] ^T\left[ \begin{array}{c} 	p_1\\ 	p_2\\ \end{array} \right] = \\ =\left[ \begin{matrix} 	964&		672\\ \end{matrix} \right] \left[ \begin{array}{c} 	2\\ 	3\\ \end{array} \right] =1928+2016=3944 $$","I'm reading Numerical Optimization from Nocedal/Wright, and was playing around the matrix notation of the Taylor expansion, regarding which I would have two questions. We have a 3-times differentiable function , for which the Taylor expansion around the point , up to the quadratic term, is the following: where , and is the distance between and . ( ) My first question is: Can the quadratic term be expressed the following way? (I tried to prove it with mapping matrix indices with each other, but always got lost somewhere.) My second question stands only if the answer to the first is yes. Is it possible to write the third-order term in this manner? I know that these forms would be of little practical use, I'm just asking out of curiosity. EDIT: I've made some calculations, and it worked out in the case I tried, but I still can't prove it:","f\,:\,\mathbb{R}^n\rightarrow \mathbb{R} a x,\,\,a,\,\,p\,\in\,\mathbb{R}^n p a x x = a + p 
f\left( x \right) =f\left( a+p \right) \approx f\left( a \right) +\nabla f\left( a \right) ^Tp+\frac{1}{2}p^T\nabla ^2f\left( a \right) p
 
\frac{1}{2}p^T\nabla ^2f\left( a \right) p\,\,\overset{?}{=}\,\,\frac{1}{2}\nabla \left( \nabla f\left( a \right) ^Tp \right) ^Tp
 
\frac{1}{6}\nabla \left( p^T\nabla ^2f\left( a \right) p \right) ^Tp
 
f\left( a \right) =2a_{1}^{3}a_{2}^{4}+a_{1}^{2}
 
a=\left[ \begin{array}{c}
	a_1\\
	a_2\\
\end{array} \right] =\left[ \begin{array}{c}
	1\\
	2\\
\end{array} \right] \,\,\,\,\,\,\,\,\,\,\,\,p=\left[ \begin{array}{c}
	p_1\\
	p_2\\
\end{array} \right] =\left[ \begin{array}{c}
	2\\
	3\\
\end{array} \right] 
 
\nabla f\left( a \right) =\left[ \begin{array}{c}
	6a_{1}^{2}a_{2}^{4}+2a_1\\
	8a_{1}^{3}a_{2}^{3}\\
\end{array} \right] 
 
\nabla ^2f\left( a \right) =\left[ \begin{matrix}
	12a_1a_{2}^{4}+2&		24a_{1}^{2}a_{2}^{3}\\
	24a_{1}^{2}a_{2}^{3}&		24a_{1}^{3}a_{2}^{2}\\
\end{matrix} \right] 
 
p^T\nabla ^2f\left( a \right) p=\left[ \begin{matrix}
	2&		3\\
\end{matrix} \right] \left[ \begin{matrix}
	194&		192\\
	192&		96\\
\end{matrix} \right] \left[ \begin{array}{c}
	2\\
	3\\
\end{array} \right] =3944
 
\nabla \left( \nabla f\left( a \right) ^Tp \right) ^Tp=\nabla \left( \left[ \begin{matrix}
	6a_{1}^{2}a_{2}^{4}+2a_1&		8a_{1}^{3}a_{2}^{3}\\
\end{matrix} \right] \left[ \begin{array}{c}
	p_1\\
	p_2\\
\end{array} \right] \right) ^Tp=
\\
=\nabla \left( 6a_{1}^{2}a_{2}^{4}p_1+2a_1p_1+8a_{1}^{3}a_{2}^{3}p_2 \right) ^Tp=
\\
=\left[ \begin{array}{c}
	12a_1a_{2}^{4}p_1+2p_1+24a_{1}^{2}a_{2}^{3}p_2\\
	24a_{1}^{2}a_{2}^{3}p_1+24a_{1}^{3}a_{2}^{2}p_2\\
\end{array} \right] ^T\left[ \begin{array}{c}
	p_1\\
	p_2\\
\end{array} \right] =
\\
=\left[ \begin{matrix}
	964&		672\\
\end{matrix} \right] \left[ \begin{array}{c}
	2\\
	3\\
\end{array} \right] =1928+2016=3944
","['linear-algebra', 'multivariable-calculus', 'optimization']"
41,Notation confusion about multivariable derivative,Notation confusion about multivariable derivative,,"First of all I'd like to say that English is not my native language but I hope I've translated most of the concepts correctly. So I'm going over my multivariable calculus textbook (also not in English, obviously) and most of the stuff I've understood. All the concepts are presented with functions of two variables. They've defined partial derivatives with the following notations: $$f^{'}_{x}(x_0, y_0) = \lim_{h\to0}\frac{f(x_0+h, y_0)-f(x_0, y_0)}{h}$$ $$f^{'}_{y}(x_0, y_0) = \lim_{h\to0}\frac{f(x_0, y_0+h)-f(x_0, y_0)}{h}$$ So far so good. Then for the differentiability in general, they defined it as follows: The function $f$ is differentiable at some point $\mathbf{x_0} = (x_0, y_0)$ if there is a linear mapping $L: \mathbb{R^2} \rightarrow \mathbb{R}$ such that $f(\mathbf{x_0} + \mathbf{h}) = f(\mathbf{x_0}) + L\mathbf{h} + o(\mathbf{h})$ when $\mathbf{h} \to \mathbf{0}$ . I don't want to get into the whole deal but essentially they write everything in coordinates, using $\mathbf{h} = (h, k)$ and the linear mapping then becomes $L(h, k) = ah + bk$ where we later prove that $a$ and $b$ are partial derivatives: $$a = f^{'}_{x}(x_0, y_0), \quad b = f^{'}_{y}(x_0, y_0) $$ And from there we got $L(h, k) = f^{'}_{x}(x_0, y_0)h + f^{'}_{y}(x_0, y_0)k $ Then comes the following paragraph which confuses me completely: The linear mapping $L$ for which we've seen is of form $L(h, k) = f^{'}_{x}(x_0, y_0)h + f^{'}_{y}(x_0, y_0)k $ is called the differential or the derivative of the function f at the point $(x_0, y_0)$ and we use the notation $df(x_0, y_0)$ . Here's the confusing sentence: If we mark linear mappings $(x, y) \to x$ and $(x, y) \to y$ as $dx$ and $dy$ respectively we get $df(x_0, y_0) = f^{'}_{x}(x_0, y_0)dx +  f^{'}_{y}(x_0, y_0)dy $ Where did that come from? Why do we even introduce those two mappings? Where did the $h$ and $k$ go? Thanks in advance.","First of all I'd like to say that English is not my native language but I hope I've translated most of the concepts correctly. So I'm going over my multivariable calculus textbook (also not in English, obviously) and most of the stuff I've understood. All the concepts are presented with functions of two variables. They've defined partial derivatives with the following notations: So far so good. Then for the differentiability in general, they defined it as follows: The function is differentiable at some point if there is a linear mapping such that when . I don't want to get into the whole deal but essentially they write everything in coordinates, using and the linear mapping then becomes where we later prove that and are partial derivatives: And from there we got Then comes the following paragraph which confuses me completely: The linear mapping for which we've seen is of form is called the differential or the derivative of the function f at the point and we use the notation . Here's the confusing sentence: If we mark linear mappings and as and respectively we get Where did that come from? Why do we even introduce those two mappings? Where did the and go? Thanks in advance.","f^{'}_{x}(x_0, y_0) = \lim_{h\to0}\frac{f(x_0+h, y_0)-f(x_0, y_0)}{h} f^{'}_{y}(x_0, y_0) = \lim_{h\to0}\frac{f(x_0, y_0+h)-f(x_0, y_0)}{h} f \mathbf{x_0} = (x_0, y_0) L: \mathbb{R^2} \rightarrow \mathbb{R} f(\mathbf{x_0} + \mathbf{h}) = f(\mathbf{x_0})
+ L\mathbf{h} + o(\mathbf{h}) \mathbf{h} \to \mathbf{0} \mathbf{h} = (h, k) L(h, k) = ah + bk a b a = f^{'}_{x}(x_0, y_0), \quad b = f^{'}_{y}(x_0, y_0)  L(h, k) = f^{'}_{x}(x_0, y_0)h + f^{'}_{y}(x_0, y_0)k  L L(h, k) = f^{'}_{x}(x_0, y_0)h + f^{'}_{y}(x_0, y_0)k  (x_0, y_0) df(x_0, y_0) (x, y) \to x (x, y) \to y dx dy df(x_0, y_0) = f^{'}_{x}(x_0, y_0)dx +
 f^{'}_{y}(x_0, y_0)dy  h k","['multivariable-calculus', 'derivatives']"
42,"Checking if $f: \Phi_1 \to \Phi_2 \; ; f_1(\theta, v) \mapsto f_2(\theta, \sinh v)$ is an isometry.",Checking if  is an isometry.,"f: \Phi_1 \to \Phi_2 \; ; f_1(\theta, v) \mapsto f_2(\theta, \sinh v)","Exercise : Consider the surfaces : $$\Phi_1 : f_1(\theta, v) = \left( \cos \theta \cosh v, \sin \theta \cosh v, v\right), \; (\theta, v) \in (0,2 \pi) \times \mathbb R$$ $$\Phi_2 : f_2(\phi, u) = \left( u\cos \phi , u\sin \phi, \phi\right), \; (\theta, v) \in (0,2 \pi) \times \mathbb R$$ Check if the following mapping is an isometry between them : $$f: \Phi_1 \to \Phi_2 \;; f_1(\theta, v) \mapsto f_2(\theta, \sinh v)$$ Thoughts-Question : To start off, this is a Differential Geometry related question which I am not that experienced, thus if it feels trivial, excuse me. From my continuous experience, interest and studying of a whole differnt subject (Functional Analysis - Operator Theory), I know very well that a Linear Isometry is essentialy achieved if $\|Av\|_Y = \|v\|_X$ where $A:X \to Y$ is a linear operator. This means that they are distance preserving. It is a global isometry if it also is surjective. Now, a similar correspondance can be found in Differential Geometry. Specifically, if we have $2$ surfaces, $\Phi_1$ and $\Phi_2$ , then the function $f: \Phi_1 \to \Phi_2$ is an isometry if and only if $f:\Phi_1 \to \Phi_2$ is a differentiable mapping which is an inective and surjective local isometry. Now, I am having a hard time proving the following statements. First of all, I start by constructing my function as stated by the exercise body : $$f(f_1(\theta,v)) = f_2(\theta, \sinh v)$$ $$\implies$$ $$f(\cos\theta\cosh v, \sin \theta\cosh v, v) = (\sinh v \cos \theta, \sinh v\sin \theta, \theta)$$ So, checking the statements needed, first of all, that $f$ is differentiable. Now, how does one show that this $f$ is injective and surjective ? Also, what about the local isometry ? I know that we can check if it is a local isometry or not, since the fundamental quantities of the fundamental form must oblige the following relations : $$E_p = E_{f(p)}, \; F_p = F_{f(p)}, \; G_p = G_{f(p)}$$ I am kind of confused on the calculations of the fundamental quantities though. In a solved (but poorly elaborated) example I've seen, one must first calculate the inverse of $f$ and then correlate the argument of $f$ with what it's mapped to. I would really appreciate any thorough elaboration which can help me how to handle showing the injectivity, surjectivity but most importantly on how to find the fundamental quantities stated.","Exercise : Consider the surfaces : Check if the following mapping is an isometry between them : Thoughts-Question : To start off, this is a Differential Geometry related question which I am not that experienced, thus if it feels trivial, excuse me. From my continuous experience, interest and studying of a whole differnt subject (Functional Analysis - Operator Theory), I know very well that a Linear Isometry is essentialy achieved if where is a linear operator. This means that they are distance preserving. It is a global isometry if it also is surjective. Now, a similar correspondance can be found in Differential Geometry. Specifically, if we have surfaces, and , then the function is an isometry if and only if is a differentiable mapping which is an inective and surjective local isometry. Now, I am having a hard time proving the following statements. First of all, I start by constructing my function as stated by the exercise body : So, checking the statements needed, first of all, that is differentiable. Now, how does one show that this is injective and surjective ? Also, what about the local isometry ? I know that we can check if it is a local isometry or not, since the fundamental quantities of the fundamental form must oblige the following relations : I am kind of confused on the calculations of the fundamental quantities though. In a solved (but poorly elaborated) example I've seen, one must first calculate the inverse of and then correlate the argument of with what it's mapped to. I would really appreciate any thorough elaboration which can help me how to handle showing the injectivity, surjectivity but most importantly on how to find the fundamental quantities stated.","\Phi_1 : f_1(\theta, v) = \left( \cos \theta \cosh v, \sin \theta \cosh v, v\right), \; (\theta, v) \in (0,2 \pi) \times \mathbb R \Phi_2 : f_2(\phi, u) = \left( u\cos \phi , u\sin \phi, \phi\right), \; (\theta, v) \in (0,2 \pi) \times \mathbb R f: \Phi_1 \to \Phi_2 \;; f_1(\theta, v) \mapsto f_2(\theta, \sinh v) \|Av\|_Y = \|v\|_X A:X \to Y 2 \Phi_1 \Phi_2 f: \Phi_1 \to \Phi_2 f:\Phi_1 \to \Phi_2 f(f_1(\theta,v)) = f_2(\theta, \sinh v) \implies f(\cos\theta\cosh v, \sin \theta\cosh v, v) = (\sinh v \cos \theta, \sinh v\sin \theta, \theta) f f E_p = E_{f(p)}, \; F_p = F_{f(p)}, \; G_p = G_{f(p)} f f","['multivariable-calculus', 'differential-geometry', 'surfaces', 'conformal-geometry', 'isometry']"
43,Showing that $f:S_1 \to S_2$ is a conformal mapping.,Showing that  is a conformal mapping.,f:S_1 \to S_2,"Exercise : Given the surfaces $S_1 = \{ x \in \mathbb R^3 : \|x\| = 1\}$ and $S_2 = \{x \in \mathbb R^3 : \|x\| = r\}$ where $r >0$ , check if the mapping $f:S_1 \to S_2$ with the formula $f(x,y,z) = (rx,ry,-rz)$ is conformal. Thoughts : According to my Differential Geometry notes, I have the following equivalent definitions : 1 : The topical diffeomorphism $f : S_1 \to S_2$ is conformal, if and only if $f_p^* = \lambda(p)\langle \cdot, \cdot\rangle_p, \; \forall p$ where $\lambda : S_1 \to \mathbb R$ is a function. 2 : The topical diffeomorphism $f:S_1 \to S_2$ is conformal, if and only if $\forall r$ patches around $p$ , $f \circ r$ is also a patch around $f(p)$ with : $$E_p = \lambda(p)E_{f(p)}, \; F_p = \lambda(p)E_{f(p)}, \; G_p = \lambda(p)G_{f(p)}$$ I assume that the second equivalent definition is or more use, as we have an explicit formula for $f$ that can help finding the fundamental form. But, I am kind of stuck in decoding it. What is $E_p$ and what $E_{f(p)}$ in that case ? Also, I first need to show that $f$ is a Diffeomorphism, which I found to have many equivalent remarks here . Any explanation or thorough elaboration will be much appreciated as I am still a beginner in differential geometry regarding my experience on the subject.","Exercise : Given the surfaces and where , check if the mapping with the formula is conformal. Thoughts : According to my Differential Geometry notes, I have the following equivalent definitions : 1 : The topical diffeomorphism is conformal, if and only if where is a function. 2 : The topical diffeomorphism is conformal, if and only if patches around , is also a patch around with : I assume that the second equivalent definition is or more use, as we have an explicit formula for that can help finding the fundamental form. But, I am kind of stuck in decoding it. What is and what in that case ? Also, I first need to show that is a Diffeomorphism, which I found to have many equivalent remarks here . Any explanation or thorough elaboration will be much appreciated as I am still a beginner in differential geometry regarding my experience on the subject.","S_1 = \{ x \in \mathbb R^3 : \|x\| = 1\} S_2 = \{x \in \mathbb R^3 : \|x\| = r\} r >0 f:S_1 \to S_2 f(x,y,z) = (rx,ry,-rz) f : S_1 \to S_2 f_p^* = \lambda(p)\langle \cdot, \cdot\rangle_p, \; \forall p \lambda : S_1 \to \mathbb R f:S_1 \to S_2 \forall r p f \circ r f(p) E_p = \lambda(p)E_{f(p)}, \; F_p = \lambda(p)E_{f(p)}, \; G_p = \lambda(p)G_{f(p)} f E_p E_{f(p)} f","['multivariable-calculus', 'differential-geometry', 'conformal-geometry', 'diffeomorphism']"
44,Can we plug in values before evaluating a partial derivative?,Can we plug in values before evaluating a partial derivative?,,"If $f(x,y)$ is a function, to compute $\partial f / \partial y$ at some point $(a,b)$ , is it always acceptable to plug in the value $a$ for $x$ before computing the derivative?  I am convinced the answer is ""yes"", and that no justification needs to be given: literally the definition of the partial derivative at a point $(a,b)$ involves doing this.  Is this correct (including the justification)? The source of my thinking about this question is Problem 2-19 in Spivak's Calculus on Manifolds , which asks us to compute $\partial f / \partial y$ at a point of the form $(1,y)$ , where $f$ is some more complicated version of $$f(x,y) = x^{x^y}.$$ Clearly the trick is to plug in $x = 1$ first, and I am convinced no justification for this step needs to be given, but something about it makes me nervous. Thank you!","If is a function, to compute at some point , is it always acceptable to plug in the value for before computing the derivative?  I am convinced the answer is ""yes"", and that no justification needs to be given: literally the definition of the partial derivative at a point involves doing this.  Is this correct (including the justification)? The source of my thinking about this question is Problem 2-19 in Spivak's Calculus on Manifolds , which asks us to compute at a point of the form , where is some more complicated version of Clearly the trick is to plug in first, and I am convinced no justification for this step needs to be given, but something about it makes me nervous. Thank you!","f(x,y) \partial f / \partial y (a,b) a x (a,b) \partial f / \partial y (1,y) f f(x,y) = x^{x^y}. x = 1",['multivariable-calculus']
45,How does the vector triple product BAC-CAB Identity come about?,How does the vector triple product BAC-CAB Identity come about?,,"As in the title, I was studying the proof (from Vector Analysis - Louis Brand) of this identity, however I do not completely understand all of the steps. $$a \times (b \times c) = b(a \cdot c) - c(a \cdot b)$$ I also visited an excellent related post - do the BAC-CAB identity for vector triple product have some interpretation? , but I am still stuck. I reproduce the proof in the book here. I don't really understand, how did the author deduce $\alpha=-\lambda(\textbf{v}\cdot\textbf{w}),\beta=\lambda(\textbf{u}\cdot\textbf{w})$ and the basis steps. Any inputs, suggestions or tips to understand the proof would be incredibly helpful! Proof. The vector $(\textbf{u} \times \textbf{v}) \times \textbf{w}$ is perpendicular to both $\textbf{u}\times\textbf{v}$ and therefore coplanar with $\textbf{u}$ and $\textbf{v}$ . $$(\textbf{u}\times\textbf{v})\times\textbf{w}=\alpha\textbf{u}+\beta\textbf{v}$$ But, since $(\textbf{u}\times\textbf{v})\times\textbf{w}$ is also perpendicular to $\textbf{w}$ , $$(\alpha\textbf{u}+\beta\textbf{v})\cdot\textbf{w}=0$$ All numbers $\alpha,\beta$ that satisfy this equation must be of the form $\alpha=\lambda(\textbf{v}\cdot\textbf{w}),\beta=\lambda(\textbf{u}\cdot\textbf{w})$ , where $\lambda$ is arbitrary. Thus, we have $$\textbf{u}\times(\textbf{v}\times\textbf{w})=\lambda\{(\textbf{u}\cdot\textbf{w})v-(\textbf{v}\cdot\textbf{w})\textbf{u}\}$$ In order to determine $\lambda$ , we use a special basis in which $\hat{i}$ is collinear with $\textbf{u}$ , $\hat{j}$ is co-planar with $\textbf{u, v}$ ; then $$\textbf{u}=u_{1}i,\textbf{v}=v_{1}i+v_{2}j,\textbf{w}=w_{1}i+w_{2}j+w_{3}k$$ On substituting these values, we obtain, after a simple calculation, $\lambda=1$ . We therefore have important expansion formulas, $$(\textbf{u}\times\textbf{v})\times\textbf{w}=(\textbf{u}\cdot\textbf{w})\textbf{v}-(\textbf{v}\cdot\textbf{w})\textbf{u}$$ $$\textbf{w}\times(\textbf{u}\times\textbf{v})=(\textbf{w}\cdot\textbf{v})\textbf{u}-(\textbf{w}\cdot\textbf{u})\textbf{v}$$","As in the title, I was studying the proof (from Vector Analysis - Louis Brand) of this identity, however I do not completely understand all of the steps. I also visited an excellent related post - do the BAC-CAB identity for vector triple product have some interpretation? , but I am still stuck. I reproduce the proof in the book here. I don't really understand, how did the author deduce and the basis steps. Any inputs, suggestions or tips to understand the proof would be incredibly helpful! Proof. The vector is perpendicular to both and therefore coplanar with and . But, since is also perpendicular to , All numbers that satisfy this equation must be of the form , where is arbitrary. Thus, we have In order to determine , we use a special basis in which is collinear with , is co-planar with ; then On substituting these values, we obtain, after a simple calculation, . We therefore have important expansion formulas,","a \times (b \times c) = b(a \cdot c) - c(a \cdot b) \alpha=-\lambda(\textbf{v}\cdot\textbf{w}),\beta=\lambda(\textbf{u}\cdot\textbf{w}) (\textbf{u} \times \textbf{v}) \times \textbf{w} \textbf{u}\times\textbf{v} \textbf{u} \textbf{v} (\textbf{u}\times\textbf{v})\times\textbf{w}=\alpha\textbf{u}+\beta\textbf{v} (\textbf{u}\times\textbf{v})\times\textbf{w} \textbf{w} (\alpha\textbf{u}+\beta\textbf{v})\cdot\textbf{w}=0 \alpha,\beta \alpha=\lambda(\textbf{v}\cdot\textbf{w}),\beta=\lambda(\textbf{u}\cdot\textbf{w}) \lambda \textbf{u}\times(\textbf{v}\times\textbf{w})=\lambda\{(\textbf{u}\cdot\textbf{w})v-(\textbf{v}\cdot\textbf{w})\textbf{u}\} \lambda \hat{i} \textbf{u} \hat{j} \textbf{u, v} \textbf{u}=u_{1}i,\textbf{v}=v_{1}i+v_{2}j,\textbf{w}=w_{1}i+w_{2}j+w_{3}k \lambda=1 (\textbf{u}\times\textbf{v})\times\textbf{w}=(\textbf{u}\cdot\textbf{w})\textbf{v}-(\textbf{v}\cdot\textbf{w})\textbf{u} \textbf{w}\times(\textbf{u}\times\textbf{v})=(\textbf{w}\cdot\textbf{v})\textbf{u}-(\textbf{w}\cdot\textbf{u})\textbf{v}","['multivariable-calculus', 'proof-explanation', 'vector-analysis', 'cross-product']"
46,Derivative of a multivariable function (arises in mathematical statistics),Derivative of a multivariable function (arises in mathematical statistics),,"Suppose that $f(\theta_1,\dots,\theta_n,x)$ is a (positive) probability density function over a finite set $\mathbb{X}$ defined for an open subset $\Theta\in\mathbb{R}^n$ . That is, for every $\theta\in\Theta$ and $x\in\mathbb{X}$ , $f(\theta_1,\dots,\theta_n;x)>0$ and $$ \sum_x f(\theta_1,\dots,\theta_n;x) = 1. $$ Let \begin{equation} y_{i,j}(\theta) = \sum_x f(\theta_1,\dots,\theta_n;;x)\frac{\partial}{\partial\theta_i}\log f(\theta_1,\dots,\theta_n;x) \frac{\partial}{\partial\theta_j}\log f(\theta_1,\dots,\theta_n;x). \end{equation} (This is called Fisher information in statistics) Let $$\widehat{\eta_i}(x) := g_i(\theta_1,\dots,\theta_n;x)$$ and $$\eta_i := \sum_x f(\theta_1,\dots,\theta_n;x) g_i(\theta_1,\dots,\theta_n;x),$$ for some ""nice enough"" functions $g_i$ . Hence $\eta$ is a function of $\theta$ . It can also be shown that $$ \frac{\partial}{\partial\theta_i}\log f(\theta_1,\dots,\theta_n;x) = \widehat{\eta_i}(x) - \eta_i. $$ By a calculation I am getting $$ \frac{\partial\eta_i}{\partial\theta_j} = y_{i,j}(\theta) + k \cdot \eta_i\eta_j, $$ for some real constant $k$ . However, what I would like to have is $$ \frac{\partial\eta_i}{\partial\theta_j} = y_{i,j}(\theta). $$ So is it possible to modify the $g_i$ 's (by scaling/shifting the existing $g_i$ ) appropriately so that we get the above equation? I tried a bit, couldn't succeed. Any help is greatly appreciated.","Suppose that is a (positive) probability density function over a finite set defined for an open subset . That is, for every and , and Let (This is called Fisher information in statistics) Let and for some ""nice enough"" functions . Hence is a function of . It can also be shown that By a calculation I am getting for some real constant . However, what I would like to have is So is it possible to modify the 's (by scaling/shifting the existing ) appropriately so that we get the above equation? I tried a bit, couldn't succeed. Any help is greatly appreciated.","f(\theta_1,\dots,\theta_n,x) \mathbb{X} \Theta\in\mathbb{R}^n \theta\in\Theta x\in\mathbb{X} f(\theta_1,\dots,\theta_n;x)>0 
\sum_x f(\theta_1,\dots,\theta_n;x) = 1.
 \begin{equation}
y_{i,j}(\theta) = \sum_x f(\theta_1,\dots,\theta_n;;x)\frac{\partial}{\partial\theta_i}\log f(\theta_1,\dots,\theta_n;x) \frac{\partial}{\partial\theta_j}\log f(\theta_1,\dots,\theta_n;x).
\end{equation} \widehat{\eta_i}(x) := g_i(\theta_1,\dots,\theta_n;x) \eta_i := \sum_x f(\theta_1,\dots,\theta_n;x) g_i(\theta_1,\dots,\theta_n;x), g_i \eta \theta 
\frac{\partial}{\partial\theta_i}\log f(\theta_1,\dots,\theta_n;x) = \widehat{\eta_i}(x) - \eta_i.
 
\frac{\partial\eta_i}{\partial\theta_j} = y_{i,j}(\theta) + k \cdot \eta_i\eta_j,
 k 
\frac{\partial\eta_i}{\partial\theta_j} = y_{i,j}(\theta).
 g_i g_i","['calculus', 'multivariable-calculus']"
47,Partial Derivative with Respect to Multiple Variables,Partial Derivative with Respect to Multiple Variables,,"If we take a multivariable function such as $w=f(x,y,z)=x^2+y^2+z^2$ , I understand that we can take its partial derivative with respect to any one of its arguments, while the others stay unchanged. In this case, we can take the partial derivative with respect to $z$ as $\frac{\partial}{\partial z}f(x,y,z)$ . I also understand that we can take its total derivative which is with respect to all of its arguments, which can be expressed as $\frac{dt}{dw}f(x,y,z)$ . My question: how do we represent the derivative with respect to some but not all of a multivariable function's variables? How is this derivative classified? I am tempted to think it is partial but I am not sure since all of the definitions I have seen give it with respect to a single variable. Is it a ""partial"" total derivative?","If we take a multivariable function such as , I understand that we can take its partial derivative with respect to any one of its arguments, while the others stay unchanged. In this case, we can take the partial derivative with respect to as . I also understand that we can take its total derivative which is with respect to all of its arguments, which can be expressed as . My question: how do we represent the derivative with respect to some but not all of a multivariable function's variables? How is this derivative classified? I am tempted to think it is partial but I am not sure since all of the definitions I have seen give it with respect to a single variable. Is it a ""partial"" total derivative?","w=f(x,y,z)=x^2+y^2+z^2 z \frac{\partial}{\partial z}f(x,y,z) \frac{dt}{dw}f(x,y,z)","['multivariable-calculus', 'partial-derivative']"
48,Need help computing the double integral $\int_{0}^{\infty} \int_{0}^{\infty} \frac{f(x + y)}{x + y} \mathop{dy} \mathop{dx}$,Need help computing the double integral,\int_{0}^{\infty} \int_{0}^{\infty} \frac{f(x + y)}{x + y} \mathop{dy} \mathop{dx},"Need help computing the double integral $\int_{0}^{\infty} \int_{0}^{\infty} \frac{f(x + y)}{x + y} \mathop{dy} \mathop{dx}.$ I know that $\int_{0}^{\infty} f(u) \mathop{du}$ equals $1$ . The entire integral should come out to be $1$ . I am new to multivariable calculus and would appreciate some help. Making the substitution $u = x + y$ and $v = y$ , we get $\mathop{du} = \mathop{dx} + \mathop{dy}$ and $\mathop{dv} = \mathop{dy}$ . So, $$\int_{0}^{\infty} \int_{0}^{\infty} \frac{f(u)}{u} \mathop{dy} \mathop{dx}$$ I don't know what to change the $dy$ and $dx$ to since $du$ has both $dx$ and $dy$ in it. Also, for the Jacobian, I know that it will equal $1$ , but I don't know what I'm taking partial derivatives of in the determinant. Can someone please help me with this integral?","Need help computing the double integral I know that equals . The entire integral should come out to be . I am new to multivariable calculus and would appreciate some help. Making the substitution and , we get and . So, I don't know what to change the and to since has both and in it. Also, for the Jacobian, I know that it will equal , but I don't know what I'm taking partial derivatives of in the determinant. Can someone please help me with this integral?",\int_{0}^{\infty} \int_{0}^{\infty} \frac{f(x + y)}{x + y} \mathop{dy} \mathop{dx}. \int_{0}^{\infty} f(u) \mathop{du} 1 1 u = x + y v = y \mathop{du} = \mathop{dx} + \mathop{dy} \mathop{dv} = \mathop{dy} \int_{0}^{\infty} \int_{0}^{\infty} \frac{f(u)}{u} \mathop{dy} \mathop{dx} dy dx du dx dy 1,"['integration', 'multivariable-calculus']"
49,Finding Christoffel Symbols using via variational method.,Finding Christoffel Symbols using via variational method.,,"I'm trying to find the Christoffel Symbols for the Lorentz metric $${\rm d}s^2 = \cos(2\pi x)({\rm d}x^2-{\rm d}y^2) - 2\sin(2\pi x)\,{\rm d}x\,{\rm d}y$$ by looking at the Euler-Lagrange equations for $$L(x,\dot{x},y,\dot{y}) = \cos(2\pi x)(\dot{x}^2-\dot{y}^2) - 2\sin(2\pi x)\,\dot{x}\,\dot{y}.$$ I have already done my fair share of computations like this, but I must be making some algebraic mistake that I cannot find for the life of me. If we write $$\begin{align}\frac{\partial L}{\partial x} - \frac{{\rm d}}{{\rm d}t}&\left(\frac{\partial L}{\partial \dot{x}}\right) = -2\pi\sin(2\pi x)(\dot{x}^2-\dot{y}^2)-4\pi\cos(2\pi x)\dot{x}\dot{y} \\ &\qquad - \frac{{\rm d}}{{\rm d}t}\left(2\dot{x}\cos(2\pi x) - 2\dot{y}\sin(2\pi x)\right),\end{align}  $$ and we will have a term with $\ddot{y}$ . This is a problem, since as far as I understand the geodesic equation corresponding to the $x$ -coordinate should have the form $$\ddot{x} + \Gamma(\dot{x},\dot{y})=0,$$ maybe after dividing by something. What is going on?","I'm trying to find the Christoffel Symbols for the Lorentz metric by looking at the Euler-Lagrange equations for I have already done my fair share of computations like this, but I must be making some algebraic mistake that I cannot find for the life of me. If we write and we will have a term with . This is a problem, since as far as I understand the geodesic equation corresponding to the -coordinate should have the form maybe after dividing by something. What is going on?","{\rm d}s^2 = \cos(2\pi x)({\rm d}x^2-{\rm d}y^2) - 2\sin(2\pi x)\,{\rm d}x\,{\rm d}y L(x,\dot{x},y,\dot{y}) = \cos(2\pi x)(\dot{x}^2-\dot{y}^2) - 2\sin(2\pi x)\,\dot{x}\,\dot{y}. \begin{align}\frac{\partial L}{\partial x} - \frac{{\rm d}}{{\rm d}t}&\left(\frac{\partial L}{\partial \dot{x}}\right) = -2\pi\sin(2\pi x)(\dot{x}^2-\dot{y}^2)-4\pi\cos(2\pi x)\dot{x}\dot{y} \\ &\qquad - \frac{{\rm d}}{{\rm d}t}\left(2\dot{x}\cos(2\pi x) - 2\dot{y}\sin(2\pi x)\right),\end{align}   \ddot{y} x \ddot{x} + \Gamma(\dot{x},\dot{y})=0,","['multivariable-calculus', 'differential-geometry', 'riemannian-geometry', 'calculus-of-variations']"
50,Is the Euclidean Distance and the Euclidean Norm the same thing?,Is the Euclidean Distance and the Euclidean Norm the same thing?,,"I understand that a norm is defined as: $$||\space.||:V\to \mathbb{R}$$ which is a notion of distance defined on a vector space to give the magnitude of a vector (distance from the origin). Is it true that this distance can be any type of distance you want to define it to be? For example, taking the norm to be the Manhattan (Taxi Metric) distance, where $$||x||=\sum_{i=1}^{n}{|x_{i}|^{2}} \text{ where }x\in\mathbb{R}^{n}$$ or you could simply define the norm to be the absolute value of the first component of $x$ . I also understand that distance is a function $$d:V\times V\to \mathbb{R}$$ which measures between two points. So as the Euclidean distance is defined to be: $$dist(x,y)=\bigg(\sum_{i=1}^{n}{(x_{i}-y_{i})^2}\bigg)^{\frac{1}{2}}$$ and the Euclidean Norm is defined to be $$|x|=\bigg(\sum_{i=1}^{n}{x_{i}^2}\bigg)^{\frac{1}{2}}$$ then the norm is simply the Euclidean distance between a vector $x$ and $0$ so can you essentially think of them as the same thing? Is it true to say that a norm is a type of distance? Or would it be the other way around to say that the distance is a type of norm? Thanks!","I understand that a norm is defined as: which is a notion of distance defined on a vector space to give the magnitude of a vector (distance from the origin). Is it true that this distance can be any type of distance you want to define it to be? For example, taking the norm to be the Manhattan (Taxi Metric) distance, where or you could simply define the norm to be the absolute value of the first component of . I also understand that distance is a function which measures between two points. So as the Euclidean distance is defined to be: and the Euclidean Norm is defined to be then the norm is simply the Euclidean distance between a vector and so can you essentially think of them as the same thing? Is it true to say that a norm is a type of distance? Or would it be the other way around to say that the distance is a type of norm? Thanks!","||\space.||:V\to \mathbb{R} ||x||=\sum_{i=1}^{n}{|x_{i}|^{2}} \text{ where }x\in\mathbb{R}^{n} x d:V\times V\to \mathbb{R} dist(x,y)=\bigg(\sum_{i=1}^{n}{(x_{i}-y_{i})^2}\bigg)^{\frac{1}{2}} |x|=\bigg(\sum_{i=1}^{n}{x_{i}^2}\bigg)^{\frac{1}{2}} x 0","['multivariable-calculus', 'functions', 'euclidean-geometry', 'normed-spaces', 'vector-analysis']"
51,Munkres-Analysis on Manifolds: Theorem 20.1,Munkres-Analysis on Manifolds: Theorem 20.1,,"I am studying Analysis on Manifolds by Munkres. I have a problem with a proof in section 20: It states that: Let $A$ be an $n$ by $n$ matrix. Let $h:\mathbb{R}^n\to \mathbb{R}^n$ be the linear transformation $h(x)=A x$. Let $S$ be a rectifiable set (the boundary of $S=BdS$ has measure $0$) in $\mathbb{R}^n$. Then $v(h(S))=|\det A|v(S)$ ($v=$volume). The author starts his proof by considering tha case of $A$ being a non-singular matrix (invertible). I think I understand his steps in that case (I basically had to prove that $h(int S)=int$ $h(S)$ and $h(S)$ is rectifiable, if anybody knows a way this statements are proven autumatically please tell me). He proceeds by considering the case where $A$ is singular, so $\det A=0$. He tries to show now that $v(T)=0$. He states that since $S$ is bounded so is $h(S)$ (I think thats true because $|h(x)-h(a)|\leq n|A||x-a|$ for each $x$ in $S$ and fixed a in $S$, if there is again a better explanation please tell me). Then he says that $h(\mathbb{R}^n)=V$ with $\dim V=p<n$ and that $V$ has measure $0$ (for each $ε>0$ it can be covered by countably many open rectangles of total volume less than $ε$), a statemant that I have no clue how to prove. Then he says that the closure of $h(S)=cl(h(S))$ is closed and bounded and has neasure $0$ (of course $cl((h(S))$ is closed but why is it bounded with measure $0$?). Then makes an addition step (which I understand) and proves the theorem for that case too. Cound someone help me clarify the points of the proof that I don't understand? Thank you in advance!","I am studying Analysis on Manifolds by Munkres. I have a problem with a proof in section 20: It states that: Let $A$ be an $n$ by $n$ matrix. Let $h:\mathbb{R}^n\to \mathbb{R}^n$ be the linear transformation $h(x)=A x$. Let $S$ be a rectifiable set (the boundary of $S=BdS$ has measure $0$) in $\mathbb{R}^n$. Then $v(h(S))=|\det A|v(S)$ ($v=$volume). The author starts his proof by considering tha case of $A$ being a non-singular matrix (invertible). I think I understand his steps in that case (I basically had to prove that $h(int S)=int$ $h(S)$ and $h(S)$ is rectifiable, if anybody knows a way this statements are proven autumatically please tell me). He proceeds by considering the case where $A$ is singular, so $\det A=0$. He tries to show now that $v(T)=0$. He states that since $S$ is bounded so is $h(S)$ (I think thats true because $|h(x)-h(a)|\leq n|A||x-a|$ for each $x$ in $S$ and fixed a in $S$, if there is again a better explanation please tell me). Then he says that $h(\mathbb{R}^n)=V$ with $\dim V=p<n$ and that $V$ has measure $0$ (for each $ε>0$ it can be covered by countably many open rectangles of total volume less than $ε$), a statemant that I have no clue how to prove. Then he says that the closure of $h(S)=cl(h(S))$ is closed and bounded and has neasure $0$ (of course $cl((h(S))$ is closed but why is it bounded with measure $0$?). Then makes an addition step (which I understand) and proves the theorem for that case too. Cound someone help me clarify the points of the proof that I don't understand? Thank you in advance!",,"['linear-algebra', 'multivariable-calculus', 'linear-transformations', 'determinant', 'lebesgue-measure']"
52,Fubini's theorem and the Gaussian Integral,Fubini's theorem and the Gaussian Integral,,"I have been reading K. Conrad's very useful monograph on the Gaussian Integral ( http://www.math.uconn.edu/~kconrad/blurbs/analysis/gaussianintegral.pdf ) but I have a couple of questions which I am having trouble fully justifying to myself: Let $J=\int^\infty_0 e^{-x^2}dx$ where obviously $2J=\int^\infty_{-\infty} e^{-x^2}dx$ is the more traditional Gaussian integral. In Solution 1, Conrad solves $J$ using polar coordinates, while in Solution 2, he uses the substitution $x=yt$ in the double integral $J^2=\int^\infty_0 e^{-x^2}dx\int^\infty_0 e^{-y^2}dy=\int^\infty_0(\int^\infty_0 e^{-(x^2+y^2)}dx)dy$. I don't really have a problem with either solution (except for one point which I will outline below), except that he says his Solution 2 uses only single variable calculus, while Solution 1 uses multivariable calculus. Perhaps it is my own ignorance showing, but why does Solution 2 not use multivariable calculus? Is it because $y$ is a dummy variable? My other issue comes from the implicit use of Fubini's theorem. Above, where I have written $J^2=...$ I presume it is Fubini that allows me to change the order of integration. However, how does this work when we are working with improper integrals? Further, what would I need to check to make sure Fubini holds? It is easy to show $J$ exists (as a limit), and $e^{-x^2}$ is everywhere non-negative, but is that enough?","I have been reading K. Conrad's very useful monograph on the Gaussian Integral ( http://www.math.uconn.edu/~kconrad/blurbs/analysis/gaussianintegral.pdf ) but I have a couple of questions which I am having trouble fully justifying to myself: Let $J=\int^\infty_0 e^{-x^2}dx$ where obviously $2J=\int^\infty_{-\infty} e^{-x^2}dx$ is the more traditional Gaussian integral. In Solution 1, Conrad solves $J$ using polar coordinates, while in Solution 2, he uses the substitution $x=yt$ in the double integral $J^2=\int^\infty_0 e^{-x^2}dx\int^\infty_0 e^{-y^2}dy=\int^\infty_0(\int^\infty_0 e^{-(x^2+y^2)}dx)dy$. I don't really have a problem with either solution (except for one point which I will outline below), except that he says his Solution 2 uses only single variable calculus, while Solution 1 uses multivariable calculus. Perhaps it is my own ignorance showing, but why does Solution 2 not use multivariable calculus? Is it because $y$ is a dummy variable? My other issue comes from the implicit use of Fubini's theorem. Above, where I have written $J^2=...$ I presume it is Fubini that allows me to change the order of integration. However, how does this work when we are working with improper integrals? Further, what would I need to check to make sure Fubini holds? It is easy to show $J$ exists (as a limit), and $e^{-x^2}$ is everywhere non-negative, but is that enough?",,"['calculus', 'real-analysis', 'integration', 'multivariable-calculus', 'improper-integrals']"
53,What topics I need to cover in order to understand Black Scholes Option Pricing Model?,What topics I need to cover in order to understand Black Scholes Option Pricing Model?,,"I have been searching the web on the prerequisites of understanding Black Scholes Option Pricing Model. And I have drawn the following diagram to summarize the relationship between different topics: However, I do not know how far I should go in each topic. Linear Algebra, real analysis, to my understanding, involve lots of topics. So, my question is under each area (linear algebra, real analysis, etc), what topics should I cover? Where should I stop before moving on to the next area? Many thanks!","I have been searching the web on the prerequisites of understanding Black Scholes Option Pricing Model. And I have drawn the following diagram to summarize the relationship between different topics: However, I do not know how far I should go in each topic. Linear Algebra, real analysis, to my understanding, involve lots of topics. So, my question is under each area (linear algebra, real analysis, etc), what topics should I cover? Where should I stop before moving on to the next area? Many thanks!",,"['real-analysis', 'linear-algebra', 'multivariable-calculus', 'stochastic-processes', 'stochastic-calculus']"
54,Inverse-Square vector fields have both a divergence and curl of $0$?,Inverse-Square vector fields have both a divergence and curl of ?,0,"Consider an inverse- square vector field $$ \vec{F} = \frac{x}{r^3}\hat{x} + \frac{y}{r^3}\hat{y} + \frac{z}{r^3}\hat{z} = \frac{\hat{r}}{r^2}$$ where $r = \sqrt{x^2 + y^2 + z^2}$. The curl $\nabla \times \vec{F} = \vec{0}$, therefore we might go looking for a potential $V$. I find that $V = -1/r$ works and therefore one can say that $\vec{F} = \nabla V$ is derivable from a potential function $V$. I'll point out right now that $\vec{F}$ is undefined at the origin. The divergence $\nabla \cdot \vec{F} = 0$. Therefore, we might go looking for a vector potential $\vec{A}$ such that $\vec{F} = \nabla \times \vec{A}$. One would say that $\vec{F}$ is derivable from a vector potential $\vec{A}$. But I'm having trouble seeing that an inverse-square vector field is derivable from both a vector potential and a scalar potential. So I know we have a trouble point at the origin. Yet this trouble point doesn't really seem to affect the ""conservativeness"" or path-independence of the vector field. But this trouble point does seem to affect the surface-independence of the vector field. As long as the surface doesn't wrap around the origin, I'd expect the inverse-square vector field to be surface-independent for a given boundary curve. Can an inverse-square vector field be derivable from both a scalar potential and a separate a vector potential? (Helmholtz theorem comes to mind. But the question I'm asking involves two separate equations. One $\nabla V$ gives $\vec{F}$ and another $\nabla \times \vec{A}$ gives $\vec{F}$ as well).","Consider an inverse- square vector field $$ \vec{F} = \frac{x}{r^3}\hat{x} + \frac{y}{r^3}\hat{y} + \frac{z}{r^3}\hat{z} = \frac{\hat{r}}{r^2}$$ where $r = \sqrt{x^2 + y^2 + z^2}$. The curl $\nabla \times \vec{F} = \vec{0}$, therefore we might go looking for a potential $V$. I find that $V = -1/r$ works and therefore one can say that $\vec{F} = \nabla V$ is derivable from a potential function $V$. I'll point out right now that $\vec{F}$ is undefined at the origin. The divergence $\nabla \cdot \vec{F} = 0$. Therefore, we might go looking for a vector potential $\vec{A}$ such that $\vec{F} = \nabla \times \vec{A}$. One would say that $\vec{F}$ is derivable from a vector potential $\vec{A}$. But I'm having trouble seeing that an inverse-square vector field is derivable from both a vector potential and a scalar potential. So I know we have a trouble point at the origin. Yet this trouble point doesn't really seem to affect the ""conservativeness"" or path-independence of the vector field. But this trouble point does seem to affect the surface-independence of the vector field. As long as the surface doesn't wrap around the origin, I'd expect the inverse-square vector field to be surface-independent for a given boundary curve. Can an inverse-square vector field be derivable from both a scalar potential and a separate a vector potential? (Helmholtz theorem comes to mind. But the question I'm asking involves two separate equations. One $\nabla V$ gives $\vec{F}$ and another $\nabla \times \vec{A}$ gives $\vec{F}$ as well).",,"['multivariable-calculus', 'vector-fields', 'divergence-operator', 'curl']"
55,(NOT a physics question) Is electric field always asymptopic to $x^{\alpha}$ for some rational $\alpha$?,(NOT a physics question) Is electric field always asymptopic to  for some rational ?,x^{\alpha} \alpha,"In three dimensional space with origin $O$, you pick a finite number of points $P_1, P_2, \cdots, P_n$. To each point $P_i$ you assign a nonzero integer (positive or negative) $q_i$. For all other points $R$ in the plane, define the vector valued function $$\displaystyle \vec{F(R)} = \sum_{i = 1}^{n} \frac{q_i}{D(P_i, R)^2} \vec{r_i}, $$ where $D(P_i, R)$ is the Euclidean distance between $P_i, R$, and $r_i$ is a vector of unit magnitude directed from $P_i$ to $R$. Now you pick a ray $\vec{\ell}$ originating from $O$ in any direction. Is it true that for any such configuration of such points, there always exist an rational number $\alpha$ such that $\displaystyle \lim_{x \rightarrow \infty} \| F(R_x) \| x^{\alpha}$ converges to some nonzero constant, where $R_x \in \ell$  with $D(O, R_x) = x$ and $\| F(R_x) \|$ is the magnitude of the function at $R_x?$","In three dimensional space with origin $O$, you pick a finite number of points $P_1, P_2, \cdots, P_n$. To each point $P_i$ you assign a nonzero integer (positive or negative) $q_i$. For all other points $R$ in the plane, define the vector valued function $$\displaystyle \vec{F(R)} = \sum_{i = 1}^{n} \frac{q_i}{D(P_i, R)^2} \vec{r_i}, $$ where $D(P_i, R)$ is the Euclidean distance between $P_i, R$, and $r_i$ is a vector of unit magnitude directed from $P_i$ to $R$. Now you pick a ray $\vec{\ell}$ originating from $O$ in any direction. Is it true that for any such configuration of such points, there always exist an rational number $\alpha$ such that $\displaystyle \lim_{x \rightarrow \infty} \| F(R_x) \| x^{\alpha}$ converges to some nonzero constant, where $R_x \in \ell$  with $D(O, R_x) = x$ and $\| F(R_x) \|$ is the magnitude of the function at $R_x?$",,"['multivariable-calculus', 'asymptotics', 'vector-analysis', 'physics']"
56,"Find $\lim_\limits{(x,y)\to (0,0)}\frac{x^\alpha y^4}{x^2+y^4}$ where $\alpha > 0$",Find  where,"\lim_\limits{(x,y)\to (0,0)}\frac{x^\alpha y^4}{x^2+y^4} \alpha > 0","How can we find the following limit? $$\lim_{(x,y)\to(0,0)}\frac{x^\alpha y^4}{x^2+y^4}\qquad \alpha>0$$ By using the polar coordinate, we get $$\lim_{r\to 0}r^{\alpha+2}\frac{\cos^\alpha\theta \sin^4\theta}{\cos^2\theta+r^2\sin^4{\theta}}=0$$ if $\theta\notin\{\frac{\pi}{2}+\pi k:k\in\Bbb Z\}$. Now, if $\theta = \frac{\pi}{2}+\pi k$ for some $k\in\Bbb Z$, then we get $$\lim_{(0,y)\to (0,0)}\frac{x^\alpha y^4}{x^2+y^4}=0.$$ Can we conclude that $$\lim_{(x,y)\to(0,0)}\frac{x^\alpha y^4}{x^2+y^4}=0?$$","How can we find the following limit? $$\lim_{(x,y)\to(0,0)}\frac{x^\alpha y^4}{x^2+y^4}\qquad \alpha>0$$ By using the polar coordinate, we get $$\lim_{r\to 0}r^{\alpha+2}\frac{\cos^\alpha\theta \sin^4\theta}{\cos^2\theta+r^2\sin^4{\theta}}=0$$ if $\theta\notin\{\frac{\pi}{2}+\pi k:k\in\Bbb Z\}$. Now, if $\theta = \frac{\pi}{2}+\pi k$ for some $k\in\Bbb Z$, then we get $$\lim_{(0,y)\to (0,0)}\frac{x^\alpha y^4}{x^2+y^4}=0.$$ Can we conclude that $$\lim_{(x,y)\to(0,0)}\frac{x^\alpha y^4}{x^2+y^4}=0?$$",,"['calculus', 'multivariable-calculus']"
57,A Sobolev map with smooth weak derivative is smooth?,A Sobolev map with smooth weak derivative is smooth?,,"Let $f \in W^{1,p}(\mathbb{R}^d)$ be a Sobolev map.  Suppose its weak derivative is smooth; i.e. it has a representative $x \to df_x $, which is $C^{\infty}$, considered  as a map $\mathbb{R}^d \to  \mathbb{R}^d$. Is it true $f$ is smooth? (does it have a smooth representative?). For $d=1$, the answer is trivially yes: Denote by $f'$ the smooth derivative, and take a smooth anti-derivative of it $F$: Then $F,f$ are both Sobolev maps, with weak derivatives $f'$. Thus $F-f$ has zero weak derivative, hence is constant a.e .","Let $f \in W^{1,p}(\mathbb{R}^d)$ be a Sobolev map.  Suppose its weak derivative is smooth; i.e. it has a representative $x \to df_x $, which is $C^{\infty}$, considered  as a map $\mathbb{R}^d \to  \mathbb{R}^d$. Is it true $f$ is smooth? (does it have a smooth representative?). For $d=1$, the answer is trivially yes: Denote by $f'$ the smooth derivative, and take a smooth anti-derivative of it $F$: Then $F,f$ are both Sobolev maps, with weak derivatives $f'$. Thus $F-f$ has zero weak derivative, hence is constant a.e .",,"['real-analysis', 'multivariable-calculus', 'sobolev-spaces', 'weak-derivatives']"
58,"Let $\omega$ be an (n-1)-form on $\mathbb{R}^n-{0}$, calculate $d\omega$","Let  be an (n-1)-form on , calculate",\omega \mathbb{R}^n-{0} d\omega,"$$\omega=\sum_{i=1}^n(-1)^{i-1}f_idx_{1}\wedge dx_{i-1}\wedge dx_{i+1}\wedge ...\wedge dx_{n}$$ Where $$f_i= {x_i\over|x|^m}$$ Calculate $d\omega$ Now, I feel like this is the next step to make, but im unsure where to go from here. The textbook explained the differential operator fairly quickly, so im not sure i really understand it.  $$d\omega=\sum_{i=1}^n(-1)^{i-1}df_i\wedge dx_{1}\wedge dx_{i-1}\wedge dx_{i+1}\wedge ...\wedge dx_{n}$$","$$\omega=\sum_{i=1}^n(-1)^{i-1}f_idx_{1}\wedge dx_{i-1}\wedge dx_{i+1}\wedge ...\wedge dx_{n}$$ Where $$f_i= {x_i\over|x|^m}$$ Calculate $d\omega$ Now, I feel like this is the next step to make, but im unsure where to go from here. The textbook explained the differential operator fairly quickly, so im not sure i really understand it.  $$d\omega=\sum_{i=1}^n(-1)^{i-1}df_i\wedge dx_{1}\wedge dx_{i-1}\wedge dx_{i+1}\wedge ...\wedge dx_{n}$$",,['multivariable-calculus']
59,Proving integral inequality for all $f\in H^1(D)$,Proving integral inequality for all,f\in H^1(D),LET $D\subset\mathbb R^n$ and $s\ge 0$ be some constant. I want to show that there exists a constant $C$ such that for all $f\in H^1(D)$ that satisfies $s\le \sharp\{t \mid f(t)=0\}$. Can someone please give me some ideas?,LET $D\subset\mathbb R^n$ and $s\ge 0$ be some constant. I want to show that there exists a constant $C$ such that for all $f\in H^1(D)$ that satisfies $s\le \sharp\{t \mid f(t)=0\}$. Can someone please give me some ideas?,,['multivariable-calculus']
60,On necessary conditions for constrained optimization in $\mathbb{R}^{n}$,On necessary conditions for constrained optimization in,\mathbb{R}^{n},"I am confused by a what Liberzon writes in his book on optimal control on page 12 and beginning of 13 regarding necessary conditions for optimality. He starts of by showing the first order condition for constrained optimality of $f$ w.r.t $h$ i.e If $x$ is constrained minima of $f$ then $\nabla f(x)+ \lambda\nabla h(x)=0$ for some $\lambda$. To make this more explicit he defines the augmented cost functional, $\ell(x,\lambda)=\nabla f(x)+ \lambda\nabla h(x)$. He then shows that if $x$ is constrained minima and $\lambda$ is its Lagrange multiplier then $\nabla \ell=0$ at this point. All good and well so far. Next he argues, If $(x,\lambda)$ is a minima for $\ell$ then $h(x)=0$ and subject to these constraints $x$ also minimize $f$ i.e the gradient  of $\ell$ is zero. But then he stresses that this is not a necessary condition for optimality if we only assume that $x$ is a constrained minima of $f$. I don't understand what he means here. What situation is he referring to? And what does he wanna say? He just showed that there always are multipliers for any constrained minima and hence we should be able to repeat one of his above arguments to get that the gradient is zero of $\ell$. But this can't be it. Is he referring to a situation where he don't wanna assume anything about $\lambda$?. Does anyone understand this? His book is free online http://liberzon.csl.illinois.edu/teaching/cvoc.pdf Also here is the text OBSERVE THAT THE BOUNTY IS NOT AWARDED TO THE RIGHT ANSWER! Read comments if you wanna understand why he got it.","I am confused by a what Liberzon writes in his book on optimal control on page 12 and beginning of 13 regarding necessary conditions for optimality. He starts of by showing the first order condition for constrained optimality of $f$ w.r.t $h$ i.e If $x$ is constrained minima of $f$ then $\nabla f(x)+ \lambda\nabla h(x)=0$ for some $\lambda$. To make this more explicit he defines the augmented cost functional, $\ell(x,\lambda)=\nabla f(x)+ \lambda\nabla h(x)$. He then shows that if $x$ is constrained minima and $\lambda$ is its Lagrange multiplier then $\nabla \ell=0$ at this point. All good and well so far. Next he argues, If $(x,\lambda)$ is a minima for $\ell$ then $h(x)=0$ and subject to these constraints $x$ also minimize $f$ i.e the gradient  of $\ell$ is zero. But then he stresses that this is not a necessary condition for optimality if we only assume that $x$ is a constrained minima of $f$. I don't understand what he means here. What situation is he referring to? And what does he wanna say? He just showed that there always are multipliers for any constrained minima and hence we should be able to repeat one of his above arguments to get that the gradient is zero of $\ell$. But this can't be it. Is he referring to a situation where he don't wanna assume anything about $\lambda$?. Does anyone understand this? His book is free online http://liberzon.csl.illinois.edu/teaching/cvoc.pdf Also here is the text OBSERVE THAT THE BOUNTY IS NOT AWARDED TO THE RIGHT ANSWER! Read comments if you wanna understand why he got it.",,"['multivariable-calculus', 'optimization', 'lagrange-multiplier']"
61,Lagrange Multiplier Problem with trivial gradient of $g$,Lagrange Multiplier Problem with trivial gradient of,g,"Consider the Lagrange Multiplier problem where we want to maximize $f(x,y) = x$ subject to $g(x,y) = y^2+x^4-x^3 = 0$ Now, setting up $\nabla f(x,y) = \lambda \nabla g(x,y)$, we obtain $$\langle 1, 0 \rangle = \lambda \langle 4x^{3}-3x^{2},2y \rangle \\ \implies \begin{cases} 1& = &\lambda(4x^{3}-3x^{2}) \\ 0 & = &\lambda 2y\end{cases} $$ However, I am confused about 3 things : 1) How to proceed at this point if $\lambda = 0$. 2) If $\lambda \neq 0$, then $y = 0$, and we must have both that $x^{4} - x^{3} = 0$ and $4x^{3}-3x^{2} = 0$. But, in the case where $x \neq 0$ here, we would have that both $x = 1$ and $x = 3/4$ at the same time, which we can't have. So, I'm assuming then that if $\lambda \neq 0$, then $(0,0)$ is the only critical point? 3) I was told that this problem is a case where the gradient of $g$ is trivial. Trivial as in $\nabla g(x,y) = (0, 0)$? How is it trivial? Because the only critical point is $(0,0)$? Also, this is supposed to be an example of what this article is talking about, but I'm not seeing it. Could someone please explain this to me? Thank you.","Consider the Lagrange Multiplier problem where we want to maximize $f(x,y) = x$ subject to $g(x,y) = y^2+x^4-x^3 = 0$ Now, setting up $\nabla f(x,y) = \lambda \nabla g(x,y)$, we obtain $$\langle 1, 0 \rangle = \lambda \langle 4x^{3}-3x^{2},2y \rangle \\ \implies \begin{cases} 1& = &\lambda(4x^{3}-3x^{2}) \\ 0 & = &\lambda 2y\end{cases} $$ However, I am confused about 3 things : 1) How to proceed at this point if $\lambda = 0$. 2) If $\lambda \neq 0$, then $y = 0$, and we must have both that $x^{4} - x^{3} = 0$ and $4x^{3}-3x^{2} = 0$. But, in the case where $x \neq 0$ here, we would have that both $x = 1$ and $x = 3/4$ at the same time, which we can't have. So, I'm assuming then that if $\lambda \neq 0$, then $(0,0)$ is the only critical point? 3) I was told that this problem is a case where the gradient of $g$ is trivial. Trivial as in $\nabla g(x,y) = (0, 0)$? How is it trivial? Because the only critical point is $(0,0)$? Also, this is supposed to be an example of what this article is talking about, but I'm not seeing it. Could someone please explain this to me? Thank you.",,"['calculus', 'multivariable-calculus']"
62,Jacobian of implicit functions,Jacobian of implicit functions,,"Question Statement:- If $u^3+v^3=x+y$ and $u^2+v^2=x^3+y^3$, show that $$\frac{\partial(u,v)}{\partial(x,y)}=\frac{1}{2}\frac{y^2-x^2}{uv(u-v)}$$ My Solution:- As the relation that are given between $u,v,x \;\& \; y$ do not seemed to me to be separated so as to get the $u$ and $v$ in the terms of $x\;\&\;y$, so I went with the following approach. We are given with the following relations:- $$u^3+v^3=x+y\tag{1}$$ $u^2+v^2=x^3+y^3\tag{2}$ On partially differentiating the above given relations we have, $$3u^2\frac{\partial u}{\partial{x}}+3v^2\frac{\partial{v}}{\partial{x}}=1\tag{3}$$ $$2u\frac{\partial u}{\partial{x}}+2v\frac{\partial{v}}{\partial{x}}=3x^2\tag{4}$$ The equations $(3)$ and $(4)$ can be written as a matrix as follows $$\begin{bmatrix} 3u^2 & 3v^2\\ 2u & 2v \end{bmatrix} \begin{bmatrix} \dfrac{\partial{u}}{\partial{x}}\\ \dfrac{\partial{v}}{\partial{x}} \end{bmatrix} = \begin{bmatrix} 1 \\ 3x^2 \end{bmatrix} \\ \implies  \begin{bmatrix} \dfrac{\partial{u}}{\partial{x}}\\ \dfrac{\partial{v}}{\partial{x}} \end{bmatrix}= \dfrac{1}{6uv(u-v)}\begin{bmatrix} 2v & -2u\\ -3v^2 & 3u^2 \end{bmatrix} \begin{bmatrix} 1 \\ 3x^2 \end{bmatrix}\\ \implies  \begin{bmatrix} \dfrac{\partial{u}}{\partial{x}}\\ \dfrac{\partial{v}}{\partial{x}} \end{bmatrix}= \dfrac{1}{6uv(u-v)}\begin{bmatrix} 2v-6ux^2\\ 9u^2x^2-3v^2 \end{bmatrix} $$ So, we get $$\dfrac{\partial{u}}{\partial{x}}=\dfrac{2v-6ux^2}{6uv(u-v)}\\ \&\\ \dfrac{\partial{v}}{\partial{x}}=\dfrac{9u^2x^2-3v^2}{6uv(u-v)}$$ And due to the symmetry in the equations $(1)$ and $(2)$, we get $$\dfrac{\partial{u}}{\partial{y}}=\dfrac{2v-6uy^2}{6uv(u-v)}\\ \&\\ \dfrac{\partial{v}}{\partial{y}}=\dfrac{9u^2y^2-3v^2}{6uv(u-v)}$$ So, we get the Jacobian determinant as $$\frac{\partial(u,v)}{\partial(x,y)}= \begin{vmatrix} \dfrac{\partial{u}}{\partial{x}} & \dfrac{\partial{u}}{\partial{y}}\\ \dfrac{\partial{v}}{\partial{x}} & \dfrac{\partial{v}}{\partial{y}} \end{vmatrix}= \begin{vmatrix} \dfrac{2v-6ux^2}{6uv(u-v)} & \dfrac{2v-6uy^2}{6uv(u-v)}\\ \dfrac{9u^2x^2-3v^2}{6uv(u-v)} & \dfrac{9u^2y^2-3v^2}{6uv(u-v)} \end{vmatrix}=\frac{1}{2}\frac{y^2-x^2}{uv(u-v)}$$ My deal with the question:- This method seemed a bit too long, can you suggest a shorter method. If the method teaches me something new that would be good.","Question Statement:- If $u^3+v^3=x+y$ and $u^2+v^2=x^3+y^3$, show that $$\frac{\partial(u,v)}{\partial(x,y)}=\frac{1}{2}\frac{y^2-x^2}{uv(u-v)}$$ My Solution:- As the relation that are given between $u,v,x \;\& \; y$ do not seemed to me to be separated so as to get the $u$ and $v$ in the terms of $x\;\&\;y$, so I went with the following approach. We are given with the following relations:- $$u^3+v^3=x+y\tag{1}$$ $u^2+v^2=x^3+y^3\tag{2}$ On partially differentiating the above given relations we have, $$3u^2\frac{\partial u}{\partial{x}}+3v^2\frac{\partial{v}}{\partial{x}}=1\tag{3}$$ $$2u\frac{\partial u}{\partial{x}}+2v\frac{\partial{v}}{\partial{x}}=3x^2\tag{4}$$ The equations $(3)$ and $(4)$ can be written as a matrix as follows $$\begin{bmatrix} 3u^2 & 3v^2\\ 2u & 2v \end{bmatrix} \begin{bmatrix} \dfrac{\partial{u}}{\partial{x}}\\ \dfrac{\partial{v}}{\partial{x}} \end{bmatrix} = \begin{bmatrix} 1 \\ 3x^2 \end{bmatrix} \\ \implies  \begin{bmatrix} \dfrac{\partial{u}}{\partial{x}}\\ \dfrac{\partial{v}}{\partial{x}} \end{bmatrix}= \dfrac{1}{6uv(u-v)}\begin{bmatrix} 2v & -2u\\ -3v^2 & 3u^2 \end{bmatrix} \begin{bmatrix} 1 \\ 3x^2 \end{bmatrix}\\ \implies  \begin{bmatrix} \dfrac{\partial{u}}{\partial{x}}\\ \dfrac{\partial{v}}{\partial{x}} \end{bmatrix}= \dfrac{1}{6uv(u-v)}\begin{bmatrix} 2v-6ux^2\\ 9u^2x^2-3v^2 \end{bmatrix} $$ So, we get $$\dfrac{\partial{u}}{\partial{x}}=\dfrac{2v-6ux^2}{6uv(u-v)}\\ \&\\ \dfrac{\partial{v}}{\partial{x}}=\dfrac{9u^2x^2-3v^2}{6uv(u-v)}$$ And due to the symmetry in the equations $(1)$ and $(2)$, we get $$\dfrac{\partial{u}}{\partial{y}}=\dfrac{2v-6uy^2}{6uv(u-v)}\\ \&\\ \dfrac{\partial{v}}{\partial{y}}=\dfrac{9u^2y^2-3v^2}{6uv(u-v)}$$ So, we get the Jacobian determinant as $$\frac{\partial(u,v)}{\partial(x,y)}= \begin{vmatrix} \dfrac{\partial{u}}{\partial{x}} & \dfrac{\partial{u}}{\partial{y}}\\ \dfrac{\partial{v}}{\partial{x}} & \dfrac{\partial{v}}{\partial{y}} \end{vmatrix}= \begin{vmatrix} \dfrac{2v-6ux^2}{6uv(u-v)} & \dfrac{2v-6uy^2}{6uv(u-v)}\\ \dfrac{9u^2x^2-3v^2}{6uv(u-v)} & \dfrac{9u^2y^2-3v^2}{6uv(u-v)} \end{vmatrix}=\frac{1}{2}\frac{y^2-x^2}{uv(u-v)}$$ My deal with the question:- This method seemed a bit too long, can you suggest a shorter method. If the method teaches me something new that would be good.",,"['multivariable-calculus', 'jacobian', 'implicit-function']"
63,Is my analogy of a directional derivative correct?,Is my analogy of a directional derivative correct?,,"I find analogies to real life are a good way for me to understand some faucets of multivariate calculus, and a wrong analogy can be an exact and glaring indication of misunderstanding. As far as I am aware, in 3D space, there is no total derivative because there is an input space rather than line, and so output $z$ no longer depends on one variable alone - the rate of a change of a slice of a surface in Euclidean space depends on which slice of the surface we take. For instance, if a skateboarder, from a bird's eye view, is directly left of a hill that he can ride down and to the right a ramp he can ride up, and if further up the $y$ axis from the skateboarder's location (above and below the hill and ramp respectively) is just flat asphalt, then if $x, y$ and $z$ represents length and the origin is centered at the skateboarder's location, the rate of change of $z$ (elevation) will be different as the skateboarder moves along $x$ at some velocity depending on where he is -- not changing on flat asphalt above him, but changing if moving between the hill and ramp. Because of that, there is no total derivative, and a different one per slice of $y$. And as such, we need partial derivatives -- finding the derivative with a constant $y$ slice and then taking the general case where $y$ is some constant -- then if we have $x$ and $y$ we can compute the derivative at that slice. Directional Derivatives seem as arbitrary as moving from partial differentiating along some straight line $x$ or $y$ and applying it so some arbitrary vector $\vec v$. I see it as being analogous to the scenario with before, but the ramp being, say, above and to the left of the skater (from a bird's eye view, so $+y$ away and $-x$ away). Taking the directional derivative of a vector causing the skateboarder to go up the ramp will cause a different relationship with $z$ if the velocity vector was some arbitrary vector moving on flat asphalt once again. Please critique my understanding by pointing out the flaws in my analogies if there are any, which is certainly likely.","I find analogies to real life are a good way for me to understand some faucets of multivariate calculus, and a wrong analogy can be an exact and glaring indication of misunderstanding. As far as I am aware, in 3D space, there is no total derivative because there is an input space rather than line, and so output $z$ no longer depends on one variable alone - the rate of a change of a slice of a surface in Euclidean space depends on which slice of the surface we take. For instance, if a skateboarder, from a bird's eye view, is directly left of a hill that he can ride down and to the right a ramp he can ride up, and if further up the $y$ axis from the skateboarder's location (above and below the hill and ramp respectively) is just flat asphalt, then if $x, y$ and $z$ represents length and the origin is centered at the skateboarder's location, the rate of change of $z$ (elevation) will be different as the skateboarder moves along $x$ at some velocity depending on where he is -- not changing on flat asphalt above him, but changing if moving between the hill and ramp. Because of that, there is no total derivative, and a different one per slice of $y$. And as such, we need partial derivatives -- finding the derivative with a constant $y$ slice and then taking the general case where $y$ is some constant -- then if we have $x$ and $y$ we can compute the derivative at that slice. Directional Derivatives seem as arbitrary as moving from partial differentiating along some straight line $x$ or $y$ and applying it so some arbitrary vector $\vec v$. I see it as being analogous to the scenario with before, but the ramp being, say, above and to the left of the skater (from a bird's eye view, so $+y$ away and $-x$ away). Taking the directional derivative of a vector causing the skateboarder to go up the ramp will cause a different relationship with $z$ if the velocity vector was some arbitrary vector moving on flat asphalt once again. Please critique my understanding by pointing out the flaws in my analogies if there are any, which is certainly likely.",,['multivariable-calculus']
64,Intuition behind applying the implicit function theorem to parameterization of unit circle,Intuition behind applying the implicit function theorem to parameterization of unit circle,,"This post is a modification of a previous post I have since deleted, which was much too messy and basically had two separate questions. (It received no answers.) A common motivating example for the implicit function theorem (IFT) is the equation $x^2+y^2=1$ where $(x,y)\in \mathbb R^2$, as we can locally write $y$ as an equation of $x$ (and vice versa) in the familiar way. Consider the following exercise from Wade's An Introduction to Analysis . Suppose that $f:=(u,v):\mathbb R\to \mathbb R^2$ is $C^2$ and set   $(x_0,y_0)=f(t_0)$. Also assume that $f'(t_0)\ne 0$. Show that either   there is a $C^1$ function $g$ with $g(x_0)=t_0$ and $u(g(x))=x$ for   any $x$ near $x_0$ or there is a $C^1$ function $h$ with $h(y_0)=t_0$   and $v(h(x))=y$ for any $y$ near $y_0$. (Note that I think the   textbook has a typo since we just need to assume that $f$ is $C^1$.   Correct me if I'm wrong.) I can do this by applying the IFT to the function $F(x,t)=x-u(t)$ and to the function $G(y,t)=y-v(t)$. Now, the unit circle can be parameterized (infinitely many times) by $(\sin(t), \cos(t))$ for $t\in \mathbb R$. And the map $(\sin(t), \cos(t))$ is everywhere $C^1$. Therefore, I am wondering if the exercise can be seen as describing the motivating example of the unit circle I described initially. That is, is there any way to relate this problem to the simple idea of writing the unit circle $x^2+y^2=1$ locally as a function of $x$ or $y$? I think that the answer is ""yes"" in some sense but am unsure how. I just want to gain a bit of intuition.","This post is a modification of a previous post I have since deleted, which was much too messy and basically had two separate questions. (It received no answers.) A common motivating example for the implicit function theorem (IFT) is the equation $x^2+y^2=1$ where $(x,y)\in \mathbb R^2$, as we can locally write $y$ as an equation of $x$ (and vice versa) in the familiar way. Consider the following exercise from Wade's An Introduction to Analysis . Suppose that $f:=(u,v):\mathbb R\to \mathbb R^2$ is $C^2$ and set   $(x_0,y_0)=f(t_0)$. Also assume that $f'(t_0)\ne 0$. Show that either   there is a $C^1$ function $g$ with $g(x_0)=t_0$ and $u(g(x))=x$ for   any $x$ near $x_0$ or there is a $C^1$ function $h$ with $h(y_0)=t_0$   and $v(h(x))=y$ for any $y$ near $y_0$. (Note that I think the   textbook has a typo since we just need to assume that $f$ is $C^1$.   Correct me if I'm wrong.) I can do this by applying the IFT to the function $F(x,t)=x-u(t)$ and to the function $G(y,t)=y-v(t)$. Now, the unit circle can be parameterized (infinitely many times) by $(\sin(t), \cos(t))$ for $t\in \mathbb R$. And the map $(\sin(t), \cos(t))$ is everywhere $C^1$. Therefore, I am wondering if the exercise can be seen as describing the motivating example of the unit circle I described initially. That is, is there any way to relate this problem to the simple idea of writing the unit circle $x^2+y^2=1$ locally as a function of $x$ or $y$? I think that the answer is ""yes"" in some sense but am unsure how. I just want to gain a bit of intuition.",,"['real-analysis', 'multivariable-calculus', 'derivatives', 'intuition']"
65,Examples of $C^1(\mathbb{R}^2)$ functions where mixed partials are equal nowhere,Examples of  functions where mixed partials are equal nowhere,C^1(\mathbb{R}^2),"I know there are examples of functions which are twice differentiable, but the mixed partials $f_{xy}\neq f_{yx}$ at the origin $(0,0)$. Are there any examples of functions where the mixed partials exist, but are equal nowhere ? I know a necessary condition is that the mixed partials are discontinuous. My question can be formally stated in a three parts: For both of these cases, assume the second partial derivatives exist. $1)$ let $f\in C^1(\mathbb{R}^2)$. Do there exists subsets $E \subset\mathbb{R}^2$ where $\mu(E)> 0$ where $f_{xy}\neq f_{yx}$, (where $\mu(E)$ denotes the Lebesgue measure of $E$). Note that having mixed partial $f_{xy}\neq f_{yx}$ at the origin answers this question in the affirmative for $\mu(E)=0$, since if $E=\{(0,0)\}$, then $\mu(E)=0$, and we know of many examples where this occurs. $2)$ let $f\in C^1(\mathbb{R}^2)$. Is it possible for $f_{xy}$ to be continuous on a set $\mu(E)\geq 0, $ and $f_{xy}\neq f_{yx}$ everywhere or at least almost everywhere? $3)$ would answers to these questions be interesting enough to publish as counterexamples or theorems? I was unable to find information about these two cases, let alone generalizations to functions with $f\in C^1(\mathbb{R}^n)$, leading me to believe these could be interesting research questions.","I know there are examples of functions which are twice differentiable, but the mixed partials $f_{xy}\neq f_{yx}$ at the origin $(0,0)$. Are there any examples of functions where the mixed partials exist, but are equal nowhere ? I know a necessary condition is that the mixed partials are discontinuous. My question can be formally stated in a three parts: For both of these cases, assume the second partial derivatives exist. $1)$ let $f\in C^1(\mathbb{R}^2)$. Do there exists subsets $E \subset\mathbb{R}^2$ where $\mu(E)> 0$ where $f_{xy}\neq f_{yx}$, (where $\mu(E)$ denotes the Lebesgue measure of $E$). Note that having mixed partial $f_{xy}\neq f_{yx}$ at the origin answers this question in the affirmative for $\mu(E)=0$, since if $E=\{(0,0)\}$, then $\mu(E)=0$, and we know of many examples where this occurs. $2)$ let $f\in C^1(\mathbb{R}^2)$. Is it possible for $f_{xy}$ to be continuous on a set $\mu(E)\geq 0, $ and $f_{xy}\neq f_{yx}$ everywhere or at least almost everywhere? $3)$ would answers to these questions be interesting enough to publish as counterexamples or theorems? I was unable to find information about these two cases, let alone generalizations to functions with $f\in C^1(\mathbb{R}^n)$, leading me to believe these could be interesting research questions.",,"['real-analysis', 'multivariable-calculus']"
66,Kalman Filter constant steady state value,Kalman Filter constant steady state value,,"Without explaining you the details of my problem I would like to ask you a theoretical question. I think I know the answer but I would like to be sure. I have a set of $n$ agents moving in 3D space. In some way I am estimating their pose (positions+orientations). The pose of the agents represents the state of my Nonlinear System . The goal was to design an Extended Kalman Filter (EKF) to observe the poses of the agents. I understood how to implement the EKF but now I arrived to estimate the poses with a constant error at steady state. Is this, in your opinion, correct? I thought that the EKF, assumed that the difference between the initial state and the estimated state is not too big and the nonlinearities are not too big , would converge to the true state of the system. NOTE: beside the difference between the actual and estimated states I have no noise, neither in the measurement nor in the process (for now I am working just in simulation, MATLAB/SIMULINK) Thanks for your time in reading my question.","Without explaining you the details of my problem I would like to ask you a theoretical question. I think I know the answer but I would like to be sure. I have a set of $n$ agents moving in 3D space. In some way I am estimating their pose (positions+orientations). The pose of the agents represents the state of my Nonlinear System . The goal was to design an Extended Kalman Filter (EKF) to observe the poses of the agents. I understood how to implement the EKF but now I arrived to estimate the poses with a constant error at steady state. Is this, in your opinion, correct? I thought that the EKF, assumed that the difference between the initial state and the estimated state is not too big and the nonlinearities are not too big , would converge to the true state of the system. NOTE: beside the difference between the actual and estimated states I have no noise, neither in the measurement nor in the process (for now I am working just in simulation, MATLAB/SIMULINK) Thanks for your time in reading my question.",,"['linear-algebra', 'multivariable-calculus', 'multilinear-algebra', 'kalman-filter']"
67,"Differentiability of $x^2\log(x^4+y^2)$ at $(0,0)$",Differentiability of  at,"x^2\log(x^4+y^2) (0,0)","Based on this question , I have the function $$ f(x,y)=\begin{cases} x^2\log(x^4+y^2), & (x,y)\in\mathbb{R}^2\setminus\{(0,0)\},\\ 0,                & (x,y)=(0,0). \end{cases} $$ I would like to study its continuity and differentiability at $(0,0).$ Continuity For the continuity, I see that I can rewrite the expression $x^2\log(x^4+y^2)$ as $$ \sqrt\frac{x^4}{x^4+y^2}\cdot\sqrt{x^4+y^2}\log(x^4+y^2), $$ and given that: $\sqrt\frac{x^4}{x^4+y^2}$ is bounded: $$ 0\leq\sqrt\frac{x^4}{x^4+y^2}\leq1; $$ for $\sqrt{x^4+y^2}\log(x^4+y^2)$ I can use the known limit $$ \lim_{t\to0}t^\alpha\log t=0,\qquad\forall\alpha>0; $$ I conclude that  $$ \lim_{(x,y)\to(0,0)}f(x,y)=0, $$ so the function is continuous in $(0,0).$ Existence and continuity of derivative with respect to $x$ Something similar can be done for the derivative with respect to $x$ in $(0,0),$ in fact $$ f'_x(0,0)=\lim_{(x,y)\to(0,0)}\frac{x^2\log(x^4+y^2)-0}{x-0}=\lim_{(x,y)\to(0,0)}x\log(x^4+y^2) $$ and the expression $x\log(x^4+y^2)$ could be written as $$ \operatorname{sign}x\cdot\sqrt[4]\frac{x^4}{x^4+y^2}\cdot\sqrt[4]{x^4+y^2}\log(x^4+y^2), $$ whose limit, as before, is $0.$ As for the limit of the derivative function with respect to $x$, it is $$ \lim_{(x,y)\to(0,0)}f'_x(x,y)=\lim_{(x,y)\to(0,0)}\left(2x\log(x^4+y^2)+\frac{4x^5}{x^4+y^2}\right) $$ and the first term of the sum is as before, while the second can be written as the product of $4x$, that goes to $0$, by a bounded ratio, so the limit is $0$ and I can conclude that $f'_x(x,y)$ is continuous at $(0,0).$ Existence and continuity of derivative with respect to $y$ For the derivative with respect to $y$ the things are different, I have $$ f'_y(0,0)=\lim_{(x,y)\to(0,0)}\frac{x^2\log(x^4+y^2)-0}{y-0}=\lim_{(x,y)\to(0,0)}\frac{x^2}{y}\log(x^4+y^2) $$ that I am not able to rewrite in a form that is simpler to manage. Moreover, if I make the limit along the curves $y=x^2$ and $y=-x^2$ I get $$ \lim_{x\to0}\frac{x^2}{x^2}\log(2x^4)=-\infty\\ \lim_{x\to0}\frac{x^2}{-x^2}\log(2x^4)=+\infty $$ so the derivative with respect to $y$ does not exist in $(0,0),$ and the function cannot be differentiable in $(0,0).$ The question Given all that, my question is: this question is wrong saying $f$ is differentiable, or I am making some mistake? Also the graphics of $f$ seems rather smooth around $(0,0)$:","Based on this question , I have the function $$ f(x,y)=\begin{cases} x^2\log(x^4+y^2), & (x,y)\in\mathbb{R}^2\setminus\{(0,0)\},\\ 0,                & (x,y)=(0,0). \end{cases} $$ I would like to study its continuity and differentiability at $(0,0).$ Continuity For the continuity, I see that I can rewrite the expression $x^2\log(x^4+y^2)$ as $$ \sqrt\frac{x^4}{x^4+y^2}\cdot\sqrt{x^4+y^2}\log(x^4+y^2), $$ and given that: $\sqrt\frac{x^4}{x^4+y^2}$ is bounded: $$ 0\leq\sqrt\frac{x^4}{x^4+y^2}\leq1; $$ for $\sqrt{x^4+y^2}\log(x^4+y^2)$ I can use the known limit $$ \lim_{t\to0}t^\alpha\log t=0,\qquad\forall\alpha>0; $$ I conclude that  $$ \lim_{(x,y)\to(0,0)}f(x,y)=0, $$ so the function is continuous in $(0,0).$ Existence and continuity of derivative with respect to $x$ Something similar can be done for the derivative with respect to $x$ in $(0,0),$ in fact $$ f'_x(0,0)=\lim_{(x,y)\to(0,0)}\frac{x^2\log(x^4+y^2)-0}{x-0}=\lim_{(x,y)\to(0,0)}x\log(x^4+y^2) $$ and the expression $x\log(x^4+y^2)$ could be written as $$ \operatorname{sign}x\cdot\sqrt[4]\frac{x^4}{x^4+y^2}\cdot\sqrt[4]{x^4+y^2}\log(x^4+y^2), $$ whose limit, as before, is $0.$ As for the limit of the derivative function with respect to $x$, it is $$ \lim_{(x,y)\to(0,0)}f'_x(x,y)=\lim_{(x,y)\to(0,0)}\left(2x\log(x^4+y^2)+\frac{4x^5}{x^4+y^2}\right) $$ and the first term of the sum is as before, while the second can be written as the product of $4x$, that goes to $0$, by a bounded ratio, so the limit is $0$ and I can conclude that $f'_x(x,y)$ is continuous at $(0,0).$ Existence and continuity of derivative with respect to $y$ For the derivative with respect to $y$ the things are different, I have $$ f'_y(0,0)=\lim_{(x,y)\to(0,0)}\frac{x^2\log(x^4+y^2)-0}{y-0}=\lim_{(x,y)\to(0,0)}\frac{x^2}{y}\log(x^4+y^2) $$ that I am not able to rewrite in a form that is simpler to manage. Moreover, if I make the limit along the curves $y=x^2$ and $y=-x^2$ I get $$ \lim_{x\to0}\frac{x^2}{x^2}\log(2x^4)=-\infty\\ \lim_{x\to0}\frac{x^2}{-x^2}\log(2x^4)=+\infty $$ so the derivative with respect to $y$ does not exist in $(0,0),$ and the function cannot be differentiable in $(0,0).$ The question Given all that, my question is: this question is wrong saying $f$ is differentiable, or I am making some mistake? Also the graphics of $f$ seems rather smooth around $(0,0)$:",,"['real-analysis', 'multivariable-calculus', 'derivatives']"
68,"If $u \in C^1(\mathbb{R}^2)$ and $yu_x + (-3y-2x)u_y = 0$, then $u$ is constant.","If  and , then  is constant.",u \in C^1(\mathbb{R}^2) yu_x + (-3y-2x)u_y = 0 u,"Show that if $u \in C^1(\mathbb{R}^2)$ and $yu_x + (-3y-2x)u_y = 0$,   then $u$ is constant. Find a non constant solution defined in an open set $U \subset \mathbb{R^2}. $ I'm a little stuck, I tried to look at the characteristics equations but I couldn't see how it helps to prove that the solution is constant. And I don't know how to prove that there exists a non constant solution Can someone please suggest how do I proceed to solve this question? Thanks","Show that if $u \in C^1(\mathbb{R}^2)$ and $yu_x + (-3y-2x)u_y = 0$,   then $u$ is constant. Find a non constant solution defined in an open set $U \subset \mathbb{R^2}. $ I'm a little stuck, I tried to look at the characteristics equations but I couldn't see how it helps to prove that the solution is constant. And I don't know how to prove that there exists a non constant solution Can someone please suggest how do I proceed to solve this question? Thanks",,"['multivariable-calculus', 'partial-differential-equations', 'partial-derivative']"
69,How to compute the partial derivatives of this function?,How to compute the partial derivatives of this function?,,"$$f(x,y) = \int_{-x}^{y}\sinh(xyt^2)dt$$ I tried replacing $\sinh(xyt^2)$ by its taylor series $$\sum_{n=1}^{\infty} \frac{(xyt^2)^{2n+1}}{(2n+1)!} = \sum_{n=1}^{\infty} \frac{x^{2n+1}y^{2n+1}t^{4n+2}}{(2n+1)!} $$ so I think that an anti derivative of this function with respect to $t$ would be $$\sum_{n=1}^{\infty} \frac{x^{2n+1}y^{2n+1}t^{4n+3}}{(2n+1)!\cdot(4n+3)}$$ so $$ \int_{-x}^{y}\sinh(xyt^2)dt = \sum_{n=1}^{\infty} \frac{x^{2n+1}y^{6n+4}}{(2n+1)!\cdot(4n+3)} +\sum_{n=1}^{\infty} \frac{x^{6n+4}y^{2n+1}}{(2n+1)!\cdot(4n+3)} $$ computing the partial derivatives of the above thing is a pain in the neck and  recognizing the function that matches the results is even painful. maybe I'm doing it wrong... I don't know but anyway I don't like the way I'm solving it show me an easier/softer method. thank you !","$$f(x,y) = \int_{-x}^{y}\sinh(xyt^2)dt$$ I tried replacing $\sinh(xyt^2)$ by its taylor series $$\sum_{n=1}^{\infty} \frac{(xyt^2)^{2n+1}}{(2n+1)!} = \sum_{n=1}^{\infty} \frac{x^{2n+1}y^{2n+1}t^{4n+2}}{(2n+1)!} $$ so I think that an anti derivative of this function with respect to $t$ would be $$\sum_{n=1}^{\infty} \frac{x^{2n+1}y^{2n+1}t^{4n+3}}{(2n+1)!\cdot(4n+3)}$$ so $$ \int_{-x}^{y}\sinh(xyt^2)dt = \sum_{n=1}^{\infty} \frac{x^{2n+1}y^{6n+4}}{(2n+1)!\cdot(4n+3)} +\sum_{n=1}^{\infty} \frac{x^{6n+4}y^{2n+1}}{(2n+1)!\cdot(4n+3)} $$ computing the partial derivatives of the above thing is a pain in the neck and  recognizing the function that matches the results is even painful. maybe I'm doing it wrong... I don't know but anyway I don't like the way I'm solving it show me an easier/softer method. thank you !",,"['calculus', 'multivariable-calculus', 'partial-derivative']"
70,Help to use change of variables to solve the double integral,Help to use change of variables to solve the double integral,,"I'm trying to evaluate: $$\iint_D \left(\sqrt{a^2-x^2-y^2}-\sqrt{x^2+y^2}~\right)dxdy$$ where $D_{xy}$ is the disk $x^2+y^2\le a^2$. The exercise is to use change of variables to solve this integral. My solution I chose $\varphi (r,\theta)=(ra\cos\theta,ra\sin\theta)$, where $0\le r\le 1$ and $0\le \theta\le 2\pi$ to be the change of variables. The determinant of the Jacobian is $ra^2$ and  \begin{align*} &\iint_{D_{xy}}\left(\sqrt{a^2-x^2-y^2}-\sqrt{x^2+y^2}~\right)dxdy \\&=\int_0^{2\pi}\int^1_0\left(\sqrt{a^2-r^2a^2}-ra\right)ra^2 drd\theta\\ &=2\pi a^3\int^1_0 \left(r\sqrt{1-r^2}-r^2 \right)dr\\ &=2\pi a^3\left(\int^1_0r\sqrt{1-r^2}dr-\int^1_0r^2dr\right)\\ &=2\pi a^3\left( \frac{1}{3}-\frac{1}{3} \right)\\ &=0 \end{align*} I would like to know where I'm mistaken. The answer in the end of the book shows $\pi a^3/3$.","I'm trying to evaluate: $$\iint_D \left(\sqrt{a^2-x^2-y^2}-\sqrt{x^2+y^2}~\right)dxdy$$ where $D_{xy}$ is the disk $x^2+y^2\le a^2$. The exercise is to use change of variables to solve this integral. My solution I chose $\varphi (r,\theta)=(ra\cos\theta,ra\sin\theta)$, where $0\le r\le 1$ and $0\le \theta\le 2\pi$ to be the change of variables. The determinant of the Jacobian is $ra^2$ and  \begin{align*} &\iint_{D_{xy}}\left(\sqrt{a^2-x^2-y^2}-\sqrt{x^2+y^2}~\right)dxdy \\&=\int_0^{2\pi}\int^1_0\left(\sqrt{a^2-r^2a^2}-ra\right)ra^2 drd\theta\\ &=2\pi a^3\int^1_0 \left(r\sqrt{1-r^2}-r^2 \right)dr\\ &=2\pi a^3\left(\int^1_0r\sqrt{1-r^2}dr-\int^1_0r^2dr\right)\\ &=2\pi a^3\left( \frac{1}{3}-\frac{1}{3} \right)\\ &=0 \end{align*} I would like to know where I'm mistaken. The answer in the end of the book shows $\pi a^3/3$.",,['multivariable-calculus']
71,Confused about proof of Leibniz Integral Rule,Confused about proof of Leibniz Integral Rule,,"If we set $G(x) = \int_{0}^{x} f(x,y) dy$, then \begin{align} \frac{G(x+d)-G(x)}d =& \frac{\int_{0}^{x+d} f(x+d,y)dy - \int_{0}^{x} f(x,y)dy}d \\ =& \frac{\int_{0}^{x}f(x+d,y)dy+\int_{x}^{x+d} - \int_{0}^{x}f(x,y)dy}d \end{align} By grouping, $$\int_{0}^{x}\frac{f(x+d,y)-f(x,y)}ddy + \int_{x}^{x+d}f(x+d,y)dy,$$ which leads to: $$\int_{0}^{x}f'(x,y)dy + \frac{\int_{0}^{x+d}f(x+d,y)dy-\int_{0}^{x}f(x+d,y)dy}d.$$ Why is the second term not equal to $f(x+d,x)$? I know it isn't, I'm just trying to see where I'm going wrong in the proof.","If we set $G(x) = \int_{0}^{x} f(x,y) dy$, then \begin{align} \frac{G(x+d)-G(x)}d =& \frac{\int_{0}^{x+d} f(x+d,y)dy - \int_{0}^{x} f(x,y)dy}d \\ =& \frac{\int_{0}^{x}f(x+d,y)dy+\int_{x}^{x+d} - \int_{0}^{x}f(x,y)dy}d \end{align} By grouping, $$\int_{0}^{x}\frac{f(x+d,y)-f(x,y)}ddy + \int_{x}^{x+d}f(x+d,y)dy,$$ which leads to: $$\int_{0}^{x}f'(x,y)dy + \frac{\int_{0}^{x+d}f(x+d,y)dy-\int_{0}^{x}f(x+d,y)dy}d.$$ Why is the second term not equal to $f(x+d,x)$? I know it isn't, I'm just trying to see where I'm going wrong in the proof.",,"['calculus', 'integration', 'multivariable-calculus', 'proof-verification']"
72,Continuous partial derivatives implies continuous differential,Continuous partial derivatives implies continuous differential,,"We have the well-known statement (Analysis I by Zorich, p.457): Let $f: U(x) \to \mathbb{R}$ be a function defined in a neighbourhood   $U(x) \subseteq \mathbb{R}^m$ of the point $x = (x^1,\dots,x^m)$. If   the function $f$ has all partial derivatives $\frac{\partial  f}{\partial x^1},\dots,\frac{\partial f}{\partial x^m}$ at each point   of $U(x)$ and they are continuous at $x$, then $f$ is differentiable   at $x$. Now my question is, that somehow my lecture notes suggest, that when the partial derivatives are continuous at all points of $U(x)$, $f$ is then continuously differentiable, i.e. the map $U(x) \to \text{Hom}(\mathbb{R}^m,\mathbb{R})$ is continuous. How can this be seen?","We have the well-known statement (Analysis I by Zorich, p.457): Let $f: U(x) \to \mathbb{R}$ be a function defined in a neighbourhood   $U(x) \subseteq \mathbb{R}^m$ of the point $x = (x^1,\dots,x^m)$. If   the function $f$ has all partial derivatives $\frac{\partial  f}{\partial x^1},\dots,\frac{\partial f}{\partial x^m}$ at each point   of $U(x)$ and they are continuous at $x$, then $f$ is differentiable   at $x$. Now my question is, that somehow my lecture notes suggest, that when the partial derivatives are continuous at all points of $U(x)$, $f$ is then continuously differentiable, i.e. the map $U(x) \to \text{Hom}(\mathbb{R}^m,\mathbb{R})$ is continuous. How can this be seen?",,['multivariable-calculus']
73,A Lagrange Mulipliers Problem,A Lagrange Mulipliers Problem,,"My problem is this: Find the min and max values of $f(x,y)=x^2+3xy+y^2$ on the domain $(x-1)^2+y^2=1$. I used lagrange multipliers to find that the $y$ coordinate satisfies $f\left(x\right)=8y+(46y^2)/3-16y^3-24y^4=0$ for any critical point $(x,y)$, and $x=2y^2+(2/3)y$ Using Wolfram Alpha, I found that this has roots (aprox.), $y=-0.97110$ $y=-0.45311$ $y=0.75755$ $y=0$ Wolfram alpha tells me that the function is optimized at the first and third points. However, it is unable to get them in exact form that is not horrendous(multiple radicals involving i). But my professor is expecting an exact answer-and certainly not the exact answer I have. Am I missing something?","My problem is this: Find the min and max values of $f(x,y)=x^2+3xy+y^2$ on the domain $(x-1)^2+y^2=1$. I used lagrange multipliers to find that the $y$ coordinate satisfies $f\left(x\right)=8y+(46y^2)/3-16y^3-24y^4=0$ for any critical point $(x,y)$, and $x=2y^2+(2/3)y$ Using Wolfram Alpha, I found that this has roots (aprox.), $y=-0.97110$ $y=-0.45311$ $y=0.75755$ $y=0$ Wolfram alpha tells me that the function is optimized at the first and third points. However, it is unable to get them in exact form that is not horrendous(multiple radicals involving i). But my professor is expecting an exact answer-and certainly not the exact answer I have. Am I missing something?",,"['multivariable-calculus', 'optimization']"
74,Integral bounds when finding surface are of hyperbolic paraboloid $z=y^2-x^2$ between cylinders by switching to cylindrical coords,Integral bounds when finding surface are of hyperbolic paraboloid  between cylinders by switching to cylindrical coords,z=y^2-x^2,"I came to this problem, and had to look up cylindrical coords since we didn't cover them in any of my classes. But from everything I've read so far I'm still unsure about how they got the 4 and 2 fro the upper and lower bounds of the inner integral. Can someone please explain this?","I came to this problem, and had to look up cylindrical coords since we didn't cover them in any of my classes. But from everything I've read so far I'm still unsure about how they got the 4 and 2 fro the upper and lower bounds of the inner integral. Can someone please explain this?",,"['integration', 'multivariable-calculus', 'partial-derivative', 'surfaces', 'cylindrical-coordinates']"
75,What shapes are described with $\rho = \cos{(\phi)}$ and $\rho = \cos{(2\theta)}$?,What shapes are described with  and ?,\rho = \cos{(\phi)} \rho = \cos{(2\theta)},"I have started an Multivariable course, and I'm learning about spherical coordinates. My problem now is learn how to graph this kind of shapes. This is the problem: What shapes are described when...? Solution: a) $\rho = 1$ : Sphere with radius 1. b) $\phi = \frac{\pi}{3}$ : Cone with angle $\frac{\pi}{3}$. c) $\theta = \frac{\pi}{4}$ : Semi-circular cross-section with diameter along z-axis d) $\rho = \cos{(\phi)}$ : ? e) $\rho = \cos{(2\theta)}$ : ...? Are they correct? How to describe, verbally, the last two -d) and e).","I have started an Multivariable course, and I'm learning about spherical coordinates. My problem now is learn how to graph this kind of shapes. This is the problem: What shapes are described when...? Solution: a) $\rho = 1$ : Sphere with radius 1. b) $\phi = \frac{\pi}{3}$ : Cone with angle $\frac{\pi}{3}$. c) $\theta = \frac{\pi}{4}$ : Semi-circular cross-section with diameter along z-axis d) $\rho = \cos{(\phi)}$ : ? e) $\rho = \cos{(2\theta)}$ : ...? Are they correct? How to describe, verbally, the last two -d) and e).",,['multivariable-calculus']
76,"How to take the ""gradient"" of a matrix?","How to take the ""gradient"" of a matrix?",,"I want to find $(D^2 F)$ where $\vec{F}(\vec{x}) = \frac{\vec{x}}{\|\vec{x}\|}$ ($F$ is a row vector and $Du$ is a ""column vector"", where $u\in \mathbb{R}$ ). I know that $$(DF)_{ij} = \frac{\delta_{ij}\|x\|^2 - 2 x_ix_j}{\|x\|^4}\qquad\rightarrow\qquad (DF) = \frac{I}{\|x\|^2} - \frac{2xx^T}{\|x\|^4}.$$ How exactly do I find $(D^2F)$? I tried reading the section on wikipedia and it hasn't been productive so far.","I want to find $(D^2 F)$ where $\vec{F}(\vec{x}) = \frac{\vec{x}}{\|\vec{x}\|}$ ($F$ is a row vector and $Du$ is a ""column vector"", where $u\in \mathbb{R}$ ). I know that $$(DF)_{ij} = \frac{\delta_{ij}\|x\|^2 - 2 x_ix_j}{\|x\|^4}\qquad\rightarrow\qquad (DF) = \frac{I}{\|x\|^2} - \frac{2xx^T}{\|x\|^4}.$$ How exactly do I find $(D^2F)$? I tried reading the section on wikipedia and it hasn't been productive so far.",,"['calculus', 'linear-algebra', 'multivariable-calculus', 'partial-differential-equations']"
77,Divergence theorem in curvilinear coordinates,Divergence theorem in curvilinear coordinates,,"Suppose I have a tensor \begin{gather} \stackrel{\leftrightarrow}{A} = \begin{bmatrix} a_{11}(\vec{r}) & a_{12}(\vec{r}) & a_{13}(\vec{r})\\ a_{21}(\vec{r}) & a_{22}(\vec{r}) & a_{23}(\vec{r})\\ a_{31}(\vec{r}) & a_{32}(\vec{r}) & a_{33}(\vec{r}) \end{bmatrix} \end{gather} where $\vec{r} = x_{1} \hat{e}_1 + x_{2} \hat{e}_2 + x_{3} \hat{e}_3$ The divergence of this tensor in general curvilinear coordinates is  given by \begin{gather} \nabla^{c} \cdot \stackrel{\leftrightarrow}{A} = \left[   \frac{\partial A_{ij}}{\partial x^{k}} - \Gamma_{ki}^{l} A_{lj} - \Gamma_{kj}^{l} A_{il} \right] g^{ik} \vec{b}^{j}\\ \vec{b}_{i} = \frac{\partial_{x_{i}} \vec{r}}{   \left|     \partial_{x_{i}} \vec{r}   \right| } \end{gather} Using Mathematica, I computed the volume integral of the curvilinear divergence for cylindrical coordinates, giving \begin{gather}   \iiint \nabla^{c} \cdot \stackrel{\leftrightarrow}{A} dV =   \begin{bmatrix}     r \int a_{11} d\theta dz + \int a_{12} dr dz +     \int r a_{13} dr d\theta -     \int a_{22} dr d\theta dz\\     r \int a_{21} d\theta dz + \int a_{22} dr dz +     \int r a_{23} dr d\theta +     \int a_{12} dr d\theta dz\\     r \int a_{31} d\theta dz + \int a_{32} dr dz + \int r a_{33} dr d\theta   \end{bmatrix} \end{gather} This does not match the traditional Divergence theorem I'm familiar with, or at least it doesn't appear so to me because of the extra triple integral terms $\vec{C}$: \begin{gather} \iiint \nabla^{c} \cdot \stackrel{\leftrightarrow}{A} dV = \oint A_{ij} n_j \vec{b}_i dS + \vec{C}\\ \vec{C} \neq 0 \end{gather} What is the correct transformation from the volume integral of the curvilinear divergence to some surface integral?","Suppose I have a tensor \begin{gather} \stackrel{\leftrightarrow}{A} = \begin{bmatrix} a_{11}(\vec{r}) & a_{12}(\vec{r}) & a_{13}(\vec{r})\\ a_{21}(\vec{r}) & a_{22}(\vec{r}) & a_{23}(\vec{r})\\ a_{31}(\vec{r}) & a_{32}(\vec{r}) & a_{33}(\vec{r}) \end{bmatrix} \end{gather} where $\vec{r} = x_{1} \hat{e}_1 + x_{2} \hat{e}_2 + x_{3} \hat{e}_3$ The divergence of this tensor in general curvilinear coordinates is  given by \begin{gather} \nabla^{c} \cdot \stackrel{\leftrightarrow}{A} = \left[   \frac{\partial A_{ij}}{\partial x^{k}} - \Gamma_{ki}^{l} A_{lj} - \Gamma_{kj}^{l} A_{il} \right] g^{ik} \vec{b}^{j}\\ \vec{b}_{i} = \frac{\partial_{x_{i}} \vec{r}}{   \left|     \partial_{x_{i}} \vec{r}   \right| } \end{gather} Using Mathematica, I computed the volume integral of the curvilinear divergence for cylindrical coordinates, giving \begin{gather}   \iiint \nabla^{c} \cdot \stackrel{\leftrightarrow}{A} dV =   \begin{bmatrix}     r \int a_{11} d\theta dz + \int a_{12} dr dz +     \int r a_{13} dr d\theta -     \int a_{22} dr d\theta dz\\     r \int a_{21} d\theta dz + \int a_{22} dr dz +     \int r a_{23} dr d\theta +     \int a_{12} dr d\theta dz\\     r \int a_{31} d\theta dz + \int a_{32} dr dz + \int r a_{33} dr d\theta   \end{bmatrix} \end{gather} This does not match the traditional Divergence theorem I'm familiar with, or at least it doesn't appear so to me because of the extra triple integral terms $\vec{C}$: \begin{gather} \iiint \nabla^{c} \cdot \stackrel{\leftrightarrow}{A} dV = \oint A_{ij} n_j \vec{b}_i dS + \vec{C}\\ \vec{C} \neq 0 \end{gather} What is the correct transformation from the volume integral of the curvilinear divergence to some surface integral?",,"['multivariable-calculus', 'vector-analysis', 'coordinate-systems', 'tensors']"
78,Integration between circle and ellipse,Integration between circle and ellipse,,"I need to evaluate an integral over the $D=\{x^2+y^2 >1; \frac{x^2}{a^2}+\frac{y^2}{b^2}<1\}$, but I can't find the limits of integration simply by changing to polar coordinates. Thanks","I need to evaluate an integral over the $D=\{x^2+y^2 >1; \frac{x^2}{a^2}+\frac{y^2}{b^2}<1\}$, but I can't find the limits of integration simply by changing to polar coordinates. Thanks",,"['integration', 'multivariable-calculus']"
79,Improper multiple integals involving inner porduct,Improper multiple integals involving inner porduct,,"I'm trying to evaluate the following expression: $$\int_{\mathbb{R}^n} \langle x,a \rangle^2e^{-\frac{1}{2}\lvert x \rvert^2}dx$$ where $x,a \in \mathbb{R}^n$. I tried reduction to the case where $a$ is an element of the ""natural basis"" in $\mathbb{R}^n$ but I can't see how to go from there.","I'm trying to evaluate the following expression: $$\int_{\mathbb{R}^n} \langle x,a \rangle^2e^{-\frac{1}{2}\lvert x \rvert^2}dx$$ where $x,a \in \mathbb{R}^n$. I tried reduction to the case where $a$ is an element of the ""natural basis"" in $\mathbb{R}^n$ but I can't see how to go from there.",,"['calculus', 'linear-algebra', 'integration', 'multivariable-calculus']"
80,Multivariable chain rule exercise,Multivariable chain rule exercise,,"$F:\mathbb{R}^2\rightarrow\mathbb{R}$ is a $C^2$ function with $F_x(1,1)=F_{yy}(1,1)=1$ and $F_y(1,1)=F_{xx}(1,1)=0$ and $g:\mathbb{R}^2\rightarrow\mathbb{R}$ such that $g(r,\theta)=F(r\cos\theta,r\sin\theta)$. I'm asked to find the value of $g_{r\theta}(\sqrt2,\pi/4)$. This what I understand about the chain rule and what I've done so far. I can see $g$ as the composition of $F$ and $M=(r\cos\theta,r\sin\theta)$. $M$ is differentiable at $(\sqrt2,\pi/4)$ and $F$ is differentiable at $M(\sqrt2,\pi/4)=(1,1)$ because its partial derivatives exist and are continuous. The chain rule tells me that (*) $\frac{\partial g}{\partial r}=\frac{\partial f}{\partial x}\frac{\partial x}{\partial r}+\frac{\partial f}{\partial y}\frac{\partial y}{\partial r}=\frac{\partial f}{\partial x}\cos\theta+\frac{\partial f}{\partial y}\sin\theta$. Then I have to take the derivative of that with respect to $\theta$, and I don't know how to do that. EDIT: This is a silly question, but I'd also like to know what are the points of evaluation in (*) and/or the chain rule thesis in general. My textbook says it's understood, but I'm not too good at this. English is not my first language, so feel free to correct any mistake.","$F:\mathbb{R}^2\rightarrow\mathbb{R}$ is a $C^2$ function with $F_x(1,1)=F_{yy}(1,1)=1$ and $F_y(1,1)=F_{xx}(1,1)=0$ and $g:\mathbb{R}^2\rightarrow\mathbb{R}$ such that $g(r,\theta)=F(r\cos\theta,r\sin\theta)$. I'm asked to find the value of $g_{r\theta}(\sqrt2,\pi/4)$. This what I understand about the chain rule and what I've done so far. I can see $g$ as the composition of $F$ and $M=(r\cos\theta,r\sin\theta)$. $M$ is differentiable at $(\sqrt2,\pi/4)$ and $F$ is differentiable at $M(\sqrt2,\pi/4)=(1,1)$ because its partial derivatives exist and are continuous. The chain rule tells me that (*) $\frac{\partial g}{\partial r}=\frac{\partial f}{\partial x}\frac{\partial x}{\partial r}+\frac{\partial f}{\partial y}\frac{\partial y}{\partial r}=\frac{\partial f}{\partial x}\cos\theta+\frac{\partial f}{\partial y}\sin\theta$. Then I have to take the derivative of that with respect to $\theta$, and I don't know how to do that. EDIT: This is a silly question, but I'd also like to know what are the points of evaluation in (*) and/or the chain rule thesis in general. My textbook says it's understood, but I'm not too good at this. English is not my first language, so feel free to correct any mistake.",,['multivariable-calculus']
81,Existence of a Function?,Existence of a Function?,,"I'm trying to find a smooth function $f: \mathbb{R} \times (0, \infty) \rightarrow \mathbb{R}$ which satisfies the following conditions: 1) $\lim_{t \rightarrow 0^+} t\cdot \left(\frac{\partial f}{\partial x}\right)^2$ exists and is not identically $0$ as a function of $x$. 2) $\lim_{t \rightarrow 0^+}t\cdot \left(\frac{\partial f}{\partial x}\right)\left(\frac{\partial f}{\partial t}\right)$ exists for all $x$. 3)  $\lim_{t \rightarrow 0^+} t\cdot \left(\frac{\partial f}{\partial t}\right)^2$ exists for all $x$. I've proved that $f$ can't be separable (i.e.  $f(x,t)$ can't be of the form $f(x,t) = X(x)T(t)$). In fact I'm starting to believe that no such function can exist, but I haven't been able to prove it. Any help would be greatly appreciated!","I'm trying to find a smooth function $f: \mathbb{R} \times (0, \infty) \rightarrow \mathbb{R}$ which satisfies the following conditions: 1) $\lim_{t \rightarrow 0^+} t\cdot \left(\frac{\partial f}{\partial x}\right)^2$ exists and is not identically $0$ as a function of $x$. 2) $\lim_{t \rightarrow 0^+}t\cdot \left(\frac{\partial f}{\partial x}\right)\left(\frac{\partial f}{\partial t}\right)$ exists for all $x$. 3)  $\lim_{t \rightarrow 0^+} t\cdot \left(\frac{\partial f}{\partial t}\right)^2$ exists for all $x$. I've proved that $f$ can't be separable (i.e.  $f(x,t)$ can't be of the form $f(x,t) = X(x)T(t)$). In fact I'm starting to believe that no such function can exist, but I haven't been able to prove it. Any help would be greatly appreciated!",,"['calculus', 'multivariable-calculus']"
82,Multivariable Calculus problem: studying continuity of a function,Multivariable Calculus problem: studying continuity of a function,,"I'm trying to solve the following problem: Let $\varphi : \mathbb{R}\to\mathbb{R}$ differentiable, and $\varphi'(x)$ continuous; $f:\mathbb{R}^2\to\mathbb{R}$ given by $\begin{equation*}      f(x,y) = \begin{cases}                 \frac{\varphi(y) - \varphi(x)}{y-x}         & x \neq y\\                 \varphi'(y)               & x = y\\             \end{cases} \end{equation*} $ Determine in which points $f$ is continuous. If $x \neq y$, $f$ is clearly continuous because is sum and product of continuous functions. If $x = y$, the problem is to check if: $\lim_{(x,y)\to(x_0,x_0)} f(x,y) = \varphi'(x_0)$. Let $A= \{ (x,y)\in\mathbb{R}^2:x=y \}$, it's obvious that    $\lim_{(x,y)\to(x_0,x_0)} f|_A(x,y) = \varphi'(x_0)$, because $\varphi'$ is continuous. But what about $f|_{A^C}$? I tried unsuccessfully to use the definition of $\varphi'(x_0)$ to solve the limit. Any suggestion? Thanks.","I'm trying to solve the following problem: Let $\varphi : \mathbb{R}\to\mathbb{R}$ differentiable, and $\varphi'(x)$ continuous; $f:\mathbb{R}^2\to\mathbb{R}$ given by $\begin{equation*}      f(x,y) = \begin{cases}                 \frac{\varphi(y) - \varphi(x)}{y-x}         & x \neq y\\                 \varphi'(y)               & x = y\\             \end{cases} \end{equation*} $ Determine in which points $f$ is continuous. If $x \neq y$, $f$ is clearly continuous because is sum and product of continuous functions. If $x = y$, the problem is to check if: $\lim_{(x,y)\to(x_0,x_0)} f(x,y) = \varphi'(x_0)$. Let $A= \{ (x,y)\in\mathbb{R}^2:x=y \}$, it's obvious that    $\lim_{(x,y)\to(x_0,x_0)} f|_A(x,y) = \varphi'(x_0)$, because $\varphi'$ is continuous. But what about $f|_{A^C}$? I tried unsuccessfully to use the definition of $\varphi'(x_0)$ to solve the limit. Any suggestion? Thanks.",,"['calculus', 'multivariable-calculus', 'continuity']"
83,Volume Integral of this set,Volume Integral of this set,,"I'm not sure about this exercise. Be: $$E=\left\{(x,y,z) \in \mathbb{R^3} : x\geq 0, y\geq 0, 0\leq z\leq \frac{1}{\sqrt{x^2+y^2}}-1\right\}$$ Find: $$\int_{E} z\, \max\{x,y\}\: dx \, dy \, dz$$ My idea is that since $x,y\geq 0$, one of the two variables will be greater than the other in a section of the quadrant divided by the bisector and vice versa. For example, let's suppose that $x\geq y$, the  section of the volume would be something like: $$\left\{(x,y,z)\in \mathbb{R^3}  : 0\leq x\leq \infty, 0\leq y\leq x,0\leq z\leq \frac{1}{\sqrt{x^2+y^2}}-1\right\}$$ The volume of this would be: $$\int_0^\infty\int_0^x\int_0^{\frac{1}{\sqrt{x^2+y^2}}-1} zx\: dz\, dy\, dx$$  Then the volume of $E$ would be this volume plus the volume of the case where $y\geq x$, does this look correct to you?","I'm not sure about this exercise. Be: $$E=\left\{(x,y,z) \in \mathbb{R^3} : x\geq 0, y\geq 0, 0\leq z\leq \frac{1}{\sqrt{x^2+y^2}}-1\right\}$$ Find: $$\int_{E} z\, \max\{x,y\}\: dx \, dy \, dz$$ My idea is that since $x,y\geq 0$, one of the two variables will be greater than the other in a section of the quadrant divided by the bisector and vice versa. For example, let's suppose that $x\geq y$, the  section of the volume would be something like: $$\left\{(x,y,z)\in \mathbb{R^3}  : 0\leq x\leq \infty, 0\leq y\leq x,0\leq z\leq \frac{1}{\sqrt{x^2+y^2}}-1\right\}$$ The volume of this would be: $$\int_0^\infty\int_0^x\int_0^{\frac{1}{\sqrt{x^2+y^2}}-1} zx\: dz\, dy\, dx$$  Then the volume of $E$ would be this volume plus the volume of the case where $y\geq x$, does this look correct to you?",,"['calculus', 'integration', 'multivariable-calculus', 'volume']"
84,Find the angle between two planes using their normal vectors,Find the angle between two planes using their normal vectors,,"The angle between two intersecting planes is defined to be the angle between   their normal vectors. Find the angle between the planes $x – 2y + z = 0$ and $2x + 3y – 2z = 0$ . Find the parametric equations of the line of intersection of the two planes above. For the first plane I said $\overrightarrow n_0 =\langle 1, -2, 1 \rangle$ and for the second plane $\overrightarrow n_1 = \langle 2, 3, -2 \rangle$ Then using $\cos(\theta)= \frac{\mathbf A \bullet \mathbf B}{|\mathbf A||\mathbf B|}$ so $\theta = \cos^{-1}\left(\frac{\mathbf A \bullet \mathbf B}{|\mathbf A||\mathbf B|}\right)$ $\mathbf A \bullet \mathbf B = -6$ $|\mathbf A||\mathbf B|= \sqrt{102}$ $\theta = \cos^{-1}\left(\frac{-6}{\sqrt{102}}\right)$ Assuming this is correct so far, how do I find the parametric equations from here?","The angle between two intersecting planes is defined to be the angle between   their normal vectors. Find the angle between the planes and . Find the parametric equations of the line of intersection of the two planes above. For the first plane I said and for the second plane Then using so Assuming this is correct so far, how do I find the parametric equations from here?","x – 2y + z = 0 2x + 3y – 2z = 0 \overrightarrow n_0 =\langle 1, -2, 1 \rangle \overrightarrow n_1 = \langle 2, 3, -2 \rangle \cos(\theta)= \frac{\mathbf A \bullet \mathbf B}{|\mathbf A||\mathbf B|} \theta = \cos^{-1}\left(\frac{\mathbf A \bullet \mathbf B}{|\mathbf A||\mathbf B|}\right) \mathbf A \bullet \mathbf B = -6 |\mathbf A||\mathbf B|= \sqrt{102} \theta = \cos^{-1}\left(\frac{-6}{\sqrt{102}}\right)","['multivariable-calculus', 'parametric', 'plane-curves']"
85,Derivative of continuous multilinear maps,Derivative of continuous multilinear maps,,"Suppose, $E_1,\dots, E_n$, $F$ are are complete normed linear spaces(Banach spaces). Suppose $f:E_1\times\dots\times E_n\to F$ be a continuous multilinear map. I need to find its derivative $Df(x)(h)$, with $x,h \in E_1\times\dots\times E_n$. I tried expanding $f(x_1+h_1,\dots, x_n+h_n)$ for $n=3$ and I got $\displaystyle f(x_1+h_1,\dots, x_n+h_n)=\sum_{\pi_i\in \{x_i,h_i\}}f(\pi_1,\dots,\pi_n)$. So, my guess is that $Df(x)(h)$ is $\displaystyle \sum_{\pi_i\in \{x_i,h_i\}}f(\pi_1,\dots,\pi_n)-f(x_1,\dots,x_n)$ and so I need to show that $f(h_1,\dots,h_n)=o(||(h_1,\dots,h_n)||)$ but I am not sure how to prove it. I tried using the max norm i.e. suppose $||(h_1,\dots,h_n)||=||h_k||$ which means $\displaystyle f(h_1,\dots,h_n)/||(h_1,\dots,h_n)||=f(h_1,\dots,\frac{h_k}{||h_k||},\dots,h_n)$ but I don't know how to proceed further. Thanks.","Suppose, $E_1,\dots, E_n$, $F$ are are complete normed linear spaces(Banach spaces). Suppose $f:E_1\times\dots\times E_n\to F$ be a continuous multilinear map. I need to find its derivative $Df(x)(h)$, with $x,h \in E_1\times\dots\times E_n$. I tried expanding $f(x_1+h_1,\dots, x_n+h_n)$ for $n=3$ and I got $\displaystyle f(x_1+h_1,\dots, x_n+h_n)=\sum_{\pi_i\in \{x_i,h_i\}}f(\pi_1,\dots,\pi_n)$. So, my guess is that $Df(x)(h)$ is $\displaystyle \sum_{\pi_i\in \{x_i,h_i\}}f(\pi_1,\dots,\pi_n)-f(x_1,\dots,x_n)$ and so I need to show that $f(h_1,\dots,h_n)=o(||(h_1,\dots,h_n)||)$ but I am not sure how to prove it. I tried using the max norm i.e. suppose $||(h_1,\dots,h_n)||=||h_k||$ which means $\displaystyle f(h_1,\dots,h_n)/||(h_1,\dots,h_n)||=f(h_1,\dots,\frac{h_k}{||h_k||},\dots,h_n)$ but I don't know how to proceed further. Thanks.",,['real-analysis']
86,Basis-Independent Definition of Gradient of Multivector Fields,Basis-Independent Definition of Gradient of Multivector Fields,,"The definition of the gradient operator on multivector fields in geometric calculus seems to be $$\nabla = \sum_i e_i\partial_i$$ where $\{e_i\}$ is an orthonormal basis. That's useful, but it's inconvenient to have to derive the equivalent expressions for the gradient in a non-Cartesian basis. Is there a basis independent definition of the gradient, similar to the one from vector calculus $$\nabla f = \lim_{\Delta V \to 0} \frac 1{\Delta V}\iint_{S} \hat n f\ dS $$ for multivector fields?","The definition of the gradient operator on multivector fields in geometric calculus seems to be $$\nabla = \sum_i e_i\partial_i$$ where $\{e_i\}$ is an orthonormal basis. That's useful, but it's inconvenient to have to derive the equivalent expressions for the gradient in a non-Cartesian basis. Is there a basis independent definition of the gradient, similar to the one from vector calculus $$\nabla f = \lim_{\Delta V \to 0} \frac 1{\Delta V}\iint_{S} \hat n f\ dS $$ for multivector fields?",,"['multivariable-calculus', 'definition', 'geometric-algebras']"
87,"If $f(x,y)$ is continuous in $(x_0,y_0)$, then there is a neighborhood of $(x_0,y_0)$ such that $f(x,y)>\frac12f(x_0,y_0)$","If  is continuous in , then there is a neighborhood of  such that","f(x,y) (x_0,y_0) (x_0,y_0) f(x,y)>\frac12f(x_0,y_0)","The exercise asks me to prove 2 things: 1) $f(x,y) $ is continuous in $(x_0,y_0)$, $f(x_0,y_0)>0$ then there is a neighborhood such that $f(x,y)>\frac{1}{2}f(x_0,y_0)$ My idea: $f$ is continuous, then $|(x,y)-(x_0,y_0)|< \delta\implies |f(x,y)-f(x_0,y_0)|< \epsilon \implies f(x_0,y_0) -\epsilon <f(x,y) < f(x_0,y_0) + \epsilon$. It's true for all $\epsilon$, so if I choose $\epsilon = \frac{1}{2}f(x_0,y_0)$ we have: $$\frac{1}{2}f(x_0,y_0)<f(x,y) < f(x_0,y_0) + \frac{1}{2}f(x_0,y_0)$$ 2) Suppose $f$ is continuous in a domain $D$. Suppose that $f(x,y)$ is positive for at least $1$ point of $D$ and negative for at least one point of D. Then $f(x,y) = 0$ for at least one point of $D$. (suggestion: use $1$) How to use $1$ to prove $2$? As I know, this can be understood as the mean value theorem for multivariables, but I couldn't find a proof that used $1$.","The exercise asks me to prove 2 things: 1) $f(x,y) $ is continuous in $(x_0,y_0)$, $f(x_0,y_0)>0$ then there is a neighborhood such that $f(x,y)>\frac{1}{2}f(x_0,y_0)$ My idea: $f$ is continuous, then $|(x,y)-(x_0,y_0)|< \delta\implies |f(x,y)-f(x_0,y_0)|< \epsilon \implies f(x_0,y_0) -\epsilon <f(x,y) < f(x_0,y_0) + \epsilon$. It's true for all $\epsilon$, so if I choose $\epsilon = \frac{1}{2}f(x_0,y_0)$ we have: $$\frac{1}{2}f(x_0,y_0)<f(x,y) < f(x_0,y_0) + \frac{1}{2}f(x_0,y_0)$$ 2) Suppose $f$ is continuous in a domain $D$. Suppose that $f(x,y)$ is positive for at least $1$ point of $D$ and negative for at least one point of D. Then $f(x,y) = 0$ for at least one point of $D$. (suggestion: use $1$) How to use $1$ to prove $2$? As I know, this can be understood as the mean value theorem for multivariables, but I couldn't find a proof that used $1$.",,"['calculus', 'multivariable-calculus']"
88,Relearning multivariable calculus through differential forms,Relearning multivariable calculus through differential forms,,"While I learned multivariable calculus a few years ago, I have never felt I understand it well enough. Now I have time to go back and correct this. Since I have been through subjects like real analysis, I want this understanding to be more rigorous than the more intuition based learning that I initially encountered. That lead me to consider differential form. Is it any good idea to learn things like line integrals, Lagrange multipliers and the like through differential forms? Things that bother me the most about my understanding of multivariable calculus is, for instance, while I do know the purpose of the Jacobian in changing variables, I cannot prove it. In fact, beyond an intuitive understanding of what it does, I cannot show what exactly it does. Also, I have very little understanding of the equivalents of the fundamental theorem of calculus in multivariable calculus. So, considering this... Should I embark upon trying to relearn calculus through differential forms? I am considering A Geometric Approach to Differential Forms by David Bachman.","While I learned multivariable calculus a few years ago, I have never felt I understand it well enough. Now I have time to go back and correct this. Since I have been through subjects like real analysis, I want this understanding to be more rigorous than the more intuition based learning that I initially encountered. That lead me to consider differential form. Is it any good idea to learn things like line integrals, Lagrange multipliers and the like through differential forms? Things that bother me the most about my understanding of multivariable calculus is, for instance, while I do know the purpose of the Jacobian in changing variables, I cannot prove it. In fact, beyond an intuitive understanding of what it does, I cannot show what exactly it does. Also, I have very little understanding of the equivalents of the fundamental theorem of calculus in multivariable calculus. So, considering this... Should I embark upon trying to relearn calculus through differential forms? I am considering A Geometric Approach to Differential Forms by David Bachman.",,"['multivariable-calculus', 'differential-forms']"
89,Evaluating the line integral $\int_C{F\cdot dr}$ for a particular conservative vector field $F$,Evaluating the line integral  for a particular conservative vector field,\int_C{F\cdot dr} F,"So I have this two dimensional vector field: $$F=\langle (1+xy)e^{xy},x^2e^{xy}\rangle$$ How can I tell whether $F$ is conservative or not? And also how do I calculate $\int_C{F\cdot dr}$, where $C$ is $x^2+y^2=1, y\ge 0$ and oriented to the right. So far I think that we have to first find the partial derivatives first. Not sure how to proceed though. I have figured out that it is conservative by finding the partial derivatives, but I'm having trouble with the second calculation.","So I have this two dimensional vector field: $$F=\langle (1+xy)e^{xy},x^2e^{xy}\rangle$$ How can I tell whether $F$ is conservative or not? And also how do I calculate $\int_C{F\cdot dr}$, where $C$ is $x^2+y^2=1, y\ge 0$ and oriented to the right. So far I think that we have to first find the partial derivatives first. Not sure how to proceed though. I have figured out that it is conservative by finding the partial derivatives, but I'm having trouble with the second calculation.",,['multivariable-calculus']
90,Find a unit tangent vector to a curve that is an intersection of two surfaces.,Find a unit tangent vector to a curve that is an intersection of two surfaces.,,"The intersection of the two surfaces given by the Cartesian equations $2x^2+3y^2-z^2=25$ and $x^2+y^2=z^2$ contains a curve $C$ passing through the point $P=(\sqrt{7},3,4)$. These equations may be solved for $x$ and $y$ in terms of $z$ to give a parametric representation of $C$ with $z$ as a parameter. (a) Find a unit tangent vector $T$ to $C$ at the point $P$ without using an explicit knowledge of the parametric representation. (b) Check the result in part (a) by determining a parametric representation of $C$ with $z$ as a parameter. For (b), I solved the two equations for the surfaces given to get $y^2=25-z^2$ and $x^2=2z^2-25$. Since we're looking for the curve that contains $P$, $x$, $y$ should be positive so we get $y=\sqrt{25-z^2}$ and $x=\sqrt{2z^2-25}$. So from this I get the parametric representation $(\sqrt{2z^2-25}, \sqrt{25-z^2},z)$ for the curve $C$. Is this the correct way of finding the parametrization? Moreover, I do not know how to find the unit targent vector $T$ to $C$ at $P$, without getting a parametrization. How can I find this? The answer to $T$ is $\frac{1}{\sqrt{751}}(24,-4\sqrt{7},3\sqrt{7})$. I would greatly appreciate any solutions, hints or suggestions.","The intersection of the two surfaces given by the Cartesian equations $2x^2+3y^2-z^2=25$ and $x^2+y^2=z^2$ contains a curve $C$ passing through the point $P=(\sqrt{7},3,4)$. These equations may be solved for $x$ and $y$ in terms of $z$ to give a parametric representation of $C$ with $z$ as a parameter. (a) Find a unit tangent vector $T$ to $C$ at the point $P$ without using an explicit knowledge of the parametric representation. (b) Check the result in part (a) by determining a parametric representation of $C$ with $z$ as a parameter. For (b), I solved the two equations for the surfaces given to get $y^2=25-z^2$ and $x^2=2z^2-25$. Since we're looking for the curve that contains $P$, $x$, $y$ should be positive so we get $y=\sqrt{25-z^2}$ and $x=\sqrt{2z^2-25}$. So from this I get the parametric representation $(\sqrt{2z^2-25}, \sqrt{25-z^2},z)$ for the curve $C$. Is this the correct way of finding the parametrization? Moreover, I do not know how to find the unit targent vector $T$ to $C$ at $P$, without getting a parametrization. How can I find this? The answer to $T$ is $\frac{1}{\sqrt{751}}(24,-4\sqrt{7},3\sqrt{7})$. I would greatly appreciate any solutions, hints or suggestions.",,"['multivariable-calculus', 'vector-analysis']"
91,Flux through a paraboloid.,Flux through a paraboloid.,,"Let S be the surface formed by the part of the paraboloid $z = 1- x^2-y^2$ lying above the $xy$ -plane and let $\vec F= x\hat i + y\hat j+2(1-z) \hat k$ . Calculate the flux of $\vec F$ across S, taking the upward direction as the one for which the flux is positive. Do this in two ways: a) By direct calculation of flux by $\iint_s \vec F .\hat n \;dS$ . b) By computing the flux of $\vec F$ across a simpler surface and using the divergence theorem. I am quite new to multi-variable ,so please bear with me. The problem i am having is for part a) , however i have tried part b) as follow. I know that, $\text{div}\; \vec F=0$ . Hence we can imagine a imaginary surface $x^2+y^2\leq1$ at $z=0$ with normal vector $ \hat k$ And the surface S already present. Combining them to create a closed surface through which flux will be zero as. $\iiint \text{div}\; \vec F . dV=F_1+F_2$ where, $F_1$ is considered as flux through as Paraboloid surface $S$ and $F_2$ is through the circular disc described. So $F_1=-F_2$ . Now, flux through circular disc is inward hence negative. $F_2=-\iint \vec F . \hat k \; dA$ gives $F_2=-\iint 2 . dA= -2\pi$ and $F_1=2 \pi$ . For part a), Here is what i tried, I tried to evaluate this in polar form, firstly i find normal in cartesian to paraboloid, which is given as. $\hat n = \frac{2x \hat i + 2y \hat j + \hat k}{\sqrt{1+4x^2+4y^2}}$ . Computing $\vec F . \hat n = \frac{2x^2+2y^2+2(1-z)}{\sqrt{1+4x^2+4y^2}}=\frac{4(1-z)}{\sqrt{5-4z}}$ , since all this flux is evaluated on $S$ . Now converting this to polar coordinates in 3D $<r\cos \theta \sin \phi , r \sin \theta \sin \phi , r \cos \phi>$ . and $dS=r^2 \sin \phi d\theta d\phi$ . Gives $F_1=\iint_{s}\frac{4(1-r \cos \phi)}{\sqrt{5-4r\cos \phi}}. r^2 \sin \phi d \theta d \phi $ . I tried to solve for $r$ in terms of $\phi$ giving $r=\frac{2}{\sqrt{1+3\sin^2 \phi}+\cos \phi}$ (after removing discontinuity and solving for $r$ by equation of paraboloid, taking positive root.) Add : Here's how i did this, take the equation of paraboloid we get, $r^2 \sin^2 \phi = 1- r\cos \phi$ , And solving quadratic equation in terms of $r$ . Taking the positive root. Since $r$ is independent of $\theta$ our integral becomes. $F_1= 8 \pi \int_{0}^{\pi/2}\frac{1-r \cos \phi}{\sqrt{5-4r \cos \phi}}\times r^2 \sin \phi d\phi $ . This is where i get stuck, I have no idea from here on. If this is relevant, http://ocw.mit.edu/courses/mathematics/18-02-multivariable-calculus-fall-2007/exams/prac4b.pdf","Let S be the surface formed by the part of the paraboloid lying above the -plane and let . Calculate the flux of across S, taking the upward direction as the one for which the flux is positive. Do this in two ways: a) By direct calculation of flux by . b) By computing the flux of across a simpler surface and using the divergence theorem. I am quite new to multi-variable ,so please bear with me. The problem i am having is for part a) , however i have tried part b) as follow. I know that, . Hence we can imagine a imaginary surface at with normal vector And the surface S already present. Combining them to create a closed surface through which flux will be zero as. where, is considered as flux through as Paraboloid surface and is through the circular disc described. So . Now, flux through circular disc is inward hence negative. gives and . For part a), Here is what i tried, I tried to evaluate this in polar form, firstly i find normal in cartesian to paraboloid, which is given as. . Computing , since all this flux is evaluated on . Now converting this to polar coordinates in 3D . and . Gives . I tried to solve for in terms of giving (after removing discontinuity and solving for by equation of paraboloid, taking positive root.) Add : Here's how i did this, take the equation of paraboloid we get, , And solving quadratic equation in terms of . Taking the positive root. Since is independent of our integral becomes. . This is where i get stuck, I have no idea from here on. If this is relevant, http://ocw.mit.edu/courses/mathematics/18-02-multivariable-calculus-fall-2007/exams/prac4b.pdf","z = 1- x^2-y^2 xy \vec F= x\hat i + y\hat j+2(1-z) \hat k \vec F \iint_s \vec F .\hat n \;dS \vec F \text{div}\; \vec F=0 x^2+y^2\leq1 z=0  \hat k \iiint \text{div}\; \vec F . dV=F_1+F_2 F_1 S F_2 F_1=-F_2 F_2=-\iint \vec F . \hat k \; dA F_2=-\iint 2 . dA= -2\pi F_1=2 \pi \hat n = \frac{2x \hat i + 2y \hat j + \hat k}{\sqrt{1+4x^2+4y^2}} \vec F . \hat n = \frac{2x^2+2y^2+2(1-z)}{\sqrt{1+4x^2+4y^2}}=\frac{4(1-z)}{\sqrt{5-4z}} S <r\cos \theta \sin \phi , r \sin \theta \sin \phi , r \cos \phi> dS=r^2 \sin \phi d\theta d\phi F_1=\iint_{s}\frac{4(1-r \cos \phi)}{\sqrt{5-4r\cos \phi}}. r^2 \sin \phi d \theta d \phi  r \phi r=\frac{2}{\sqrt{1+3\sin^2 \phi}+\cos \phi} r r^2 \sin^2 \phi = 1- r\cos \phi r r \theta F_1= 8 \pi \int_{0}^{\pi/2}\frac{1-r \cos \phi}{\sqrt{5-4r \cos \phi}}\times r^2 \sin \phi d\phi ",['multivariable-calculus']
92,Volume integral in $R^3$,Volume integral in,R^3,"Compute the volume of the body defined by the inequalities $$x^2+y^2 \leq 4x, \, |z| \leq x^2+y^2 \\$$ I write the first inequality as $(x-2)^2+y^2 \leq 4$ so it is a disk with radius $2$ and centrum in $(2,0)$. The second inequality is a paraboloid but I don't see how the absolute value of $z$ affects the figure. If $z^+ \leq x^2+y^2$ then $z^- \leq z^2+y^2$ so I don't see the difference it makes. Also, what method is preferable when computing this type of body? As a difference between two double integrals?","Compute the volume of the body defined by the inequalities $$x^2+y^2 \leq 4x, \, |z| \leq x^2+y^2 \\$$ I write the first inequality as $(x-2)^2+y^2 \leq 4$ so it is a disk with radius $2$ and centrum in $(2,0)$. The second inequality is a paraboloid but I don't see how the absolute value of $z$ affects the figure. If $z^+ \leq x^2+y^2$ then $z^- \leq z^2+y^2$ so I don't see the difference it makes. Also, what method is preferable when computing this type of body? As a difference between two double integrals?",,['multivariable-calculus']
93,Lagrange multipliers method - absolute maximum and minimum,Lagrange multipliers method - absolute maximum and minimum,,"Using the Lagrange multipliers method I have to find the absolute maximum and minimum value of $f(x, y)=x^2+y^2-x-y+1$ in the unit disc. So, I have to find the extremas of $f(x, y)=x^2+y^2-x-y+1$ subject to $x^2+y^2 \leq 1$, or not?? Do we not apply Lagrange multipliers method when we have a function $f(x,y)$ and a constaint $g(x, y)=0$?? So, shouldn't we have to have an equality at the constraint?? But in this case we have an inequality... What do we do??","Using the Lagrange multipliers method I have to find the absolute maximum and minimum value of $f(x, y)=x^2+y^2-x-y+1$ in the unit disc. So, I have to find the extremas of $f(x, y)=x^2+y^2-x-y+1$ subject to $x^2+y^2 \leq 1$, or not?? Do we not apply Lagrange multipliers method when we have a function $f(x,y)$ and a constaint $g(x, y)=0$?? So, shouldn't we have to have an equality at the constraint?? But in this case we have an inequality... What do we do??",,"['multivariable-calculus', 'optimization', 'lagrange-multiplier']"
94,Spivak Calculus on Manifolds: Problem 2-13,Spivak Calculus on Manifolds: Problem 2-13,,"I'm going through Spivak's Calculus on Manifolds, and I'm currently working on Problem 2-13 part (b). The problem statement is If $f,g: \mathbb{R} \rightarrow \mathbb{R}^{n}$ are differentiable and $h: \mathbb{R} \rightarrow \mathbb{R}$ is defined by $h(t) = \langle f(t),g(t)\rangle$, show that   $h'(a) = \langle f'(a)^T,g(a) \rangle + \langle f(a),g'(a)^T \rangle.$ What's confusing me is that $f'(a)$ is a vector in $\mathbb{R}^{n}$ and taking its transpose yields a vector in the dual space to $\mathbb{R}^{n}$, so taking the inner product with $g(a)$, which is a vector in $\mathbb{R}^{n}$ yields a scalar in $\mathbb{R}$, corresponding to the left term in the equation. Whereas the right term in the equation is an outer product, so it sends a vector and a dual vector to a linear map in Hom($\mathbb{R}^{n}$), since the vector $f(a)$ is $n\times 1$ and the dual vector $g'(a)^T$ is $1\times n$ so their product is $n\times n$. How can these two terms be added to yield a scalar in $\mathbb{R}$?","I'm going through Spivak's Calculus on Manifolds, and I'm currently working on Problem 2-13 part (b). The problem statement is If $f,g: \mathbb{R} \rightarrow \mathbb{R}^{n}$ are differentiable and $h: \mathbb{R} \rightarrow \mathbb{R}$ is defined by $h(t) = \langle f(t),g(t)\rangle$, show that   $h'(a) = \langle f'(a)^T,g(a) \rangle + \langle f(a),g'(a)^T \rangle.$ What's confusing me is that $f'(a)$ is a vector in $\mathbb{R}^{n}$ and taking its transpose yields a vector in the dual space to $\mathbb{R}^{n}$, so taking the inner product with $g(a)$, which is a vector in $\mathbb{R}^{n}$ yields a scalar in $\mathbb{R}$, corresponding to the left term in the equation. Whereas the right term in the equation is an outer product, so it sends a vector and a dual vector to a linear map in Hom($\mathbb{R}^{n}$), since the vector $f(a)$ is $n\times 1$ and the dual vector $g'(a)^T$ is $1\times n$ so their product is $n\times n$. How can these two terms be added to yield a scalar in $\mathbb{R}$?",,"['linear-algebra', 'multivariable-calculus']"
95,Conceptual explanation of integral of divergence.,Conceptual explanation of integral of divergence.,,"$\textbf{My understanding of divergence:}$ Consider any vector field $\textbf{u}$, then $\operatorname{div}(u) = \nabla \cdot u$. More conceptually, if I place an arbitrarily small sphere around any point of the vector field $\textbf{u}$, divergence measures the amount of ""particles"" exiting the sphere, i.e.  positive divergence represent a vector field which is ""moving faster"" as we move to the right. However, how do I interpret $$ \int_U \operatorname{div}(u) \, dx$$ where $U$ is any bounded open subset of $\mathbb{R}^n$.","$\textbf{My understanding of divergence:}$ Consider any vector field $\textbf{u}$, then $\operatorname{div}(u) = \nabla \cdot u$. More conceptually, if I place an arbitrarily small sphere around any point of the vector field $\textbf{u}$, divergence measures the amount of ""particles"" exiting the sphere, i.e.  positive divergence represent a vector field which is ""moving faster"" as we move to the right. However, how do I interpret $$ \int_U \operatorname{div}(u) \, dx$$ where $U$ is any bounded open subset of $\mathbb{R}^n$.",,['multivariable-calculus']
96,How to calculate this multivariable limit?,How to calculate this multivariable limit?,,"$$ \lim_{(x,y,z)\to (0,0,0) } \frac{\sin(x^2+y^2+z^2) + \tan(x+y+z) }{|x|+|y|+|z|} $$ I know the entire limit should not exist. In addition, the limit: $$ \lim_{(x,y,z)\to (0,0,0) } \frac{\tan(x+y+z) }{|x|+|y|+|z|} $$ does not exist and it seems like the limit: $$ \lim_{(x,y,z)\to (0,0,0) } \frac{\sin(x^2+y^2+z^2) }{|x|+|y|+|z|} $$ is zero. But, how can I calculate this limit (the last one) ? Will you please help me? Thanks a lot in advance","$$ \lim_{(x,y,z)\to (0,0,0) } \frac{\sin(x^2+y^2+z^2) + \tan(x+y+z) }{|x|+|y|+|z|} $$ I know the entire limit should not exist. In addition, the limit: $$ \lim_{(x,y,z)\to (0,0,0) } \frac{\tan(x+y+z) }{|x|+|y|+|z|} $$ does not exist and it seems like the limit: $$ \lim_{(x,y,z)\to (0,0,0) } \frac{\sin(x^2+y^2+z^2) }{|x|+|y|+|z|} $$ is zero. But, how can I calculate this limit (the last one) ? Will you please help me? Thanks a lot in advance",,['multivariable-calculus']
97,Finding field lines given a plane vector field,Finding field lines given a plane vector field,,"Problem: Determine the field lines of the vector field \begin{align*} \mathbf{F}(x,y) = x\hat{i} + y \hat{j}. \end{align*} The field lines satisfy the system \begin{align*} \frac{dx}{x} = \frac{dy}{y}. \end{align*} Integrating gives \begin{align*} \ln|x| + A = \ln|y| + B, \end{align*} where $A$ and $B$ are constants. I'm not sure how to proceed now, any help? Edit: Exponentiating both sides, we get \begin{align*}e^{\ln(x)} e^A = e^{\ln(y)} e^B, \end{align*}, and so \begin{align*} x e^{A-B} = y. \end{align*} Can I now replace $e^{A-B}$ by another constant $C$, and conclude that the field lines are straight lines?","Problem: Determine the field lines of the vector field \begin{align*} \mathbf{F}(x,y) = x\hat{i} + y \hat{j}. \end{align*} The field lines satisfy the system \begin{align*} \frac{dx}{x} = \frac{dy}{y}. \end{align*} Integrating gives \begin{align*} \ln|x| + A = \ln|y| + B, \end{align*} where $A$ and $B$ are constants. I'm not sure how to proceed now, any help? Edit: Exponentiating both sides, we get \begin{align*}e^{\ln(x)} e^A = e^{\ln(y)} e^B, \end{align*}, and so \begin{align*} x e^{A-B} = y. \end{align*} Can I now replace $e^{A-B}$ by another constant $C$, and conclude that the field lines are straight lines?",,"['calculus', 'multivariable-calculus', 'vector-fields']"
98,Mean value theorem and scalar field proof,Mean value theorem and scalar field proof,,"Assume that $f′(x;y)=0$ for every $x$ in some $n$-ball $B(a)$ and for every vector $y$. Use the mean value theorem to prove that $f$ is constant on $B(a)$. And if $f′(x;y)=0$ for a fixed vector $y$ and for every $x$ in $B(a)$, what can you conclude about $f$ in this case?","Assume that $f′(x;y)=0$ for every $x$ in some $n$-ball $B(a)$ and for every vector $y$. Use the mean value theorem to prove that $f$ is constant on $B(a)$. And if $f′(x;y)=0$ for a fixed vector $y$ and for every $x$ in $B(a)$, what can you conclude about $f$ in this case?",,"['calculus', 'multivariable-calculus', 'vector-spaces']"
99,"Integration of the vector field $\mathbf {F } (x,y)=\frac{y}{x^2+y^2}i-\frac{x}{x^2+y^2}j $ over two ellipses",Integration of the vector field  over two ellipses,"\mathbf {F } (x,y)=\frac{y}{x^2+y^2}i-\frac{x}{x^2+y^2}j ","Let $\mathbf{F}$ be a vector field defined on $\mathbb R^2 \setminus\{(0,0)\}$ by  $$\mathbf {F } (x,y)=\frac{y}{x^2+y^2}i-\frac{x}{x^2+y^2}j $$ Let $\gamma,\alpha:[0,1]\to\mathbb R^2$ be defined by $$\gamma (t)=(8\cos 2\pi t,17\sin 2\pi t)$$ and  $$\alpha (t)=(26\cos 2\pi t,-10\sin 2\pi t)$$ If $$3\int_{\alpha} \mathbf{F\cdot dr}  -4  \int_{\gamma} \mathbf{F\cdot dr}= 2m\pi,$$ then what is $m$? How should I approach this question? Progress I see that the parametrization of ellipses are given already. For evaluating say first integral,  I need to substitute given parametrization of ellipse in vector field. The parameter $t$ will vary from $ 0$ to $2\pi$. Am I correct?","Let $\mathbf{F}$ be a vector field defined on $\mathbb R^2 \setminus\{(0,0)\}$ by  $$\mathbf {F } (x,y)=\frac{y}{x^2+y^2}i-\frac{x}{x^2+y^2}j $$ Let $\gamma,\alpha:[0,1]\to\mathbb R^2$ be defined by $$\gamma (t)=(8\cos 2\pi t,17\sin 2\pi t)$$ and  $$\alpha (t)=(26\cos 2\pi t,-10\sin 2\pi t)$$ If $$3\int_{\alpha} \mathbf{F\cdot dr}  -4  \int_{\gamma} \mathbf{F\cdot dr}= 2m\pi,$$ then what is $m$? How should I approach this question? Progress I see that the parametrization of ellipses are given already. For evaluating say first integral,  I need to substitute given parametrization of ellipse in vector field. The parameter $t$ will vary from $ 0$ to $2\pi$. Am I correct?",,"['multivariable-calculus', 'vector-analysis']"
