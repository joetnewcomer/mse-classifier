,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,Proving the AM-GM Inequality with Lagrange Multipliers,Proving the AM-GM Inequality with Lagrange Multipliers,,"Exercise: Let $x_1,x_2,...,x_n$ be real positive numbers. Prove the arithmetic-geometric mean inequality, $(x_1x_2...x_n)^{1/n}\le (x_1+x_2+...+x_n)/n$. Hint: Consider the function $f(x_1,x_2,...,x_n)=(x_1+x_2+...+x_n)/n$ subject to the constraint $x_1x_2...x_n=c$ a constant. My work: Consider $f(x_1,x_2,...x_n)=(x_1+x_2+...+x_n)/n$ on $S=\{(x_1,x_2,...,x_n) | x_1x_2...x_n=c\}$ $g(x_1,x_2,...,x_n)=x_1x_2...x_n-c=0$ $\nabla f(x_1,x_2,...,x_n)=(1/n,1/n...,1/n)$ $\nabla g(x_1,x_2,...,x_n)=(c/x_1,c/x_2,...c/x_n)$ $\nabla f = \lambda \nabla g$ $1/(n\lambda)=c/x_1=c/x_2=...=c/x_n$ $x_1=x_2=...=x_n=n\lambda c$ At the point $(n\lambda c, n\lambda c,... n\lambda c)$, $f$ on $S$ takes either a maximum or a minimum. $f(n\lambda c, n\lambda c,...,n\lambda c)=n(n\lambda c)/n=n\lambda c$ $(n\lambda c*n\lambda c*...*n\lambda c)^{1/n}=((n\lambda c)^n)^{1/n}=n\lambda c = f(n\lambda c, n\lambda c,...,n\lambda c)$ This shows the equality case. To prove the inequality, I want to show that at the point $(n\lambda c,n\lambda c,...,n\lambda c)$, $f$ on $S$ takes a maximum minimum . I tried proving that the Hessian is negative positive definite, but I had trouble working with the second derivatives. I've looked up various proofs for the AM-GM inequality. The Lagrange ones didn't make full sense to me, so I thought I would turn to MSE. Any ideas? Thanks!","Exercise: Let $x_1,x_2,...,x_n$ be real positive numbers. Prove the arithmetic-geometric mean inequality, $(x_1x_2...x_n)^{1/n}\le (x_1+x_2+...+x_n)/n$. Hint: Consider the function $f(x_1,x_2,...,x_n)=(x_1+x_2+...+x_n)/n$ subject to the constraint $x_1x_2...x_n=c$ a constant. My work: Consider $f(x_1,x_2,...x_n)=(x_1+x_2+...+x_n)/n$ on $S=\{(x_1,x_2,...,x_n) | x_1x_2...x_n=c\}$ $g(x_1,x_2,...,x_n)=x_1x_2...x_n-c=0$ $\nabla f(x_1,x_2,...,x_n)=(1/n,1/n...,1/n)$ $\nabla g(x_1,x_2,...,x_n)=(c/x_1,c/x_2,...c/x_n)$ $\nabla f = \lambda \nabla g$ $1/(n\lambda)=c/x_1=c/x_2=...=c/x_n$ $x_1=x_2=...=x_n=n\lambda c$ At the point $(n\lambda c, n\lambda c,... n\lambda c)$, $f$ on $S$ takes either a maximum or a minimum. $f(n\lambda c, n\lambda c,...,n\lambda c)=n(n\lambda c)/n=n\lambda c$ $(n\lambda c*n\lambda c*...*n\lambda c)^{1/n}=((n\lambda c)^n)^{1/n}=n\lambda c = f(n\lambda c, n\lambda c,...,n\lambda c)$ This shows the equality case. To prove the inequality, I want to show that at the point $(n\lambda c,n\lambda c,...,n\lambda c)$, $f$ on $S$ takes a maximum minimum . I tried proving that the Hessian is negative positive definite, but I had trouble working with the second derivatives. I've looked up various proofs for the AM-GM inequality. The Lagrange ones didn't make full sense to me, so I thought I would turn to MSE. Any ideas? Thanks!",,"['real-analysis', 'inequality', 'lagrange-multiplier', 'a.m.-g.m.-inequality']"
1,Show that finitely many disjoint discs can be inscribed in a unit square with total area approaching 1,Show that finitely many disjoint discs can be inscribed in a unit square with total area approaching 1,,"(a) Given $\epsilon > 0$ , show that the unit disc contains finitely many dyadic squares whose total area exceeds $\pi - \epsilon$ , and which intersect each other only along their boundaries. (b) Show that the assertion is false for $\epsilon < \pi / 2$ if we demand that the dyadic squares be disjoint. (c) Formulate (a) in dimension $m = 3$ and $m \geq 4$ . (d) Do the analysis with squares and discs interchanged. That is, given $\epsilon > 0$ prove that finitely many disjoint discs can be drawn inside the unit square so that the total area of the discs exceeds $1 - \epsilon$ . I have done parts (a) to (b), and at least sketched out the proof for (c), but part (d) is totally eluding me. For a start, I'm not even sure it's true, but since it's given as a problem in a printed book ( Real Mathematical Anlsyis , by Pugh), I am inclined to think it is valid. Does anyone have any suggestions on how to approach this? Or even better, a proof sketch? I've tried a number of different ways, involving suprema, self-similarity, and what have you, but none seems to lead anywhere constructive.","(a) Given , show that the unit disc contains finitely many dyadic squares whose total area exceeds , and which intersect each other only along their boundaries. (b) Show that the assertion is false for if we demand that the dyadic squares be disjoint. (c) Formulate (a) in dimension and . (d) Do the analysis with squares and discs interchanged. That is, given prove that finitely many disjoint discs can be drawn inside the unit square so that the total area of the discs exceeds . I have done parts (a) to (b), and at least sketched out the proof for (c), but part (d) is totally eluding me. For a start, I'm not even sure it's true, but since it's given as a problem in a printed book ( Real Mathematical Anlsyis , by Pugh), I am inclined to think it is valid. Does anyone have any suggestions on how to approach this? Or even better, a proof sketch? I've tried a number of different ways, involving suprema, self-similarity, and what have you, but none seems to lead anywhere constructive.",\epsilon > 0 \pi - \epsilon \epsilon < \pi / 2 m = 3 m \geq 4 \epsilon > 0 1 - \epsilon,"['real-analysis', 'geometry']"
2,How much larger is the $\sigma$-algebra than the algebra in Caratheodory extension?,How much larger is the -algebra than the algebra in Caratheodory extension?,\sigma,"Given a 'measure' $\lambda$ on an algebra $\mathcal{A}$ of sets, Caratheodory gives a way to extend this $\lambda$ to a $\sigma$-algebra. The idea is we define an outer measure (on all subsets) $\lambda^*$ by \begin{equation}\lambda^*(E)=\operatorname{inf} \Sigma\lambda(A_j),\end{equation} where the $\operatorname{inf}$ is taken over all countable coverings of $E$ using sets in $\mathcal{A}$. Then one shows that the collection of subsets satisfying the Caratheodory condition \begin{equation}\lambda^*(E)=\lambda^*(E\cap A)+\lambda^*(E\backslash A)\end{equation} for all $E$ forms a $\sigma$-algebra, say $\mathcal{A}^*$, and $\lambda^*$ restricted to this $\mathcal{A}^*$ is actually a measure. So my question is, how much larger is this $\mathcal{A}^*$ than $\mathcal{A}$? We know $\mathcal{A}^*$ contains $\sigma(\mathcal{A})$, the $\sigma$-algebra generated by $\mathcal{A}$, and it contains all the null sets. But can it be even larger than this? In case of the Lebesgue measure, we start with the algebra of all 'boxes' then ended up of the Lebesgue measurable sets, which are in turn union of sets from $\sigma$(boxes) and null sets. So the difference of $\mathcal{A}^*$ and $\sigma(\mathcal{A})$ is exactly the null sets. But can $\mathcal{A}^*$ be significantly larger than $\sigma(\mathcal{A})$? Thanks!","Given a 'measure' $\lambda$ on an algebra $\mathcal{A}$ of sets, Caratheodory gives a way to extend this $\lambda$ to a $\sigma$-algebra. The idea is we define an outer measure (on all subsets) $\lambda^*$ by \begin{equation}\lambda^*(E)=\operatorname{inf} \Sigma\lambda(A_j),\end{equation} where the $\operatorname{inf}$ is taken over all countable coverings of $E$ using sets in $\mathcal{A}$. Then one shows that the collection of subsets satisfying the Caratheodory condition \begin{equation}\lambda^*(E)=\lambda^*(E\cap A)+\lambda^*(E\backslash A)\end{equation} for all $E$ forms a $\sigma$-algebra, say $\mathcal{A}^*$, and $\lambda^*$ restricted to this $\mathcal{A}^*$ is actually a measure. So my question is, how much larger is this $\mathcal{A}^*$ than $\mathcal{A}$? We know $\mathcal{A}^*$ contains $\sigma(\mathcal{A})$, the $\sigma$-algebra generated by $\mathcal{A}$, and it contains all the null sets. But can it be even larger than this? In case of the Lebesgue measure, we start with the algebra of all 'boxes' then ended up of the Lebesgue measurable sets, which are in turn union of sets from $\sigma$(boxes) and null sets. So the difference of $\mathcal{A}^*$ and $\sigma(\mathcal{A})$ is exactly the null sets. But can $\mathcal{A}^*$ be significantly larger than $\sigma(\mathcal{A})$? Thanks!",,"['real-analysis', 'probability', 'analysis', 'measure-theory', 'reference-request']"
3,Writing Integrals using Differential Forms,Writing Integrals using Differential Forms,,"Consider some smooth curve $C \subset \mathbb{R^n}$ and $\gamma:[a,b] \subset\mathbb{R}\rightarrow C$ a parametrisation of $C$ and a continuous vector field $K:\mathbb{R^n} \rightarrow \mathbb{R^n}$ . Let $\omega = K_{1}dx^{1}+...+K_{n} dx^{n}$ where $K_{1},...,K_{n}$ are the components of $K$ with respect to the standard basis of $\mathbb{R^n}$ . Now the following holds: $$\int_{c}\vec{K}\cdot\vec{ds} = \int_{c}\vec{K}\cdot\hat{n}\space ds:=\int_{a}^{b}\langle K(\gamma(t)),\dot{\gamma}(t)\rangle \space dt =\int_{a}^{b}\sum_{i=1}^{n}K_{i}(\gamma(t))\space\dot{\gamma_{i}}(t)\space dt$$ where $\langle.,.\rangle$ is the standard inner product. One can also write the same integral using a differential form: $$\int_{\gamma}\omega:=\int_{a}^{b}\gamma^{*}\omega=\int_{a}^{b}\omega(\gamma(t))\space \dot{\gamma}(t)\space dt =\int_{a}^{b}\sum_{i=1}^{n}K_{i}(\gamma(t))\space\dot{\gamma_{i}}(t)\space dt$$ Similarly let $S \subset \mathbb{R^3}$ be a smooth surface (2-dim submanifold) and $\phi:U\subset\mathbb{R^2}\rightarrow S$ a parametrisation of $S$ . $\space F:\mathbb{R^3}\rightarrow\mathbb{R^3}$ a continuous vectorfield. Let $\eta = F_{1}\space dx\wedge dy -F_{2}\space dx \wedge dz +F_{3}\space dy \wedge dz$ where $F_{1},F_{2},F_{3}$ are the components of F with respect to the standard basis of $\mathbb{R^3}$ . Now the following holds: $$\int_{S}\vec{F}\cdot \vec{dA} = \int_{S}\vec{F}\cdot\hat{n}\space dA :=\int_{U}\langle F,\frac{\partial\phi}{\partial u}\times\frac{\partial\phi}{\partial v}\rangle\space d\mu(u,v)$$ And the same Integral using the differential form: $$\int_{S}\eta:=\int_{U}\phi^{*}\eta= \int_{U}\langle F,\frac{\partial\phi}{\partial u}\times\frac{\partial\phi}{\partial v}\rangle\space d\mu(u,v)$$ My Question is: How do I express the following integrals using differential forms? Let $\space f:\mathbb{R^n}\rightarrow\mathbb{R}$ and $g:\mathbb{R^3}\rightarrow \mathbb{R}$ be continuous functions. $$\int_{c}f\space ds := \int_{a}^{b}f(\gamma(t))\space\lVert\dot{\gamma}(t)\rVert\space dt$$ $$\int_{S}g\space dA := \int_{U} g(\phi(u,v)) \space\left\|\frac{\partial\phi}{\partial u}\times\frac{\partial\phi}{\partial v}\right\|\space d\mu(u,v)$$ Help is greatly appreciated. Vincent Pfenninger",Consider some smooth curve and a parametrisation of and a continuous vector field . Let where are the components of with respect to the standard basis of . Now the following holds: where is the standard inner product. One can also write the same integral using a differential form: Similarly let be a smooth surface (2-dim submanifold) and a parametrisation of . a continuous vectorfield. Let where are the components of F with respect to the standard basis of . Now the following holds: And the same Integral using the differential form: My Question is: How do I express the following integrals using differential forms? Let and be continuous functions. Help is greatly appreciated. Vincent Pfenninger,"C \subset \mathbb{R^n} \gamma:[a,b] \subset\mathbb{R}\rightarrow C C K:\mathbb{R^n} \rightarrow \mathbb{R^n} \omega = K_{1}dx^{1}+...+K_{n} dx^{n} K_{1},...,K_{n} K \mathbb{R^n} \int_{c}\vec{K}\cdot\vec{ds} = \int_{c}\vec{K}\cdot\hat{n}\space ds:=\int_{a}^{b}\langle K(\gamma(t)),\dot{\gamma}(t)\rangle \space dt =\int_{a}^{b}\sum_{i=1}^{n}K_{i}(\gamma(t))\space\dot{\gamma_{i}}(t)\space dt \langle.,.\rangle \int_{\gamma}\omega:=\int_{a}^{b}\gamma^{*}\omega=\int_{a}^{b}\omega(\gamma(t))\space \dot{\gamma}(t)\space dt =\int_{a}^{b}\sum_{i=1}^{n}K_{i}(\gamma(t))\space\dot{\gamma_{i}}(t)\space dt S \subset \mathbb{R^3} \phi:U\subset\mathbb{R^2}\rightarrow S S \space F:\mathbb{R^3}\rightarrow\mathbb{R^3} \eta = F_{1}\space dx\wedge dy -F_{2}\space dx \wedge dz +F_{3}\space dy \wedge dz F_{1},F_{2},F_{3} \mathbb{R^3} \int_{S}\vec{F}\cdot \vec{dA} = \int_{S}\vec{F}\cdot\hat{n}\space dA :=\int_{U}\langle F,\frac{\partial\phi}{\partial u}\times\frac{\partial\phi}{\partial v}\rangle\space d\mu(u,v) \int_{S}\eta:=\int_{U}\phi^{*}\eta= \int_{U}\langle F,\frac{\partial\phi}{\partial u}\times\frac{\partial\phi}{\partial v}\rangle\space d\mu(u,v) \space f:\mathbb{R^n}\rightarrow\mathbb{R} g:\mathbb{R^3}\rightarrow \mathbb{R} \int_{c}f\space ds := \int_{a}^{b}f(\gamma(t))\space\lVert\dot{\gamma}(t)\rVert\space dt \int_{S}g\space dA := \int_{U} g(\phi(u,v)) \space\left\|\frac{\partial\phi}{\partial u}\times\frac{\partial\phi}{\partial v}\right\|\space d\mu(u,v)","['real-analysis', 'integration', 'multivariable-calculus', 'differential-forms']"
4,Does there exist a sequence of real numbers $\{a_n\}$ such that $\sum_na_n^k$ converges for $k=1$ but diverges for every other odd positive integer?,Does there exist a sequence of real numbers  such that  converges for  but diverges for every other odd positive integer?,\{a_n\} \sum_na_n^k k=1,Does there exist a sequence of real numbers $\{a_n\}$ such that $\sum_na_n^k$ converges for $k=1$ but diverges for every other odd positive integer?,Does there exist a sequence of real numbers $\{a_n\}$ such that $\sum_na_n^k$ converges for $k=1$ but diverges for every other odd positive integer?,,"['real-analysis', 'sequences-and-series', 'analysis', 'convergence-divergence', 'contest-math']"
5,"Is the set of all polynomials in $\log(x)$ dense in $L^2[0,1]$?",Is the set of all polynomials in  dense in ?,"\log(x) L^2[0,1]","Is $\{(\log(x))^k\mid k=0,1,2,\ldots\}$ dense in $L^2 [0,1]$? That is, is the set of all polynomials of logarithm functions dense in the set of square integrable functions on $[0,1]$?","Is $\{(\log(x))^k\mid k=0,1,2,\ldots\}$ dense in $L^2 [0,1]$? That is, is the set of all polynomials of logarithm functions dense in the set of square integrable functions on $[0,1]$?",,['real-analysis']
6,Functions from the Cantor set,Functions from the Cantor set,,"Consider the Cantor set $\Delta\subset [0,1]$, and let $f\colon \Delta\to [0,\infty)$ be a continuous injection. Must $f$ be monotone on some uncountable closed subset of $\Delta$? Note that that Van Der Waerden's construction is not applicable here.","Consider the Cantor set $\Delta\subset [0,1]$, and let $f\colon \Delta\to [0,\infty)$ be a continuous injection. Must $f$ be monotone on some uncountable closed subset of $\Delta$? Note that that Van Der Waerden's construction is not applicable here.",,"['real-analysis', 'continuity', 'cantor-set']"
7,Continuous partials at a point without being defined throughout a neighborhood and not differentiable there?,Continuous partials at a point without being defined throughout a neighborhood and not differentiable there?,,"This is a follow-up to Continuous partials at a point but not differentiable there? , but I'll make this question self-contained. Throughout, $f$ will denote a function $\mathbb{R}^2\to\mathbb{R}$. An answer to Equivalent condition for differentiability on partial derivatives cites a theorem implying ""If $f_x$ and $f_y$ exist at $P$ and $f_x$ is defined throughout a neighborhood of $P$ and $f_x$ is continuous at $P$, then $f$ is differentiable at $P$."", which is stronger than what you find in many calculus texts. First note that this can't possibly be weakened to something like ""exists in almost all of a neighborhood and is continuous along paths within that neighborhood of all but one limiting direction"". One counterexample would be $f(x,y)=\sqrt[3]{xy}$, which has partials that both exist at the origin and only blow up along some axis (with the origin deleted) so that limits of a partial along paths whose limiting direction is not the direction of the problem-axis all exist and agree. But $f$ is not differentiable at the origin. I wonder ""is the fact that the partials blow up the problem?"" Or is mere non-existence of the partials arbitrarily close to the point enough to give the function a chance to be non-differentiable? More formally... My question: Is there a function $f:\mathbb{R}^2\to\mathbb{R}$ such that $f_x$ and $f_y$ exist at $P$. $f_x$ is defined in a neighborhood of $P$ minus an open ray (or a line with $P$ deleted, if it makes things easier). $f_x$ is continuous at $P$ in the sense that for all $Q$ in the domain of $f_x$ and for all $\varepsilon>0$ there exists $\delta>0$ so that $\Vert P-Q\Vert<\delta\Rightarrow\Vert f_x(Q)-f_x(P)\Vert<\varepsilon$. $f$ is not differentiable at $P$. It may be the case that the answer is ""no, because no function satisfies 1. through 3. without $f_x$ being defined in a neighborhood of $P$ so that 4. is impossible by the theorem in Apostol's book"", but even if that's the case, I'd like to know. The reason I worry about that possibility is that I know that in the 1-d case a derivative can't have jump discontinuities, and this seems like a similar issue (something reminiscent of branch cuts). Additionally, naive functions built out of $x^2\sin\left(\frac{1}{x}\right)$ tend to be differentiable.","This is a follow-up to Continuous partials at a point but not differentiable there? , but I'll make this question self-contained. Throughout, $f$ will denote a function $\mathbb{R}^2\to\mathbb{R}$. An answer to Equivalent condition for differentiability on partial derivatives cites a theorem implying ""If $f_x$ and $f_y$ exist at $P$ and $f_x$ is defined throughout a neighborhood of $P$ and $f_x$ is continuous at $P$, then $f$ is differentiable at $P$."", which is stronger than what you find in many calculus texts. First note that this can't possibly be weakened to something like ""exists in almost all of a neighborhood and is continuous along paths within that neighborhood of all but one limiting direction"". One counterexample would be $f(x,y)=\sqrt[3]{xy}$, which has partials that both exist at the origin and only blow up along some axis (with the origin deleted) so that limits of a partial along paths whose limiting direction is not the direction of the problem-axis all exist and agree. But $f$ is not differentiable at the origin. I wonder ""is the fact that the partials blow up the problem?"" Or is mere non-existence of the partials arbitrarily close to the point enough to give the function a chance to be non-differentiable? More formally... My question: Is there a function $f:\mathbb{R}^2\to\mathbb{R}$ such that $f_x$ and $f_y$ exist at $P$. $f_x$ is defined in a neighborhood of $P$ minus an open ray (or a line with $P$ deleted, if it makes things easier). $f_x$ is continuous at $P$ in the sense that for all $Q$ in the domain of $f_x$ and for all $\varepsilon>0$ there exists $\delta>0$ so that $\Vert P-Q\Vert<\delta\Rightarrow\Vert f_x(Q)-f_x(P)\Vert<\varepsilon$. $f$ is not differentiable at $P$. It may be the case that the answer is ""no, because no function satisfies 1. through 3. without $f_x$ being defined in a neighborhood of $P$ so that 4. is impossible by the theorem in Apostol's book"", but even if that's the case, I'd like to know. The reason I worry about that possibility is that I know that in the 1-d case a derivative can't have jump discontinuities, and this seems like a similar issue (something reminiscent of branch cuts). Additionally, naive functions built out of $x^2\sin\left(\frac{1}{x}\right)$ tend to be differentiable.",,"['real-analysis', 'analysis', 'multivariable-calculus', 'examples-counterexamples']"
8,How not to define density of a subset of $\mathbb{Z}^2$,How not to define density of a subset of,\mathbb{Z}^2,"The next exercise illustrates a bad definition of density of a subset of $\mathbb{Z}^2$ (it always ends up being either $0$ or $1$ ). Let $S\subset \mathbb{Z}^2$ . Define $$d_k(S)=\max  \limits_{\substack{A,B\subset \mathbb{Z}  \\ |A|=|B|=k}}\frac{|S\cap  (A\times B)|}{|A||B|}.$$ Show that $\lim \limits_{k\to \infty} d_k(S)$ exists and is always either 0 or 1. This is the original version of the problem but I think it is more correctly to replace $\max$ with $\sup$ , i.e. $$d_k(S)=\sup  \limits_{\substack{A,B\subset \mathbb{Z}  \\ |A|=|B|=k}}\frac{|S\cap  (A\times B)|}{|A||B|}.$$ I have no idea how to attack this problem but after some thoughts I was able to solve the problem in particular case, when $S\subset\mathbb{Z}^2$ is a bounded set. More precisely, if $S\subset [-N,N]^2$ for some $N\in \mathbb{N}$ , then $d_k(S)=\dfrac{|S|}{k^2}$ for $k\geq 4N$ . Therefore, $\lim \limits_{k\to \infty} d_k(S)=0$ . But I think the solution should be different since there are unbounded sets  with density zero also. I'd be very grateful if someone can show the solution please. Thank you!","The next exercise illustrates a bad definition of density of a subset of (it always ends up being either or ). Let . Define Show that exists and is always either 0 or 1. This is the original version of the problem but I think it is more correctly to replace with , i.e. I have no idea how to attack this problem but after some thoughts I was able to solve the problem in particular case, when is a bounded set. More precisely, if for some , then for . Therefore, . But I think the solution should be different since there are unbounded sets  with density zero also. I'd be very grateful if someone can show the solution please. Thank you!","\mathbb{Z}^2 0 1 S\subset \mathbb{Z}^2 d_k(S)=\max
 \limits_{\substack{A,B\subset \mathbb{Z}  \\ |A|=|B|=k}}\frac{|S\cap
 (A\times B)|}{|A||B|}. \lim \limits_{k\to \infty} d_k(S) \max \sup d_k(S)=\sup
 \limits_{\substack{A,B\subset \mathbb{Z}  \\ |A|=|B|=k}}\frac{|S\cap
 (A\times B)|}{|A||B|}. S\subset\mathbb{Z}^2 S\subset [-N,N]^2 N\in \mathbb{N} d_k(S)=\dfrac{|S|}{k^2} k\geq 4N \lim \limits_{k\to \infty} d_k(S)=0","['real-analysis', 'limits']"
9,"When should continued fraction expansions which start from an original value, converge to that same value? $\tan x=x/1-x^2/3-x^2/5-\dots$","When should continued fraction expansions which start from an original value, converge to that same value?",\tan x=x/1-x^2/3-x^2/5-\dots,"$\newcommand{\K}{\operatorname{\large\mathcal{K}}}$ I am asking about a very common practice in proofs, that I see online, concerning continued fractions. There is an implicit assumption which I’d like to examine: a concrete, concise question can be found in the middle section, but I think a lot of context is necessary. Here is what I mean: sometimes, when dealing with continued fractions, we begin with the answer , and then recursively expand the answer, ad infinitum, implicitly assuming that the limit will be the same. I occasionally edit this post to add new thoughts, and sufficient conditions (a necessary one would be very well-received!) in the middle section. To clarify, here is a trivial example: $$\begin{align}1&=\cfrac{2}{3-1}\\&=\cfrac{2}{3-\cfrac{2}{3-1}}\\&=\cfrac{2}{3-\cfrac{2}{3-\cfrac{2}{3-1}}}\\&\overset{?}{=}\cdots\\&=\cfrac{2}{3-\cfrac{2}{3-\cfrac{2}{3-\cfrac{2}{3-\ddots}}}}\end{align}$$ We started with $1$ , then expressed it as a ratio $\frac{b_1}{a_1+\alpha_1}$ . We then expanded $\alpha_1$ as a ratio $\frac{b_2}{a_2+\alpha_2}$ ... et cetera. We can show that (Gauss Kettenbrücher K-notation, similar to $\sum,\prod$ notation) $\K_{n=1}^\infty\frac{b_n}{a_n}$ is convergent, and because we obtained this continued fraction from a sequence of expressions, all equal to $1$ , we have reason to believe that the limiting fraction is also equal to $1$ . However, in so doing we lose the helper $\alpha_n$ terms, so I feel, a priori , we cannot be certain that the limit is actually $1$ . It is, as it happens... the convergents are $-\K_{n=1}^m\frac{-2}{3}=1-\frac{1}{2^{m+1}-1}$ , which obviously tend to $1$ . But I ask about the general procedure. For example, Wikipedia’s article on Gauss’ continued fraction uses recurring expansions, just like how we expanded “ $1$ ” above, to derive continued fractions for ratios of hypergeometric functions. I like this derivation, but there is always the concern that, even if the continued fraction is convergent, the limiting value might not be the value we started with! A simple example to demonstrate this concern: in the above expansion of $1$ , we could just as easily write: $$\begin{align}2&=\frac{2}{3-2}\\&=\cfrac{2}{3-\cfrac{2}{3-2}}\\&=\cdots\\&\overset{!?}{=}\cfrac{2}{3-\cfrac{2}{3-\cfrac{2}{3-\ddots}}}\end{align}$$ But we already know that that converges to $1$ , not to $2$ ! The core question. My analysis, and a concrete setup: Let $L$ be any nonzero real number. Suppose we find a sequence of nonzero reals $(a_k),(b_k),(\alpha_k)$ with the property that, for all $n\in\Bbb N$ : $$L=\K_{k=1}^n\frac{b_k}{a’_k}$$ Where $a’_k=a_k$ except for $a’_n:=\alpha_n$ . Suppose further that: $$\lim_{n\to\infty}\K_{k=1}^n\frac{b_k}{a_k}=L’\in\Bbb R\setminus\{0\}$$ That is, the continued fraction converges. We want to know if $L’=L$ . If $p_n,q_n$ are the numerators and denominators of the $n$ th convergent $\K_{k=1}^n\frac{b_k}{a_k}$ (with no simplification!) then we know that (writing $\alpha_n=\alpha_n/1$ ): $$L=\frac{p_n+\alpha_np_{n-1}}{q_n+\alpha_nq_{n-1}}$$ For every $n$ . The difference $|L-L’|$ is equal to: $$\left|\frac{q_n\left(\frac{p_n}{q_n}-L’\right)+\alpha_nq_{n-1}\left(\frac{p_{n-1}}{q_{n-1}}-L’\right)}{q_n+\alpha_nq_{n-1}}\right|=\left|\left(\frac{p_n}{q_n}-L’\right)+\cfrac{1}{1+\cfrac{q_n}{\alpha_nq_{n-1}}}\left(\frac{p_{n-1}}{q_{n-1}}-\frac{p_n}{q_n}\right)\right|$$ For every $n$ . Thus I can hope to send $n\to\infty$ - so long as $\frac{q_n}{\alpha_nq_{n-1}}$ is bounded away from $-1$ , it is easy to see all terms tend to zero, whence $L=L’$ . However, there’s the rub - can we necessarily say that there is some $\delta>0$ , that for any $n\in\Bbb N$ , $\left|\frac{q_n}{\alpha_nq_{n-1}}+1\right|>\delta$ ? One quick observation is that, if all $(a_k),(b_k),(\alpha_k)$ are positive, then the answer is a definite: ‘yes’. Another observation, inspired by the partial answer below, is that I really only need to find a subsequence $(n_k)$ along which $\frac{q_{n_k}}{\alpha_{n_k}q_{n_k-1}}$ is bounded away from $-1$ . Note: if no such subsequence can be found, that actually implies the stronger assertion that: $\lim_{n\to\infty}\alpha_n\frac{q_{n-1}}{q_n}=-1$ as a strong limit. Furthermore, we see that, in order for $L$ to be finite and well defined, if $q_n/\alpha_n q_{n-1}$ comes arbitrarily close to $-1$ , then $\frac{\alpha_n p_{n-1}}{q_n}$ comes arbitrarily close to $-L’$ . I’ve tried using that to obtain a contradiction, but I’ve had no such luck yet. Furthermore, $\alpha_np_{n-1}/p_n$ would also tend to $-1$ . A new observation: if $\alpha_n\to0$ , as happens in the case of Lambert’s tangent continued fraction, and if one can be sure that $|q_n|\ge|q_{n-1}|$ for large $n$ , or at least along a subsequence (more generally, we only need to assume the ratio $q_{n-1}/q_n$ is bounded) - which is often the case - then $\alpha_n q_{n-1}/q_n$ tends to $0$ , and is in particular bounded away from $-1$ . This is not a necessary condition, since my trivial example involving $1=3/2-\cdots$ has $\alpha_n=-1$ for all $n$ . If the $\alpha_n$ are constant, then so long as we can say $|q_n|$ is sufficiently larger than $|q_{n-1}|$ (eventually, perhaps along a subsequence) then it will also be possible to bound away from $-1$ (more generally, the condition would be the unwieldy: “ $q_{n-1}/q_n$ is bounded away from the constant $-1/\alpha_n$ ”). See the counterexample expansion of $2$ : $-1/\alpha=\frac{1}{2}$ in this case, and $1/2$ is precisely the limiting ratio of $\frac{q_{n-1}}{q_n}$ , explaining why convergence fails. I don’t however think that this is anywhere near a complete, or usefully general, list. There is surely more to this story. My question : can we say more about the general case? I’d be happy to see relatively simple, easy-to-apply conditions, other than the conditions I’ve already supplied. As I have shown, and more recently G. Edgar has shown, counterexamples do exist. A less trivial, and more motivating, example: through some series manipulations and divisions of the Maclaurin expansions for $\sin,\cos$ , we can find: $$\tan x=\cfrac{x}{1-\cfrac{x^2}{3-\cfrac{x^2}{P_1/P_2}}}$$ Where: $$P_m:=\sum_{n=0}^\infty(-1)^n\frac{x^{2n}}{(2(n+m)+1)!}\prod_{k=1}^m2(n+k)$$ It can be shown that, using identical manipulations: $$\frac{P_m}{P_{m+1}}=(2m+3)-\frac{x^2}{P_{m+1}/P_{m+2}}$$ So, inductively, a continued fraction for $\tan x$ is born: $$\tan x=\cfrac{x}{1-\cfrac{x^2}{3-\cfrac{x^2}{5-\cfrac{x^2}{7-\ddots}}}}$$ This continued fraction can be shown to converge - but does it necessarily converge to $\tan x$ ? The concern being, again, that we have lost the helper $\alpha_m:=-x^2/(P_m/P_{m+1})$ terms.","I am asking about a very common practice in proofs, that I see online, concerning continued fractions. There is an implicit assumption which I’d like to examine: a concrete, concise question can be found in the middle section, but I think a lot of context is necessary. Here is what I mean: sometimes, when dealing with continued fractions, we begin with the answer , and then recursively expand the answer, ad infinitum, implicitly assuming that the limit will be the same. I occasionally edit this post to add new thoughts, and sufficient conditions (a necessary one would be very well-received!) in the middle section. To clarify, here is a trivial example: We started with , then expressed it as a ratio . We then expanded as a ratio ... et cetera. We can show that (Gauss Kettenbrücher K-notation, similar to notation) is convergent, and because we obtained this continued fraction from a sequence of expressions, all equal to , we have reason to believe that the limiting fraction is also equal to . However, in so doing we lose the helper terms, so I feel, a priori , we cannot be certain that the limit is actually . It is, as it happens... the convergents are , which obviously tend to . But I ask about the general procedure. For example, Wikipedia’s article on Gauss’ continued fraction uses recurring expansions, just like how we expanded “ ” above, to derive continued fractions for ratios of hypergeometric functions. I like this derivation, but there is always the concern that, even if the continued fraction is convergent, the limiting value might not be the value we started with! A simple example to demonstrate this concern: in the above expansion of , we could just as easily write: But we already know that that converges to , not to ! The core question. My analysis, and a concrete setup: Let be any nonzero real number. Suppose we find a sequence of nonzero reals with the property that, for all : Where except for . Suppose further that: That is, the continued fraction converges. We want to know if . If are the numerators and denominators of the th convergent (with no simplification!) then we know that (writing ): For every . The difference is equal to: For every . Thus I can hope to send - so long as is bounded away from , it is easy to see all terms tend to zero, whence . However, there’s the rub - can we necessarily say that there is some , that for any , ? One quick observation is that, if all are positive, then the answer is a definite: ‘yes’. Another observation, inspired by the partial answer below, is that I really only need to find a subsequence along which is bounded away from . Note: if no such subsequence can be found, that actually implies the stronger assertion that: as a strong limit. Furthermore, we see that, in order for to be finite and well defined, if comes arbitrarily close to , then comes arbitrarily close to . I’ve tried using that to obtain a contradiction, but I’ve had no such luck yet. Furthermore, would also tend to . A new observation: if , as happens in the case of Lambert’s tangent continued fraction, and if one can be sure that for large , or at least along a subsequence (more generally, we only need to assume the ratio is bounded) - which is often the case - then tends to , and is in particular bounded away from . This is not a necessary condition, since my trivial example involving has for all . If the are constant, then so long as we can say is sufficiently larger than (eventually, perhaps along a subsequence) then it will also be possible to bound away from (more generally, the condition would be the unwieldy: “ is bounded away from the constant ”). See the counterexample expansion of : in this case, and is precisely the limiting ratio of , explaining why convergence fails. I don’t however think that this is anywhere near a complete, or usefully general, list. There is surely more to this story. My question : can we say more about the general case? I’d be happy to see relatively simple, easy-to-apply conditions, other than the conditions I’ve already supplied. As I have shown, and more recently G. Edgar has shown, counterexamples do exist. A less trivial, and more motivating, example: through some series manipulations and divisions of the Maclaurin expansions for , we can find: Where: It can be shown that, using identical manipulations: So, inductively, a continued fraction for is born: This continued fraction can be shown to converge - but does it necessarily converge to ? The concern being, again, that we have lost the helper terms.","\newcommand{\K}{\operatorname{\large\mathcal{K}}} \begin{align}1&=\cfrac{2}{3-1}\\&=\cfrac{2}{3-\cfrac{2}{3-1}}\\&=\cfrac{2}{3-\cfrac{2}{3-\cfrac{2}{3-1}}}\\&\overset{?}{=}\cdots\\&=\cfrac{2}{3-\cfrac{2}{3-\cfrac{2}{3-\cfrac{2}{3-\ddots}}}}\end{align} 1 \frac{b_1}{a_1+\alpha_1} \alpha_1 \frac{b_2}{a_2+\alpha_2} \sum,\prod \K_{n=1}^\infty\frac{b_n}{a_n} 1 1 \alpha_n 1 -\K_{n=1}^m\frac{-2}{3}=1-\frac{1}{2^{m+1}-1} 1 1 1 \begin{align}2&=\frac{2}{3-2}\\&=\cfrac{2}{3-\cfrac{2}{3-2}}\\&=\cdots\\&\overset{!?}{=}\cfrac{2}{3-\cfrac{2}{3-\cfrac{2}{3-\ddots}}}\end{align} 1 2 L (a_k),(b_k),(\alpha_k) n\in\Bbb N L=\K_{k=1}^n\frac{b_k}{a’_k} a’_k=a_k a’_n:=\alpha_n \lim_{n\to\infty}\K_{k=1}^n\frac{b_k}{a_k}=L’\in\Bbb R\setminus\{0\} L’=L p_n,q_n n \K_{k=1}^n\frac{b_k}{a_k} \alpha_n=\alpha_n/1 L=\frac{p_n+\alpha_np_{n-1}}{q_n+\alpha_nq_{n-1}} n |L-L’| \left|\frac{q_n\left(\frac{p_n}{q_n}-L’\right)+\alpha_nq_{n-1}\left(\frac{p_{n-1}}{q_{n-1}}-L’\right)}{q_n+\alpha_nq_{n-1}}\right|=\left|\left(\frac{p_n}{q_n}-L’\right)+\cfrac{1}{1+\cfrac{q_n}{\alpha_nq_{n-1}}}\left(\frac{p_{n-1}}{q_{n-1}}-\frac{p_n}{q_n}\right)\right| n n\to\infty \frac{q_n}{\alpha_nq_{n-1}} -1 L=L’ \delta>0 n\in\Bbb N \left|\frac{q_n}{\alpha_nq_{n-1}}+1\right|>\delta (a_k),(b_k),(\alpha_k) (n_k) \frac{q_{n_k}}{\alpha_{n_k}q_{n_k-1}} -1 \lim_{n\to\infty}\alpha_n\frac{q_{n-1}}{q_n}=-1 L q_n/\alpha_n q_{n-1} -1 \frac{\alpha_n p_{n-1}}{q_n} -L’ \alpha_np_{n-1}/p_n -1 \alpha_n\to0 |q_n|\ge|q_{n-1}| n q_{n-1}/q_n \alpha_n q_{n-1}/q_n 0 -1 1=3/2-\cdots \alpha_n=-1 n \alpha_n |q_n| |q_{n-1}| -1 q_{n-1}/q_n -1/\alpha_n 2 -1/\alpha=\frac{1}{2} 1/2 \frac{q_{n-1}}{q_n} \sin,\cos \tan x=\cfrac{x}{1-\cfrac{x^2}{3-\cfrac{x^2}{P_1/P_2}}} P_m:=\sum_{n=0}^\infty(-1)^n\frac{x^{2n}}{(2(n+m)+1)!}\prod_{k=1}^m2(n+k) \frac{P_m}{P_{m+1}}=(2m+3)-\frac{x^2}{P_{m+1}/P_{m+2}} \tan x \tan x=\cfrac{x}{1-\cfrac{x^2}{3-\cfrac{x^2}{5-\cfrac{x^2}{7-\ddots}}}} \tan x \alpha_m:=-x^2/(P_m/P_{m+1})","['real-analysis', 'sequences-and-series', 'limits', 'continued-fractions']"
10,Local maximum uniqueness of the logarithm of the Student-t distribution,Local maximum uniqueness of the logarithm of the Student-t distribution,,"The negative logarithm of the Student-t distribution partial density function is $$f(\nu,x) := -\ln\Gamma\left(\frac{\nu+1}{2}\right)             +\ln\Gamma\left(\frac{\nu}{2}\right)             +\frac{1}{2}\ln(\pi\nu)             +\frac{\nu+1}{2}\ln\left(1+\frac{x^2}\nu\right)$$ How would one prove or disprove there is only one local minimum with respect to $\nu>0$ for any given $x$ ? Numerical computation seems to suggest $f(\nu,x)$ strictly decreases with $\nu\in(0,\infty)$ for $x\in[0,1.5]$ , and $f(\nu,x)$ is convex in $(0,a)$ and concave in $(a,\infty)$ for some $a>0$ for $x\in (b,\infty)$ for some $b\ge 1.5$ . To facilitate the solution, I post the first and second partial derivative of $f$ as follows. \begin{align}2\frac{\partial f}{\partial \nu}=\frac{1-x^2}{\nu+x^2}+\ln\Big(1+\frac{x^2}\nu\Big)-\int_0^\infty \frac{e^{-\frac \nu2t}}{1+e^{-\frac t2}}\,dt, \end{align} $$4\frac{\partial^2 f}{\partial \nu^2}=-2\frac{\nu+x^4}{\nu(\nu+x^2)^2}+\int_0^\infty \frac{te^{-\frac \nu2t}}{1+e^{-\frac t2}}\,dt$$","The negative logarithm of the Student-t distribution partial density function is How would one prove or disprove there is only one local minimum with respect to for any given ? Numerical computation seems to suggest strictly decreases with for , and is convex in and concave in for some for for some . To facilitate the solution, I post the first and second partial derivative of as follows.","f(\nu,x) := -\ln\Gamma\left(\frac{\nu+1}{2}\right)
            +\ln\Gamma\left(\frac{\nu}{2}\right)
            +\frac{1}{2}\ln(\pi\nu)
            +\frac{\nu+1}{2}\ln\left(1+\frac{x^2}\nu\right) \nu>0 x f(\nu,x) \nu\in(0,\infty) x\in[0,1.5] f(\nu,x) (0,a) (a,\infty) a>0 x\in (b,\infty) b\ge 1.5 f \begin{align}2\frac{\partial f}{\partial \nu}=\frac{1-x^2}{\nu+x^2}+\ln\Big(1+\frac{x^2}\nu\Big)-\int_0^\infty \frac{e^{-\frac \nu2t}}{1+e^{-\frac t2}}\,dt,
\end{align} 4\frac{\partial^2 f}{\partial \nu^2}=-2\frac{\nu+x^4}{\nu(\nu+x^2)^2}+\int_0^\infty \frac{te^{-\frac \nu2t}}{1+e^{-\frac t2}}\,dt","['real-analysis', 'inequality', 'special-functions', 'maxima-minima', 'gamma-function']"
11,"$\{(x,f(x)): x\in E\}$ is compact in $\mathbb R^2 \implies f:E\to\mathbb R$ is continuous",is compact in  is continuous,"\{(x,f(x)): x\in E\} \mathbb R^2 \implies f:E\to\mathbb R","Given $f: E\to\mathbb R$ , define $G: E\to\mathbb R^2$ by $G(x) = (x,f(x))$ for all $x\in E$ . $E$ is a compact subset of $\mathbb R$ . The following are equivalent: $f$ is continuous. $G$ is continuous. The graph of $f$ is a compact subset of $\mathbb R^2$ . First, let $E = [a,b]$ . I have shown $1\implies 2$ and $2\implies 3$ . Proving $3\implies 1$ would help me complete the argument full circle. Please let me know if the following makes sense (point out errors if any), and help me complete the proof. $[1\implies 2]$ : If $x_n\to x$ and $f$ is continuous, then $f(x_n)\to f(x)$ , and hence $G(x_n)\to G(x)$ meaning $G$ is continuous. Does this need more argument? $[2\implies 3]:$ The continuous image of a compact set is compact. Hence, $G([a,b])$ , i.e. the graph of $f$ is compact in $\mathbb R^2$ . I found the following for $[3\implies 1]$ : Let $G([a,b])$ be compact and $x_n\rightarrow x$ be a convergent sequence in $[a,b]$ . We show that $f(x_n)$ converges to $f(x)$ . Since the graph is compact, $f(x_n)$ has a convergent subsequence, Q1. $G([a,b])$ is compact, so $(x_n,f(x_n))$ has a convergent subsequence. How does this tell us that $f(x_n)$ has a convergent subsequence? I feel we need to add some details here. i.e. $f(x_{n_j})\rightarrow y$ . That is $(x_{n_j}, f(x_{n_j}))\rightarrow (x, y)$ . The graph is closed. That is, the limit of every convergent sequence in $G([a,b])$ is again in $G([a,b])$ . Therefore $(x,y)\in G([a,b])$ , i.e. $y=f(x)$ . Since this is true for every convergent subsequence, we showed that $f(x)$ is the only limit point of $f(x_n)$ , i.e. $f(x_n)$ converges to $f(x)$ . Q2. How is the last sentence concluded? We choose an arbitrary convergent subsequence $f(x_{n_j}) \to y$ , and found $y = f(x)$ . What now? My attempt for $[3\implies 1]$ : Suppose $G_f = \{(x,f(x)): x\in [a,b]\}$ is compact in $\mathbb R^2$ . Define $\phi_1(x,y) = x, \phi_2(x,y) = y$ . Suppose $S\subset\mathbb R$ is an arbitrary closed set in $\mathbb R$ . $$f^{-1}(S) = \{x\in [a,b]: f(x) \in S\}$$ We can write $$f^{-1}(S) = \phi_1(G_f \cap \phi_2^{-1}(S))$$ The above expression seems good intuitively but I need help establishing it by showing $\subseteq$ and $\supseteq$ inclusions. Next, $S\subset \mathbb R$ is closed tells us that $\phi_2^{-1}(S) \subset \mathbb R^2$ is closed - since $\phi_1,\phi_2$ are continuous. $G_f \cap \phi_2^{-1}(S)\subset G_f$ , and $G_f \cap \phi_2^{-1}(S)$ is closed. So, $G_f \cap \phi_2^{-1}(S)$ is compact. $\phi_1$ is continuous, which means $f^{-1}(S)$ is compact. Hence, $f^{-1}(S)$ is closed. Done! Please help me fill in the gaps in my attempt , and let me know if you've any other ideas to solve the problem, such as $[3\implies 2]$ and $[2\implies 1]$ . I'd also like to understand the proof posted above my attempt. Thank you!","Given , define by for all . is a compact subset of . The following are equivalent: is continuous. is continuous. The graph of is a compact subset of . First, let . I have shown and . Proving would help me complete the argument full circle. Please let me know if the following makes sense (point out errors if any), and help me complete the proof. : If and is continuous, then , and hence meaning is continuous. Does this need more argument? The continuous image of a compact set is compact. Hence, , i.e. the graph of is compact in . I found the following for : Let be compact and be a convergent sequence in . We show that converges to . Since the graph is compact, has a convergent subsequence, Q1. is compact, so has a convergent subsequence. How does this tell us that has a convergent subsequence? I feel we need to add some details here. i.e. . That is . The graph is closed. That is, the limit of every convergent sequence in is again in . Therefore , i.e. . Since this is true for every convergent subsequence, we showed that is the only limit point of , i.e. converges to . Q2. How is the last sentence concluded? We choose an arbitrary convergent subsequence , and found . What now? My attempt for : Suppose is compact in . Define . Suppose is an arbitrary closed set in . We can write The above expression seems good intuitively but I need help establishing it by showing and inclusions. Next, is closed tells us that is closed - since are continuous. , and is closed. So, is compact. is continuous, which means is compact. Hence, is closed. Done! Please help me fill in the gaps in my attempt , and let me know if you've any other ideas to solve the problem, such as and . I'd also like to understand the proof posted above my attempt. Thank you!","f: E\to\mathbb R G: E\to\mathbb R^2 G(x) = (x,f(x)) x\in E E \mathbb R f G f \mathbb R^2 E = [a,b] 1\implies 2 2\implies 3 3\implies 1 [1\implies 2] x_n\to x f f(x_n)\to f(x) G(x_n)\to G(x) G [2\implies 3]: G([a,b]) f \mathbb R^2 [3\implies 1] G([a,b]) x_n\rightarrow x [a,b] f(x_n) f(x) f(x_n) G([a,b]) (x_n,f(x_n)) f(x_n) f(x_{n_j})\rightarrow y (x_{n_j}, f(x_{n_j}))\rightarrow (x, y) G([a,b]) G([a,b]) (x,y)\in G([a,b]) y=f(x) f(x) f(x_n) f(x_n) f(x) f(x_{n_j}) \to y y = f(x) [3\implies 1] G_f = \{(x,f(x)): x\in [a,b]\} \mathbb R^2 \phi_1(x,y) = x, \phi_2(x,y) = y S\subset\mathbb R \mathbb R f^{-1}(S) = \{x\in [a,b]: f(x) \in S\} f^{-1}(S) = \phi_1(G_f \cap \phi_2^{-1}(S)) \subseteq \supseteq S\subset \mathbb R \phi_2^{-1}(S) \subset \mathbb R^2 \phi_1,\phi_2 G_f \cap \phi_2^{-1}(S)\subset G_f G_f \cap \phi_2^{-1}(S) G_f \cap \phi_2^{-1}(S) \phi_1 f^{-1}(S) f^{-1}(S) [3\implies 2] [2\implies 1]","['real-analysis', 'analysis', 'solution-verification', 'proof-explanation']"
12,How do I evaluate $\int_{-1}^1\frac{dx}{(1+x^2)(e^x+1)}$?,How do I evaluate ?,\int_{-1}^1\frac{dx}{(1+x^2)(e^x+1)},"How do I evaluate: $$\int_{-1}^1\frac{dx}{(1+x^2)(e^x+1)}$$ The answer given in the book is written below. $$\int_{-1}^1\frac{dx}{(1+x^2)(e^{-x}+1)} \tag{1}$$ $$=\int_{-1}^1\frac{e^x}{(1+x^2)(e^x+1)}dx \tag{2}$$ On adding $(1)$ and $(2)$ , we get: $$\Rightarrow 2I=\int_{-1}^1\frac{e^x+1}{(1+x^2)(e^x+1)}dx \tag{3}$$ $$=\int_{-1}^1\frac{dx}{1+x^2} =2\int_{0}^1\frac{dx}{1+x^2} \tag{4}$$ $$\Rightarrow I=\int_{0}^1\frac{dx}{1+x^2}=[ \tan^{-1}x]\:_{0}^{1}\:=\:\frac{\pi}{4} \tag{5}$$ $(2): \int_{a}^{b}{f(x)dx}=\int_{a}^{b}{f(a+b-x)dx}$ $(4): \frac{1}{1+x^2} \text{ is an even function}$ I understood how to solve the integral, but I'm not able to understand how the original integral changed to the integral in $(1)$ . It might be something basic, most probably related to exponents but please tell because I'm a complete beginner in calculus.","How do I evaluate: The answer given in the book is written below. On adding and , we get: I understood how to solve the integral, but I'm not able to understand how the original integral changed to the integral in . It might be something basic, most probably related to exponents but please tell because I'm a complete beginner in calculus.",\int_{-1}^1\frac{dx}{(1+x^2)(e^x+1)} \int_{-1}^1\frac{dx}{(1+x^2)(e^{-x}+1)} \tag{1} =\int_{-1}^1\frac{e^x}{(1+x^2)(e^x+1)}dx \tag{2} (1) (2) \Rightarrow 2I=\int_{-1}^1\frac{e^x+1}{(1+x^2)(e^x+1)}dx \tag{3} =\int_{-1}^1\frac{dx}{1+x^2} =2\int_{0}^1\frac{dx}{1+x^2} \tag{4} \Rightarrow I=\int_{0}^1\frac{dx}{1+x^2}=[ \tan^{-1}x]\:_{0}^{1}\:=\:\frac{\pi}{4} \tag{5} (2): \int_{a}^{b}{f(x)dx}=\int_{a}^{b}{f(a+b-x)dx} (4): \frac{1}{1+x^2} \text{ is an even function} (1),"['real-analysis', 'calculus', 'integration']"
13,Uniqueness of the heat equation for initial data in $L^\infty$,Uniqueness of the heat equation for initial data in,L^\infty,"For $1 \leq p < \infty$ and initial-data $u _ 0 \in L ^p ( \mathbb{R} ^d)$ due to P. Li (Uniqueness of $L^1$ solutions for the Laplace equation and the heat equation on Riemannian manifolds) there exists a unique solution $u$ of the heat equation $u_t - \Delta u = 0$ in $(0, \infty) \times \mathbb{ R }^d$ which satisfies \begin{equation*} 		u \in C ( [0 , \infty ) ; L^p ( \mathbb{ R }^d) ) \quad \text{and} \quad u ( 0 , \cdot ) = u_0. 	\end{equation*} This solution is given by $u ( t ) = e ^{t \Delta} u_0$ , the convolution with the fundamental solution. My question is, whether there is known literature for the uniqueness in the case of non-negative $u_0 \in L^\infty ( \mathbb{ R }^d )$ for the heat equation in $(0, \infty) \times \mathbb{ R }^d$ , where we replace $u \in C ( [0 , \infty ) ; L^p ( \mathbb{ R }^d) )$ by $u \in L^\infty ( (0, \infty) \times \mathbb{ R }^d)$ and $u(t,x) \to u_0(x)$ for almost every $x \in \mathbb{ R }^d$ as $t \to 0$ . Is weak star convergency of $u(t, \cdot)$ to $u_0$ in $L^\infty(\mathbb{R}^d)$ as $t \to 0$ sufficient?","For and initial-data due to P. Li (Uniqueness of solutions for the Laplace equation and the heat equation on Riemannian manifolds) there exists a unique solution of the heat equation in which satisfies This solution is given by , the convolution with the fundamental solution. My question is, whether there is known literature for the uniqueness in the case of non-negative for the heat equation in , where we replace by and for almost every as . Is weak star convergency of to in as sufficient?","1 \leq p < \infty u _ 0 \in L ^p ( \mathbb{R} ^d) L^1 u u_t - \Delta u = 0 (0, \infty) \times \mathbb{ R }^d \begin{equation*}
		u \in C ( [0 , \infty ) ; L^p ( \mathbb{ R }^d) ) \quad \text{and} \quad u ( 0 , \cdot ) = u_0.
	\end{equation*} u ( t ) = e ^{t \Delta} u_0 u_0 \in L^\infty ( \mathbb{ R }^d ) (0, \infty) \times \mathbb{ R }^d u \in C ( [0 , \infty ) ; L^p ( \mathbb{ R }^d) ) u \in L^\infty ( (0, \infty) \times \mathbb{ R }^d) u(t,x) \to u_0(x) x \in \mathbb{ R }^d t \to 0 u(t, \cdot) u_0 L^\infty(\mathbb{R}^d) t \to 0","['real-analysis', 'partial-differential-equations', 'heat-equation', 'parabolic-pde']"
14,Counterexample to Riemann sum limit,Counterexample to Riemann sum limit,,"For convergent Riemann sums of $f \in C^1([0,1])$ there is the property: $$\tag{A}\lim_{n \to + \infty} \left[\, \sum_{k=0}^{n} f \left( \frac{k}{n+1} \right) - \sum_{k=0}^{n-1} f \left( \frac{k}{n} \right) \, \right] = \int_0^1 f (x) \ dx$$ Proof using the mean value theorem goes like $$\, \sum_{k=0}^{n} f \left( \frac{k}{n+1} \right) - \sum_{k=0}^{n-1} f \left( \frac{k}{n} \right) = f \left( \frac{n}{n+1} \right) + \sum_{k=0}^{n-1}f'(\theta_k)\left(\frac{k}{n+1} - \frac{k}{n} \right)$$ and then shows the right side converges to $f(1) - \int_0^1 xf'(x) dx = \int_0^1f(x)dx.$ So it is very helpful that $f \in C^1$ . If we only have $f \in C[0,1]$ , then equation (A) may not be true, and this was shown by Daniel Fischer using Banach-Steinhaus theorem in this answer . Here a related statement was considered: $$\tag{B}\lim_{n\to\infty}\left[\, \sum_{k=0}^{n-1} f\left(\frac{k}{n}\right) - n\int_0^1 f(x)\,dx\, \right] = \frac{f(0) - f(1)}{2}$$ My question : Can someone please show me a concrete counterexample, i.e., a continuous but not continuously differentiable function where (A) or (B) is false?","For convergent Riemann sums of there is the property: Proof using the mean value theorem goes like and then shows the right side converges to So it is very helpful that . If we only have , then equation (A) may not be true, and this was shown by Daniel Fischer using Banach-Steinhaus theorem in this answer . Here a related statement was considered: My question : Can someone please show me a concrete counterexample, i.e., a continuous but not continuously differentiable function where (A) or (B) is false?","f \in C^1([0,1]) \tag{A}\lim_{n \to + \infty} \left[\, \sum_{k=0}^{n} f \left( \frac{k}{n+1} \right) - \sum_{k=0}^{n-1} f \left( \frac{k}{n} \right) \, \right] = \int_0^1 f (x) \ dx \, \sum_{k=0}^{n} f \left( \frac{k}{n+1} \right) - \sum_{k=0}^{n-1} f \left( \frac{k}{n} \right) = f \left( \frac{n}{n+1} \right) + \sum_{k=0}^{n-1}f'(\theta_k)\left(\frac{k}{n+1} - \frac{k}{n} \right) f(1) - \int_0^1 xf'(x) dx = \int_0^1f(x)dx. f \in C^1 f \in C[0,1] \tag{B}\lim_{n\to\infty}\left[\, \sum_{k=0}^{n-1} f\left(\frac{k}{n}\right) - n\int_0^1 f(x)\,dx\, \right] = \frac{f(0) - f(1)}{2}","['real-analysis', 'examples-counterexamples', 'riemann-integration']"
15,prove this inequality with $63$,prove this inequality with,63,"Let $x,y,z,w>0$, and such $x^2+y^2+z^2+w^2=1$. show that   $$x+y+z+w+\dfrac{1}{63xyzw}\ge\dfrac{142}{63}\tag{1}$$ I know $$x^2+y^2+z^2+w^2\ge 4\sqrt[4]{x^2y^2z^2w^2}\Longrightarrow xyzw\le \dfrac{1}{16}$$ so we have $$\dfrac{1}{63xyzw}\ge\dfrac{16}{63}$$ But other hand $$(x^2+y^2+z^2+w^2)\ge\dfrac{1}{4}(x+y+z+w)^2\Longrightarrow x+y+z+w\le 2$$ So the above solution is not correct, so how do you prove this inequality $(1)$","Let $x,y,z,w>0$, and such $x^2+y^2+z^2+w^2=1$. show that   $$x+y+z+w+\dfrac{1}{63xyzw}\ge\dfrac{142}{63}\tag{1}$$ I know $$x^2+y^2+z^2+w^2\ge 4\sqrt[4]{x^2y^2z^2w^2}\Longrightarrow xyzw\le \dfrac{1}{16}$$ so we have $$\dfrac{1}{63xyzw}\ge\dfrac{16}{63}$$ But other hand $$(x^2+y^2+z^2+w^2)\ge\dfrac{1}{4}(x+y+z+w)^2\Longrightarrow x+y+z+w\le 2$$ So the above solution is not correct, so how do you prove this inequality $(1)$",,"['real-analysis', 'multivariable-calculus', 'inequality', 'uvw', 'mixing-variables']"
16,Show that Hill's Equation $u'' + a(t)u=0$ if $a(t)<0$ for all $t$ then $u\to\infty$ as $t\to\infty$,Show that Hill's Equation  if  for all  then  as,u'' + a(t)u=0 a(t)<0 t u\to\infty t\to\infty,"(a) Consider the Hill's equation   $$u'' + a(t)u = 0,$$   where $a(t+T) = a(t)$ for all $t$. Show that, if $a(t)<0$ for all $t$, then the solution satisfying the initial condition    $$u(0)=u'(0)=1$$   is unbounded as $t\to\infty$. Suppose $a(t) < 0$ for all $t$. Then, consider the expression, \begin{align} u'(t) = 1 - \int_0^t a(s)u(s)ds, \tag{1}\end{align} so that $$u''(t) = -a(t)u(t)$$ Now we'll show $u'(t) \geq 1$ for all $t \geq 0$. By contradiction, suppose there exists some $t_0 > 0$ such that $u'(t_0) \leq \frac{1}{2}$, then $u(s) = 1 + \frac{s}{2}$ for all $0 \leq s \leq t_0$, then since $a(s) < 0$ for all $s$, and $u(s) \geq 1$ on the interval $(0,s)$, which is a contradiction since $u(t_0)\leq\frac{1}{2}$. Then, the integral $\int_0^t a(s)u(s) \leq 0$ so that $u'(t) \geq 1$ for all $t\geq 0$ and, hence,  \begin{align*} \lim_{t\to\infty} u(t) &= \lim_{t\to\infty} \int u'(t) dt \\ &= \lim_{t\to\infty} \int \left( 1 - \int_0^t a(s)u(s)ds \right) \\ &\to \infty \end{align*} since $a(s)<0$ for all $s$ and $u(s)>0$ for all $s\in(0,t)$ we know that  $$-\int_0^t a(s)u(s) \geq 0$$ Then,  $$\lim_{t\to\infty} \int \left( 1 - \int_0^t a(s)u(s)ds \right) \geq \lim_{t\to\infty} \int_0^t dt \to \infty$$ so $u$ is unbounded as $t\to\infty$. (b) Next suppose that $a(t)>0$ for all $t$ and    $$\int_0^T a(t)dt < \dfrac{4}{T}$$   It may be shown that all solutions are bounded as $t\to\infty$. Use this result and that of part (a) to estimate the stable and unstable zones in the $\delta-\epsilon$ plane for the Mathieu equation and Missner's equation. I am not sure how to proceed with this part. Also, I do not feel too confident in my proof of part (a) the proof by contradiction felt a little wonky since I never actually reached a contradiction. Any advise would be appreciated.","(a) Consider the Hill's equation   $$u'' + a(t)u = 0,$$   where $a(t+T) = a(t)$ for all $t$. Show that, if $a(t)<0$ for all $t$, then the solution satisfying the initial condition    $$u(0)=u'(0)=1$$   is unbounded as $t\to\infty$. Suppose $a(t) < 0$ for all $t$. Then, consider the expression, \begin{align} u'(t) = 1 - \int_0^t a(s)u(s)ds, \tag{1}\end{align} so that $$u''(t) = -a(t)u(t)$$ Now we'll show $u'(t) \geq 1$ for all $t \geq 0$. By contradiction, suppose there exists some $t_0 > 0$ such that $u'(t_0) \leq \frac{1}{2}$, then $u(s) = 1 + \frac{s}{2}$ for all $0 \leq s \leq t_0$, then since $a(s) < 0$ for all $s$, and $u(s) \geq 1$ on the interval $(0,s)$, which is a contradiction since $u(t_0)\leq\frac{1}{2}$. Then, the integral $\int_0^t a(s)u(s) \leq 0$ so that $u'(t) \geq 1$ for all $t\geq 0$ and, hence,  \begin{align*} \lim_{t\to\infty} u(t) &= \lim_{t\to\infty} \int u'(t) dt \\ &= \lim_{t\to\infty} \int \left( 1 - \int_0^t a(s)u(s)ds \right) \\ &\to \infty \end{align*} since $a(s)<0$ for all $s$ and $u(s)>0$ for all $s\in(0,t)$ we know that  $$-\int_0^t a(s)u(s) \geq 0$$ Then,  $$\lim_{t\to\infty} \int \left( 1 - \int_0^t a(s)u(s)ds \right) \geq \lim_{t\to\infty} \int_0^t dt \to \infty$$ so $u$ is unbounded as $t\to\infty$. (b) Next suppose that $a(t)>0$ for all $t$ and    $$\int_0^T a(t)dt < \dfrac{4}{T}$$   It may be shown that all solutions are bounded as $t\to\infty$. Use this result and that of part (a) to estimate the stable and unstable zones in the $\delta-\epsilon$ plane for the Mathieu equation and Missner's equation. I am not sure how to proceed with this part. Also, I do not feel too confident in my proof of part (a) the proof by contradiction felt a little wonky since I never actually reached a contradiction. Any advise would be appreciated.",,"['real-analysis', 'ordinary-differential-equations']"
17,Prove that $f(ax + (1-a)y) = \frac{1}{y-x}\int_x^y f(t)dt$ implies $a = \frac{1}{2}$,Prove that  implies,f(ax + (1-a)y) = \frac{1}{y-x}\int_x^y f(t)dt a = \frac{1}{2},"Let $f: \mathbb{R} \to \mathbb{R}$ be a non-constant function, integrable on every interval $[x,y] \subset \mathbb{R}$ and $a \in \mathbb{R}$ such that $$f(ax + (1-a)y) = \frac{1}{y-x}\int_x^y{f(t)dt}, \quad \forall x<y$$   Prove that $\displaystyle a = \frac{1}{2}$. This is a small part from another problem. If I can prove that $\displaystyle{a = \frac{1}{2}}$ then the whole problem is solved, but I don't know how. Edit : If $f$ is integrable, then the function $F : \mathbb{R} \to \mathbb{R}, F(x) = \displaystyle{\int_m^x{f(t)dt}}, m \in \mathbb{R}$ is continuous. Since we have that $\displaystyle{f(ax + (1-a)y) = \frac{1}{y-x}\int_x^y{f(t)dt}, \forall x<y}$, then $f$ is continuous and $F$ is differentiable. This implies that $f \in C^{\infty}(\mathbb{R})$. If $\displaystyle{a = \frac{1}{2}}$, then $\displaystyle{f\left(\frac{x+y}{2}\right) = \frac{1}{y-x}\int_x^y{f(t)dt}, \forall x < y}$. So we have equality in the Hermite-Hadamard inequality, which is achieved when $f$ is linear on that interval.","Let $f: \mathbb{R} \to \mathbb{R}$ be a non-constant function, integrable on every interval $[x,y] \subset \mathbb{R}$ and $a \in \mathbb{R}$ such that $$f(ax + (1-a)y) = \frac{1}{y-x}\int_x^y{f(t)dt}, \quad \forall x<y$$   Prove that $\displaystyle a = \frac{1}{2}$. This is a small part from another problem. If I can prove that $\displaystyle{a = \frac{1}{2}}$ then the whole problem is solved, but I don't know how. Edit : If $f$ is integrable, then the function $F : \mathbb{R} \to \mathbb{R}, F(x) = \displaystyle{\int_m^x{f(t)dt}}, m \in \mathbb{R}$ is continuous. Since we have that $\displaystyle{f(ax + (1-a)y) = \frac{1}{y-x}\int_x^y{f(t)dt}, \forall x<y}$, then $f$ is continuous and $F$ is differentiable. This implies that $f \in C^{\infty}(\mathbb{R})$. If $\displaystyle{a = \frac{1}{2}}$, then $\displaystyle{f\left(\frac{x+y}{2}\right) = \frac{1}{y-x}\int_x^y{f(t)dt}, \forall x < y}$. So we have equality in the Hermite-Hadamard inequality, which is achieved when $f$ is linear on that interval.",,"['real-analysis', 'integration', 'definite-integrals']"
18,When can't you change the order of integration?,When can't you change the order of integration?,,"Apparently there are integrals which you can express as $\int_A \int_B f(x, y) \mathop{}\!\mathrm{d}x \mathop{}\!\mathrm{d}y$ but not $\int_C \int_D f(x, y) \mathop{}\!\mathrm{d}y \mathop{}\!\mathrm{d}x$. When would this be the case? Is it to do with their limits not being invertible functions?","Apparently there are integrals which you can express as $\int_A \int_B f(x, y) \mathop{}\!\mathrm{d}x \mathop{}\!\mathrm{d}y$ but not $\int_C \int_D f(x, y) \mathop{}\!\mathrm{d}y \mathop{}\!\mathrm{d}x$. When would this be the case? Is it to do with their limits not being invertible functions?",,"['calculus', 'real-analysis', 'multivariable-calculus']"
19,Is there a tangential surface integral?,Is there a tangential surface integral?,,"In $\mathbb{R}^2$, we have two different types of line integrals, the tangential line integral $$\int_C  \mathbf{F}\cdot d\mathbf r$$ and  the normal line integral $$\int_C  \mathbf{F}\cdot \mathbf n \,ds.$$ To give a motivation, these two different integrals are very helpful for understanding Divergence theorem and Stoke's theorem. Let $\mathbf {F} = P \mathbf i + Q\mathbf j$. The Divergence Theorem says $$\iint_D \text{div}(\mathbf F)\,dx\,dy = \int_C  \mathbf{F}\cdot \mathbf n \,ds.$$ This gives  $$\iint_D (P_x + Q_y) \,dx\,dy=\int_C \mathbf{F}\cdot (-dy, dx) = \int_C P\,dy - Q\,dx.$$ Stoke's theorem is  $$\iint_D \text{curl}(\mathbf F)\cdot \mathbf k \,dx\,dy = \int_C  \mathbf{F}\cdot d\mathbf r$$ and this gives  $$\iint_D (P_y - Q_x) \,dx\,dy=\int_C \mathbf{F}\cdot (dx, dy) = \int_C P\,dx + Q\,dy.$$ Green's theorem can be derived from either of the two above theorems. However, for all the surface integral, I have only seen the normal surface integral defined by  $$\iint_D \mathbf F\cdot \mathbf n\, dS$$ is there a similar concept that is close to the tangential line integral $\int_C  \mathbf{F}\cdot d\mathbf r$?","In $\mathbb{R}^2$, we have two different types of line integrals, the tangential line integral $$\int_C  \mathbf{F}\cdot d\mathbf r$$ and  the normal line integral $$\int_C  \mathbf{F}\cdot \mathbf n \,ds.$$ To give a motivation, these two different integrals are very helpful for understanding Divergence theorem and Stoke's theorem. Let $\mathbf {F} = P \mathbf i + Q\mathbf j$. The Divergence Theorem says $$\iint_D \text{div}(\mathbf F)\,dx\,dy = \int_C  \mathbf{F}\cdot \mathbf n \,ds.$$ This gives  $$\iint_D (P_x + Q_y) \,dx\,dy=\int_C \mathbf{F}\cdot (-dy, dx) = \int_C P\,dy - Q\,dx.$$ Stoke's theorem is  $$\iint_D \text{curl}(\mathbf F)\cdot \mathbf k \,dx\,dy = \int_C  \mathbf{F}\cdot d\mathbf r$$ and this gives  $$\iint_D (P_y - Q_x) \,dx\,dy=\int_C \mathbf{F}\cdot (dx, dy) = \int_C P\,dx + Q\,dy.$$ Green's theorem can be derived from either of the two above theorems. However, for all the surface integral, I have only seen the normal surface integral defined by  $$\iint_D \mathbf F\cdot \mathbf n\, dS$$ is there a similar concept that is close to the tangential line integral $\int_C  \mathbf{F}\cdot d\mathbf r$?",,"['real-analysis', 'multivariable-calculus', 'partial-differential-equations', 'vector-analysis', 'surface-integrals']"
20,finite polynomials satisfy $|f(x)|\le 2^x$,finite polynomials satisfy,|f(x)|\le 2^x,"This is a problem from TsingHua University math competition for high school students. Prove there exists only finite number of polynomials $f\in \mathbb{Z}[x]$ such that for any $x\in \mathbb{N}$ , $|f(x)|\le 2^x$. My attempts: since $f(x)=o(2^x)$, $f$ can only be bounded when $x$ is small, for example $f(0)=0,1, \ \ f(1)=0,1,2, \cdot \cdot \cdot$. Thus using Lagrange interpolation it concludes that for any given $n\in \mathbb{N}$ the number for such polynomial is finite. But I don't know how to proceed from here. Is my methods wrong or there's a better way? Also, I'd like to know something about the background of this problem. Thanks in advance.","This is a problem from TsingHua University math competition for high school students. Prove there exists only finite number of polynomials $f\in \mathbb{Z}[x]$ such that for any $x\in \mathbb{N}$ , $|f(x)|\le 2^x$. My attempts: since $f(x)=o(2^x)$, $f$ can only be bounded when $x$ is small, for example $f(0)=0,1, \ \ f(1)=0,1,2, \cdot \cdot \cdot$. Thus using Lagrange interpolation it concludes that for any given $n\in \mathbb{N}$ the number for such polynomial is finite. But I don't know how to proceed from here. Is my methods wrong or there's a better way? Also, I'd like to know something about the background of this problem. Thanks in advance.",,"['real-analysis', 'polynomials', 'contest-math', 'nonlinear-optimization']"
21,Is there a mathematical statement that is linking integer limits to real limits?,Is there a mathematical statement that is linking integer limits to real limits?,,"I saw a question asking for the limit $$\lim_{n \to \infty}\frac{\tan(n)}{n}.$$ At first I thought that the limit assumed $n$ to be a real number. So I gave the advice to use $\pi/2+2\pi k$ and $2\pi k$ as two sequences with different limits. The real limit for $x\to \infty$, in which $x \in \mathbb{R}$, is much easier to handle than the limit $n \to \infty$, in which $n \in \mathbb{N}$. Here is my question: Is there a mathematical theorem that is linking the integer limit    $$\lim_{n\to \infty}f(n)$$ to the real limit $$\lim_{x\to\infty}f(x)?$$ Is the equidistribution theorem such a mathematical statement?","I saw a question asking for the limit $$\lim_{n \to \infty}\frac{\tan(n)}{n}.$$ At first I thought that the limit assumed $n$ to be a real number. So I gave the advice to use $\pi/2+2\pi k$ and $2\pi k$ as two sequences with different limits. The real limit for $x\to \infty$, in which $x \in \mathbb{R}$, is much easier to handle than the limit $n \to \infty$, in which $n \in \mathbb{N}$. Here is my question: Is there a mathematical theorem that is linking the integer limit    $$\lim_{n\to \infty}f(n)$$ to the real limit $$\lim_{x\to\infty}f(x)?$$ Is the equidistribution theorem such a mathematical statement?",,"['real-analysis', 'limits', 'elementary-number-theory', 'real-numbers']"
22,$\sum \frac{a_n}{\ln a_n}$ converges $\implies \sum \frac{a_n}{\ln (1+n)}$ converges,converges  converges,\sum \frac{a_n}{\ln a_n} \implies \sum \frac{a_n}{\ln (1+n)},"Let $0<a_n<1$ be real numbers. Prove that $\sum \frac{a_n}{\ln a_n}$ converges $\implies \sum \frac{a_n}{\ln (1+n)}$ converges Note that $\displaystyle \frac{a_n}{\ln (1+n)} \leq \frac{a_n}{-\ln a_n}\iff a_n\geq \frac{1}{n+1}$ If $a_n\geq \frac{1}{n+1}$ for sufficiently many $n$, the series $\sum \frac{a_n}{\ln (1+n)}$ can thefore be compared with $\sum \frac{a_n}{-\ln a_n}$ and we're done. Otherwise, $a_n< \frac{1}{n+1}$ and we're tempted to consider the series $\displaystyle \sum \frac{1}{(n+1)\ln (1+n)}$ which is unfortunately divergent. That's as I far as I can go with  this problem. Summation by part is a dead end as well.","Let $0<a_n<1$ be real numbers. Prove that $\sum \frac{a_n}{\ln a_n}$ converges $\implies \sum \frac{a_n}{\ln (1+n)}$ converges Note that $\displaystyle \frac{a_n}{\ln (1+n)} \leq \frac{a_n}{-\ln a_n}\iff a_n\geq \frac{1}{n+1}$ If $a_n\geq \frac{1}{n+1}$ for sufficiently many $n$, the series $\sum \frac{a_n}{\ln (1+n)}$ can thefore be compared with $\sum \frac{a_n}{-\ln a_n}$ and we're done. Otherwise, $a_n< \frac{1}{n+1}$ and we're tempted to consider the series $\displaystyle \sum \frac{1}{(n+1)\ln (1+n)}$ which is unfortunately divergent. That's as I far as I can go with  this problem. Summation by part is a dead end as well.",,"['real-analysis', 'sequences-and-series']"
23,"Proving continuity and monotonicity of $t\mapsto t^x, t>0$ with minimal assumptions.",Proving continuity and monotonicity of  with minimal assumptions.,"t\mapsto t^x, t>0","I'm trying to prove that The function $t\mapsto t^x,\, x\in \Bbb R,\, t>0$ is continuous and monotonic. Suppose $+, \cdot\,:\Bbb R^2\to \Bbb R$ (addition and multiplication) have already been defined (via the standard dedekind cuts construction). If $a\in \Bbb Z$, we define $t^a=t^{a-1}\cdot t$, $t^{1}=t$. If $r=\frac a b\in \Bbb Q$, we define $t^r=\sup \{x: x^b<t^a\}$. If $x\in\Bbb R$, we define, for $t>1$, $t^x=\sup\{t^r: r<x, r\in\Bbb Q\}$.  And if $t<1$ we swap $\sup$ by $\inf$ in the previous definition. I want to avoid usage of the exponential function $e^x=\sum \frac {x^n}{n!}$, series and further concepts, as I want to build the foundation for those first.","I'm trying to prove that The function $t\mapsto t^x,\, x\in \Bbb R,\, t>0$ is continuous and monotonic. Suppose $+, \cdot\,:\Bbb R^2\to \Bbb R$ (addition and multiplication) have already been defined (via the standard dedekind cuts construction). If $a\in \Bbb Z$, we define $t^a=t^{a-1}\cdot t$, $t^{1}=t$. If $r=\frac a b\in \Bbb Q$, we define $t^r=\sup \{x: x^b<t^a\}$. If $x\in\Bbb R$, we define, for $t>1$, $t^x=\sup\{t^r: r<x, r\in\Bbb Q\}$.  And if $t<1$ we swap $\sup$ by $\inf$ in the previous definition. I want to avoid usage of the exponential function $e^x=\sum \frac {x^n}{n!}$, series and further concepts, as I want to build the foundation for those first.",,['real-analysis']
24,Calculating 2 integrals in polylogarithmic functions,Calculating 2 integrals in polylogarithmic functions,,"Are we aware of any nice way of calculating these $2$ integrals? $$i) \space \int_0^1 \frac{\text{Li}_2\left(x-x^2\right)}{x^2-x+1} \, dx$$ $$ii)\space \int_0^1 \frac{\text{Li}_2\left(x-x^2\right)}{\left(x^2-x+1\right)^2} \, dx$$ If considering in the first integral that  $$\frac{1}{{x^2-x+1}}=\frac{1}{i \sqrt{3}}\left(\frac{1}{ \left(x-\left(\frac{1}{2}+\frac{i \sqrt{3}}{2}\right)\right)}-\frac{1}{ \left(x-\left(\frac{1}{2}-\frac{i \sqrt{3}}{2}\right)\right)}\right)$$ then it seems Mathematica can handle the resulting integrals. It's also easy to note that $$\frac{1}{i \sqrt{3}}\int_0^1 \frac{(1-2x)\log \left(x-\frac{1}{2}-\frac{i \sqrt{3}}{2}\right) \log \left(x-\frac{1}{2}+\frac{i \sqrt{3}}{2}\right)}{ x(1-x)} \, dx=0$$ and then for the remaining part Mathematica yields some shorter result in polylogarithms. So, we have that  $$ \int_0^1 \frac{\text{Li}_2\left(x-x^2\right)}{x^2-x+1} \, dx$$ $$=\frac{i}{\sqrt{3}}\int_0^1 \frac{ (1-2 x) \log ^2\left(x-\frac{1}{2}+\frac{i \sqrt{3}}{2}\right)}{ x-x^2} \, dx-\frac{i}{\sqrt{3}}\int_0^1 \frac{ (1-2 x) \log ^2\left(x-\frac{1}{2}-\frac{i \sqrt{3}}{2}\right)}{ x-x^2} \, dx$$ $$=\frac{2i}{\sqrt{3}}\int_0^1 \frac{ (1-2 x) \log ^2\left(x-\frac{1}{2}+\frac{i \sqrt{3}}{2}\right)}{ x-x^2} \, dx$$ where the last equality is simply got by using the variable change $x\mapsto 1-x$. EDIT : Using the idea of dilogarithm reflection formula, we get that $$\int_0^1 \frac{\text{Li}_2\left(x^2-x+1\right)}{x^2-x+1} \, dx=\frac{2}{\sqrt{3}} \int_{-\pi/6}^{\pi/6} \text{Li}_2\left(\frac{3 \sec ^2(x)}{4}\right) \, dx$$ $$=\frac{4}{\sqrt{3}} \int_{-\pi/6}^{\pi/6} \text{Li}_2\left(-\frac{\sqrt{3}}{2}  \sec (x)\right) \, dx+\frac{4}{\sqrt{3}} \int_{-\pi/6}^{\pi/6} \text{Li}_2\left(\frac{\sqrt{3}}{2}  \sec (x)\right) \, dx$$ but not sure yet if it might lead to an elegant solution.","Are we aware of any nice way of calculating these $2$ integrals? $$i) \space \int_0^1 \frac{\text{Li}_2\left(x-x^2\right)}{x^2-x+1} \, dx$$ $$ii)\space \int_0^1 \frac{\text{Li}_2\left(x-x^2\right)}{\left(x^2-x+1\right)^2} \, dx$$ If considering in the first integral that  $$\frac{1}{{x^2-x+1}}=\frac{1}{i \sqrt{3}}\left(\frac{1}{ \left(x-\left(\frac{1}{2}+\frac{i \sqrt{3}}{2}\right)\right)}-\frac{1}{ \left(x-\left(\frac{1}{2}-\frac{i \sqrt{3}}{2}\right)\right)}\right)$$ then it seems Mathematica can handle the resulting integrals. It's also easy to note that $$\frac{1}{i \sqrt{3}}\int_0^1 \frac{(1-2x)\log \left(x-\frac{1}{2}-\frac{i \sqrt{3}}{2}\right) \log \left(x-\frac{1}{2}+\frac{i \sqrt{3}}{2}\right)}{ x(1-x)} \, dx=0$$ and then for the remaining part Mathematica yields some shorter result in polylogarithms. So, we have that  $$ \int_0^1 \frac{\text{Li}_2\left(x-x^2\right)}{x^2-x+1} \, dx$$ $$=\frac{i}{\sqrt{3}}\int_0^1 \frac{ (1-2 x) \log ^2\left(x-\frac{1}{2}+\frac{i \sqrt{3}}{2}\right)}{ x-x^2} \, dx-\frac{i}{\sqrt{3}}\int_0^1 \frac{ (1-2 x) \log ^2\left(x-\frac{1}{2}-\frac{i \sqrt{3}}{2}\right)}{ x-x^2} \, dx$$ $$=\frac{2i}{\sqrt{3}}\int_0^1 \frac{ (1-2 x) \log ^2\left(x-\frac{1}{2}+\frac{i \sqrt{3}}{2}\right)}{ x-x^2} \, dx$$ where the last equality is simply got by using the variable change $x\mapsto 1-x$. EDIT : Using the idea of dilogarithm reflection formula, we get that $$\int_0^1 \frac{\text{Li}_2\left(x^2-x+1\right)}{x^2-x+1} \, dx=\frac{2}{\sqrt{3}} \int_{-\pi/6}^{\pi/6} \text{Li}_2\left(\frac{3 \sec ^2(x)}{4}\right) \, dx$$ $$=\frac{4}{\sqrt{3}} \int_{-\pi/6}^{\pi/6} \text{Li}_2\left(-\frac{\sqrt{3}}{2}  \sec (x)\right) \, dx+\frac{4}{\sqrt{3}} \int_{-\pi/6}^{\pi/6} \text{Li}_2\left(\frac{\sqrt{3}}{2}  \sec (x)\right) \, dx$$ but not sure yet if it might lead to an elegant solution.",,"['calculus', 'real-analysis', 'integration', 'definite-integrals']"
25,Is $\int_{\mathbb R} f(\sum_{k=1}^n\frac{1}{x-x_k})dx$ independent of $x_k$'s for certain $f$?,Is  independent of 's for certain ?,\int_{\mathbb R} f(\sum_{k=1}^n\frac{1}{x-x_k})dx x_k f,"My question below arises from the linked question $ \int\limits_{-\infty}^{+\infty}\frac{(p'(x))^2}{(p'(x))^2+(p(x))^2}\,dx \leq n^{3/2}\pi$ and the comments by jack and David Speyer under that question. Let $p(x)=\prod_{k=1}^n(x-x_k)$, where $n\ge 1$ and $x_1,\dots,x_n\in\mathbb R$. Then for $f(t)=\frac{t^2}{1+t^2}$, the integral appearing in the linked question can be rewritten as $$\int_{-\infty}^{+\infty}f\left(\frac{p'(x)}{p(x)}\right)dx= \int_{-\infty}^{+\infty}f\left(\sum_{k=1}^n\frac{1}{x-x_k}\right)dx. $$ Then as noted by jack and David Speyer, the first question is: for $f(t)=\frac{t^2}{1+t^2}$, how to prove $$\int_{-\infty}^{+\infty}f\left(\sum_{k=1}^n\frac{1}{x-x_k}\right)dx=\int_{-\infty}^{+\infty}f\left(\frac{n}{x}\right)dx \tag{$*$} $$ always hold for every $n\ge 2$? Moreover, it seems that $(*)$ also holds for more general form of $f$. Then the second  question comes: for what $f$, $(*)$ holds for every $n\ge 2$? Any  suggestion is appreciated. Thanks in advance.","My question below arises from the linked question $ \int\limits_{-\infty}^{+\infty}\frac{(p'(x))^2}{(p'(x))^2+(p(x))^2}\,dx \leq n^{3/2}\pi$ and the comments by jack and David Speyer under that question. Let $p(x)=\prod_{k=1}^n(x-x_k)$, where $n\ge 1$ and $x_1,\dots,x_n\in\mathbb R$. Then for $f(t)=\frac{t^2}{1+t^2}$, the integral appearing in the linked question can be rewritten as $$\int_{-\infty}^{+\infty}f\left(\frac{p'(x)}{p(x)}\right)dx= \int_{-\infty}^{+\infty}f\left(\sum_{k=1}^n\frac{1}{x-x_k}\right)dx. $$ Then as noted by jack and David Speyer, the first question is: for $f(t)=\frac{t^2}{1+t^2}$, how to prove $$\int_{-\infty}^{+\infty}f\left(\sum_{k=1}^n\frac{1}{x-x_k}\right)dx=\int_{-\infty}^{+\infty}f\left(\frac{n}{x}\right)dx \tag{$*$} $$ always hold for every $n\ge 2$? Moreover, it seems that $(*)$ also holds for more general form of $f$. Then the second  question comes: for what $f$, $(*)$ holds for every $n\ge 2$? Any  suggestion is appreciated. Thanks in advance.",,"['real-analysis', 'polynomials', 'definite-integrals']"
26,Use of $L^2$ norm in calculus of variations,Use of  norm in calculus of variations,L^2,"I am trying to make an introduction to the calculus of variations. This field has many connections with  functional analysis, in which I do not have an experience. I recently learned about function spaces, for example the space of all continuous functions $C[a,b]$ on the interval $[a,b]$ and the space of square integrable functions $L^2[a,b]$ on the interval $[a,b]$. It seems that in many resources about Calculus of Variations, the functional derivative in the direction of a function $v$ is given with the inner product $\langle \nabla J[u],v\rangle$ where $\nabla J[u]$ is the functional gradient. The inner product is defined as the $L^2$ norm of the form $\langle f, g \rangle = \int_{b}^{a}f(x)g(x)dx$. From there it follows the derivation of the Euler-Langrange formula, etc. My question is, why does one chooses the $L^2$ norm here specifically, in the context of Calculus of Variations? It seems that there are different norms associated with different function spaces in functional analysis: Doesn't this limit the candidate functions in an optimization problem to be in $L^2$ space? Maybe the reason is that $L^2$ norm can be seen as a natural expansion of the dot product in finite Euclidean spaces to infinite dimensional function spaces? I have not well understood this point; maybe I am asking something too obvious, excuse me if it is so, since I am a beginner in this field.","I am trying to make an introduction to the calculus of variations. This field has many connections with  functional analysis, in which I do not have an experience. I recently learned about function spaces, for example the space of all continuous functions $C[a,b]$ on the interval $[a,b]$ and the space of square integrable functions $L^2[a,b]$ on the interval $[a,b]$. It seems that in many resources about Calculus of Variations, the functional derivative in the direction of a function $v$ is given with the inner product $\langle \nabla J[u],v\rangle$ where $\nabla J[u]$ is the functional gradient. The inner product is defined as the $L^2$ norm of the form $\langle f, g \rangle = \int_{b}^{a}f(x)g(x)dx$. From there it follows the derivation of the Euler-Langrange formula, etc. My question is, why does one chooses the $L^2$ norm here specifically, in the context of Calculus of Variations? It seems that there are different norms associated with different function spaces in functional analysis: Doesn't this limit the candidate functions in an optimization problem to be in $L^2$ space? Maybe the reason is that $L^2$ norm can be seen as a natural expansion of the dot product in finite Euclidean spaces to infinite dimensional function spaces? I have not well understood this point; maybe I am asking something too obvious, excuse me if it is so, since I am a beginner in this field.",,"['real-analysis', 'functional-analysis', 'calculus-of-variations']"
27,Innocent looking open problems in real analysis,Innocent looking open problems in real analysis,,"Are there any apparently easy problems or conjectures in basic real analysis (that is, calculus) that are still open? By apparently easy, I mean: so much so, that, if it was for the statement alone, they could be part of a calculus book for undergraduates?","Are there any apparently easy problems or conjectures in basic real analysis (that is, calculus) that are still open? By apparently easy, I mean: so much so, that, if it was for the statement alone, they could be part of a calculus book for undergraduates?",,"['calculus', 'real-analysis', 'soft-question', 'big-list', 'open-problem']"
28,Divergence of a vector field on a sequence of spheres,Divergence of a vector field on a sequence of spheres,,"I'm studying for my exams and I found this problem in the book ""Advanced Calculus"", written by Friedman: ""Consider a sequence of spheres $S_n$ in $\mathbb{R}^3$ with center $P_n$ and radius $r_n$, such that $P_n \to P, r_n \to 0$ as $ n \to \infty.$ Let $\textbf{F}$ be a continuously differentiable vector field in a neighborhood of $P$. Prove that $$(\nabla \cdot \textbf{F})(P)=\lim_{n \to \infty}\frac{1}{V_n}\iint\limits_{S_n}\textbf{F}\cdot \textbf{n}\ dS,$$ where $V_n$ is the volume of the ball with boundary $S_n$."" This is what I did: Observe that, by the Divergence Theorem, $$\frac{1}{V_n}\iint\limits_{S_n}\textbf{F}\cdot \textbf{n}\ dS = \frac{1}{V_n}\iiint\limits_{D_n}\nabla\cdot\textbf{F} \ dV,$$ where $D_n$ is the region bounded by $S_n$. Now, notice that $\dfrac{1}{V_n}\displaystyle \iiint\limits_{D_n}\nabla\cdot\textbf{F} \ dV$ is the mean value of $\nabla \cdot \textbf{F}$ on $D_n$. Since F is continuously differentiable, we have that $\nabla \cdot \textbf{F}$ is continuous, so, for each $n$, there is a point $P_n$ such that $$\frac{1}{V_n} \iiint\limits_{D_n}\nabla\cdot\textbf{F} \ dV = (\nabla \cdot \textbf{F})(P_n)$$ So, $$\lim_{n \to \infty}\frac{1}{V_n}\iint\limits_{S_n}\textbf{F}\cdot \textbf{n} \ dS = \lim_{n \to \infty}\frac{1}{V_n}\iiint\limits_{D_n}\nabla\cdot\textbf{F} \ dV \\ =\lim_{n \to \infty}(\nabla \cdot \textbf{F})(P_n) = (\nabla \cdot \textbf{F})(P),$$ as we wanted. I have two problems here: 1) I don't know how to prove that because $\nabla \cdot \textbf{F}$ is continuous, we certainly have that there is a point $P_n$ such that $\frac{1}{V_n} \iiint\limits_{D_n}\nabla\cdot\textbf{F} \ dV = (\nabla \cdot \textbf{F})(P_n)$. I know it's true because I saw it in some calculus books, but I haven't found a proof. 2) After I finished, I realize that my points $P_n$ doesn't necessarily have to be the points $P_n$ center of $S_n$. So, because I know that the radius $r_n$ of $S_n$ goes to 0, can I assume that $P$ will be the limit of every sequence $(a_n)$ such that $a_k \in S_k$ $\forall k \in \mathbb{N}$? I'd be really grateful if someone could help me with my questions above :) Thanks!","I'm studying for my exams and I found this problem in the book ""Advanced Calculus"", written by Friedman: ""Consider a sequence of spheres $S_n$ in $\mathbb{R}^3$ with center $P_n$ and radius $r_n$, such that $P_n \to P, r_n \to 0$ as $ n \to \infty.$ Let $\textbf{F}$ be a continuously differentiable vector field in a neighborhood of $P$. Prove that $$(\nabla \cdot \textbf{F})(P)=\lim_{n \to \infty}\frac{1}{V_n}\iint\limits_{S_n}\textbf{F}\cdot \textbf{n}\ dS,$$ where $V_n$ is the volume of the ball with boundary $S_n$."" This is what I did: Observe that, by the Divergence Theorem, $$\frac{1}{V_n}\iint\limits_{S_n}\textbf{F}\cdot \textbf{n}\ dS = \frac{1}{V_n}\iiint\limits_{D_n}\nabla\cdot\textbf{F} \ dV,$$ where $D_n$ is the region bounded by $S_n$. Now, notice that $\dfrac{1}{V_n}\displaystyle \iiint\limits_{D_n}\nabla\cdot\textbf{F} \ dV$ is the mean value of $\nabla \cdot \textbf{F}$ on $D_n$. Since F is continuously differentiable, we have that $\nabla \cdot \textbf{F}$ is continuous, so, for each $n$, there is a point $P_n$ such that $$\frac{1}{V_n} \iiint\limits_{D_n}\nabla\cdot\textbf{F} \ dV = (\nabla \cdot \textbf{F})(P_n)$$ So, $$\lim_{n \to \infty}\frac{1}{V_n}\iint\limits_{S_n}\textbf{F}\cdot \textbf{n} \ dS = \lim_{n \to \infty}\frac{1}{V_n}\iiint\limits_{D_n}\nabla\cdot\textbf{F} \ dV \\ =\lim_{n \to \infty}(\nabla \cdot \textbf{F})(P_n) = (\nabla \cdot \textbf{F})(P),$$ as we wanted. I have two problems here: 1) I don't know how to prove that because $\nabla \cdot \textbf{F}$ is continuous, we certainly have that there is a point $P_n$ such that $\frac{1}{V_n} \iiint\limits_{D_n}\nabla\cdot\textbf{F} \ dV = (\nabla \cdot \textbf{F})(P_n)$. I know it's true because I saw it in some calculus books, but I haven't found a proof. 2) After I finished, I realize that my points $P_n$ doesn't necessarily have to be the points $P_n$ center of $S_n$. So, because I know that the radius $r_n$ of $S_n$ goes to 0, can I assume that $P$ will be the limit of every sequence $(a_n)$ such that $a_k \in S_k$ $\forall k \in \mathbb{N}$? I'd be really grateful if someone could help me with my questions above :) Thanks!",,"['calculus', 'real-analysis', 'integration', 'sequences-and-series']"
29,"Understanding probabilistic Borel-Cantelli Lemma ""Corollary"" in Royden and Fitzpatrick","Understanding probabilistic Borel-Cantelli Lemma ""Corollary"" in Royden and Fitzpatrick",,"The Borel-Cantelli Lemma in Royden and Fitzpatrick's ""Real Analysis"" seems to be a sort of ""corollary"" of the non-probabilistic ones I see online . It says: ""Let $(E_k)_{k=1}^{\infty}$ be a countable collection of measurable sets for which $\sum_{k=1}^{\infty} m(E_k) < \infty$. Then almost all x $\epsilon \ \mathbb{R}$ belong to at most finitely many of the $E_k$'s"" I don't get three things: ""Then almost all x $\in \ \mathbb{R}$ belong to at most finitely many of the $E_k$'s"" What does that mean? Since the Borel-Cantelli Lemma comes after the ""almost everywhere"" definition, I guess: $\exists \ Z \ \subset \mathbb{R}$ s.t. $m(Z) = 0$ and $\forall x \ \in \ \mathbb{R} \setminus Z$, x belongs to $(E_{j}, E_{j+1}, E_{l}, ..., E_{m})$. In the proof, one can see the $m(\limsup \ E_k) = 0$ part, stated as $m(\cap_{n=1}^{\infty} [\cup_{k=n}^{\infty} E_k]) = 0$. For some reason, $m(\cap_{n=1}^{\infty} [\cup_{k=n}^{\infty} E_k]) = 0$ implies that ""Therefore almost all x $\in \ \mathbb{R}$ fail to belong to $\cap_{n=1}^{\infty} [\cup_{k=n}^{\infty} E_k]$ and therefore belong to at most finitely many $E_k$'s"" Okay, why? What is the contradiction if almost all $x \epsilon \ \mathbb{R}$ belong to all the $E_k$'s? Or a countably infinite subcollection? What is the significance of this? Why not just state the Borel-Cantelli Lemma as ""Let $(E_k)_{k=1}^{\infty}$ be a countable collection of measurable sets for which $\sum_{k=1}^{\infty} m(E_k) < \infty$. $m(\cap_{n=1}^{\infty} [\cup_{k=n}^{\infty} E_k]) = 0$""?","The Borel-Cantelli Lemma in Royden and Fitzpatrick's ""Real Analysis"" seems to be a sort of ""corollary"" of the non-probabilistic ones I see online . It says: ""Let $(E_k)_{k=1}^{\infty}$ be a countable collection of measurable sets for which $\sum_{k=1}^{\infty} m(E_k) < \infty$. Then almost all x $\epsilon \ \mathbb{R}$ belong to at most finitely many of the $E_k$'s"" I don't get three things: ""Then almost all x $\in \ \mathbb{R}$ belong to at most finitely many of the $E_k$'s"" What does that mean? Since the Borel-Cantelli Lemma comes after the ""almost everywhere"" definition, I guess: $\exists \ Z \ \subset \mathbb{R}$ s.t. $m(Z) = 0$ and $\forall x \ \in \ \mathbb{R} \setminus Z$, x belongs to $(E_{j}, E_{j+1}, E_{l}, ..., E_{m})$. In the proof, one can see the $m(\limsup \ E_k) = 0$ part, stated as $m(\cap_{n=1}^{\infty} [\cup_{k=n}^{\infty} E_k]) = 0$. For some reason, $m(\cap_{n=1}^{\infty} [\cup_{k=n}^{\infty} E_k]) = 0$ implies that ""Therefore almost all x $\in \ \mathbb{R}$ fail to belong to $\cap_{n=1}^{\infty} [\cup_{k=n}^{\infty} E_k]$ and therefore belong to at most finitely many $E_k$'s"" Okay, why? What is the contradiction if almost all $x \epsilon \ \mathbb{R}$ belong to all the $E_k$'s? Or a countably infinite subcollection? What is the significance of this? Why not just state the Borel-Cantelli Lemma as ""Let $(E_k)_{k=1}^{\infty}$ be a countable collection of measurable sets for which $\sum_{k=1}^{\infty} m(E_k) < \infty$. $m(\cap_{n=1}^{\infty} [\cup_{k=n}^{\infty} E_k]) = 0$""?",,"['real-analysis', 'probability-theory', 'measure-theory', 'borel-cantelli-lemmas']"
30,limit of increasing sequence of measures is a measure,limit of increasing sequence of measures is a measure,,"Statement Let $(X,\Sigma,\mu)$ be a measurable space and let $(\mu_n)_{n\geq 1}$ be a sequence of measure in this space. Suppose that this is a monotone increasing sequence, in the sense that $\mu_n(E)\leq \mu_{n+1}(E)$ for all $E \in \Sigma$, and for all $n$. If we define, for every $E \in \Sigma$ $\space$  $\mu(E)=\lim_{n \to \infty} \mu_n(E)$, prove that $\mu$ is a measure. The attempt at a solution I am having problems trying to prove the countable additivity property, which is, if $\{E_i\}_{i \in \mathbb N}$ is a countable collection of pairwise disjoint sets in $\Sigma$, then  $\mu(\bigcup_{i \in \mathbb N} E_i)=\sum_{i \in \mathbb N} \mu(E_i)$ In this particular case, the property is satisfied if and only if $\lim_{n \to \infty} \mu_n(\bigcup_{i \in \mathbb N} E_i)=\sum_{i \in \mathbb N}\lim_{n \to \infty}\mu_n(E_i)$ Each $\mu_n$ is a measure, so $\lim_{n \to \infty} \mu_n(\bigcup_{i \in \mathbb N} E_i)=\lim_{n \to \infty} \sum_{i \in \mathbb N} \mu_n(E_i)$. How could I go on from here? I know I must use the fact that $(\mu_n)_{n \in \mathbb N}$ is a monotone increasing sequence, I would appreciate any help.","Statement Let $(X,\Sigma,\mu)$ be a measurable space and let $(\mu_n)_{n\geq 1}$ be a sequence of measure in this space. Suppose that this is a monotone increasing sequence, in the sense that $\mu_n(E)\leq \mu_{n+1}(E)$ for all $E \in \Sigma$, and for all $n$. If we define, for every $E \in \Sigma$ $\space$  $\mu(E)=\lim_{n \to \infty} \mu_n(E)$, prove that $\mu$ is a measure. The attempt at a solution I am having problems trying to prove the countable additivity property, which is, if $\{E_i\}_{i \in \mathbb N}$ is a countable collection of pairwise disjoint sets in $\Sigma$, then  $\mu(\bigcup_{i \in \mathbb N} E_i)=\sum_{i \in \mathbb N} \mu(E_i)$ In this particular case, the property is satisfied if and only if $\lim_{n \to \infty} \mu_n(\bigcup_{i \in \mathbb N} E_i)=\sum_{i \in \mathbb N}\lim_{n \to \infty}\mu_n(E_i)$ Each $\mu_n$ is a measure, so $\lim_{n \to \infty} \mu_n(\bigcup_{i \in \mathbb N} E_i)=\lim_{n \to \infty} \sum_{i \in \mathbb N} \mu_n(E_i)$. How could I go on from here? I know I must use the fact that $(\mu_n)_{n \in \mathbb N}$ is a monotone increasing sequence, I would appreciate any help.",,"['real-analysis', 'measure-theory']"
31,Scratch work for delta-epsilon proof for $\lim_{x \to 13} \sqrt{x-4} = 3$,Scratch work for delta-epsilon proof for,\lim_{x \to 13} \sqrt{x-4} = 3,"Prove $\lim_{x \to 13} \sqrt{x-4} = 3$. We need to show for all $E> 0$ there exists $D > 0$ such that if $0 < |x - 13| < D$, then $|\sqrt{x-4} - 3| < E$. Let me write D for delta and E for epsilon please. Scratch-work here: $\color{darkred}{\text{ I understand the formal proof. Ergo just asking about this. } }$ Note that $|\sqrt{x-4} - 3| = |\sqrt{x-4} - 3|\dfrac{|\sqrt{x-4} + 3|}{|\sqrt{x-4} + 3|} = \dfrac{|x - 13|}{|\sqrt{x-4} + 3|}$. We can bound $|x - 13|$ for any choice of D, but we need a certain D to also bound $|\sqrt{x-4} + 3|$. 1. Ishfaaq's answer says 'the denominator will definitely cause trouble.' How? Why won't bounding $|x - a|, |x - 13|$ bound the whole statements? Beneath pp 83 of Spivak claims this too. How does '$|x + a|$ cause trouble'? Assume that $D < 1.$ 2. What sanctioned assuming $D < 1$? How to know if $1$ is too small, too big? $\\$ In pp 83 of Spivak beneath, $D$ looks randomly chosen? 4. Ishfaaq's last paragraph. $\color{darkred}{\left|{x^2 - a^2}\right| \lt \delta^2 + \left|{}2a\right|\delta. \text{ But we can hardly equate this to ϵ... }}$ Why not? $d^2 + \left|{}2a\right|d = e \iff d(d + |2a|) = e$? Ishfaaq says $a = 1/2, e = d/2$ is a counterexample. But then $d^2 + 2|a|d < e \iff d^2 + d < d/2 \iff d(d - \frac{1}{2}) < 0 \iff 0 < d < 1/2.$ What founders? If this works, then any smaller D will also work.   Then $|x - 13| < 1 \iff -1 < x - 13 < 1 \iff 12 < x < 14 \iff 8 < x - 4 < 10$ $\implies \sqrt(8) < \sqrt{x - 4} < \sqrt(10) \iff  \color{blue}{\sqrt(8) + 3 < \sqrt{x - 4} + 3 < \sqrt(10) + 3} \\ \iff  0 < \color{blue}{\dfrac{1}{\sqrt(10) + 3} < \dfrac{1}{\sqrt{x - 4} + 3} < \dfrac{1}{\sqrt(8) + 3}} $   $\implies \color{green}{\frac{1}{|\sqrt{x - 4} + 3|} < \frac{1}{\sqrt(8) + 3}} $ So then $|x - 13|\color{green}{\dfrac{1}{|\sqrt{x - 4} + 3} < \dfrac{1}{\sqrt(8) + 3}}|x - 13|$ and need this $ < E.$ Thus, we need $|x - 13| < E * \color{green}{(\sqrt{8} + 3)}$. So this is our choice of D. But note that this only works when $D < 1$. Thus, we can take care of both conditions by choosing $D = \min\{1, E(\sqrt{8}+3)\}.$ █ $\color{darkred}{ \text{ 3. All this algebra fazed me. How to graph this to view all this algebra and $D$? Thanks. } }$ Also tried http://www.ocf.berkeley.edu/~yosenl/math/epsilon-delta.pdf .","Prove $\lim_{x \to 13} \sqrt{x-4} = 3$. We need to show for all $E> 0$ there exists $D > 0$ such that if $0 < |x - 13| < D$, then $|\sqrt{x-4} - 3| < E$. Let me write D for delta and E for epsilon please. Scratch-work here: $\color{darkred}{\text{ I understand the formal proof. Ergo just asking about this. } }$ Note that $|\sqrt{x-4} - 3| = |\sqrt{x-4} - 3|\dfrac{|\sqrt{x-4} + 3|}{|\sqrt{x-4} + 3|} = \dfrac{|x - 13|}{|\sqrt{x-4} + 3|}$. We can bound $|x - 13|$ for any choice of D, but we need a certain D to also bound $|\sqrt{x-4} + 3|$. 1. Ishfaaq's answer says 'the denominator will definitely cause trouble.' How? Why won't bounding $|x - a|, |x - 13|$ bound the whole statements? Beneath pp 83 of Spivak claims this too. How does '$|x + a|$ cause trouble'? Assume that $D < 1.$ 2. What sanctioned assuming $D < 1$? How to know if $1$ is too small, too big? $\\$ In pp 83 of Spivak beneath, $D$ looks randomly chosen? 4. Ishfaaq's last paragraph. $\color{darkred}{\left|{x^2 - a^2}\right| \lt \delta^2 + \left|{}2a\right|\delta. \text{ But we can hardly equate this to ϵ... }}$ Why not? $d^2 + \left|{}2a\right|d = e \iff d(d + |2a|) = e$? Ishfaaq says $a = 1/2, e = d/2$ is a counterexample. But then $d^2 + 2|a|d < e \iff d^2 + d < d/2 \iff d(d - \frac{1}{2}) < 0 \iff 0 < d < 1/2.$ What founders? If this works, then any smaller D will also work.   Then $|x - 13| < 1 \iff -1 < x - 13 < 1 \iff 12 < x < 14 \iff 8 < x - 4 < 10$ $\implies \sqrt(8) < \sqrt{x - 4} < \sqrt(10) \iff  \color{blue}{\sqrt(8) + 3 < \sqrt{x - 4} + 3 < \sqrt(10) + 3} \\ \iff  0 < \color{blue}{\dfrac{1}{\sqrt(10) + 3} < \dfrac{1}{\sqrt{x - 4} + 3} < \dfrac{1}{\sqrt(8) + 3}} $   $\implies \color{green}{\frac{1}{|\sqrt{x - 4} + 3|} < \frac{1}{\sqrt(8) + 3}} $ So then $|x - 13|\color{green}{\dfrac{1}{|\sqrt{x - 4} + 3} < \dfrac{1}{\sqrt(8) + 3}}|x - 13|$ and need this $ < E.$ Thus, we need $|x - 13| < E * \color{green}{(\sqrt{8} + 3)}$. So this is our choice of D. But note that this only works when $D < 1$. Thus, we can take care of both conditions by choosing $D = \min\{1, E(\sqrt{8}+3)\}.$ █ $\color{darkred}{ \text{ 3. All this algebra fazed me. How to graph this to view all this algebra and $D$? Thanks. } }$ Also tried http://www.ocf.berkeley.edu/~yosenl/math/epsilon-delta.pdf .",,['real-analysis']
32,Uniform Convergence verification for Sequence of functions - NBHM,Uniform Convergence verification for Sequence of functions - NBHM,,"Following is a list of problems from an exam for admission into Ph.D program. I have just compiled all previous questions on uniform convergence of sequence of functions and i tried to work out . I would be thankful if some one can check the solutions and please suggest if there are any better ways to do and if the solution is wrong please let me know what could be done for them. $f_n(x)=\sin^n x$ on $[0,\frac{\pi}{2}]$ $f_n(x) = \frac{x^n}{n}+1$ on $[0,1)$ $f_n(x) = \dfrac{1}{1+(x-n)^2}$ on $(-∞,0)$ $f_n(x) = \dfrac{1}{1+(x-n)^2}$ on $(0, ∞)$ $f_n(x) = nxe^{−nx}$ on $(0,∞)$ $f_n(x) = x^n$ on $[0, 1]$ $f_n(x) = \dfrac{\sin(nx)}{\sqrt{n}}$ on $\mathbb{R}$ $f_n(x)=\dfrac{nx}{1 + nx}$ on $(0,1)$ $f_n(x) = \dfrac{x^n}{1 + x^n}$ ; on  $[0, 2]$ $f_n(x)=n^2x^2e^{-nx}$ over $(0,\infty)$ $f_n(x)=(\cos (\pi n!x))^{2n}$ on $[0,1]$ $f_n(x)=n^2x(1-x^2)^n$ on $[0,1]$ I would like to explain what all i have tried and if there is any better way to do please let Me know. For $f_n(x)=\sin^n x$ on $[0,\frac{\pi}{2}]$ we do as follows : for $x=\frac{\pi}{2}$ we have $f_n(x)=1$ and for $x\in [0,\frac{\pi}{2})$ we have $f_n(x)=(a)^n$ where $0\leq a <1$ so, $f_n(x) \rightarrow 0$ for each  $x\in [0,\frac{\pi}{2})$ so limit function is : $$\lim_{n\rightarrow \infty}f_n(x) = \begin{cases} 0, & x \in [0,\frac{\pi}{2}) \\ 1, & x=\frac{\pi}{2}. \end{cases}$$ so the limit function is not continuous so convergence is not uniform. For $f_n(x) = \frac{x^n}{n}+1$ on $[0,1)$ limit function is $f(x)=1$ . Now $|f_n(x)-f(x)|=\frac{x^n}{n}$ let $d_n =\sup \{ |f_n(x)-f(x)| : x\in [0,1) \}==\sup \{ \frac{x^n}{n} : x\in [0,1) \}$ I could see that $d_n\rightarrow 0$ intuitively but could not produce a complete proof.please help me with that.keeping that gap aside, as $d_n\rightarrow 0$ we see that convergence is uniform For $f_n(x) = \dfrac{1}{1+(x-n)^2}$ on $(-∞,0)$ we see that the limit function is $f\equiv 0$ I do not see any problem that stops this from being uniform convergence (please see below solution for what i mean when i say problem) at the same time i can not say that it is uniform convergent for sure. For $f_n(x) = \dfrac{1}{1+(x-n)^2}$ on $(0, ∞)$ we have limit function to be $f(x)\equiv 0$  but then, $|f_n(x)-f(x)|=|f_n(x)|$ equals  $1$ at $x=n$ so the condition $$|f_n(x)-f(x)|<\epsilon  \textbf { for all } x\in (0,\infty)$$ does not hold so the convergence is not uniform. For $f_n(x) = nxe^{−nx}$ on $(0,∞)$ we have $f_n(x)=\frac{nx}{e^{nx}}$. But $e^{nx}$ diverges faster that $nx$ so limit function is $f\equiv 0$. But then at $x=\frac{1}{n}$ we have $f_n(x)=\frac{1}{e}$ so I can not say that $|f_n(x)-f(x)|< \epsilon $ for all $x$ and for all $n\geq N$ for some $N$ if my epsilon is less than $\frac{1}{e}$ thus the convergence is Not uniform. For $f_n(x) = x^n$ on $[0, 1]$  we see that limit function is  $$\lim_{n\rightarrow \infty}f_n(x) = \begin{cases} 0, & x \in [0,1) \\ 1, & x=1. \end{cases}$$ so the limit function is not continuous so convergence is not uniform. For $f_n(x) = \dfrac{\sin(nx)}{\sqrt{n}}$ on $\mathbb{R}$ we see that the limit function is $f\equiv 0$. $|f_n(x)-f(x)|=|f_n(x)|=|\dfrac{\sin(nx)}{\sqrt{n}}|\leq\frac{1}{\sqrt{n}}$. Now, for given  $\epsilon>0$ if i choose $N$ such that $\frac{1}{\sqrt{N}}<\epsilon$ then we have $|f_n(x)-f(x)|\leq\frac{1}{\sqrt{n}}<\epsilon$ for all $n\geq N$ irrespective of choice of $x$. thus the convergence is uniform. For $f_n(x)=\dfrac{nx}{1 + nx}$ on $(0,1)$ we have $f_n(x)=\dfrac{1}{\frac{1}{nx}+1}$ the limit function is $1$. Now $|f_n(x)-f(x)|=|\dfrac{nx}{1 + nx}-1|=|\frac{nx-1-nx}{1+nx}|=\frac{1}{1+nx}$ Now $0<x\Rightarrow 0<nx \Rightarrow 1<1+nx \Rightarrow \frac{1}{1+nx} < 1$ But then I do not know how to proceed further... For $f_n(x) = \dfrac{x^n}{1 + x^n}$ ; on  $[0, 2]$ we see that $f_n(x)=0$ for all $x\in [0,1)$ at $x=1$ we have $f_n(x)=\frac{1}{2}$ and $f_n(x)=1$  for all $x\in[1,2]$ so limit function is : $$\lim_{n\rightarrow \infty}f_n(x) = \begin{cases} 0, & x \in [0,1) \\ \frac{1}{2}, & x=1,\\ 1&x\in[1,2] \end{cases}$$ so the limit function is not continuous so convergence is not uniform. For $f_n(x)=n^2x^2e^{-nx}$ over $(0,\infty)$ we have $f_n(x)=\frac{n^2x^2}{e^{nx}}$. But $e^{nx}$ diverges faster that $n^2x^2$ so limit function is $f\equiv 0$. But then at $x=\frac{1}{n}$ we have $f_n(x)=\frac{1}{e}$ so I can not say that $|f_n(x)-f(x)|< \epsilon $ for all $x$ and for all $n\geq N$ for some $N$ if my epsilon is less than $\frac{1}{e}$ thus the convergence is Not uniform. For $f_n(x)=(\cos (\pi n!x))^{2n}$ on $[0,1]$ at $x=0$ we have $f_n(x)=1$ and for $x\in(0,1]$ we have $f_n(x)\rightarrow 0$  $$\lim_{n\rightarrow \infty}f_n(x) = \begin{cases} 1, & x=0 \\ 0, & x\in(0,1]. \end{cases}$$ so the limit function is not continuous so convergence is not uniform. $For f_n(x)=n^2x(1-x^2)^n$ on $[0,1]$ limit function is $f\equiv 0$ Now if i consider $|f_n(x)-f(x)|=n^2x(1-x^2)^n$ by considering derivative i have seen that maximum value is obtained at $x^2=\frac{1}{2n+1}$ so for uniform convergence we should make sure $f_n(x)$ for $x^2=\frac{1}{2n+1}$ goes to $0$ but then$f_n(x)=\dfrac{n^2}{}\sqrt{2n+1}(\frac{2n}{2n+1})^n$  but the limit does not even exist. So, the convergence is not uniform. Thank you for sparing your valuable time in checking my solutions. Once somebody confirm that every thing is fine and no modifications needed i would delete this question so please post only comments. Thank you :)","Following is a list of problems from an exam for admission into Ph.D program. I have just compiled all previous questions on uniform convergence of sequence of functions and i tried to work out . I would be thankful if some one can check the solutions and please suggest if there are any better ways to do and if the solution is wrong please let me know what could be done for them. $f_n(x)=\sin^n x$ on $[0,\frac{\pi}{2}]$ $f_n(x) = \frac{x^n}{n}+1$ on $[0,1)$ $f_n(x) = \dfrac{1}{1+(x-n)^2}$ on $(-∞,0)$ $f_n(x) = \dfrac{1}{1+(x-n)^2}$ on $(0, ∞)$ $f_n(x) = nxe^{−nx}$ on $(0,∞)$ $f_n(x) = x^n$ on $[0, 1]$ $f_n(x) = \dfrac{\sin(nx)}{\sqrt{n}}$ on $\mathbb{R}$ $f_n(x)=\dfrac{nx}{1 + nx}$ on $(0,1)$ $f_n(x) = \dfrac{x^n}{1 + x^n}$ ; on  $[0, 2]$ $f_n(x)=n^2x^2e^{-nx}$ over $(0,\infty)$ $f_n(x)=(\cos (\pi n!x))^{2n}$ on $[0,1]$ $f_n(x)=n^2x(1-x^2)^n$ on $[0,1]$ I would like to explain what all i have tried and if there is any better way to do please let Me know. For $f_n(x)=\sin^n x$ on $[0,\frac{\pi}{2}]$ we do as follows : for $x=\frac{\pi}{2}$ we have $f_n(x)=1$ and for $x\in [0,\frac{\pi}{2})$ we have $f_n(x)=(a)^n$ where $0\leq a <1$ so, $f_n(x) \rightarrow 0$ for each  $x\in [0,\frac{\pi}{2})$ so limit function is : $$\lim_{n\rightarrow \infty}f_n(x) = \begin{cases} 0, & x \in [0,\frac{\pi}{2}) \\ 1, & x=\frac{\pi}{2}. \end{cases}$$ so the limit function is not continuous so convergence is not uniform. For $f_n(x) = \frac{x^n}{n}+1$ on $[0,1)$ limit function is $f(x)=1$ . Now $|f_n(x)-f(x)|=\frac{x^n}{n}$ let $d_n =\sup \{ |f_n(x)-f(x)| : x\in [0,1) \}==\sup \{ \frac{x^n}{n} : x\in [0,1) \}$ I could see that $d_n\rightarrow 0$ intuitively but could not produce a complete proof.please help me with that.keeping that gap aside, as $d_n\rightarrow 0$ we see that convergence is uniform For $f_n(x) = \dfrac{1}{1+(x-n)^2}$ on $(-∞,0)$ we see that the limit function is $f\equiv 0$ I do not see any problem that stops this from being uniform convergence (please see below solution for what i mean when i say problem) at the same time i can not say that it is uniform convergent for sure. For $f_n(x) = \dfrac{1}{1+(x-n)^2}$ on $(0, ∞)$ we have limit function to be $f(x)\equiv 0$  but then, $|f_n(x)-f(x)|=|f_n(x)|$ equals  $1$ at $x=n$ so the condition $$|f_n(x)-f(x)|<\epsilon  \textbf { for all } x\in (0,\infty)$$ does not hold so the convergence is not uniform. For $f_n(x) = nxe^{−nx}$ on $(0,∞)$ we have $f_n(x)=\frac{nx}{e^{nx}}$. But $e^{nx}$ diverges faster that $nx$ so limit function is $f\equiv 0$. But then at $x=\frac{1}{n}$ we have $f_n(x)=\frac{1}{e}$ so I can not say that $|f_n(x)-f(x)|< \epsilon $ for all $x$ and for all $n\geq N$ for some $N$ if my epsilon is less than $\frac{1}{e}$ thus the convergence is Not uniform. For $f_n(x) = x^n$ on $[0, 1]$  we see that limit function is  $$\lim_{n\rightarrow \infty}f_n(x) = \begin{cases} 0, & x \in [0,1) \\ 1, & x=1. \end{cases}$$ so the limit function is not continuous so convergence is not uniform. For $f_n(x) = \dfrac{\sin(nx)}{\sqrt{n}}$ on $\mathbb{R}$ we see that the limit function is $f\equiv 0$. $|f_n(x)-f(x)|=|f_n(x)|=|\dfrac{\sin(nx)}{\sqrt{n}}|\leq\frac{1}{\sqrt{n}}$. Now, for given  $\epsilon>0$ if i choose $N$ such that $\frac{1}{\sqrt{N}}<\epsilon$ then we have $|f_n(x)-f(x)|\leq\frac{1}{\sqrt{n}}<\epsilon$ for all $n\geq N$ irrespective of choice of $x$. thus the convergence is uniform. For $f_n(x)=\dfrac{nx}{1 + nx}$ on $(0,1)$ we have $f_n(x)=\dfrac{1}{\frac{1}{nx}+1}$ the limit function is $1$. Now $|f_n(x)-f(x)|=|\dfrac{nx}{1 + nx}-1|=|\frac{nx-1-nx}{1+nx}|=\frac{1}{1+nx}$ Now $0<x\Rightarrow 0<nx \Rightarrow 1<1+nx \Rightarrow \frac{1}{1+nx} < 1$ But then I do not know how to proceed further... For $f_n(x) = \dfrac{x^n}{1 + x^n}$ ; on  $[0, 2]$ we see that $f_n(x)=0$ for all $x\in [0,1)$ at $x=1$ we have $f_n(x)=\frac{1}{2}$ and $f_n(x)=1$  for all $x\in[1,2]$ so limit function is : $$\lim_{n\rightarrow \infty}f_n(x) = \begin{cases} 0, & x \in [0,1) \\ \frac{1}{2}, & x=1,\\ 1&x\in[1,2] \end{cases}$$ so the limit function is not continuous so convergence is not uniform. For $f_n(x)=n^2x^2e^{-nx}$ over $(0,\infty)$ we have $f_n(x)=\frac{n^2x^2}{e^{nx}}$. But $e^{nx}$ diverges faster that $n^2x^2$ so limit function is $f\equiv 0$. But then at $x=\frac{1}{n}$ we have $f_n(x)=\frac{1}{e}$ so I can not say that $|f_n(x)-f(x)|< \epsilon $ for all $x$ and for all $n\geq N$ for some $N$ if my epsilon is less than $\frac{1}{e}$ thus the convergence is Not uniform. For $f_n(x)=(\cos (\pi n!x))^{2n}$ on $[0,1]$ at $x=0$ we have $f_n(x)=1$ and for $x\in(0,1]$ we have $f_n(x)\rightarrow 0$  $$\lim_{n\rightarrow \infty}f_n(x) = \begin{cases} 1, & x=0 \\ 0, & x\in(0,1]. \end{cases}$$ so the limit function is not continuous so convergence is not uniform. $For f_n(x)=n^2x(1-x^2)^n$ on $[0,1]$ limit function is $f\equiv 0$ Now if i consider $|f_n(x)-f(x)|=n^2x(1-x^2)^n$ by considering derivative i have seen that maximum value is obtained at $x^2=\frac{1}{2n+1}$ so for uniform convergence we should make sure $f_n(x)$ for $x^2=\frac{1}{2n+1}$ goes to $0$ but then$f_n(x)=\dfrac{n^2}{}\sqrt{2n+1}(\frac{2n}{2n+1})^n$  but the limit does not even exist. So, the convergence is not uniform. Thank you for sparing your valuable time in checking my solutions. Once somebody confirm that every thing is fine and no modifications needed i would delete this question so please post only comments. Thank you :)",,"['real-analysis', 'proof-verification']"
33,Is there any notable difference between studying the Riemann integral over open intervals and studying it over closed intervals? [duplicate],Is there any notable difference between studying the Riemann integral over open intervals and studying it over closed intervals? [duplicate],,"This question already has an answer here : Why is the Riemann integral only defined on compact sets? (1 answer) Closed 1 year ago . (1) A function $f:[a,b]\to\mathbb{R}$ is said to be Riemann integrable on $[a,b]$ . . . (2) A function $f:(a,b)\to\mathbb{R}$ is said to be Riemann integrable on $(a,b)$ . . . Suppose a book started the exposition like (1) and other started like (2). My question is: Has every theorem that is valid for first definition, a analogue for the second? I think most of the books (at least my calculus books and analysis books) uses closed intervals, but I started to study Lebesgue Integral and in the first chapter of book the author presents a review about Riemann integral and considers functions defined on open intervals. Maybe it's trivial, but I would like know if there is a reason for choose to work with open intervals instead of closed intervals. Thanks.","This question already has an answer here : Why is the Riemann integral only defined on compact sets? (1 answer) Closed 1 year ago . (1) A function $f:[a,b]\to\mathbb{R}$ is said to be Riemann integrable on $[a,b]$ . . . (2) A function $f:(a,b)\to\mathbb{R}$ is said to be Riemann integrable on $(a,b)$ . . . Suppose a book started the exposition like (1) and other started like (2). My question is: Has every theorem that is valid for first definition, a analogue for the second? I think most of the books (at least my calculus books and analysis books) uses closed intervals, but I started to study Lebesgue Integral and in the first chapter of book the author presents a review about Riemann integral and considers functions defined on open intervals. Maybe it's trivial, but I would like know if there is a reason for choose to work with open intervals instead of closed intervals. Thanks.",,"['calculus', 'real-analysis', 'integration']"
34,Easy but hard question regarding concave functions!,Easy but hard question regarding concave functions!,,"I have a question about concave functions. Let $f:[0,T]\rightarrow \mathbb{R}^+$ is a concave function. Let $S=\int_0 ^ T f(x)dx$ and $R=\frac S T$ . Show that there is an interval $[a,b]$ , where $0\leq a \leq b \leq T$ and $b-a\geq \frac T 2$ such that for every $x\in [a,b]$ we have: $f(x) \geq R$ . If $f$ is non-decreasing function it is quite simple since by use of Jensen's inequality one can argue that $\frac 1 T \int_0 ^T f(x)dx \leq f(\frac T 2)$ . Also I could prove this for piecewise linear concave functions. My proof is by induction on the number of line segments in the piecewise linear concave function. It is messy and use lots of geometric stuff. I wonder if there are any nice and tidy proofs for general concave functions. Any comments is much appreciated!","I have a question about concave functions. Let is a concave function. Let and . Show that there is an interval , where and such that for every we have: . If is non-decreasing function it is quite simple since by use of Jensen's inequality one can argue that . Also I could prove this for piecewise linear concave functions. My proof is by induction on the number of line segments in the piecewise linear concave function. It is messy and use lots of geometric stuff. I wonder if there are any nice and tidy proofs for general concave functions. Any comments is much appreciated!","f:[0,T]\rightarrow \mathbb{R}^+ S=\int_0 ^ T f(x)dx R=\frac S T [a,b] 0\leq a \leq b \leq T b-a\geq \frac T 2 x\in [a,b] f(x) \geq R f \frac 1 T \int_0 ^T f(x)dx \leq f(\frac T 2)","['real-analysis', 'convex-analysis']"
35,Does Newton's method for inverting a series work?,Does Newton's method for inverting a series work?,,"Suppose we have $z=f(x)$ with $f$ an infinite series. We want to find $f^{-1}(z)=x$. Newton proposed the following method (as described in Dunham ): First, we say $x=z+r$. We find $z=f(z+r)$, drop all terms quadratic or higher in $r$ to find $r = g(z)$. Then we drop any quadratic or higher terms of $z$ to find $r = a + bz$. We repeat, writing $x=z+(a+bz)+r'$ and so forth, finding $x=z+r+r'+r''+\dots$. I greatly enjoy this method, as it saves one the work of, well, finding the actual inverse. But I wonder: will this method always work? Intuitively, it seems like there must be ""poorly behaved"" series for which $z+r+\dots$ does not converge to $x$.","Suppose we have $z=f(x)$ with $f$ an infinite series. We want to find $f^{-1}(z)=x$. Newton proposed the following method (as described in Dunham ): First, we say $x=z+r$. We find $z=f(z+r)$, drop all terms quadratic or higher in $r$ to find $r = g(z)$. Then we drop any quadratic or higher terms of $z$ to find $r = a + bz$. We repeat, writing $x=z+(a+bz)+r'$ and so forth, finding $x=z+r+r'+r''+\dots$. I greatly enjoy this method, as it saves one the work of, well, finding the actual inverse. But I wonder: will this method always work? Intuitively, it seems like there must be ""poorly behaved"" series for which $z+r+\dots$ does not converge to $x$.",,"['calculus', 'real-analysis', 'sequences-and-series', 'convergence-divergence']"
36,A few counterexamples in the convergence of functions,A few counterexamples in the convergence of functions,,"I'm studying the various types of convergence for sequences of real valued functions defined on measure spaces: pointwise convergence a.e. , convergence in $L^p$ norm, weak convergence in $L^p$, and convergence in measure. It is quite complicated to see exactly what the relationships between the various types of convergence are, here is a diagram that shows the possibilities (when dealing with measurable subsets of $\mathbb{R}^N$, with $1 \leq p < \infty$ ; note that areas (2) and (3) are empty when the space has finite measure, since in this case convergence a.e. implies convergence in measure). Working mainly with these lecture notes (sorry, they're in Italian... anyway, many examples are on pages 124, 125), I have managed to fill all except the shaded areas in the diagram. So here are my questions: Do there exist sequences of real valued funcions, defined on measurable subsets of $\mathbb{R}^N$ that: $(6)$ converge in measure, but not a.e. and not weakly in $L^p$ for some $p \in [1, +\infty)$ $(7)$ converge in measure, weakly in $L^p$, but not a.e. and not in $L^p$ $(8)$ converge in measure, weakly in $L^p$, a.e., but not in $L^p$ I don't really know if I'm missing something stupid here, or if these examples are really tricky to find (or perhaps they don't exist). Any help is much appreciated, thank you.","I'm studying the various types of convergence for sequences of real valued functions defined on measure spaces: pointwise convergence a.e. , convergence in $L^p$ norm, weak convergence in $L^p$, and convergence in measure. It is quite complicated to see exactly what the relationships between the various types of convergence are, here is a diagram that shows the possibilities (when dealing with measurable subsets of $\mathbb{R}^N$, with $1 \leq p < \infty$ ; note that areas (2) and (3) are empty when the space has finite measure, since in this case convergence a.e. implies convergence in measure). Working mainly with these lecture notes (sorry, they're in Italian... anyway, many examples are on pages 124, 125), I have managed to fill all except the shaded areas in the diagram. So here are my questions: Do there exist sequences of real valued funcions, defined on measurable subsets of $\mathbb{R}^N$ that: $(6)$ converge in measure, but not a.e. and not weakly in $L^p$ for some $p \in [1, +\infty)$ $(7)$ converge in measure, weakly in $L^p$, but not a.e. and not in $L^p$ $(8)$ converge in measure, weakly in $L^p$, a.e., but not in $L^p$ I don't really know if I'm missing something stupid here, or if these examples are really tricky to find (or perhaps they don't exist). Any help is much appreciated, thank you.",,"['real-analysis', 'measure-theory', 'convergence-divergence']"
37,What is the root linear coefficient theorem?,What is the root linear coefficient theorem?,,"MathWorld gives the root linear coefficient theorem as The sum of the reciprocals of roots of an equation equals the negative coefficient of the linear term in the Maclaurin series. The theorem appears to me to be false as stated.  For example, the equation $e^x = 0$ has no roots, yet (taking the Maclaurin series of $e^x$) the root linear coefficient theorem claims that the sum of the reciprocals of these nonexistent roots would be $-1$. Is MathWorld missing some hypotheses?  Or is there something happening in the complex plane that I'm not aware of?  The MathWorld entry also says to see Vieta's formulas , but those are for polynomials and not Maclaurin series. The only real information I could find from a Google search on ""root linear coefficient theorem"" was this statement (from Robert Israel of UBC): This won't work in general for non-polynomials (e.g. try it for   $p(x) exp(x)$.  For a rational function such that $0$ is neither a root nor   a pole, you want to take the sum of the reciprocals of the roots minus the   sum of the reciprocals of the poles (again counting multiplicity). O.K., so it won't work for non-polynomials, and for rational functions you have to include the poles. But then why is MathWorld applying it to $\sin z/z$ in this ""proof"" that $\zeta(2) = \sum_{n=1}^{\infty} \frac{1}{n^2} = \pi^2/6$ ?  (Start near eq. (18).) The value $\zeta(2)$ can also be found simply using the root linear coefficient theorem. Consider the equation $\sin z=0$ and expand $\sin$ in a Maclaurin series   $$\sin z = z- \frac{z^3}{3!}+\frac{z^5}{5!}+ \ldots =0$$   $$0	=	1-\frac{z^2}{3!}+\frac{z^4}{5!}+ \ldots$$   $$	=	1-\frac{w}{3!}+\frac{w^2}{5!}+ \ldots,$$ where $w=z^2$. But the zeros of $\sin z$ occur at $z=\pi, 2\pi, 3\pi, \ldots$, or $w=\pi^2, (2\pi)^2, \ldots$.  Therefore, the sum of the [reciprocals of the] roots equals the [negative of the] coefficient of the leading term   $$\frac{1}{\pi^2}+\frac{1}{(2\pi)^2}+\frac{1}{(3\pi)^2}+ \ldots =\frac{1}{3!}=\frac{1}{6},$$ which can be rearranged to yield   $$\zeta(2)=\frac{\pi^2}{6}.$$ (This is where I ran across the root linear coefficient theorem in the first place.) Could someone enlighten me with respect to these questions: Is MathWorld just wrong? Am I missing something here? What is the correct statement of the root linear coefficient theorem?","MathWorld gives the root linear coefficient theorem as The sum of the reciprocals of roots of an equation equals the negative coefficient of the linear term in the Maclaurin series. The theorem appears to me to be false as stated.  For example, the equation $e^x = 0$ has no roots, yet (taking the Maclaurin series of $e^x$) the root linear coefficient theorem claims that the sum of the reciprocals of these nonexistent roots would be $-1$. Is MathWorld missing some hypotheses?  Or is there something happening in the complex plane that I'm not aware of?  The MathWorld entry also says to see Vieta's formulas , but those are for polynomials and not Maclaurin series. The only real information I could find from a Google search on ""root linear coefficient theorem"" was this statement (from Robert Israel of UBC): This won't work in general for non-polynomials (e.g. try it for   $p(x) exp(x)$.  For a rational function such that $0$ is neither a root nor   a pole, you want to take the sum of the reciprocals of the roots minus the   sum of the reciprocals of the poles (again counting multiplicity). O.K., so it won't work for non-polynomials, and for rational functions you have to include the poles. But then why is MathWorld applying it to $\sin z/z$ in this ""proof"" that $\zeta(2) = \sum_{n=1}^{\infty} \frac{1}{n^2} = \pi^2/6$ ?  (Start near eq. (18).) The value $\zeta(2)$ can also be found simply using the root linear coefficient theorem. Consider the equation $\sin z=0$ and expand $\sin$ in a Maclaurin series   $$\sin z = z- \frac{z^3}{3!}+\frac{z^5}{5!}+ \ldots =0$$   $$0	=	1-\frac{z^2}{3!}+\frac{z^4}{5!}+ \ldots$$   $$	=	1-\frac{w}{3!}+\frac{w^2}{5!}+ \ldots,$$ where $w=z^2$. But the zeros of $\sin z$ occur at $z=\pi, 2\pi, 3\pi, \ldots$, or $w=\pi^2, (2\pi)^2, \ldots$.  Therefore, the sum of the [reciprocals of the] roots equals the [negative of the] coefficient of the leading term   $$\frac{1}{\pi^2}+\frac{1}{(2\pi)^2}+\frac{1}{(3\pi)^2}+ \ldots =\frac{1}{3!}=\frac{1}{6},$$ which can be rearranged to yield   $$\zeta(2)=\frac{\pi^2}{6}.$$ (This is where I ran across the root linear coefficient theorem in the first place.) Could someone enlighten me with respect to these questions: Is MathWorld just wrong? Am I missing something here? What is the correct statement of the root linear coefficient theorem?",,"['real-analysis', 'analysis', 'sequences-and-series']"
38,Metrization of the weak*-topology on a set of probability measures,Metrization of the weak*-topology on a set of probability measures,,"Let $X$ denote a metric space. One can assume that $X$ is Polish if that helps, but I was trying to avoid to assume that $X$ is compact. Let $P(X)$ denote the set of Borel probability measures on $X$. The weak*-topology on $P(X)$ is defined as usual: a net $(p_{\alpha})$ in $P(X)$ converges to $p \in P(X)$ iff $|\int fdp_{\alpha}-\int fdp|$ converges to $0$. Question: Is there any metric that metrizes the weak*-topology on $P(X)$ and has convex open balls?","Let $X$ denote a metric space. One can assume that $X$ is Polish if that helps, but I was trying to avoid to assume that $X$ is compact. Let $P(X)$ denote the set of Borel probability measures on $X$. The weak*-topology on $P(X)$ is defined as usual: a net $(p_{\alpha})$ in $P(X)$ converges to $p \in P(X)$ iff $|\int fdp_{\alpha}-\int fdp|$ converges to $0$. Question: Is there any metric that metrizes the weak*-topology on $P(X)$ and has convex open balls?",,"['real-analysis', 'probability-theory']"
39,Does $\sum_n x_{\lceil \alpha^n\rceil} < \infty $ imply $x_n \to 0$?,Does  imply ?,\sum_n x_{\lceil \alpha^n\rceil} < \infty  x_n \to 0,"Suppose we have a (nonnegative if you like) sequence $x_n$ in $\mathbb{R}$ or a general Banach space. If for all $\alpha > 1$ , $\sum_{n = 1}^\infty x_{\lceil \alpha^n\rceil} < \infty$ , does this imply that $x_n \to 0$ ? This question is inspired by the ""sub-sub-sequence"" characterization of convergence (e.g. this question and duplicates). (Application: I am trying to reduce a question of stochastic sequence convergence to stochastic series convergence.) My thoughts: the most likely counterexample to this claim would be a sequence that doesn't converge to zero, but has its fluctuations super-exponentially far apart, e.g. $x_n = 1$ if $n = m!$ for some $m$ , and $0$ otherwise. In any case, if $x_n > \epsilon$ infinitely often, I am not sure whether such $n$ certainly have an infinite intersection with the indices $\{\lceil \alpha^n\rceil\}$ for sufficiently small $\alpha$ . (Maybe I am missing something obvious here.) Edit 0: The prototype sequence I have in mind is $x_n = n^{-1}$ . Edit 1: The summability criterion implies $\lim \inf x_n = 0$ . Intuitively it ought to imply more, as summability is stronger than convergence to zero. Perhaps we can find a creative way to combine series at varying $\alpha$ ?","Suppose we have a (nonnegative if you like) sequence in or a general Banach space. If for all , , does this imply that ? This question is inspired by the ""sub-sub-sequence"" characterization of convergence (e.g. this question and duplicates). (Application: I am trying to reduce a question of stochastic sequence convergence to stochastic series convergence.) My thoughts: the most likely counterexample to this claim would be a sequence that doesn't converge to zero, but has its fluctuations super-exponentially far apart, e.g. if for some , and otherwise. In any case, if infinitely often, I am not sure whether such certainly have an infinite intersection with the indices for sufficiently small . (Maybe I am missing something obvious here.) Edit 0: The prototype sequence I have in mind is . Edit 1: The summability criterion implies . Intuitively it ought to imply more, as summability is stronger than convergence to zero. Perhaps we can find a creative way to combine series at varying ?",x_n \mathbb{R} \alpha > 1 \sum_{n = 1}^\infty x_{\lceil \alpha^n\rceil} < \infty x_n \to 0 x_n = 1 n = m! m 0 x_n > \epsilon n \{\lceil \alpha^n\rceil\} \alpha x_n = n^{-1} \lim \inf x_n = 0 \alpha,"['real-analysis', 'sequences-and-series', 'convergence-divergence']"
40,"Finding the smallest $\alpha>0$ for which $\exists\beta(\alpha)>0$ so that $\sqrt{1+x}+\sqrt{1-x}\le 2-\frac{x^\alpha}\beta,\forall x\in[0,1]$.",Finding the smallest  for which  so that .,"\alpha>0 \exists\beta(\alpha)>0 \sqrt{1+x}+\sqrt{1-x}\le 2-\frac{x^\alpha}\beta,\forall x\in[0,1]","Find the smallest $\alpha>0$ for which there is $\beta(\alpha)>0$ so that the following inequality holds $\forall x\in[0,1]$ : $$\sqrt{1+x}+\sqrt{1-x}\le 2-\frac{x^\alpha}\beta$$ and find $\min\beta(a)$ for that particular $a$ . Source: Elezović, N., Odabrani zadatci elementarne matematike, Zagreb, 1992 EDIT: By $\beta(\alpha)$ I denoted a positive $\beta$ depends on $\alpha$ . If $\beta>0$ exists for some $\alpha>0$ , it doesn't have to be unique. My thoughts: Function $f(x)=\sqrt{1+x}+\sqrt{1-x}$ is concave on $(-1,1)$ , and particulary on $(0,1)$ , which follows from the fact that $$f''(x)=-\frac14\left(\frac1{\sqrt{(1+x)^3}}+\frac1{\sqrt{(1-x)^3}}\right)<0,\space\forall x\in(0,1).$$ Since $f$ is concave, each of its tangents is above the graph $\Gamma(f)$ . My first idea was to observe the polynomial $g(x)=f'(c)(x-c)+f(c),c\in (0,1),\deg g=1,$ which made me suspect that $\alpha\ge 1$ . I noticed that a function $f_2(x)=-\frac{x^\alpha}\beta$ is concave for $\alpha>1,$ convex for $0<\alpha<1$ and both convex and concave for $\alpha=1,$ when $f_2$ is linear. Then, $$\alpha\ge 1\implies f_3(x)=2-\frac{x^\alpha}\beta\text{ is also concave}.$$ and this polynomial $f_3$ and the function $f$ should have at most one intersection and at $x=0$ . However, I'm not able to justify my claim that this is the only possibility. Also, $f'$ is strictly decreasing on an open interval, and hence, my attempt to express $\min\beta(\alpha)$ in terms of $f'$ in the second part of the task failed. May I ask for advice on solving this task? Thank you very much in advance!","Find the smallest for which there is so that the following inequality holds : and find for that particular . Source: Elezović, N., Odabrani zadatci elementarne matematike, Zagreb, 1992 EDIT: By I denoted a positive depends on . If exists for some , it doesn't have to be unique. My thoughts: Function is concave on , and particulary on , which follows from the fact that Since is concave, each of its tangents is above the graph . My first idea was to observe the polynomial which made me suspect that . I noticed that a function is concave for convex for and both convex and concave for when is linear. Then, and this polynomial and the function should have at most one intersection and at . However, I'm not able to justify my claim that this is the only possibility. Also, is strictly decreasing on an open interval, and hence, my attempt to express in terms of in the second part of the task failed. May I ask for advice on solving this task? Thank you very much in advance!","\alpha>0 \beta(\alpha)>0 \forall x\in[0,1] \sqrt{1+x}+\sqrt{1-x}\le 2-\frac{x^\alpha}\beta \min\beta(a) a \beta(\alpha) \beta \alpha \beta>0 \alpha>0 f(x)=\sqrt{1+x}+\sqrt{1-x} (-1,1) (0,1) f''(x)=-\frac14\left(\frac1{\sqrt{(1+x)^3}}+\frac1{\sqrt{(1-x)^3}}\right)<0,\space\forall x\in(0,1). f \Gamma(f) g(x)=f'(c)(x-c)+f(c),c\in (0,1),\deg g=1, \alpha\ge 1 f_2(x)=-\frac{x^\alpha}\beta \alpha>1, 0<\alpha<1 \alpha=1, f_2 \alpha\ge 1\implies f_3(x)=2-\frac{x^\alpha}\beta\text{ is also concave}. f_3 f x=0 f' \min\beta(\alpha) f'","['real-analysis', 'functions']"
41,Every separable Banach space is isometrically isomorphic to a quotient of $\ell^1$,Every separable Banach space is isometrically isomorphic to a quotient of,\ell^1,"I am currently trying to prove the statement above. So let $X$ be a Banach space and choose a dense sequence $(x_n)_n$ in the closed unit ball of $X$ . Then it is easy to see that $$T: \ell^1 \to X, \quad Ta = \sum_{n = 1}^\infty a_n x_n$$ is well-defined and $\lVert T \rVert \leq 1$ . To prove surjectivity of the map pick $x \in X$ with $\lVert x \rVert \leq 1$ . Then by density of the sequence $(x_n)_n$ I can find some $n_1 \in \mathbb N$ such that $\lVert 2x - x_{n_1} \rVert \leq 1$ . For the same reason I can find $n_2 \in \mathbb N$ such that $\lVert 2(2x - x_{n_1}) - x_{n_2}\rVert \leq 1$ . So inductively I obtain a sequence $(n_k)_k$ such that $$ \bigg \lVert x - \sum_{k = 1}^N \frac{1}{2^k} x_{n_k} \bigg \rVert < \frac{1}{2^N}$$ for all $N \in \mathbb N$ . Now set $a := \sum_{k = 1}^\infty \frac{1}{2^k} e_{n_k}$ (where $e_n$ denotes the $n$ -th unit vector) and obtain that $a \in \ell_1$ and $\lVert a \rVert_{\ell^1} \leq 1$ . Moreover, one easily calculates that $Ta = x$ . Hence $T$ maps the closed unit ball of $\ell^1$ onto the closed unit ball of $X$ . In particular, $T$ is surjective and by the isomorphism theorem $$ S: \ell^1/ \ker T \to X,\quad a + \ker T \mapsto Ta $$ is an isomorphism. Hence, $\ell^1/ \ker T \cong X$ as vector spaces. So it is just left to show that $S$ is an isometry. So let $a \in \ell^1$ . Then one has clearly that $$ \lVert S(a + \ker T) \rVert = \lVert T(a + b) \rVert \leq \lVert a + b \rVert_{\ell^1}$$ for each $b \in \ker T$ and therefore $$ \lVert S(a + \ker T) \rVert \leq \inf \{\lVert a + b \rVert_{\ell^1} : b \in \ker T\} = \lVert a + \ker T \rVert,$$ i.e. $\lVert S \rVert \leq 1$ . This means by the inverse theorem that $\ell^1/ \ker T \cong X$ as Banach spaces. But I have no idea how to show that $\lVert S(a + \ker T) \rVert \geq \lVert a + \ker T \rVert$ for all $a \in \ell^1$ . I think I have to come up with some fancy representative $\tilde a \in \ell^1$ but I do not see how to get that. What am I missing? Please tell me anyone :-)","I am currently trying to prove the statement above. So let be a Banach space and choose a dense sequence in the closed unit ball of . Then it is easy to see that is well-defined and . To prove surjectivity of the map pick with . Then by density of the sequence I can find some such that . For the same reason I can find such that . So inductively I obtain a sequence such that for all . Now set (where denotes the -th unit vector) and obtain that and . Moreover, one easily calculates that . Hence maps the closed unit ball of onto the closed unit ball of . In particular, is surjective and by the isomorphism theorem is an isomorphism. Hence, as vector spaces. So it is just left to show that is an isometry. So let . Then one has clearly that for each and therefore i.e. . This means by the inverse theorem that as Banach spaces. But I have no idea how to show that for all . I think I have to come up with some fancy representative but I do not see how to get that. What am I missing? Please tell me anyone :-)","X (x_n)_n X T: \ell^1 \to X, \quad Ta = \sum_{n = 1}^\infty a_n x_n \lVert T \rVert \leq 1 x \in X \lVert x \rVert \leq 1 (x_n)_n n_1 \in \mathbb N \lVert 2x - x_{n_1} \rVert \leq 1 n_2 \in \mathbb N \lVert 2(2x - x_{n_1}) - x_{n_2}\rVert \leq 1 (n_k)_k  \bigg \lVert x - \sum_{k = 1}^N \frac{1}{2^k} x_{n_k} \bigg \rVert < \frac{1}{2^N} N \in \mathbb N a := \sum_{k = 1}^\infty \frac{1}{2^k} e_{n_k} e_n n a \in \ell_1 \lVert a \rVert_{\ell^1} \leq 1 Ta = x T \ell^1 X T  S: \ell^1/ \ker T \to X,\quad a + \ker T \mapsto Ta  \ell^1/ \ker T \cong X S a \in \ell^1  \lVert S(a + \ker T) \rVert = \lVert T(a + b) \rVert \leq \lVert a + b \rVert_{\ell^1} b \in \ker T  \lVert S(a + \ker T) \rVert \leq \inf \{\lVert a + b \rVert_{\ell^1} : b \in \ker T\} = \lVert a + \ker T \rVert, \lVert S \rVert \leq 1 \ell^1/ \ker T \cong X \lVert S(a + \ker T) \rVert \geq \lVert a + \ker T \rVert a \in \ell^1 \tilde a \in \ell^1","['real-analysis', 'functional-analysis', 'operator-theory', 'banach-spaces']"
42,"Higher derivatives of the map $I:T \mapsto T^{-1}$, where $T \in \mathcal B(X)$.","Higher derivatives of the map , where .",I:T \mapsto T^{-1} T \in \mathcal B(X),"Let $X$ be a Banach space, $\mathcal B(X;Y)$ denotes the set of bounded linear operators $X\to Y$ . Consider the inverting map $I:U\subset\mathcal B(Y;X)\to \mathcal B(X;Y)$ defined by $I(T) = T^{-1}$ , where $U$ is the set where this makes sense. It is known, e.g. here , that $I$ is (Frechet) differentiable and $$ I'(T)[A] = -T^{-1}AT^{-1},  $$ here $I'(T)$ is viewed as an element of $\mathcal B(\mathcal B(Y;X);\mathcal B(X;Y))$ . How do we prove that the $k^{\text{th}}$ -derivative of $I$ is the $k$ -multilinear map $$ (A_1,\dots,A_k) \mapsto (-1)^{k} \sum_{\sigma\in S_k} T^{-1}A_{\sigma(1)}T^{-1}\dots T^{-1}A_{\sigma(k)} T^{-1}, $$ where the sum is over all permutations $\sigma$ of $\{1,\dots,k\}$ ? This formula is given in a book by Hormander without a proof (as usual). It looks like a symmetrization of the higher order terms in the Taylor expansion of $I$ (some details are seen in this thread ). To obtain higher order derivatives, I tried to differentiate $I'$ by writing $I' = -M\circ I$ , where $M(T)[A] = TAT$ , and repeatedly apply chain rule. However, the higher derivatives of $M$ gets ugly really fast (or that I don't know a clean way to write it down). Is there a nice way to prove this result?","Let be a Banach space, denotes the set of bounded linear operators . Consider the inverting map defined by , where is the set where this makes sense. It is known, e.g. here , that is (Frechet) differentiable and here is viewed as an element of . How do we prove that the -derivative of is the -multilinear map where the sum is over all permutations of ? This formula is given in a book by Hormander without a proof (as usual). It looks like a symmetrization of the higher order terms in the Taylor expansion of (some details are seen in this thread ). To obtain higher order derivatives, I tried to differentiate by writing , where , and repeatedly apply chain rule. However, the higher derivatives of gets ugly really fast (or that I don't know a clean way to write it down). Is there a nice way to prove this result?","X \mathcal B(X;Y) X\to Y I:U\subset\mathcal B(Y;X)\to \mathcal B(X;Y) I(T) = T^{-1} U I 
I'(T)[A] = -T^{-1}AT^{-1}, 
 I'(T) \mathcal B(\mathcal B(Y;X);\mathcal B(X;Y)) k^{\text{th}} I k 
(A_1,\dots,A_k) \mapsto (-1)^{k} \sum_{\sigma\in S_k} T^{-1}A_{\sigma(1)}T^{-1}\dots T^{-1}A_{\sigma(k)} T^{-1},
 \sigma \{1,\dots,k\} I I' I' = -M\circ I M(T)[A] = TAT M","['real-analysis', 'linear-algebra', 'complex-analysis', 'functional-analysis', 'operator-theory']"
43,Prove that between any two rational numbers there is a rational whose numerator and denominator are both perfect squares.,Prove that between any two rational numbers there is a rational whose numerator and denominator are both perfect squares.,,"(1) Suppose $\frac{p_1}{q_1}$ and $\frac{p_2}{q_2}$ are rationals with $0<\frac{p_1}{q_1} <\frac{p_2}{q_2}$ .  We want to find a rational $\frac{a}{b}$ such that $\frac{p_1}{q_1}<\frac{a^2}{b^2} <\frac{p_2}{q_2}$ . I know that if we choose any rational in the open interval between $\sqrt{\frac{p_1}{q_1}}$ and $\sqrt{\frac{p_2}{q_2}}$ , then this rational will have the desired property.  However, the issue is that I am trying to use this proof as a part of a larger proof to prove the existence of the square root of any positive real number.  So I cannot use any argument that directly references square roots. Here is some work I have done so far.  I went down the rabbit hole a bit and this may be a dead end, I'm not sure. Since $\frac{p_1}{q_1} = \frac{p_1q_1q_2^2}{q_1^2q_2^2}$ and $\frac{p_2}{q_2} = \frac{p_2q_1^2q_2}{q_1^2q_2^2}$ , it is sufficient to prove that: (2) there exists a perfect square, $n^2$ , (possibly 1) such that the interval between $n^2p_1q_1q_2^2$ and $n^2p_2q_1^2q_2$ contains a perfect square. To prove (2) it is sufficient to prove that: (3) for all positive integers $n$ there exist perfect squares, $k^2$ and $m^2$ , such that $k^2n<m^2<k^2(n+1)$ So my question is either for assistance proving any of (1), (2), or (3) without using the existence of square roots of positive real numbers, OR if this is something that MUST be proven using the existence of square roots, then just state that.  I would NOT like help in proving the existence of square roots another way.  If it MUST be done another way, I would like to figure out the other way myself. That being said, I am sure that proving the existence of square roots of positive reals CAN be proven another way (likely much simpler), but I am hoping that the path I am taking may work too.  I just need help with this last part.","(1) Suppose and are rationals with .  We want to find a rational such that . I know that if we choose any rational in the open interval between and , then this rational will have the desired property.  However, the issue is that I am trying to use this proof as a part of a larger proof to prove the existence of the square root of any positive real number.  So I cannot use any argument that directly references square roots. Here is some work I have done so far.  I went down the rabbit hole a bit and this may be a dead end, I'm not sure. Since and , it is sufficient to prove that: (2) there exists a perfect square, , (possibly 1) such that the interval between and contains a perfect square. To prove (2) it is sufficient to prove that: (3) for all positive integers there exist perfect squares, and , such that So my question is either for assistance proving any of (1), (2), or (3) without using the existence of square roots of positive real numbers, OR if this is something that MUST be proven using the existence of square roots, then just state that.  I would NOT like help in proving the existence of square roots another way.  If it MUST be done another way, I would like to figure out the other way myself. That being said, I am sure that proving the existence of square roots of positive reals CAN be proven another way (likely much simpler), but I am hoping that the path I am taking may work too.  I just need help with this last part.",\frac{p_1}{q_1} \frac{p_2}{q_2} 0<\frac{p_1}{q_1} <\frac{p_2}{q_2} \frac{a}{b} \frac{p_1}{q_1}<\frac{a^2}{b^2} <\frac{p_2}{q_2} \sqrt{\frac{p_1}{q_1}} \sqrt{\frac{p_2}{q_2}} \frac{p_1}{q_1} = \frac{p_1q_1q_2^2}{q_1^2q_2^2} \frac{p_2}{q_2} = \frac{p_2q_1^2q_2}{q_1^2q_2^2} n^2 n^2p_1q_1q_2^2 n^2p_2q_1^2q_2 n k^2 m^2 k^2n<m^2<k^2(n+1),['real-analysis']
44,"With the condition $\lim_{x\to\infty}(f(x+a)−f(x))=0$, how to prove that $f(x)$ is uniformly continuous?","With the condition , how to prove that  is uniformly continuous?",\lim_{x\to\infty}(f(x+a)−f(x))=0 f(x),"Assume $f\in C[0,+\infty)$ , and for all $a>0$ , we have $$\lim_{x\to\infty}(f(x+a)−f(x))=0.$$ Prove that $f(x)$ is uniformly continuous. One hint is that we can use Baire category theorem, but I still don't know how to use it. Maybe there is another way to answer this question, I'm not sure. Looking forward to your answer.","Assume , and for all , we have Prove that is uniformly continuous. One hint is that we can use Baire category theorem, but I still don't know how to use it. Maybe there is another way to answer this question, I'm not sure. Looking forward to your answer.","f\in C[0,+\infty) a>0 \lim_{x\to\infty}(f(x+a)−f(x))=0. f(x)","['real-analysis', 'calculus', 'general-topology', 'baire-category']"
45,Corollary of the Malgrange Preparation Theorem,Corollary of the Malgrange Preparation Theorem,,"Let $f:\mathbb{R}\times \mathbb{R}^n \to \mathbb{R}$ be a smooth function, such that $$f(0,0)=0,\ \frac{\partial f}{\partial t} (0,0) = 0,\ldots, \frac{\partial^{k-1} f}{\partial t^{k-1}} (0,0) = 0,\ \frac{\partial^{k} f}{\partial t^{k}} (0,0) \neq 0,$$ then the Malgrange preparation theorem, states that there exists smooth functions $c,a_0,...,a_{k-1}:\mathbb{R^n}\to\mathbb{R}$ , such that near the origin $${\displaystyle f(t,x)=c(t,x)\left(t^{k}+a_{k-1}(x)t^{k-1}+\cdots +a_{0}(x)\right)},$$ and $c(0,0)\neq 0$ . I'm reading the paper ""S. M Vishik - Vector Fields Near the Boundary of a Manifold"", and on page $17$ , the author says that the following theorem is a corollary of the Magrange preparation theorem. Corollary: Let $\varphi:\mathbb{R}\times \mathbb{R}^n\to \mathbb{R}$ a smooth function such that $$\varphi(0,0)=0,\ \frac{\partial \varphi}{\partial t} (0,0) = 0,\ldots, \frac{\partial^{k-1} \varphi}{\partial t^{k-1}} (0,0) = 0,\ \frac{\partial^{k} \varphi}{\partial t^{k}} (0,0) \neq 0,$$ then there exists smooth functions $b,a_1,...,a_k:\mathbb{R}^n\times\mathbb{R} \to \mathbb{R}$ such that $$t^{k}+\sum_{i=1}^{k-1} a_i(x,\varphi(t,x))\cdot t^i = b(x,\varphi(t,x)), $$ in a neighborhood of the origin. Does anyone know this corollary and could you give me information on how to demonstrate it or let me know where I can find its demonstration? Just some commentaries This is the part that the author claims such corollary, $M$ is a manifold, $Q$ is a codimension 1 submanifold of $M$ and ""shoots"" is a fancy name for ""germs"". And the numbered reference: however, I was not able to find this paper.","Let be a smooth function, such that then the Malgrange preparation theorem, states that there exists smooth functions , such that near the origin and . I'm reading the paper ""S. M Vishik - Vector Fields Near the Boundary of a Manifold"", and on page , the author says that the following theorem is a corollary of the Magrange preparation theorem. Corollary: Let a smooth function such that then there exists smooth functions such that in a neighborhood of the origin. Does anyone know this corollary and could you give me information on how to demonstrate it or let me know where I can find its demonstration? Just some commentaries This is the part that the author claims such corollary, is a manifold, is a codimension 1 submanifold of and ""shoots"" is a fancy name for ""germs"". And the numbered reference: however, I was not able to find this paper.","f:\mathbb{R}\times \mathbb{R}^n \to \mathbb{R} f(0,0)=0,\ \frac{\partial f}{\partial t} (0,0) = 0,\ldots, \frac{\partial^{k-1} f}{\partial t^{k-1}} (0,0) = 0,\ \frac{\partial^{k} f}{\partial t^{k}} (0,0) \neq 0, c,a_0,...,a_{k-1}:\mathbb{R^n}\to\mathbb{R} {\displaystyle f(t,x)=c(t,x)\left(t^{k}+a_{k-1}(x)t^{k-1}+\cdots +a_{0}(x)\right)}, c(0,0)\neq 0 17 \varphi:\mathbb{R}\times \mathbb{R}^n\to \mathbb{R} \varphi(0,0)=0,\ \frac{\partial \varphi}{\partial t} (0,0) = 0,\ldots, \frac{\partial^{k-1} \varphi}{\partial t^{k-1}} (0,0) = 0,\ \frac{\partial^{k} \varphi}{\partial t^{k}} (0,0) \neq 0, b,a_1,...,a_k:\mathbb{R}^n\times\mathbb{R} \to \mathbb{R} t^{k}+\sum_{i=1}^{k-1} a_i(x,\varphi(t,x))\cdot t^i = b(x,\varphi(t,x)),  M Q M","['real-analysis', 'analysis', 'modules', 'dynamical-systems', 'germs']"
46,Prove or disprove that if $\lim\limits_{x\to0^+}f(x)=0$ and $|x^2f''(x)|\leq c$ then $\lim\limits_{x\to0^+}xf'(x)=0$,Prove or disprove that if  and  then,\lim\limits_{x\to0^+}f(x)=0 |x^2f''(x)|\leq c \lim\limits_{x\to0^+}xf'(x)=0,"A function $f$ defined on interval $(0,1)$ with a continuous twice derivation $(f\in{C^2(0,1)})$ satisfies $\lim_{x\to0^+}f(x)=0$ and $|x^2f''(x)|\leq{C}$ where $C$ is a fixed positive real number. Prove $\lim_{x\to0^+}xf'(x)=0$ (or disprove it!) I've tried several ways like calculating $yf'(y)-xf'(x)=f(y)-f(x)+\int_{x}^{y}tf''(t)\,dt$ to prove the limitation exists (and failed). I also think it is similer to L'Hospital rule $\lim_{x\to0^+}\frac{f'(x)}{\frac{1}{x}}=\lim_{x\to0^+}\frac{f''(x)}{-\frac{1}{x^2}}$ ,but it's only a sufficient condition and probably wrong.","A function defined on interval with a continuous twice derivation satisfies and where is a fixed positive real number. Prove (or disprove it!) I've tried several ways like calculating to prove the limitation exists (and failed). I also think it is similer to L'Hospital rule ,but it's only a sufficient condition and probably wrong.","f (0,1) (f\in{C^2(0,1)}) \lim_{x\to0^+}f(x)=0 |x^2f''(x)|\leq{C} C \lim_{x\to0^+}xf'(x)=0 yf'(y)-xf'(x)=f(y)-f(x)+\int_{x}^{y}tf''(t)\,dt \lim_{x\to0^+}\frac{f'(x)}{\frac{1}{x}}=\lim_{x\to0^+}\frac{f''(x)}{-\frac{1}{x^2}}","['real-analysis', 'limits', 'derivatives']"
47,Convergence of a series of translations of a Lebesgue integrable function,Convergence of a series of translations of a Lebesgue integrable function,,Let $f: \mathbb{R} \rightarrow \mathbb{R}$ be a Lebesgue integrable function. Prove that $$\sum_{n=1}^{\infty} \frac{f(x-\sqrt{n})}{\sqrt{n}}$$ converges almost for every $x \in \mathbb{R}$. My tactic here (which of course might lead me to nowhere) is to express this series as an integral or sum of integrals and use in some way  the fact that $f$ is integrable along with a convergence theorem of course. From integrability of $f$ we know that $f$ is finite almost everywhere which I believe would help me in my proof. But I don't know exactly how to start with this. Any useful hint would be appreciated. I don't want a full solution to this. Thank you in advance.,Let $f: \mathbb{R} \rightarrow \mathbb{R}$ be a Lebesgue integrable function. Prove that $$\sum_{n=1}^{\infty} \frac{f(x-\sqrt{n})}{\sqrt{n}}$$ converges almost for every $x \in \mathbb{R}$. My tactic here (which of course might lead me to nowhere) is to express this series as an integral or sum of integrals and use in some way  the fact that $f$ is integrable along with a convergence theorem of course. From integrability of $f$ we know that $f$ is finite almost everywhere which I believe would help me in my proof. But I don't know exactly how to start with this. Any useful hint would be appreciated. I don't want a full solution to this. Thank you in advance.,,"['real-analysis', 'measure-theory', 'convergence-divergence', 'lebesgue-integral', 'lebesgue-measure']"
48,Reference request for CMI M.Sc entrance exam,Reference request for CMI M.Sc entrance exam,,"Today I would like to ask you for any references, books, pdf's etc. that comprises of a lot problems with the level advance graduate. The syllabus is that of teaching in most under-graduate program in the topics algebra, analysis, complex analysis, general topology. I'm specifically looking for the type of books that deals with mostly problems that cross over different topics. For example, consider the following problem whose solution requires concepts from analysis and algebra: let $R$ be a ring of all real valued continuous functions on the closed unit interval . If M is a maximal ideal of R , prove that there exists a real number $\gamma$ , $0\leq \gamma \leq 1$ such that $M=\{f(x)\in R :f(\gamma)=0\} $ . More for instances of the kind the probelms i'm looking see this links . http://www.cmi.ac.in/admissions/sample-qp/pgmath2016.pdf http://www.cmi.ac.in/admissions/sample-qp/pgmath2015.pdf http://www.cmi.ac.in/admissions/sample-qp/pgmath2014.pdf . . The books i mostly use are : Alegbra: Herstein , dummit-foote , Artin ,Hoffman kunze , Sheldon Axlers Real analysis- Rudin ,Pugh , Apostol , Complex analysis - Ahlfors, Conway , Bak-Newman . Topology -Munkers ,JK Joshi , Willard , Simmons . I already going through many Ph-D qualifying exams of various universities across the globe . But i feel this isn't enough . I know i'm asking a lot but cracking this exams means a lot to me . Any help will be greatly appreciated . Thank you all .","Today I would like to ask you for any references, books, pdf's etc. that comprises of a lot problems with the level advance graduate. The syllabus is that of teaching in most under-graduate program in the topics algebra, analysis, complex analysis, general topology. I'm specifically looking for the type of books that deals with mostly problems that cross over different topics. For example, consider the following problem whose solution requires concepts from analysis and algebra: let $R$ be a ring of all real valued continuous functions on the closed unit interval . If M is a maximal ideal of R , prove that there exists a real number $\gamma$ , $0\leq \gamma \leq 1$ such that $M=\{f(x)\in R :f(\gamma)=0\} $ . More for instances of the kind the probelms i'm looking see this links . http://www.cmi.ac.in/admissions/sample-qp/pgmath2016.pdf http://www.cmi.ac.in/admissions/sample-qp/pgmath2015.pdf http://www.cmi.ac.in/admissions/sample-qp/pgmath2014.pdf . . The books i mostly use are : Alegbra: Herstein , dummit-foote , Artin ,Hoffman kunze , Sheldon Axlers Real analysis- Rudin ,Pugh , Apostol , Complex analysis - Ahlfors, Conway , Bak-Newman . Topology -Munkers ,JK Joshi , Willard , Simmons . I already going through many Ph-D qualifying exams of various universities across the globe . But i feel this isn't enough . I know i'm asking a lot but cracking this exams means a lot to me . Any help will be greatly appreciated . Thank you all .",,"['real-analysis', 'abstract-algebra', 'general-topology', 'complex-analysis', 'reference-request']"
49,How to prove that $x^{1-x}+(1-x)^{x}\leq x^{1/2}+(1-x)^{1/2}$?,How to prove that ?,x^{1-x}+(1-x)^{x}\leq x^{1/2}+(1-x)^{1/2},"Let $x\in [0,1]$,try to prove that: $$x^{1-x}+(1-x)^{x}\leq x^{1/2}+(1-x)^{1/2}$$ My try: let $x=\sin ^{2}t$,and it is equal to show that  $$\sin^{2}t^{\cos^2{t}}+\cos^{2}t^{\sin^2{t}}\leq \sin t+\cos t$$ but still nothing. Thanks :-)","Let $x\in [0,1]$,try to prove that: $$x^{1-x}+(1-x)^{x}\leq x^{1/2}+(1-x)^{1/2}$$ My try: let $x=\sin ^{2}t$,and it is equal to show that  $$\sin^{2}t^{\cos^2{t}}+\cos^{2}t^{\sin^2{t}}\leq \sin t+\cos t$$ but still nothing. Thanks :-)",,"['real-analysis', 'inequality']"
50,Recursive Sequence $a_n = \frac{1}{2} (a_{n-1} + 5) $,Recursive Sequence,a_n = \frac{1}{2} (a_{n-1} + 5) ,"I got this question in which they ask me to explain why it is convergent and evaluate its limit. $$a_1=3\;and\;a_n = \frac{1}{2} (a_{n-1} + 5) \\ n=2,3,4,... $$ To prove it's convergent, I show that it is increasing and bounded above by 5. Also, I find its limit by showing that Let $L=\lim_{n\to\infty} a_n$ Notice that $a_n = \frac{1}{2} (a_{n-1} + 5)$ Hence $\lim_{n\to\infty} a_n = \lim_{n\to\infty} \frac{1}{2} (a_{n-1} + 5)$ $\Rightarrow L = \frac{1}{2}(L+5)$ $\Rightarrow 2L = L+5$ $\Rightarrow L = 5$ As the sequence is non-decreasing, $$L =\lim_{n\to\infty} a_n=5 $$ That's what I got. However, the book's answer for this question's limit is $\frac{5}{2}$ Is there anything wrong with my proof? Thanks in advance.","I got this question in which they ask me to explain why it is convergent and evaluate its limit. $$a_1=3\;and\;a_n = \frac{1}{2} (a_{n-1} + 5) \\ n=2,3,4,... $$ To prove it's convergent, I show that it is increasing and bounded above by 5. Also, I find its limit by showing that Let $L=\lim_{n\to\infty} a_n$ Notice that $a_n = \frac{1}{2} (a_{n-1} + 5)$ Hence $\lim_{n\to\infty} a_n = \lim_{n\to\infty} \frac{1}{2} (a_{n-1} + 5)$ $\Rightarrow L = \frac{1}{2}(L+5)$ $\Rightarrow 2L = L+5$ $\Rightarrow L = 5$ As the sequence is non-decreasing, $$L =\lim_{n\to\infty} a_n=5 $$ That's what I got. However, the book's answer for this question's limit is $\frac{5}{2}$ Is there anything wrong with my proof? Thanks in advance.",,"['real-analysis', 'sequences-and-series', 'limits', 'recurrence-relations']"
51,Product of metric outer measures,Product of metric outer measures,,"The problem below has been asked recently already but, as a naive user, I got burned (well singed perhaps) because I asked the question in the wrong place.  So if this looks like a redundant question ... excuse me (as Steve Martin would say)! Problem:  Show that the product of two metric outer measures (also known as Borel measures on a metric space) is again a metric outer measure. Does anyone know if the problem is valid? The hints given already on this forum work only for the separable case, but I don't have a solution to the general case. I found the problem originally in the text by Munroe ( Measure and Integration 1953). It seemed that the obvious approach would work so I added it without further thought to our book ( Real Analysis , Bruckner/Bruckner/Thomson 1996). We received some valuable feedback from many sources, mostly from R. B. Burckel who pointed out among many other things that this problem was not straightforward and may be wrong. For the second 2008 edition I left the problem (since it is interesting) but included a footnote to indicate that there was some doubt. Anybody have a definitive answer? The problem appears as Exercise 6:1.5. in our 2nd edition. You can get a free PDF from the website classicalrealanalysis.com. Hope to hear from someone (not, I hope, from someone assigned this problem because it appeared in our first edition--if so blame Munroe -- please). Brian","The problem below has been asked recently already but, as a naive user, I got burned (well singed perhaps) because I asked the question in the wrong place.  So if this looks like a redundant question ... excuse me (as Steve Martin would say)! Problem:  Show that the product of two metric outer measures (also known as Borel measures on a metric space) is again a metric outer measure. Does anyone know if the problem is valid? The hints given already on this forum work only for the separable case, but I don't have a solution to the general case. I found the problem originally in the text by Munroe ( Measure and Integration 1953). It seemed that the obvious approach would work so I added it without further thought to our book ( Real Analysis , Bruckner/Bruckner/Thomson 1996). We received some valuable feedback from many sources, mostly from R. B. Burckel who pointed out among many other things that this problem was not straightforward and may be wrong. For the second 2008 edition I left the problem (since it is interesting) but included a footnote to indicate that there was some doubt. Anybody have a definitive answer? The problem appears as Exercise 6:1.5. in our 2nd edition. You can get a free PDF from the website classicalrealanalysis.com. Hope to hear from someone (not, I hope, from someone assigned this problem because it appeared in our first edition--if so blame Munroe -- please). Brian",,"['real-analysis', 'measure-theory', 'set-theory']"
52,My proof that sum of convergent sequences converges to sum of limits,My proof that sum of convergent sequences converges to sum of limits,,"Does my proof appear correct? Also, do you like the notation? $\textbf{Theorem.}$ If $(a_n)_{n \in \mathbb{N}}$ and $(b_n)_{n \in \mathbb{N}}$ are convergent real sequences, then $$ \lim_{n \to \infty} \left( a_n + b_n \right) = \left( \lim_{n \to \infty} a_n \right) + \left( \lim_{n \to \infty} b_n \right) . $$ $\textit{Proof.}$ Let $\varepsilon > 0$. It remains to prove that there is $N \in \mathbb{N}$ such that, for every natural number $m > N$, we have $$ \left|   a_m + b_m -   \left( \lim_{n \to \infty} a_n \right) -   \left( \lim_{n \to \infty} b_n \right) \right| < \varepsilon . $$ By assumption, there is $N_a \in \mathbb{N}$ such that, for every natural number $m > N_a$, we have $$ \left|   a_m -   \left( \lim_{n \to \infty} a_n \right) \right| < \varepsilon / 2 . $$ Analogously, there is $N_b \in \mathbb{N}$ such that, for every natural number $m > N_b$, we have $$ \left|   b_m -   \left( \lim_{n \to \infty} b_n \right) \right| < \varepsilon / 2 . $$ We choose $N := \max(N_a, N_b)$. Let $m \in \mathbb{N}$ such that $m > N$. Obviously, for this $m$, each of the above two inequalities holds. Thus, we may add the inequalities. Doing so, we obtain \begin{equation*} \begin{split} \varepsilon = \varepsilon / 2 + \varepsilon / 2 & > \left|   a_m -   \left( \lim_{n \to \infty} a_n \right) \right| + \left|   b_m -   \left( \lim_{n \to \infty} b_n \right) \right| \\ & \ge \left|   a_m -   \left( \lim_{n \to \infty} a_n \right) +   b_m -   \left( \lim_{n \to \infty} b_n \right) \right| &\quad& \text{by subadditivity of abs. val.} \\ & = \left|   a_m + b_m -   \left( \lim_{n \to \infty} a_n \right) -   \left( \lim_{n \to \infty} b_n \right) \right| . \end{split} \end{equation*} Hence, by transitivity, \begin{equation*} \varepsilon > \left|   a_m + b_m -   \left( \lim_{n \to \infty} a_n \right) -   \left( \lim_{n \to \infty} b_n \right) \right|, \end{equation*} QED.","Does my proof appear correct? Also, do you like the notation? $\textbf{Theorem.}$ If $(a_n)_{n \in \mathbb{N}}$ and $(b_n)_{n \in \mathbb{N}}$ are convergent real sequences, then $$ \lim_{n \to \infty} \left( a_n + b_n \right) = \left( \lim_{n \to \infty} a_n \right) + \left( \lim_{n \to \infty} b_n \right) . $$ $\textit{Proof.}$ Let $\varepsilon > 0$. It remains to prove that there is $N \in \mathbb{N}$ such that, for every natural number $m > N$, we have $$ \left|   a_m + b_m -   \left( \lim_{n \to \infty} a_n \right) -   \left( \lim_{n \to \infty} b_n \right) \right| < \varepsilon . $$ By assumption, there is $N_a \in \mathbb{N}$ such that, for every natural number $m > N_a$, we have $$ \left|   a_m -   \left( \lim_{n \to \infty} a_n \right) \right| < \varepsilon / 2 . $$ Analogously, there is $N_b \in \mathbb{N}$ such that, for every natural number $m > N_b$, we have $$ \left|   b_m -   \left( \lim_{n \to \infty} b_n \right) \right| < \varepsilon / 2 . $$ We choose $N := \max(N_a, N_b)$. Let $m \in \mathbb{N}$ such that $m > N$. Obviously, for this $m$, each of the above two inequalities holds. Thus, we may add the inequalities. Doing so, we obtain \begin{equation*} \begin{split} \varepsilon = \varepsilon / 2 + \varepsilon / 2 & > \left|   a_m -   \left( \lim_{n \to \infty} a_n \right) \right| + \left|   b_m -   \left( \lim_{n \to \infty} b_n \right) \right| \\ & \ge \left|   a_m -   \left( \lim_{n \to \infty} a_n \right) +   b_m -   \left( \lim_{n \to \infty} b_n \right) \right| &\quad& \text{by subadditivity of abs. val.} \\ & = \left|   a_m + b_m -   \left( \lim_{n \to \infty} a_n \right) -   \left( \lim_{n \to \infty} b_n \right) \right| . \end{split} \end{equation*} Hence, by transitivity, \begin{equation*} \varepsilon > \left|   a_m + b_m -   \left( \lim_{n \to \infty} a_n \right) -   \left( \lim_{n \to \infty} b_n \right) \right|, \end{equation*} QED.",,"['real-analysis', 'sequences-and-series', 'limits', 'solution-verification']"
53,Does $\sum a_n$ converge if $a_n = \sin( \sin (...( \sin(x))...)$,Does  converge if,\sum a_n a_n = \sin( \sin (...( \sin(x))...),"Does $\sum a_n$ converge if $a_n = \sin( \sin (\cdots( \sin(x))\cdots)$, $\sin$ being applied n times and $x \in (0, \pi/2)$? What about $\sum a_n^r$ for $r \in \mathbb{R^+}$?","Does $\sum a_n$ converge if $a_n = \sin( \sin (\cdots( \sin(x))\cdots)$, $\sin$ being applied n times and $x \in (0, \pi/2)$? What about $\sum a_n^r$ for $r \in \mathbb{R^+}$?",,"['calculus', 'real-analysis', 'sequences-and-series', 'convergence-divergence', 'divergent-series']"
54,On the stochastic definition of $e$,On the stochastic definition of,e,"I've read on Wikipedia that one can give a stochastic representation of $e$: In addition to exact analytical expressions for representation of $e$, there are stochastic techniques for estimating $e$. One such approach begins with an infinite sequence of independent random variables $X_1, X_2,\dots$, drawn from the uniform distribution on $[0, 1]$. Let $V$ be the least number $n$ such that the sum of the first $n$ samples exceeds $1$:   $$V = \min \left \{ n \mid X_1+X_2+\cdots+X_n > 1 \right \}.$$   Then the expected value of $V$ is $e$: $\mathbb{E}(V) = e$. I was wondering how to show (analytically) that $\mathbb{E}(V) = e$. I looked at the references but they seems to deal just with numerical aspects.","I've read on Wikipedia that one can give a stochastic representation of $e$: In addition to exact analytical expressions for representation of $e$, there are stochastic techniques for estimating $e$. One such approach begins with an infinite sequence of independent random variables $X_1, X_2,\dots$, drawn from the uniform distribution on $[0, 1]$. Let $V$ be the least number $n$ such that the sum of the first $n$ samples exceeds $1$:   $$V = \min \left \{ n \mid X_1+X_2+\cdots+X_n > 1 \right \}.$$   Then the expected value of $V$ is $e$: $\mathbb{E}(V) = e$. I was wondering how to show (analytically) that $\mathbb{E}(V) = e$. I looked at the references but they seems to deal just with numerical aspects.",,"['real-analysis', 'probability']"
55,“Visualizing” Mathematical Objects - Tips & Tricks,“Visualizing” Mathematical Objects - Tips & Tricks,,"It has been a while since I am kind of stuck with my skills concerning the visualization of mathematical objects. Here there is the problem. First of all, let me point out that I am completely self-taught. In other words, this actually means that this site is the only possibility I have to speak mathematics, which is a bit like trying to learn japanese by rarely pronouncing some utterance to a random native speaker hoping not to sound too idiotic. In second place, I am trying to move towards rather abstract things, and –from time to time– I do have the feeling I have a decent grasp of what is going on. However… There are some objects that I simply don’t know how to approach! I think the problem can be rephrased in terms of extensive vs. intensive definition of a given mathematical object. I always tend to look for an extensive definition, but –when you start to deal with real analysis, topology or measure theory– those kind of definitions, that quite literally show how an object looks like, simply start to rarify. Thus, when I find a definition of –let’s say– $\ell^p$, or $\mathbb{N}^{\mathbb{N}}$, or Borel Sets, then I really fail to see what is going on. Maybe I can actually manipulate the symbols, and even get a “proof” or something that could almost look like a proof, but still, in most of the cases I have no idea what is going on. [Small aside: this is actually interesting. As long as definitions and objects are sort of trivial, there are ""pictures"" or advice to visualize objects, but when you get to the level of those objects, it is assumed that the reader/learner somewhere has got the skills to visualize those structures.] If you want an analogy, it is a bit like being in a completely dark room, with various objects and the explicit task to make them fit perfectly. I can do it (from time to time), but this does not imply that I see what the single objects where at the beginning, and how they look like now that they are assambled. Said so, here it comes the questions. How do you actually perceive or visualize the mathematical objects? How did your teachers/professors/supervisors taught you to visualize them? How do you teach your students to visualize mathematical objects? What are the “tricks”? There is something else that should be added, and that I think is related to the fact that I am self-taught. A lot of books of advanced maths simply discard “pictures”, even if (I suppose) maybe the authors were implicitly referring to them when they started to learn the topics. But here there is the point of not -being self taught: there is somebody who gives you this “tricks” (if you don’t like the word, we can use "" upaya "" instead) to build, and then erase every trace of the use of those hints. Thank you for your time and for any feedback that could come! PS: For those who are wondering, I added the real-analysis and measure-theory tags, because I would love to actually see the objects I was referring in the text, and to get tips and hints on how to do it. EDIT: After the nice feedback from user86418, I would like to point out something. The idea behind the question is not to head to the psychology of mathematics , or in other words how each user see things in her/his own mathematical world. Actually, the idea is to look for the general tips that are shown on the whiteboard (quite literally!) to visualize objects. An example could be (if I remember correctly history of math) the Argan plane: Gauss was the first to get the idea, but he erased any mention to it, and people were kind of stuck to visualize properly complex numbers for a while!","It has been a while since I am kind of stuck with my skills concerning the visualization of mathematical objects. Here there is the problem. First of all, let me point out that I am completely self-taught. In other words, this actually means that this site is the only possibility I have to speak mathematics, which is a bit like trying to learn japanese by rarely pronouncing some utterance to a random native speaker hoping not to sound too idiotic. In second place, I am trying to move towards rather abstract things, and –from time to time– I do have the feeling I have a decent grasp of what is going on. However… There are some objects that I simply don’t know how to approach! I think the problem can be rephrased in terms of extensive vs. intensive definition of a given mathematical object. I always tend to look for an extensive definition, but –when you start to deal with real analysis, topology or measure theory– those kind of definitions, that quite literally show how an object looks like, simply start to rarify. Thus, when I find a definition of –let’s say– $\ell^p$, or $\mathbb{N}^{\mathbb{N}}$, or Borel Sets, then I really fail to see what is going on. Maybe I can actually manipulate the symbols, and even get a “proof” or something that could almost look like a proof, but still, in most of the cases I have no idea what is going on. [Small aside: this is actually interesting. As long as definitions and objects are sort of trivial, there are ""pictures"" or advice to visualize objects, but when you get to the level of those objects, it is assumed that the reader/learner somewhere has got the skills to visualize those structures.] If you want an analogy, it is a bit like being in a completely dark room, with various objects and the explicit task to make them fit perfectly. I can do it (from time to time), but this does not imply that I see what the single objects where at the beginning, and how they look like now that they are assambled. Said so, here it comes the questions. How do you actually perceive or visualize the mathematical objects? How did your teachers/professors/supervisors taught you to visualize them? How do you teach your students to visualize mathematical objects? What are the “tricks”? There is something else that should be added, and that I think is related to the fact that I am self-taught. A lot of books of advanced maths simply discard “pictures”, even if (I suppose) maybe the authors were implicitly referring to them when they started to learn the topics. But here there is the point of not -being self taught: there is somebody who gives you this “tricks” (if you don’t like the word, we can use "" upaya "" instead) to build, and then erase every trace of the use of those hints. Thank you for your time and for any feedback that could come! PS: For those who are wondering, I added the real-analysis and measure-theory tags, because I would love to actually see the objects I was referring in the text, and to get tips and hints on how to do it. EDIT: After the nice feedback from user86418, I would like to point out something. The idea behind the question is not to head to the psychology of mathematics , or in other words how each user see things in her/his own mathematical world. Actually, the idea is to look for the general tips that are shown on the whiteboard (quite literally!) to visualize objects. An example could be (if I remember correctly history of math) the Argan plane: Gauss was the first to get the idea, but he erased any mention to it, and people were kind of stuck to visualize properly complex numbers for a while!",,"['real-analysis', 'measure-theory', 'soft-question', 'self-learning']"
56,Find $\sum_{n=1}^{\infty}\int_0^{\frac{1}{\sqrt{n}}}\frac{2x^2}{1+x^4}dx$ [duplicate],Find  [duplicate],\sum_{n=1}^{\infty}\int_0^{\frac{1}{\sqrt{n}}}\frac{2x^2}{1+x^4}dx,"This question already has an answer here : Evaluate $\sum_{n=1}^{\infty }\int_{0}^{\frac{1}{n}}\frac{\sqrt{x}}{1+x^{2}}\,\mathrm{d}x$ (1 answer) Closed last year . I want to find the sum: $$\sum_{n=1}^{\infty}\int_0^{\frac{1}{\sqrt{n}}}\frac{2x^2}{1+x^4}dx$$ I start with finding the antiderivative of the integrant, which is: $$\frac{1}{2\sqrt{2}}[\ln(x^2-\sqrt{2}x+1)-\ln(x^2+\sqrt{2}x+1)+2\arctan(\sqrt{2}x-1)+2\arctan(\sqrt{2}x+1)]$$ Then I use the fundamental theorem of calculus to evaluate the integral. It turns out that the result is really ugly and I have no idea how to hanled it. Is there any tricks to tackle this?","This question already has an answer here : Evaluate $\sum_{n=1}^{\infty }\int_{0}^{\frac{1}{n}}\frac{\sqrt{x}}{1+x^{2}}\,\mathrm{d}x$ (1 answer) Closed last year . I want to find the sum: I start with finding the antiderivative of the integrant, which is: Then I use the fundamental theorem of calculus to evaluate the integral. It turns out that the result is really ugly and I have no idea how to hanled it. Is there any tricks to tackle this?",\sum_{n=1}^{\infty}\int_0^{\frac{1}{\sqrt{n}}}\frac{2x^2}{1+x^4}dx \frac{1}{2\sqrt{2}}[\ln(x^2-\sqrt{2}x+1)-\ln(x^2+\sqrt{2}x+1)+2\arctan(\sqrt{2}x-1)+2\arctan(\sqrt{2}x+1)],"['real-analysis', 'integration', 'sequences-and-series']"
57,"Find all functions $f:\mathbb{R}^+\to \mathbb{R}^+$ such that for all $x,y\in\mathbb{R}^+$, $f(x)f(yf(x))=f(x+y)$","Find all functions  such that for all ,","f:\mathbb{R}^+\to \mathbb{R}^+ x,y\in\mathbb{R}^+ f(x)f(yf(x))=f(x+y)","Find all functions $f:\mathbb{R}^+\to \mathbb{R}^+$ such that for all $x,y\in\mathbb{R}^+$$$f(x)f(yf(x))=f(x+y)$$ A start: set y=0 to get $f(x)f(0)=f(x)$. So $f(0)=1$ unless $f$ is identically zero.","Find all functions $f:\mathbb{R}^+\to \mathbb{R}^+$ such that for all $x,y\in\mathbb{R}^+$$$f(x)f(yf(x))=f(x+y)$$ A start: set y=0 to get $f(x)f(0)=f(x)$. So $f(0)=1$ unless $f$ is identically zero.",,"['real-analysis', 'analysis', 'functions', 'contest-math', 'functional-equations']"
58,Convexity and equality in Jensen's inequality,Convexity and equality in Jensen's inequality,,"The problem: Recall that we saw that when $\mu$ is a probability measure on $X$ and $f$ is integrable with respect to $\mu$, then   $$ \exp\left(\int_X f(x)d\mu(x)\right) \leq \int_X e^{f(x)}d\mu(x).$$   What can you conclude if we have equality? I know the solution is the functions that are constant on $X$ for all but a set of measure 0, but I'm not too sure how to prove this. So far what I've done is to show that a constant function on a set of measure 1 in $X$ gives equality, but I haven't been able to come up with a way to show a function with at least two distinct values on subsets of $X$ of positive measure cannot give equality. Namely: If we have some integrable function (with respect to $\mu$) that is constant on all but a set of measure 0, say $f(x) = c$ for all $x\in Y\subset X$, with $\mu(Y) = 1$, then the left hand side is \begin{align*}  \exp\left(\int_X f(x)d\mu(x)\right) &= \exp\left(c\int_X d\mu(x)\right)\newline                           &= \exp\left(c\int_Y d\mu(x)\right)\newline                           &= \exp(c\mu(Y))\newline                           &= e^c. \end{align*} The right hand side is \begin{align*}  \int_X e^{f(x)}d\mu(x) &= \int_X e^c d\mu(x)\newline                         &= e^c \int_X d\mu(x)\newline                         &= e^c \int_Y d\mu(x)\newline                         &= e^c \mu(Y)\newline                         &= e^c, \end{align*} and so we have $$ \exp\left(\int_X f(x)d\mu(x)\right) = \int_X e^{f(x)}d\mu(x).$$ These are the only possible $f$ for equality to occur. To see this, suppose $f$ were not constant (on all but a set of measure 0). Start with the simplest case, namely, suppose $X = N\cup Y\cup Z$ with $N,Y$ and $Z$ pairwise disjoint, $\mu(Y) > 0 < \mu(Z)$, and $\mu(N) = 0$ such that $f(y) = a$ for all $y\in Y$ and $f(z) = b$ for all $z\in Z$, with $a\neq b$. Then the left hand side is \begin{align*}  \exp\left(\int_X f(x)d\mu(x)\right) &= \exp\left(\int_Y a d\mu(x) + \int_Z b d\mu(x)\right)\newline                           &= \exp\left(a\int_Y d\mu(x) + b\int_Z d\mu(x)\right)\newline                           &= \exp(a\mu(Y) + b\mu(Z)). \end{align*} whereas the right hand side is \begin{align*}  \int_X e^{f(x)}d\mu(x) &= \int_Y e^a d\mu(x) + \int_Z e^b d\mu(x)\newline                         &= e^a \int_Y d\mu(x) + e^b \int_Z d\mu(x)\newline                         &= e^a \mu(Y) + e^b \mu(Z). \end{align*} So...? I'm not sure how to show $a\neq b$ implies $$ \exp(a\mu(Y) + b\mu(Z)) < e^a \mu(Y) + e^b \mu(Z).$$ My guess is it's something simple I'm missing, but I don't really see how to proceed. Edit: I suppose I can add some of the fiddling around I've done. We know $$\mu(Z) = 1 - \mu(Y),$$ so the original equality can be written as $$ \exp(a\mu(Y) + b(1-\mu(Y))) \leq e^a \mu(Y) + e^b (1-\mu(Y))$$ which means $$ \exp((a-b)\mu(Y) + b) \leq (e^a - e^b)\mu(Y) + e^b.$$ Hence as $$ \exp((a-b)\mu(Y) + b) = \exp((a-b)\mu(Y))e^b,$$ we have $$ \exp((a-b)\mu(Y)) \leq (e^{a-b} - 1)\mu(Y) + 1.$$ Edit some more: I suppose one could look at this last inequality as a function of $a-b$. To make things neater, just a function of $\exp(a-b)$, so let $x = \exp(a-b)$. Then we have $$ x^{\mu(Y)} \leq (x - 1)\mu(Y) + 1.$$ Now we know if $a = b$ (i.e., $x = 1$) we have equality, so we can compute the derivative of both sides. The derivative of the left hand side is $$ \mu(Y)x^{\mu(Y) - 1},$$ whereas the derivative of the right hand side is simply $\mu(Y)$. Hence as $x$ increases the functions can never be equal again. I think this works, though I'll sit on it and think a little more about it (and it's generalization to all non-constant functions on $X$)","The problem: Recall that we saw that when $\mu$ is a probability measure on $X$ and $f$ is integrable with respect to $\mu$, then   $$ \exp\left(\int_X f(x)d\mu(x)\right) \leq \int_X e^{f(x)}d\mu(x).$$   What can you conclude if we have equality? I know the solution is the functions that are constant on $X$ for all but a set of measure 0, but I'm not too sure how to prove this. So far what I've done is to show that a constant function on a set of measure 1 in $X$ gives equality, but I haven't been able to come up with a way to show a function with at least two distinct values on subsets of $X$ of positive measure cannot give equality. Namely: If we have some integrable function (with respect to $\mu$) that is constant on all but a set of measure 0, say $f(x) = c$ for all $x\in Y\subset X$, with $\mu(Y) = 1$, then the left hand side is \begin{align*}  \exp\left(\int_X f(x)d\mu(x)\right) &= \exp\left(c\int_X d\mu(x)\right)\newline                           &= \exp\left(c\int_Y d\mu(x)\right)\newline                           &= \exp(c\mu(Y))\newline                           &= e^c. \end{align*} The right hand side is \begin{align*}  \int_X e^{f(x)}d\mu(x) &= \int_X e^c d\mu(x)\newline                         &= e^c \int_X d\mu(x)\newline                         &= e^c \int_Y d\mu(x)\newline                         &= e^c \mu(Y)\newline                         &= e^c, \end{align*} and so we have $$ \exp\left(\int_X f(x)d\mu(x)\right) = \int_X e^{f(x)}d\mu(x).$$ These are the only possible $f$ for equality to occur. To see this, suppose $f$ were not constant (on all but a set of measure 0). Start with the simplest case, namely, suppose $X = N\cup Y\cup Z$ with $N,Y$ and $Z$ pairwise disjoint, $\mu(Y) > 0 < \mu(Z)$, and $\mu(N) = 0$ such that $f(y) = a$ for all $y\in Y$ and $f(z) = b$ for all $z\in Z$, with $a\neq b$. Then the left hand side is \begin{align*}  \exp\left(\int_X f(x)d\mu(x)\right) &= \exp\left(\int_Y a d\mu(x) + \int_Z b d\mu(x)\right)\newline                           &= \exp\left(a\int_Y d\mu(x) + b\int_Z d\mu(x)\right)\newline                           &= \exp(a\mu(Y) + b\mu(Z)). \end{align*} whereas the right hand side is \begin{align*}  \int_X e^{f(x)}d\mu(x) &= \int_Y e^a d\mu(x) + \int_Z e^b d\mu(x)\newline                         &= e^a \int_Y d\mu(x) + e^b \int_Z d\mu(x)\newline                         &= e^a \mu(Y) + e^b \mu(Z). \end{align*} So...? I'm not sure how to show $a\neq b$ implies $$ \exp(a\mu(Y) + b\mu(Z)) < e^a \mu(Y) + e^b \mu(Z).$$ My guess is it's something simple I'm missing, but I don't really see how to proceed. Edit: I suppose I can add some of the fiddling around I've done. We know $$\mu(Z) = 1 - \mu(Y),$$ so the original equality can be written as $$ \exp(a\mu(Y) + b(1-\mu(Y))) \leq e^a \mu(Y) + e^b (1-\mu(Y))$$ which means $$ \exp((a-b)\mu(Y) + b) \leq (e^a - e^b)\mu(Y) + e^b.$$ Hence as $$ \exp((a-b)\mu(Y) + b) = \exp((a-b)\mu(Y))e^b,$$ we have $$ \exp((a-b)\mu(Y)) \leq (e^{a-b} - 1)\mu(Y) + 1.$$ Edit some more: I suppose one could look at this last inequality as a function of $a-b$. To make things neater, just a function of $\exp(a-b)$, so let $x = \exp(a-b)$. Then we have $$ x^{\mu(Y)} \leq (x - 1)\mu(Y) + 1.$$ Now we know if $a = b$ (i.e., $x = 1$) we have equality, so we can compute the derivative of both sides. The derivative of the left hand side is $$ \mu(Y)x^{\mu(Y) - 1},$$ whereas the derivative of the right hand side is simply $\mu(Y)$. Hence as $x$ increases the functions can never be equal again. I think this works, though I'll sit on it and think a little more about it (and it's generalization to all non-constant functions on $X$)",,"['real-analysis', 'convex-analysis']"
59,Proof of Clairaut’s theorem in Terence Tao Analysis 2,Proof of Clairaut’s theorem in Terence Tao Analysis 2,,"I'm self-studying the book Real Analysis 2 by Terence Tao. I'm having some trouble working with vector-valued functions. Particularly, I'm stuck at his proof of Clairaut's theorem for a vector-valued function while having no math professor to ask :( screenshot 1 Theorem 6.5.4 (Clairaut's theorem). Let $E$ be an open subset of $\mathbf{R}^{n}$ , and let $f: E \rightarrow \mathbf{R}^{m}$ be a twice continuously differentiable function on E. Then we have $\frac{\partial}{\partial x_{j}} \frac{\partial f}{\partial x_{i}}\left(x_{0}\right)=\frac{\partial}{\partial x_{i}} \frac{\partial f}{\partial x_{j}}\left(x_{0}\right)$ for all $1 \leq i, j \leq n$ . In his proof, he proves for the case $\fbox{m=1}$ and $x_0 = 0$ . The beginning of the proof is easily to understood: screenshot 2 Let $a$ be the number $a:=\frac{\partial}{\partial x_{j}} \frac{\partial f}{\partial x_{i}}(0),$ and $a^{\prime}$ denote the quantity $a^{\prime}:=\frac{\partial}{\partial x_{i}} \frac{\partial f}{\partial x_{j}}(0) .$ Our task is to show that $a^{\prime}=a$ . Let $\varepsilon>0$ . Because the double derivatives of $f$ are continuous, we can find a $\delta>0$ such that $$ \left|\frac{\partial}{\partial x_{j}} \frac{\partial f}{\partial x_{i}}(x)-a\right| \leq \varepsilon $$ and $$ \left|\frac{\partial}{\partial x_{i}} \frac{\partial f}{\partial x_{j}}(x)-a^{\prime}\right| \leq \varepsilon $$ whenever $|x| \leq 2 \delta$ . Then comes the difficult part that I can't understand his notation: screenshot 3 Now we consider the quantity $$ X:=f\left(\delta e_{i}+\delta e_{j}\right)-f\left(\delta e_{i}\right)-f\left(\delta e_{j}\right)+f(0) $$ From the fundamental theorem of calculus in the $e_{i}$ variable, we have $$ f\left(\delta e_{i}+\delta e_{j}\right)-f\left(\delta e_{j}\right)=\int_{0}^{\delta} \frac{\partial f}{\partial x_{i}}\left(x_{i} e_{i}+\delta e_{j}\right) d x_{i} \qquad\color{red}{(*)}$$ and $$ f\left(\delta e_{i}\right)-f(0)=\int_{0}^{\delta} \frac{\partial f}{\partial x_{i}}\left(x_{i} e_{i}\right) d x_{i} $$ and hence $$ X=\int_{0}^{\delta}\left(\frac{\partial f}{\partial x_{i}}\left(x_{i} e_{i}+\delta e_{j}\right)-\frac{\partial f}{\partial x_{i}}\left(x_{i} e_{i}\right)\right) d x_{i}. $$ Where $e_i$ is a $n-dimensional$ vector with all zeros except the $1$ in the $i^{th}$ position. My question concerns the $\color{red}{(*)}$ equations: How to use the Fundamental theorem of calculus to lead to the (*) equation? Why the limit of the integral is from $0$ $\to$ $\delta$ and NOT from ( $\delta \, e_j$ $\to$ $\delta \, e_i + \delta \, e_j$ ) ? (like in the case of single variable integration: $F(b) - F(a) = \int \limits_{(a,b)} f $ ) ? ( $i.e$ In my opinion, we consider $b = \delta \, . e_i + \delta \, .e_j$ and $a= \delta \, .e_j$ , so the limit of the integration should be from $a$ to $b$ which is different from what is written in the text: $0$ $\to$ $\delta$ ) Why the ""main"" function to integrate is $\frac{\partial f}{\partial x_j}$ ? Why is it evaluated at the point ( $x_i*e_i + \delta * e_j$ )? Why the differential part in the integral is $dx_i$ ? Could you please suggest to me a $\fbox{document}$ or some $\fbox{keywords}$ so that I can read more to further understand the things related to the $\color{red}{(*)}$ equation? In the case where $m > 1$ , does the equation $\color{red}{(*)}$ still hold ? Is there any definition of integration of a vector-valued function? ( $i.e.$ $\int f dx$ where $f : \mathbb{R^n} \to \mathbb{R}^m$ with $m > 1$ and $x$ is a single variable. In other words, does an integration always take scalar- value or it can be a vector) ? Thank you very much for your help!","I'm self-studying the book Real Analysis 2 by Terence Tao. I'm having some trouble working with vector-valued functions. Particularly, I'm stuck at his proof of Clairaut's theorem for a vector-valued function while having no math professor to ask :( screenshot 1 Theorem 6.5.4 (Clairaut's theorem). Let be an open subset of , and let be a twice continuously differentiable function on E. Then we have for all . In his proof, he proves for the case and . The beginning of the proof is easily to understood: screenshot 2 Let be the number and denote the quantity Our task is to show that . Let . Because the double derivatives of are continuous, we can find a such that and whenever . Then comes the difficult part that I can't understand his notation: screenshot 3 Now we consider the quantity From the fundamental theorem of calculus in the variable, we have and and hence Where is a vector with all zeros except the in the position. My question concerns the equations: How to use the Fundamental theorem of calculus to lead to the (*) equation? Why the limit of the integral is from and NOT from ( ) ? (like in the case of single variable integration: ) ? ( In my opinion, we consider and , so the limit of the integration should be from to which is different from what is written in the text: ) Why the ""main"" function to integrate is ? Why is it evaluated at the point ( )? Why the differential part in the integral is ? Could you please suggest to me a or some so that I can read more to further understand the things related to the equation? In the case where , does the equation still hold ? Is there any definition of integration of a vector-valued function? ( where with and is a single variable. In other words, does an integration always take scalar- value or it can be a vector) ? Thank you very much for your help!","E \mathbf{R}^{n} f: E \rightarrow \mathbf{R}^{m} \frac{\partial}{\partial x_{j}} \frac{\partial f}{\partial x_{i}}\left(x_{0}\right)=\frac{\partial}{\partial x_{i}} \frac{\partial f}{\partial x_{j}}\left(x_{0}\right) 1 \leq i, j \leq n \fbox{m=1} x_0 = 0 a a:=\frac{\partial}{\partial x_{j}} \frac{\partial f}{\partial x_{i}}(0), a^{\prime} a^{\prime}:=\frac{\partial}{\partial x_{i}} \frac{\partial f}{\partial x_{j}}(0) . a^{\prime}=a \varepsilon>0 f \delta>0 
\left|\frac{\partial}{\partial x_{j}} \frac{\partial f}{\partial x_{i}}(x)-a\right| \leq \varepsilon
 
\left|\frac{\partial}{\partial x_{i}} \frac{\partial f}{\partial x_{j}}(x)-a^{\prime}\right| \leq \varepsilon
 |x| \leq 2 \delta 
X:=f\left(\delta e_{i}+\delta e_{j}\right)-f\left(\delta e_{i}\right)-f\left(\delta e_{j}\right)+f(0)
 e_{i} 
f\left(\delta e_{i}+\delta e_{j}\right)-f\left(\delta e_{j}\right)=\int_{0}^{\delta} \frac{\partial f}{\partial x_{i}}\left(x_{i} e_{i}+\delta e_{j}\right) d x_{i}
\qquad\color{red}{(*)} 
f\left(\delta e_{i}\right)-f(0)=\int_{0}^{\delta} \frac{\partial f}{\partial x_{i}}\left(x_{i} e_{i}\right) d x_{i}
 
X=\int_{0}^{\delta}\left(\frac{\partial f}{\partial x_{i}}\left(x_{i} e_{i}+\delta e_{j}\right)-\frac{\partial f}{\partial x_{i}}\left(x_{i} e_{i}\right)\right) d x_{i}.
 e_i n-dimensional 1 i^{th} \color{red}{(*)} 0 \to \delta \delta \, e_j \to \delta \, e_i + \delta \, e_j F(b) - F(a) = \int \limits_{(a,b)} f  i.e b = \delta \, . e_i + \delta \, .e_j a= \delta \, .e_j a b 0 \to \delta \frac{\partial f}{\partial x_j} x_i*e_i + \delta * e_j dx_i \fbox{document} \fbox{keywords} \color{red}{(*)} m > 1 \color{red}{(*)} i.e. \int f dx f : \mathbb{R^n} \to \mathbb{R}^m m > 1 x","['real-analysis', 'integration', 'multivariable-calculus', 'vector-analysis']"
60,What is the function $f(x)$ which is differentiable everywhere and $f(x-1)f(x-2)+1=f(x)$?,What is the function  which is differentiable everywhere and ?,f(x) f(x-1)f(x-2)+1=f(x),"What is the function $f(x)$ which is differentiable everywhere and $f(x-1)f(x-2)+1=f(x)$ and $f(1)=f(2)=1$ ? I've been wondering about this problem for about $1 \frac{1}{2}$ years. I don't know the tools to solve this problem. So, if you could show me how to find a solution, I would like that. I found the values of $f(x)$ from $0$ to $10$ : $$0,1,1,2,3,7,22,155,3411,528706,1803416167,\dots,f(n)$$ I realized that at $f(-1)$ can't be found just using $f(x-1)f(x-2)+1=f(x)$ because $n\times f(0)=f(1)$ has infinite solutions. $f(-1)$ can't be zero because then $f(-2)$ wouldn't be defined because $n\times 0=-1$ . So maybe if I add $f(x)$ must differentiable everywhere I could get an answer.","What is the function which is differentiable everywhere and and ? I've been wondering about this problem for about years. I don't know the tools to solve this problem. So, if you could show me how to find a solution, I would like that. I found the values of from to : I realized that at can't be found just using because has infinite solutions. can't be zero because then wouldn't be defined because . So maybe if I add must differentiable everywhere I could get an answer.","f(x) f(x-1)f(x-2)+1=f(x) f(1)=f(2)=1 1 \frac{1}{2} f(x) 0 10 0,1,1,2,3,7,22,155,3411,528706,1803416167,\dots,f(n) f(-1) f(x-1)f(x-2)+1=f(x) n\times f(0)=f(1) f(-1) f(-2) n\times 0=-1 f(x)",['real-analysis']
61,"Prove that there exists $c\in[0,1]$ such that $\int_0^cf(t)dt=f(c)^3.$",Prove that there exists  such that,"c\in[0,1] \int_0^cf(t)dt=f(c)^3.","Question: Let $f:[0,1]\to\mathbb{R}$ be a continuous function with $\int_0^1f(t)dt=0$ . Prove that there exists $c\in[0,1]$ such that $$\int_0^cf(t)dt=f(c)^3.$$ Solution: Let $g:[0,1]\to\mathbb{R}$ be such that $$g(x)=\int_0^xf(t)dt-f(x)^3, \forall x\in[0,1].$$ Now since $f$ is continuous $\forall x\in[0,1]$ , thus, by the first fundamental theorem of calculus, we can conclude that $g$ is continuous $\forall x\in[0,1]$ . Thereafter, observe that $g(x)=0$ for some $x\in[0,1]\iff \int_0^xf(t)dt=f(x)^3$ for some $x\in[0,1]$ . Hence, to prove the statement of the problem it is sufficient to show that $g(c)=0$ for some $c\in[0,1]$ . Now $g(0)=-f(0)^3$ and $g(1)=-f(1)^3$ . Observe that if $f(0)$ and $f(1)$ are of different signs, then $g(0)$ and $g(1)$ are also of different signs, in which case, by IVT we can conclude that $\exists c\in(0,1)\subset[0,1],$ such that $g(c)=0$ . Hence, we are done in this case. Again, if $f(0)=0$ or $f(1)=0$ , then at least one of $g(0)$ and $g(1)=0$ , in which case we are done. Now, we are left with the case that both $f(0)$ and $f(1)$ are of the same sign. Thus, let us assume WLOG that $f(0)>0$ and $f(1)>0$ . Hence, $g(0)<0$ and $g(1)<0$ . Now since $\int_0^1f(t)dt=0$ and $f(0),f(1)>0$ , implies that $\exists$ at least two points $a,b\in(0,1)$ , such that $b>a$ satisfying $f(a)=f(b)=0$ . Thus, we can conclude that $\exists c_1\in(0,1),$ such that $f(x)>0, \forall x\in[0,c_1)$ and $f(c_1)=0$ . Hence, we have $$g(c_1)=\int_0^{c_1}f(t)dt-f(c_1)^3=\int_0^{c_1}f(t)dt>0.$$ Thus, we have $g(c_1)>0$ and $g(1)<0$ , which implies that, by IVT we can conclude that $\exists c\in(c_1,1)\subset[0,1]$ , such that $g(c)=0$ . Hence, we are done in this case too. Hence, we are done with all the cases and in each case we have shown that $\exists c\in[0,1]$ such that $g(c)=0$ . Thus, we are done. Is this solution correct and rigorous enough? If yes, is there any alternative solution?","Question: Let be a continuous function with . Prove that there exists such that Solution: Let be such that Now since is continuous , thus, by the first fundamental theorem of calculus, we can conclude that is continuous . Thereafter, observe that for some for some . Hence, to prove the statement of the problem it is sufficient to show that for some . Now and . Observe that if and are of different signs, then and are also of different signs, in which case, by IVT we can conclude that such that . Hence, we are done in this case. Again, if or , then at least one of and , in which case we are done. Now, we are left with the case that both and are of the same sign. Thus, let us assume WLOG that and . Hence, and . Now since and , implies that at least two points , such that satisfying . Thus, we can conclude that such that and . Hence, we have Thus, we have and , which implies that, by IVT we can conclude that , such that . Hence, we are done in this case too. Hence, we are done with all the cases and in each case we have shown that such that . Thus, we are done. Is this solution correct and rigorous enough? If yes, is there any alternative solution?","f:[0,1]\to\mathbb{R} \int_0^1f(t)dt=0 c\in[0,1] \int_0^cf(t)dt=f(c)^3. g:[0,1]\to\mathbb{R} g(x)=\int_0^xf(t)dt-f(x)^3, \forall x\in[0,1]. f \forall x\in[0,1] g \forall x\in[0,1] g(x)=0 x\in[0,1]\iff \int_0^xf(t)dt=f(x)^3 x\in[0,1] g(c)=0 c\in[0,1] g(0)=-f(0)^3 g(1)=-f(1)^3 f(0) f(1) g(0) g(1) \exists c\in(0,1)\subset[0,1], g(c)=0 f(0)=0 f(1)=0 g(0) g(1)=0 f(0) f(1) f(0)>0 f(1)>0 g(0)<0 g(1)<0 \int_0^1f(t)dt=0 f(0),f(1)>0 \exists a,b\in(0,1) b>a f(a)=f(b)=0 \exists c_1\in(0,1), f(x)>0, \forall x\in[0,c_1) f(c_1)=0 g(c_1)=\int_0^{c_1}f(t)dt-f(c_1)^3=\int_0^{c_1}f(t)dt>0. g(c_1)>0 g(1)<0 \exists c\in(c_1,1)\subset[0,1] g(c)=0 \exists c\in[0,1] g(c)=0","['real-analysis', 'solution-verification']"
62,Is a germ equivalent to an infinite jet?,Is a germ equivalent to an infinite jet?,,"Not all smooth functions are analytic, as it is well known, so they in general cannot be represented as a power series. If we restrict our attention to analytic functions, then a specification of the values of all derivatives of a function at a point will give us the function. My question is essentially about how much information is contained in knowing all derivatives of a smooth but not necessarily analytic function at a point. In particular, let $(E, \pi, M) $ be a fibred manifold and let $\Gamma_x(E) $ denote the space of germs of smooth sections at $x\in M$ . A germ contains information about the section in an arbitrarily small open neighborhood of $x$ . However, practically thinking, the only definite values I can associate to a germ is the value of a representative section at $x$ , and the values of its derivatives at the same point to arbitrary orders. On the other hand, points of the infinite jet space $J_x^\infty(E) $ literally consist of values of a section and derivatives up to all orders at $x$ . Nonetheless I feel that the germ space might contain more ""nonlocal"" information than the infinite jet space. So the question is, how are $J_x^\infty(E) $ and $\Gamma_x(E) $ related?","Not all smooth functions are analytic, as it is well known, so they in general cannot be represented as a power series. If we restrict our attention to analytic functions, then a specification of the values of all derivatives of a function at a point will give us the function. My question is essentially about how much information is contained in knowing all derivatives of a smooth but not necessarily analytic function at a point. In particular, let be a fibred manifold and let denote the space of germs of smooth sections at . A germ contains information about the section in an arbitrarily small open neighborhood of . However, practically thinking, the only definite values I can associate to a germ is the value of a representative section at , and the values of its derivatives at the same point to arbitrary orders. On the other hand, points of the infinite jet space literally consist of values of a section and derivatives up to all orders at . Nonetheless I feel that the germ space might contain more ""nonlocal"" information than the infinite jet space. So the question is, how are and related?","(E, \pi, M)  \Gamma_x(E)  x\in M x x J_x^\infty(E)  x J_x^\infty(E)  \Gamma_x(E) ","['real-analysis', 'analysis', 'differential-geometry', 'germs', 'jet-bundles']"
63,"If $f$ is continuous and $f'(x)\ge 0$, outside of a countable set, then $f$ is increasing","If  is continuous and , outside of a countable set, then  is increasing",f f'(x)\ge 0 f,"PROBLEM. Let $f:[a,b]\to\mathbb R$ be a continuous function, such that $f'(x)\ge 0$ , for all $x\in [a,b]\setminus A$ , where $A\subset [a,b]$ is a countable set. Show that $f$ is increasing. Attention. In this problem, we DO NOT assume that $f$ is differentiable in the whole $[a,b]$ . Notes. (1) If we assume that $f$ is differentiable in the whole interval, then we can easily show that $f'(x)\ge 0$ , everywhere. For otherwise, if $f'(x_0)=c<0$ , for some $x_0\in [a,b]$ , then by virtue of Darboux's Theorem , $(c,0)\subset f'([a,b])$ , and hence, $f'(x)<0$ , for uncountably many $x$ 's. (2) The conclusion of the problem does not hold if we replace the assumption $A$ is countable with $A$ is a set of measure zero . Take for example the Devil's staircase , with a negative sign in front. (3) If the hypothesis $f'(x)\ge 0$ , is replaced by $f'(x)=0$ , then the conclusion becomes f is constant .","PROBLEM. Let be a continuous function, such that , for all , where is a countable set. Show that is increasing. Attention. In this problem, we DO NOT assume that is differentiable in the whole . Notes. (1) If we assume that is differentiable in the whole interval, then we can easily show that , everywhere. For otherwise, if , for some , then by virtue of Darboux's Theorem , , and hence, , for uncountably many 's. (2) The conclusion of the problem does not hold if we replace the assumption is countable with is a set of measure zero . Take for example the Devil's staircase , with a negative sign in front. (3) If the hypothesis , is replaced by , then the conclusion becomes f is constant .","f:[a,b]\to\mathbb R f'(x)\ge 0 x\in [a,b]\setminus A A\subset [a,b] f f [a,b] f f'(x)\ge 0 f'(x_0)=c<0 x_0\in [a,b] (c,0)\subset f'([a,b]) f'(x)<0 x A A f'(x)\ge 0 f'(x)=0","['real-analysis', 'calculus', 'derivatives', 'continuity', 'monotone-functions']"
64,"Proving $f : (-1, 1) \rightarrow \mathbb{R}$ has a property",Proving  has a property,"f : (-1, 1) \rightarrow \mathbb{R}","I'd like to answer the two questions below. I believe my solution to the first one is right, but it'd help if someone could please help verify this, as I am working through this book independently through self-study. I don't know how to do the second one, which is a continuation of the first problem. I'm pretty sure both questions utilize a Theorem stated in my book, so I've provided that as well. Suppose $f : (-1, 1) \rightarrow \mathbb{R}$ has $n$ derivatives and its $n^{\text{th}}$ , suppose its $n^{\text{th}}$ derivative $f^{(n)} : (-1, 1) \rightarrow \mathbb{R}$ is bounded. Finally, suppose we have $$f(0) = f'(0) = f''(0) = \cdots f^{(n - 1)}(0) = 0.$$ Prove there exists a positive number $M$ such that $$|f(x)| \leq M|x|^{n}.$$ Second question: Suppose $f : (-1, 1) \rightarrow \mathbb{R}$ has $n$ derivatives and there is a positive number $M$ such that $$|f(x)| \leq M|x|^{n}.$$ Prove $$f(0) = f'(0) =  \cdots f^{(n - 1)}(0) = 0.$$ Also, here's the theorem I was referring to: Theorem: Let $I$ be an open interval and $n$ be a natural number and suppose that the function $f : I \rightarrow \mathbb{R}$ has $n$ derivatives.  Suppose also at the point $x_{0}$ in $I$ , $$f^{(k)}(x_{0}) = 0$$ for $0 \leq k \leq n - 1$ . Then, for each point $x \neq x_{0}$ in $I$ , there is a point $z$ strictly between $x$ and $x_{0}$ at which $$f(x) = \frac{f^{(n)}(z)}{n!}(x - x_{0})^{n}.$$ Here's my attempt at problem $1$ : By the Theorem above, for each $x \in (-1, 1)$ , $x \neq 0$ , there's a point $z$ between $x$ and $0$ such that $$f(x) = \frac{f^{(n)}(z)}{n!}x^{n}.$$ Using the fact that $f^{(n)}$ is bounded, we know there exists a bound $N$ such that $|f^{(n)}(x)| \leq N$ for all $x$ in $(-1, 1)$ .  Therefore, $$|f(x)| = \left|\frac{f^{(n)}(z)}{n!}x^{n}\right| \leq |\frac{N}{n!}x^{n}| = (N/n!)|x|^{n} $$ So setting $M = N/n!$ suffices. Is this correct? How do I do the next one?","I'd like to answer the two questions below. I believe my solution to the first one is right, but it'd help if someone could please help verify this, as I am working through this book independently through self-study. I don't know how to do the second one, which is a continuation of the first problem. I'm pretty sure both questions utilize a Theorem stated in my book, so I've provided that as well. Suppose has derivatives and its , suppose its derivative is bounded. Finally, suppose we have Prove there exists a positive number such that Second question: Suppose has derivatives and there is a positive number such that Prove Also, here's the theorem I was referring to: Theorem: Let be an open interval and be a natural number and suppose that the function has derivatives.  Suppose also at the point in , for . Then, for each point in , there is a point strictly between and at which Here's my attempt at problem : By the Theorem above, for each , , there's a point between and such that Using the fact that is bounded, we know there exists a bound such that for all in .  Therefore, So setting suffices. Is this correct? How do I do the next one?","f : (-1, 1) \rightarrow \mathbb{R} n n^{\text{th}} n^{\text{th}} f^{(n)} : (-1, 1) \rightarrow \mathbb{R} f(0) = f'(0) = f''(0) = \cdots f^{(n - 1)}(0) = 0. M |f(x)| \leq M|x|^{n}. f : (-1, 1) \rightarrow \mathbb{R} n M |f(x)| \leq M|x|^{n}. f(0) = f'(0) =  \cdots f^{(n - 1)}(0) = 0. I n f : I \rightarrow \mathbb{R} n x_{0} I f^{(k)}(x_{0}) = 0 0 \leq k \leq n - 1 x \neq x_{0} I z x x_{0} f(x) = \frac{f^{(n)}(z)}{n!}(x - x_{0})^{n}. 1 x \in (-1, 1) x \neq 0 z x 0 f(x) = \frac{f^{(n)}(z)}{n!}x^{n}. f^{(n)} N |f^{(n)}(x)| \leq N x (-1, 1) |f(x)| = \left|\frac{f^{(n)}(z)}{n!}x^{n}\right| \leq |\frac{N}{n!}x^{n}| = (N/n!)|x|^{n}  M = N/n!",['real-analysis']
65,"Approximating smooth function on $[0,1]$ by Bernstein polynomial (interested in approximation rate in $L^2$ norm)",Approximating smooth function on  by Bernstein polynomial (interested in approximation rate in  norm),"[0,1] L^2","Consider a smooth function $f$ on $[0,1]$ and its Bernstein polynomial of power $n$:  $$B_n(f)=\sum_{k=0}^n f\left(\frac{k}{n}\right) b_{n,k}(x)$$ with $$b_{n,k}(x) = \binom{n}{k}x^k (1-x)^{n-k}.$$ It is well known that  $$\sup_{x \in[0,1]} |B_n(f)(x)-f(x)|=O\left(\frac{1}{\sqrt{n}}\right).$$ But what if one considers the $L^2$ norm instead of $L^{\infty}$ norm? Will the approximation power be better in $L^2$ norm? In particular, is it possible to conclude that  $$\left(\int_{0}^1 (B_n(f)(x)-f(x))^2 dx \right)^{\frac{1}{2}}=O\left(\frac{1}{n}\right)?$$","Consider a smooth function $f$ on $[0,1]$ and its Bernstein polynomial of power $n$:  $$B_n(f)=\sum_{k=0}^n f\left(\frac{k}{n}\right) b_{n,k}(x)$$ with $$b_{n,k}(x) = \binom{n}{k}x^k (1-x)^{n-k}.$$ It is well known that  $$\sup_{x \in[0,1]} |B_n(f)(x)-f(x)|=O\left(\frac{1}{\sqrt{n}}\right).$$ But what if one considers the $L^2$ norm instead of $L^{\infty}$ norm? Will the approximation power be better in $L^2$ norm? In particular, is it possible to conclude that  $$\left(\int_{0}^1 (B_n(f)(x)-f(x))^2 dx \right)^{\frac{1}{2}}=O\left(\frac{1}{n}\right)?$$",,"['real-analysis', 'functions', 'polynomials', 'reference-request', 'approximation-theory']"
66,Do any authors take the sheaf-theoretic viewpoint on multivalued functions and/or indefinite integrals?,Do any authors take the sheaf-theoretic viewpoint on multivalued functions and/or indefinite integrals?,,"It seems to me that multivalued functions and/or indefinite integrals can be thought of as sheaves. For example: The real square-root function can be viewed as the sheaf $\mathcal{F}$ defined on the open sets of $\mathbb{R}$ by letting $\mathcal{F}(U)$ denote the set of all continuous function $f : U \rightarrow \mathbb{R}$ satisfying $\forall x \in U(f(x)^2 = x).$ The complex square-root function can be viewed as the sheaf $\mathcal{F}$ defined on the open sets of $\mathbb{C}$ by letting $\mathcal{F}(U)$ denote the set of all continuous function $f : U \rightarrow \mathbb{C}$ satisfying $\forall z \in U(f(z)^2 = z).$ The complex logarithm function can be viewed as the sheaf $\mathcal{F}$ defined on the open sets of $\mathbb{C}$ by letting $\mathcal{F}(U)$ denote the set of all continuous function $f : U \rightarrow \mathbb{C}$ satisfying $\forall z \in U(e^{f(z)} = z).$ If $f$ is a continuous real-valued function, we can define $\int f(x) dx$ to be the sheaf $\mathcal{F}$ such that $\mathcal{F}(U)$ is the set of all differentiable functions $F$ on $U$ satifying $F'=f$. More generally, if $\mathfrak{f}$ is a sheaf of continuous functions, we can define $\int \mathfrak{f}(x)dx$ to be the sheaf $\mathcal{F}$ such that $\mathcal{F}(U)$ is the set of all differentiable function $F : U \rightarrow \mathbb{R}$ satisfying $F' \in \mathfrak{f}(U).$ Similar statements to the above probably hold for the complex case, allowing us to prove claims like $$\log(z) \subseteq \int \frac{1}{z}dz,$$ etc. where $\log$ is viewed as the sheaf-theoretic inverse of $z \mapsto e^z$ as described above, and the $\subseteq$ in this context really means something like: for all open $U \subseteq \mathbb{C}$, we have $$(z \mapsto \log(z))(U) \subseteq \left(z \mapsto \int \frac{1}{z}dz\right)(U).$$ Question. Do any published books or articles take this point of view?","It seems to me that multivalued functions and/or indefinite integrals can be thought of as sheaves. For example: The real square-root function can be viewed as the sheaf $\mathcal{F}$ defined on the open sets of $\mathbb{R}$ by letting $\mathcal{F}(U)$ denote the set of all continuous function $f : U \rightarrow \mathbb{R}$ satisfying $\forall x \in U(f(x)^2 = x).$ The complex square-root function can be viewed as the sheaf $\mathcal{F}$ defined on the open sets of $\mathbb{C}$ by letting $\mathcal{F}(U)$ denote the set of all continuous function $f : U \rightarrow \mathbb{C}$ satisfying $\forall z \in U(f(z)^2 = z).$ The complex logarithm function can be viewed as the sheaf $\mathcal{F}$ defined on the open sets of $\mathbb{C}$ by letting $\mathcal{F}(U)$ denote the set of all continuous function $f : U \rightarrow \mathbb{C}$ satisfying $\forall z \in U(e^{f(z)} = z).$ If $f$ is a continuous real-valued function, we can define $\int f(x) dx$ to be the sheaf $\mathcal{F}$ such that $\mathcal{F}(U)$ is the set of all differentiable functions $F$ on $U$ satifying $F'=f$. More generally, if $\mathfrak{f}$ is a sheaf of continuous functions, we can define $\int \mathfrak{f}(x)dx$ to be the sheaf $\mathcal{F}$ such that $\mathcal{F}(U)$ is the set of all differentiable function $F : U \rightarrow \mathbb{R}$ satisfying $F' \in \mathfrak{f}(U).$ Similar statements to the above probably hold for the complex case, allowing us to prove claims like $$\log(z) \subseteq \int \frac{1}{z}dz,$$ etc. where $\log$ is viewed as the sheaf-theoretic inverse of $z \mapsto e^z$ as described above, and the $\subseteq$ in this context really means something like: for all open $U \subseteq \mathbb{C}$, we have $$(z \mapsto \log(z))(U) \subseteq \left(z \mapsto \int \frac{1}{z}dz\right)(U).$$ Question. Do any published books or articles take this point of view?",,"['calculus', 'real-analysis', 'complex-analysis', 'sheaf-theory', 'multivalued-functions']"
67,Is there a variant of the implicit function theorem covering a branch of a curve around a singular point?,Is there a variant of the implicit function theorem covering a branch of a curve around a singular point?,,"I am looking for a variant of the Implicit function theorem such that it guarantees the analyticity of the implicitly defined function, and works for a single branch of a curve around a singular point? What kind of such variants of the IFT are out there? Hopefully clarifying background. I was teaching a short course on series and their applications to sophomores last fall. In their last set of homework problems I had a question about the Folium of Descartes $$x^3+y^3=3xy.$$ I asked the students to find two first non-zero terms of the Taylor series of the branch $y=y(x)$ that obviously has a local minimum at $x=0$ (shown in thick red in the image below). My goals with this exercise were modest. Basically I wanted the students to do a minimum amount of work with the arithmetic of formal power series. I didn't ask them anything about convergence this time. We had not covered any tools for that (they were scheduled to take a first course in complex analysis only later). My crude thinking. The folium is a genus zero algebraic curve, and has the rational parametrization (coming from blowing up the singularity with $y=tx$) $$x=\frac{3t}{1+t^3},\qquad y=\frac{3t^2}{1+t^3}.$$ The branch of interest is gotten by restricting $t$ to a suitable neigborhood of $t=0$ (when producing that image I used $t\in[-1/2,1/2]$). When viewed as complex variables, we can invert the $x=x(t)$ to a function $t=t(x)$ that is holomorphic in a neighborhood of $x=0$. Therefore it is represented by a convergent Taylor series. Consequently so is $y=tx$. But. This depends heavily on our ability to blow up the singularity allowing us to isolate the chosen branch. I am too ignorant to know for sure, but I suspect that we may need some structures from algebraic geometry to be able to even define a branch around the singular point in such a way that the IFT can be applied. My solution (if you can call it that) also depended on the genus. Having a rational parametrization made it easier. I'm aware of the existence of analytic parametrizations of some curves (like the formal group law parameter of an elliptic curve), but I'm ignorant about the more general results. So the main question may have evolved to. (feel free to discuss your favorite variants) Do we need the singularity to come from an algebraic equation to be able to discuss the branches, and study the analyticity of an individual branch? Is there a variant of the IFT that guarantees the analyticity of a single branch? Extra credit for students. (in case some students read my question up to this point, and want to work on something related) Show that in a solution $y=\sum_{n=0}^\infty a_n x^n$ to the equation $x^3+y^3=3xy$ only every third term can be non-zero. More precisely, show that if $a_n\neq0$ then $n\equiv2\pmod3$. Edit: Thinking about this topic more (particularly in light of Cactus314's answer) it dawned on me that type of algebraic singularity makes a huge difference. When we work around a multiple point, with distinct tangents for each branch, we can, like in the case of the Folium, work with any branch as long as the corresponding tangent isn't vertical. But, when we have a cusp things can go wrong. Consider the textbook cusp that the curve $y^2=x^3$ has at the origin. No amount of resolving of the singularity is going to make either $y=x^{3/2}$ or $y=-x^{3/2}$ analytic! So my updated guess is that for this to work we need to able to isolate branch such that $x-x_0$ becomes a local parameter along that branch. Can we formulate this in a non-algebraic way?","I am looking for a variant of the Implicit function theorem such that it guarantees the analyticity of the implicitly defined function, and works for a single branch of a curve around a singular point? What kind of such variants of the IFT are out there? Hopefully clarifying background. I was teaching a short course on series and their applications to sophomores last fall. In their last set of homework problems I had a question about the Folium of Descartes $$x^3+y^3=3xy.$$ I asked the students to find two first non-zero terms of the Taylor series of the branch $y=y(x)$ that obviously has a local minimum at $x=0$ (shown in thick red in the image below). My goals with this exercise were modest. Basically I wanted the students to do a minimum amount of work with the arithmetic of formal power series. I didn't ask them anything about convergence this time. We had not covered any tools for that (they were scheduled to take a first course in complex analysis only later). My crude thinking. The folium is a genus zero algebraic curve, and has the rational parametrization (coming from blowing up the singularity with $y=tx$) $$x=\frac{3t}{1+t^3},\qquad y=\frac{3t^2}{1+t^3}.$$ The branch of interest is gotten by restricting $t$ to a suitable neigborhood of $t=0$ (when producing that image I used $t\in[-1/2,1/2]$). When viewed as complex variables, we can invert the $x=x(t)$ to a function $t=t(x)$ that is holomorphic in a neighborhood of $x=0$. Therefore it is represented by a convergent Taylor series. Consequently so is $y=tx$. But. This depends heavily on our ability to blow up the singularity allowing us to isolate the chosen branch. I am too ignorant to know for sure, but I suspect that we may need some structures from algebraic geometry to be able to even define a branch around the singular point in such a way that the IFT can be applied. My solution (if you can call it that) also depended on the genus. Having a rational parametrization made it easier. I'm aware of the existence of analytic parametrizations of some curves (like the formal group law parameter of an elliptic curve), but I'm ignorant about the more general results. So the main question may have evolved to. (feel free to discuss your favorite variants) Do we need the singularity to come from an algebraic equation to be able to discuss the branches, and study the analyticity of an individual branch? Is there a variant of the IFT that guarantees the analyticity of a single branch? Extra credit for students. (in case some students read my question up to this point, and want to work on something related) Show that in a solution $y=\sum_{n=0}^\infty a_n x^n$ to the equation $x^3+y^3=3xy$ only every third term can be non-zero. More precisely, show that if $a_n\neq0$ then $n\equiv2\pmod3$. Edit: Thinking about this topic more (particularly in light of Cactus314's answer) it dawned on me that type of algebraic singularity makes a huge difference. When we work around a multiple point, with distinct tangents for each branch, we can, like in the case of the Folium, work with any branch as long as the corresponding tangent isn't vertical. But, when we have a cusp things can go wrong. Consider the textbook cusp that the curve $y^2=x^3$ has at the origin. No amount of resolving of the singularity is going to make either $y=x^{3/2}$ or $y=-x^{3/2}$ analytic! So my updated guess is that for this to work we need to able to isolate branch such that $x-x_0$ becomes a local parameter along that branch. Can we formulate this in a non-algebraic way?",,"['real-analysis', 'sequences-and-series', 'complex-analysis', 'algebraic-geometry', 'implicit-function-theorem']"
68,"Prob. 23, Chap. 4, in Baby Rudin: Every convex function is continuous and every increasing convex function of a convex function is convex","Prob. 23, Chap. 4, in Baby Rudin: Every convex function is continuous and every increasing convex function of a convex function is convex",,"Here is Prob. 23, Chap. 4, in the book Principles of Mathematical Analysis by Walter Rudin, 3rd edition: A real-valued function $f$ defined in $(a, b)$ is said to be convex if $$ f \big( \lambda x + (1- \lambda) y \big) \leq \lambda f(x) + (1-\lambda) f(y)$$ whenever $a < x < b$ , $a < y < b$ , $0 < \lambda < 1$ . Prove that every convex function is continuous. [Although this proof is available at Math SE, I would prefer a direct, $\varepsilon$ - $\delta$ proof.] Prove that every increasing convex function of a convex function is convex. [ How to?] (For example, if $f$ is convex, so is $e^f$ .) If $f$ is convex in $(a, b)$ and if $a < s < t < u < b$ , show that $$ \frac{ f(t)-f(s)}{t-s} \leq \frac{ f(u)-f(s)}{u-s} \leq \frac{ f(u)-f(t)}{u-t}.$$ [ How to? ] I would appreciate if the proofs are elementary (but rigorous enough for Rudin) and are only based on the machinary developed by Rudin up to this point in the book. Is every real convex function $f$ defined in $(a, b)$ also uniformly continuous? An afterthought: Here's my attempt: Let $x$ , $y$ , and $z$ be any three real numbers such that $$x < y < z.$$ Then we have $$0 < y-x < z-x,$$ which implies that $$0< \frac{ y-x}{z-x} < 1;$$ moreover if we put $$\lambda \colon= \frac{y-x}{z-x},$$ then we see that $$ \begin{align} (1-\lambda)x+\lambda z &= \left( 1- \frac{y-x}{z-x} \right) x + \frac{y-x}{z-x} z \\ &= \frac{(z-y)x+(y-x)z}{z-x} \\ &= y. \end{align} $$ Thus we have shown that for any three real numbers $x$ , $y$ , and $z$ such that $x < y < z$ , we can write $y$ as $$y = (1-\lambda)x + \lambda z, \ \mbox{ where } \ \lambda = \frac{y-x}{z-x} \ \mbox{ and } \ 0 < \lambda < 1. \ \tag{0}$$ In what follows, we will be using this fact on several occasions. First, we show that $f$ is bounded on every closed interval $[ c, d] \subset (a, b)$ . Now let $t \in (c, d)$ , where $c$ and $d$ are any two real numbers such that $a < c < d < b$ . Then we can write $t$ as $$ t = (1- \lambda) c + \lambda d, \ \mbox{ where } \ \lambda = \frac{t-c}{d-c} \in (0, 1).$$ Then $$  \begin{align} f(t) &= f \left( (1-\lambda)c+\lambda d \right) \\ &\leq (1-\lambda) f(c) + \lambda f(d) \\ &\leq (1-\lambda + \lambda ) \max \left\{ f(c), f(d) \right\} \\ &=  \max \left\{ f(c), f(d) \right\}. \end{align} $$ Thus $$f(t) \leq \max \left\{ f(c), f(d) \right\} \ \mbox{ for all } \ t \in [c, d] \ \tag{1}.$$ If $$c < t < \frac{c+d}{2},$$ then we can also conclude that $$t < \frac{c+d}{2} < d, $$ and so $$\frac{c+d}{2} = (1-\lambda) t + \lambda d, \ \mbox{ where } \ \lambda = \frac{\frac{c+d}{2} - t}{d-t} =  \frac{c+d-2t}{2(d-t)} \in (0, 1).$$ Then $$ \begin{align} f\left( \frac{c+d}{2} \right) &= f \left( (1- \lambda) t + \lambda d \right) \\ &\leq (1-\lambda) f(t) + \lambda f(d) \\ &\leq f(t) + f(d), \end{align} $$ which implies that $$f(t) \geq f\left( \frac{c+d}{2} \right) - f(d) \ \mbox{ for all } \ t \in \left(c, \frac{c+d}{2} \right)  \ \tag{2} $$ And, if $$\frac{c+d}{2} < t < d,$$ then we can also conclude that $$c < \frac{c+d}{2} < t,$$ and so $$\frac{c+d}{2} = (1-\lambda) c + \lambda t, \ \mbox{ where } \ \lambda = \frac{\frac{c+d}{2}-c}{t-c} = \frac{d-c}{2(t-c)} \  \mbox{ so that } \ 0 < \lambda < 1.$$ Then $$ \begin{align} f\left( \frac{c+d}{2} \right) &= f\left( (1-\lambda) c + \lambda t\right) \\ &\leq (1-\lambda) f(c) + \lambda f(t) \\ &\leq f(c) + f(t), \end{align} $$ which implies that $$f(t) \geq f\left( \frac{c+d}{2} \right) - f(c) \ \mbox{ for all } \ t \in \left( \frac{c+d}{2}, d \right) \  \tag{3}.$$ From (2) and (3) we can conclude that $$f(t) \geq \min \left\{ \ f(c), \ f(d), \ f\left( \frac{c+d}{2} \right) - f(c), \ f\left( \frac{c+d}{2} \right) - f(d), \   f\left( \frac{c+d}{2} \right) \ \right\} \\  \mbox{ for all } \ t \in [c, d] \ \tag{4}  $$ From (1) and (4) we can comclude that, given any two real numbers $c$ and $d$ which satisfy $a < c < d < b$ , we can find a real number $M > 0$ such that $$ \left\vert f(t) \right\vert \leq M \ \mbox{ for all } \ t \in [c, d]. \ \tag{5} $$ Now let $\eta$ be a real number such that $$0 < \eta < \frac{d-c}{2}, \ \tag{6a} $$ and let $x$ and $y$ be any two real numbers such that $$ c +\eta <  x < y  < d-\eta. \ \tag{6b} $$ Thus, we have the following chain of inequalities: $$c < c+\eta < x < y < d-\eta < d, \ \mbox{ and } \ c+\eta < \frac{c+d}{2} < d-\eta. \ \tag{6} $$ Therefore we can conclude that $$ c < x < y < d, \ \tag{7}$$ and so $$ y = (1-\lambda) x + \lambda d, \ \mbox{ where } \ \lambda = \frac{y-x}{d-x} \ \mbox{ so that } \ 0 < \lambda < 1, $$ and then $$ \begin{align} f(y) - f(x) &= f\left( (1-\lambda) x + \lambda d \right) - f(x) \\ &\leq  (1-\lambda) f(x) + \lambda f(d) - f(x) \\ &= \lambda \left( f(d) - f(x) \right) \\ &\leq \lambda \left| f(d) - f(x) \right| \\ &\leq \lambda \left( \left| f(d) \right| + \left| f(x) \right| \right) \\ &\leq 2\lambda M \ \mbox{ [ by (5) above ] } \\ &= \frac{2M(y-x)}{d-x} \\ &< \frac{2M(y-x)}{\eta}. \ \mbox{ [ by (6b) above ] }  \ \tag{8a} \end{align} $$ And, from (7) we can also write $$x = (1-\lambda) c + \lambda y, \ \mbox{ where } \ \lambda = \frac{x-c}{y-c} \  \mbox{ so that } \ 0 < \lambda < 1, $$ and then $$ \begin{align} f(x) - f(y)  &= f\left( (1-\lambda) c + \lambda y \right) - f(y) \\ &\leq (1-\lambda) f(c) + \lambda f(y) - f(y) \\ &= (1-\lambda) \left( f(c) - f(y) \right) \\ &\leq (1-\lambda)  \left| f(c) - f(y) \right| \\ &\leq (1-\lambda) \left( \left| f(c) \right| + \left| f(y) \right|  \right) \\ &\leq 2(1-\lambda) M \ \mbox{ [ again by (5) above ]} \\ &= \frac{2M(y-x)}{y-c} \\ &< \frac{2M(y-x)}{\eta}. \ \mbox{ [ again by (6b) above ] } \ \tag{8b} \end{align} $$ From (8a) and (8b) we can conclude that $$\left\vert f(x) - f(y) \right\vert < \frac{2M}{\eta} \left(y-x \right) $$ whenever $a < c < d < b$ , $0 < \eta < \frac{d-c}{2}$ , and $c+\eta < x < y < d-\eta$ . Therefore, interchanging the roles of $x$ and $y$ in the last result we can also conclude that,  whenever $a < c < d < b$ and $0 < \eta < \frac{d-c}{2}$ , we have $$\left\vert f(x) - f(y) \right\vert < \frac{2M}{\eta} \left\vert x-y \right\vert  \ \mbox{ for all } \ x, y \in (c+\eta, d-\eta). \ \tag{8} $$ Now let $p$ be any given point of $(a, b)$ , and let $\varepsilon$ be any positive real number. We can choose some real numbers $c$ and $d$ such that $$a < c < p < d < b,$$ and then we can choose a real number $\eta$ such that $$0 < \eta <  \min \left\{ \ \frac{d-c}{2}, \ p-c, \ d-p \ \right\}.$$ Then $$p \in (c+\eta, d-\eta);$$ that is, $$c+\eta < p < d-\eta.$$ Let us choose a real number $\delta$ such that $$ 0 < \delta < \min \left\{ \ \frac{\eta}{2M+1}\varepsilon, \ p-c-\eta, \ d-\eta-p \ \right\}.$$ Then any $x \in (a, b)$ which satisfies $\left\vert x-p \right\vert < \delta$ also belongs to $(c+\eta, d-\eta)$ and therefore by (8) above also satisfies $$ \begin{align} \left\vert f(x) - f(p) \right\vert &\leq \frac{2M}{\eta} \vert x-y\vert \\ &\leq \frac{2M}{\eta} \frac{\eta}{2M+1}\varepsilon \\ &< \varepsilon. \end{align} $$ Hence $f$ is continuous at every point $p \in (a, b)$ . Is this proof correct? If so, then is my presentation good enough? If not, then where lie the flaws? Now let $f$ be a real convex function on $(a, b)$ , let $g$ be a real increasing convex function defined on a segment $(c, d)$ in $\mathbb{R}^1$ such that $$ f\left( (a, b) \right) \subset (c, d),$$ and let $h$ be the  function defined in $(a, b)$ as follows: $$h(x) = g\left(f(x) \right) \ \mbox{ for all } \ x \in (a, b).$$ We show that $h$ is convex. For this, let $x, y \in (a, b)$ and $\lambda \in (0, 1)$ . Then we note that $$ \begin{align} h \left( (1-\lambda) x + \lambda y \right) &= g \left( f \left( (1-\lambda) x + \lambda y \right) \right) \\ &\leq g\left( \ (1-\lambda) f(x) + \lambda f(y) \ \right) \\ & \ \ \  \mbox{ [ because of the convexity of $f$, } \\ & \ \ \ \mbox{   we have $f \left( (1-\lambda) x + \lambda y \right) \leq (1-\lambda) f(x) + \lambda f(y)$ } \\  & \ \ \ \mbox{   and because $g$ is increasing ]} \\ &\leq (1-\lambda) g \left( f(x) \right) + \lambda g \left( f(y) \right) \\ & \ \ \ \mbox{ [ because of the convexity of $g$ ] } \\ &= (1-\lambda) h(x) + \lambda h(y). \end{align} $$ Hence $h$ is convex. Is the formulation of this result correct and general enough? If so, then is my proof (and the presentation thereof) good enough? If $a < s < t < u < b$ , then we can write $$t = (1-\lambda) s + \lambda u, \ \mbox{ where } \ \lambda = \frac{t-s}{u-s} \in (0, 1), $$ and then $$ \begin{align} f(t) &= f\left( (1-\lambda) s + \lambda u \right) \\ &\leq (1-\lambda) f(s) + \lambda f(u) \\ &= \left( 1 - \frac{t-s}{u-s} \right) f(s) + \frac{t-s}{u-s} f(u) \\ &= \frac{u-t}{u-s} f(s) + \frac{t-s}{u-s} f(u), \ \tag{9} \end{align} $$ and so $$  \begin{align} \frac{f(t) - f(s)}{t-s}  &\leq \frac{1}{t-s} \left[ \frac{u-t}{u-s} f(s) + \frac{t-s}{u-s} f(u) - f(s) \right] \\ &= \frac{1}{t-s} \left[ \left( \frac{u-t}{u-s} - 1 \right) f(s) + \frac{t-s}{u-s} f(u) \right] \\ &= \frac{1}{t-s} \left[ \frac{s-t}{u-s} f(s) + \frac{t-s}{u-s} f(u) \right]  \\  &= \frac{1}{t-s} \frac{t-s}{u-s} \left[ f(u) - f(s) \right]  \\  &= \frac{ f(u)-f(s)}{u-s}. \end{align} $$ Thus we have shown that if $a < s < t < u < b$ , then $$ \frac{f(t) - f(s)}{t-s}  \leq \frac{ f(u)-f(s)}{u-s}. \ \tag{10} $$ Now from (9) above, we obtain $$f(t) \leq \frac{u-t}{u-s} f(s) + \frac{t-s}{u-s} f(u),$$ which upon dividing both sides by $u-t$ becomes $$ \begin{align} \frac{ f(t)}{u-t} &\leq \frac{f(s)}{u-s} + \frac{t-s}{(u-s)(u-t)} f(u) \\ &= \frac{f(s)}{u-s} + \frac{(u-s) - (u-t) }{(u-s)(u-t)} f(u) \\ &= \frac{f(s)}{u-s} + \left( \frac{1}{u-t} - \frac{1}{u-s} \right) f(u) \\ &= \frac{f(s)- f(u) }{u-s} +  \frac{f(u)}{u-t}.   \end{align} $$ Thus we have shown that $$\frac{ f(t)}{u-t} \leq \frac{f(s)- f(u) }{u-s} +  \frac{f(u)}{u-t},$$ which implies that $$\frac{f(u)- f(s) }{u-s} \leq \frac{f(u)- f(t) }{u-t} \ \tag{11} $$ if $a < s < t < u < b$ . From (10) and (11), we conclude that if $a< s< t< u< b$ , then $$\frac{f(t) - f(s)}{t-s}  \leq  \frac{f(u)- f(s) }{u-s} \leq \frac{f(u)- f(t) }{u-t}, $$ as required. Is this proof correct? If so, then what is the presentation like? If there is (are) any problem(s) in this proof, then at what point?","Here is Prob. 23, Chap. 4, in the book Principles of Mathematical Analysis by Walter Rudin, 3rd edition: A real-valued function defined in is said to be convex if whenever , , . Prove that every convex function is continuous. [Although this proof is available at Math SE, I would prefer a direct, - proof.] Prove that every increasing convex function of a convex function is convex. [ How to?] (For example, if is convex, so is .) If is convex in and if , show that [ How to? ] I would appreciate if the proofs are elementary (but rigorous enough for Rudin) and are only based on the machinary developed by Rudin up to this point in the book. Is every real convex function defined in also uniformly continuous? An afterthought: Here's my attempt: Let , , and be any three real numbers such that Then we have which implies that moreover if we put then we see that Thus we have shown that for any three real numbers , , and such that , we can write as In what follows, we will be using this fact on several occasions. First, we show that is bounded on every closed interval . Now let , where and are any two real numbers such that . Then we can write as Then Thus If then we can also conclude that and so Then which implies that And, if then we can also conclude that and so Then which implies that From (2) and (3) we can conclude that From (1) and (4) we can comclude that, given any two real numbers and which satisfy , we can find a real number such that Now let be a real number such that and let and be any two real numbers such that Thus, we have the following chain of inequalities: Therefore we can conclude that and so and then And, from (7) we can also write and then From (8a) and (8b) we can conclude that whenever , , and . Therefore, interchanging the roles of and in the last result we can also conclude that,  whenever and , we have Now let be any given point of , and let be any positive real number. We can choose some real numbers and such that and then we can choose a real number such that Then that is, Let us choose a real number such that Then any which satisfies also belongs to and therefore by (8) above also satisfies Hence is continuous at every point . Is this proof correct? If so, then is my presentation good enough? If not, then where lie the flaws? Now let be a real convex function on , let be a real increasing convex function defined on a segment in such that and let be the  function defined in as follows: We show that is convex. For this, let and . Then we note that Hence is convex. Is the formulation of this result correct and general enough? If so, then is my proof (and the presentation thereof) good enough? If , then we can write and then and so Thus we have shown that if , then Now from (9) above, we obtain which upon dividing both sides by becomes Thus we have shown that which implies that if . From (10) and (11), we conclude that if , then as required. Is this proof correct? If so, then what is the presentation like? If there is (are) any problem(s) in this proof, then at what point?","f (a, b)  f \big( \lambda x + (1- \lambda) y \big) \leq \lambda f(x) + (1-\lambda) f(y) a < x < b a < y < b 0 < \lambda < 1 \varepsilon \delta f e^f f (a, b) a < s < t < u < b  \frac{ f(t)-f(s)}{t-s} \leq \frac{ f(u)-f(s)}{u-s} \leq \frac{ f(u)-f(t)}{u-t}. f (a, b) x y z x < y < z. 0 < y-x < z-x, 0< \frac{ y-x}{z-x} < 1; \lambda \colon= \frac{y-x}{z-x}, 
\begin{align}
(1-\lambda)x+\lambda z &= \left( 1- \frac{y-x}{z-x} \right) x + \frac{y-x}{z-x} z \\
&= \frac{(z-y)x+(y-x)z}{z-x} \\
&= y.
\end{align}
 x y z x < y < z y y = (1-\lambda)x + \lambda z, \ \mbox{ where } \ \lambda = \frac{y-x}{z-x} \ \mbox{ and } \ 0 < \lambda < 1. \ \tag{0} f [ c, d] \subset (a, b) t \in (c, d) c d a < c < d < b t  t = (1- \lambda) c + \lambda d, \ \mbox{ where } \ \lambda = \frac{t-c}{d-c} \in (0, 1).  
\begin{align}
f(t) &= f \left( (1-\lambda)c+\lambda d \right) \\
&\leq (1-\lambda) f(c) + \lambda f(d) \\
&\leq (1-\lambda + \lambda ) \max \left\{ f(c), f(d) \right\} \\
&=  \max \left\{ f(c), f(d) \right\}.
\end{align}
 f(t) \leq \max \left\{ f(c), f(d) \right\} \ \mbox{ for all } \ t \in [c, d] \ \tag{1}. c < t < \frac{c+d}{2}, t < \frac{c+d}{2} < d,  \frac{c+d}{2} = (1-\lambda) t + \lambda d, \ \mbox{ where } \ \lambda = \frac{\frac{c+d}{2} - t}{d-t} = 
\frac{c+d-2t}{2(d-t)} \in (0, 1). 
\begin{align}
f\left( \frac{c+d}{2} \right) &= f \left( (1- \lambda) t + \lambda d \right) \\
&\leq (1-\lambda) f(t) + \lambda f(d) \\
&\leq f(t) + f(d),
\end{align}
 f(t) \geq f\left( \frac{c+d}{2} \right) - f(d) \ \mbox{ for all } \ t \in \left(c, \frac{c+d}{2} \right) 
\ \tag{2}  \frac{c+d}{2} < t < d, c < \frac{c+d}{2} < t, \frac{c+d}{2} = (1-\lambda) c + \lambda t, \ \mbox{ where } \ \lambda = \frac{\frac{c+d}{2}-c}{t-c} = \frac{d-c}{2(t-c)} \ 
\mbox{ so that } \ 0 < \lambda < 1. 
\begin{align}
f\left( \frac{c+d}{2} \right) &= f\left( (1-\lambda) c + \lambda t\right) \\
&\leq (1-\lambda) f(c) + \lambda f(t) \\
&\leq f(c) + f(t),
\end{align}
 f(t) \geq f\left( \frac{c+d}{2} \right) - f(c) \ \mbox{ for all } \ t \in \left( \frac{c+d}{2}, d \right) \ 
\tag{3}. f(t) \geq \min \left\{ \ f(c), \ f(d), \ f\left( \frac{c+d}{2} \right) - f(c), \ f\left( \frac{c+d}{2} \right) - f(d), \ 
 f\left( \frac{c+d}{2} \right) \ \right\} \\
 \mbox{ for all } \ t \in [c, d] \ \tag{4} 
 c d a < c < d < b M > 0  \left\vert f(t) \right\vert \leq M \ \mbox{ for all } \ t \in [c, d]. \ \tag{5}  \eta 0 < \eta < \frac{d-c}{2}, \ \tag{6a}  x y  c +\eta <  x < y  < d-\eta. \ \tag{6b}  c < c+\eta < x < y < d-\eta < d, \ \mbox{ and } \ c+\eta < \frac{c+d}{2} < d-\eta. \ \tag{6}   c < x < y < d, \ \tag{7} 
y = (1-\lambda) x + \lambda d, \ \mbox{ where } \ \lambda = \frac{y-x}{d-x} \ \mbox{ so that } \ 0 < \lambda < 1,
 
\begin{align}
f(y) - f(x) &= f\left( (1-\lambda) x + \lambda d \right) - f(x) \\
&\leq  (1-\lambda) f(x) + \lambda f(d) - f(x) \\
&= \lambda \left( f(d) - f(x) \right) \\
&\leq \lambda \left| f(d) - f(x) \right| \\
&\leq \lambda \left( \left| f(d) \right| + \left| f(x) \right| \right) \\
&\leq 2\lambda M \ \mbox{ [ by (5) above ] } \\
&= \frac{2M(y-x)}{d-x} \\
&< \frac{2M(y-x)}{\eta}. \ \mbox{ [ by (6b) above ] }  \ \tag{8a}
\end{align}
 x = (1-\lambda) c + \lambda y, \ \mbox{ where } \ \lambda = \frac{x-c}{y-c} \ 
\mbox{ so that } \ 0 < \lambda < 1,  
\begin{align}
f(x) - f(y) 
&= f\left( (1-\lambda) c + \lambda y \right) - f(y) \\
&\leq (1-\lambda) f(c) + \lambda f(y) - f(y) \\
&= (1-\lambda) \left( f(c) - f(y) \right) \\
&\leq (1-\lambda)  \left| f(c) - f(y) \right| \\
&\leq (1-\lambda) \left( \left| f(c) \right| + \left| f(y) \right|  \right) \\
&\leq 2(1-\lambda) M \ \mbox{ [ again by (5) above ]} \\
&= \frac{2M(y-x)}{y-c} \\
&< \frac{2M(y-x)}{\eta}. \ \mbox{ [ again by (6b) above ] } \ \tag{8b}
\end{align}
 \left\vert f(x) - f(y) \right\vert < \frac{2M}{\eta} \left(y-x \right)  a < c < d < b 0 < \eta < \frac{d-c}{2} c+\eta < x < y < d-\eta x y a < c < d < b 0 < \eta < \frac{d-c}{2} \left\vert f(x) - f(y) \right\vert < \frac{2M}{\eta} \left\vert x-y \right\vert  \ \mbox{ for all } \ x, y \in (c+\eta, d-\eta). \ \tag{8}  p (a, b) \varepsilon c d a < c < p < d < b, \eta 0 < \eta < 
\min \left\{ \ \frac{d-c}{2}, \ p-c, \ d-p \ \right\}. p \in (c+\eta, d-\eta); c+\eta < p < d-\eta. \delta  0 < \delta < \min \left\{ \ \frac{\eta}{2M+1}\varepsilon, \ p-c-\eta, \ d-\eta-p \ \right\}. x \in (a, b) \left\vert x-p \right\vert < \delta (c+\eta, d-\eta) 
\begin{align}
\left\vert f(x) - f(p) \right\vert &\leq \frac{2M}{\eta} \vert x-y\vert \\
&\leq \frac{2M}{\eta} \frac{\eta}{2M+1}\varepsilon \\
&< \varepsilon.
\end{align}
 f p \in (a, b) f (a, b) g (c, d) \mathbb{R}^1  f\left( (a, b) \right) \subset (c, d), h (a, b) h(x) = g\left(f(x) \right) \ \mbox{ for all } \ x \in (a, b). h x, y \in (a, b) \lambda \in (0, 1) 
\begin{align}
h \left( (1-\lambda) x + \lambda y \right) &= g \left( f \left( (1-\lambda) x + \lambda y \right) \right) \\
&\leq g\left( \ (1-\lambda) f(x) + \lambda f(y) \ \right) \\
& \ \ \  \mbox{ [ because of the convexity of f, } \\
& \ \ \ \mbox{   we have f \left( (1-\lambda) x + \lambda y \right) \leq (1-\lambda) f(x) + \lambda f(y) } \\ 
& \ \ \ \mbox{   and because g is increasing ]} \\
&\leq (1-\lambda) g \left( f(x) \right) + \lambda g \left( f(y) \right) \\
& \ \ \ \mbox{ [ because of the convexity of g ] } \\
&= (1-\lambda) h(x) + \lambda h(y).
\end{align}
 h a < s < t < u < b t = (1-\lambda) s + \lambda u, \ \mbox{ where } \ \lambda = \frac{t-s}{u-s} \in (0, 1),  
\begin{align}
f(t) &= f\left( (1-\lambda) s + \lambda u \right) \\
&\leq (1-\lambda) f(s) + \lambda f(u) \\
&= \left( 1 - \frac{t-s}{u-s} \right) f(s) + \frac{t-s}{u-s} f(u) \\
&= \frac{u-t}{u-s} f(s) + \frac{t-s}{u-s} f(u), \ \tag{9}
\end{align}
  
\begin{align}
\frac{f(t) - f(s)}{t-s}  &\leq \frac{1}{t-s} \left[ \frac{u-t}{u-s} f(s) + \frac{t-s}{u-s} f(u) - f(s) \right] \\
&= \frac{1}{t-s} \left[ \left( \frac{u-t}{u-s} - 1 \right) f(s) + \frac{t-s}{u-s} f(u) \right] \\
&= \frac{1}{t-s} \left[ \frac{s-t}{u-s} f(s) + \frac{t-s}{u-s} f(u) \right]  \\ 
&= \frac{1}{t-s} \frac{t-s}{u-s} \left[ f(u) - f(s) \right]  \\ 
&= \frac{ f(u)-f(s)}{u-s}.
\end{align}
 a < s < t < u < b  \frac{f(t) - f(s)}{t-s}  \leq \frac{ f(u)-f(s)}{u-s}. \ \tag{10}  f(t) \leq \frac{u-t}{u-s} f(s) + \frac{t-s}{u-s} f(u), u-t 
\begin{align}
\frac{ f(t)}{u-t} &\leq \frac{f(s)}{u-s} + \frac{t-s}{(u-s)(u-t)} f(u) \\
&= \frac{f(s)}{u-s} + \frac{(u-s) - (u-t) }{(u-s)(u-t)} f(u) \\
&= \frac{f(s)}{u-s} + \left( \frac{1}{u-t} - \frac{1}{u-s} \right) f(u) \\
&= \frac{f(s)- f(u) }{u-s} +  \frac{f(u)}{u-t}.  
\end{align}
 \frac{ f(t)}{u-t} \leq \frac{f(s)- f(u) }{u-s} +  \frac{f(u)}{u-t}, \frac{f(u)- f(s) }{u-s} \leq \frac{f(u)- f(t) }{u-t} \ \tag{11}  a < s < t < u < b a< s< t< u< b \frac{f(t) - f(s)}{t-s}  \leq  \frac{f(u)- f(s) }{u-s} \leq \frac{f(u)- f(t) }{u-t}, ","['real-analysis', 'analysis', 'continuity', 'convex-analysis', 'uniform-continuity']"
69,"Is the function $d(x,y) = \frac{\|x-y\|}{\|x\|\|y\|}$ a metric?",Is the function  a metric?,"d(x,y) = \frac{\|x-y\|}{\|x\|\|y\|}","$d$ is defined for all $x,y \in \mathbb{R}^2 - \{0\}$. It's clear that $d(x,y) = 0 \iff x=y$ and $d(x,y)=d(y,x)$ I am having issues with triangle inequality. I couldn't find a counterexample for which the triangle inequality doesn't hold. So I tried to prove it. What I have so far is: $$d(x,z) =  \frac{\|x-z\|}{\|x\|\|z\|} \leq  \frac{\|x-y\|}{\|x\|\|z\|} +  \frac{\|y-z\|}{\|x\|\|z\|} $$ I'm stuck here. I appreciate if you could give me some hints. Thanks.","$d$ is defined for all $x,y \in \mathbb{R}^2 - \{0\}$. It's clear that $d(x,y) = 0 \iff x=y$ and $d(x,y)=d(y,x)$ I am having issues with triangle inequality. I couldn't find a counterexample for which the triangle inequality doesn't hold. So I tried to prove it. What I have so far is: $$d(x,z) =  \frac{\|x-z\|}{\|x\|\|z\|} \leq  \frac{\|x-y\|}{\|x\|\|z\|} +  \frac{\|y-z\|}{\|x\|\|z\|} $$ I'm stuck here. I appreciate if you could give me some hints. Thanks.",,"['real-analysis', 'metric-spaces']"
70,Euler Transform elementary Proof,Euler Transform elementary Proof,,"In this webpage Computing the Digits in π there is a proof of the Euler Transform (page 22). The proof there relies on measure theory and Lebesgue integration, I haven't studied that yet. In page 22 there is the following statement: Euler didn’t actually prove any general theorems about this transformation. He did use it in several specific cases, where he could show that it really did converge to the original sum, and converged much more quickly. I was wondering if anyone knows any elementary proof of this transformation or a proof for a particular series ? I don't find many information of this transformation online, a resource recommendation is welcome","In this webpage Computing the Digits in π there is a proof of the Euler Transform (page 22). The proof there relies on measure theory and Lebesgue integration, I haven't studied that yet. In page 22 there is the following statement: Euler didn’t actually prove any general theorems about this transformation. He did use it in several specific cases, where he could show that it really did converge to the original sum, and converged much more quickly. I was wondering if anyone knows any elementary proof of this transformation or a proof for a particular series ? I don't find many information of this transformation online, a resource recommendation is welcome",,"['real-analysis', 'numerical-methods', 'power-series']"
71,"If $\int_{0}^{\infty} f(x) \, dx $ converges, will $\int_{0}^{\infty}e^{-sx} f(x) \, dx$ always converge uniformly on $[0, \infty)$?","If  converges, will  always converge uniformly on ?","\int_{0}^{\infty} f(x) \, dx  \int_{0}^{\infty}e^{-sx} f(x) \, dx [0, \infty)","I previously asked about sufficient conditions to conclude that $$\lim_{s \to 0^{+}}\int_{0}^{\infty} e^{-sx} f(x) \, dx = \int_{0}^{\infty} f(x) \, dx$$ when $\int_{0}^{\infty} f(x) \, dx$ does not converge absolutely. Daniel Fischer showed that a sufficient condition is if $\int_{0}^{\infty} e^{-sx} f(x) \, dx$ converges uniformly on $[0, \delta]$ for some $\delta >0$. Recently I came across the following exercise: Show that if $F(s) = \int_{0}^{\infty} e^{-sx} f(x) \, dx$ converges   for $s=s_{0}$, then it converges uniformly on $[s_{0}, \infty)$. The above excercise is exercise 27 in the first supplement to the textbook Introduction to Real Analysis by William F. Trench. It's basically a stronger version of a theorem that states that if $f(x)$ is continuous on $[0, \infty)$ and $\int^{{\color{red}{x}}}_{0} e^{-s_{0}u} f(u) \, du$ is bounded for all $x \ge 0$, then $\int_{0}^{\infty} e^{-sx} f(x) \, dx$ will converge uniformly on $[s_{1}, \infty)$ for $s_{1} >s_{0}$. A proof of this theorem can be found on page 20 of the supplement. But with the only condition being that $\int_{0}^{\infty} e^{-s_{0}x} f(x) \, dx$ must converge, it's hard to believe that there is not a counterexample. Perhaps it has something to do with $e^{-sx}$ being monotonic in the parameter $s$.","I previously asked about sufficient conditions to conclude that $$\lim_{s \to 0^{+}}\int_{0}^{\infty} e^{-sx} f(x) \, dx = \int_{0}^{\infty} f(x) \, dx$$ when $\int_{0}^{\infty} f(x) \, dx$ does not converge absolutely. Daniel Fischer showed that a sufficient condition is if $\int_{0}^{\infty} e^{-sx} f(x) \, dx$ converges uniformly on $[0, \delta]$ for some $\delta >0$. Recently I came across the following exercise: Show that if $F(s) = \int_{0}^{\infty} e^{-sx} f(x) \, dx$ converges   for $s=s_{0}$, then it converges uniformly on $[s_{0}, \infty)$. The above excercise is exercise 27 in the first supplement to the textbook Introduction to Real Analysis by William F. Trench. It's basically a stronger version of a theorem that states that if $f(x)$ is continuous on $[0, \infty)$ and $\int^{{\color{red}{x}}}_{0} e^{-s_{0}u} f(u) \, du$ is bounded for all $x \ge 0$, then $\int_{0}^{\infty} e^{-sx} f(x) \, dx$ will converge uniformly on $[s_{1}, \infty)$ for $s_{1} >s_{0}$. A proof of this theorem can be found on page 20 of the supplement. But with the only condition being that $\int_{0}^{\infty} e^{-s_{0}x} f(x) \, dx$ must converge, it's hard to believe that there is not a counterexample. Perhaps it has something to do with $e^{-sx}$ being monotonic in the parameter $s$.",,"['calculus', 'real-analysis', 'limits', 'improper-integrals', 'laplace-transform']"
72,Is every real valued function on an interval a sum of two functions with Intermediate Value Property?,Is every real valued function on an interval a sum of two functions with Intermediate Value Property?,,"If $I$ is an interval of real numbers , then is it true that any function $f:I \to \mathbb R$ can be written as $f=f_1+f_2$ , where $f_1 , f_2 : I \to \mathbb R$ have the Intermediate value property?","If $I$ is an interval of real numbers , then is it true that any function $f:I \to \mathbb R$ can be written as $f=f_1+f_2$ , where $f_1 , f_2 : I \to \mathbb R$ have the Intermediate value property?",,"['real-analysis', 'functions']"
73,Is $GL_2(\mathbb Z)\cdot X$ a dense subset of $\mathbb R^2$?,Is  a dense subset of ?,GL_2(\mathbb Z)\cdot X \mathbb R^2,"We know that the set $D=\{a+b\sqrt{2} \mid a,b\in \mathbb Z\}$ is dense in $\mathbb R$ because $D$ is a subgroup of $(\mathbb R,+)$ that is not of the form $\alpha \mathbb Z$. So, the following set $$\left\{(a+b\sqrt{2},c+d\sqrt{2}) \mid a,b,c,d\in \mathbb Z \right\}$$ is also dense in $\mathbb R^2$. Here is my question : is it also true that the subset $$\left\{(a+b\sqrt{2},c+d\sqrt{2}) \mid a,b,c,d\in \mathbb Z, ad-bc=\pm 1 \right\}$$ is dense in $\mathbb R^2$ ? If the answer is no, then can one find a vector $X\in \mathbb R^2$ such that $$\{MX \mid M\in GL_2(\mathbb Z)\}=\left\{\begin{pmatrix}a&b\\c&d\end{pmatrix}X \mid a,b,c,d\in \mathbb Z, ad-bc=\pm 1\right\}$$ is dense in $\mathbb R^2$ ?","We know that the set $D=\{a+b\sqrt{2} \mid a,b\in \mathbb Z\}$ is dense in $\mathbb R$ because $D$ is a subgroup of $(\mathbb R,+)$ that is not of the form $\alpha \mathbb Z$. So, the following set $$\left\{(a+b\sqrt{2},c+d\sqrt{2}) \mid a,b,c,d\in \mathbb Z \right\}$$ is also dense in $\mathbb R^2$. Here is my question : is it also true that the subset $$\left\{(a+b\sqrt{2},c+d\sqrt{2}) \mid a,b,c,d\in \mathbb Z, ad-bc=\pm 1 \right\}$$ is dense in $\mathbb R^2$ ? If the answer is no, then can one find a vector $X\in \mathbb R^2$ such that $$\{MX \mid M\in GL_2(\mathbb Z)\}=\left\{\begin{pmatrix}a&b\\c&d\end{pmatrix}X \mid a,b,c,d\in \mathbb Z, ad-bc=\pm 1\right\}$$ is dense in $\mathbb R^2$ ?",,"['real-analysis', 'general-topology', 'group-actions']"
74,Definition of Banach limit,Definition of Banach limit,,"In my Bachelor Thesis I have defined a Banach limit as a functional $\operatorname{LIM} : l^\infty (\mathbb{N})\rightarrow \mathbb R$ that has the following properties: B1 If $(x_n)$ is a convergent sequence, then $\operatorname{LIM}(x_n)=\lim_{n\to \infty} x_n$. B2 If $x_n\geq 0$ for all $n\in\mathbb N$, then $\operatorname{LIM}(x_n)\geq 0$. B3 $\forall\alpha,\beta\in\mathbb{R}\forall (x_n),(y_n)\in l^\infty(\mathbb{N})[\operatorname{LIM}(\alpha (x_n)+\beta (y_n)) = \alpha \operatorname{LIM}((x_n)) + \beta \operatorname{LIM}((y_n))]$. B4 If $y_n=x_{n+1}$ for all $n\in\mathbb{N}$, then $\operatorname{LIM}((x_n))=\operatorname{LIM}((y_n))$. My supervisor thought that the condition B2 might not be necessary, because maybe it followed from the other conditions. I've looked up a lot of definitions of Banach limits, but they're all a little different, which makes it hard to compare sometimes. But it looks like most of the time, the conditions are at least as strong as mine. I have not been able to prove that B2 is necessary, nor have I been able to find a counterexample. Can anybody tell me if the other three conditions are sufficient? If not, can you give me a counterexample? I figured that maybe I haven't been able to find a counterexample, because there are only non-constructive counterexamples.","In my Bachelor Thesis I have defined a Banach limit as a functional $\operatorname{LIM} : l^\infty (\mathbb{N})\rightarrow \mathbb R$ that has the following properties: B1 If $(x_n)$ is a convergent sequence, then $\operatorname{LIM}(x_n)=\lim_{n\to \infty} x_n$. B2 If $x_n\geq 0$ for all $n\in\mathbb N$, then $\operatorname{LIM}(x_n)\geq 0$. B3 $\forall\alpha,\beta\in\mathbb{R}\forall (x_n),(y_n)\in l^\infty(\mathbb{N})[\operatorname{LIM}(\alpha (x_n)+\beta (y_n)) = \alpha \operatorname{LIM}((x_n)) + \beta \operatorname{LIM}((y_n))]$. B4 If $y_n=x_{n+1}$ for all $n\in\mathbb{N}$, then $\operatorname{LIM}((x_n))=\operatorname{LIM}((y_n))$. My supervisor thought that the condition B2 might not be necessary, because maybe it followed from the other conditions. I've looked up a lot of definitions of Banach limits, but they're all a little different, which makes it hard to compare sometimes. But it looks like most of the time, the conditions are at least as strong as mine. I have not been able to prove that B2 is necessary, nor have I been able to find a counterexample. Can anybody tell me if the other three conditions are sufficient? If not, can you give me a counterexample? I figured that maybe I haven't been able to find a counterexample, because there are only non-constructive counterexamples.",,"['real-analysis', 'functional-analysis']"
75,"$\epsilon>0$ there is a polynomial $p$ such that $|f(x)-e^{-x}p|<\epsilon\forall x\in[0,\infty)$",there is a polynomial  such that,"\epsilon>0 p |f(x)-e^{-x}p|<\epsilon\forall x\in[0,\infty)","Could any one tell me how to solve this one? Given $f\in C[0,\infty)$ such that $f(x)\to 0$ as $x\to\infty$ we need to show that for any $\epsilon>0$ there is a polynomial $p$ such that $|f(x)-e^{-x}p(x)|<\epsilon \qquad \forall~ x\in[0,\infty)$ I just know the statement of Weierstrass Polynomial Approximation Theorem and that seems very far from the given problem, but somehow I feel I need to apply the theorem.","Could any one tell me how to solve this one? Given $f\in C[0,\infty)$ such that $f(x)\to 0$ as $x\to\infty$ we need to show that for any $\epsilon>0$ there is a polynomial $p$ such that $|f(x)-e^{-x}p(x)|<\epsilon \qquad \forall~ x\in[0,\infty)$ I just know the statement of Weierstrass Polynomial Approximation Theorem and that seems very far from the given problem, but somehow I feel I need to apply the theorem.",,"['real-analysis', 'polynomials', 'approximation-theory']"
76,Functions satisfying $\sum_{n=0}^k(-1)^n\binom{k}{k-n}f^{k-n}(x)=0$.,Functions satisfying .,\sum_{n=0}^k(-1)^n\binom{k}{k-n}f^{k-n}(x)=0,"This question was motivated by this one . Let $f:\mathbb{R}\rightarrow\mathbb{R}$ be a continuous function. Define $f^n(x)=f\circ f\circ\cdot\cdot\cdot\circ f$, $n$ times, $f^0(x)=x$ and $k\geq 2$ a integer. Suppose that $f$ satisfies the equation: $$\tag{1}\sum_{n=0}^k(-1)^n\binom{k}{k-n}f^{k-n}(x)=0$$ Note that for each fixed $a\in\mathbb{R}$, $f_a(x)=x+a$ satisfies $(1)$. Are the only solutions of $(1)$ of the form $f_a$ for some $a$? Also note that $f_a$ is an solution of the equation $$\tag{2}(f-I)^k(x)=a$$ The same question applies here:  Is the only solution of $(2)$ of the form $f_a$? If we let $a$ vary in $(2)$, we get a family of equations. Is that family of equations equivalently to $(1)$?","This question was motivated by this one . Let $f:\mathbb{R}\rightarrow\mathbb{R}$ be a continuous function. Define $f^n(x)=f\circ f\circ\cdot\cdot\cdot\circ f$, $n$ times, $f^0(x)=x$ and $k\geq 2$ a integer. Suppose that $f$ satisfies the equation: $$\tag{1}\sum_{n=0}^k(-1)^n\binom{k}{k-n}f^{k-n}(x)=0$$ Note that for each fixed $a\in\mathbb{R}$, $f_a(x)=x+a$ satisfies $(1)$. Are the only solutions of $(1)$ of the form $f_a$ for some $a$? Also note that $f_a$ is an solution of the equation $$\tag{2}(f-I)^k(x)=a$$ The same question applies here:  Is the only solution of $(2)$ of the form $f_a$? If we let $a$ vary in $(2)$, we get a family of equations. Is that family of equations equivalently to $(1)$?",,"['real-analysis', 'functional-equations']"
77,"Every path has a simple ""subpath""","Every path has a simple ""subpath""",,"I've been thinking about this for a while, and can't seem to find any way to do it despite the statement itself seeming obvious.  The problem is: Let $f:[0,1] \to \mathbb{R}^n$ be a continuous map, not necessarily injective, such that $f(0) \not = f(1)$.  Let $Y$ denote the image of $f$ as a compact subset of $\mathbb{R}^n$.  Then there exists an injective map $g:[0,1] \to Y$ such that $g(0) = f(0)$ and $g(1) = f(1)$. Obviously, the problem is trivial in many cases, but gets tough when you consider ""wild"" curves.  Below I'll discuss how I've tried to solve it, but feel free to ignore it if you know the answer. My first idea was to start Zorn's lemma argument using a sequence of maps $f_0,f_1,f_2,f_3,\dots$ with $f_0 = f$ and $f_0([0,1]) \supseteq f_1([0,1]) \supseteq f_2([0,1]) \supseteq \cdots$ and trying to find a bound for the chain, but it seems this fails because given compact path-connected subsets $X_0,X_1,\dots$ where $X_0 \supseteq X_1 \supseteq \cdots$, the intersection $\bigcap_{k=1}^\infty X_k$ is not necessarily path connected.  For example, if $S \subset \mathbb{R}^2$ is a segment of the (closed) topologist's sine curve, then the family $S \cup \bar{B}_{1/n}(0)$ provide a counterexample. The other approach seemed to be trying to define a sequence of functions $f_0,f_1,\dots$ where each $f_n$ is ""closer"" to being injective than the one before it, and such that the sequence converges in some meaningful way to an injective function, or at least to a function that allows us to use a simpler method to finish the problem.  However, I don't really know enough analysis to follow through with this approach, so I'm asking for help here.  Any solution would be great, and solutions a method similar to what I've mentioned above would be doubly appreciated.","I've been thinking about this for a while, and can't seem to find any way to do it despite the statement itself seeming obvious.  The problem is: Let $f:[0,1] \to \mathbb{R}^n$ be a continuous map, not necessarily injective, such that $f(0) \not = f(1)$.  Let $Y$ denote the image of $f$ as a compact subset of $\mathbb{R}^n$.  Then there exists an injective map $g:[0,1] \to Y$ such that $g(0) = f(0)$ and $g(1) = f(1)$. Obviously, the problem is trivial in many cases, but gets tough when you consider ""wild"" curves.  Below I'll discuss how I've tried to solve it, but feel free to ignore it if you know the answer. My first idea was to start Zorn's lemma argument using a sequence of maps $f_0,f_1,f_2,f_3,\dots$ with $f_0 = f$ and $f_0([0,1]) \supseteq f_1([0,1]) \supseteq f_2([0,1]) \supseteq \cdots$ and trying to find a bound for the chain, but it seems this fails because given compact path-connected subsets $X_0,X_1,\dots$ where $X_0 \supseteq X_1 \supseteq \cdots$, the intersection $\bigcap_{k=1}^\infty X_k$ is not necessarily path connected.  For example, if $S \subset \mathbb{R}^2$ is a segment of the (closed) topologist's sine curve, then the family $S \cup \bar{B}_{1/n}(0)$ provide a counterexample. The other approach seemed to be trying to define a sequence of functions $f_0,f_1,\dots$ where each $f_n$ is ""closer"" to being injective than the one before it, and such that the sequence converges in some meaningful way to an injective function, or at least to a function that allows us to use a simpler method to finish the problem.  However, I don't really know enough analysis to follow through with this approach, so I'm asking for help here.  Any solution would be great, and solutions a method similar to what I've mentioned above would be doubly appreciated.",,"['real-analysis', 'general-topology', 'analysis']"
78,German Analysis Texts,German Analysis Texts,,"My question is somewhat related to this one but is somewhat more specific. Since a lot of good mathematics is written in German, I have decided to start developing my German reading abilities. So far, my basic strategy has been to come upon some random German lecture notes in topology, analysis or algebra and together with Google translate work my way through random pages here-and-there. I think this basic approach will work for me, but I would like to be a little more methodical. I would like to add a few good German analysis texts to my library and begin working my way through those. From the user t.b., in particular, I learned of the Analysis texts by Königsberger and since they come highly-recommended I will probably get these. I have also learned of the existence of a sequence of four texts by Storch and Wiebe: Band 1: Analysis einer Veränderlichen Band 2: Lineare Algebra Band 3: Analysis mehrerer Varänderlicher - Integrationstheorie Band 4: Analysis auf Mannigfaltigkeiten I have been able to look at the tables of contents and they cover and tremendous amount of material; in fact, I know of no English equivalent. My question though concerns their pedagogical value; for those who are familiar with these texts, are they easy to learn from or are the basically specialist-level reference texts? By way of comparison, for example, I find Royden's Analysis well-written and sufficiently detailed to follow. Rudin, not so much. So if you are familiar with these texts and can comment on their pedagogical value or if you could suggest other German analysis (or algebra/topology) texts that would be good to read, I would appreciate your comments.","My question is somewhat related to this one but is somewhat more specific. Since a lot of good mathematics is written in German, I have decided to start developing my German reading abilities. So far, my basic strategy has been to come upon some random German lecture notes in topology, analysis or algebra and together with Google translate work my way through random pages here-and-there. I think this basic approach will work for me, but I would like to be a little more methodical. I would like to add a few good German analysis texts to my library and begin working my way through those. From the user t.b., in particular, I learned of the Analysis texts by Königsberger and since they come highly-recommended I will probably get these. I have also learned of the existence of a sequence of four texts by Storch and Wiebe: Band 1: Analysis einer Veränderlichen Band 2: Lineare Algebra Band 3: Analysis mehrerer Varänderlicher - Integrationstheorie Band 4: Analysis auf Mannigfaltigkeiten I have been able to look at the tables of contents and they cover and tremendous amount of material; in fact, I know of no English equivalent. My question though concerns their pedagogical value; for those who are familiar with these texts, are they easy to learn from or are the basically specialist-level reference texts? By way of comparison, for example, I find Royden's Analysis well-written and sufficiently detailed to follow. Rudin, not so much. So if you are familiar with these texts and can comment on their pedagogical value or if you could suggest other German analysis (or algebra/topology) texts that would be good to read, I would appreciate your comments.",,"['real-analysis', 'reference-request', 'mathematical-german']"
79,Proving a complicated inequality involving integers,Proving a complicated inequality involving integers,,"Let $a,b,c,d$ be integers such that $$\left( \begin{matrix} a & b \\ c & d \end{matrix} \right) = \left( \begin{matrix} 1 & 0 \\ 0 & 1 \end{matrix}\right) \mod 2$$ $$ ad-bc =1$$ $$\left( \begin{matrix} a & b \\ c & d \end{matrix} \right) \neq  \left( \begin{matrix} \pm 1 & 0 \\ 0 & \pm 1 \end{matrix}\right) .$$ Let $(x,y)\in \mathbf{R}^2$ such that $-1\leq x\leq 1$ and $1/2 \leq y\leq 2$. I highly suspect that   $$c^2(y^2+2x^2) + a^2+d^2+2cx(d-a) + \frac{1}{y^2}(b-(d-a+3cx)x)^2 \geq 3.$$ The proof is actually very easy and was obtained after Parsa's answer.","Let $a,b,c,d$ be integers such that $$\left( \begin{matrix} a & b \\ c & d \end{matrix} \right) = \left( \begin{matrix} 1 & 0 \\ 0 & 1 \end{matrix}\right) \mod 2$$ $$ ad-bc =1$$ $$\left( \begin{matrix} a & b \\ c & d \end{matrix} \right) \neq  \left( \begin{matrix} \pm 1 & 0 \\ 0 & \pm 1 \end{matrix}\right) .$$ Let $(x,y)\in \mathbf{R}^2$ such that $-1\leq x\leq 1$ and $1/2 \leq y\leq 2$. I highly suspect that   $$c^2(y^2+2x^2) + a^2+d^2+2cx(d-a) + \frac{1}{y^2}(b-(d-a+3cx)x)^2 \geq 3.$$ The proof is actually very easy and was obtained after Parsa's answer.",,"['real-analysis', 'analysis', 'inequality', 'integer-lattices']"
80,Proof of a theorem of Cauchy's on the convergence of an infinite product,Proof of a theorem of Cauchy's on the convergence of an infinite product,,"Well it is relatively well known that the condition for absolute convergence is given by the following theorem: In order that the infinite product $\prod _{n=1}^{\infty }\left( 1+a_{n}\right) $ may be absolutely convergent, it is necessary and sufficient that the series $\sum _{n=1}^{\infty }a_{n}$ should be absolutely convergent. I am trying to prove a little less famous result from Cauchy, which states If $\sum _{n=1}^{\infty }a_{n}$ be a conditionally convergent series of real terms, then $\prod _{n=1}^{\infty }\left( 1+a_{n}\right) $ converges (but not absolutely) or diverges to zero according as $\sum _{n=1}^{\infty }a_{n}^{2}$ converges or diverges. Some thoughts towards the Proof Although i could eb wrong here but since we do not know that $a_{n}\rightarrow 0$ under the given circumstances i guess a proof by comparison to $\sum _{k=0}^{\infty }\dfrac {1} {k^{2}}$ or which required the ln series kind of seem to fall apart. I was hoping some one could possibly provide an idea/ strategy for this proof. Any help would be much appreciated.","Well it is relatively well known that the condition for absolute convergence is given by the following theorem: In order that the infinite product $\prod _{n=1}^{\infty }\left( 1+a_{n}\right) $ may be absolutely convergent, it is necessary and sufficient that the series $\sum _{n=1}^{\infty }a_{n}$ should be absolutely convergent. I am trying to prove a little less famous result from Cauchy, which states If $\sum _{n=1}^{\infty }a_{n}$ be a conditionally convergent series of real terms, then $\prod _{n=1}^{\infty }\left( 1+a_{n}\right) $ converges (but not absolutely) or diverges to zero according as $\sum _{n=1}^{\infty }a_{n}^{2}$ converges or diverges. Some thoughts towards the Proof Although i could eb wrong here but since we do not know that $a_{n}\rightarrow 0$ under the given circumstances i guess a proof by comparison to $\sum _{k=0}^{\infty }\dfrac {1} {k^{2}}$ or which required the ln series kind of seem to fall apart. I was hoping some one could possibly provide an idea/ strategy for this proof. Any help would be much appreciated.",,"['real-analysis', 'sequences-and-series', 'convergence-divergence', 'infinite-product']"
81,Limit of the quotient of sums of recurrent relations,Limit of the quotient of sums of recurrent relations,,"We define a sequence $a_n(b,x)$ for any $b,x\in [0,1]$ by letting $a_0(b) = 1$ and further: $$ a_{n+1}(b,x) = x \cdot ( b \cdot a_n(b,x)^2+(1-b)\cdot a_n(b,x)\ ), $$ note that in the extreme values, $b\in \{0,1\}$ we have $a_n(0,x)=x^n$ and $a_n(1,x)=x^{2^n-1}$ . Let us define $A(b,x)=\sum_{n=0}^{\infty} a_n(b,x)$ . It is clear that for any $x \in (0,1)$ and $b \in [0,1]$ we have $A(b,x)$ is a convergent sequence. However for $x=1$ this sequence becomes divergent for all values of $b$ . I am interested in computing the following limit: $$ \lim_{x\rightarrow 1^-} \frac{A(b,x)}{A(1,x)}. $$ It is not too hard to show that, for $b=0$ , this quotient is equal to infinity. However, for other values $b\in (0,1)$ I have found numerically that this quotient is some finite number. The numerical values I found seem to suggest that the following should hold: $$ \lim_{x\rightarrow 1^-} \frac{A(b,x)}{A(1,x)} = 1 + \frac{(1-b)\log(2)}{b} $$ I can however not find how to verify this in an analytical way. I have tried to obtain upper/lower bounds on the values of $a_{n+1}(b,x)$ which would allow for some closed formula but with no success. I am looking for methods which may be used to tackle this problem.","We define a sequence for any by letting and further: note that in the extreme values, we have and . Let us define . It is clear that for any and we have is a convergent sequence. However for this sequence becomes divergent for all values of . I am interested in computing the following limit: It is not too hard to show that, for , this quotient is equal to infinity. However, for other values I have found numerically that this quotient is some finite number. The numerical values I found seem to suggest that the following should hold: I can however not find how to verify this in an analytical way. I have tried to obtain upper/lower bounds on the values of which would allow for some closed formula but with no success. I am looking for methods which may be used to tackle this problem.","a_n(b,x) b,x\in [0,1] a_0(b) = 1 
a_{n+1}(b,x)
=
x \cdot ( b \cdot a_n(b,x)^2+(1-b)\cdot a_n(b,x)\ ),
 b\in \{0,1\} a_n(0,x)=x^n a_n(1,x)=x^{2^n-1} A(b,x)=\sum_{n=0}^{\infty} a_n(b,x) x \in (0,1) b \in [0,1] A(b,x) x=1 b 
\lim_{x\rightarrow 1^-} \frac{A(b,x)}{A(1,x)}.
 b=0 b\in (0,1) 
\lim_{x\rightarrow 1^-} \frac{A(b,x)}{A(1,x)}
=
1 + \frac{(1-b)\log(2)}{b}
 a_{n+1}(b,x)","['real-analysis', 'calculus', 'sequences-and-series', 'limits', 'recurrence-relations']"
82,Convergence of Riemann sums of a periodic function,Convergence of Riemann sums of a periodic function,,"Short version for people who don't like reading: Let $f\colon\mathbb{R}\to\mathbb{R}$ be $1$-periodic, measurable and bounded.  Is it true that, for almost all $x$, the average of $f(x)$, $f(x+\frac{1}{n})$, $f(x+\frac{2}{n})$, …, $f(x-\frac{1}{n})$ tends to $\int_0^1 f$ when $n\to+\infty$? And now for a more detailed version of the question: Let $\mathbb{T} := \mathbb{R}/\mathbb{Z}$ so that functions $\mathbb{T}\to\mathbb{R}$ can be identified with $1$-periodic functions $\mathbb{R}\to\mathbb{R}$. If $f\colon\mathbb{T}\to\mathbb{R}$, we define $\mathscr{M}_n(f)\colon\mathbb{T}\to\mathbb{R}$ by $$(\mathscr{M}_n(f))(x) := \frac{1}{n}\sum_{k=0}^{n-1} f\Big(\!x+\frac{k}{n}\Big)$$ the $n$-th “Riemann sum” of $f$, i.e., the average of the $n$ translates of $f$ by $n$-th periods.  If also $f \in L^1(\mathbb{T})$ we define $\mathscr{E}(f)\colon\mathbb{T}\to\mathbb{R}$ by $$(\mathscr{E}(f))(x) := \int_{\mathbb{T}} f(t)\,dt$$ (constant function!) the integral, i.e., overall average of $f$. The general problem is in what ways and under what assumptions we can say that $\mathscr{M}_n(f) \to \mathscr{E}(f)$. Precise questions are below (at end), but first let me first state a few simple known facts relevant to this situation, that might help provide some background: If $f$ is a step function (where ""step function"" means a linear combination of characteristic functions of intervals) then $|\mathscr{M}_n(f) - \mathscr{E}(f)| \leq \frac{\|f\|_\infty}{n}$ everywhere.  (Sketch of proof: if $f = \mathbf{1}_{[0,r/n)}$ with $r\in\mathbb{N}$ then in fact $\mathscr{M}_n(f) = \mathscr{E}(f)$, and if  $f_c = \mathbf{1}_{[0,c)}$ with $\frac{r}{n}\leq c<\frac{r+1}{n}$ then $f_{r/n} \leq f_c \leq f_{(r+1)/n}$ everywhere so that the same inequality holds after applying $\mathscr{M}_n$, i.e., $\frac{r}{n} \leq \mathscr{M}_n(f_c) \leq \frac{r+1}{n}$, whence the conclusion for $f_c$, and then for a general step function by translating and taking linear combinations.) If $f \in L^p(\mathbb{T})$ with $1\leq p<\infty$ then $\mathscr{M}_n(f) \to \mathscr{E}(f)$ in $L^p(\mathbb{T})$.  (Follows from the above by density of step functions in $L^p$ and the fact that $\mathscr{M}_n$ and $\mathscr{E}$ have norm $1$.) If $f$ is Riemann-integrable then $\mathscr{M}_n(f) \to \mathscr{E}(f)$ uniformly on $\mathbb{T}$.  (Fairly obvious using the first point and the following definition of R-integrable functions: for every $\varepsilon>0$ there exist $h$ and $\varphi$ step functions such that $|f-h|\leq\varphi$ everywhere and $\int\varphi \leq \varepsilon$.) If $u_n = n\mathbf{1}_{[0,1/n)}$ then $\mathscr{M}_n(f) - \mathscr{E}(f) = \mathscr{M}_n(f - (f*u_n))$ (writing $*$ for convolution), and when $f$ is measurable we have $f*u_n \to f$ almost everywhere (by the existence of right Lebesgue points). The Fourier coefficients of $\mathscr{M}_n(f)$ are those of $f$ at indices multiple of $n$, the other being $0$; so they converge punctually (i.e., for a given idnex) to those of $\mathscr{E}(f)$.  Also, if the Fourier coefficients of $f$ are $\ell^q$ then the convergence of Fourier coefficients of $\mathscr{M}_n(f)$ to those of $\mathscr{E}(f)$ holds in $\ell^q$. Update 2016-02-10: If $f$ is $L^1(\mathbb{T})$, it is not necessarily the case that $\mathscr{M}_n(f) \to \mathscr{E}(f)$ almost everywhere, or indeed, anywhere : this is a theorem of Marcinkiewicz and Zygmund ("" Mean values of trigonometrical polynomials "", Fund. Math. 28 (1937), chapter II, theorem 3 on p. 157).  Their counterexample (which is $(-\log|x|)/\sqrt{|x|}$ on $[0,\frac{1}{2}]$) is certainly not bounded, however. Birkhoff's ergodic theorem is probably also worth mentioning here: if $\xi$ is irrational, then for all $f\in L^1(\mathbb{T})$, for almost all $x$ we have $\frac{1}{n}\sum_{k=0}^{n-1} f(x+k\xi) \to \mathscr{E}(f)$. Now at last here are my questions , motivated by the gaps left in the above facts: If $f \in L^\infty(\mathbb{T})$, do we have $\mathscr{M}_n(f) \to \mathscr{E}(f)$ in $L^\infty(\mathbb{T})$?  If not, do we have $\mathscr{M}_n(f) \to \mathscr{E}(f)$ in $L^\infty(\mathbb{T})$ almost everywhere?  [This is the ""short version"" at the start of this post.]","Short version for people who don't like reading: Let $f\colon\mathbb{R}\to\mathbb{R}$ be $1$-periodic, measurable and bounded.  Is it true that, for almost all $x$, the average of $f(x)$, $f(x+\frac{1}{n})$, $f(x+\frac{2}{n})$, …, $f(x-\frac{1}{n})$ tends to $\int_0^1 f$ when $n\to+\infty$? And now for a more detailed version of the question: Let $\mathbb{T} := \mathbb{R}/\mathbb{Z}$ so that functions $\mathbb{T}\to\mathbb{R}$ can be identified with $1$-periodic functions $\mathbb{R}\to\mathbb{R}$. If $f\colon\mathbb{T}\to\mathbb{R}$, we define $\mathscr{M}_n(f)\colon\mathbb{T}\to\mathbb{R}$ by $$(\mathscr{M}_n(f))(x) := \frac{1}{n}\sum_{k=0}^{n-1} f\Big(\!x+\frac{k}{n}\Big)$$ the $n$-th “Riemann sum” of $f$, i.e., the average of the $n$ translates of $f$ by $n$-th periods.  If also $f \in L^1(\mathbb{T})$ we define $\mathscr{E}(f)\colon\mathbb{T}\to\mathbb{R}$ by $$(\mathscr{E}(f))(x) := \int_{\mathbb{T}} f(t)\,dt$$ (constant function!) the integral, i.e., overall average of $f$. The general problem is in what ways and under what assumptions we can say that $\mathscr{M}_n(f) \to \mathscr{E}(f)$. Precise questions are below (at end), but first let me first state a few simple known facts relevant to this situation, that might help provide some background: If $f$ is a step function (where ""step function"" means a linear combination of characteristic functions of intervals) then $|\mathscr{M}_n(f) - \mathscr{E}(f)| \leq \frac{\|f\|_\infty}{n}$ everywhere.  (Sketch of proof: if $f = \mathbf{1}_{[0,r/n)}$ with $r\in\mathbb{N}$ then in fact $\mathscr{M}_n(f) = \mathscr{E}(f)$, and if  $f_c = \mathbf{1}_{[0,c)}$ with $\frac{r}{n}\leq c<\frac{r+1}{n}$ then $f_{r/n} \leq f_c \leq f_{(r+1)/n}$ everywhere so that the same inequality holds after applying $\mathscr{M}_n$, i.e., $\frac{r}{n} \leq \mathscr{M}_n(f_c) \leq \frac{r+1}{n}$, whence the conclusion for $f_c$, and then for a general step function by translating and taking linear combinations.) If $f \in L^p(\mathbb{T})$ with $1\leq p<\infty$ then $\mathscr{M}_n(f) \to \mathscr{E}(f)$ in $L^p(\mathbb{T})$.  (Follows from the above by density of step functions in $L^p$ and the fact that $\mathscr{M}_n$ and $\mathscr{E}$ have norm $1$.) If $f$ is Riemann-integrable then $\mathscr{M}_n(f) \to \mathscr{E}(f)$ uniformly on $\mathbb{T}$.  (Fairly obvious using the first point and the following definition of R-integrable functions: for every $\varepsilon>0$ there exist $h$ and $\varphi$ step functions such that $|f-h|\leq\varphi$ everywhere and $\int\varphi \leq \varepsilon$.) If $u_n = n\mathbf{1}_{[0,1/n)}$ then $\mathscr{M}_n(f) - \mathscr{E}(f) = \mathscr{M}_n(f - (f*u_n))$ (writing $*$ for convolution), and when $f$ is measurable we have $f*u_n \to f$ almost everywhere (by the existence of right Lebesgue points). The Fourier coefficients of $\mathscr{M}_n(f)$ are those of $f$ at indices multiple of $n$, the other being $0$; so they converge punctually (i.e., for a given idnex) to those of $\mathscr{E}(f)$.  Also, if the Fourier coefficients of $f$ are $\ell^q$ then the convergence of Fourier coefficients of $\mathscr{M}_n(f)$ to those of $\mathscr{E}(f)$ holds in $\ell^q$. Update 2016-02-10: If $f$ is $L^1(\mathbb{T})$, it is not necessarily the case that $\mathscr{M}_n(f) \to \mathscr{E}(f)$ almost everywhere, or indeed, anywhere : this is a theorem of Marcinkiewicz and Zygmund ("" Mean values of trigonometrical polynomials "", Fund. Math. 28 (1937), chapter II, theorem 3 on p. 157).  Their counterexample (which is $(-\log|x|)/\sqrt{|x|}$ on $[0,\frac{1}{2}]$) is certainly not bounded, however. Birkhoff's ergodic theorem is probably also worth mentioning here: if $\xi$ is irrational, then for all $f\in L^1(\mathbb{T})$, for almost all $x$ we have $\frac{1}{n}\sum_{k=0}^{n-1} f(x+k\xi) \to \mathscr{E}(f)$. Now at last here are my questions , motivated by the gaps left in the above facts: If $f \in L^\infty(\mathbb{T})$, do we have $\mathscr{M}_n(f) \to \mathscr{E}(f)$ in $L^\infty(\mathbb{T})$?  If not, do we have $\mathscr{M}_n(f) \to \mathscr{E}(f)$ in $L^\infty(\mathbb{T})$ almost everywhere?  [This is the ""short version"" at the start of this post.]",,"['real-analysis', 'functional-analysis', 'measure-theory', 'lp-spaces']"
83,How to prove the cubic formula without root extraction,How to prove the cubic formula without root extraction,,"I'm trying to prove the cubic formula, in the following form: Given a field $F$ and $x,p,q\in F$, define $m=\frac p3$ and $n=\frac q2$, and suppose also that $\gamma,\tau$ are given such that $\gamma^2=n^2+m^3$ and $\tau^3=\gamma-n\ne0$. Then $x^3+px+q=0$ if and only if $x=\omega\tau-\frac m{\omega\tau}$ for some $\omega$ satisfying $\omega^3=1$. (I have already reduced the statement from the more general $ax^3+bx^2+cx+d=0$ by a linear transformation.) The usage of $\gamma$ and $\tau$ substitutes for the extraction of square and cube roots, and allows me to stick to just field axioms in the proof - ideally I want this proof to work for any field with characteristic other than $2$ or $3$. But making this approach work with Cardano's method is giving me some issues. Since the reverse implication is just a matter of algebra with the given expressions of $\tau,\gamma$, suppose that we are given $x$ such that $x^3+px+q=0$; we want to find some root of unity $\omega$ satisfying $x=\omega\tau-\frac m{\omega\tau}$. The key of Cardano's method is to observe that the conditions $u+v=x$, $uv+m=0$ together with the condition on $x$ yields a quadratic polynomial $z^2+qz-m^3=0$ whose roots are $u^3$, $v^3$, and solving this with the quadratic equation gives something amenable to a cube root extraction. But why can we choose such $u,v$ in the first place? Equivalently, why should we expect a priori that the ""substitution"" $x=u-\frac mu$ has a solution in $u$? It follows from the quadratic formula, with discriminant $x^2+4m$, but I see no reason to believe that this is a square given only $\gamma$ and $\tau$. If we assume that there is a $\Delta^2=x^2+4m$, then $u=\frac x2+\Delta$ and $v=\frac x2-\Delta$ solve the equations, so we can proceed as normal: $u^6+qu^3-m^3=0$, so by the quadratic formula with discriminant $q^2+4m^3=(2\gamma)^2$, we get $u^3=-n\pm\gamma$ and $v^3=-n\mp\gamma$; in one case we have $u^3=\gamma-n=\tau^3$ so $\frac u\tau$ is the desired root of unity, and in the other case $\frac v\tau$ is the desired root of unity (since $x=u-\frac mu=v-\frac mv$).","I'm trying to prove the cubic formula, in the following form: Given a field $F$ and $x,p,q\in F$, define $m=\frac p3$ and $n=\frac q2$, and suppose also that $\gamma,\tau$ are given such that $\gamma^2=n^2+m^3$ and $\tau^3=\gamma-n\ne0$. Then $x^3+px+q=0$ if and only if $x=\omega\tau-\frac m{\omega\tau}$ for some $\omega$ satisfying $\omega^3=1$. (I have already reduced the statement from the more general $ax^3+bx^2+cx+d=0$ by a linear transformation.) The usage of $\gamma$ and $\tau$ substitutes for the extraction of square and cube roots, and allows me to stick to just field axioms in the proof - ideally I want this proof to work for any field with characteristic other than $2$ or $3$. But making this approach work with Cardano's method is giving me some issues. Since the reverse implication is just a matter of algebra with the given expressions of $\tau,\gamma$, suppose that we are given $x$ such that $x^3+px+q=0$; we want to find some root of unity $\omega$ satisfying $x=\omega\tau-\frac m{\omega\tau}$. The key of Cardano's method is to observe that the conditions $u+v=x$, $uv+m=0$ together with the condition on $x$ yields a quadratic polynomial $z^2+qz-m^3=0$ whose roots are $u^3$, $v^3$, and solving this with the quadratic equation gives something amenable to a cube root extraction. But why can we choose such $u,v$ in the first place? Equivalently, why should we expect a priori that the ""substitution"" $x=u-\frac mu$ has a solution in $u$? It follows from the quadratic formula, with discriminant $x^2+4m$, but I see no reason to believe that this is a square given only $\gamma$ and $\tau$. If we assume that there is a $\Delta^2=x^2+4m$, then $u=\frac x2+\Delta$ and $v=\frac x2-\Delta$ solve the equations, so we can proceed as normal: $u^6+qu^3-m^3=0$, so by the quadratic formula with discriminant $q^2+4m^3=(2\gamma)^2$, we get $u^3=-n\pm\gamma$ and $v^3=-n\mp\gamma$; in one case we have $u^3=\gamma-n=\tau^3$ so $\frac u\tau$ is the desired root of unity, and in the other case $\frac v\tau$ is the desired root of unity (since $x=u-\frac mu=v-\frac mv$).",,"['real-analysis', 'abstract-algebra', 'diophantine-equations', 'systems-of-equations']"
84,"Prove that there exist linear functionals $L_1, L_2$ on $X$",Prove that there exist linear functionals  on,"L_1, L_2 X","Let $X$ be a linear space, $p, q$ sublinear functionals on $X$, and $L$ a linear functional on $X$ such that $|L(x)| ≤ p(x) + q(x),$ for all $x ∈ X$. Prove that there exist linear functionals $L_1, L_2$ on $X$ such that $L(x) = L_1(x) + L_2(x),$ and $|L_1(x)| ≤ p(x), |L_2(x)| ≤ q(x),$ for all $x ∈ X.$ My Work: First I thought to use Hahn Banach Theorem. But since there is no known subspace it was useless. Then I tried to make $L(x)$ as $L(x)=\frac{L(x+\lambda)+L(x-\lambda)}{2}$ for some scalar $\lambda$ but failed to find suitable $L_1$ and $L_2$. I think this problem is little bit tricky. I want to try it myself and I only need a hint to start. Can somebody please give me a hint?","Let $X$ be a linear space, $p, q$ sublinear functionals on $X$, and $L$ a linear functional on $X$ such that $|L(x)| ≤ p(x) + q(x),$ for all $x ∈ X$. Prove that there exist linear functionals $L_1, L_2$ on $X$ such that $L(x) = L_1(x) + L_2(x),$ and $|L_1(x)| ≤ p(x), |L_2(x)| ≤ q(x),$ for all $x ∈ X.$ My Work: First I thought to use Hahn Banach Theorem. But since there is no known subspace it was useless. Then I tried to make $L(x)$ as $L(x)=\frac{L(x+\lambda)+L(x-\lambda)}{2}$ for some scalar $\lambda$ but failed to find suitable $L_1$ and $L_2$. I think this problem is little bit tricky. I want to try it myself and I only need a hint to start. Can somebody please give me a hint?",,"['real-analysis', 'functional-analysis']"
85,"Is $\mathbb{Z}= \{\dots -3, -2, -1, 0 ,1 ,2 , 3, \dots \}$ countable?",Is  countable?,"\mathbb{Z}= \{\dots -3, -2, -1, 0 ,1 ,2 , 3, \dots \}","Question: Is $\mathbb{Z}= \{\dots, -3, -2, -1, 0 ,1 ,2 , 3, \dots \}$ countable? My attemp so far: Let us create the following one-to-one correspondence between $\mathbb{Z}$ and $\mathbb{N}$ . $$\begin{matrix}1 & 2 & 3 & 4 & 5 & 6 & 7 & \dots \\ \updownarrow & \updownarrow & \updownarrow & \updownarrow & \updownarrow & \updownarrow & \updownarrow & \dots\\0 & 1 & -1 & 2 & -2 & 3 & -3 & \dots\end{matrix}$$ In order for $\mathbb{Z}$ to be countable we must define a function $f:\mathbb{N} \to \mathbb{Z}$ so that $\mathbb{Z} \sim \mathbb{N}$ . $$\displaystyle f(n) = \begin{cases} \frac{n}{2} & n \text{ is even} \\ -\frac{n-1}{2} & n\text{ is odd} \end{cases}$$ To show that $\mathbb{Z} \sim \mathbb{N}$ we require $f$ to be bijective. From the picture above, we can clearly see that $f$ is surjective since $\forall z \in \mathbb{Z}\  \exists n \in \mathbb{N}$ such that $f(n) = z$ . Hence we must now show that $f$ is injective. We need to consider three cases: $n_1, n_2$ are odd $n_1, n_2$ are even $n_1$ is odd and $n_2$ is even Case 1: \begin{align}f(n_1) &= f(n_2) \\ \implies -\frac{n_1 -1}{2} &= -\frac{n_2 -1}{2} \\ \implies n_1 -1 & = n_2 -1 \\ \implies n_1 &= n_2\end{align} Case 2: \begin{align}f(n_1) &= f(n_2) \\ \implies \frac{n_1}{2} &= \frac{n_2}{2} \\ \implies n_1 &= n_2\end{align} I am, however, experiencing some difficulty showing the injective property for the $3^{rd}$ case. Can anyone please give me some assistance with that specific case?","Question: Is countable? My attemp so far: Let us create the following one-to-one correspondence between and . In order for to be countable we must define a function so that . To show that we require to be bijective. From the picture above, we can clearly see that is surjective since such that . Hence we must now show that is injective. We need to consider three cases: are odd are even is odd and is even Case 1: Case 2: I am, however, experiencing some difficulty showing the injective property for the case. Can anyone please give me some assistance with that specific case?","\mathbb{Z}= \{\dots, -3, -2, -1, 0 ,1 ,2 , 3, \dots \} \mathbb{Z} \mathbb{N} \begin{matrix}1 & 2 & 3 & 4 & 5 & 6 & 7 & \dots \\ \updownarrow & \updownarrow & \updownarrow & \updownarrow & \updownarrow & \updownarrow & \updownarrow & \dots\\0 & 1 & -1 & 2 & -2 & 3 & -3 & \dots\end{matrix} \mathbb{Z} f:\mathbb{N} \to \mathbb{Z} \mathbb{Z} \sim \mathbb{N} \displaystyle f(n) = \begin{cases} \frac{n}{2} & n \text{ is even} \\ -\frac{n-1}{2} & n\text{ is odd} \end{cases} \mathbb{Z} \sim \mathbb{N} f f \forall z \in \mathbb{Z}\  \exists n \in \mathbb{N} f(n) = z f n_1, n_2 n_1, n_2 n_1 n_2 \begin{align}f(n_1) &= f(n_2) \\ \implies -\frac{n_1 -1}{2} &= -\frac{n_2 -1}{2} \\ \implies n_1 -1 & = n_2 -1 \\ \implies n_1 &= n_2\end{align} \begin{align}f(n_1) &= f(n_2) \\ \implies \frac{n_1}{2} &= \frac{n_2}{2} \\ \implies n_1 &= n_2\end{align} 3^{rd}","['real-analysis', 'elementary-set-theory']"
86,Lebesgue space and weak Lebesgue space,Lebesgue space and weak Lebesgue space,,"Let $1\le p<\infty$. We define the weak Lebesgue space $wL^p(\mathbb{R}^d)$ as the set of all measurable functions $f$ on $\mathbb{R}^d$ such that \begin{equation} \|f\|_{wL^p}=\sup_{\gamma>0} \gamma (\{x\in \mathbb{R}^d : |f(x)|>\gamma  \})^{1/p}<\infty. \end{equation} By Chebyshev inequality, we have $\|f\|_{wL^p}\le \|f\|_{L^p}$ for every $f\in L^p$. My question is as follows: Let $1\le p, q <\infty$. Could we obtain some conditions for $p$ and $q$ such that  \begin{equation} \|f\|_{L^p} \le C_1\|f\|_{wL^q} \end{equation} for every $f \in wL^q$ or  \begin{equation} \|f\|_{wL^p} \le C_2\|f\|_{L^q} \end{equation} for every $f \in L^q$. Here, we denote by $C_1,C_2$ the positive constants that independent to $f$. Edit: Maybe we may restrict the question to bounded subset $K$ of $\mathbb{R}^n$, that is,  \begin{equation} \|f\|_{L^p(K)} \le C_1\|f\|_{wL^q(K)} \end{equation} for every $f \in wL^q$ or  \begin{equation} \|f\|_{wL^p(K)} \le C_2\|f\|_{L^q(K)} \end{equation} for every $f \in L^q$.","Let $1\le p<\infty$. We define the weak Lebesgue space $wL^p(\mathbb{R}^d)$ as the set of all measurable functions $f$ on $\mathbb{R}^d$ such that \begin{equation} \|f\|_{wL^p}=\sup_{\gamma>0} \gamma (\{x\in \mathbb{R}^d : |f(x)|>\gamma  \})^{1/p}<\infty. \end{equation} By Chebyshev inequality, we have $\|f\|_{wL^p}\le \|f\|_{L^p}$ for every $f\in L^p$. My question is as follows: Let $1\le p, q <\infty$. Could we obtain some conditions for $p$ and $q$ such that  \begin{equation} \|f\|_{L^p} \le C_1\|f\|_{wL^q} \end{equation} for every $f \in wL^q$ or  \begin{equation} \|f\|_{wL^p} \le C_2\|f\|_{L^q} \end{equation} for every $f \in L^q$. Here, we denote by $C_1,C_2$ the positive constants that independent to $f$. Edit: Maybe we may restrict the question to bounded subset $K$ of $\mathbb{R}^n$, that is,  \begin{equation} \|f\|_{L^p(K)} \le C_1\|f\|_{wL^q(K)} \end{equation} for every $f \in wL^q$ or  \begin{equation} \|f\|_{wL^p(K)} \le C_2\|f\|_{L^q(K)} \end{equation} for every $f \in L^q$.",,"['real-analysis', 'functional-analysis', 'measure-theory', 'lebesgue-integral', 'lebesgue-measure']"
87,"$f$ is a non-constant polynomial, $A $ is a set of measure zero, Is this true that $m(f^{-1}A)=0$, where $m$ stands for the Lebesgue measure.","is a non-constant polynomial,  is a set of measure zero, Is this true that , where  stands for the Lebesgue measure.",f A  m(f^{-1}A)=0 m,"Let $f$ be a non-constant polynomial, and let $A \subset \mathbb{R}$ be a set of measure zero, Is this true that $m(f^{-1}A)=0$, where $m$ stands for the Lebesgue measure. If $A$ is a countable set, it is easy to see that $Cardinality (f^{-1}A) \leq Cardinality (\mathbb{N} * \mathbb{N})$ and it means that $m(f^{-1}A)=0$. My problem in proving that this is true, is sets like Cantor set, whose measure is zero but is uncountable. Any hints or ideas is appreciated. Thanks !","Let $f$ be a non-constant polynomial, and let $A \subset \mathbb{R}$ be a set of measure zero, Is this true that $m(f^{-1}A)=0$, where $m$ stands for the Lebesgue measure. If $A$ is a countable set, it is easy to see that $Cardinality (f^{-1}A) \leq Cardinality (\mathbb{N} * \mathbb{N})$ and it means that $m(f^{-1}A)=0$. My problem in proving that this is true, is sets like Cantor set, whose measure is zero but is uncountable. Any hints or ideas is appreciated. Thanks !",,"['real-analysis', 'measure-theory']"
88,Rigorous separation of variables.,Rigorous separation of variables.,,"Let $I \subseteq \mathbb{R}$ denote an open, non-empty subinterval of the real line. We're given functions: $$f : I \rightarrow \mathbb{R}, \;\;g : \mathbb{R} \rightarrow \mathbb{R}.$$ Now suppose we're interested in finding the set $A$, defined below, by the method of separation of variables. Basically, this involves finding the sets $B$ and $C$ (also defined below) and then using the fact that $A = B \cup C$. Question. What constraints on $f$ and $g$ allow us to conclude that $A = B \cup C$? Definitions. $A = $ the set of all differentiable functions $y : I \rightarrow \mathbb{R}$ satisfying $$y'(x) = f (x) g(y(x)).$$ $B = $ as above, except we demand the stronger condition: $$\forall x \in I, g(y(x)) \neq 0\qquad \frac{y'(x)}{g(y(x))} = f(x).$$ $C = $ the set of all constant functions $y : I \rightarrow \mathbb{R}$ of the form $$y(x) = c,$$ such that $c \in \mathbb{R}$ satisfies $g(c)=0$.","Let $I \subseteq \mathbb{R}$ denote an open, non-empty subinterval of the real line. We're given functions: $$f : I \rightarrow \mathbb{R}, \;\;g : \mathbb{R} \rightarrow \mathbb{R}.$$ Now suppose we're interested in finding the set $A$, defined below, by the method of separation of variables. Basically, this involves finding the sets $B$ and $C$ (also defined below) and then using the fact that $A = B \cup C$. Question. What constraints on $f$ and $g$ allow us to conclude that $A = B \cup C$? Definitions. $A = $ the set of all differentiable functions $y : I \rightarrow \mathbb{R}$ satisfying $$y'(x) = f (x) g(y(x)).$$ $B = $ as above, except we demand the stronger condition: $$\forall x \in I, g(y(x)) \neq 0\qquad \frac{y'(x)}{g(y(x))} = f(x).$$ $C = $ the set of all constant functions $y : I \rightarrow \mathbb{R}$ of the form $$y(x) = c,$$ such that $c \in \mathbb{R}$ satisfies $g(c)=0$.",,"['real-analysis', 'ordinary-differential-equations']"
89,Osgood condition,Osgood condition,,"Let $h$ and $g$ be continuous, non-decreasing and concave functions in the interval $[0,\infty)$ with $h(0)=g(0)=0$ and $h(x)>0$ and $g(x)>0$ for $x>0$ such that both satisfy the Osgood condition $$\int_{0+}\frac{dx}{f(x)}=\infty.$$ Does there exist a concave function $F$ such that $F(x)\geq h(x)$ and $F(x)\geq g(x)$ for all $x$, and satisfies the Osgood condition?","Let $h$ and $g$ be continuous, non-decreasing and concave functions in the interval $[0,\infty)$ with $h(0)=g(0)=0$ and $h(x)>0$ and $g(x)>0$ for $x>0$ such that both satisfy the Osgood condition $$\int_{0+}\frac{dx}{f(x)}=\infty.$$ Does there exist a concave function $F$ such that $F(x)\geq h(x)$ and $F(x)\geq g(x)$ for all $x$, and satisfies the Osgood condition?",,"['real-analysis', 'functional-analysis', 'ordinary-differential-equations']"
90,"Outer Measure of the complement of a Vitali Set in [0,1] equal to 1","Outer Measure of the complement of a Vitali Set in [0,1] equal to 1",,"I am trying to prove the first part of exercise 33, ch. 1 in Stein and Shakarchi ( Real Analysis ). I am running into some difficulties following the hint though. Here is the problem (note, $N$ is a Vitali set constructed in $[0,1]$): Show that the set $[0,1]-N$ has outer measure $m_*(N^c)=1$. [Hint: argue by contradiction, and pick a measurable set such that $N^c \subset U \subset [0,1]$ and $m_*(U) \le 1-\epsilon$. I know that both $N$ and its complement are not measurable, so neither are countable. I know that measurable subsets of non-measurable sets have measure 0. I am not sure how to proceed given the proof though. A point to note: the book does not work with the inner measure at all, and even though I have used the inner measure in a previous course, I do not think I am allowed to for this proof.","I am trying to prove the first part of exercise 33, ch. 1 in Stein and Shakarchi ( Real Analysis ). I am running into some difficulties following the hint though. Here is the problem (note, $N$ is a Vitali set constructed in $[0,1]$): Show that the set $[0,1]-N$ has outer measure $m_*(N^c)=1$. [Hint: argue by contradiction, and pick a measurable set such that $N^c \subset U \subset [0,1]$ and $m_*(U) \le 1-\epsilon$. I know that both $N$ and its complement are not measurable, so neither are countable. I know that measurable subsets of non-measurable sets have measure 0. I am not sure how to proceed given the proof though. A point to note: the book does not work with the inner measure at all, and even though I have used the inner measure in a previous course, I do not think I am allowed to for this proof.",,"['real-analysis', 'measure-theory']"
91,Show that a limit exists,Show that a limit exists,,"Let $f : (0, \infty) \rightarrow \mathbb{R}$ be differentiable and suppose that $|f(x)| \leq \frac{C}{x^k}$ ($k$ is non-negative) and this inequality would not hold for a smaller $k$ (even if you change $C$). Suppose this also holds for $|f'(x)|$ but with a possibly different $C$, but same $k$. Show that $\lim_{x \rightarrow 0^+} f(x)$ exists.","Let $f : (0, \infty) \rightarrow \mathbb{R}$ be differentiable and suppose that $|f(x)| \leq \frac{C}{x^k}$ ($k$ is non-negative) and this inequality would not hold for a smaller $k$ (even if you change $C$). Suppose this also holds for $|f'(x)|$ but with a possibly different $C$, but same $k$. Show that $\lim_{x \rightarrow 0^+} f(x)$ exists.",,"['real-analysis', 'limits']"
92,Characterization of real functions which have limit at each point,Characterization of real functions which have limit at each point,,"The following problem is Exercise 7.K from the book van Rooij-Schikhof: A Second Course on Real Functions and it is very close to a question which was recently discussed in chat . So I thought that sharing this interesting problem with other MSE users could be useful. Here's the problem: Let $L$ be the set of all functions $f\colon [0,1]\to\mathbb R$ that have the property that $\lim\limits_{x\to a} f(x)$ exists for all $a \in [0, 1]$.   Show that: (i) $L$ is a vector space. Each $f \in L$ is bounded. (ii) For each $f \in L$, define $f^c(x): = \lim\limits_{y\to x} f(y)$ ($x \in [0, 1]$). $f^c$ is continuous. (iii) '$f^c =0$' is equivalent to 'there exist $x_1, x_2, \dots$ in $[0,1]$ and $a_1, a_2,\dots$ in $U$ with   $\lim\limits_{n\to\infty} a_n = 0$, such that $f(x_n) = a_n$ for every $n$, and $f=0$ elsewhere'. (iv) Describe the general form of an element of $L$. Show that every $f\in L$ is Riemann integrable. The original question in the chat was about functions $\mathbb R\to\mathbb R$, but it does not change much in the parts (iii) and (iv).","The following problem is Exercise 7.K from the book van Rooij-Schikhof: A Second Course on Real Functions and it is very close to a question which was recently discussed in chat . So I thought that sharing this interesting problem with other MSE users could be useful. Here's the problem: Let $L$ be the set of all functions $f\colon [0,1]\to\mathbb R$ that have the property that $\lim\limits_{x\to a} f(x)$ exists for all $a \in [0, 1]$.   Show that: (i) $L$ is a vector space. Each $f \in L$ is bounded. (ii) For each $f \in L$, define $f^c(x): = \lim\limits_{y\to x} f(y)$ ($x \in [0, 1]$). $f^c$ is continuous. (iii) '$f^c =0$' is equivalent to 'there exist $x_1, x_2, \dots$ in $[0,1]$ and $a_1, a_2,\dots$ in $U$ with   $\lim\limits_{n\to\infty} a_n = 0$, such that $f(x_n) = a_n$ for every $n$, and $f=0$ elsewhere'. (iv) Describe the general form of an element of $L$. Show that every $f\in L$ is Riemann integrable. The original question in the chat was about functions $\mathbb R\to\mathbb R$, but it does not change much in the parts (iii) and (iv).",,['real-analysis']
93,What are the integrals defined for $\mathbb{R}^n$?,What are the integrals defined for ?,\mathbb{R}^n,"In multivariate calculus , I was wondering what types of integrals are studied? Here are my naive view: Multiple integral : If I understand correctly, it is just a plain generalization of Riemann integral on $\mathbb{R}$ to on $\mathbb{R^n}$. Integral of differential forms : It is something I am not able to truly understand. Is it a special kind of Lebesgue integral? Does its definition rely on measure? When trying to compare them together, I have some further questions: Are multiple integrals and integrals of differential forms two different types of integrals? Do they belong to some common type of integral, similarly to that Riemann integral and Lebesgue-Stieltjes integrals both belong to Lebesgue integral? How are they related? Is it correct that multiple integrals have no orientation involved, but an integral of differential forms does, in the sense of changing the order of dummy variables in $dx_1 dx_2$ will or will not change the integral? What type of integral is used in vector calculus , for topics such as gradient, divergence, curl, Laplacian, the gradient theorem, Green's theorem, Stokes' theorem, divergence theorem? Are the line integral, surface integral and volume integral defined as belonging to Lebesgue integrals or Riemann integrals, integrals of differential forms, or something else? Do their definitions rely on measure? Thanks and regards!","In multivariate calculus , I was wondering what types of integrals are studied? Here are my naive view: Multiple integral : If I understand correctly, it is just a plain generalization of Riemann integral on $\mathbb{R}$ to on $\mathbb{R^n}$. Integral of differential forms : It is something I am not able to truly understand. Is it a special kind of Lebesgue integral? Does its definition rely on measure? When trying to compare them together, I have some further questions: Are multiple integrals and integrals of differential forms two different types of integrals? Do they belong to some common type of integral, similarly to that Riemann integral and Lebesgue-Stieltjes integrals both belong to Lebesgue integral? How are they related? Is it correct that multiple integrals have no orientation involved, but an integral of differential forms does, in the sense of changing the order of dummy variables in $dx_1 dx_2$ will or will not change the integral? What type of integral is used in vector calculus , for topics such as gradient, divergence, curl, Laplacian, the gradient theorem, Green's theorem, Stokes' theorem, divergence theorem? Are the line integral, surface integral and volume integral defined as belonging to Lebesgue integrals or Riemann integrals, integrals of differential forms, or something else? Do their definitions rely on measure? Thanks and regards!",,"['real-analysis', 'measure-theory', 'lebesgue-integral']"
94,How does one come up with the continued fraction for the arctangent? $\arctan x=\frac{x}{1+}\frac{x^2}{3+}\frac{(2x)^2}{5+}\cdots$,How does one come up with the continued fraction for the arctangent?,\arctan x=\frac{x}{1+}\frac{x^2}{3+}\frac{(2x)^2}{5+}\cdots,"$\newcommand{\K}{\operatorname{\large{K}}}$ The question has already been asked here . However, I find the accepted answer unsatisfactory since it details all the parts which are, in my opinion, trivial, and the one step it doesn't elaborate on is precisely the step I consider non-trivial. I am a bit stuck, so I'd like to ask here. I’ll be using Gauss’ Kettenbrücher K-notation for continued fractions, analogous to the $\sum,\prod$ notations. We know that: $$\arctan x=x-\frac{x^3}{3}+\frac{x^5}{5}-\cdots$$ For $|x|\lt1$ and we know Euler's continued fraction formula: $$a_0\left(1+\K_{n=1}^\infty\frac{-a_n}{1+a_n}\right)^{-1}=a_0+a_0a_1+a_0a_1a_2+\cdots$$ So it is quite easy to find $a_0=x,\,a_n=-\frac{2n-1}{2n+1}x^2$ as suitable for the arctangent, to get: $$\arctan x=x\left(1+\K_{n=1}^\infty\frac{\frac{2n-1}{2n+1}x^2}{1-\frac{2n-1}{2n+1}x^2}\right)^{-1}=\cfrac{x}{1+\cfrac{\frac{1}{3}x^2}{1-\frac{1}{3}x^2+\cfrac{\frac{3}{5}x^2}{1-\frac{3}{5}x^2+\cdots}}}$$ And by clearing the denominators we obtain: $$\tag{1}\arctan x=\cfrac{x}{1+\cfrac{x^2}{3-x^2+\cfrac{(3x)^2}{5-3x^2+\cdots}}}$$ But the accepted answer to the linked post claims a different fraction follows. Apparently, if one uses Euler's formula - with the same coefficients I used - it is a matter of simple algebra to arrive at: $$\tag{2}x\left(1+\K_{n=1}^\infty\frac{\frac{2n-1}{2n+1}x^2}{1-\frac{2n-1}{2n+1}x^2}\right)^{-1}\overset{?}{=}\cfrac{x}{1+\cfrac{x^2}{3+\cfrac{(2x)^2}{5+\cfrac{(3x)^2}{7+\cdots}}}}$$ This is corroborated by Wikipedia. Unfortunately I am at a complete loss as to how to go from $(1)$ to $(2)$ . One thing I have noticed - it suffices to show: $$\K_{n=1}^\infty\frac{(nx)^2}{2n+1}=\K_{n=1}^\infty\frac{((2n-1)x)^2}{(2n+1)-(2n-1)x^2}=\K_{n=1}^\infty\frac{(n^2x-(n-1)^2x)^2}{(n+1)^2-n^2-(nx)^2+((n-1)x)^2}$$ As $2n-1=n^2-(n-1)^2$ , which seems half-promising, but I just don't see the trick needed to procede. Other than equivalence transformations given by clearing denominators, I don't know how to manipulate continued fractions. What are we supposed to do here? Note: I see that it is also possible to prove this result, and many others, using the Gauss continued fraction identities for the hypergeometric functions. I have never studied the hypergeometric functions: if you can answer using them, I’d really appreciate it if the answer was written in such a way that avoids the general theory. But, that shouldn’t be necessary, since the accepted answer to the linked post claims the result follows from “algebra”.","The question has already been asked here . However, I find the accepted answer unsatisfactory since it details all the parts which are, in my opinion, trivial, and the one step it doesn't elaborate on is precisely the step I consider non-trivial. I am a bit stuck, so I'd like to ask here. I’ll be using Gauss’ Kettenbrücher K-notation for continued fractions, analogous to the notations. We know that: For and we know Euler's continued fraction formula: So it is quite easy to find as suitable for the arctangent, to get: And by clearing the denominators we obtain: But the accepted answer to the linked post claims a different fraction follows. Apparently, if one uses Euler's formula - with the same coefficients I used - it is a matter of simple algebra to arrive at: This is corroborated by Wikipedia. Unfortunately I am at a complete loss as to how to go from to . One thing I have noticed - it suffices to show: As , which seems half-promising, but I just don't see the trick needed to procede. Other than equivalence transformations given by clearing denominators, I don't know how to manipulate continued fractions. What are we supposed to do here? Note: I see that it is also possible to prove this result, and many others, using the Gauss continued fraction identities for the hypergeometric functions. I have never studied the hypergeometric functions: if you can answer using them, I’d really appreciate it if the answer was written in such a way that avoids the general theory. But, that shouldn’t be necessary, since the accepted answer to the linked post claims the result follows from “algebra”.","\newcommand{\K}{\operatorname{\large{K}}} \sum,\prod \arctan x=x-\frac{x^3}{3}+\frac{x^5}{5}-\cdots |x|\lt1 a_0\left(1+\K_{n=1}^\infty\frac{-a_n}{1+a_n}\right)^{-1}=a_0+a_0a_1+a_0a_1a_2+\cdots a_0=x,\,a_n=-\frac{2n-1}{2n+1}x^2 \arctan x=x\left(1+\K_{n=1}^\infty\frac{\frac{2n-1}{2n+1}x^2}{1-\frac{2n-1}{2n+1}x^2}\right)^{-1}=\cfrac{x}{1+\cfrac{\frac{1}{3}x^2}{1-\frac{1}{3}x^2+\cfrac{\frac{3}{5}x^2}{1-\frac{3}{5}x^2+\cdots}}} \tag{1}\arctan x=\cfrac{x}{1+\cfrac{x^2}{3-x^2+\cfrac{(3x)^2}{5-3x^2+\cdots}}} \tag{2}x\left(1+\K_{n=1}^\infty\frac{\frac{2n-1}{2n+1}x^2}{1-\frac{2n-1}{2n+1}x^2}\right)^{-1}\overset{?}{=}\cfrac{x}{1+\cfrac{x^2}{3+\cfrac{(2x)^2}{5+\cfrac{(3x)^2}{7+\cdots}}}} (1) (2) \K_{n=1}^\infty\frac{(nx)^2}{2n+1}=\K_{n=1}^\infty\frac{((2n-1)x)^2}{(2n+1)-(2n-1)x^2}=\K_{n=1}^\infty\frac{(n^2x-(n-1)^2x)^2}{(n+1)^2-n^2-(nx)^2+((n-1)x)^2} 2n-1=n^2-(n-1)^2","['real-analysis', 'limits', 'trigonometry', 'proof-explanation', 'continued-fractions']"
95,Mean Value Theorem Integrals,Mean Value Theorem Integrals,,"Let $f,g:[a,b]\to\mathbb{R}$ be smooth and integrable. Then , there exists an $x_0\in[a,b]$ with $$ \int_a^b f(x)g(x)dx=f(x_0)\cdot\int_a^b g(x)dx.$$ Is there any way of approximating $x_0$ , without evaluating $\int_a^b f(x)g(x)dx$ ? We may assume $g$ to be positive and monotone increasing and $\int_a^b g(x)dx$ to be known. Setting $g(x)\equiv1$ , we obtain $$ \int_a^b f(x)dx=f(x_0)\cdot(b-a),$$ so finding this $x_0$ numerically may be a strong tool in approximating any integral?","Let be smooth and integrable. Then , there exists an with Is there any way of approximating , without evaluating ? We may assume to be positive and monotone increasing and to be known. Setting , we obtain so finding this numerically may be a strong tool in approximating any integral?","f,g:[a,b]\to\mathbb{R} x_0\in[a,b]  \int_a^b f(x)g(x)dx=f(x_0)\cdot\int_a^b g(x)dx. x_0 \int_a^b f(x)g(x)dx g \int_a^b g(x)dx g(x)\equiv1  \int_a^b f(x)dx=f(x_0)\cdot(b-a), x_0","['real-analysis', 'integration', 'definite-integrals', 'numerical-methods', 'approximation']"
96,"Study continuity of $f(x) = |x|$ for $x\in\Bbb R \setminus \Bbb Q$, $f(x) = \frac{qx}{q+1}$ for $x\in\Bbb Q$.","Study continuity of  for ,  for .",f(x) = |x| x\in\Bbb R \setminus \Bbb Q f(x) = \frac{qx}{q+1} x\in\Bbb Q,"Study continuity of the following function: $$ f(x) = \begin{cases} |x|,\ \text{if $x$ is irrational}\\ \frac{qx}{q+1},\ \text{if}\ x = {p\over q}, q\in\Bbb N, p\in\Bbb Z, p\perp q \end{cases} $$ I've been recently studying some similar functions. The usual trick was to consider different sequences $x_n$ and then study the behavior of $f(x)$ as $x_n$ approaches some point $x_0$ . I wasn't able to apply the same trick for this function but here some intuition though, which I want to formalize somehow. If we take any sequence $\{x_n\}_{n\in\Bbb N}$ of irrational numbers such that: $$ \lim_{n\to\infty}x_n = x_0 $$ Then: $$ \forall x_n \in\Bbb R\setminus \Bbb Q:f(x_n) = |x_n| $$ In such case: $$ \lim_{n\to\infty} f(x_n) = |x_0| $$ So it looks like the function is continuous at every irrational point. For the rational ones, I was trying to use a similar approach. Let $\{y_n\}$ be a sequence of rational numbers such that for $y_n \in\Bbb Q$ and $y_0\in\Bbb R\setminus\Bbb Q$ : $$ \lim_{n\to\infty}y_n = y_0 $$ But: $$ \lim_{n\to\infty}f(y_n) \ne f(y_0) = |y_0| $$ Now every neighborhood of a given point in $\Bbb R$ contains infinitely many rationals and irrationals. So we might approximate $y_0$ with points from $y_n$ closer and closer to $y_0$ so if we introduce a $\{q_n\}$ denoting consequent denominators from $p\over q$ then it is going to grow and eventually: $$ \lim_{n\to\infty}{q_n\over q_n + 1} = 1 $$ So looks like every rational point is a point of removable discontinuity at least for $x \ge 0$ . My problem is with putting down a rigorous proof behind that intuition. Could you please help me with that?","Study continuity of the following function: I've been recently studying some similar functions. The usual trick was to consider different sequences and then study the behavior of as approaches some point . I wasn't able to apply the same trick for this function but here some intuition though, which I want to formalize somehow. If we take any sequence of irrational numbers such that: Then: In such case: So it looks like the function is continuous at every irrational point. For the rational ones, I was trying to use a similar approach. Let be a sequence of rational numbers such that for and : But: Now every neighborhood of a given point in contains infinitely many rationals and irrationals. So we might approximate with points from closer and closer to so if we introduce a denoting consequent denominators from then it is going to grow and eventually: So looks like every rational point is a point of removable discontinuity at least for . My problem is with putting down a rigorous proof behind that intuition. Could you please help me with that?","
f(x) = \begin{cases}
|x|,\ \text{if x is irrational}\\
\frac{qx}{q+1},\ \text{if}\ x = {p\over q}, q\in\Bbb N, p\in\Bbb Z, p\perp q
\end{cases}
 x_n f(x) x_n x_0 \{x_n\}_{n\in\Bbb N} 
\lim_{n\to\infty}x_n = x_0
 
\forall x_n \in\Bbb R\setminus \Bbb Q:f(x_n) = |x_n|
 
\lim_{n\to\infty} f(x_n) = |x_0|
 \{y_n\} y_n \in\Bbb Q y_0\in\Bbb R\setminus\Bbb Q 
\lim_{n\to\infty}y_n = y_0
 
\lim_{n\to\infty}f(y_n) \ne f(y_0) = |y_0|
 \Bbb R y_0 y_n y_0 \{q_n\} p\over q 
\lim_{n\to\infty}{q_n\over q_n + 1} = 1
 x \ge 0","['real-analysis', 'calculus', 'continuity']"
97,"Is the function $A \mapsto \sum\limits_{j=0}^{\infty} \langle A^j v, A^j v \rangle$ differentiable everywhere?",Is the function  differentiable everywhere?,"A \mapsto \sum\limits_{j=0}^{\infty} \langle A^j v, A^j v \rangle","Suppose $v \in \mathbb R^n$ is a fixed vector. We define a scalar-valued function on $n \times n$ matrices $f: M_n(\mathbb R) \to \mathbb R$ by \begin{align*} A \mapsto \sum\limits_{j=0}^{\infty} \langle A^j v, A^j v \rangle. \end{align*} Let us denote the domain of $f$ by $\text{Dom}(f) = \{A \in M_n(\mathbb R): f(A) < \infty\}$ . It is clear if $\rho(A) < 1$ (spectral radius), then $A \in \text{Dom}(f)$ . If I am not mistaken, $f$ should also be differentiable on $\{A: \rho(A) < 1\}$ . On the other hand, if $\rho(A) \ge 1$ , it is still possible $A \in \text{Dom}(f)$ . For example, if $v$ is chosen to be an eigenvector corresponding to an eigenvalue strictly smaller than $1$ . The question bugs me is: could the function be differentiable on the set $\text{Dom(f)} \setminus \{A:\rho(A) < 1\}$ .","Suppose is a fixed vector. We define a scalar-valued function on matrices by Let us denote the domain of by . It is clear if (spectral radius), then . If I am not mistaken, should also be differentiable on . On the other hand, if , it is still possible . For example, if is chosen to be an eigenvector corresponding to an eigenvalue strictly smaller than . The question bugs me is: could the function be differentiable on the set .","v \in \mathbb R^n n \times n f: M_n(\mathbb R) \to \mathbb R \begin{align*} A \mapsto \sum\limits_{j=0}^{\infty} \langle A^j v, A^j v \rangle. \end{align*} f \text{Dom}(f) = \{A \in M_n(\mathbb R): f(A) < \infty\} \rho(A) < 1 A \in \text{Dom}(f) f \{A: \rho(A) < 1\} \rho(A) \ge 1 A \in \text{Dom}(f) v 1 \text{Dom(f)} \setminus \{A:\rho(A) < 1\}","['real-analysis', 'linear-algebra', 'matrix-analysis']"
98,Baby Rudin 1.14 is essentially Thales's theorem?,Baby Rudin 1.14 is essentially Thales's theorem?,,"Exercise 1.14 in Baby Rudin asks to compute $$ \mid 1+z\mid^2+\mid 1-z\mid^2 $$ for (arbitrary) complex $z$ lying on the unit circle. This evaluates to $4$. Consider the triangle $0,1+z,1-z$ and shift it to the left by $1$. We get a triangle inscribed in a circle with one of its sides lying on a diagonal. Conversely, if we start with any triangle inscribed in a circle lying on a diagonal we can embed the figure in $\mathbb{C}$, so that the image of the circle is the unit circle, and the vertices of the triangle are $-1,z,-z$ for some $z\in\mathbb{C}$. Now, shift the triangle to the right by 1. Two of its sides become $1+z$ and $1-z$. Our computation shows that this triangle is a right triangle. This result is known as the Thales's theorem. Question 1. Is this a well-known proof of the Thales's theorem? Question 2. Did the author actually mean this to be ""discovered""? If yes, then should one expect to find a lot of such ""purposefully hidden"" stuff in Rudin's books?","Exercise 1.14 in Baby Rudin asks to compute $$ \mid 1+z\mid^2+\mid 1-z\mid^2 $$ for (arbitrary) complex $z$ lying on the unit circle. This evaluates to $4$. Consider the triangle $0,1+z,1-z$ and shift it to the left by $1$. We get a triangle inscribed in a circle with one of its sides lying on a diagonal. Conversely, if we start with any triangle inscribed in a circle lying on a diagonal we can embed the figure in $\mathbb{C}$, so that the image of the circle is the unit circle, and the vertices of the triangle are $-1,z,-z$ for some $z\in\mathbb{C}$. Now, shift the triangle to the right by 1. Two of its sides become $1+z$ and $1-z$. Our computation shows that this triangle is a right triangle. This result is known as the Thales's theorem. Question 1. Is this a well-known proof of the Thales's theorem? Question 2. Did the author actually mean this to be ""discovered""? If yes, then should one expect to find a lot of such ""purposefully hidden"" stuff in Rudin's books?",,"['real-analysis', 'geometry', 'complex-numbers', 'soft-question']"
99,Prove that the equation $x^2=y$ has a unique solution,Prove that the equation  has a unique solution,x^2=y,"If $y$ is a real number greater than zero, there is only one real number $x$ greater than zero such that: $$x^2=y$$ The similar question For every real $x>0$ and every integer $n>0$, there is one and only one real $y>0$ such that $y^n=x$ has been asked, but the answers reach a depth greater than I'm comfortable with for what seems to be a more basic proof. My strategy is: Suppose that there are positive numbers $a$ and $b$ each of whose square is $c$, then: $$0=a^2-b^2=(a-b)(a+b)$$ Since since $a+b$ is greater than $0$, it follows that $a=b$. q.e.d. Is this a good strategy/does it accurately prove the proof? The textbook I've been reading from has mentioned this general proposition twice and mentioned proving it, but later on, however I felt it was strange as it seems we already have the tools to prove it.","If $y$ is a real number greater than zero, there is only one real number $x$ greater than zero such that: $$x^2=y$$ The similar question For every real $x>0$ and every integer $n>0$, there is one and only one real $y>0$ such that $y^n=x$ has been asked, but the answers reach a depth greater than I'm comfortable with for what seems to be a more basic proof. My strategy is: Suppose that there are positive numbers $a$ and $b$ each of whose square is $c$, then: $$0=a^2-b^2=(a-b)(a+b)$$ Since since $a+b$ is greater than $0$, it follows that $a=b$. q.e.d. Is this a good strategy/does it accurately prove the proof? The textbook I've been reading from has mentioned this general proposition twice and mentioned proving it, but later on, however I felt it was strange as it seems we already have the tools to prove it.",,"['real-analysis', 'proof-verification']"
