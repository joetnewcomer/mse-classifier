,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,Understanding proof that the gradient of a scalar field transforms as a vector under rotations.,Understanding proof that the gradient of a scalar field transforms as a vector under rotations.,,"Chapter 1 of Griffiths' Electrodynamics is called ""Vector Analysis"". There is a problem in that chapter that I would like to understand in detail. A question about this particular problem has been asked before , but it doesn't go into the types of details I will go through and ask about in the current question. Problem 1.14 Suppose that $f$ is a function of two variables $y$ and $z$ only. Show that the gradient $\nabla f=(\partial f/\partial  y)\hat{y}+(\partial f/\partial z)\hat{z}$ transforms as a vector under rotations, Eq. 1.29. Hint: $(\partial f/\partial\bar{y}) = (\partial f/\partial y)(\partial  y/\partial \bar{y})+(\partial f/\partial z)(\partial z/\partial  \bar{y})$ , and the analogous formula for $\partial f/\partial  \bar{z}$ . We know that $\bar{y}=y\cos{\phi}+z\sin{\phi}$ and $\bar{z}=-y\sin{\phi}+z\cos{\phi}$ ; ""solve"" these equations for $y$ and $z$ (as functions of $\bar{y}$ and $\bar{z}$ ), and compute the needed derivatives $\partial y/\partial\bar{y}$ , $\partial  z/\partial\bar{y}$ , etc. The cited equation 1.29 is a matrix equation for transforming coordinates in one set of axes to coordinates in another set of axes that is rotated by $\phi$ radians relative to the first coordinates. $$\begin{pmatrix} \overline{y}\\ \overline{z} \end{pmatrix}=\begin{pmatrix} \cos{\phi} & \sin{\phi} \\ -\sin{\phi} & \cos{\phi} \end{pmatrix}\begin{pmatrix} y \\ z \end{pmatrix}\tag{1}$$ From the hint, it seems we are asked to figure out what $\partial f/\partial\bar{y}$ and $\partial f/\partial\bar{z}$ are, and to verify that they satisfy the relationship $$\begin{pmatrix} \partial f/\partial\bar{y}\\ \partial f/\partial\bar{z} \end{pmatrix}=\begin{pmatrix} \cos{\phi} & \sin{\phi} \\ -\sin{\phi} & \cos{\phi} \end{pmatrix}\begin{pmatrix} \partial f/\partial y\\ \partial f/\partial z \end{pmatrix}\tag{2}$$ That is, $$\overline{\nabla f}=\begin{pmatrix} \cos{\phi} & \sin{\phi} \\ -\sin{\phi} & \cos{\phi} \end{pmatrix}\nabla f\tag{3}$$ If these two vectors do satisfy this relationship, then it means that they behave as expected under the rotation transformation. But what is happening at the linear algebra level here? My first question is: what do $\frac{\partial f}{\partial\overline{y}}$ and $\frac{\partial f}{\partial\overline{z}}$ mean exactly? The square matrix, let's call it $R$ , in (1) is the transformation. The columns are the coordinates of the transformed standard basis vectors $\hat{i}$ and $\hat{j}$ , namely $R\hat{i}$ and $R\hat{j}$ , and these form a basis for the range of the transformation. If we stick the gradient vector on the right hand side of (1) we are transforming the vector and obtaining coordinates in the new basis. This happens by taking the same linear combination used with the old basis, but now with the new basis. As far as I can tell $\frac{\partial f}{\partial\overline{y}}$ and $\frac{\partial f}{\partial\overline{z}}$ are technically the partial derivatives of $f$ under the new basis (that is, if we were to figure out what $f(\overline{y},\overline{z})$ is). Thus, what (3) says is that this new gradient turns out to have the same coordinates under the new basis as under the old basis, which is how all vectors behave under a linear transformation (is this correct?). That is, by showing (3) we are showing that gradients are just regular ol' vectors like any others in its vector space. My second question is: why, ex ante, would there be a possibility that these two vectors would not satisfy this relationship? Here are the calculations I did to accomplish this task Let $$g(\overline{y}, \overline{z})=f(y(\overline{y}, \overline{z}),z(\overline{y}, \overline{z}))$$ That is, $f$ as a function of coordinates in the new basis. $$\frac{\partial g}{\partial \overline{y}}=\frac{\partial f}{\partial \overline{y}}\tag{4}$$ $$\frac{\partial g}{\partial \overline{z}}=\frac{\partial f}{\partial \overline{z}}\tag{5}$$ Therefore, if we can find these partial derivatives of $g$ we will have found $\overline{\nabla f}$ , and we hope that this is an expression in terms of $\nabla f$ . Using the chain rule, we have $$\frac{\partial g}{\partial \overline{y}}=\nabla f(y(\overline{y},\overline{z}),z(\overline{y},\overline{z}))\cdot \left ( \frac{\partial y}{\partial\overline{y}}\hat{i} +\frac{\partial z}{\partial\overline{y}}\hat{j} \right )\tag{6}$$ $$=\frac{\partial f}{\partial y}\frac{\partial y}{\partial \overline{y}}+\frac{\partial f}{\partial z}\frac{\partial z}{\partial \overline{y}}\tag{7}$$ Similarly, $$\frac{\partial g}{\partial \overline{z}}=\nabla f(y(\overline{y},\overline{z}),z(\overline{y},\overline{z}))\cdot \left ( \frac{\partial y}{\partial\overline{z}}\hat{i} +\frac{\partial z}{\partial\overline{z}}\hat{j} \right )\tag{8}$$ $$=\frac{\partial f}{\partial y}\frac{\partial y}{\partial \overline{z}}+\frac{\partial f}{\partial z}\frac{\partial z}{\partial \overline{z}}\tag{9}$$ Note that we know $\overline{y}(y,z)$ and $\overline{z}(y,z)$ because this is given by (1), but we don't know $y(\overline{y},\overline{z})$ or $z(\overline{y},\overline{z})$ . This is why we need to solve for $y$ and $z$ in (1), and when we do this we obtain $$y=-\overline{z}\sin{\phi}+\overline{y}\cos{\phi}$$ $$z=\overline{y}\sin{\phi}+\overline{z}\cos{\phi}$$ From which we can compute $$\frac{\partial y}{\partial\overline{y}}=\cos{\phi}$$ $$\frac{\partial y}{\partial\overline{z}}=-\sin{\phi}$$ $$\frac{\partial z}{\partial\overline{y}}=\sin{\phi}$$ $$\frac{\partial z}{\partial\overline{z}}=\cos{\phi}$$ and plugging these into (7) and (9) we obtain $$\frac{\partial f}{\partial\overline{y}}=\frac{\partial f}{\partial y}\cos{\phi}+\frac{\partial f}{\partial z}\sin{\phi}\tag{7}$$ $$\frac{\partial f}{\partial\overline{z}}=\frac{\partial f}{\partial y}(-\sin{\phi})+\frac{\partial f}{\partial z}\cos{\phi}\tag{7}$$ Thus, we have shown by direct computation of the partial derivatives that (2) and (3) are true.","Chapter 1 of Griffiths' Electrodynamics is called ""Vector Analysis"". There is a problem in that chapter that I would like to understand in detail. A question about this particular problem has been asked before , but it doesn't go into the types of details I will go through and ask about in the current question. Problem 1.14 Suppose that is a function of two variables and only. Show that the gradient transforms as a vector under rotations, Eq. 1.29. Hint: , and the analogous formula for . We know that and ; ""solve"" these equations for and (as functions of and ), and compute the needed derivatives , , etc. The cited equation 1.29 is a matrix equation for transforming coordinates in one set of axes to coordinates in another set of axes that is rotated by radians relative to the first coordinates. From the hint, it seems we are asked to figure out what and are, and to verify that they satisfy the relationship That is, If these two vectors do satisfy this relationship, then it means that they behave as expected under the rotation transformation. But what is happening at the linear algebra level here? My first question is: what do and mean exactly? The square matrix, let's call it , in (1) is the transformation. The columns are the coordinates of the transformed standard basis vectors and , namely and , and these form a basis for the range of the transformation. If we stick the gradient vector on the right hand side of (1) we are transforming the vector and obtaining coordinates in the new basis. This happens by taking the same linear combination used with the old basis, but now with the new basis. As far as I can tell and are technically the partial derivatives of under the new basis (that is, if we were to figure out what is). Thus, what (3) says is that this new gradient turns out to have the same coordinates under the new basis as under the old basis, which is how all vectors behave under a linear transformation (is this correct?). That is, by showing (3) we are showing that gradients are just regular ol' vectors like any others in its vector space. My second question is: why, ex ante, would there be a possibility that these two vectors would not satisfy this relationship? Here are the calculations I did to accomplish this task Let That is, as a function of coordinates in the new basis. Therefore, if we can find these partial derivatives of we will have found , and we hope that this is an expression in terms of . Using the chain rule, we have Similarly, Note that we know and because this is given by (1), but we don't know or . This is why we need to solve for and in (1), and when we do this we obtain From which we can compute and plugging these into (7) and (9) we obtain Thus, we have shown by direct computation of the partial derivatives that (2) and (3) are true.","f y z \nabla f=(\partial f/\partial
 y)\hat{y}+(\partial f/\partial z)\hat{z} (\partial f/\partial\bar{y}) = (\partial f/\partial y)(\partial
 y/\partial \bar{y})+(\partial f/\partial z)(\partial z/\partial
 \bar{y}) \partial f/\partial
 \bar{z} \bar{y}=y\cos{\phi}+z\sin{\phi} \bar{z}=-y\sin{\phi}+z\cos{\phi} y z \bar{y} \bar{z} \partial y/\partial\bar{y} \partial
 z/\partial\bar{y} \phi \begin{pmatrix}
\overline{y}\\ \overline{z}
\end{pmatrix}=\begin{pmatrix}
\cos{\phi} & \sin{\phi} \\ -\sin{\phi} & \cos{\phi}
\end{pmatrix}\begin{pmatrix}
y \\ z
\end{pmatrix}\tag{1} \partial f/\partial\bar{y} \partial f/\partial\bar{z} \begin{pmatrix}
\partial f/\partial\bar{y}\\ \partial f/\partial\bar{z}
\end{pmatrix}=\begin{pmatrix}
\cos{\phi} & \sin{\phi} \\ -\sin{\phi} & \cos{\phi}
\end{pmatrix}\begin{pmatrix}
\partial f/\partial y\\ \partial f/\partial z
\end{pmatrix}\tag{2} \overline{\nabla f}=\begin{pmatrix}
\cos{\phi} & \sin{\phi} \\ -\sin{\phi} & \cos{\phi}
\end{pmatrix}\nabla f\tag{3} \frac{\partial f}{\partial\overline{y}} \frac{\partial f}{\partial\overline{z}} R \hat{i} \hat{j} R\hat{i} R\hat{j} \frac{\partial f}{\partial\overline{y}} \frac{\partial f}{\partial\overline{z}} f f(\overline{y},\overline{z}) g(\overline{y}, \overline{z})=f(y(\overline{y}, \overline{z}),z(\overline{y}, \overline{z})) f \frac{\partial g}{\partial \overline{y}}=\frac{\partial f}{\partial \overline{y}}\tag{4} \frac{\partial g}{\partial \overline{z}}=\frac{\partial f}{\partial \overline{z}}\tag{5} g \overline{\nabla f} \nabla f \frac{\partial g}{\partial \overline{y}}=\nabla f(y(\overline{y},\overline{z}),z(\overline{y},\overline{z}))\cdot \left ( \frac{\partial y}{\partial\overline{y}}\hat{i} +\frac{\partial z}{\partial\overline{y}}\hat{j} \right )\tag{6} =\frac{\partial f}{\partial y}\frac{\partial y}{\partial \overline{y}}+\frac{\partial f}{\partial z}\frac{\partial z}{\partial \overline{y}}\tag{7} \frac{\partial g}{\partial \overline{z}}=\nabla f(y(\overline{y},\overline{z}),z(\overline{y},\overline{z}))\cdot \left ( \frac{\partial y}{\partial\overline{z}}\hat{i} +\frac{\partial z}{\partial\overline{z}}\hat{j} \right )\tag{8} =\frac{\partial f}{\partial y}\frac{\partial y}{\partial \overline{z}}+\frac{\partial f}{\partial z}\frac{\partial z}{\partial \overline{z}}\tag{9} \overline{y}(y,z) \overline{z}(y,z) y(\overline{y},\overline{z}) z(\overline{y},\overline{z}) y z y=-\overline{z}\sin{\phi}+\overline{y}\cos{\phi} z=\overline{y}\sin{\phi}+\overline{z}\cos{\phi} \frac{\partial y}{\partial\overline{y}}=\cos{\phi} \frac{\partial y}{\partial\overline{z}}=-\sin{\phi} \frac{\partial z}{\partial\overline{y}}=\sin{\phi} \frac{\partial z}{\partial\overline{z}}=\cos{\phi} \frac{\partial f}{\partial\overline{y}}=\frac{\partial f}{\partial y}\cos{\phi}+\frac{\partial f}{\partial z}\sin{\phi}\tag{7} \frac{\partial f}{\partial\overline{z}}=\frac{\partial f}{\partial y}(-\sin{\phi})+\frac{\partial f}{\partial z}\cos{\phi}\tag{7}","['linear-algebra', 'multivariable-calculus', 'vector-analysis', 'vector-fields']"
1,Does the implicit function theorem if it holds give a curve in $\mathbb{R}^2$ (or in $\mathbb{R}^3$)?,Does the implicit function theorem if it holds give a curve in  (or in )?,\mathbb{R}^2 \mathbb{R}^3,"Prove that the equations $\begin{cases}e^x+2e^{y}+e^z=4\\ x+y+z=0\end{cases}$ define a curve in space and the curve doesn't cross itself. I let $z=-x-y$ and define $f(x,y)=e^x+2e^y+e^{-x-y}=4$ . My idea is to show that the graph of $f$ is a curve that doesn't cross itself. Then projecting it onto the plane $x+y+z=0$ will be a curve in $\mathbb{R}^3$ that also doesn't cross itself. That the graph of $f$ is a curve can supposedly be shown by the implicit function theorem. If I haven't understood the following incorrectly Does statement of Implicit Function Theorem imply that level set is a curve? We have that $f_x=e^x-e^{-x-y}$ and $f_y=2e^y-e^{-x-y}$ . These are zero only when: $\begin{cases} e^x=e^{-x-y}\\ 2e^y=e^{-x-y} \end{cases}\iff \begin{cases}-2x=y\\ \ln 2+2y=-x\end{cases}$ . The first equations has solutions $(x,-2x)$ , $x\in\mathbb{R}$ and putting it in the second yields $\ln2=3x$ which gives the critical point $\left(\frac{\ln2}{3},\frac{-2ln2}{3}\right)$ . Since $f\left(\frac{\ln2}{3},\frac{-2ln2}{3}\right)\neq 4$ by the implicit function theorem the graph of $f$ is a injective curve(it doesn't cross itself). Does the implicit function theorem actually give a curve? Consider $x^2-y^2=2$ , the critical point for this equation is $(0,0)$ but the ""curve"" is a hyperbola. Now with the above in mind, projecting the hyperbola onto the plane gives two split curves in $\mathbb{R^3}$ , is this still considered a curve and if not how can I be sure in my solution? Edit: Not cross itself means no self-intersection. I don't know anything else this is a question from my exam in calc 3. The solution given (it literally says advice on how to solve it). Let $f(x, y, z) = e^x + 2e^y + e^z$ och $g(x, y, z) = x + y + z$ . We see that $x=z=y=0$ is a solution, thus the suspected curve contains atleast a point. We have that $\nabla f(x,y,z,)=(e^x,2e^y,e^z)$ and $\nabla g(x,y,z)=(1,1,1)$ . The vector product of the gradients is then $\nabla f \times \nabla g=(2e^y − e^z, e^z-e^x,e^x-2e^y)$ . This vector is 0 if and only if $e^x=2e^y=e^z$ . If $(x,y,z)$ is such a point with $g(x,y,z)=0$ then $x=z=(1/3)\ln2$ and $y=-(2/3)\ln2$ . Then $f(x,y,z)\neq 4$ . This shows that $\nabla g$ and $\nabla f$ thever are parallel if $g(x,y,z)=0$ and $f(x,y,z)=4$ and therefore by the implicit function theorem a curve in space .","Prove that the equations define a curve in space and the curve doesn't cross itself. I let and define . My idea is to show that the graph of is a curve that doesn't cross itself. Then projecting it onto the plane will be a curve in that also doesn't cross itself. That the graph of is a curve can supposedly be shown by the implicit function theorem. If I haven't understood the following incorrectly Does statement of Implicit Function Theorem imply that level set is a curve? We have that and . These are zero only when: . The first equations has solutions , and putting it in the second yields which gives the critical point . Since by the implicit function theorem the graph of is a injective curve(it doesn't cross itself). Does the implicit function theorem actually give a curve? Consider , the critical point for this equation is but the ""curve"" is a hyperbola. Now with the above in mind, projecting the hyperbola onto the plane gives two split curves in , is this still considered a curve and if not how can I be sure in my solution? Edit: Not cross itself means no self-intersection. I don't know anything else this is a question from my exam in calc 3. The solution given (it literally says advice on how to solve it). Let och . We see that is a solution, thus the suspected curve contains atleast a point. We have that and . The vector product of the gradients is then . This vector is 0 if and only if . If is such a point with then and . Then . This shows that and thever are parallel if and and therefore by the implicit function theorem a curve in space .","\begin{cases}e^x+2e^{y}+e^z=4\\
x+y+z=0\end{cases} z=-x-y f(x,y)=e^x+2e^y+e^{-x-y}=4 f x+y+z=0 \mathbb{R}^3 f f_x=e^x-e^{-x-y} f_y=2e^y-e^{-x-y} \begin{cases}
e^x=e^{-x-y}\\
2e^y=e^{-x-y} \end{cases}\iff \begin{cases}-2x=y\\
\ln 2+2y=-x\end{cases} (x,-2x) x\in\mathbb{R} \ln2=3x \left(\frac{\ln2}{3},\frac{-2ln2}{3}\right) f\left(\frac{\ln2}{3},\frac{-2ln2}{3}\right)\neq 4 f x^2-y^2=2 (0,0) \mathbb{R^3} f(x, y, z) = e^x + 2e^y + e^z g(x, y, z) = x + y + z x=z=y=0 \nabla f(x,y,z,)=(e^x,2e^y,e^z) \nabla g(x,y,z)=(1,1,1) \nabla f \times \nabla g=(2e^y − e^z, e^z-e^x,e^x-2e^y) e^x=2e^y=e^z (x,y,z) g(x,y,z)=0 x=z=(1/3)\ln2 y=-(2/3)\ln2 f(x,y,z)\neq 4 \nabla g \nabla f g(x,y,z)=0 f(x,y,z)=4","['multivariable-calculus', 'differential-geometry']"
2,"Composition between mixed functions (scalar, vector...)","Composition between mixed functions (scalar, vector...)",,"Consider $$f(x, y) = x^2+y^2 \qquad \qquad g(t) = (2t, t^2)$$ So $f: \mathbb{R}^2 \to \mathbb{R}$ and $g: \mathbb{R}\to \mathbb{R}^2$ . I know that then $g\circ f: \mathbb{R}\to \mathbb{R}$ . If instead I want $f\circ g$ , in this case I can do it because it's like to write $g: \mathbb{R}\to \mathbb{R}^2$ and $f: \mathbb{R}^2 \to \mathbb{R}$ , so then $$f\circ g: \mathbb{R}^2\to \mathbb{R}^2$$ But in the end, I don't really know how to compose. I don't get it at all. perpahs is it $$f\circ g = f(g(t)) = (2t)^2 + (t^2)^2$$ ?? And is it $$g\circ f = g(f(x,y)) = 2t^2 + t^4$$ ?? Help me thank you!","Consider So and . I know that then . If instead I want , in this case I can do it because it's like to write and , so then But in the end, I don't really know how to compose. I don't get it at all. perpahs is it ?? And is it ?? Help me thank you!","f(x, y) = x^2+y^2 \qquad \qquad g(t) = (2t, t^2) f: \mathbb{R}^2 \to \mathbb{R} g: \mathbb{R}\to \mathbb{R}^2 g\circ f: \mathbb{R}\to \mathbb{R} f\circ g g: \mathbb{R}\to \mathbb{R}^2 f: \mathbb{R}^2 \to \mathbb{R} f\circ g: \mathbb{R}^2\to \mathbb{R}^2 f\circ g = f(g(t)) = (2t)^2 + (t^2)^2 g\circ f = g(f(x,y)) = 2t^2 + t^4","['real-analysis', 'calculus', 'multivariable-calculus', 'functions']"
3,Can a cross-product of two $C_1$ gradient fields fail to be a curl?,Can a cross-product of two  gradient fields fail to be a curl?,C_1,"Let $u,v : U \subseteq \mathbb{R}^{3} \to \mathbb{R}$ be a pair of functions of class $C^2$ defined on an open set $U$ . Is it possible to always find a field $F$ such that $\nabla u \times \nabla v = \text{curl} (F)$ ? My original intuition was that it wasn't possible to always do this and that the function $G: \mathbb{R}^{3} \setminus \{\vec{0}\} \to \mathbb{R}^{3}$ defined by $$G(x,y,z) := \left(\frac{x}{\left(x^2+y^2+z^2\right)^{3/2}},\frac{y}{\left(x^2+y^2+z^2\right)^{3/2}},\frac{z}{\left(x^2+y^2+z^2 \right)^{3/2}},\right)$$ would somehow help prove this fact.",Let be a pair of functions of class defined on an open set . Is it possible to always find a field such that ? My original intuition was that it wasn't possible to always do this and that the function defined by would somehow help prove this fact.,"u,v : U \subseteq \mathbb{R}^{3} \to \mathbb{R} C^2 U F \nabla u \times \nabla v = \text{curl} (F) G: \mathbb{R}^{3} \setminus \{\vec{0}\} \to \mathbb{R}^{3} G(x,y,z) := \left(\frac{x}{\left(x^2+y^2+z^2\right)^{3/2}},\frac{y}{\left(x^2+y^2+z^2\right)^{3/2}},\frac{z}{\left(x^2+y^2+z^2 \right)^{3/2}},\right)","['multivariable-calculus', 'differential-geometry']"
4,Find all the values that define an implicit function,Find all the values that define an implicit function,,"Let $h\colon\mathbb{R}^2\to\mathbb{R}$ defined by $h(x,y)=x^2+y^3+xy+x^3+ay$ (where $a\in\mathbb{R}$ ). a) Find the values of $a$ for which $h(x,y)=0$ defines $y$ as a $\mathscr{C}^1$ implicit function of $x$ in some open neighborhood $U\times V$ of $(0,0).$ b) Find the values of $a$ for which $h(x,y)=0$ defines $x$ as a $\mathscr{C}^1$ implicit function of $y$ in some open neighborhood of $(0,0)$ . c) Let $f$ be the implicit function $U\to\mathbb{R}$ found in question a), and let $F\colon U\times\mathbb{R}\to\mathbb{R}$ defined as $$F(x,y)=\big(e^{x+y}+x^2-1\mathbin,f(x)+y\cos(x)\big).$$ Show that $F$ has an inverse $F^{-1}$ , of class $\mathscr{C}^1$ , defined in some neighborhood of $(0,0)$ . Show that $G:=F\circ F+F^{-1}$ is differentiable at $(0,0)$ , and calculate ${\rm D}G(0,0)$ . Hi everyone, I'm having problems with this exercise, in the first part, what I did was using the implicit function theorem to show that if $a \neq 0$ , then y=f(x) and bla, bla. BUT, if $a \neq 0$ , I don't know how to procede, 'cause the differential isn't invertible, and that isn't enough to prove that x doesn't define y as an implicit function. Same goes for the second one, cause the differential is equal to zero, and I can't use the implicit function theorem. So, any ideas of how I should proceed?","Let defined by (where ). a) Find the values of for which defines as a implicit function of in some open neighborhood of b) Find the values of for which defines as a implicit function of in some open neighborhood of . c) Let be the implicit function found in question a), and let defined as Show that has an inverse , of class , defined in some neighborhood of . Show that is differentiable at , and calculate . Hi everyone, I'm having problems with this exercise, in the first part, what I did was using the implicit function theorem to show that if , then y=f(x) and bla, bla. BUT, if , I don't know how to procede, 'cause the differential isn't invertible, and that isn't enough to prove that x doesn't define y as an implicit function. Same goes for the second one, cause the differential is equal to zero, and I can't use the implicit function theorem. So, any ideas of how I should proceed?","h\colon\mathbb{R}^2\to\mathbb{R} h(x,y)=x^2+y^3+xy+x^3+ay a\in\mathbb{R} a h(x,y)=0 y \mathscr{C}^1 x U\times V (0,0). a h(x,y)=0 x \mathscr{C}^1 y (0,0) f U\to\mathbb{R} F\colon U\times\mathbb{R}\to\mathbb{R} F(x,y)=\big(e^{x+y}+x^2-1\mathbin,f(x)+y\cos(x)\big). F F^{-1} \mathscr{C}^1 (0,0) G:=F\circ F+F^{-1} (0,0) {\rm D}G(0,0) a \neq 0 a \neq 0","['real-analysis', 'analysis', 'multivariable-calculus', 'implicit-function-theorem', 'inverse-function-theorem']"
5,Proving: $\partial_v f(a)$ exists if and only if so does $\partial_v f_k(a)$ $(\forall k)$ (in Normed Vector Spaces).,Proving:  exists if and only if so does   (in Normed Vector Spaces).,\partial_v f(a) \partial_v f_k(a) (\forall k),"I'm rereading some sections of Spivak's Calculus on Manifolds and attempting to generalize some results (usually to Normed Vector Spaces). I'm struggling to prove the (generelized version of) the theorem below, so I thought of asking for a hand. Throughout the post let $V$ and $W$ be NVSs, with $V'\subseteq V$ open and $f$ a function $V'\to W$ . Definition: given $a\in V'$ and a vector $v\in V$ , we define (assuming the limit exists) $$\frac{\partial f}{\partial v}(a) := \lim_{t\rightarrow 0}\frac{f(a+tv)-f(a)}{t},$$ and, in the case $v=e_i$ for a basis vector $e_i$ , $$\frac{\partial f}{\partial x_i}(a) := \frac{\partial f}{\partial e_i}(a).$$ Theorem: let $\dim W = m$ so that $f(v)=(f_1(v), \ldots ,f_m(v))$ . Then $\partial_v f(a)$ exists if and only if so does $\partial_v f_k(a)$ $(\forall k)$ , in which case $$\frac{\partial f}{\partial v}(a) = \sum_{k=1}^me_k\frac{\partial f_k}{\partial v}(a).$$ Proof: \begin{equation} \begin{split} \frac{\partial f}{\partial v}(a) & = \lim_{h\to 0}\frac{f(a+hv)-f(a)}{h}\\ & = \lim_{h\to 0}\left(\sum_{k=1}^me_k\frac{f_k(a+hv)-f_k(a)}{h}\right)\\ & \stackrel{*}{=} \sum_{k=1}^me_k\left(\lim_{h\to 0}\frac{f_k(a+hv)-f_k(a)}{h}\right)\\ & = \sum_{k=1}^me_k\frac{\partial f_k}{\partial v}(a).\\ \end{split} \end{equation} In my attempt to justify the equality $\stackrel{*}{=}$ , I've come up with -what I believe is- a lemma which if true would legitimize the equality. However, now I find myself struggling to prove said lemma. Is the lemma below true? If so, how could one prove it? If not, how else could one justify the equality $\stackrel{*}{=}$ ? Lemma (?): Let $X$ be an arbitrary topological space with $p$ a limit point of $X'\subseteq X$ . Let $W$ be a topological vector space with $\{e_\alpha\}_{\alpha\in A}$ as a basis of $W$ , and let $f:X'\to W$ be a function. Then $$\lim_{x\to p} f(x) = L \iff \lim_{x\to p} f_\alpha(x) = L_\alpha \ (\forall \alpha\in A)$$ where by $f_\alpha$ (or $L_\alpha$ ) I mean the $\alpha$ component of $f$ (or $L$ ).","I'm rereading some sections of Spivak's Calculus on Manifolds and attempting to generalize some results (usually to Normed Vector Spaces). I'm struggling to prove the (generelized version of) the theorem below, so I thought of asking for a hand. Throughout the post let and be NVSs, with open and a function . Definition: given and a vector , we define (assuming the limit exists) and, in the case for a basis vector , Theorem: let so that . Then exists if and only if so does , in which case Proof: In my attempt to justify the equality , I've come up with -what I believe is- a lemma which if true would legitimize the equality. However, now I find myself struggling to prove said lemma. Is the lemma below true? If so, how could one prove it? If not, how else could one justify the equality ? Lemma (?): Let be an arbitrary topological space with a limit point of . Let be a topological vector space with as a basis of , and let be a function. Then where by (or ) I mean the component of (or ).","V W V'\subseteq V f V'\to W a\in V' v\in V \frac{\partial f}{\partial v}(a) := \lim_{t\rightarrow 0}\frac{f(a+tv)-f(a)}{t}, v=e_i e_i \frac{\partial f}{\partial x_i}(a) := \frac{\partial f}{\partial e_i}(a). \dim W = m f(v)=(f_1(v), \ldots ,f_m(v)) \partial_v f(a) \partial_v f_k(a) (\forall k) \frac{\partial f}{\partial v}(a) = \sum_{k=1}^me_k\frac{\partial f_k}{\partial v}(a). \begin{equation}
\begin{split}
\frac{\partial f}{\partial v}(a) & = \lim_{h\to 0}\frac{f(a+hv)-f(a)}{h}\\
& = \lim_{h\to 0}\left(\sum_{k=1}^me_k\frac{f_k(a+hv)-f_k(a)}{h}\right)\\
& \stackrel{*}{=} \sum_{k=1}^me_k\left(\lim_{h\to 0}\frac{f_k(a+hv)-f_k(a)}{h}\right)\\
& = \sum_{k=1}^me_k\frac{\partial f_k}{\partial v}(a).\\
\end{split}
\end{equation} \stackrel{*}{=} \stackrel{*}{=} X p X'\subseteq X W \{e_\alpha\}_{\alpha\in A} W f:X'\to W \lim_{x\to p} f(x) = L \iff \lim_{x\to p} f_\alpha(x) = L_\alpha \ (\forall \alpha\in A) f_\alpha L_\alpha \alpha f L","['analysis', 'multivariable-calculus', 'solution-verification', 'normed-spaces']"
6,Smooth partitions of unity of subsets of $\mathbb{R}^n$ with the same index set as the open cover,Smooth partitions of unity of subsets of  with the same index set as the open cover,\mathbb{R}^n,"This question is strongly related to this one , but the existing answer doesn't cover the point I will ask about below. The context I am concerned about is the following. Let $S \subseteq \mathbb{R}^n$ be some subset, and $\{U_i\}$ some open cover of $S$ . A smooth partition of unity for $S$ subordinate to the open cover $\{U_i\}$ is a collection $\{\varphi_j\}$ of smooth (i.e. $C^\infty$ ) functions $\varphi_j: V \to [0,1]$ defined on some open neighbourhood $V$ of $S$ , such that: For each $j$ , there exists some $i$ such that $\operatorname{supp} \varphi_j \subseteq U_i$ ; Each $x \in S$ has an open neighbourhood where all but finitely many of the $\varphi_j$ vanish; For all $x \in S$ , $\sum_j \varphi_j(x) = 1$ (this is a finite sum by point 2). The existence of such partitions of unity is proved, for example, in Spivak's Calculus on Manifolds (Theorem 3-11). However, sometimes we like that the partition of unity is indexed by the same set as the open cover. Is it true that we can always modify a partition of unity as defined above such that it has the same index set as the open cover? The first approach one would think of trying is something akin to the one in the linked question: choose a map from the $j$ indexes to the $i$ indexes (respecting condition 1 above) and then sum all the $\varphi_j$ corresponding to the same $i$ . The linked question was concerned about the well-definedness of this sum (it is well-defined, since it is actually a finite sum for each $x$ , and with a bit more care we can see that this gives a smooth function, at least if we construct the partition of unity following Spivak's proof). However, we still need that this sum has support contained in $U_i$ . This is clear in the case where we only have finitely many $j$ corresponding to the same $i$ , but in the infinite case it seems to me that the sum could possibly be nonzero over the whole open $U_i$ . I have a feeling that maybe this difficulty could be avoided by some clever multiplication with a cutoff function which makes the $\varphi_j$ vanish close to the boundary in a careful way, but I couldn't actually make it work.","This question is strongly related to this one , but the existing answer doesn't cover the point I will ask about below. The context I am concerned about is the following. Let be some subset, and some open cover of . A smooth partition of unity for subordinate to the open cover is a collection of smooth (i.e. ) functions defined on some open neighbourhood of , such that: For each , there exists some such that ; Each has an open neighbourhood where all but finitely many of the vanish; For all , (this is a finite sum by point 2). The existence of such partitions of unity is proved, for example, in Spivak's Calculus on Manifolds (Theorem 3-11). However, sometimes we like that the partition of unity is indexed by the same set as the open cover. Is it true that we can always modify a partition of unity as defined above such that it has the same index set as the open cover? The first approach one would think of trying is something akin to the one in the linked question: choose a map from the indexes to the indexes (respecting condition 1 above) and then sum all the corresponding to the same . The linked question was concerned about the well-definedness of this sum (it is well-defined, since it is actually a finite sum for each , and with a bit more care we can see that this gives a smooth function, at least if we construct the partition of unity following Spivak's proof). However, we still need that this sum has support contained in . This is clear in the case where we only have finitely many corresponding to the same , but in the infinite case it seems to me that the sum could possibly be nonzero over the whole open . I have a feeling that maybe this difficulty could be avoided by some clever multiplication with a cutoff function which makes the vanish close to the boundary in a careful way, but I couldn't actually make it work.","S \subseteq \mathbb{R}^n \{U_i\} S S \{U_i\} \{\varphi_j\} C^\infty \varphi_j: V \to [0,1] V S j i \operatorname{supp} \varphi_j \subseteq U_i x \in S \varphi_j x \in S \sum_j \varphi_j(x) = 1 j i \varphi_j i x U_i j i U_i \varphi_j","['real-analysis', 'multivariable-calculus', 'differential-geometry', 'differential-topology']"
7,Limit of $u(x):=\int_{\mathbb{R}^n}\frac{f(y)}{|x-y|^{n-2}}\; dy.$,Limit of,u(x):=\int_{\mathbb{R}^n}\frac{f(y)}{|x-y|^{n-2}}\; dy.,"Let $f\in C_c(\mathbb{R}^n)$ , $n\ge 3$ , be a compact support function. We consider $u(x):=\int_{\mathbb{R}^n}\frac{f(y)}{|x-y|^{n-2}}\; dy.$ How can I prove that $u(x)\to 0,\quad\text{for}\; |x|\to\infty$ ?","Let , , be a compact support function. We consider How can I prove that ?","f\in C_c(\mathbb{R}^n) n\ge 3 u(x):=\int_{\mathbb{R}^n}\frac{f(y)}{|x-y|^{n-2}}\; dy. u(x)\to 0,\quad\text{for}\; |x|\to\infty","['real-analysis', 'multivariable-calculus']"
8,Intuition behind polar coordinate to find limit in multivariate calculus,Intuition behind polar coordinate to find limit in multivariate calculus,,"Find limit using the polar coordinate for the function at $(0,0)$ $$  f(x,y) = \frac{x+y}{\sqrt{x^2+y^2}}  $$ I started using $x = r\cos(\theta),\, y = r\sin(\theta)$ Then $(x,y) \to (0,0) \implies r \to 0$ Then we get as the following $$  f\bigl(r\cos(\theta),\, r\sin(\theta)\bigr)  = \cos(\theta) + \sin(\theta)  $$ Now I have no idea how to proceed from here. Some of the ideas from youtube videos I had was: $\theta$ is a free variable and the limit is more like spiraling into $(0,0)$ . Can someone explain to me why is this happening? What is the idea behind $r \to 0$ ?",Find limit using the polar coordinate for the function at I started using Then Then we get as the following Now I have no idea how to proceed from here. Some of the ideas from youtube videos I had was: is a free variable and the limit is more like spiraling into . Can someone explain to me why is this happening? What is the idea behind ?,"(0,0)  
f(x,y) = \frac{x+y}{\sqrt{x^2+y^2}} 
 x = r\cos(\theta),\, y = r\sin(\theta) (x,y) \to (0,0) \implies r \to 0  
f\bigl(r\cos(\theta),\, r\sin(\theta)\bigr) 
= \cos(\theta) + \sin(\theta) 
 \theta (0,0) r \to 0","['calculus', 'limits', 'multivariable-calculus', 'intuition', 'polar-coordinates']"
9,mass of a body with a density function [closed],mass of a body with a density function [closed],,"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 1 year ago . Improve this question Let $V$ be the body confined by the following quadratic surfaces: $$ x^{2} + y^{2} = 12z,\quad x^{2} = 3z,\quad z = 3 $$ Compute the mass of $V$ with the density function $\rho = 1$ . My problem is I can't figure the integration boundaries $\ldots$ I know that substituting to spherical or cylindrical coordinates won't help because it says so in the question.","Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 1 year ago . Improve this question Let be the body confined by the following quadratic surfaces: Compute the mass of with the density function . My problem is I can't figure the integration boundaries I know that substituting to spherical or cylindrical coordinates won't help because it says so in the question.","V 
x^{2} + y^{2} = 12z,\quad x^{2} = 3z,\quad z = 3
 V \rho = 1 \ldots","['multivariable-calculus', 'density-function']"
10,Calculating the volume of an elliptical region,Calculating the volume of an elliptical region,,"Problem : Calculate the volume of the finite body $K$ that is limited by the surfaces $$ z=2-x^2-y^2 \\z=y^2 $$ Answer : $\pi\sqrt2$ My Attempt : The surfaces intersect when $x^2+2y^2 = 2$ Therefore, the volume is: $$ \iint_{A}(2-x^2-y^2)-(y^2)=\iint_{A}2-x^2-2y^2 $$ where $A : x^2+2y^2 \le 2$ . The variable substitution $$ u = x \\v = \sqrt2y $$ transforms the ellipsis into a circle, and $\frac{d(x, y)}{d(u,v)} = \frac{1}{\sqrt2}$ . Consequently, the volume can be written as: $$ \iint_{B}(2-u^2-v^2)\frac{1}{\sqrt2}\,dudv $$ where $B : u^2+v^2 \le 2$ Polar coordinates can now be used, and we finally have the volume: $$ \iint_{B}(2-u^2-v^2)\frac{1}{\sqrt2}\,dudv=\int_{0}^{2\pi}\int_{0}^{2}(2-r^2)\frac{r}{\sqrt2}drd\theta = 0 $$ which is clearly wrong! Where do I run off the rails?","Problem : Calculate the volume of the finite body that is limited by the surfaces Answer : My Attempt : The surfaces intersect when Therefore, the volume is: where . The variable substitution transforms the ellipsis into a circle, and . Consequently, the volume can be written as: where Polar coordinates can now be used, and we finally have the volume: which is clearly wrong! Where do I run off the rails?","K 
z=2-x^2-y^2
\\z=y^2
 \pi\sqrt2 x^2+2y^2 = 2 
\iint_{A}(2-x^2-y^2)-(y^2)=\iint_{A}2-x^2-2y^2
 A : x^2+2y^2 \le 2 
u = x
\\v = \sqrt2y
 \frac{d(x, y)}{d(u,v)} = \frac{1}{\sqrt2} 
\iint_{B}(2-u^2-v^2)\frac{1}{\sqrt2}\,dudv
 B : u^2+v^2 \le 2 
\iint_{B}(2-u^2-v^2)\frac{1}{\sqrt2}\,dudv=\int_{0}^{2\pi}\int_{0}^{2}(2-r^2)\frac{r}{\sqrt2}drd\theta = 0
",['multivariable-calculus']
11,"$\int_{0\leq v\leq 2\pi,-1\leq t\leq 1}f(at+\sqrt{1-t^2}(b\cos v+c\sin v))dtdv=2\pi\int_{-1}^{1}f(u\sqrt{a^2+b^2+c^2})du$",,"\int_{0\leq v\leq 2\pi,-1\leq t\leq 1}f(at+\sqrt{1-t^2}(b\cos v+c\sin v))dtdv=2\pi\int_{-1}^{1}f(u\sqrt{a^2+b^2+c^2})du","Suppose $a, b, c$ are given real numbers, such that all of them are not zero. Let $f:\mathbb{R}\rightarrow\mathbb{R}$ be a continuous function on $[-\sqrt{{a}^{2}+{b}^{2}+{c}^{2}},\sqrt{{a}^{2}+{b}^{2}+{c}^{2}}]$ , prove that $$\int_{0\leq v\leq 2\pi,-1\leq t\leq 1}f(at+\sqrt{1-t^2}(b\cos v+c\sin v))dtdv=2\pi\int_{-1}^{1}f(u\sqrt{a^2+b^2+c^2})du$$ I have noticed that $$LHS=\int_{0\leq v\leq 2\pi,-1\leq t\leq 1}f(at+\sqrt{1-t^2}\sqrt{{b}^{2}+{c}^{2}}\mathrm{sin}(v+\alpha ))dtdv$$ $$={\int }_{-1}^{1}({\int }_{0}^{2\pi }f(at+\sqrt{1-t^2}\sqrt{{b}^{2}+{c}^{2}}\mathrm{sin}\left(v+\alpha \right))dv)dt$$ (By Fubini's theorem) $$={\int }_{-1}^{1}({\int }_{0}^{2\pi }f(at+\sqrt{1-t^2}\sqrt{{b}^{2}+{c}^{2}}\mathrm{sin}v)dv)dt$$ (Since $f(x+2\pi)=f(x)$ ) But then what can be done to complete the proof?","Suppose are given real numbers, such that all of them are not zero. Let be a continuous function on , prove that I have noticed that (By Fubini's theorem) (Since ) But then what can be done to complete the proof?","a, b, c f:\mathbb{R}\rightarrow\mathbb{R} [-\sqrt{{a}^{2}+{b}^{2}+{c}^{2}},\sqrt{{a}^{2}+{b}^{2}+{c}^{2}}] \int_{0\leq v\leq 2\pi,-1\leq t\leq 1}f(at+\sqrt{1-t^2}(b\cos v+c\sin v))dtdv=2\pi\int_{-1}^{1}f(u\sqrt{a^2+b^2+c^2})du LHS=\int_{0\leq v\leq 2\pi,-1\leq t\leq 1}f(at+\sqrt{1-t^2}\sqrt{{b}^{2}+{c}^{2}}\mathrm{sin}(v+\alpha ))dtdv ={\int }_{-1}^{1}({\int }_{0}^{2\pi }f(at+\sqrt{1-t^2}\sqrt{{b}^{2}+{c}^{2}}\mathrm{sin}\left(v+\alpha \right))dv)dt ={\int }_{-1}^{1}({\int }_{0}^{2\pi }f(at+\sqrt{1-t^2}\sqrt{{b}^{2}+{c}^{2}}\mathrm{sin}v)dv)dt f(x+2\pi)=f(x)","['calculus', 'multivariable-calculus', 'definite-integrals']"
12,A(n easy ?) partial differential equation,A(n easy ?) partial differential equation,,"This might be a classic partial differential equation, but I couldn't find anything on the Internet: Find all the functions $f: \mathbb{R}^n \to \mathbb{R}$ verifying $ \displaystyle\sum\limits_{k=1}^n x_k \frac{\partial f}{\partial x_k}(x) = 0 $ . I do understand this means that $\forall x \in \mathbb{R}^n, \langle \nabla f(x), x \rangle = 0$ , but I still do not see how to proceed. Thank you for your help.","This might be a classic partial differential equation, but I couldn't find anything on the Internet: Find all the functions verifying . I do understand this means that , but I still do not see how to proceed. Thank you for your help.","f: \mathbb{R}^n \to \mathbb{R}  \displaystyle\sum\limits_{k=1}^n x_k \frac{\partial f}{\partial x_k}(x) = 0  \forall x \in \mathbb{R}^n, \langle \nabla f(x), x \rangle = 0","['multivariable-calculus', 'partial-differential-equations']"
13,Derivative of $f(x)\cdot x$ where $f$ is a $\mathbb{R}^3$ function: how to compute it?,Derivative of  where  is a  function: how to compute it?,f(x)\cdot x f \mathbb{R}^3,"Let $f:\mathbb{R}^3\to\mathbb{R}^3$ . Could someone please help me to compute the the derivative of $$f(x)\cdot x?$$ If $f$ would be a real valued function I think it should be the derivative of a product, but how to proceed in this case? Thank you in advance.","Let . Could someone please help me to compute the the derivative of If would be a real valued function I think it should be the derivative of a product, but how to proceed in this case? Thank you in advance.",f:\mathbb{R}^3\to\mathbb{R}^3 f(x)\cdot x? f,"['real-analysis', 'calculus', 'multivariable-calculus', 'derivatives']"
14,"Compute the triple integral over the region $x^2+y^2+z^2 \leq 2x$, $z\leq 0$","Compute the triple integral over the region ,",x^2+y^2+z^2 \leq 2x z\leq 0,"Compute the triple integral over the region $D = \{x^{2} + y^{2} + z^{2} \leq 2x,\, z\leq 0\}$ : $$ \iiint_{D}\left(y^{2}z + x\right){\rm d}V $$ I'm struggling a great deal with setting up the integration bounds. From playing around, I think the best way to go about it is using spherical coordinates as opposed to cylindrical coordinates. It's easy to make a quick sketch of the $xy$ plane to deduce that we have to be below the line $y=2x$ , however then we must find the angle of intersection with the line $y=2x$ with the circle $x^2+y^2=1$ , and using polar coordinates, we can find that it corresponds to $\arctan2$ and then of course $\arctan2 + \pi$ , and by $z \leq 0$ , we ought to obtain that $\frac{\pi}{2}\leq\phi \leq \pi$ . I'm unsure about the $R$ bound (and everything previously stated to be honest), but using the provided inequality, we would obtain that $R \leq2\cos\theta \sin\phi$ , but by my sketch it would make more sense to have $0 \leq R \leq1$ . Thus, my suggested integral would look something like: $$\int_{\arctan{2}+\pi}^{\arctan{2}}d\theta\int_{\frac{\pi}{2}}^{\pi}d\phi\int_{0}^1(R^2 \sin{\phi})(R^3\sin^2{\theta}\sin^2{\phi}\cos{\phi+R\cos{\theta}\sin{\phi}})dR$$ which is obviously a nightmare. Any help/ hints on how to establish the bounds would be very appreciated!","Compute the triple integral over the region : I'm struggling a great deal with setting up the integration bounds. From playing around, I think the best way to go about it is using spherical coordinates as opposed to cylindrical coordinates. It's easy to make a quick sketch of the plane to deduce that we have to be below the line , however then we must find the angle of intersection with the line with the circle , and using polar coordinates, we can find that it corresponds to and then of course , and by , we ought to obtain that . I'm unsure about the bound (and everything previously stated to be honest), but using the provided inequality, we would obtain that , but by my sketch it would make more sense to have . Thus, my suggested integral would look something like: which is obviously a nightmare. Any help/ hints on how to establish the bounds would be very appreciated!","D = \{x^{2} + y^{2} + z^{2} \leq 2x,\, z\leq 0\} 
\iiint_{D}\left(y^{2}z + x\right){\rm d}V
 xy y=2x y=2x x^2+y^2=1 \arctan2 \arctan2 + \pi z \leq 0 \frac{\pi}{2}\leq\phi \leq \pi R R \leq2\cos\theta \sin\phi 0 \leq R \leq1 \int_{\arctan{2}+\pi}^{\arctan{2}}d\theta\int_{\frac{\pi}{2}}^{\pi}d\phi\int_{0}^1(R^2 \sin{\phi})(R^3\sin^2{\theta}\sin^2{\phi}\cos{\phi+R\cos{\theta}\sin{\phi}})dR","['calculus', 'integration', 'multivariable-calculus', 'definite-integrals', 'multiple-integral']"
15,How to show this mapping from the upper half space to the unit ball is a bijection,How to show this mapping from the upper half space to the unit ball is a bijection,,"I want to show this mapping from the upper half space to the unit ball is a bijection: So one thing I tried is to just use the direct definitions of being injective and surjective, but it didn't take too long to get an algebraic mess. But then I was starting to suspect this mapping is in fact the real and imaginary parts of some Mobius transform in complex analysis. However, finding out the coefficients of the Mobius transform is also not an easy task, but is there a way to do so?","I want to show this mapping from the upper half space to the unit ball is a bijection: So one thing I tried is to just use the direct definitions of being injective and surjective, but it didn't take too long to get an algebraic mess. But then I was starting to suspect this mapping is in fact the real and imaginary parts of some Mobius transform in complex analysis. However, finding out the coefficients of the Mobius transform is also not an easy task, but is there a way to do so?",,"['complex-analysis', 'multivariable-calculus']"
16,"Evaluate $\lim_{(x,y)\to (0,0)}{\frac{x^3 y}{x^2 - y}}$.",Evaluate .,"\lim_{(x,y)\to (0,0)}{\frac{x^3 y}{x^2 - y}}","The way I did it is that $$\left|\frac{x^3 y}{x^2 - y}\right|\leq \left|\frac{x^3}{x^2 - y^2}\right|$$ for small values of $x$ . Hence, if the limit of the expression on the right exists, then the limit of the expression on the left is the same by the Squeeze Theorem. Since the limit goes to $(0,0)$ , we can use a polar coordinate substitution. Thus, let $x=r\cos \theta$ and $y = r\sin \theta$ . Then \begin{align*} \lim_{(x,y)\to (0,0)}{\left|\frac{x^3}{x^2 - y^2}\right|} &= \lim_{r\to 0}{\left|\frac{r^3 \cos^3 \theta}{r^2\left(\cos^2 \theta - \sin^2 \theta\right)}\right|} \\ &=\frac{\cos^3 \theta}{\cos 2\theta}\cdot \lim_{r\to 0}{r} \\ &=0, \end{align*} meaning the original limit (in the title) is also $0$ , by the Squeeze Theorem. As far as I can tell, this works. However, I wanted to know if there is a simpler way to do this problem, especially since the ""small $x$ "" criterion does not have a 'nice' boundary, such as $x\in (-1,1)$ . So any advice on alternative ways of doing this problem, maybe with the limit laws, would be appreciated.","The way I did it is that for small values of . Hence, if the limit of the expression on the right exists, then the limit of the expression on the left is the same by the Squeeze Theorem. Since the limit goes to , we can use a polar coordinate substitution. Thus, let and . Then meaning the original limit (in the title) is also , by the Squeeze Theorem. As far as I can tell, this works. However, I wanted to know if there is a simpler way to do this problem, especially since the ""small "" criterion does not have a 'nice' boundary, such as . So any advice on alternative ways of doing this problem, maybe with the limit laws, would be appreciated.","\left|\frac{x^3 y}{x^2 - y}\right|\leq \left|\frac{x^3}{x^2 - y^2}\right| x (0,0) x=r\cos \theta y = r\sin \theta \begin{align*}
\lim_{(x,y)\to (0,0)}{\left|\frac{x^3}{x^2 - y^2}\right|}
&= \lim_{r\to 0}{\left|\frac{r^3 \cos^3 \theta}{r^2\left(\cos^2 \theta - \sin^2 \theta\right)}\right|}
\\
&=\frac{\cos^3 \theta}{\cos 2\theta}\cdot \lim_{r\to 0}{r}
\\
&=0,
\end{align*} 0 x x\in (-1,1)","['limits', 'multivariable-calculus']"
17,Intuition behind writing the Laplacian operator in different ways: $\nabla^2 u$ versus $\nabla \cdot \nabla u$ [duplicate],Intuition behind writing the Laplacian operator in different ways:  versus  [duplicate],\nabla^2 u \nabla \cdot \nabla u,"This question already has answers here : Intuitive interpretation of the Laplacian Operator (7 answers) Closed 1 year ago . The community reviewed whether to reopen this question 1 year ago and left it closed: Original close reason(s) were not resolved I was looking over Langtangen's book on finite volume methods, and he mentions in section 5.5.1 that the second order or Laplacian term in a PDE can be written either as: $$ \nabla^2 u \quad \text{or} \quad \nabla \cdot \nabla u = \frac{\partial^2 u}{\partial x^2} + \frac{\partial^2 u}{\partial y^2} $$ Both of these notations are analytically equivalent, but the $\nabla \cdot \nabla u$ approach seems to focus on this idea of the divergence of the gradient. I was just trying to understand the physical intuition behind defining the Laplacian as the divergence of the gradient? I understand that the Laplacian operator represents diffusion and that it looks to reduce the deviation between a point and its neighboring points. But I was not clear on how this idea relates to the ideas of divergence--which relates to flux, and gradient--which describes how a scalar valued function is changing at a point given its variables.","This question already has answers here : Intuitive interpretation of the Laplacian Operator (7 answers) Closed 1 year ago . The community reviewed whether to reopen this question 1 year ago and left it closed: Original close reason(s) were not resolved I was looking over Langtangen's book on finite volume methods, and he mentions in section 5.5.1 that the second order or Laplacian term in a PDE can be written either as: Both of these notations are analytically equivalent, but the approach seems to focus on this idea of the divergence of the gradient. I was just trying to understand the physical intuition behind defining the Laplacian as the divergence of the gradient? I understand that the Laplacian operator represents diffusion and that it looks to reduce the deviation between a point and its neighboring points. But I was not clear on how this idea relates to the ideas of divergence--which relates to flux, and gradient--which describes how a scalar valued function is changing at a point given its variables.","
\nabla^2 u \quad \text{or} \quad \nabla \cdot \nabla u = \frac{\partial^2 u}{\partial x^2} + \frac{\partial^2 u}{\partial y^2}
 \nabla \cdot \nabla u","['multivariable-calculus', 'partial-differential-equations', 'notation', 'intuition', 'divergence-operator']"
18,What did I do wrong solving this 2nd order differential equation?,What did I do wrong solving this 2nd order differential equation?,,"I'm trying to find the solution of this differential equation; $$y \cdot \frac{\partial{f}}{\partial{y}} -2x \cdot \frac{\partial{f}}{\partial{x}} = 2xy^4, \quad f(1,y) = \frac{1}{2}y^4 $$ But I think I went wrong somewhere because when I try to differentiate my solution I don't get the right answer! This is my solution, I substituted $u = xy^2$ and $v=y$ then: \begin{align*} \frac{\partial{f}}{\partial{x}} &= y^2 \cdot \frac{\partial{f}}{\partial{u}}\\[10pt]  \frac{\partial{f}}{\partial{y}} &= 2xy \cdot \frac{\partial{f}}{\partial{u}} + \frac{\partial{f}}{\partial{v}} \end{align*} Then I get: $$ \frac{\partial{f}}{\partial{v}} = 2xy^3 \implies f(v) = 2xy^3\cdot v + C(u)$$ \begin{align*} f(x,y) &= 2xy^4 + C(xy^2)\\[5pt] f(1,y) &= 2y^4 + C(y^2) = \frac{1}{2} y^2 \end{align*} $$\implies C(y^2) = \frac{-3}{2}y^4 \implies C(xy^2) = \frac{-3}{2}x^2y^4 $$ $$ f(x,y) = 2xy^4 - \frac{3}{2}x^2y^4$$ Which apparently is wrong, what did I miss here?","I'm trying to find the solution of this differential equation; But I think I went wrong somewhere because when I try to differentiate my solution I don't get the right answer! This is my solution, I substituted and then: Then I get: Which apparently is wrong, what did I miss here?","y \cdot \frac{\partial{f}}{\partial{y}} -2x \cdot \frac{\partial{f}}{\partial{x}} = 2xy^4, \quad f(1,y) = \frac{1}{2}y^4  u = xy^2 v=y \begin{align*}
\frac{\partial{f}}{\partial{x}} &= y^2 \cdot \frac{\partial{f}}{\partial{u}}\\[10pt]
 \frac{\partial{f}}{\partial{y}} &= 2xy \cdot \frac{\partial{f}}{\partial{u}} + \frac{\partial{f}}{\partial{v}}
\end{align*}  \frac{\partial{f}}{\partial{v}} = 2xy^3 \implies f(v) = 2xy^3\cdot v + C(u) \begin{align*}
f(x,y) &= 2xy^4 + C(xy^2)\\[5pt]
f(1,y) &= 2y^4 + C(y^2) = \frac{1}{2} y^2
\end{align*} \implies C(y^2) = \frac{-3}{2}y^4 \implies C(xy^2) = \frac{-3}{2}x^2y^4
  f(x,y) = 2xy^4 - \frac{3}{2}x^2y^4","['calculus', 'multivariable-calculus', 'partial-differential-equations', 'characteristics']"
19,Double Integrals - Region delimited by triangle in counterclockwise,Double Integrals - Region delimited by triangle in counterclockwise,,"A triangle with vertices (0,0), (0,1) and (1,2), counterclockwise, delimits a region. Determine an integral over this region of the following expression: $$\int_{\Omega}(x-y)dx+e^{x+y}dy$$ This kind of exercises fit the line integral, but I don't know how to start solving it, I thought I'd use Green's Theorem. The key is to integrate x first, then y. $$\int_{\Omega}(x-y)dx+e^{x+y}dy=\int_{\Omega}\left[\dfrac{\partial}{\partial x}(e^{x+y}) - \dfrac{\partial}{\partial x}(x-y)\right]\,dx\,dy =\int_{\Omega}\left[e^{x+y} +1\right]\,dx\,dy $$ What would the extremes of integration look like? I don't know how to continue","A triangle with vertices (0,0), (0,1) and (1,2), counterclockwise, delimits a region. Determine an integral over this region of the following expression: This kind of exercises fit the line integral, but I don't know how to start solving it, I thought I'd use Green's Theorem. The key is to integrate x first, then y. What would the extremes of integration look like? I don't know how to continue","\int_{\Omega}(x-y)dx+e^{x+y}dy \int_{\Omega}(x-y)dx+e^{x+y}dy=\int_{\Omega}\left[\dfrac{\partial}{\partial x}(e^{x+y}) - \dfrac{\partial}{\partial x}(x-y)\right]\,dx\,dy =\int_{\Omega}\left[e^{x+y} +1\right]\,dx\,dy ","['calculus', 'integration', 'multivariable-calculus', 'change-of-variable']"
20,Verifying the divergence theorem over B,Verifying the divergence theorem over B,,"This is the exercise: Let $B$ be the region of $\mathbb{R}^3$ $$B = {(x,y,z) | x^2+y^2+z^2 \le 1,\ z^2\geq x^2+y^2}$$ and and be the field $$\mathbf{F}(x,y,z) = (x,y,z)$$ My guess was since we are dealing with a hyperbole (top part) and a sphere, use spherical coordinates for the triple integral, and for the other part of the theorem use spherical for the sphere and cylindric for the upper cone. the triple integral, I integrate 3, which is the result of the divergence of F with $0\leq \theta\leq2\pi$ ; $0\leq\phi\leq1\pi/4$ and $0\leq\rho\leq1$ but it didn't work, results are weird and for the other part of the theorem the sum of the double integrals did not give me as the triple integral. Sorry about my english, is so bad.","This is the exercise: Let be the region of and and be the field My guess was since we are dealing with a hyperbole (top part) and a sphere, use spherical coordinates for the triple integral, and for the other part of the theorem use spherical for the sphere and cylindric for the upper cone. the triple integral, I integrate 3, which is the result of the divergence of F with ; and but it didn't work, results are weird and for the other part of the theorem the sum of the double integrals did not give me as the triple integral. Sorry about my english, is so bad.","B \mathbb{R}^3 B = {(x,y,z) | x^2+y^2+z^2 \le 1,\ z^2\geq x^2+y^2} \mathbf{F}(x,y,z) = (x,y,z) 0\leq \theta\leq2\pi 0\leq\phi\leq1\pi/4 0\leq\rho\leq1","['integration', 'multivariable-calculus', 'divergence-theorem']"
21,Proving Hadamard's Variational Formula $\dot{\lambda} = -\int_{\partial U(\tau)}\left|\frac{\partial w}{\partial v}\right|^2v\cdot \nu dS$,Proving Hadamard's Variational Formula,\dot{\lambda} = -\int_{\partial U(\tau)}\left|\frac{\partial w}{\partial v}\right|^2v\cdot \nu dS,"Setting to this question is the following problem from Evans' book Partial Differential Equations (second edition, page 369): Let $U(\tau)_{\tau \in \mathbb{R}}$ be a family of smooth bounded domains in $\mathbb{R}^n$ , which depend smoothly upon the parameter $\tau \in \mathbb{R}$ . Suppose that as $\tau$ changes each point on $\partial U(\tau)$ moves with velocity $v$ . For each $\tau$ , let us consider eigenvalues $\lambda = \lambda(\tau)$ and the corresponding eigenfunctions $w = w(x, \tau):\begin{cases}-\Delta w = \lambda w&: w \in U(\tau)\\w = 0&: w \in \partial U(\tau)\end{cases}$ normalized such that $||w||_{L^2(U(\tau))} = 1$ . Suppose that $\lambda$ and $w$ are smooth functions of $\tau$ and $x$ . Show the Hadamard's variational formula : \begin{equation}     \begin{aligned}         \frac{d\lambda}{d\tau} &= -\int_{\partial U(\tau)}\left|\frac{\partial w}{\partial \nu}\right|^2v\cdot \nu dS     \end{aligned} \end{equation} This question has already been asked in Hadamard variational formula Evans chapter 6 problem 15 , but I have some questions regarding the given explanation: https://math.stackexchange.com/a/1198921/820472 Namely, 1.) from what does it follow a priori that $\lambda(\tau) = \int_{U}w(-\Delta w)dx = \int_{U}|\nabla w|^2dx$ ? Please see the edit. 2.) I am also stuck at trying to conclude the claimed formula $\dot{\lambda} = -\int_{\partial U(\tau)}\left|\frac{\partial w}{\partial v}\right|^2v\cdot \nu dS$ . Namely, assuming the claimed equality holds for $\lambda(\tau)$ , if I apply the Leibniz formula as suggested, I get $\dot{\lambda}(\tau) = \int_{U(\tau)}(w(-\Delta)w)v \cdot \nu dS(\tau) + \int_{\partial U(\tau)}\frac{\partial}{\partial \tau}(w(-\Delta)w)dx$ . Substituting $-\Delta w = \lambda w$ gives $\dot{\lambda}(\tau) = \int_{U(\tau)}(\lambda w^2)v \cdot \nu dS(\tau) + \int_{\partial U(\tau)}\frac{\partial}{\partial \tau}(\lambda w^2)dx = \int_{U(\tau)}(\lambda w^2)v \cdot \nu dS(\tau) + \int_{\partial U(\tau)}\lambda_\tau w^2 + 2\lambda ww_\tau dx$ But as $w = 0$ on $\partial U(\tau)$ , it follows that $\dot{\lambda}(\tau) = \int_{U(\tau)}(\lambda w^2)v \cdot \nu dS(\tau)$ and I have no clue how to proceed in this branch. The other suggested equality yields $\dot{\lambda}(\tau) = \int_{U(\tau)}(|\nabla w|^2)v \cdot \nu dS(\tau) + \int_{\partial U(\tau)}\frac{\partial}{\partial \tau}(|\nabla w|^2)dx = \int_{U(\tau)}(|\nabla w|^2)v \cdot \nu dS(\tau) + \int_{\partial U(\tau)}\sum_{i=1}^n\frac{\partial}{\partial \tau}\left(\frac{\partial^2}{\partial x_i^2}w\right)^2dx$ where $x := (x_1,\dots,x_n)$ which seems almost the equality we want, provided that we can show $\int_{\partial U(\tau)}\sum_{i=1}^n\frac{\partial}{\partial \tau}\left(\frac{\partial^2}{\partial x_i^2}w\right)^2dx = \int_{\partial U(\tau)}\sum_{i=1}^n2\left(\frac{\partial^2}{\partial x_i^2}w\right)(\frac{\partial^3}{\partial \tau\partial x_i^2}w)dx = 0$ . Unfortunately I don't know how to finish the proof. Edit: I realized just after posting this question that the Hadamard's Variational Formula follows (almost) immediately after applying the Leibniz rule to the equality $\lambda(\tau) = \int_{U}|\nabla w|^2dx$ , as $w \equiv 0$ on $\partial U$ implies that $\int_{\partial U(\tau)}\sum_{i=1}^n\frac{\partial}{\partial \tau}\left(\frac{\partial^2}{\partial x_i^2}w\right)^2dx = 0$ . Therefore my renewed questions are the original 1.) and why $\nabla w || \nu \implies \left|\frac{\partial w}{\partial \nu}\right|^2 = |\nabla w|^2$ ? That is, why if the gradient is parallel to $\nu$ we have that the derivative of $w$ w.r.t. $\nu$ is the squared gradient of $w$ ?","Setting to this question is the following problem from Evans' book Partial Differential Equations (second edition, page 369): Let be a family of smooth bounded domains in , which depend smoothly upon the parameter . Suppose that as changes each point on moves with velocity . For each , let us consider eigenvalues and the corresponding eigenfunctions normalized such that . Suppose that and are smooth functions of and . Show the Hadamard's variational formula : This question has already been asked in Hadamard variational formula Evans chapter 6 problem 15 , but I have some questions regarding the given explanation: https://math.stackexchange.com/a/1198921/820472 Namely, 1.) from what does it follow a priori that ? Please see the edit. 2.) I am also stuck at trying to conclude the claimed formula . Namely, assuming the claimed equality holds for , if I apply the Leibniz formula as suggested, I get . Substituting gives But as on , it follows that and I have no clue how to proceed in this branch. The other suggested equality yields where which seems almost the equality we want, provided that we can show . Unfortunately I don't know how to finish the proof. Edit: I realized just after posting this question that the Hadamard's Variational Formula follows (almost) immediately after applying the Leibniz rule to the equality , as on implies that . Therefore my renewed questions are the original 1.) and why ? That is, why if the gradient is parallel to we have that the derivative of w.r.t. is the squared gradient of ?","U(\tau)_{\tau \in \mathbb{R}} \mathbb{R}^n \tau \in \mathbb{R} \tau \partial U(\tau) v \tau \lambda = \lambda(\tau) w = w(x, \tau):\begin{cases}-\Delta w = \lambda w&: w \in U(\tau)\\w = 0&: w \in \partial U(\tau)\end{cases} ||w||_{L^2(U(\tau))} = 1 \lambda w \tau x \begin{equation}
    \begin{aligned}
        \frac{d\lambda}{d\tau} &= -\int_{\partial U(\tau)}\left|\frac{\partial w}{\partial \nu}\right|^2v\cdot \nu dS
    \end{aligned}
\end{equation} \lambda(\tau) = \int_{U}w(-\Delta w)dx = \int_{U}|\nabla w|^2dx \dot{\lambda} = -\int_{\partial U(\tau)}\left|\frac{\partial w}{\partial v}\right|^2v\cdot \nu dS \lambda(\tau) \dot{\lambda}(\tau) = \int_{U(\tau)}(w(-\Delta)w)v \cdot \nu dS(\tau) + \int_{\partial U(\tau)}\frac{\partial}{\partial \tau}(w(-\Delta)w)dx -\Delta w = \lambda w \dot{\lambda}(\tau) = \int_{U(\tau)}(\lambda w^2)v \cdot \nu dS(\tau) + \int_{\partial U(\tau)}\frac{\partial}{\partial \tau}(\lambda w^2)dx = \int_{U(\tau)}(\lambda w^2)v \cdot \nu dS(\tau) + \int_{\partial U(\tau)}\lambda_\tau w^2 + 2\lambda ww_\tau dx w = 0 \partial U(\tau) \dot{\lambda}(\tau) = \int_{U(\tau)}(\lambda w^2)v \cdot \nu dS(\tau) \dot{\lambda}(\tau) = \int_{U(\tau)}(|\nabla w|^2)v \cdot \nu dS(\tau) + \int_{\partial U(\tau)}\frac{\partial}{\partial \tau}(|\nabla w|^2)dx = \int_{U(\tau)}(|\nabla w|^2)v \cdot \nu dS(\tau) + \int_{\partial U(\tau)}\sum_{i=1}^n\frac{\partial}{\partial \tau}\left(\frac{\partial^2}{\partial x_i^2}w\right)^2dx x := (x_1,\dots,x_n) \int_{\partial U(\tau)}\sum_{i=1}^n\frac{\partial}{\partial \tau}\left(\frac{\partial^2}{\partial x_i^2}w\right)^2dx = \int_{\partial U(\tau)}\sum_{i=1}^n2\left(\frac{\partial^2}{\partial x_i^2}w\right)(\frac{\partial^3}{\partial \tau\partial x_i^2}w)dx = 0 \lambda(\tau) = \int_{U}|\nabla w|^2dx w \equiv 0 \partial U \int_{\partial U(\tau)}\sum_{i=1}^n\frac{\partial}{\partial \tau}\left(\frac{\partial^2}{\partial x_i^2}w\right)^2dx = 0 \nabla w || \nu \implies \left|\frac{\partial w}{\partial \nu}\right|^2 = |\nabla w|^2 \nu w \nu w","['multivariable-calculus', 'differential-geometry', 'partial-differential-equations', 'divergence-theorem']"
22,Image of non-linear function $\mathbb{R}^2 \to \mathbb{R}^2$,Image of non-linear function,\mathbb{R}^2 \to \mathbb{R}^2,"I have the function $F:\mathbb{R}^2 \to \mathbb{R}^2$ defined $F(x,y)=(x^2-y-1,y)$ and wish to find its image (range, whatever): the subset $F(\mathbb{R}^2)=\{F(x,y)|(x,y) \in \mathbb{R}^2\} \subseteq \mathbb{R}^2.$ Well, $\{F(x,y)|(x,y) \in \mathbb{R}^2\}=\{(x^2-y-1,y)|(x,y) \in \mathbb{R}^2\},$ so I am looking for ordered pairs $(u,v)$ satisfying $(u,v)=(x^2-y-1,y)$ . I got that $(x,y)=(\pm \sqrt{u + v + 1},v),$ but am not sure where to go from here, or if these first steps are even correct. I have also noticed that $F(-x,y)=F(x,y)$ , but I'm not sure what to do with this. What is the general procedure for these problems? When things are linear I know exactly what to do.","I have the function defined and wish to find its image (range, whatever): the subset Well, so I am looking for ordered pairs satisfying . I got that but am not sure where to go from here, or if these first steps are even correct. I have also noticed that , but I'm not sure what to do with this. What is the general procedure for these problems? When things are linear I know exactly what to do.","F:\mathbb{R}^2 \to \mathbb{R}^2 F(x,y)=(x^2-y-1,y) F(\mathbb{R}^2)=\{F(x,y)|(x,y) \in \mathbb{R}^2\} \subseteq \mathbb{R}^2. \{F(x,y)|(x,y) \in \mathbb{R}^2\}=\{(x^2-y-1,y)|(x,y) \in \mathbb{R}^2\}, (u,v) (u,v)=(x^2-y-1,y) (x,y)=(\pm \sqrt{u + v + 1},v), F(-x,y)=F(x,y)","['calculus', 'multivariable-calculus', 'functions']"
23,Find a conservative vector field that has the indicated potential,Find a conservative vector field that has the indicated potential,,"Find a conservative vector field that has the indicated potential $$f(x,y,z)=\sin \left (x^2+y^2+z^2\right ).$$ My answer: $$2x\cos \left (x^2+y^2+z^2\right )i+2y\cos \left (x^2+y^2+z^2\right )j+2z\cos \left (x^2+y^2+z^2\right )k.$$ For this question, do I need to derivate $x$ for $i$ , $y$ for $j$ , $z$ for $k$ ?","Find a conservative vector field that has the indicated potential My answer: For this question, do I need to derivate for , for , for ?","f(x,y,z)=\sin \left (x^2+y^2+z^2\right ). 2x\cos \left (x^2+y^2+z^2\right )i+2y\cos \left (x^2+y^2+z^2\right )j+2z\cos \left (x^2+y^2+z^2\right )k. x i y j z k","['calculus', 'multivariable-calculus', 'vector-fields']"
24,How to find the equation of a plane with two identical lines?,How to find the equation of a plane with two identical lines?,,"How do you find the equation of a plane with two identical lines? Example: In each case, determine whether or not the given pair of lines intersect. Also, find all planes containing the pair of lines. $$\langle x,y,z \rangle = \langle 3,2,-2 \rangle + s \langle -2,-2,2 \rangle, \quad \langle x,y,z \rangle = \langle 2,1,-1 \rangle + s \langle 1,1,-1 \rangle.$$ I equated the two lines together but didn't get a sufficient value to continue with the problem. NOTE: This not an assignment problem it's just for practice before I start multivariable calculus.","How do you find the equation of a plane with two identical lines? Example: In each case, determine whether or not the given pair of lines intersect. Also, find all planes containing the pair of lines. I equated the two lines together but didn't get a sufficient value to continue with the problem. NOTE: This not an assignment problem it's just for practice before I start multivariable calculus.","\langle x,y,z \rangle = \langle 3,2,-2 \rangle + s \langle -2,-2,2 \rangle, \quad \langle x,y,z \rangle = \langle 2,1,-1 \rangle + s \langle 1,1,-1 \rangle.","['linear-algebra', 'multivariable-calculus']"
25,"Proving that $ \lim_{(x,y) \to (0,0)}\frac{xy^3}{x^2+2y^4} $ doesn't exist [duplicate]",Proving that  doesn't exist [duplicate]," \lim_{(x,y) \to (0,0)}\frac{xy^3}{x^2+2y^4} ","This question already has answers here : Method to prove limit in $\mathbb{R}^2$ (3 answers) Closed 2 years ago . I want to prove that $ \lim_{(x,y) \to (0,0)}\frac{xy^3}{x^2+2y^4} $ doesn't exist. Every path I choose in real plane tends to 0. Is it possible to do it that way?",This question already has answers here : Method to prove limit in $\mathbb{R}^2$ (3 answers) Closed 2 years ago . I want to prove that doesn't exist. Every path I choose in real plane tends to 0. Is it possible to do it that way?," \lim_{(x,y) \to (0,0)}\frac{xy^3}{x^2+2y^4} ","['real-analysis', 'multivariable-calculus']"
26,Is there a differentiable function $f$ with partial derivatives $f_x = y$ and $f_y = -x$?,Is there a differentiable function  with partial derivatives  and ?,f f_x = y f_y = -x,"There is $f: \mathbb{R}^2 → \mathbb{R}$ differentiable such that $∂x(x, y) = y$ and $∂y(x, y) = −x$ for all $(x, y) \in \mathbb{R}^2$ A hint on how to prove if this is true or false, please? I've been trying to find a function $f(x,y)$ to prove this, but I can't find one that is both differentiable and has $f_x = y$ and $f_y = -x$","There is differentiable such that and for all A hint on how to prove if this is true or false, please? I've been trying to find a function to prove this, but I can't find one that is both differentiable and has and","f: \mathbb{R}^2 → \mathbb{R} ∂x(x, y) = y ∂y(x, y) = −x (x, y) \in \mathbb{R}^2 f(x,y) f_x = y f_y = -x",['multivariable-calculus']
27,Why Lagrange Multiplier Doesn't Work?,Why Lagrange Multiplier Doesn't Work?,,"Question: Find maximum value $f(x,y,z) = xy + zy + xz - 4xyz$ subject to constraint $x + y + z = 1$ and $x,y,z \geq 0$ . $$ g(x,y,z) = x + y + z - 1 $$ and $$ \nabla g(x,y,z) \neq 0, \qquad  \nabla g(x,y,z) = \langle 1,1,1 \rangle.  $$ When we apply Lagrange Multiplier Method, we find $f \bigl( \frac{1}{2}, \frac{1}{4}, \frac{1}{4} \bigr) = \frac{3}{16}$ , the maximum value, but the answer is $f \bigl( 0, \frac{1}{2}, \frac{1}{2} \bigr) = \frac{1}{4}$ . Why does that happen? Lagrange Multiplier Method has a only $1$ rule: $\nabla g(x,y,z) \neq 0$ , there is no rule break. EDİT : I find f(0,1/2,1/2) with another method and f(1/2,1/4,1/4) is not both a global maxima and a global minima point but Lagrange Multiplier gives that result, and f(1/2,1/4,1/4) is not a local minimum f(0,0,1) is lower than that.","Question: Find maximum value subject to constraint and . and When we apply Lagrange Multiplier Method, we find , the maximum value, but the answer is . Why does that happen? Lagrange Multiplier Method has a only rule: , there is no rule break. EDİT : I find f(0,1/2,1/2) with another method and f(1/2,1/4,1/4) is not both a global maxima and a global minima point but Lagrange Multiplier gives that result, and f(1/2,1/4,1/4) is not a local minimum f(0,0,1) is lower than that.","f(x,y,z) = xy + zy + xz - 4xyz x + y + z = 1 x,y,z \geq 0 
g(x,y,z) = x + y + z - 1
 
\nabla g(x,y,z) \neq 0,
\qquad 
\nabla g(x,y,z) = \langle 1,1,1 \rangle. 
 f \bigl( \frac{1}{2}, \frac{1}{4}, \frac{1}{4} \bigr) = \frac{3}{16} f \bigl( 0, \frac{1}{2}, \frac{1}{2} \bigr) = \frac{1}{4} 1 \nabla g(x,y,z) \neq 0","['calculus', 'multivariable-calculus', 'derivatives', 'optimization', 'lagrange-multiplier']"
28,"Directional derivative of f(x,y)=xy at (0,0) in direction (1,1)","Directional derivative of f(x,y)=xy at (0,0) in direction (1,1)",,"Intuitively, I think of the directional derivative as the slope of the graph in a particular direction. But this intuition seems flawed. Consider the directional derivative of $f(x, y) = xy$ at the point $(0, 0)$ in the direction $(1, 1)$ . The slope of the graph in this direction should be positive, but the directional derivative is $0$ . If the directional derivative doesn't represent the slope of the graph in some direction, what does it represent? How should I think of it intuitively?","Intuitively, I think of the directional derivative as the slope of the graph in a particular direction. But this intuition seems flawed. Consider the directional derivative of at the point in the direction . The slope of the graph in this direction should be positive, but the directional derivative is . If the directional derivative doesn't represent the slope of the graph in some direction, what does it represent? How should I think of it intuitively?","f(x, y) = xy (0, 0) (1, 1) 0","['multivariable-calculus', 'derivatives', 'partial-derivative']"
29,Problems with differential geometry,Problems with differential geometry,,"I have problems with this statement: ""Let $\pi: \mathbb{R} \to S^1$ given by $\pi (x) = (\cos(x), \sin(x))$ . Let $f\colon[0, l] \to S^1$ a differentiable function where $f(t)=(f_1(t), f_2(t))$ with $f_1$ and $f_2$ real-valued functions on $[0, l]$ . Let $x_0 \in \pi^{-1} (f(0)) \subset \mathbb{R}$ and it defines $$\tilde{f} (t) = x_0 +\int_0^t (f_1 f_2' - f_2 f_1')du$$ Proof that $ \pi \circ \tilde{f} =f $ (this means that it is lifting of f)"" My questions: What is the definition of lifting? Which procedure you would suggest to me to do this proof? Sorry for my English but I'm from latinoamerica","I have problems with this statement: ""Let given by . Let a differentiable function where with and real-valued functions on . Let and it defines Proof that (this means that it is lifting of f)"" My questions: What is the definition of lifting? Which procedure you would suggest to me to do this proof? Sorry for my English but I'm from latinoamerica","\pi: \mathbb{R} \to S^1 \pi (x) = (\cos(x), \sin(x)) f\colon[0, l] \to S^1 f(t)=(f_1(t), f_2(t)) f_1 f_2 [0, l] x_0 \in \pi^{-1} (f(0)) \subset \mathbb{R} \tilde{f} (t) = x_0 +\int_0^t (f_1 f_2' - f_2 f_1')du  \pi \circ \tilde{f} =f ","['geometry', 'multivariable-calculus', 'differential-geometry', 'curves', 'frenet-frame']"
30,Volume between paraboloid $x^2 +y^2 -4a(z+a)=0$ and sphere $x^2 + y^2 +z^2 =R^2$,Volume between paraboloid  and sphere,x^2 +y^2 -4a(z+a)=0 x^2 + y^2 +z^2 =R^2,"I'm trying to obtein the volume via triple integral but think I'm setting the wrong radius. The solid in particular is bounded by the sphere $x^2 + y^2 +z^2 =R^2$ and above the parabolloid $x^2 +y^2 -4a(z+a)=0$ (consedering $R>a>0$ ). I'm setting cylindrical coordinates and I do eventually get $\theta \in [0,2\pi[$ and $(\rho^2 / 4a) -a\leq z \leq R^2-\rho^2$ . I deduce that the radius must be limited in between $0$ and $4Ra-4a^2$ (Intersection is at height $z=R-2a$ ), so the volume should be: $$\int_{0} ^{2\pi} \int_0 ^{4Ra-4a^2} \int_{(\rho^2 /4a)-a} ^{R^2 -\rho^2} \mathrm{d}V$$ , but this integral results in a different expression from the original solution. My teacher told us the solution is $2\pi \left ( \frac{a^3}{3} - aR^2 + \frac{2R^3}{3} \right )$ .","I'm trying to obtein the volume via triple integral but think I'm setting the wrong radius. The solid in particular is bounded by the sphere and above the parabolloid (consedering ). I'm setting cylindrical coordinates and I do eventually get and . I deduce that the radius must be limited in between and (Intersection is at height ), so the volume should be: , but this integral results in a different expression from the original solution. My teacher told us the solution is .","x^2 + y^2 +z^2 =R^2 x^2 +y^2 -4a(z+a)=0 R>a>0 \theta \in [0,2\pi[ (\rho^2 / 4a) -a\leq z \leq R^2-\rho^2 0 4Ra-4a^2 z=R-2a \int_{0} ^{2\pi} \int_0 ^{4Ra-4a^2} \int_{(\rho^2 /4a)-a} ^{R^2 -\rho^2} \mathrm{d}V 2\pi \left ( \frac{a^3}{3} - aR^2 + \frac{2R^3}{3} \right )","['calculus', 'integration', 'multivariable-calculus', 'volume', 'spheres']"
31,Fallacy in the proof of Lebesgue theorem in the script,Fallacy in the proof of Lebesgue theorem in the script,,"On the lectures today, another professor came and told us there is a minor omission/mistake in the proof of one direction of the Lebesgue theorem, namely $f:[a,b]\times[c,d]\to\Bbb R$ bounded and integrable implies $0$ measure set of discontinuities that we might discuss when our professor returns, so I've been thinking about it as an exercise. First, a definition and some results: $\underline{\boldsymbol{\text{definition}}}$ : Let $A\subset\Bbb R^2$ and $f:A\to\Bbb R$ any function. Oscillation $O(f,c)$ of the function $f$ at the point $c\in A$ is the infimum of the expression $\sup\{|f(x_1)-f(x_2)|:x_1,x_2\in U\cap A\}$ over all the open neighbourhoods $U\subset\Bbb R^2$ of the point $c$ . In other words $$O(f,c)=\inf_{U\ni c}\sup_{x_1,x_2\in U\cap A}|f(x_1)-f(x_2)|.$$ $\underline{\boldsymbol{\text{ lemma }} 7.1}$ : A function $f:A\to\Bbb R$ is continuous at $c\in A$ if and only if $O(f,c)=0.$ $\underline{\boldsymbol{\text{result}}}$ If $D$ is the set of discontinuities of the function $f,$ then $D=\bigcup_{\varepsilon >0}D_\varepsilon,$ where $$D_\varepsilon=\{x\in A\mid O(f,c)\ge \varepsilon\}.$$ $\underline{\boldsymbol{\text{lemma } 7.5.}}$ Suppose $A$ is closed. For each $\varepsilon>0,\space D_\varepsilon$ is closed. Here is the proof as written in the script: Now, suppose that $f:[a,b]\times[c,d]\to\Bbb R$ is integrable and let $D$ be the set of all its discontinuities. Since $D=\bigcup_{n\in\Bbb N}D_{1/n}$ it is enough to show $D_{1/n}$ is of measure $0$ for all $n\in\Bbb N.$ For a given $\varepsilon>0,$ there is a partition $P$ of the rectangle $A=[a,b]\times[c,d]$ s. t. $U(P,f)- L(P,f)<\varepsilon.$ In particular, for the rectangles $A_{ij}$ that intersect $D_{1/n},$ it holds $\sum(M_{ij}-m_{ij})\nu(A_{ij})<\varepsilon,$ and, since $M_{ij}-m_{ij}\ge\frac1n$ for those rectangles $A_{ij},$ we see that the overall area of those rectangles is less than $n\varepsilon.$ Obviously, those rectangles cover $D_{1/n}.$ If $\varepsilon'>0$ is now arbitrary, we see that, with $\varepsilon=\frac{\varepsilon'}n,$ we've found finitely many rectangles which cover $D_{1/n}$ and whose overall area is less than $n\varepsilon=\varepsilon'$ . Hence, $D_{1/n}$ is of Lebesgue measure $0$ (in fact, of Jordan measure $0$ ). My attempt is that since $M_{ij}-m_{ij}\ge\frac1n$ for those rectangles $A_{ij},$ is somewhat wrong. $A\subset\Bbb R^2$ is a closed rectangle and the rectangles $A_{ij}$ in the partition of $A$ are closed. If some of them intersect $D_{1/n},$ the intersection might be contained in their boundaries. For example, if we take the function $f:[0,2]\times[0,2]\to\Bbb R,$ $$f(x,y)=\begin{cases}1, x<1,\\2, x\ge 1\end{cases}$$ and take any suitable partition so that the segment $[(1,0),(1,2)]$ contains boundaries of some rectangles, they might be a counterexample. I would like to ask if I'm on the right track and if we should isolate such cases?","On the lectures today, another professor came and told us there is a minor omission/mistake in the proof of one direction of the Lebesgue theorem, namely bounded and integrable implies measure set of discontinuities that we might discuss when our professor returns, so I've been thinking about it as an exercise. First, a definition and some results: : Let and any function. Oscillation of the function at the point is the infimum of the expression over all the open neighbourhoods of the point . In other words : A function is continuous at if and only if If is the set of discontinuities of the function then where Suppose is closed. For each is closed. Here is the proof as written in the script: Now, suppose that is integrable and let be the set of all its discontinuities. Since it is enough to show is of measure for all For a given there is a partition of the rectangle s. t. In particular, for the rectangles that intersect it holds and, since for those rectangles we see that the overall area of those rectangles is less than Obviously, those rectangles cover If is now arbitrary, we see that, with we've found finitely many rectangles which cover and whose overall area is less than . Hence, is of Lebesgue measure (in fact, of Jordan measure ). My attempt is that since for those rectangles is somewhat wrong. is a closed rectangle and the rectangles in the partition of are closed. If some of them intersect the intersection might be contained in their boundaries. For example, if we take the function and take any suitable partition so that the segment contains boundaries of some rectangles, they might be a counterexample. I would like to ask if I'm on the right track and if we should isolate such cases?","f:[a,b]\times[c,d]\to\Bbb R 0 \underline{\boldsymbol{\text{definition}}} A\subset\Bbb R^2 f:A\to\Bbb R O(f,c) f c\in A \sup\{|f(x_1)-f(x_2)|:x_1,x_2\in U\cap A\} U\subset\Bbb R^2 c O(f,c)=\inf_{U\ni c}\sup_{x_1,x_2\in U\cap A}|f(x_1)-f(x_2)|. \underline{\boldsymbol{\text{ lemma }} 7.1} f:A\to\Bbb R c\in A O(f,c)=0. \underline{\boldsymbol{\text{result}}} D f, D=\bigcup_{\varepsilon >0}D_\varepsilon, D_\varepsilon=\{x\in A\mid O(f,c)\ge \varepsilon\}. \underline{\boldsymbol{\text{lemma } 7.5.}} A \varepsilon>0,\space D_\varepsilon f:[a,b]\times[c,d]\to\Bbb R D D=\bigcup_{n\in\Bbb N}D_{1/n} D_{1/n} 0 n\in\Bbb N. \varepsilon>0, P A=[a,b]\times[c,d] U(P,f)- L(P,f)<\varepsilon. A_{ij} D_{1/n}, \sum(M_{ij}-m_{ij})\nu(A_{ij})<\varepsilon, M_{ij}-m_{ij}\ge\frac1n A_{ij}, n\varepsilon. D_{1/n}. \varepsilon'>0 \varepsilon=\frac{\varepsilon'}n, D_{1/n} n\varepsilon=\varepsilon' D_{1/n} 0 0 M_{ij}-m_{ij}\ge\frac1n A_{ij}, A\subset\Bbb R^2 A_{ij} A D_{1/n}, f:[0,2]\times[0,2]\to\Bbb R, f(x,y)=\begin{cases}1, x<1,\\2, x\ge 1\end{cases} [(1,0),(1,2)]","['real-analysis', 'multivariable-calculus']"
32,How to find formulas for spherical coordinates in terms of Cartesian coordinates using Maple?,How to find formulas for spherical coordinates in terms of Cartesian coordinates using Maple?,,"We can go from spherical coordinates $(r, \theta, \phi)$ to cartesian coordinates $(x,y,z)$ using the equations: $x=r sin(\theta) cos(\phi)$ $y = r sin(\theta) sin(\phi)$ $z = r cos(\theta)$ Consider the task of finding formulas for $r$ , $\theta$ , and $\phi$ in terms of $x,y$ , and $z$ . The three equations above are a system that can be solved for $r$ , $\theta$ , and $\phi$ . This is a relatively easy task with pen and paper. $r = \sqrt{x^2+y^2+z^2}$ $\theta = cos^{-1}\frac{z}{r}=cos^{-1}\frac{z}{\sqrt{x^2+y^2+z^2}}$ $\phi = tan^{-1} \frac{y}{x}$ How can we get Maple to obtain the same result? If we try: solve({r*sin(t)*cos(p) = x, r*sin(t)*sin(p) = y, r*cos(t) = z}, [r, t, p]) We obtain some very long and relatively illegible expression. If my reasoning is correct about the simplicity of solving the original problem, why is it apparently not as straightforward in Maple? Furthermore, is there a way in Maple to obtain a relatively nice-looking solution?","We can go from spherical coordinates to cartesian coordinates using the equations: Consider the task of finding formulas for , , and in terms of , and . The three equations above are a system that can be solved for , , and . This is a relatively easy task with pen and paper. How can we get Maple to obtain the same result? If we try: solve({r*sin(t)*cos(p) = x, r*sin(t)*sin(p) = y, r*cos(t) = z}, [r, t, p]) We obtain some very long and relatively illegible expression. If my reasoning is correct about the simplicity of solving the original problem, why is it apparently not as straightforward in Maple? Furthermore, is there a way in Maple to obtain a relatively nice-looking solution?","(r, \theta, \phi) (x,y,z) x=r sin(\theta) cos(\phi) y = r sin(\theta) sin(\phi) z = r cos(\theta) r \theta \phi x,y z r \theta \phi r = \sqrt{x^2+y^2+z^2} \theta = cos^{-1}\frac{z}{r}=cos^{-1}\frac{z}{\sqrt{x^2+y^2+z^2}} \phi = tan^{-1} \frac{y}{x}","['multivariable-calculus', 'maple']"
33,Show that if $\epsilon\to0$ then $(1/\alpha(\epsilon))\int_{\partial B_\epsilon(0)}\varphi(x) d\sigma =\varphi(0)$,Show that if  then,\epsilon\to0 (1/\alpha(\epsilon))\int_{\partial B_\epsilon(0)}\varphi(x) d\sigma =\varphi(0),"I'm trying to prove that for a function $\varphi\in C_c^\infty(\mathbb{R}^n)$ and $\alpha(\epsilon)$ , the surface of the ball $B_\epsilon (0)$ \begin{align*}\underset{\epsilon\to0}{\lim} (1/\alpha(\epsilon))\int_{\partial B_\epsilon(0)}\varphi(x) d\sigma =\varphi(0). \end{align*} At some point, it's necessary using the fact that $\varphi$ is continuous. I was thinking about rewriting $\alpha(\epsilon)$ like an integral but I don't achieve anything with it. Please, ¿can you give me some hint?","I'm trying to prove that for a function and , the surface of the ball At some point, it's necessary using the fact that is continuous. I was thinking about rewriting like an integral but I don't achieve anything with it. Please, ¿can you give me some hint?",\varphi\in C_c^\infty(\mathbb{R}^n) \alpha(\epsilon) B_\epsilon (0) \begin{align*}\underset{\epsilon\to0}{\lim} (1/\alpha(\epsilon))\int_{\partial B_\epsilon(0)}\varphi(x) d\sigma =\varphi(0). \end{align*} \varphi \alpha(\epsilon),"['integration', 'multivariable-calculus']"
34,tough GRE subject problem,tough GRE subject problem,,"what is the smallest value for $n$ for which the following limit exists for all $r \geq n$ ? $$\lim_{(x,y) \rightarrow (0,0)}\frac{x^r}{\vert x \vert^2+\vert y \vert^2}$$ I thought of using LHopitals rule but get stuck differentiating the absolute values. I wonder if there's a trick I am unaware of. So the comments are saying to rewrite as $$\lim_{(x,y) \rightarrow (0,0)}\frac{x^r}{x^2+ y^2}$$ Then by LHopitals Rule (twice) I get $$\frac{r(r-1)x^{r-2}}{4}$$ But I get stuck here.",what is the smallest value for for which the following limit exists for all ? I thought of using LHopitals rule but get stuck differentiating the absolute values. I wonder if there's a trick I am unaware of. So the comments are saying to rewrite as Then by LHopitals Rule (twice) I get But I get stuck here.,"n r \geq n \lim_{(x,y) \rightarrow (0,0)}\frac{x^r}{\vert x \vert^2+\vert y \vert^2} \lim_{(x,y) \rightarrow (0,0)}\frac{x^r}{x^2+ y^2} \frac{r(r-1)x^{r-2}}{4}","['limits', 'multivariable-calculus']"
35,Is double integration an easier way to find volume of rotation?,Is double integration an easier way to find volume of rotation?,,"AP Calculus BC student here, One of the most hated topics from Calculus 1 & 2 is often the disk method, washer method, and the shell method. Disk Method = $\pi \int [f(x)^2]dx$ (rotate x-axis) Washer Method = $\pi \int [R(x)^2-r(x)^2]dx$ (rotate x-axis) Shell Method = $2\pi \int xf(x)dx$ (rotate y-axis) Is there a method from multivariable calculus that uses double integration to calculate the volume of rotation?","AP Calculus BC student here, One of the most hated topics from Calculus 1 & 2 is often the disk method, washer method, and the shell method. Disk Method = (rotate x-axis) Washer Method = (rotate x-axis) Shell Method = (rotate y-axis) Is there a method from multivariable calculus that uses double integration to calculate the volume of rotation?",\pi \int [f(x)^2]dx \pi \int [R(x)^2-r(x)^2]dx 2\pi \int xf(x)dx,"['calculus', 'multivariable-calculus']"
36,Why isn't this closed loop curve integral $0$?,Why isn't this closed loop curve integral ?,0,"I had to solve a problem where I had to calculate the work done by the force field given by: $$ \vec{F} = \frac{(-y,x)}{x^2+4y^2}, (x,y) \neq (0,0)$$ where we travel along the whole unit circle in a positive orientation. I managed to find the potential function given by: $\phi = \frac{-1}{2}\arctan(\frac{x}{2y})$ . Since there exists a potential function for which $\vec{F} = \nabla(\phi)$ , we have that the force field is conservative. We also note that the potential function isn't defined at $y = 0$ . This means I can't use the fundamental theorem of line integrals directly. Maybe I should extend my $\phi$ in so that I get a continous function by computing the limit as we tend to $0$ from the left and the right. Then I'm also thinking that our curve we are walking along is a closed loop, and then indeed, for a conservative force field, this has to become $0$ . Instead, my answer sheet tells me it's $\pi$ . I'd be thankful if you could describe to me what went wrong in my solution, and why that's the case.","I had to solve a problem where I had to calculate the work done by the force field given by: where we travel along the whole unit circle in a positive orientation. I managed to find the potential function given by: . Since there exists a potential function for which , we have that the force field is conservative. We also note that the potential function isn't defined at . This means I can't use the fundamental theorem of line integrals directly. Maybe I should extend my in so that I get a continous function by computing the limit as we tend to from the left and the right. Then I'm also thinking that our curve we are walking along is a closed loop, and then indeed, for a conservative force field, this has to become . Instead, my answer sheet tells me it's . I'd be thankful if you could describe to me what went wrong in my solution, and why that's the case."," \vec{F} = \frac{(-y,x)}{x^2+4y^2}, (x,y) \neq (0,0) \phi = \frac{-1}{2}\arctan(\frac{x}{2y}) \vec{F} = \nabla(\phi) y = 0 \phi 0 0 \pi","['integration', 'multivariable-calculus', 'vector-fields']"
37,Find the length of the following parametric curve in R^3,Find the length of the following parametric curve in R^3,,"$$\gamma(t)=(\log t\sqrt2,\frac{1}{3t},3t+1)$$ $$t\in [1/3,3]$$ my try : the tangent to the curve is : $$\gamma'(t)=\left(\frac{\sqrt2}{t},\frac{-1}{3t^2},3\right)$$ the norm of the tangent to the curve: $$||\gamma(t)||=\left(\frac{2}{t^2}+\frac{1}{9t^4}+9\right)^\frac{1}{2}$$ expanding and simplifying : $$||\gamma(t)||=\frac{((12t^2+5)(12t^2+1))^\frac{1}{2}}{6t^2}$$ I don't know how to proceed to solve the integral from this",my try : the tangent to the curve is : the norm of the tangent to the curve: expanding and simplifying : I don't know how to proceed to solve the integral from this,"\gamma(t)=(\log t\sqrt2,\frac{1}{3t},3t+1) t\in [1/3,3] \gamma'(t)=\left(\frac{\sqrt2}{t},\frac{-1}{3t^2},3\right) ||\gamma(t)||=\left(\frac{2}{t^2}+\frac{1}{9t^4}+9\right)^\frac{1}{2} ||\gamma(t)||=\frac{((12t^2+5)(12t^2+1))^\frac{1}{2}}{6t^2}","['multivariable-calculus', 'curves']"
38,Proving Jordan measurability from integrability of a function,Proving Jordan measurability from integrability of a function,,"I'm working with the next exercise and I don't know how to solve it. Let $f:A\subseteq\mathbb{R}^{n}\to\mathbb{R}$ an integrable function over $A$ where $A$ is a bounded set. Prove that if there exist $c>0$ such that for all $x\in A$ we have that $f(x)\geq c$ then $A$ is Jordan measurable set. Recall that a set $A$ is Jordan measurable iff the indicator function of $A$ is integrable over a rectangle containing $A$ iff the boundary of $A$ has zero content. My first idea was to prove that $h(x)=c\chi_{A}(x)$ is integrable over $R$ (and therefore $\chi_A(x)$ ), i.e., consider a rectangle $R$ such that $A\subseteq R$ and proving that the differente of upper and lower sums are small. As $f$ is integrable over $A$ , the zero extension of $f$ to all rectangle, called $f_A$ (i.e., $f_A$ is defined as $f$ if $x\in A$ and $0$ if $x\in R\setminus A$ ), is integrable. Then, there exist a partition $P$ of $R$ such that $U(f_A,P)-L(f_A,P)$ is less than a small number. But now, if we take a rectangle $R_i$ generated by $P$ we can see the next relation between the infimum and supremum of $f_A$ and $h$ over $R_i$ : $$\inf(h(x),R_i )\leq \inf(f_A,R_i) \ \text{and} \ \sup(h(x),R_i)\leq \sup(f_A,R_i)$$ This is not a good inequality because with this I can't bound (or not directly) $U(c\chi_A,P)-L(c\chi_A,P)$ . So, I don´t know how to continue... My next idea was to use the Lebesgue criterion of integrability to prove directly that $h$ is integrable. But, again, clearly, the set of discontinuities of $c\chi_A$ is $\operatorname{bd}(A)$ (boundary of $A$ ). If I knew that the boundary of $A$ has zero content or zero Lebesgue measure (they are equivalents as $A$ is bounded) I have finished, but this is equivalent to Jordan measurability of $A$ . So, again, I'm stucked. Any hint or idea? I really appreciate. Thanks!","I'm working with the next exercise and I don't know how to solve it. Let an integrable function over where is a bounded set. Prove that if there exist such that for all we have that then is Jordan measurable set. Recall that a set is Jordan measurable iff the indicator function of is integrable over a rectangle containing iff the boundary of has zero content. My first idea was to prove that is integrable over (and therefore ), i.e., consider a rectangle such that and proving that the differente of upper and lower sums are small. As is integrable over , the zero extension of to all rectangle, called (i.e., is defined as if and if ), is integrable. Then, there exist a partition of such that is less than a small number. But now, if we take a rectangle generated by we can see the next relation between the infimum and supremum of and over : This is not a good inequality because with this I can't bound (or not directly) . So, I don´t know how to continue... My next idea was to use the Lebesgue criterion of integrability to prove directly that is integrable. But, again, clearly, the set of discontinuities of is (boundary of ). If I knew that the boundary of has zero content or zero Lebesgue measure (they are equivalents as is bounded) I have finished, but this is equivalent to Jordan measurability of . So, again, I'm stucked. Any hint or idea? I really appreciate. Thanks!","f:A\subseteq\mathbb{R}^{n}\to\mathbb{R} A A c>0 x\in A f(x)\geq c A A A A A h(x)=c\chi_{A}(x) R \chi_A(x) R A\subseteq R f A f f_A f_A f x\in A 0 x\in R\setminus A P R U(f_A,P)-L(f_A,P) R_i P f_A h R_i \inf(h(x),R_i )\leq \inf(f_A,R_i) \ \text{and} \ \sup(h(x),R_i)\leq \sup(f_A,R_i) U(c\chi_A,P)-L(c\chi_A,P) h c\chi_A \operatorname{bd}(A) A A A A","['calculus', 'multivariable-calculus', 'lebesgue-measure', 'riemann-integration']"
39,Computing the Jacobian of $\mathbf{x} \mapsto \mathbf{A}\mathbf{x}\mathbf{x}^T\mathbf{A}\mathbf{\dot{x}}$,Computing the Jacobian of,\mathbf{x} \mapsto \mathbf{A}\mathbf{x}\mathbf{x}^T\mathbf{A}\mathbf{\dot{x}},"I am trying to compute the following vector-by-vector derivative $$ \frac{\text{d}}{\text{d}\mathbf{x}}\left(\mathbf{A}\mathbf{x}\mathbf{x}^T\mathbf{A}\mathbf{\dot{x}}\right), $$ where $\mathbf{x}$ and $\mathbf{\dot{x}}$ are $n \times 1$ column vectors, $\mathbf{A}$ is a constant $n \times n$ matrix, and $\dfrac{\text{d}\mathbf{\dot{x}}}{\text{d}\mathbf{x}}$ is a known $n \times n$ matrix. I try to solve this by switching to index notation: $$ \begin{aligned} \frac{\text{d}}{\text{d}x_m}\left(A_{ij} x_j x_k A_{kl} \dot{x}_l\right) &= A_{ij} \delta_{jm} x_k A_{kl} \dot{x}_l + A_{ij} x_j \delta_{km} A_{kl} \dot{x}_l + A_{ij} x_j x_k A_{kl} \frac{\text{d}\dot{x}_l}{\text{d}x_m} \\ &\Rightarrow \mathbf{A} \left(\mathbf{x}^T \mathbf{A} \mathbf{\dot{x}}\right) + \mathbf{A} \mathbf{x} \,\left(\mathbf{A} \mathbf{\dot{x}}\right)^T + \mathbf{A} \mathbf{x} \mathbf{x}^T \mathbf{A} \dfrac{\text{d}\mathbf{\dot{x}}}{\text{d}\mathbf{x}} \end{aligned} $$ Is this correct? I don't have much experience with this kind of thing. Thank you.","I am trying to compute the following vector-by-vector derivative where and are column vectors, is a constant matrix, and is a known matrix. I try to solve this by switching to index notation: Is this correct? I don't have much experience with this kind of thing. Thank you."," \frac{\text{d}}{\text{d}\mathbf{x}}\left(\mathbf{A}\mathbf{x}\mathbf{x}^T\mathbf{A}\mathbf{\dot{x}}\right),  \mathbf{x} \mathbf{\dot{x}} n \times 1 \mathbf{A} n \times n \dfrac{\text{d}\mathbf{\dot{x}}}{\text{d}\mathbf{x}} n \times n 
\begin{aligned}
\frac{\text{d}}{\text{d}x_m}\left(A_{ij} x_j x_k A_{kl} \dot{x}_l\right)
&= A_{ij} \delta_{jm} x_k A_{kl} \dot{x}_l
+ A_{ij} x_j \delta_{km} A_{kl} \dot{x}_l
+ A_{ij} x_j x_k A_{kl} \frac{\text{d}\dot{x}_l}{\text{d}x_m} \\
&\Rightarrow \mathbf{A} \left(\mathbf{x}^T \mathbf{A} \mathbf{\dot{x}}\right)
+ \mathbf{A} \mathbf{x} \,\left(\mathbf{A} \mathbf{\dot{x}}\right)^T
+ \mathbf{A} \mathbf{x} \mathbf{x}^T \mathbf{A} \dfrac{\text{d}\mathbf{\dot{x}}}{\text{d}\mathbf{x}}
\end{aligned}
","['matrices', 'multivariable-calculus', 'matrix-calculus', 'vector-fields', 'jacobian']"
40,The composition of soft curves is not always soft.,The composition of soft curves is not always soft.,,"An exercise I found in one of my text books: (In my book a curve that is ""soft by pieces"" is a curve $\gamma :[a,b]\subset \mathbb{R}\to\mathbb{R}^n$ such that there is a partition $P=\{a=t_0<t_1<...t_k=b\}$ of $[a,b]$ such that $\gamma$ has a continuous derivative on each $[t_{i-1},t_i]$ for $i \in \{1,2,...k\}$ ; I believe ""soft by pieces"" may not be a universal term to refer to such curves, this is a direct translation of the term used in the book, which is in Spanish) ''If $\gamma :[a,b]\subset \mathbb{R}\to\mathbb{R}^n$ is $\textit{soft by pieces}$ and $\alpha:[c,d]\subset\mathbb{R}\to[a,b]\subset\mathbb{R}$ , surjective on $[a,b]$ and $C^1$ on $[c,d]$ . Show, with an example, that $\gamma \circ \alpha:[c,d]\subset\mathbb{R}\to \mathbb{R}^2$ is not always soft by pieces"" I took $\gamma:[-1,1]\to\mathbb{R}^2$ , $\gamma(t)=(t,|t|)$ , I know that this function is soft by pieces (I need only to take $P=\{-1,0,1\}$ , $\gamma$ 's derivative is definitely continuous in $[-1,0]$ and $[0,1]$ ). The problem I'm faced with lies in finding the adequate $\alpha$ , at first I thought about proposing a function similar to $sin(\frac{1}{x})$ such that this function is surjective, but has problems on 0, the problem with this function is that is is not differentiable on zero. Then I thought about $x^2\sin\left(\frac{1}{x}\right)$ and $x^3\sin\left(\frac{1}{x}\right)$ , however these don't work either because even though I could make them continuous on zero (by defining a function that is 0 if $x=0$ and $x^2\sin\left(\frac{1}{x}\right)$ or $x^3\sin\left(\frac{1}{x}\right)$ ) everywhere else) and their derivative is continuous, they are not surjectve on $[-1,1]$ I believe that my intuition about my choice if $\gamma$ is sound and the idea of choosing an $\alpha$ that ""has problems on zero"" could work. Still, I'm stuck and unable to find an $\alpha$ that ""has problems on zero"", but is surjective and has a continuous derivative.","An exercise I found in one of my text books: (In my book a curve that is ""soft by pieces"" is a curve such that there is a partition of such that has a continuous derivative on each for ; I believe ""soft by pieces"" may not be a universal term to refer to such curves, this is a direct translation of the term used in the book, which is in Spanish) ''If is and , surjective on and on . Show, with an example, that is not always soft by pieces"" I took , , I know that this function is soft by pieces (I need only to take , 's derivative is definitely continuous in and ). The problem I'm faced with lies in finding the adequate , at first I thought about proposing a function similar to such that this function is surjective, but has problems on 0, the problem with this function is that is is not differentiable on zero. Then I thought about and , however these don't work either because even though I could make them continuous on zero (by defining a function that is 0 if and or ) everywhere else) and their derivative is continuous, they are not surjectve on I believe that my intuition about my choice if is sound and the idea of choosing an that ""has problems on zero"" could work. Still, I'm stuck and unable to find an that ""has problems on zero"", but is surjective and has a continuous derivative.","\gamma :[a,b]\subset \mathbb{R}\to\mathbb{R}^n P=\{a=t_0<t_1<...t_k=b\} [a,b] \gamma [t_{i-1},t_i] i \in \{1,2,...k\} \gamma :[a,b]\subset \mathbb{R}\to\mathbb{R}^n \textit{soft by pieces} \alpha:[c,d]\subset\mathbb{R}\to[a,b]\subset\mathbb{R} [a,b] C^1 [c,d] \gamma \circ \alpha:[c,d]\subset\mathbb{R}\to \mathbb{R}^2 \gamma:[-1,1]\to\mathbb{R}^2 \gamma(t)=(t,|t|) P=\{-1,0,1\} \gamma [-1,0] [0,1] \alpha sin(\frac{1}{x}) x^2\sin\left(\frac{1}{x}\right) x^3\sin\left(\frac{1}{x}\right) x=0 x^2\sin\left(\frac{1}{x}\right) x^3\sin\left(\frac{1}{x}\right) [-1,1] \gamma \alpha \alpha","['calculus', 'multivariable-calculus']"
41,Evaluating line integral using The Stokes' Theorem,Evaluating line integral using The Stokes' Theorem,,"I want to evaluate $$\oint_C (x-z) dx + (x + y) dy + (y+z) dz$$ where $C$ is the ellipse, in which the plane $z=y$ intersects the cylinder $x^2 + y^2 = 1$ , oriented counterclockwise as viewed from above. However, solution is somehow wrong. First, I say that the integral equals to $$\oint_{C} \vec F \cdot dr =\int\int_{S} \nabla \times \vec F \cdot \hat n d\sigma $$ where $F= (x-z)\hat i + (x+y)\hat j + (y+z)\hat k$ . Then, I found $\nabla \times \vec F = \hat i - \hat j + \hat k$ . Next, I wrote $$\hat n = \frac{\nabla f}{ |\nabla f|} = \frac{-\hat j + \hat k}{\sqrt{2}}$$ and $$d \sigma = \frac{|\nabla f|}{|\nabla f \cdot \hat n|}dxdy$$ where $f(x,y,z) = z-y$ . Finally, I got $$\int\int_{S} \nabla \times \vec F \cdot \hat n d\sigma  = \int\int_{x^2 +y^2 \le 1} (\hat i - \hat j + \hat k) \cdot (\frac{-\hat j + \hat k}{\sqrt{2}})dxdy = \int\int_{x^2 +y^2 \le 1} \frac{2}{\sqrt{2}} dxdy = \frac{2}{\sqrt{2}}\pi $$ but the solution must be just $2 \pi.$ Where is my mistake?","I want to evaluate where is the ellipse, in which the plane intersects the cylinder , oriented counterclockwise as viewed from above. However, solution is somehow wrong. First, I say that the integral equals to where . Then, I found . Next, I wrote and where . Finally, I got but the solution must be just Where is my mistake?","\oint_C (x-z) dx + (x + y) dy + (y+z) dz C z=y x^2 + y^2 = 1 \oint_{C} \vec F \cdot dr =\int\int_{S} \nabla \times \vec F \cdot \hat n d\sigma  F= (x-z)\hat i + (x+y)\hat j + (y+z)\hat k \nabla \times \vec F = \hat i - \hat j + \hat k \hat n = \frac{\nabla f}{ |\nabla f|} = \frac{-\hat j + \hat k}{\sqrt{2}} d \sigma = \frac{|\nabla f|}{|\nabla f \cdot \hat n|}dxdy f(x,y,z) = z-y \int\int_{S} \nabla \times \vec F \cdot \hat n d\sigma  = \int\int_{x^2 +y^2 \le 1} (\hat i - \hat j + \hat k) \cdot (\frac{-\hat j + \hat k}{\sqrt{2}})dxdy = \int\int_{x^2 +y^2 \le 1} \frac{2}{\sqrt{2}} dxdy = \frac{2}{\sqrt{2}}\pi  2 \pi.","['calculus', 'multivariable-calculus', 'surface-integrals', 'stokes-theorem']"
42,Contests with Calculus Questions,Contests with Calculus Questions,,"I am self studying Thomas's Calculus , a popular Calculus textbook in the US and a few other countries. It covers the standard theorems and properties, the easier proofs, standard numericals and some simple applications. In non-calculus maths, I have seen that contest math questions tend to be more difficult, require more creativity, and better test understanding of concepts in a larger context. A couple of contests that I know have Calculus questions include: Mu Alpha Theta : Some nice questions which assume the same concepts that Thomas does, but more creative applications. Putnam Some very difficult questions (possibly too difficult to be done immediately after Thomas). Are there any other sources from where I can get questions for Calculus? Also, what books would be recommended after finishing Thomas? Having solutions for the problems is very important, since I am self-studying. I will almost always go with a source that has solutions which can be checked when I can get stuck, or make the inevitable mistake.","I am self studying Thomas's Calculus , a popular Calculus textbook in the US and a few other countries. It covers the standard theorems and properties, the easier proofs, standard numericals and some simple applications. In non-calculus maths, I have seen that contest math questions tend to be more difficult, require more creativity, and better test understanding of concepts in a larger context. A couple of contests that I know have Calculus questions include: Mu Alpha Theta : Some nice questions which assume the same concepts that Thomas does, but more creative applications. Putnam Some very difficult questions (possibly too difficult to be done immediately after Thomas). Are there any other sources from where I can get questions for Calculus? Also, what books would be recommended after finishing Thomas? Having solutions for the problems is very important, since I am self-studying. I will almost always go with a source that has solutions which can be checked when I can get stuck, or make the inevitable mistake.",,"['calculus', 'multivariable-calculus', 'soft-question', 'contest-math']"
43,Minimum amount of material to make a cuboid with fixed volume,Minimum amount of material to make a cuboid with fixed volume,,"I'm trying to assist a student with the following optimization problem. The instructions say to use the partial derivative test which I'm having trouble with. I can solve the problem with Lagrange multipliers just fine, but the student doesn't know that method yet. A rectangular metal tank with an open top is to hold $256$ cubic feet of liquid. What are the dimensions of the tank that require the least material to build? Let $A(x,y,z)=xy+2xz+2yz$ . Then we want to find $\min \left\{A(x,y,z) \mid xyz=256\right\}$ . First I find the critical points, $$\begin{cases}\frac{\partial A}{\partial x} = y + 2z = 0 \\ \frac{\partial A}{\partial y} = x + 2z = 0 \\ \frac{\partial A}{\partial z} = 2x + 2y = 0\end{cases} \implies x=y=-2z \implies 4z^3 = 256$$ so that $z=4$ , but $x$ and $y$ both turn out to be negative. Is there something I am missing, or is this problem flawed in some fundamental way?","I'm trying to assist a student with the following optimization problem. The instructions say to use the partial derivative test which I'm having trouble with. I can solve the problem with Lagrange multipliers just fine, but the student doesn't know that method yet. A rectangular metal tank with an open top is to hold cubic feet of liquid. What are the dimensions of the tank that require the least material to build? Let . Then we want to find . First I find the critical points, so that , but and both turn out to be negative. Is there something I am missing, or is this problem flawed in some fundamental way?","256 A(x,y,z)=xy+2xz+2yz \min \left\{A(x,y,z) \mid xyz=256\right\} \begin{cases}\frac{\partial A}{\partial x} = y + 2z = 0 \\ \frac{\partial A}{\partial y} = x + 2z = 0 \\ \frac{\partial A}{\partial z} = 2x + 2y = 0\end{cases} \implies x=y=-2z \implies 4z^3 = 256 z=4 x y","['multivariable-calculus', 'optimization', 'partial-derivative']"
44,Why doesn't the definition of derivative generalize smoothly from single variable to multivariable calculus like the definition of continuity?,Why doesn't the definition of derivative generalize smoothly from single variable to multivariable calculus like the definition of continuity?,,"As from my previous questions, one may follow that I am trying to understand the differences between real and complex derivative. My question is: For a function $f:\mathbb{R}\to\mathbb{R}$ , the derivative at a point $c\in\mathbb{R}$ is defined as $$f'(c)=\lim_{h\rightarrow 0}\frac{f(c+h)-f(c)}{h}$$ For a complex function $f:\mathbb{C}\longrightarrow\mathbb{C}$ the complex derivative at a point $z$ is defined as $$f'(z)=\lim_{h\rightarrow 0}\frac{f(z+h)-f(z)}{h}.$$ But for a function $f:\mathbb{R}^2\longrightarrow \mathbb{R}^2$ , why isn't the derivative at a point $(x,y)$ defined as $$f'(x,y)=\lim_{(h,k)\rightarrow (0,0)}\frac{f((x,y)+(h,k))-f(x,y)}{(h,k)}?$$ Also, in this question, they say, Differences between the complex derivative and the multivariable derivative. , that it is because, it is not possible to define division in $\mathbb{R}^2$ . But can we not define the inverse of $(h,k)$ as $(\frac{1}{h}, \frac{1}{k})$ , for $h\neq 0, k\neq 0$ ? And define $(a,b){(c,d)}^{-1}$ this way?","As from my previous questions, one may follow that I am trying to understand the differences between real and complex derivative. My question is: For a function , the derivative at a point is defined as For a complex function the complex derivative at a point is defined as But for a function , why isn't the derivative at a point defined as Also, in this question, they say, Differences between the complex derivative and the multivariable derivative. , that it is because, it is not possible to define division in . But can we not define the inverse of as , for ? And define this way?","f:\mathbb{R}\to\mathbb{R} c\in\mathbb{R} f'(c)=\lim_{h\rightarrow 0}\frac{f(c+h)-f(c)}{h} f:\mathbb{C}\longrightarrow\mathbb{C} z f'(z)=\lim_{h\rightarrow 0}\frac{f(z+h)-f(z)}{h}. f:\mathbb{R}^2\longrightarrow \mathbb{R}^2 (x,y) f'(x,y)=\lim_{(h,k)\rightarrow (0,0)}\frac{f((x,y)+(h,k))-f(x,y)}{(h,k)}? \mathbb{R}^2 (h,k) (\frac{1}{h}, \frac{1}{k}) h\neq 0, k\neq 0 (a,b){(c,d)}^{-1}","['calculus', 'analysis', 'multivariable-calculus', 'derivatives']"
45,Calculate the circulation of the vector field alone a parameterized circle (Stoke's Theorem...?),Calculate the circulation of the vector field alone a parameterized circle (Stoke's Theorem...?),,"Find the circulation of the following vector field $\vec{F}(x, y, z) = \langle \sin(x^2+z)-2yz, 2xz + \sin(y^2+z), \sin(x^2+y^2)\rangle$ along the circle $\vec{r}(t)=\langle\cos(t), \sin(t), 1\rangle$ with $t\in [0, 2\pi]$ . I tried using Stoke's theorem to solve it, but I get a difficult integral trying it this way: $$\oint \vec{F}\cdot dr = \iint_S \nabla \times \vec{F} \cdot ds = \int_{0}^{2\pi} \vec{F}(\vec{r}(t))\cdot \vec{r}'(t) \,dt$$ What am I doing wrong?","Find the circulation of the following vector field along the circle with . I tried using Stoke's theorem to solve it, but I get a difficult integral trying it this way: What am I doing wrong?","\vec{F}(x, y, z) = \langle \sin(x^2+z)-2yz, 2xz + \sin(y^2+z), \sin(x^2+y^2)\rangle \vec{r}(t)=\langle\cos(t), \sin(t), 1\rangle t\in [0, 2\pi] \oint \vec{F}\cdot dr = \iint_S \nabla \times \vec{F} \cdot ds = \int_{0}^{2\pi} \vec{F}(\vec{r}(t))\cdot \vec{r}'(t) \,dt","['multivariable-calculus', 'stokes-theorem', 'greens-theorem', 'curl']"
46,"Does order of integration matter, while integrating over a cone?","Does order of integration matter, while integrating over a cone?",,"Suppose I want to find the center of mass of a cone, from its vertex. It is a right circular solid cone of radius $a$ . About the $z$ axis, the center of mass is given by : $$z_{com}=\frac{\int dm\,z}{\int dm}$$ In cylindrical coordinates, let $dm=rdrd\phi dz$ Moreover, we know that for a cone, we have the following relation : $$\frac{z}{r}=\frac{h}{a}$$ Hence, I can have $z=\frac{hr}{a}$ or $r=\frac{az}{h}$ . Normally this is how we do the integral : $$\frac{\int_{0}^{h}\int_{0}^{\frac{az}{h}}rdrzdz}{\int_{0}^{h}\int_{0}^{\frac{az}{h}}rdrdz} = \frac{3h}{4}$$ (I've ignored the integral over the angle and the density, since that cancels out) Here, $r$ ranges from $0$ to $\frac{az}{h}$ . Then we integrate over $z$ from $0$ to $h$ . However, I could have first integrated over $z$ and then integrated over $r$ - this should give me the same answer but it doesn't. In this case, $z$ should range from $\frac{hr}{a}$ , and then $r$ should range from $0$ to $a$ . Hence we would have : $$\frac{\int_{0}^{a}\int_{0}^{\frac{hr}{a}}zdz\,rdr}{\int_{0}^{a}\int_{0}^{\frac{hr}{a}}dz\,rdr} = \frac{3h}{8}$$ There is an extra factor of $1/2$ coming from somewhere. For some reason, the order of integration seems to matter here. However, how would one know which is the correct order, and why exactly does the order even matter ?","Suppose I want to find the center of mass of a cone, from its vertex. It is a right circular solid cone of radius . About the axis, the center of mass is given by : In cylindrical coordinates, let Moreover, we know that for a cone, we have the following relation : Hence, I can have or . Normally this is how we do the integral : (I've ignored the integral over the angle and the density, since that cancels out) Here, ranges from to . Then we integrate over from to . However, I could have first integrated over and then integrated over - this should give me the same answer but it doesn't. In this case, should range from , and then should range from to . Hence we would have : There is an extra factor of coming from somewhere. For some reason, the order of integration seems to matter here. However, how would one know which is the correct order, and why exactly does the order even matter ?","a z z_{com}=\frac{\int dm\,z}{\int dm} dm=rdrd\phi dz \frac{z}{r}=\frac{h}{a} z=\frac{hr}{a} r=\frac{az}{h} \frac{\int_{0}^{h}\int_{0}^{\frac{az}{h}}rdrzdz}{\int_{0}^{h}\int_{0}^{\frac{az}{h}}rdrdz} = \frac{3h}{4} r 0 \frac{az}{h} z 0 h z r z \frac{hr}{a} r 0 a \frac{\int_{0}^{a}\int_{0}^{\frac{hr}{a}}zdz\,rdr}{\int_{0}^{a}\int_{0}^{\frac{hr}{a}}dz\,rdr} = \frac{3h}{8} 1/2","['integration', 'multivariable-calculus', 'definite-integrals', 'multiple-integral', 'cylindrical-coordinates']"
47,Divergence of curl is zero (coordinate free approach),Divergence of curl is zero (coordinate free approach),,"I'm TAing a vector calculus course and the professor has asked the students to prove that $\nabla \cdot (\nabla \times \vec{F}) = 0$ . I'm meant to teach this problem in recitation tomorrow and I think the problem is incorrect. The specific problem statement is: Problem. Let $C$ be a simple, closed curve, $S_1, S_2$ two surfaces whose boundary is $C$ and $\vec{F}$ a vector field that is defined and differentiable throughout a simply connected region containing $C, S_1$ , and $S_2$ . Use Stokes' theorem and the divergence theorem to show that $\nabla \cdot (\nabla \times F)$ is zero. This is obviously super easy to do if one uses Euclidean coordinates ( for example , on page 3). And since we're dealing with curl, I think it's safe to assume that the domain is $\mathbb{R}^3$ , so that Euclidean coordinates are certainly available. But proof by computation in coordinates does not require the divergence theorem or Stokes' theorem, so I think the professor has a coordinate-free approach in mind. Specifically, I think he's thinking that the shared boundary $C$ will be oriented in opposite directions when considered as the boundary of $S_1$ versus $S_2$ . Therefore, one can compute \begin{align*} \iiint_V \nabla \cdot (\nabla \times \vec{F}) dV &= \iint_{S_1 \cup S_2} (\nabla \times \vec{F}) \cdot d\vec{A} \\ &= \iint_{S_1} (\nabla \times \vec{F}) \cdot d\vec{A}_1 + \iint_{S_2} (\nabla \times \vec{F}) \cdot d\vec{A}_2 \\ &= \oint_{C^+} \vec{F} \cdot d\vec{r} + \oint_{C^-} \vec{F} \cdot d\vec{r} \\ &= 0, \end{align*} where $V$ is the region enclosed by $S_1 \cup S_2$ , where $d\vec{A}_1$ and $d\vec{A}_2$ are outward normal to $S_1$ and $S_2$ respectively, and where $C^+$ indicates $C$ oriented counterclockwise while $C^-$ indicates clockwise orientation. However, that computation only shows that the integral of $\nabla \cdot (\nabla \times \vec{F})$ is zero, not that the function itself is zero. Is there some way to show from here that the function itself is zero? Another approach that I am missing? Should I just tell the students to do the computation in coordinates and ignore the divergence/Stokes stuff? Thanks in advance!","I'm TAing a vector calculus course and the professor has asked the students to prove that . I'm meant to teach this problem in recitation tomorrow and I think the problem is incorrect. The specific problem statement is: Problem. Let be a simple, closed curve, two surfaces whose boundary is and a vector field that is defined and differentiable throughout a simply connected region containing , and . Use Stokes' theorem and the divergence theorem to show that is zero. This is obviously super easy to do if one uses Euclidean coordinates ( for example , on page 3). And since we're dealing with curl, I think it's safe to assume that the domain is , so that Euclidean coordinates are certainly available. But proof by computation in coordinates does not require the divergence theorem or Stokes' theorem, so I think the professor has a coordinate-free approach in mind. Specifically, I think he's thinking that the shared boundary will be oriented in opposite directions when considered as the boundary of versus . Therefore, one can compute where is the region enclosed by , where and are outward normal to and respectively, and where indicates oriented counterclockwise while indicates clockwise orientation. However, that computation only shows that the integral of is zero, not that the function itself is zero. Is there some way to show from here that the function itself is zero? Another approach that I am missing? Should I just tell the students to do the computation in coordinates and ignore the divergence/Stokes stuff? Thanks in advance!","\nabla \cdot (\nabla \times \vec{F}) = 0 C S_1, S_2 C \vec{F} C, S_1 S_2 \nabla \cdot (\nabla \times F) \mathbb{R}^3 C S_1 S_2 \begin{align*}
\iiint_V \nabla \cdot (\nabla \times \vec{F}) dV &= \iint_{S_1 \cup S_2} (\nabla \times \vec{F}) \cdot d\vec{A} \\
&= \iint_{S_1} (\nabla \times \vec{F}) \cdot d\vec{A}_1 + \iint_{S_2} (\nabla \times \vec{F}) \cdot d\vec{A}_2 \\
&= \oint_{C^+} \vec{F} \cdot d\vec{r} + \oint_{C^-} \vec{F} \cdot d\vec{r} \\
&= 0,
\end{align*} V S_1 \cup S_2 d\vec{A}_1 d\vec{A}_2 S_1 S_2 C^+ C C^- \nabla \cdot (\nabla \times \vec{F})","['multivariable-calculus', 'vector-analysis', 'stokes-theorem', 'divergence-theorem']"
48,How to compute this double integral with $dt$ and $dx$? [closed],How to compute this double integral with  and ? [closed],dt dx,"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 2 years ago . Improve this question How to calculate the following integral? Any help is appreciated because I am not sure if there are some theorems I haven't learned to solve the question. Thanks. $\int_0^1 \frac{f(x)}{\sqrt{x}} dx $ where $f(x) = \int_1^x \frac{\ln(1+t)}{t} dt$ I just found the solution of the first two steps and it's as the following And it is more confusing to me now. $\int_0^1 \frac{1}{\sqrt{x}} \int_1^x \frac{\ln(1+t)}{t} dt dx  $ $= -\int_0^1  \frac{\ln(1+t)}{t} dt \int_t^0  \frac{1}{\sqrt{x}} dx $ $= -2\int_0^1  \frac{\ln(1+t)}{\sqrt{t}} dt $ ... (From this step on, I guess I could solve it by letting $t=u^2$ )","Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 2 years ago . Improve this question How to calculate the following integral? Any help is appreciated because I am not sure if there are some theorems I haven't learned to solve the question. Thanks. where I just found the solution of the first two steps and it's as the following And it is more confusing to me now. ... (From this step on, I guess I could solve it by letting )",\int_0^1 \frac{f(x)}{\sqrt{x}} dx  f(x) = \int_1^x \frac{\ln(1+t)}{t} dt \int_0^1 \frac{1}{\sqrt{x}} \int_1^x \frac{\ln(1+t)}{t} dt dx   = -\int_0^1  \frac{\ln(1+t)}{t} dt \int_t^0  \frac{1}{\sqrt{x}} dx  = -2\int_0^1  \frac{\ln(1+t)}{\sqrt{t}} dt  t=u^2,"['calculus', 'integration', 'multivariable-calculus']"
49,Are there any examples of non-linear functions whose contour plot is made up of ALL parallel lines? [closed],Are there any examples of non-linear functions whose contour plot is made up of ALL parallel lines? [closed],,"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 2 years ago . Improve this question I know planes are made up of all parallel lines but what about functions in 3-space?","Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 2 years ago . Improve this question I know planes are made up of all parallel lines but what about functions in 3-space?",,['multivariable-calculus']
50,Calculating the volume of a region using double integration,Calculating the volume of a region using double integration,,"Question: Calculate the volume of the region in $y > 0$ enclosed by the planes $y = 0, z = 0, z = d − x + y$ and the parabolic cylinder $y=d-{x^2}/d$ , where $d$ is a positive valued constant. I know I'll be using a double integral to find the volume but I'm having a hard time visualising it and I'm not sure what to use for the limits. I think I integrate with respect to $y$ first, but I'm not sure what the limits would be? would they be $y=0$ and $y=d-{x^2}/d$ ? $$ \displaystyle \int \left[\int_{y=0}^{y=d-{x^2}/d} (d-x+y) \,dy \right] ~ dx\, ?$$ Not sure what my $x$ limits would be I was thinking either $-d,d$ or maybe $\sqrt{d},d$ Can anyone confirm whether I'm integrating the right thing and help me with my limits? thank you","Question: Calculate the volume of the region in enclosed by the planes and the parabolic cylinder , where is a positive valued constant. I know I'll be using a double integral to find the volume but I'm having a hard time visualising it and I'm not sure what to use for the limits. I think I integrate with respect to first, but I'm not sure what the limits would be? would they be and ? Not sure what my limits would be I was thinking either or maybe Can anyone confirm whether I'm integrating the right thing and help me with my limits? thank you","y > 0 y = 0, z = 0, z = d − x + y y=d-{x^2}/d d y y=0 y=d-{x^2}/d  \displaystyle \int \left[\int_{y=0}^{y=d-{x^2}/d} (d-x+y) \,dy \right] ~ dx\, ? x -d,d \sqrt{d},d","['multivariable-calculus', 'volume', 'multiple-integral']"
51,Chain rule in Spivak's Calculus on Manifolds,Chain rule in Spivak's Calculus on Manifolds,,"In the following example from Chain Rule of Chapter 2 in Spivak's Calculus on Manifolds, I am confused about the dot after the first equality sign. According to the chain rule, $D(g\circ f)(a)=Dg(f(a))\circ Df(a)$ . How is the circle changed to the dot? How to interpret this? Thanks in advance.","In the following example from Chain Rule of Chapter 2 in Spivak's Calculus on Manifolds, I am confused about the dot after the first equality sign. According to the chain rule, . How is the circle changed to the dot? How to interpret this? Thanks in advance.",D(g\circ f)(a)=Dg(f(a))\circ Df(a),"['real-analysis', 'multivariable-calculus', 'chain-rule']"
52,Problem while calculating the area of $S^2$ using differential forms,Problem while calculating the area of  using differential forms,S^2,"I am trying to calculate the area of the semi-sphere $S^2_-$ using differential forms, I have the only local chart $(A,\phi,S^2_-)$ given by $A=\{(x,y)\in\mathbb{R}:x^2+y^2< 1\}$ and $\phi(x,y)={^t(-x, y, -\sqrt{1-x^2-y^2})}$ . Also I have the unitary normal vector field $N(x,y)={^t(-x, y, -\sqrt{1-x^2-y^2})}$ , so that $\{N,\frac{\partial{\phi}}{\partial{x}},\frac{\partial{\phi}}{\partial{y}}\}$ form an oriented basis of $\mathbb{R}^3$ . Now if I calculate by hand $\phi^*(\iota(N)dx\wedge dy\wedge dz)$ I get \begin{align} &\phi^*(-xdy\wedge dz-y dx\wedge dz-\sqrt{1-x^2-y^2}dx\wedge dy)\\ &=-(-x)dy\wedge\left(\frac{x}{\sqrt{1-x^2-y^2}}dx+\frac{y}{\sqrt{1-x^2-y^2}}dy\right)\\ &\qquad-yd(-x)\wedge\left(\frac{x}{\sqrt{1-x^2-y^2}}dx+\frac{y}{\sqrt{1-x^2-y^2}}dy\right)\\ &\qquad-\sqrt{1-x^2-y^2}d(-x)\wedge dy\\ &=\left(\frac{-x^2}{\sqrt{1-x^2-y^2}}+\frac{y^2}{\sqrt{1-x^2-y^2}}+\sqrt{1-x^2-y^2}\right)dx\wedge dy \end{align} so $\int_{S^2_-}\iota(N)dx\wedge dy\wedge dz=\int_{A}\phi^*(\iota(N)dx\wedge dy\wedge dz)=\int_A \frac{1-2x^2}{\sqrt{1-x^2-y^2}}dxdy$ . At this point I know there is a mistake because last integral should be $\int_A \frac{1}{\sqrt{1-x^2-y^2}}dxdy$ but I really can't figure out where this mistake is.","I am trying to calculate the area of the semi-sphere using differential forms, I have the only local chart given by and . Also I have the unitary normal vector field , so that form an oriented basis of . Now if I calculate by hand I get so . At this point I know there is a mistake because last integral should be but I really can't figure out where this mistake is.","S^2_- (A,\phi,S^2_-) A=\{(x,y)\in\mathbb{R}:x^2+y^2< 1\} \phi(x,y)={^t(-x, y, -\sqrt{1-x^2-y^2})} N(x,y)={^t(-x, y, -\sqrt{1-x^2-y^2})} \{N,\frac{\partial{\phi}}{\partial{x}},\frac{\partial{\phi}}{\partial{y}}\} \mathbb{R}^3 \phi^*(\iota(N)dx\wedge dy\wedge dz) \begin{align}
&\phi^*(-xdy\wedge dz-y dx\wedge dz-\sqrt{1-x^2-y^2}dx\wedge dy)\\
&=-(-x)dy\wedge\left(\frac{x}{\sqrt{1-x^2-y^2}}dx+\frac{y}{\sqrt{1-x^2-y^2}}dy\right)\\
&\qquad-yd(-x)\wedge\left(\frac{x}{\sqrt{1-x^2-y^2}}dx+\frac{y}{\sqrt{1-x^2-y^2}}dy\right)\\
&\qquad-\sqrt{1-x^2-y^2}d(-x)\wedge dy\\
&=\left(\frac{-x^2}{\sqrt{1-x^2-y^2}}+\frac{y^2}{\sqrt{1-x^2-y^2}}+\sqrt{1-x^2-y^2}\right)dx\wedge dy
\end{align} \int_{S^2_-}\iota(N)dx\wedge dy\wedge dz=\int_{A}\phi^*(\iota(N)dx\wedge dy\wedge dz)=\int_A \frac{1-2x^2}{\sqrt{1-x^2-y^2}}dxdy \int_A \frac{1}{\sqrt{1-x^2-y^2}}dxdy","['multivariable-calculus', 'differential-geometry', 'differential-forms']"
53,"$\lim_{x\rightarrow0,y\rightarrow0}\frac{xy}{\ln(x^2+y^2)}=?$",,"\lim_{x\rightarrow0,y\rightarrow0}\frac{xy}{\ln(x^2+y^2)}=?","$\lim\limits_{x\rightarrow0,y\rightarrow0}\frac{xy}{\ln(x^2+y^2)}=?$ Now intuitively(if I am right), when I substitute $x$ and $y$ with zeros I get $\frac{0}{-\infty}$ - which is $0$ . I also checked on Wolfram Alpha and solution is $0$ . If this is okay I struggle with formally writing the solution, or is it okay to leave it like this? --EDIT-- Now I also have to show the existence of partial derivatives at point $(0,0)$ $\lim\limits_{x\rightarrow 0}=\frac{f(x+0,0)-f(0,0)}{x}=...=0$ , is this okay? Same for $y$ .","Now intuitively(if I am right), when I substitute and with zeros I get - which is . I also checked on Wolfram Alpha and solution is . If this is okay I struggle with formally writing the solution, or is it okay to leave it like this? --EDIT-- Now I also have to show the existence of partial derivatives at point , is this okay? Same for .","\lim\limits_{x\rightarrow0,y\rightarrow0}\frac{xy}{\ln(x^2+y^2)}=? x y \frac{0}{-\infty} 0 0 (0,0) \lim\limits_{x\rightarrow 0}=\frac{f(x+0,0)-f(0,0)}{x}=...=0 y","['limits', 'multivariable-calculus']"
54,Is there an example such that $\frac{\partial f}{\partial y}$ vanishes at some point but we can solve for $y$ in terms of $x$ near the point and ...?,Is there an example such that  vanishes at some point but we can solve for  in terms of  near the point and ...?,\frac{\partial f}{\partial y} y x,"I am reading ""Analysis on Manifolds"" by James R. Munkres. On p.77, Example 4. Let $f:\mathbb{R}^2\to\mathbb{R}$ be given by the equation $$f(x,y)=y^2-x^4.$$ Then $(0,0)$ is a solution of the equation $f(x,y)=0$ . Because $\frac{\partial f}{\partial y}$ vanishes at $(0,0)$ , we do not expect to be able to solve for $y$ in terms of $x$ near $(0,0)$ . In fact, however, we can do so, and we can do so in such a way that the resulting function is differentiable. However, the solution is not unique. I had the following question: Is there an example such that $f(x,y)=0$ at some point and $\frac{\partial f}{\partial y}$ vanishes at the point but we can solve for $y$ in terms of $x$ near the point and the resulting function is differentiable and unique near the point?","I am reading ""Analysis on Manifolds"" by James R. Munkres. On p.77, Example 4. Let be given by the equation Then is a solution of the equation . Because vanishes at , we do not expect to be able to solve for in terms of near . In fact, however, we can do so, and we can do so in such a way that the resulting function is differentiable. However, the solution is not unique. I had the following question: Is there an example such that at some point and vanishes at the point but we can solve for in terms of near the point and the resulting function is differentiable and unique near the point?","f:\mathbb{R}^2\to\mathbb{R} f(x,y)=y^2-x^4. (0,0) f(x,y)=0 \frac{\partial f}{\partial y} (0,0) y x (0,0) f(x,y)=0 \frac{\partial f}{\partial y} y x","['multivariable-calculus', 'implicit-function-theorem']"
55,Why is the second derivative of $f:\mathbb R^n \to \mathbb R$ a function $\mathbb R^n \times \mathbb R^n \to \mathbb R$?,Why is the second derivative of  a function ?,f:\mathbb R^n \to \mathbb R \mathbb R^n \times \mathbb R^n \to \mathbb R,"In my multivariable analysis class notes my teacher wrote the following: Let $D \subseteq \Bbb R^n$ , $p \in D$ and $f:D \to \mathbb R$ be a class $C^2$ function. The function: $$\mathbb R^n \times \mathbb R^n \to \mathbb R$$ $$((y_1,...,y_n),(z_1,...,z_n)) \mapsto \sum_{i,j = 1}^n y_i z_j \frac{\partial ^2 f}{\partial x_j\partial x_i}(p)$$ Is the second derivative of the function $f$ at the point $p$ , denoted ad $f''(p)$ . Why is the second derivative a function with domain $\mathbb R^n \times \mathbb R^n$ ? If $\mathcal L(A,B)$ denotes the set of all bounded linear operators from $A$ to $B$ , shouldn't the second derivative of $f$ at $p$ be a function $f'' : \mathbb R^n \to \mathcal{L}( \mathbb R^n,\mathcal{L}(\mathbb R^n, \mathbb R) )$ instead?","In my multivariable analysis class notes my teacher wrote the following: Let , and be a class function. The function: Is the second derivative of the function at the point , denoted ad . Why is the second derivative a function with domain ? If denotes the set of all bounded linear operators from to , shouldn't the second derivative of at be a function instead?","D \subseteq \Bbb R^n p \in D f:D \to \mathbb R C^2 \mathbb R^n \times \mathbb R^n \to \mathbb R ((y_1,...,y_n),(z_1,...,z_n)) \mapsto \sum_{i,j = 1}^n y_i z_j \frac{\partial ^2 f}{\partial x_j\partial x_i}(p) f p f''(p) \mathbb R^n \times \mathbb R^n \mathcal L(A,B) A B f p f'' : \mathbb R^n \to \mathcal{L}( \mathbb R^n,\mathcal{L}(\mathbb R^n, \mathbb R) )","['multivariable-calculus', 'derivatives']"
56,Why is the component of a differentiable function also differentiable?,Why is the component of a differentiable function also differentiable?,,"I'm self studying with the Spivak book, and I'm trying to see a result that I know to be true but I'm having some time seeing how the maths pans out.  Supposing a function $f$ is differentiable.  Then given the projection operator $\pi^i$ where $\pi^i(f)=f^i$ , $f^i$ must also be differentiable. I've been trying to work it out by applying the chain rule $$Df^i(a)=D(\pi^i\circ f)(a)=D\pi^i(f(a))\circ Df(a)$$ since the projection operator is linear, it's derivative is simply itself $$Df^i(a)=\pi^i(f(a))\circ Df(a)$$ $$Df^i(a)=f^i(a)\circ Df(a)$$ I'm fairly sure the steps leading up to this point are correct, however it doesn't seem to make sense.  For example, let $f:\mathbb{R}\rightarrow\mathbb{R}^2$ $$f=\begin{bmatrix}x^2\\x\end{bmatrix}$$ then $$Df=\begin{bmatrix}2x\\1\end{bmatrix}$$ So if we let $i=1$ , $$\begin{align} Df^1(a)&=f^1(a)\circ Df(a) \\ &=a^2\circ\begin{bmatrix}2a\\1\end{bmatrix} \end{align}$$ Which I am certain is wrong, since $Df:\mathbb{R}\rightarrow\mathbb{R}^2$ , but $f^1:\mathbb{R}\rightarrow\mathbb{R}$ so it doesn't match up.  I think I have misunderstood either the derivative of the projection operator or how $f^i(a)$ forms a composition with $Df(a)$ .  Could someone point me in the right direction?  Thanks!","I'm self studying with the Spivak book, and I'm trying to see a result that I know to be true but I'm having some time seeing how the maths pans out.  Supposing a function is differentiable.  Then given the projection operator where , must also be differentiable. I've been trying to work it out by applying the chain rule since the projection operator is linear, it's derivative is simply itself I'm fairly sure the steps leading up to this point are correct, however it doesn't seem to make sense.  For example, let then So if we let , Which I am certain is wrong, since , but so it doesn't match up.  I think I have misunderstood either the derivative of the projection operator or how forms a composition with .  Could someone point me in the right direction?  Thanks!","f \pi^i \pi^i(f)=f^i f^i Df^i(a)=D(\pi^i\circ f)(a)=D\pi^i(f(a))\circ Df(a) Df^i(a)=\pi^i(f(a))\circ Df(a) Df^i(a)=f^i(a)\circ Df(a) f:\mathbb{R}\rightarrow\mathbb{R}^2 f=\begin{bmatrix}x^2\\x\end{bmatrix} Df=\begin{bmatrix}2x\\1\end{bmatrix} i=1 \begin{align}
Df^1(a)&=f^1(a)\circ Df(a) \\
&=a^2\circ\begin{bmatrix}2a\\1\end{bmatrix}
\end{align} Df:\mathbb{R}\rightarrow\mathbb{R}^2 f^1:\mathbb{R}\rightarrow\mathbb{R} f^i(a) Df(a)","['multivariable-calculus', 'self-learning']"
57,Finding the surface area of a sphere within a cylinder and above the $xy$-plane using double integral,Finding the surface area of a sphere within a cylinder and above the -plane using double integral,xy,"I'm trying to find the surface area of the part of the sphere $x^2+y^2+z^2=a^2$ that lies within the cylinder $x^2+y^2=ax$ and above the xy-plane. I want to do this using $$A(S)=\iint_D \sqrt{[f_{x}(x,y)]^2+[f_{y}(x,y)]^2+1} \; dA.$$ My understanding is that $f=z=\sqrt{a^2-x^2-y^2}$ , $z_x=-x(a^2-x^2-y^2)^{-1/2}$ , $z_y=-y(a^2-x^2-y^2)^{-1/2}$ . What I don't understand is what the bounds for the integrals should be. I think it has to be done using polar coordinates, but I'm lost as to what the radius and angle bounds would be -- I'm not visualizing the cylinder and sphere very well. Any help would be appreciated.","I'm trying to find the surface area of the part of the sphere that lies within the cylinder and above the xy-plane. I want to do this using My understanding is that , , . What I don't understand is what the bounds for the integrals should be. I think it has to be done using polar coordinates, but I'm lost as to what the radius and angle bounds would be -- I'm not visualizing the cylinder and sphere very well. Any help would be appreciated.","x^2+y^2+z^2=a^2 x^2+y^2=ax A(S)=\iint_D \sqrt{[f_{x}(x,y)]^2+[f_{y}(x,y)]^2+1} \; dA. f=z=\sqrt{a^2-x^2-y^2} z_x=-x(a^2-x^2-y^2)^{-1/2} z_y=-y(a^2-x^2-y^2)^{-1/2}","['integration', 'geometry', 'multivariable-calculus', 'multiple-integral']"
58,Curvature and Torsion for a Space Curve,Curvature and Torsion for a Space Curve,,"Calculate the curvature and torsion of x= $\theta - \sin \theta, y = 1 - \cos \theta, z = 4 \sin (\theta/2)$ $\vec r = (θ - \sin θ)i+(1-\cos θ)j + 4 \sin (θ/2) \ k$ $\vec dr/dt = (1- \cos θ)i + \sin θ \ j+2 \cos (θ/2) \ k$ $d^2\vec r/dt^2 = \sin θ \ i + \cos θ \ j - sin (θ/2) \ k$ Since $k = |\vec dr/dt \times d^2\vec r/dt^2| \  / \ |\vec dr/dt|^3$ I calculated $\vec dr/dt \times d^2\vec r/dt^2$ , which upon simplification gave, $\vec dr/dt \times d^2\vec r/dt^2 = (\sin θ \sin (θ/2) - 2 \cosθ \cos(θ/2)) \ i + (3 \cos θ + 1) \sin (θ/2) \ j + (\cos θ-1) \ k$ In calculating | $\vec dr/dt$ x $d^2$$\vec r/d$$t^2$ | is where I'm starting to face complications. I'm not sure if I am doing this correctly or not. But I understand the application of the formulas of torsion and curvature. Kindly guide me for the same.","Calculate the curvature and torsion of x= Since I calculated , which upon simplification gave, In calculating | x | is where I'm starting to face complications. I'm not sure if I am doing this correctly or not. But I understand the application of the formulas of torsion and curvature. Kindly guide me for the same.","\theta - \sin \theta, y = 1 - \cos \theta, z = 4 \sin (\theta/2) \vec r = (θ - \sin θ)i+(1-\cos θ)j + 4 \sin (θ/2) \ k \vec dr/dt = (1- \cos θ)i + \sin θ \ j+2 \cos (θ/2) \ k d^2\vec r/dt^2 = \sin θ \ i + \cos θ \ j - sin (θ/2) \ k k = |\vec dr/dt \times d^2\vec r/dt^2| \  / \ |\vec dr/dt|^3 \vec dr/dt \times d^2\vec r/dt^2 \vec dr/dt \times d^2\vec r/dt^2 = (\sin θ \sin (θ/2) - 2 \cosθ \cos(θ/2)) \ i + (3 \cos θ + 1) \sin (θ/2) \ j + (\cos θ-1) \ k \vec dr/dt d^2\vec r/dt^2","['multivariable-calculus', 'curvature']"
59,"When does the fact that $\underset{(k,k)}{\sum\sum} a_{k,k}$ converges imply convergence for $\underset{(k,\ell)}{\sum\sum} a_{k,\ell}$?",When does the fact that  converges imply convergence for ?,"\underset{(k,k)}{\sum\sum} a_{k,k} \underset{(k,\ell)}{\sum\sum} a_{k,\ell}","I have come to a problem in a multivariate calculus book that I'm having some trouble with. The book is ""A Course in Multivariate Calculus and Analysis"" by Ghorpade and Limaye. The problem goes : Let $\underset{(k,\ell)}{\sum\sum} a_{k,\ell}$ be a double series whose terms are schematically given by : \begin{equation} \begin{matrix}             1 &  2 		&  4 		&  8 & \dots \\  -\frac{1}{2} & -1  		& -2 		& -4 & \dots \\  -\frac{1}{4} & -\frac{1}{2}  	& -1 		& -2 & \dots \\  -\frac{1}{8} & -\frac{1}{4}  	& -\frac{1}{2}  & -1 & \dots \\ \vdots & \vdots & \vdots & \vdots & \;  \end{matrix} \end{equation} and let $A_{m,n}$ denote its $(m,n)$ th partial double sum. Show that each row-series is divergent, but each column-series converges to $0$ . Also, show that $A_{m,m} \rightarrow 2$ as $m \rightarrow \infty$ . Is $\underset{(k,\ell)}{\sum\sum} a_{k,\ell}$ convergent ? I am able to solve all of the problem except for the last part, namely determining if $\underset{(k,\ell)}{\sum\sum} a_{k,\ell}$ is convergent. My question really is under what conditions for $(a_{k,\ell})$ does : \begin{equation} (A_{m,m}) \text{ is convergent } \Rightarrow (A_{m,n}) \text{ is convergent } \end{equation} Could someone maybe provide an example where the above implication is not true ?","I have come to a problem in a multivariate calculus book that I'm having some trouble with. The book is ""A Course in Multivariate Calculus and Analysis"" by Ghorpade and Limaye. The problem goes : Let be a double series whose terms are schematically given by : and let denote its th partial double sum. Show that each row-series is divergent, but each column-series converges to . Also, show that as . Is convergent ? I am able to solve all of the problem except for the last part, namely determining if is convergent. My question really is under what conditions for does : Could someone maybe provide an example where the above implication is not true ?","\underset{(k,\ell)}{\sum\sum} a_{k,\ell} \begin{equation}
\begin{matrix}
            1 &  2 		&  4 		&  8 & \dots \\
 -\frac{1}{2} & -1  		& -2 		& -4 & \dots \\
 -\frac{1}{4} & -\frac{1}{2}  	& -1 		& -2 & \dots \\
 -\frac{1}{8} & -\frac{1}{4}  	& -\frac{1}{2}  & -1 & \dots \\
\vdots & \vdots & \vdots & \vdots & \; 
\end{matrix}
\end{equation} A_{m,n} (m,n) 0 A_{m,m} \rightarrow 2 m \rightarrow \infty \underset{(k,\ell)}{\sum\sum} a_{k,\ell} \underset{(k,\ell)}{\sum\sum} a_{k,\ell} (a_{k,\ell}) \begin{equation}
(A_{m,m}) \text{ is convergent } \Rightarrow (A_{m,n}) \text{ is convergent }
\end{equation}","['sequences-and-series', 'multivariable-calculus', 'convergence-divergence']"
60,Prove this multiple integrals equation,Prove this multiple integrals equation,,"let $T$ be the triangle bounded by the $x$ axis, the $y$ axis, and the line $x+y=1$ . Let $\varphi$ be a continuous function on the interval $[0,1]$ , and $m,n\in \mathbb Z^+$ , Show that $$\iint_T \varphi(x+y)x^my^n\,dx\,dy=c_{mn}\int_0^1 \varphi(t)t^{m+n+1}\,dt$$ where $$c_{mn}=\int_0^1(1-t)^mt^n\,dt$$ First i tried to evaluate the right hand side integral to get the other one, but the integrand is a bit hard to evaluate, so i did this change of variables $$x=u-v\\ y=v$$ So the equation will be $$\iint_T\varphi(u)(u-v)^m v^m \,du\,dv= c_{mn}\int_0^1 \varphi(t)t^{m+n+1}\,dt $$ you may wanna ask where is the jacobian determinant? it’s equal to $1$ . Despite the change of variables, the integrand is still hard to integrate.","let be the triangle bounded by the axis, the axis, and the line . Let be a continuous function on the interval , and , Show that where First i tried to evaluate the right hand side integral to get the other one, but the integrand is a bit hard to evaluate, so i did this change of variables So the equation will be you may wanna ask where is the jacobian determinant? it’s equal to . Despite the change of variables, the integrand is still hard to integrate.","T x y x+y=1 \varphi [0,1] m,n\in \mathbb Z^+ \iint_T \varphi(x+y)x^my^n\,dx\,dy=c_{mn}\int_0^1 \varphi(t)t^{m+n+1}\,dt c_{mn}=\int_0^1(1-t)^mt^n\,dt x=u-v\\ y=v \iint_T\varphi(u)(u-v)^m v^m \,du\,dv= c_{mn}\int_0^1 \varphi(t)t^{m+n+1}\,dt  1","['multivariable-calculus', 'multiple-integral']"
61,How to prove this function isn't differentiable at origin [duplicate],How to prove this function isn't differentiable at origin [duplicate],,"This question already has an answer here : $f(x,y) = \frac{x^3y}{x^4 + y^2}$ is not differentiable at $(0,0)$. (1 answer) Closed 3 years ago . [Exercise 5.18 - Pugh] Show that the function $f:\mathbb{R}^2\rightarrow \mathbb{R}$ defined by $$ f(x,y)= \begin{cases} \frac{x^3y}{x^4+y^2} \ ; (x,y)\neq 0 \\\quad 0\quad\ ;(x,y)=0 \end{cases}$$ has $\nabla_{(0,0)}f(u)=0$ for all $u$ but is not differentiable at $(0,0)$ . My attempt Isn't hard to show the first part (that all directional derivatives exist) but I cannot show that this function is differentiable. My ideia is standard, I need to find a path such that $\lim\limits_{\gamma\;\mapsto (0,0)} f(\gamma)$ doens't exist. In order to achieve this I tried $y=mx, y=mx^2, y=mx^3$ but no one gave me a clear result. At best, I was able to show that $$\lim\limits_{(x,y)\mapsto(0,0)}\frac{f(x,y=x^2)}{f(x,y=-x^2)}=-1 $$ But I don't think that's enough. Is that enough ? If yes, why? If not, is there a better method to solve this ?","This question already has an answer here : $f(x,y) = \frac{x^3y}{x^4 + y^2}$ is not differentiable at $(0,0)$. (1 answer) Closed 3 years ago . [Exercise 5.18 - Pugh] Show that the function defined by has for all but is not differentiable at . My attempt Isn't hard to show the first part (that all directional derivatives exist) but I cannot show that this function is differentiable. My ideia is standard, I need to find a path such that doens't exist. In order to achieve this I tried but no one gave me a clear result. At best, I was able to show that But I don't think that's enough. Is that enough ? If yes, why? If not, is there a better method to solve this ?","f:\mathbb{R}^2\rightarrow \mathbb{R}  f(x,y)= \begin{cases} \frac{x^3y}{x^4+y^2} \ ; (x,y)\neq 0 \\\quad 0\quad\ ;(x,y)=0 \end{cases} \nabla_{(0,0)}f(u)=0 u (0,0) \lim\limits_{\gamma\;\mapsto (0,0)} f(\gamma) y=mx, y=mx^2, y=mx^3 \lim\limits_{(x,y)\mapsto(0,0)}\frac{f(x,y=x^2)}{f(x,y=-x^2)}=-1 ","['real-analysis', 'calculus', 'limits', 'multivariable-calculus', 'partial-derivative']"
62,A polynomial $ f $ in three variables with real coefficients and any line $l$ in $ \mathbb R^3 $,A polynomial  in three variables with real coefficients and any line  in, f  l  \mathbb R^3 ,"Prove that for every polynomial $ f $ in three variables with real coefficients and any line $\ell$ in $ \mathbb R^3 $ either $ \ell \subset Z_f $ or $ |\ell \cap Z_f|\leq \deg f $ , where $$ Z_f :=\{(x,y,z)\in \mathbb R^3: f(x,y,z)=0 \}.$$ Here is the case for a polynomial $f$ in two variables and line in $\mathbb R^2$ : http://diposit.ub.edu/dspace/bitstream/2445/159040/2/159040.pdf ( Page 16 , Lemma : 2.2.6) which is well understood. But I am stuck on how to solve the case with $3$ variables. Any hint or help is appreciated. Thanks!","Prove that for every polynomial in three variables with real coefficients and any line in either or , where Here is the case for a polynomial in two variables and line in : http://diposit.ub.edu/dspace/bitstream/2445/159040/2/159040.pdf ( Page 16 , Lemma : 2.2.6) which is well understood. But I am stuck on how to solve the case with variables. Any hint or help is appreciated. Thanks!"," f  \ell  \mathbb R^3   \ell \subset Z_f   |\ell \cap Z_f|\leq \deg f   Z_f :=\{(x,y,z)\in \mathbb R^3: f(x,y,z)=0 \}. f \mathbb R^2 3","['real-analysis', 'abstract-algebra', 'multivariable-calculus', 'polynomials']"
63,Difference between a vector in $\Bbb{R^1}$ and a scalar,Difference between a vector in  and a scalar,\Bbb{R^1},"Many people say that they are the same, even I can't find much difference in them except that a vector/ matrix can be multiplied by any scalar, but to multiply it with a vector in $\Bbb{R^1}$ the vector or matrix should be of the order $1\times n$ . What's and why is there a difference in this case?","Many people say that they are the same, even I can't find much difference in them except that a vector/ matrix can be multiplied by any scalar, but to multiply it with a vector in the vector or matrix should be of the order . What's and why is there a difference in this case?",\Bbb{R^1} 1\times n,"['linear-algebra', 'multivariable-calculus', 'vector-spaces', 'vectors']"
64,Find the intersection of $z=1-x^2$ and $x=y^3$ in $\mathbb R^3$,Find the intersection of  and  in,z=1-x^2 x=y^3 \mathbb R^3,"I have these 2 surfaces, given in their Cartesian form, and want to find the resulting surface of intersection. My approach is to find parametric representations. But I have two approaches, and not sure if both approaches are proper, or if one or the other is only proper. Here are the equations: $$\tag{1}z = 1 - x^2$$ $$\tag{2}x = y^3$$ Approach 1 : Substitute (2) into (1),  we get the cartesian form of the intersection surface: $z = 1 - x^6$ , then try to parameterize this, but I run into $2$ independent parameters with this approach. Approach 2 : Direct parameterization: Just looking at both equations I see that ' $x$ ' is the common variable so I set $x= t$ . Then just plugging in ' $t$ ' into the equations, and put them together in $(x,y,z)$ , I get: $$(x,y,z) = (t, t^{1/3}, 1-t^2)$$ So not sure if either of these 2 approaches above give the correct parameterization. Not sure if there is a systematic procedure to do parameterization of surfaces of intersection in $\mathbb R^3$ , or if this is a creative type of process, and hence there is no systematic procedure to follow. Hope someone can explain how to think about this kind of problem.","I have these 2 surfaces, given in their Cartesian form, and want to find the resulting surface of intersection. My approach is to find parametric representations. But I have two approaches, and not sure if both approaches are proper, or if one or the other is only proper. Here are the equations: Approach 1 : Substitute (2) into (1),  we get the cartesian form of the intersection surface: , then try to parameterize this, but I run into independent parameters with this approach. Approach 2 : Direct parameterization: Just looking at both equations I see that ' ' is the common variable so I set . Then just plugging in ' ' into the equations, and put them together in , I get: So not sure if either of these 2 approaches above give the correct parameterization. Not sure if there is a systematic procedure to do parameterization of surfaces of intersection in , or if this is a creative type of process, and hence there is no systematic procedure to follow. Hope someone can explain how to think about this kind of problem.","\tag{1}z = 1 - x^2 \tag{2}x = y^3 z = 1 - x^6 2 x x= t t (x,y,z) (x,y,z) = (t, t^{1/3}, 1-t^2) \mathbb R^3","['multivariable-calculus', 'differential-geometry', 'curves', 'surfaces']"
65,Why is $\nabla_{\mathbf{x}}z \not= \nabla_{\mathbf{y}}z \times \frac{\partial\mathbf{y}}{\partial\mathbf{x}} $?,Why is ?,\nabla_{\mathbf{x}}z \not= \nabla_{\mathbf{y}}z \times \frac{\partial\mathbf{y}}{\partial\mathbf{x}} ,"I'm learning about the chain rule to compute the gradient, w.r.t to a subset of its variables, of a function that is a composition of vector-fields. Let $\mathbf{x}\in \mathbb{R}^m, \mathbf{y}\in \mathbb{R}^n, \mathbf{g}:\mathbb{R}^m\to\mathbb{R}^n, f:\mathbb{R}^n\to \mathbb{R}$ . The result is ""If $\mathbf{y}=\mathbf{g}(x), z=f(\mathbf{y})=f(\mathbf{g}(x))$ , then $\nabla_{\mathbf{x}}z=\left(\frac{\partial\mathbf{y} }{\partial\mathbf{x}}\right) ^T \times \nabla_{\mathbf{y}}z$ "" But how is this derived? My initial attempt to derive this is as follows. Please tell me where it's gone astray. $$z=f(\mathbf{g}(x))\implies \frac{\partial z}{\partial \mathbf{y}} \times \frac{\partial \mathbf{y}}{\partial \mathbf{x}} =\nabla_{y}z \times \frac{\partial \mathbf{y}}{\partial \mathbf{x}}$$ Where I have used the chain rule to obtain the second equality and the definition of the gradient to obtain the third. This can't be correct as the dimensions don't agree, the final product an $n \times1 $ matrix times a $n \times m$ matrix. Where have I misapplied the chain rule and/or a definition? Thanks :) $\nabla_{\mathbf{x}}z \not= \nabla_{\mathbf{y}}z \times \frac{\partial\mathbf{y}}{\partial\mathbf{x}} $","I'm learning about the chain rule to compute the gradient, w.r.t to a subset of its variables, of a function that is a composition of vector-fields. Let . The result is ""If , then "" But how is this derived? My initial attempt to derive this is as follows. Please tell me where it's gone astray. Where I have used the chain rule to obtain the second equality and the definition of the gradient to obtain the third. This can't be correct as the dimensions don't agree, the final product an matrix times a matrix. Where have I misapplied the chain rule and/or a definition? Thanks :)","\mathbf{x}\in \mathbb{R}^m, \mathbf{y}\in \mathbb{R}^n, \mathbf{g}:\mathbb{R}^m\to\mathbb{R}^n, f:\mathbb{R}^n\to \mathbb{R} \mathbf{y}=\mathbf{g}(x), z=f(\mathbf{y})=f(\mathbf{g}(x)) \nabla_{\mathbf{x}}z=\left(\frac{\partial\mathbf{y} }{\partial\mathbf{x}}\right) ^T \times \nabla_{\mathbf{y}}z z=f(\mathbf{g}(x))\implies
\frac{\partial z}{\partial \mathbf{y}} \times \frac{\partial \mathbf{y}}{\partial \mathbf{x}}
=\nabla_{y}z \times \frac{\partial \mathbf{y}}{\partial \mathbf{x}} n \times1  n \times m \nabla_{\mathbf{x}}z \not= \nabla_{\mathbf{y}}z \times \frac{\partial\mathbf{y}}{\partial\mathbf{x}} ","['multivariable-calculus', 'vector-analysis', 'chain-rule']"
66,Partial derivative of a function with respect to product of variables,Partial derivative of a function with respect to product of variables,,"If I have a function $f(x,y)$ , and all I know about this function is its partial derivatives $\frac{\partial f(x,y)}{\partial x}$ and $\frac{\partial f(x,y)}{\partial y}$ as well as the position $(x_i, y_i)$ at which these partial derivatives were obtained, is it possible to obtain the partial derivative of $f(x,z)$ with respect to a new variable $z = x y$ while keeping $x$ constant, i.e. $\frac{\partial f(x,z)}{\partial (z)}$ at the same point $(x_i, y_i)$ , or $(x_i, x_i y_i)$ ?","If I have a function , and all I know about this function is its partial derivatives and as well as the position at which these partial derivatives were obtained, is it possible to obtain the partial derivative of with respect to a new variable while keeping constant, i.e. at the same point , or ?","f(x,y) \frac{\partial f(x,y)}{\partial x} \frac{\partial f(x,y)}{\partial y} (x_i, y_i) f(x,z) z = x y x \frac{\partial f(x,z)}{\partial (z)} (x_i, y_i) (x_i, x_i y_i)","['multivariable-calculus', 'derivatives', 'partial-derivative']"
67,Proving a statement using the information about function's derivatives.,Proving a statement using the information about function's derivatives.,,"Function $u(x,y)$ is called a harmonic function if it has partial derivatives from the first degree ( $f'_x , f'_y)$ and second degree ( $f''_{xx}, f''_{xy}, f''_{yx}, f''_{yy}$ ) that are continuous, and for every $(x,y)$ this equation holds: $\frac{\partial ^2u}{\partial x^2}+ \frac{\partial ^2u}{\partial y^2} =0$ Prove: if $u(x,y)$ is a harmonic function and $f(u)\in C^2, (u\ne Constant)$ (I'm not sure of the notation but from the question I can understand it means that partial derivatives are continuous up to second degree), and the function $z(x,y)=f(u(x,y))$ is harmonic function, then $f(u)=Au + B$ . Before adding my work and attempt, I have to say I haven't really understood what $A,B$ are in what I needed to prove, I don't know if it's a mistake in the question or just me not understanding, but I decided to look at them as constants because it's the only thing I thought of that they could be. My attempt : All what I have tried is to take derivatives and find conclusions and try to use the information that I got about harmonifc functions: Updated $z_x=f'(u)u_x$ $z_y=f'(u)u_y$ $z_{xx}=f''(u)u_x^2 + f'(u)u_{xx}$ $z_{yy}=f''(u)u_y^2 + f'(u)u_{yy}$ Now given that $z$ is harmonic: $z_{xx}+z_{yy} = 0$ $f''(u)u_x^2 + f'(u)u_{xx} + f''(u)u_y^2 + f'(u)u_{yy}=f'(u)[u_{xx}+u_{yy}]+f''(u)[u_{x}^2+u_y^2]$ And since $u$ is harmonic we get $=f''(u)[u_x^2+u_y^2]=0$ I'm not sure how to continue since I can't see any problem in $f''(u)=0$ or $u_x^2+u_y^2=0$ . BUT I know one thing, if I get that $f''(u)=0$ that means $f'(u)=c$ where $c$ is a constant, which means $f(u)=Au + B$ . Update 2 if $u_x^2 + u_y^2 = 0$ , then $ u_x=0 and u_y=0$ , and that means $u=Const$ , which contradicts the information that $u\ne Const$ . The idea / approach I have in my mind is to find $f'(u)=A$ (some constant), and then I will have that $f(u)=Au+B$ . Any help is really appreciated, thanks in advance.","Function is called a harmonic function if it has partial derivatives from the first degree ( and second degree ( ) that are continuous, and for every this equation holds: Prove: if is a harmonic function and (I'm not sure of the notation but from the question I can understand it means that partial derivatives are continuous up to second degree), and the function is harmonic function, then . Before adding my work and attempt, I have to say I haven't really understood what are in what I needed to prove, I don't know if it's a mistake in the question or just me not understanding, but I decided to look at them as constants because it's the only thing I thought of that they could be. My attempt : All what I have tried is to take derivatives and find conclusions and try to use the information that I got about harmonifc functions: Updated Now given that is harmonic: And since is harmonic we get I'm not sure how to continue since I can't see any problem in or . BUT I know one thing, if I get that that means where is a constant, which means . Update 2 if , then , and that means , which contradicts the information that . The idea / approach I have in my mind is to find (some constant), and then I will have that . Any help is really appreciated, thanks in advance.","u(x,y) f'_x , f'_y) f''_{xx}, f''_{xy}, f''_{yx}, f''_{yy} (x,y) \frac{\partial ^2u}{\partial x^2}+ \frac{\partial ^2u}{\partial y^2} =0 u(x,y) f(u)\in C^2, (u\ne Constant) z(x,y)=f(u(x,y)) f(u)=Au + B A,B z_x=f'(u)u_x z_y=f'(u)u_y z_{xx}=f''(u)u_x^2 + f'(u)u_{xx} z_{yy}=f''(u)u_y^2 + f'(u)u_{yy} z z_{xx}+z_{yy} = 0 f''(u)u_x^2 + f'(u)u_{xx} + f''(u)u_y^2 + f'(u)u_{yy}=f'(u)[u_{xx}+u_{yy}]+f''(u)[u_{x}^2+u_y^2] u =f''(u)[u_x^2+u_y^2]=0 f''(u)=0 u_x^2+u_y^2=0 f''(u)=0 f'(u)=c c f(u)=Au + B u_x^2 + u_y^2 = 0  u_x=0 and u_y=0 u=Const u\ne Const f'(u)=A f(u)=Au+B","['multivariable-calculus', 'derivatives', 'partial-derivative']"
68,"How to find the critical points of $f(x,y,z) = x^4 + y^4 + z^4 - 4xyz$?",How to find the critical points of ?,"f(x,y,z) = x^4 + y^4 + z^4 - 4xyz","So I'm trying to find the critical points of this function $f(x,y,z) = x^4 + y^4 + z^4 - 4xyz$ , to do that I try to find the points where the gradient of f is equal to $(0,0)$ , though I can't solve the systems of equations, is this even the right way to do it? Here's the system of equations that I'm trying to solve: $4x^3-4yz = 0$ $4y^3-4xz = 0$ $4z^3-4xy = 0$ I first do this: $x^3 = yz$ $y^3 = xz$ $z^3 = xy$ Then this: $y = \dfrac{x^3}{z}$ $z = \dfrac{y^3}{x}$ $x = \dfrac{z^3}{y}$ But then I just go in circles by replacing each variable, I'm probably doing something wrong but I can't see it...","So I'm trying to find the critical points of this function , to do that I try to find the points where the gradient of f is equal to , though I can't solve the systems of equations, is this even the right way to do it? Here's the system of equations that I'm trying to solve: I first do this: Then this: But then I just go in circles by replacing each variable, I'm probably doing something wrong but I can't see it...","f(x,y,z) = x^4 + y^4 + z^4 - 4xyz (0,0) 4x^3-4yz = 0 4y^3-4xz = 0 4z^3-4xy = 0 x^3 = yz y^3 = xz z^3 = xy y = \dfrac{x^3}{z} z = \dfrac{y^3}{x} x = \dfrac{z^3}{y}","['calculus', 'multivariable-calculus', 'systems-of-equations']"
69,"Given $|f(x,y)|\le \sin^2(x^2+y^2)$. Is $f$ differentiable at $(0,0)$?",Given . Is  differentiable at ?,"|f(x,y)|\le \sin^2(x^2+y^2) f (0,0)","Knowing that $|f(x,y)|\le \sin^2(x^2+y^2)$ in all $\mathbb{R}^2$ , then $f$ is differentiable in $(0,0)$ . I don't know how to approach this question, I have learnt recently the definition of differentiability for two variable functions, which is (for point $(0,0)$ ): $f(0+\Delta x,0+ \Delta y)-f(0,0)=f_x(0,0)\Delta x+f_y(0,0)\Delta y+\epsilon\sqrt{(\Delta x)^2+(\Delta y)^2}$ , and if $\epsilon \to 0$ when $\Delta x,\Delta y \to0$ then we say $f$ is differentiable. will the definition of differentiability help me here? I'm finding it extremely hard to find counter examples and check them, so if theres any trick I would love to hear. I would appreciate any hints and pushes in the right direction.","Knowing that in all , then is differentiable in . I don't know how to approach this question, I have learnt recently the definition of differentiability for two variable functions, which is (for point ): , and if when then we say is differentiable. will the definition of differentiability help me here? I'm finding it extremely hard to find counter examples and check them, so if theres any trick I would love to hear. I would appreciate any hints and pushes in the right direction.","|f(x,y)|\le \sin^2(x^2+y^2) \mathbb{R}^2 f (0,0) (0,0) f(0+\Delta x,0+ \Delta y)-f(0,0)=f_x(0,0)\Delta x+f_y(0,0)\Delta y+\epsilon\sqrt{(\Delta x)^2+(\Delta y)^2} \epsilon \to 0 \Delta x,\Delta y \to0 f","['multivariable-calculus', 'derivatives']"
70,Two variables limit that confused my intuition and can't figure out why my intuition is wrong.,Two variables limit that confused my intuition and can't figure out why my intuition is wrong.,,"Background on how I got the intuition: I have recently learnt about two variable limits, and my professor gave us a tip that whenever we have two homogenous polynomials in the denominator and numerator, with $(0,0)$ being the only problematic point, then we can decide if the limit exists and equal to zero or DNE, based on the powers. and we got some examples: $\lim_{(x,y)\to (0,0)}\frac{xy^2}{x^2+y^4}$ , DNE because the power in the denominator is $4$ and numerator $3$ . $\lim_{(x,y)\to (0,0)}\frac{xy^2}{x^2+y^2}=0$ , because the power in the numerator is bigger than the denominator. Basically if the power of the numerator is higher than the power of the denominator, the limit exists and equal zero, else it DNE. The Limit that confused me: $\lim_{(x,y)\to (0,0)}\frac{x^4y^13}{x^8+y^{18}}=0$ , but power in denominator is $18$ and power in numerator is $17$ , so based on the tip he told us, the limit shouldn't exist.. (and I can see that $(0,0)$ is the only problematic point in the denominator). What am I missing?","Background on how I got the intuition: I have recently learnt about two variable limits, and my professor gave us a tip that whenever we have two homogenous polynomials in the denominator and numerator, with being the only problematic point, then we can decide if the limit exists and equal to zero or DNE, based on the powers. and we got some examples: , DNE because the power in the denominator is and numerator . , because the power in the numerator is bigger than the denominator. Basically if the power of the numerator is higher than the power of the denominator, the limit exists and equal zero, else it DNE. The Limit that confused me: , but power in denominator is and power in numerator is , so based on the tip he told us, the limit shouldn't exist.. (and I can see that is the only problematic point in the denominator). What am I missing?","(0,0) \lim_{(x,y)\to (0,0)}\frac{xy^2}{x^2+y^4} 4 3 \lim_{(x,y)\to (0,0)}\frac{xy^2}{x^2+y^2}=0 \lim_{(x,y)\to (0,0)}\frac{x^4y^13}{x^8+y^{18}}=0 18 17 (0,0)","['limits', 'multivariable-calculus']"
71,"Showing $\lim_{p\to\infty}\left(\int_I|f|^p\right)^{1/p} = \max|f|$, where $I$ is a generalized rectangle","Showing , where  is a generalized rectangle",\lim_{p\to\infty}\left(\int_I|f|^p\right)^{1/p} = \max|f| I,Let $I$ be a generalized rectangle and let $f: I \to \mathbb{R}$ be continuous. Show that $$\lim_{p\to\infty}\left(\int_I|f|^p\right)^{1/p} = \max|f|$$ I found it straightforward to show that these integrals in the limit sequence are properly defined and I showed the LHS $\leq$ RHS by using the definition of the integral using partitions. I am confused about the approach to showing that RHS $\leq$ LHS to give equality. Thanks for any hints and suggestions.,Let be a generalized rectangle and let be continuous. Show that I found it straightforward to show that these integrals in the limit sequence are properly defined and I showed the LHS RHS by using the definition of the integral using partitions. I am confused about the approach to showing that RHS LHS to give equality. Thanks for any hints and suggestions.,I f: I \to \mathbb{R} \lim_{p\to\infty}\left(\int_I|f|^p\right)^{1/p} = \max|f| \leq \leq,"['integration', 'sequences-and-series', 'limits', 'analysis', 'multivariable-calculus']"
72,Volume bounded by the surface $x^n + y^n + z^n = a^n$,Volume bounded by the surface,x^n + y^n + z^n = a^n,"Calculate the volume bounded by the surface $x^n + y^n + z^n = a^n$ $(x>0,y>0,z>0)$ . $$\iiint\limits_{x^n+y^n+z^n \le a^n \\ \ \ \ \ \ \ x,y,z > 0}\mathrm dx~ \mathrm dy ~\mathrm dz = \begin{bmatrix}x = r\cos\varphi\sin\psi \\ y = r\sin\varphi \sin\psi \\ z = r\cos\psi\end{bmatrix} = \iiint\limits_{r^n \le a^n} \underbrace{r^2 \sin \psi}_{J} ~\mathrm d\varphi ~\mathrm d\psi ~\mathrm dr =\\= \int_0^a r^2\mathrm dr \int_0^{\pi/2}\mathrm d\varphi \int_0^{\pi/2}\sin\psi~ \mathrm d\psi$$ Am I going right? I'm not sure about the bounds of the last three integrals. For even $n$ the graph looks like the following: For odd $n$ the first quadrant is alike.",Calculate the volume bounded by the surface . Am I going right? I'm not sure about the bounds of the last three integrals. For even the graph looks like the following: For odd the first quadrant is alike.,"x^n + y^n + z^n = a^n (x>0,y>0,z>0) \iiint\limits_{x^n+y^n+z^n \le a^n \\ \ \ \ \ \ \ x,y,z > 0}\mathrm dx~ \mathrm dy ~\mathrm dz = \begin{bmatrix}x = r\cos\varphi\sin\psi \\ y = r\sin\varphi \sin\psi \\ z = r\cos\psi\end{bmatrix} = \iiint\limits_{r^n \le a^n} \underbrace{r^2 \sin \psi}_{J} ~\mathrm d\varphi ~\mathrm d\psi ~\mathrm dr =\\= \int_0^a r^2\mathrm dr \int_0^{\pi/2}\mathrm d\varphi \int_0^{\pi/2}\sin\psi~ \mathrm d\psi n n","['calculus', 'integration', 'multivariable-calculus', 'volume', 'multiple-integral']"
73,"Find the extrema of $f(x,y) = x^3\cdot y^3$ in $\mathbb{R}^2$",Find the extrema of  in,"f(x,y) = x^3\cdot y^3 \mathbb{R}^2","I am asked to find the extrema of $$f(x,y) = x^3\cdot y^3$$ in $\mathbb{R}^2$ However, using the Hessian criteria, I get that the determinant of the Hessian matrix is zero for the two possible critical points: $(0,y)$ and $(x,0)$ . Since it is zero, I can't know if it is a $\min$ / $\max$ or a saddle point. I don't know what method or what to do next. What method is there for when this happens? How can I determine wether they are $\min$ or $\max$ ? Thanks!","I am asked to find the extrema of in However, using the Hessian criteria, I get that the determinant of the Hessian matrix is zero for the two possible critical points: and . Since it is zero, I can't know if it is a / or a saddle point. I don't know what method or what to do next. What method is there for when this happens? How can I determine wether they are or ? Thanks!","f(x,y) = x^3\cdot y^3 \mathbb{R}^2 (0,y) (x,0) \min \max \min \max","['multivariable-calculus', 'partial-derivative', 'maxima-minima', 'hessian-matrix', 'extreme-value-theorem']"
74,Change of variables between different dimensions,Change of variables between different dimensions,,"I was reading my statistical mechanics notes, and upon reading $$ \int\cdots\int_{\mathbb{R}^3} H(q,p)\space\mathrm{e}^{-\beta H(q,p)}\space\mathrm{d}^{3n}p\space\mathrm{d}^{3n}q = \int_0^{\infty} E \space \mathrm{e}^{-\beta E} \space \Omega(E) \space\mathrm{d}E $$ with $(q,p) = (q^1,\dots,q^{3n},p_1,\dots,p_{3n})$ , of course, and $$ \Omega(E) = \frac{\mathrm{d}}{\mathrm{d} E}\int\cdots\int_{H(q,p)\leq E} \mathrm{d}^{3n}p\space\mathrm{d}^{3n}q $$ and I realised I don't know how to make this formal. This looks like a change of variables, but the first integral is ${6n}$ -dimensional and the second one is just $1$ -dimensional. So, I tried with the simpler example of a function $f(u)$ and another function $g(x,y)$ , and tried to find which $\Omega(z)$ do I need for $$ \iint_{(x_0,y_0)}^{(x_1,y_1)} f(g(x,y))\space\mathrm{d}x\mathrm{d}y = \int_{g(x_0,y_0)}^{g(x_1,y_1)} f(u)\space\Omega(u)\space\mathrm{d}u. $$ Now, as $$ \mathrm{d}u = \frac{\partial g}{\partial x}\mathrm{d}x+\frac{\partial g}{\partial y}\mathrm{d}y, $$ I thought maybe I could do $$ \mathrm{d}x = \left(\frac{\partial g}{\partial x}\right)^{-1}\mathrm{d}u - \left(\frac{\partial g}{\partial x}\right)^{-1}\frac{\partial g}{\partial y}\mathrm{d}y $$ and, assuming $\mathrm{d}x\mathrm{d}y$ is actually $\mathrm{d}x\wedge\mathrm{d}y$ , I could substitute this $\mathrm{dx}$ and find $$ \mathrm{d}x\mathrm{d}y = \left(\frac{\partial g}{\partial x}\right)^{-1}\mathrm{d}u\mathrm{d}y. $$ After substituting this new measure inside the integral of $f(g(x,y))$ and manipulating a little, it is found that $$ \Omega(u) \equiv \int_{y_1}^{y_2}\left(\frac{\partial g}{\partial x}\right)^{-1}\mathrm{d}y \equiv \int_{x_1}^{x_2}\left(\frac{\partial g}{\partial y}\right)^{-1}\mathrm{d}x, $$ but I don't see how these two integrals are equal to $$ \frac{\mathrm{d}}{\mathrm{d} u}\iint_{g(x,y)\leq u}\mathrm{d}x\mathrm{d}y, $$ I'm a physicist and this requires too much rigour. How should I prove this?","I was reading my statistical mechanics notes, and upon reading with , of course, and and I realised I don't know how to make this formal. This looks like a change of variables, but the first integral is -dimensional and the second one is just -dimensional. So, I tried with the simpler example of a function and another function , and tried to find which do I need for Now, as I thought maybe I could do and, assuming is actually , I could substitute this and find After substituting this new measure inside the integral of and manipulating a little, it is found that but I don't see how these two integrals are equal to I'm a physicist and this requires too much rigour. How should I prove this?","
\int\cdots\int_{\mathbb{R}^3} H(q,p)\space\mathrm{e}^{-\beta H(q,p)}\space\mathrm{d}^{3n}p\space\mathrm{d}^{3n}q = \int_0^{\infty} E \space \mathrm{e}^{-\beta E} \space \Omega(E) \space\mathrm{d}E
 (q,p) = (q^1,\dots,q^{3n},p_1,\dots,p_{3n}) 
\Omega(E) = \frac{\mathrm{d}}{\mathrm{d} E}\int\cdots\int_{H(q,p)\leq E} \mathrm{d}^{3n}p\space\mathrm{d}^{3n}q
 {6n} 1 f(u) g(x,y) \Omega(z) 
\iint_{(x_0,y_0)}^{(x_1,y_1)} f(g(x,y))\space\mathrm{d}x\mathrm{d}y = \int_{g(x_0,y_0)}^{g(x_1,y_1)} f(u)\space\Omega(u)\space\mathrm{d}u.
 
\mathrm{d}u = \frac{\partial g}{\partial x}\mathrm{d}x+\frac{\partial g}{\partial y}\mathrm{d}y,
 
\mathrm{d}x = \left(\frac{\partial g}{\partial x}\right)^{-1}\mathrm{d}u - \left(\frac{\partial g}{\partial x}\right)^{-1}\frac{\partial g}{\partial y}\mathrm{d}y
 \mathrm{d}x\mathrm{d}y \mathrm{d}x\wedge\mathrm{d}y \mathrm{dx} 
\mathrm{d}x\mathrm{d}y = \left(\frac{\partial g}{\partial x}\right)^{-1}\mathrm{d}u\mathrm{d}y.
 f(g(x,y)) 
\Omega(u) \equiv \int_{y_1}^{y_2}\left(\frac{\partial g}{\partial x}\right)^{-1}\mathrm{d}y \equiv \int_{x_1}^{x_2}\left(\frac{\partial g}{\partial y}\right)^{-1}\mathrm{d}x,
 
\frac{\mathrm{d}}{\mathrm{d} u}\iint_{g(x,y)\leq u}\mathrm{d}x\mathrm{d}y,
","['multivariable-calculus', 'change-of-variable']"
75,"I need a limit definition for the Hessian, does this work?","I need a limit definition for the Hessian, does this work?",,"Let $f: \mathbb{R}^n \to \mathbb{R}$ be of class $C^2$ . Let $x$ be a non-degenerate critical point of $f$ . Prove that there is an open neighborhood of $x$ which contains no other critical points of $f$ . $\textbf{Proof:}$ Suppose the contrary. Then for every $n \in \mathbb{Z}_+$ there is an $x_n \in B\left(x,\frac{1}{n}\right)$ , which is a critical point of $f$ not equal to $x$ . So $x_n$ converge to $x$ . Now let $v_n = \frac{x_n - x}{||x_n-x||}$ . Let us just assume $v_n$ converges to some $v \in \mathbb{S}^{n-1}$ (since $v_n$ is a bounded sequence, it has a convergent subsequence). So by the definition of the Frechet derivative, and since $x_n \to x$ , we have that \begin{align*} 0 &= \lim_{n\to\infty}  \frac{||Df(x_n) - Df(x) - D^2f(x)(x_n -x)||}{||x_n - x||}\\   &=\lim_{n\to\infty}  \frac{||- D^2f(x)(x_n -x)||}{||x_n - x||}\\   &=\lim_{n\to\infty}  ||D^2f(x)v_n||\\   &= D^2f(x)v, \end{align*} where we use the fact that $Df(x) = Df(x_n) = 0$ for all $n \in \mathbb{Z}_+$ ( $x$ and $x_n$ are critical points). This implies that $D^2f(x)v = 0$ , but since $x$ is a nondegenerate point, $D^2f(x)$ is invertible, and since $v \ne 0$ , we have a contradiction. Thus, there is an open neighborhood of $x$ that contains no other critical points of $f$ . Does this approach work? I am not sure if I used a correct definition for the Hessian. Also, I did not use that the Hessian is continuous, which worries me. Regardless of whether this is correct or not, is there a direct approach, or even maybe a more elegant one?","Let be of class . Let be a non-degenerate critical point of . Prove that there is an open neighborhood of which contains no other critical points of . Suppose the contrary. Then for every there is an , which is a critical point of not equal to . So converge to . Now let . Let us just assume converges to some (since is a bounded sequence, it has a convergent subsequence). So by the definition of the Frechet derivative, and since , we have that where we use the fact that for all ( and are critical points). This implies that , but since is a nondegenerate point, is invertible, and since , we have a contradiction. Thus, there is an open neighborhood of that contains no other critical points of . Does this approach work? I am not sure if I used a correct definition for the Hessian. Also, I did not use that the Hessian is continuous, which worries me. Regardless of whether this is correct or not, is there a direct approach, or even maybe a more elegant one?","f: \mathbb{R}^n \to \mathbb{R} C^2 x f x f \textbf{Proof:} n \in \mathbb{Z}_+ x_n \in B\left(x,\frac{1}{n}\right) f x x_n x v_n = \frac{x_n - x}{||x_n-x||} v_n v \in \mathbb{S}^{n-1} v_n x_n \to x \begin{align*}
0 &= \lim_{n\to\infty}  \frac{||Df(x_n) - Df(x) - D^2f(x)(x_n -x)||}{||x_n - x||}\\
  &=\lim_{n\to\infty}  \frac{||- D^2f(x)(x_n -x)||}{||x_n - x||}\\
  &=\lim_{n\to\infty}  ||D^2f(x)v_n||\\
  &= D^2f(x)v,
\end{align*} Df(x) = Df(x_n) = 0 n \in \mathbb{Z}_+ x x_n D^2f(x)v = 0 x D^2f(x) v \ne 0 x f","['real-analysis', 'analysis', 'multivariable-calculus', 'solution-verification', 'hessian-matrix']"
76,How do I compute this integral with a Dirac's delta?,How do I compute this integral with a Dirac's delta?,,"While studying probability I encountered this integral $$I=\int_{\mathbb{R}^2}\exp\left({-\frac{x_1^2+x_2^2}{2}}\right)\delta\left(r-\sqrt{x_1^2+x_2^2}\right)\,dx_1\,dx_2$$ If I compute this in polar coordinates i get $$I=\int_0^{2\pi}\,d\theta \int_0^{+\infty}\exp\left(-\dfrac{\rho^2}{2} \right)\rho\delta(r-\rho)\,d\rho=2\pi r\exp\left(-\dfrac{r^2}{2}\right)$$ but in cartesian coordinates I only get $$I=\exp\left(-\frac{r^2}{2}\right)$$ I don't understand why. I just thougth that I was using the Dirac's delta's properties in both cases. I think the first result is the correct one and there is something I don't know about Dirac's delta with more than one variable. Which result is correct and why?",While studying probability I encountered this integral If I compute this in polar coordinates i get but in cartesian coordinates I only get I don't understand why. I just thougth that I was using the Dirac's delta's properties in both cases. I think the first result is the correct one and there is something I don't know about Dirac's delta with more than one variable. Which result is correct and why?,"I=\int_{\mathbb{R}^2}\exp\left({-\frac{x_1^2+x_2^2}{2}}\right)\delta\left(r-\sqrt{x_1^2+x_2^2}\right)\,dx_1\,dx_2 I=\int_0^{2\pi}\,d\theta \int_0^{+\infty}\exp\left(-\dfrac{\rho^2}{2} \right)\rho\delta(r-\rho)\,d\rho=2\pi r\exp\left(-\dfrac{r^2}{2}\right) I=\exp\left(-\frac{r^2}{2}\right)","['multivariable-calculus', 'definite-integrals', 'dirac-delta']"
77,compute $ \iiint_Kxyz\ dxdydz$,compute, \iiint_Kxyz\ dxdydz,"The question is: $$ \iiint_Kxyz\ dxdydz\quad k:=\{(x,y,z):x^2+y^2+z^2\leq1, \  \ x^2+y^2\leq z^2\leq 3(x^2+y^2), \ x,y,z\geq 0\} $$ Here how i have tried to solve this: $$\iint_{x^2+y^2\leq1}\int_{\sqrt{x^2+y^2}}^{\sqrt{3(x^2+y^2)}}xyz \ dz\ dxdy=\frac{1}{2}\iint xy\left(3(x^2+y^2)-(x^2+y^2)\right)=...=0$$ But the answer that i got is zero which is obviously wrong what is wrong with my solution? any suggestion would be great, Thanks","The question is: Here how i have tried to solve this: But the answer that i got is zero which is obviously wrong what is wrong with my solution? any suggestion would be great, Thanks"," \iiint_Kxyz\ dxdydz\quad k:=\{(x,y,z):x^2+y^2+z^2\leq1, \  \ x^2+y^2\leq z^2\leq 3(x^2+y^2), \ x,y,z\geq 0\}  \iint_{x^2+y^2\leq1}\int_{\sqrt{x^2+y^2}}^{\sqrt{3(x^2+y^2)}}xyz \ dz\ dxdy=\frac{1}{2}\iint xy\left(3(x^2+y^2)-(x^2+y^2)\right)=...=0","['integration', 'multivariable-calculus', 'definite-integrals', 'multiple-integral']"
78,determine whether a point of a set is in the interior point,determine whether a point of a set is in the interior point,,"Let $A \subseteq \Bbb R^2$ with $A = \{(x,y): 1<x<4, 1<y<3 \}$ . Is the point $(2,2)$ be an interior point of $A$ ? Is the point $(4,2)$ be an interior point of $A$ ? Justify! Attempt: Edit: I know that the answer is yes and no, respectively. But, I'm not sure how to show it. To show: $(2,2)$ is an interior point of $A$ . Let $w \in B(u,r)$ with $u=(2,2)$ . Let $r=\frac{1}{2}>0$ and $k=\frac{1}{2} - ||w-u||$ . Then, $k > 0$ since $||w-u|| < \frac{1}{2}$ . Let $x \in B(w,k)$ . Then, by the triangle inequality, \begin{equation*} ||x-u|| \le ||x-w|| + ||w-u|| \le k + ||w-u|| = \frac{1}{2}. \end{equation*} Thus, $x \in B(u,\frac{1}{2})$ so that $B(w,k) \subseteq B(u,\frac{1}{2}) \subseteq A$ . Hence, $w$ is an interior point of $A$ such that $B(u,\frac{1}{2}) \subseteq int(A)$ and so, $u$ is an interior point of $A$ . How to show it? On the other hand, a point $x \in A$ is an interior point of $A$ if there exists $r>0$ such that $B(x,r) \subseteq A$ i.e. there exists $r>0$ such that for all $y \in \Bbb R^2, ||y-x||<r \implies y \in A$ .","Let with . Is the point be an interior point of ? Is the point be an interior point of ? Justify! Attempt: Edit: I know that the answer is yes and no, respectively. But, I'm not sure how to show it. To show: is an interior point of . Let with . Let and . Then, since . Let . Then, by the triangle inequality, Thus, so that . Hence, is an interior point of such that and so, is an interior point of . How to show it? On the other hand, a point is an interior point of if there exists such that i.e. there exists such that for all .","A \subseteq \Bbb R^2 A = \{(x,y): 1<x<4, 1<y<3 \} (2,2) A (4,2) A (2,2) A w \in B(u,r) u=(2,2) r=\frac{1}{2}>0 k=\frac{1}{2} - ||w-u|| k > 0 ||w-u|| < \frac{1}{2} x \in B(w,k) \begin{equation*}
||x-u|| \le ||x-w|| + ||w-u|| \le k + ||w-u|| = \frac{1}{2}.
\end{equation*} x \in B(u,\frac{1}{2}) B(w,k) \subseteq B(u,\frac{1}{2}) \subseteq A w A B(u,\frac{1}{2}) \subseteq int(A) u A x \in A A r>0 B(x,r) \subseteq A r>0 y \in \Bbb R^2, ||y-x||<r \implies y \in A","['general-topology', 'multivariable-calculus']"
79,Solve the Integral in the following set,Solve the Integral in the following set,,"I have the Set $D=\{ (x,y) \in\mathbb{R}: |x|\le2,|x|\le y \le \sqrt{4-x^2}\}$ and I have to calculate $\int_{D}(x^2y+xy^2) dydx$ . So I cannot explain how hard it is for me to find the borders of the integral. I've tried to understand it but It's very hard for meand every time I think i have understood it, i do it wrong in another excercise. I have an exam tomorrow and of course I can't do wonders with understanding them. We often do them in the lectures that we turn them somehow to polar coordinates or so. However, my question is: is there any way, that I can read it from the definition of the set D for example, with x and y(without having to turn them in some other coordinates) and what suggestions could you give me in order to be able to do that? For example in this specific excercise, i thought that since $|x|\le2$ then $-2\le x\le 2 $ . Then I just took $y$ between $2$ (since the absolute value of $x$ can't be more than $2$ , and $\sqrt{4-x^2}$ . So I calculated $\int_{-2}^{2}\int_{2}^{\sqrt{4-x^2}}x^2y$ . My answer is not the same as the answer in the book, so I got completely lost. I would be very thankful for some help. Annalisa","I have the Set and I have to calculate . So I cannot explain how hard it is for me to find the borders of the integral. I've tried to understand it but It's very hard for meand every time I think i have understood it, i do it wrong in another excercise. I have an exam tomorrow and of course I can't do wonders with understanding them. We often do them in the lectures that we turn them somehow to polar coordinates or so. However, my question is: is there any way, that I can read it from the definition of the set D for example, with x and y(without having to turn them in some other coordinates) and what suggestions could you give me in order to be able to do that? For example in this specific excercise, i thought that since then . Then I just took between (since the absolute value of can't be more than , and . So I calculated . My answer is not the same as the answer in the book, so I got completely lost. I would be very thankful for some help. Annalisa","D=\{ (x,y) \in\mathbb{R}: |x|\le2,|x|\le y \le \sqrt{4-x^2}\} \int_{D}(x^2y+xy^2) dydx |x|\le2 -2\le x\le 2  y 2 x 2 \sqrt{4-x^2} \int_{-2}^{2}\int_{2}^{\sqrt{4-x^2}}x^2y","['integration', 'multivariable-calculus', 'multiple-integral']"
80,Norm of a smooth function is smooth?,Norm of a smooth function is smooth?,,"My lecture notes seem to use this, although I can't shake the feeling that it's wrong. Let $\phi: [a,b] \to \mathbb{R}^n$ for $n \ge 2$ be smooth (infinitely differentiable) satisfying $|\phi '(t)| \neq 0$ for all $t \in [a,b]$ . Then, $|\phi'(t)|$ is also a smooth function. Does anyone have a nice proof for this? (My first 'counterexample' was $\log$ , but it doesn't satisfy the condition $|\phi '(t)| \neq 0$ .) Also, is $|\phi(t)|$ a smooth function too? Although this is from a course in elementary differential geometry, my question is essentially a calculus question...","My lecture notes seem to use this, although I can't shake the feeling that it's wrong. Let for be smooth (infinitely differentiable) satisfying for all . Then, is also a smooth function. Does anyone have a nice proof for this? (My first 'counterexample' was , but it doesn't satisfy the condition .) Also, is a smooth function too? Although this is from a course in elementary differential geometry, my question is essentially a calculus question...","\phi: [a,b] \to \mathbb{R}^n n \ge 2 |\phi '(t)| \neq 0 t \in [a,b] |\phi'(t)| \log |\phi '(t)| \neq 0 |\phi(t)|","['real-analysis', 'calculus', 'multivariable-calculus', 'derivatives', 'differential-geometry']"
81,Trying to understand the total derivative/differential.,Trying to understand the total derivative/differential.,,"I'm asking this question because I'm having problems understanting the definition of differential/total derivative in multivariable calculus, and in order to improve my understanding of it, I want to make sure I have the right intuition behind the definition. Let $F:\mathbb{R}^n\to\mathbb{R}^m$ be a function and $a\in\mathbb{R}^n$ . We say that $F$ is differentiable at $a$ iff there exists a linear transformation $L:\mathbb{R}^n\to\mathbb{R}^m$ such that $$\lim_{x\to a}\frac{\|F(x)-(L(x-a)+F(a))\|_m}{\|x-a\|_n}=0.$$ In this case $L$ will be called the differential of $F$ at $a$ (as it is later proven that this linear transformation is unique) and will be denoted $DF_a$ . As my proffesor explained, the differential is the best linear aproximation there is to $F$ close to $a$ . Now, I understand that $G:\mathbb{R}^n\to\mathbb{R}^m$ defined as $G(x)=DF_a(x-a)+F(a)$ is moving the graph of the function $DF_a$ ( $Gr(DF_a):=\{(x_1,...,x_n,DF_a^1(x_1,...,x_n),...,DF_a^m(x_1,...,x_n))|x_i\in\mathbb{R},i=1,...,n\}$ , where each $DF_a^i:\mathbb{R}^n\to\mathbb{R}$ is the $i$ -th component function of $DF_a$ ), to the point $(a_1,...,a_n,F_1(a_1,...,a_n),...,F_m(a_1,...,a_n))$ , and in this sense $G(x)$ approaches $F(x)$ as $x$ goes to $a$ . This is where the confusion starts (please correct me in anything if I'm wrong). How is it justified that $G$ approaches $F$ as $x$ goes to $a$ ? Is it true that $\lim_{x\to a}\|F(x)-G(x)\|_m=0$ ? If so, why is that not the definition? How does dividing over something that goes to zero while preserving the existence and value of the limit guarantee that $G$ is the best linear approximation? As I've heard before, it is to be understood that $\|F(x)-G(x)\|_m$ goes to zero faster than $\|x-a\|_n$ does, as $x$ approaches $a$ , but once again, how is this justified? I'm sorry if this is too much or if it seems as if I have not worked as much in trying to learn this, but I really have been trying a lot lately and I'm having a hard time in doing so. Any help is greatly thanked for. Greetings.","I'm asking this question because I'm having problems understanting the definition of differential/total derivative in multivariable calculus, and in order to improve my understanding of it, I want to make sure I have the right intuition behind the definition. Let be a function and . We say that is differentiable at iff there exists a linear transformation such that In this case will be called the differential of at (as it is later proven that this linear transformation is unique) and will be denoted . As my proffesor explained, the differential is the best linear aproximation there is to close to . Now, I understand that defined as is moving the graph of the function ( , where each is the -th component function of ), to the point , and in this sense approaches as goes to . This is where the confusion starts (please correct me in anything if I'm wrong). How is it justified that approaches as goes to ? Is it true that ? If so, why is that not the definition? How does dividing over something that goes to zero while preserving the existence and value of the limit guarantee that is the best linear approximation? As I've heard before, it is to be understood that goes to zero faster than does, as approaches , but once again, how is this justified? I'm sorry if this is too much or if it seems as if I have not worked as much in trying to learn this, but I really have been trying a lot lately and I'm having a hard time in doing so. Any help is greatly thanked for. Greetings.","F:\mathbb{R}^n\to\mathbb{R}^m a\in\mathbb{R}^n F a L:\mathbb{R}^n\to\mathbb{R}^m \lim_{x\to a}\frac{\|F(x)-(L(x-a)+F(a))\|_m}{\|x-a\|_n}=0. L F a DF_a F a G:\mathbb{R}^n\to\mathbb{R}^m G(x)=DF_a(x-a)+F(a) DF_a Gr(DF_a):=\{(x_1,...,x_n,DF_a^1(x_1,...,x_n),...,DF_a^m(x_1,...,x_n))|x_i\in\mathbb{R},i=1,...,n\} DF_a^i:\mathbb{R}^n\to\mathbb{R} i DF_a (a_1,...,a_n,F_1(a_1,...,a_n),...,F_m(a_1,...,a_n)) G(x) F(x) x a G F x a \lim_{x\to a}\|F(x)-G(x)\|_m=0 G \|F(x)-G(x)\|_m \|x-a\|_n x a","['limits', 'multivariable-calculus', 'derivatives', 'intuition']"
82,Possessing derivative for a complex function of a complex variable vs Being differentiable for a corresponding $\Bbb R^2\to\Bbb R^2$ function,Possessing derivative for a complex function of a complex variable vs Being differentiable for a corresponding  function,\Bbb R^2\to\Bbb R^2,"Assume $f(z)$ is a complex function of a complex variable $z$ . The definition of $f$ possessing a derivative at $z$ is the existence of the limit $$\lim\limits_{h\to0}\frac{f(z+h)-f(z)}{h}.$$ See, e.g., $\S$ 1.1 of ""Complex Analysis (3rd Ed.)"" by Lars V. Ahlfors. A multivariate vector-valued function $\mathbf{f}$ mapping from $\Bbb R^n\to\Bbb R^m$ is differentiable at $\bf{x}$ if there is a linear transformation $A$ of $\Bbb R^n$ into $\Bbb R^m$ such that $$\lim\limits_{{\bf{h}}\to\bf{0}}\frac{|{\bf{f}}({\bf{x}}+{\bf{h}})-{\bf{f}}({\bf{x}})-A{\bf{h}}|}{|{\bf{h}}|}=0,$$ which is equivalent to $$\lim\limits_{{\bf{h}}\to\bf{0}}\frac{|{\bf{r}}({\bf{h}})|}{|{\bf{h}}|}=0,$$ where ${\bf{r}}({\bf{h}})={\bf{f}}({\bf{x}}+{\bf{h}})-{\bf{f}}({\bf{x}})-A{\bf{h}}$ . See, e.g., ""DIFFERENTIATION"" Section in Chapter 9 of Rudin's ""Principles of Mathematical Analysis (3rd Ed.)"". I've been being tempted to think of complex functions of a complex variable $f(z)=u(z)+iv(z)$ as a $\Bbb R^2\to\Bbb R^2$ function $\mathbf{f}(\mathbf{x})$ where $\mathbf{x}=(x,y)$ corresponds to $z$ and $\mathbf{f}=(u,v)$ , and subsequently the condition that $f(z)$ possesses derivative at $z$ (first paragraph) is equivalent to (i.e., can imply each other) $\mathbf{f}$ being differentiable at $\mathbf{x}$ with $n=m=2$ (second paragraph). But today, when I thought more about this, I found myself standing in front of an obvious contradiction that arises from the following theorem (P26 of Ahlfors' ""Complex Analysis (3rd Ed.)""): If $u(x,y)$ and $v(x,y)$ have continuous first-order partial derivatives which satisfy the Cauchy-Riemann differential equations, then $f(z) = u(z) + iv(z)$ is analytic with continuous derivative $f'(z)$ , and conversely. If the above equivalence claim were correct, the continuity of first-order partial derivatives will guarantee differentiability of $\mathbf{f}$ (by, e.g., Theorem 9.21 on P219 of Rudin's text) and in turn that $f$ possesses derivative. Then the Cauchy-Riemann differential equations would be useless in the proof. Since the Ahlfors' text is so classical, I'm sure this condition is indispensable (I guess it is needed in line 11 and 12 of P26 in Ahlfors' text, right?). As a result, the equivalence claim in the above paragraph must be wrong. Then I tried to figure out why on earth they are not equivalent. Later I came to a point that (1) $\Rightarrow$ (2) while (2) $\not\Rightarrow$ (1), where (1) stands for $f$ possessing derivative and (2) for the corresponding $\Bbb R^2\to\Bbb R^2$ function $\mathbf{f}$ being differentiable. That is, (1) is stronger than (2). I guess the strongerness comes from the fact that $\Bbb C$ defines a multiplication operation, which is not available in $\Bbb R^2$ . The contradiction can be solved by this relation: the continuity of first-order partial derivatives establishes (2), but we need additional conditions (the Cauchy-Riemann differential equations) to arrive at a stronger conclusion (1). To show the above relation, First, I gave a proof of (1) $\Rightarrow$ (2): Assume the derivative is a complex number $\alpha$ and we define linear transformation according to complex number multiplication: if ${\bf{h}}=(x,y)$ , $A{\bf{h}}\cong\alpha(x+iy)$ (an isometry between $\Bbb R^2$ and the complex plane). Since $\lim\limits_{z\to0}|f(z)|=0$ iff $\lim\limits_{z\to0}f(z)=0$ , we have $$\begin{eqnarray} \lim\limits_{{\bf{h}}\to\bf{0}}\frac{|{\bf{f}}({\bf{x}}+{\bf{h}})-{\bf{f}}({\bf{x}})-A{\bf{h}}|}{|{\bf{h}}|}&=&\lim\limits_{h\to0}\frac{|f(x+h)-f(x)-\alpha h|}{|h|}\\ &=&\lim\limits_{h\to0}\frac{f(x+h)-f(x)-\alpha h}{h}\\ &=&\lim\limits_{h\to0}\frac{f(x+h)-f(x)}{h}-\alpha\\ &=&\alpha-\alpha=0 \end{eqnarray}$$ For (2) $\not\Rightarrow$ (1): From the equivalent formulation of differentiability, we have $f(z+h)-f(z)=A{\bf{h}}+r(h)$ where $r(h)=o(h)$ . If we can write $A{\bf{h}}\cong\alpha h$ for some complex number $\alpha$ as in the last paragraph, then the derivative of $f$ is immediate. But the thing is, we can't (at least I can't). $A{\bf{h}}/h$ is not necessarily a constant complex number for a given general linear transformation $A$ . That's why (2) does not imply (1). Now comes my question: 1) Is it true that possessing derivative for a complex function of a complex variable is not equivalent to its corresponding $\Bbb R^2\to\Bbb R^2$ function being differentiable? 2) Is my relation that (1) $\Rightarrow$ (2) while (2) $\not\Rightarrow$ (1) a correct argument to explain this inequivalence? 3) Is there anything wrong (or not rigorous) throughout my argument? Thank you.","Assume is a complex function of a complex variable . The definition of possessing a derivative at is the existence of the limit See, e.g., 1.1 of ""Complex Analysis (3rd Ed.)"" by Lars V. Ahlfors. A multivariate vector-valued function mapping from is differentiable at if there is a linear transformation of into such that which is equivalent to where . See, e.g., ""DIFFERENTIATION"" Section in Chapter 9 of Rudin's ""Principles of Mathematical Analysis (3rd Ed.)"". I've been being tempted to think of complex functions of a complex variable as a function where corresponds to and , and subsequently the condition that possesses derivative at (first paragraph) is equivalent to (i.e., can imply each other) being differentiable at with (second paragraph). But today, when I thought more about this, I found myself standing in front of an obvious contradiction that arises from the following theorem (P26 of Ahlfors' ""Complex Analysis (3rd Ed.)""): If and have continuous first-order partial derivatives which satisfy the Cauchy-Riemann differential equations, then is analytic with continuous derivative , and conversely. If the above equivalence claim were correct, the continuity of first-order partial derivatives will guarantee differentiability of (by, e.g., Theorem 9.21 on P219 of Rudin's text) and in turn that possesses derivative. Then the Cauchy-Riemann differential equations would be useless in the proof. Since the Ahlfors' text is so classical, I'm sure this condition is indispensable (I guess it is needed in line 11 and 12 of P26 in Ahlfors' text, right?). As a result, the equivalence claim in the above paragraph must be wrong. Then I tried to figure out why on earth they are not equivalent. Later I came to a point that (1) (2) while (2) (1), where (1) stands for possessing derivative and (2) for the corresponding function being differentiable. That is, (1) is stronger than (2). I guess the strongerness comes from the fact that defines a multiplication operation, which is not available in . The contradiction can be solved by this relation: the continuity of first-order partial derivatives establishes (2), but we need additional conditions (the Cauchy-Riemann differential equations) to arrive at a stronger conclusion (1). To show the above relation, First, I gave a proof of (1) (2): Assume the derivative is a complex number and we define linear transformation according to complex number multiplication: if , (an isometry between and the complex plane). Since iff , we have For (2) (1): From the equivalent formulation of differentiability, we have where . If we can write for some complex number as in the last paragraph, then the derivative of is immediate. But the thing is, we can't (at least I can't). is not necessarily a constant complex number for a given general linear transformation . That's why (2) does not imply (1). Now comes my question: 1) Is it true that possessing derivative for a complex function of a complex variable is not equivalent to its corresponding function being differentiable? 2) Is my relation that (1) (2) while (2) (1) a correct argument to explain this inequivalence? 3) Is there anything wrong (or not rigorous) throughout my argument? Thank you.","f(z) z f z \lim\limits_{h\to0}\frac{f(z+h)-f(z)}{h}. \S \mathbf{f} \Bbb R^n\to\Bbb R^m \bf{x} A \Bbb R^n \Bbb R^m \lim\limits_{{\bf{h}}\to\bf{0}}\frac{|{\bf{f}}({\bf{x}}+{\bf{h}})-{\bf{f}}({\bf{x}})-A{\bf{h}}|}{|{\bf{h}}|}=0, \lim\limits_{{\bf{h}}\to\bf{0}}\frac{|{\bf{r}}({\bf{h}})|}{|{\bf{h}}|}=0, {\bf{r}}({\bf{h}})={\bf{f}}({\bf{x}}+{\bf{h}})-{\bf{f}}({\bf{x}})-A{\bf{h}} f(z)=u(z)+iv(z) \Bbb R^2\to\Bbb R^2 \mathbf{f}(\mathbf{x}) \mathbf{x}=(x,y) z \mathbf{f}=(u,v) f(z) z \mathbf{f} \mathbf{x} n=m=2 u(x,y) v(x,y) f(z) = u(z) + iv(z) f'(z) \mathbf{f} f \Rightarrow \not\Rightarrow f \Bbb R^2\to\Bbb R^2 \mathbf{f} \Bbb C \Bbb R^2 \Rightarrow \alpha {\bf{h}}=(x,y) A{\bf{h}}\cong\alpha(x+iy) \Bbb R^2 \lim\limits_{z\to0}|f(z)|=0 \lim\limits_{z\to0}f(z)=0 \begin{eqnarray}
\lim\limits_{{\bf{h}}\to\bf{0}}\frac{|{\bf{f}}({\bf{x}}+{\bf{h}})-{\bf{f}}({\bf{x}})-A{\bf{h}}|}{|{\bf{h}}|}&=&\lim\limits_{h\to0}\frac{|f(x+h)-f(x)-\alpha h|}{|h|}\\
&=&\lim\limits_{h\to0}\frac{f(x+h)-f(x)-\alpha h}{h}\\
&=&\lim\limits_{h\to0}\frac{f(x+h)-f(x)}{h}-\alpha\\
&=&\alpha-\alpha=0
\end{eqnarray} \not\Rightarrow f(z+h)-f(z)=A{\bf{h}}+r(h) r(h)=o(h) A{\bf{h}}\cong\alpha h \alpha f A{\bf{h}}/h A \Bbb R^2\to\Bbb R^2 \Rightarrow \not\Rightarrow","['complex-analysis', 'multivariable-calculus', 'derivatives']"
83,"Solutions of $f(k,s)=0$ as $k\to\infty$",Solutions of  as,"f(k,s)=0 k\to\infty","Let $f(k,s) = D(s) + kN(s)$ where $D(s)$ and $N(s)$ are polynomials of $s \in \mathbb{C}$ such that $\text{Deg}(D) = n, \ \text{Deg}(N) = m$ and $n\ge m$ . Also $D(s)$ and $N(s)$ doesn't have common factor. Find the values of $s$ such that $$\lim_{k\to \infty}f(k,s) = 0  \tag{1}$$ My try: Assume that $N(s) \not = 0$ . Obviously, in this case $\lim_{k\to \infty}f(k,s)$ is divergent for any $s$ . Now suppose $N(s) = 0$ and we have $\lim_{k\to \infty}f(k,s) = D(s)$ . So the answer is $D(s) = N(s) = 0$ which is impossible since $D(s)$ and $N(s)$ doesn't have common factor. What's wrong in my reasoning? Also is $(1)$ equivalent to find $s$ such that $f(k,s) = 0$ and then $\lim_{k \to \infty} s(k)$ ? I mean that we first find the roots of $f(k,s)$ with respect to $k$ and see what happens to the roots when $k \to \infty$ . Edit: The correct answer is as follows. Divide the equation $D(s) + kN(s) = 0$ by $k$ and let $k \to \infty$ . So we have $$\lim_{k \to \infty}(\frac{D(s)}{k} + N(s)) = N(s) = 0$$ I don't know why my argument is flawed and whether the mentioned equivalence holds. Edit2: Thanks to @Alex Ravsky, it turns out that the true question is What are the solutions of $f(k,s)=0$ as $k\to\infty$ ?","Let where and are polynomials of such that and . Also and doesn't have common factor. Find the values of such that My try: Assume that . Obviously, in this case is divergent for any . Now suppose and we have . So the answer is which is impossible since and doesn't have common factor. What's wrong in my reasoning? Also is equivalent to find such that and then ? I mean that we first find the roots of with respect to and see what happens to the roots when . Edit: The correct answer is as follows. Divide the equation by and let . So we have I don't know why my argument is flawed and whether the mentioned equivalence holds. Edit2: Thanks to @Alex Ravsky, it turns out that the true question is What are the solutions of as ?","f(k,s) = D(s) + kN(s) D(s) N(s) s \in \mathbb{C} \text{Deg}(D) = n, \ \text{Deg}(N) = m n\ge m D(s) N(s) s \lim_{k\to \infty}f(k,s) = 0  \tag{1} N(s) \not = 0 \lim_{k\to \infty}f(k,s) s N(s) = 0 \lim_{k\to \infty}f(k,s) = D(s) D(s) = N(s) = 0 D(s) N(s) (1) s f(k,s) = 0 \lim_{k \to \infty} s(k) f(k,s) k k \to \infty D(s) + kN(s) = 0 k k \to \infty \lim_{k \to \infty}(\frac{D(s)}{k} + N(s)) = N(s) = 0 f(k,s)=0 k\to\infty","['real-analysis', 'limits', 'multivariable-calculus', 'polynomials', 'convergence-divergence']"
84,Problem: Integrate using the Trigonometric Substitution method,Problem: Integrate using the Trigonometric Substitution method,,"Exercise about Trigonometric Substitution Objective: Resolve using the Trigonometric Substitution method. I've already tried solving the exercise by taking the following steps: I take e^x as 2sen(t) and I substitute. Take dx as 2cos(t)dt. After some easy straight-foward steps, I came out with this: The result has to be and the integral of sen(t) is cos(t).","Exercise about Trigonometric Substitution Objective: Resolve using the Trigonometric Substitution method. I've already tried solving the exercise by taking the following steps: I take e^x as 2sen(t) and I substitute. Take dx as 2cos(t)dt. After some easy straight-foward steps, I came out with this: The result has to be and the integral of sen(t) is cos(t).",,"['calculus', 'algebra-precalculus', 'multivariable-calculus']"
85,"Find $\lim_{(x,y)\to(0,0)} \frac{xy^3}{x+y}$",Find,"\lim_{(x,y)\to(0,0)} \frac{xy^3}{x+y}","I have to compute $\displaystyle\lim_{(x,y)\to(0,0)} \frac{xy^3}{x+y}$ . When doing polar coordinates, I get $\displaystyle\lim_{r\to0}r^3\frac{\sin^3(\theta)}{\sin(\theta)+\cos(\theta)}$ but I'm not sure if I can say it's zero because of that denominator. I also tried this limit through lines, a parabola, and axis, all of them zero. My professor said something about ""what happens whe $x+y=0$ ?"" And the hint was to take $\displaystyle\frac{xy^3}{x+y}=1$ which implies $\displaystyle x=\frac{y}{y^3-1}$ and we have to check what happens when $y\to0$ . However, that leads to $0$ . I don't understand what is happening in $\displaystyle x=\frac{y}{y^3-1}$ , maybe something related to mean values theorem?","I have to compute . When doing polar coordinates, I get but I'm not sure if I can say it's zero because of that denominator. I also tried this limit through lines, a parabola, and axis, all of them zero. My professor said something about ""what happens whe ?"" And the hint was to take which implies and we have to check what happens when . However, that leads to . I don't understand what is happening in , maybe something related to mean values theorem?","\displaystyle\lim_{(x,y)\to(0,0)} \frac{xy^3}{x+y} \displaystyle\lim_{r\to0}r^3\frac{\sin^3(\theta)}{\sin(\theta)+\cos(\theta)} x+y=0 \displaystyle\frac{xy^3}{x+y}=1 \displaystyle x=\frac{y}{y^3-1} y\to0 0 \displaystyle x=\frac{y}{y^3-1}","['real-analysis', 'calculus', 'multivariable-calculus']"
86,"Evaluate $I = \iint_{R}\frac{x}{y^{4}}\,dx\,dy$ over a given region",Evaluate  over a given region,"I = \iint_{R}\frac{x}{y^{4}}\,dx\,dy","Evaluate the double integral $$I = \iint_{R}\frac{x}{y^{4}}\,dx\,dy$$ over the region $R = \{(x,y)\mid 0\le x \le3, 3\le y \le5\}$ . Hi I was wondering if anyone could help me on this question which involves Taylor's Expansion. This is what I have done so far: From using the given boundaries I have created the integral: $$\int_3^5\int_0^3 \frac{x}{y^4}\, dx\,dy$$ Then I worked out the first integral: $$\int_0^3\frac{x}{y^4}\, dx$$ which gives me $\frac{9}{2y^4}$ and then I put this into the next integral that is with respect to $y$ : $$\int_3^5\frac{9}{2y^4} \,dy$$ Which gives me $3.415703704\times 10^{-3}$ , which is evidently wrong as the correct answer is: $49/1125$ Can someone please help me on where I am going wrong? Thanks in advance!","Evaluate the double integral over the region . Hi I was wondering if anyone could help me on this question which involves Taylor's Expansion. This is what I have done so far: From using the given boundaries I have created the integral: Then I worked out the first integral: which gives me and then I put this into the next integral that is with respect to : Which gives me , which is evidently wrong as the correct answer is: Can someone please help me on where I am going wrong? Thanks in advance!","I = \iint_{R}\frac{x}{y^{4}}\,dx\,dy R = \{(x,y)\mid 0\le x \le3, 3\le y \le5\} \int_3^5\int_0^3 \frac{x}{y^4}\, dx\,dy \int_0^3\frac{x}{y^4}\, dx \frac{9}{2y^4} y \int_3^5\frac{9}{2y^4} \,dy 3.415703704\times 10^{-3} 49/1125","['integration', 'multivariable-calculus', 'definite-integrals', 'taylor-expansion']"
87,Homeomorphism between $\mathbb{R}^2$ and the open unit disc,Homeomorphism between  and the open unit disc,\mathbb{R}^2,"Given the function $f(x,y)=(\frac{x}{1+\sqrt{x^2+y^2}},\frac{y}{1+\sqrt{x^2+y^2}})$ , I have to prove $f$ is a homemorphism between $\mathbb{R}^2$ and the open unit disc. Proving this function is conitunuous is trivial, proving it's a bijection is a bit harder (I wasn't able to find it's inverse). I did find the same function in a different notation - $f(z)=\frac{z}{1+\|z\|}$ where $z\in\mathbb{R}^2$ , but since it's my first time dealing with two variable functions and homemorphisms, I can't see why $f(z)=f(x,y)$ , other than $\|z\|=\|(x,y)\|$ . What is the best way to approach this with given notation ( $f(x,y)$ ) - more generally how can I prove this function is a bijection and find its inverse. (I was able to show that $lim_{(x,y)\rightarrow(\infty,\infty)}f(x,y)$ is (1,1) which I guess shows its a surjection but otherwise had no idea).","Given the function , I have to prove is a homemorphism between and the open unit disc. Proving this function is conitunuous is trivial, proving it's a bijection is a bit harder (I wasn't able to find it's inverse). I did find the same function in a different notation - where , but since it's my first time dealing with two variable functions and homemorphisms, I can't see why , other than . What is the best way to approach this with given notation ( ) - more generally how can I prove this function is a bijection and find its inverse. (I was able to show that is (1,1) which I guess shows its a surjection but otherwise had no idea).","f(x,y)=(\frac{x}{1+\sqrt{x^2+y^2}},\frac{y}{1+\sqrt{x^2+y^2}}) f \mathbb{R}^2 f(z)=\frac{z}{1+\|z\|} z\in\mathbb{R}^2 f(z)=f(x,y) \|z\|=\|(x,y)\| f(x,y) lim_{(x,y)\rightarrow(\infty,\infty)}f(x,y)","['general-topology', 'multivariable-calculus']"
88,How do you break two absolute values inside double integral?,How do you break two absolute values inside double integral?,,How would I split the two absolute values of this double integral? $\int_{-1}^1\int_{-1}^1(|x|+|y|)dxdy$ The answer key shows the integral = 2 with work: $\int_{-1}^1(|x|+|y|)dx$ split into $\int_{-1}^0((-x)-y)dx$ + $\int_{0}^1((x)-y)dx$ but why does y become -y instead of staying |y| inside the dx integral? Could I not carry |y| over and split it inside the dy integral?,How would I split the two absolute values of this double integral? The answer key shows the integral = 2 with work: split into + but why does y become -y instead of staying |y| inside the dx integral? Could I not carry |y| over and split it inside the dy integral?,\int_{-1}^1\int_{-1}^1(|x|+|y|)dxdy \int_{-1}^1(|x|+|y|)dx \int_{-1}^0((-x)-y)dx \int_{0}^1((x)-y)dx,"['multivariable-calculus', 'absolute-value']"
89,"Can I solve this multi-variable limit with polar coordinates: $\lim_{ (x,y)\to(0,0)} \frac{y^2\sin^2(x)}{x^4+y^4}$?",Can I solve this multi-variable limit with polar coordinates: ?,"\lim_{ (x,y)\to(0,0)} \frac{y^2\sin^2(x)}{x^4+y^4}","$$\lim_{ (x,y)\to(0,0)} \frac{y^2\sin^2(x)}{x^4+y^4}$$ I know that I could let $x=0$ and solve the limit, and then the same for $y=x$ . But I want to know if I could solve it only with polar coordinates.","I know that I could let and solve the limit, and then the same for . But I want to know if I could solve it only with polar coordinates.","\lim_{ (x,y)\to(0,0)} \frac{y^2\sin^2(x)}{x^4+y^4} x=0 y=x","['calculus', 'limits', 'multivariable-calculus', 'polar-coordinates']"
90,Prove that there exist $\mu>0$ and $\delta>0$ s.t. $|f(x)-f(y)|\geq \mu |x-y|$,Prove that there exist  and  s.t.,\mu>0 \delta>0 |f(x)-f(y)|\geq \mu |x-y|,"Assume that $\Omega$ is an open set in $R^m$ , $f \in C^1(\Omega,R^m)$ , $a\in \Omega$ . If $\det f'(a) \neq 0$ , prove that there exist $\mu>0$ and $\delta>0$ s.t. for all $x,y \in B_{\delta}(a)$ , $|f(x)-f(y)|\geq \mu |x-y|$ . I can solve the situation when $m=1$ . W.L.O.G. assume that $f'(a)>0$ , then for $\epsilon = f'(a)/2$ , there exists $\delta>0$ s.t. $\forall \xi \in B_{\delta}(a)$ we have $|f'(\xi)-f'(a)|<\epsilon$ , hence $f'(\xi)>-\epsilon+f'(a)>0$ . then by Lagrange's theorem, for all $x,y\in B_{\delta}(a)$ , there exists $\xi =x+\theta(y-x)$ where $\theta \in [0,1]$ s.t. $|f(y)-f(x)|=|f'(\xi)(y-x)| \geq |-\epsilon+f'(a)|\cdot |y-x|$ . Hence $\mu=-\epsilon+f'(a)$ and $\delta$ would meet our requirement. But I have no idea how to deal with a higher dimensional space. Any help would be appreciated.","Assume that is an open set in , , . If , prove that there exist and s.t. for all , . I can solve the situation when . W.L.O.G. assume that , then for , there exists s.t. we have , hence . then by Lagrange's theorem, for all , there exists where s.t. . Hence and would meet our requirement. But I have no idea how to deal with a higher dimensional space. Any help would be appreciated.","\Omega R^m f \in C^1(\Omega,R^m) a\in \Omega \det f'(a) \neq 0 \mu>0 \delta>0 x,y \in B_{\delta}(a) |f(x)-f(y)|\geq \mu |x-y| m=1 f'(a)>0 \epsilon = f'(a)/2 \delta>0 \forall \xi \in B_{\delta}(a) |f'(\xi)-f'(a)|<\epsilon f'(\xi)>-\epsilon+f'(a)>0 x,y\in B_{\delta}(a) \xi =x+\theta(y-x) \theta \in [0,1] |f(y)-f(x)|=|f'(\xi)(y-x)| \geq |-\epsilon+f'(a)|\cdot |y-x| \mu=-\epsilon+f'(a) \delta","['multivariable-calculus', 'derivatives', 'continuity']"
91,"Find the critical points of the function $f(x,y)=e^{-xy}\sin (xy).$",Find the critical points of the function,"f(x,y)=e^{-xy}\sin (xy).","Find the critical points of the function $f :\mathbb R^2 \to \mathbb R$ defined by : $f(x,y)=e^{-xy}\sin (xy).$ My attempts : we have $\nabla f(x,y)= e^{-xy}\begin{pmatrix}  -y\sin(xy)+y\cos(xy)& -x\sin(xy)+x\cos(xy) \end{pmatrix}$ , thus $$\nabla f(x,y)=(0 \quad0) \iff \begin{cases} y=0 \text{ or } \tan(xy)=1 & \\ x=0 \text{ or} \tan(xy)=1 \end{cases} ,$$ $\tan(xy)=1 \iff xy= n\pi+\dfrac{\pi}{4},$ so if $y=0$ , then $x=0$ and if $y\neq 0$ , then $x=\frac{1}{y}\left( n\pi+\dfrac{\pi}{4}\right)$ . Thus $(0,0)$ and $\left(\frac{1}{y}\left( n\pi+\dfrac{\pi}{4}\right),y\right)$ are the critical points. Thanksin advance !","Find the critical points of the function defined by : My attempts : we have , thus so if , then and if , then . Thus and are the critical points. Thanksin advance !","f :\mathbb R^2 \to \mathbb R f(x,y)=e^{-xy}\sin (xy). \nabla f(x,y)= e^{-xy}\begin{pmatrix}  -y\sin(xy)+y\cos(xy)& -x\sin(xy)+x\cos(xy) \end{pmatrix} \nabla f(x,y)=(0 \quad0) \iff \begin{cases} y=0 \text{ or } \tan(xy)=1 & \\ x=0 \text{ or} \tan(xy)=1 \end{cases} , \tan(xy)=1 \iff xy= n\pi+\dfrac{\pi}{4}, y=0 x=0 y\neq 0 x=\frac{1}{y}\left( n\pi+\dfrac{\pi}{4}\right) (0,0) \left(\frac{1}{y}\left( n\pi+\dfrac{\pi}{4}\right),y\right)","['multivariable-calculus', 'derivatives', 'solution-verification', 'vector-analysis']"
92,Property of Injective-Continuous-function on Compact-set.,Property of Injective-Continuous-function on Compact-set.,,If $A \subset\mathbb{R}^n$ where A is compact and $f:A\rightarrow \mathbb{R}$ is continuous. If f is injective then the points in $A$ at which $f$ attains global maxima and minima are boundary points of $A$ .  Is the statement true? The motive for this question comes from here .,If where A is compact and is continuous. If f is injective then the points in at which attains global maxima and minima are boundary points of .  Is the statement true? The motive for this question comes from here .,A \subset\mathbb{R}^n f:A\rightarrow \mathbb{R} A f A,"['analysis', 'multivariable-calculus']"
93,$\partial_xF+F\partial_yF=0$ implies that $F$=const,implies that =const,\partial_xF+F\partial_yF=0 F,"Let $F\in \mathit{C}^2(\mathbb{R^2},\mathbb{R})$ and $$\partial_xF+F\partial_yF=0.$$ How do I prove that $F$ is a constant on $\mathbb{R^2}$ ? I am trying to restrict $F$ to a line passing through the origin and prove that the values of $F$ on the line equal $F(0,0)$ , but failed.","Let and How do I prove that is a constant on ? I am trying to restrict to a line passing through the origin and prove that the values of on the line equal , but failed.","F\in \mathit{C}^2(\mathbb{R^2},\mathbb{R}) \partial_xF+F\partial_yF=0. F \mathbb{R^2} F F F(0,0)","['multivariable-calculus', 'partial-differential-equations', 'linear-pde']"
94,"Prove that $\lim_{(x,y)\to(2,0)}{\frac{xy^2}{x+y^4+3}}=0$",Prove that,"\lim_{(x,y)\to(2,0)}{\frac{xy^2}{x+y^4+3}}=0","I am trying to prove that $\lim_{(x,y)\to(2,0)}{\frac{xy^2}{x+y^4+3}}=0$ using the $\epsilon$ - $\delta$ limit definition. So to prove that $\lim_{(x,y)\to(2,0)}{\frac{xy^2}{x+y^4+3}}=0$ it would be enough to show that $\forall \epsilon>0 \; \exists \delta>0 : \; [\sqrt{(x-2)^2+y^2}<\delta] \implies \Big| \frac{xy^2}{x+y^2+3}\Big| < \epsilon $ . Now I can't figure out anything reasonable to do to find what $\delta$ should be. I also tried using a rectangle instead of a sphere, but didn't get anywhere with this approach either. I've been struggling for quite a while with this and I appreciate all the help I can get.","I am trying to prove that using the - limit definition. So to prove that it would be enough to show that . Now I can't figure out anything reasonable to do to find what should be. I also tried using a rectangle instead of a sphere, but didn't get anywhere with this approach either. I've been struggling for quite a while with this and I appreciate all the help I can get.","\lim_{(x,y)\to(2,0)}{\frac{xy^2}{x+y^4+3}}=0 \epsilon \delta \lim_{(x,y)\to(2,0)}{\frac{xy^2}{x+y^4+3}}=0 \forall \epsilon>0 \; \exists \delta>0 : \; [\sqrt{(x-2)^2+y^2}<\delta] \implies \Big| \frac{xy^2}{x+y^2+3}\Big| < \epsilon  \delta","['limits', 'multivariable-calculus', 'epsilon-delta']"
95,Line integral in polar coordinates vs change of variables,Line integral in polar coordinates vs change of variables,,"Assume we have a two-dimensional force field: $$F(r, \theta) = -4\sin(\theta)i + 4\sin(\theta)j$$ Compute the work done in moving a particle from the point $(1, 0)$ to the origin along the spiral whose polar equation is $r = e^{-\theta}$ . I know how to compute the answer in general. We need to represent the path $\alpha(\theta) = (e^{-\theta}\cos(\theta), e^{-\theta}\sin(\theta))$ as such, and then take a line integral to infinity. However, I also first made a mistake by trying the following integral: $$\int_0^{+\infty} F(r, \theta) \cdot \frac{\partial (e^{-\theta}, \theta)}{\partial \theta} d\theta$$ which of course gives an incorrect result. The problem I cannot figure is how do we do the variable change here? In particular, where do the respective coordinate systems (Cartesian vs polar) come along and which should be transformed into which? I justify the correct answer by noticing, that $F(r, \theta)$ is a vector in $R^2$ , while the path $\alpha(\theta) = (e^{-\theta}, \theta)$ is a path in some space which is NOT an $R^2$ . As such, no inner product can be formed. Why the incorrect path not working? In short, why do we need to use $\alpha(\theta) = (e^{-\theta}\cos(\theta), e^{-\theta}\sin(\theta))$ instead of $\alpha(\theta) = (e^{-\theta}, \theta)$ If the vector field $F$ is already parameterized by $\theta$ , where does $\sin(\theta)$ and $\cos(\theta)$ come from in its definition, given that the path should be described as $(e^{-\theta}\cos(\theta), e^{-\theta}\sin(\theta))$ How can I use the path $\alpha(\theta) = (e^{-\theta}, \theta)$ in whatever space it is defined?","Assume we have a two-dimensional force field: Compute the work done in moving a particle from the point to the origin along the spiral whose polar equation is . I know how to compute the answer in general. We need to represent the path as such, and then take a line integral to infinity. However, I also first made a mistake by trying the following integral: which of course gives an incorrect result. The problem I cannot figure is how do we do the variable change here? In particular, where do the respective coordinate systems (Cartesian vs polar) come along and which should be transformed into which? I justify the correct answer by noticing, that is a vector in , while the path is a path in some space which is NOT an . As such, no inner product can be formed. Why the incorrect path not working? In short, why do we need to use instead of If the vector field is already parameterized by , where does and come from in its definition, given that the path should be described as How can I use the path in whatever space it is defined?","F(r, \theta) = -4\sin(\theta)i + 4\sin(\theta)j (1, 0) r = e^{-\theta} \alpha(\theta) = (e^{-\theta}\cos(\theta), e^{-\theta}\sin(\theta)) \int_0^{+\infty} F(r, \theta) \cdot \frac{\partial (e^{-\theta}, \theta)}{\partial \theta} d\theta F(r, \theta) R^2 \alpha(\theta) = (e^{-\theta}, \theta) R^2 \alpha(\theta) = (e^{-\theta}\cos(\theta), e^{-\theta}\sin(\theta)) \alpha(\theta) = (e^{-\theta}, \theta) F \theta \sin(\theta) \cos(\theta) (e^{-\theta}\cos(\theta), e^{-\theta}\sin(\theta)) \alpha(\theta) = (e^{-\theta}, \theta)","['multivariable-calculus', 'coordinate-systems', 'polar-coordinates', 'line-integrals', 'change-of-variable']"
96,"Find $\iint_{D} y^{3}\,dx\,dy$",Find,"\iint_{D} y^{3}\,dx\,dy","$$\iint_{D} y^{3}\,dx\,dy$$ where $D$ is the domain between $x^2 + y^2 = 6$ circle and the parabola $y=x^2$ Edit: Also, I got the intersection of the curves $(\sqrt2,2)$ and $(\sqrt-3, -3)$ I draw the curves but I got confused at the bounds of the integrals.","where is the domain between circle and the parabola Edit: Also, I got the intersection of the curves and I draw the curves but I got confused at the bounds of the integrals.","\iint_{D} y^{3}\,dx\,dy D x^2 + y^2 = 6 y=x^2 (\sqrt2,2) (\sqrt-3, -3)","['integration', 'multivariable-calculus', 'multiple-integral']"
97,Green's Theorem and Line Integrals,Green's Theorem and Line Integrals,,"So I'm supposed to use Green' theorem to calculate the line integral $$ \int_{C_1} \frac{x^2-1}{x^2+4y^2}dx +\frac{x}{x^2+4y^2}dy $$ Where $C_1$ is the part of the parabola $y=1-x^2$ from point $(1,0)$ to $(-1,0)$ My first problem: I was able to calculate $\partial P/ \partial y$ and $\partial Q/ \partial x$ but it was indeed very tedious. Is there another way to calculate it? I thought about considering the derivatives only evaluated at the parabola, in a way I could write $x^2-1=-y$ but I don't know if I can do this because when we calculate the surface integral at Green's theorem we are considering the whole surface, right? Anyways, I found out $\partial Q/ \partial x - \partial P/ \partial y = 0$ . Then, since the theorem requires me to have a closed path, I chose my ""second path"" as the ellipsis $x^2+4y^2=1$ . So then I can write: $$ \int_{C_1} \frac{x^2-1}{x^2+4y^2}dx +\frac{x}{x^2+4y^2}dy= -\int_{C_2} x^2-1dx +xdy $$ After this we just need to parametrize the ellipsis as $y=\frac{1}{2} \sin(t); \enspace x=\cos(t)$ . Sadly enough the answer I get is not the correct one (which is $\pi/2$ ). What am I doing wrong/ is there a better way to proceed? I appreciate any tips/corrections.","So I'm supposed to use Green' theorem to calculate the line integral Where is the part of the parabola from point to My first problem: I was able to calculate and but it was indeed very tedious. Is there another way to calculate it? I thought about considering the derivatives only evaluated at the parabola, in a way I could write but I don't know if I can do this because when we calculate the surface integral at Green's theorem we are considering the whole surface, right? Anyways, I found out . Then, since the theorem requires me to have a closed path, I chose my ""second path"" as the ellipsis . So then I can write: After this we just need to parametrize the ellipsis as . Sadly enough the answer I get is not the correct one (which is ). What am I doing wrong/ is there a better way to proceed? I appreciate any tips/corrections.","
\int_{C_1} \frac{x^2-1}{x^2+4y^2}dx +\frac{x}{x^2+4y^2}dy
 C_1 y=1-x^2 (1,0) (-1,0) \partial P/ \partial y \partial Q/ \partial x x^2-1=-y \partial Q/ \partial x - \partial P/ \partial y = 0 x^2+4y^2=1 
\int_{C_1} \frac{x^2-1}{x^2+4y^2}dx +\frac{x}{x^2+4y^2}dy= -\int_{C_2} x^2-1dx +xdy
 y=\frac{1}{2} \sin(t); \enspace x=\cos(t) \pi/2","['multivariable-calculus', 'parametrization', 'line-integrals', 'greens-theorem']"
98,Volume of $(x^2 y^2) + (x^2 z^2) + (y^2 z^2) = 1$,Volume of,(x^2 y^2) + (x^2 z^2) + (y^2 z^2) = 1,"I've long been intrigued by this surface which tightly hugs each axis, extending to infinity but with finite volume. But integrating this formula is beyond my powers. Any suggestions on how to do this, or demonstrations of the full answer? I suspect spherical coordinates might help, but that seems to make the formulas even more complex and beyond my capacity. An upper bound on volume: Consider the surface near the x-axis only, for x > 1. The cross-section around the axis should be within the circle $y^2 + z^2 = 1/x^2$ [Gabriel's Horn, as David K helpfully pointed out below], and yet near it, since the dropped term, $y^2 z^2 / x^2$ , is << $1/x^2$ . This circle has area $\pi/x^4$ ; integrating from 1 to infinity gives an upper bound of volume around all axes of $6\pi$ plus the area within 1 unit of the origin (max. 8 there, of course), for a total <27. The graph of the cross-section still looks nearly circular at x=1, so I'm expecting a more exact answer might be in the 24-26 range. Slight progress: the smallest cube enclosed by the surface has vertices at x=y=z = $1/\sqrt[4]3$ , so its volume is $8/\sqrt[4]{27}$ . The six arms fully separate at axis values >1. In between these values one would have to integrate a complex cross section of squares with rounded corners--the square sides separating the parts closest to each axis to get a partial volume, then multiplying by 6 just as for each arm segment to get a total volume. But both this and the infinite part of the arm requires the anti-differential of $\sqrt{((1-x^2y^2)/(x^2+y^2))}$ . Latest update: I found that wolfram alpha has a double integral calculator widget; it doesn't show an anti-derivative for the above function of z, but presumably did a numerical integration and delivered the volume for the curve (x: 0-infinity; y: 0-1/x) of 3.24099; multiplying by 8 gives 25.928, in the ballpark of my earlier estimate. It would still be interesting to know if it has some more exact formulation (even if this involves square roots or other complex terms).","I've long been intrigued by this surface which tightly hugs each axis, extending to infinity but with finite volume. But integrating this formula is beyond my powers. Any suggestions on how to do this, or demonstrations of the full answer? I suspect spherical coordinates might help, but that seems to make the formulas even more complex and beyond my capacity. An upper bound on volume: Consider the surface near the x-axis only, for x > 1. The cross-section around the axis should be within the circle [Gabriel's Horn, as David K helpfully pointed out below], and yet near it, since the dropped term, , is << . This circle has area ; integrating from 1 to infinity gives an upper bound of volume around all axes of plus the area within 1 unit of the origin (max. 8 there, of course), for a total <27. The graph of the cross-section still looks nearly circular at x=1, so I'm expecting a more exact answer might be in the 24-26 range. Slight progress: the smallest cube enclosed by the surface has vertices at x=y=z = , so its volume is . The six arms fully separate at axis values >1. In between these values one would have to integrate a complex cross section of squares with rounded corners--the square sides separating the parts closest to each axis to get a partial volume, then multiplying by 6 just as for each arm segment to get a total volume. But both this and the infinite part of the arm requires the anti-differential of . Latest update: I found that wolfram alpha has a double integral calculator widget; it doesn't show an anti-derivative for the above function of z, but presumably did a numerical integration and delivered the volume for the curve (x: 0-infinity; y: 0-1/x) of 3.24099; multiplying by 8 gives 25.928, in the ballpark of my earlier estimate. It would still be interesting to know if it has some more exact formulation (even if this involves square roots or other complex terms).",y^2 + z^2 = 1/x^2 y^2 z^2 / x^2 1/x^2 \pi/x^4 6\pi 1/\sqrt[4]3 8/\sqrt[4]{27} \sqrt{((1-x^2y^2)/(x^2+y^2))},"['multivariable-calculus', 'spherical-coordinates']"
99,show that $\partial_u(f \circ g)=f'(g(x))\partial_ug(x)= f'(g(x))(\nabla g(x) \cdot u)$,show that,\partial_u(f \circ g)=f'(g(x))\partial_ug(x)= f'(g(x))(\nabla g(x) \cdot u),"Let $f :\mathbb{R} \to \mathbb{R}$ be a real-valued differentiable function. If $g :\mathbb{R^3} \to \mathbb{R}$ is differentiable, show that $$\partial_u(f \circ g)=f'(g(x))\partial_ug(x)= f'(g(x))(\nabla g(x) \cdot u)$$ where, $u$ is an unit vector. I cannot seem to get this to work. It pretty much uses only the chain rule(?), but the notation confuses me a lot. How should I go about this?","Let be a real-valued differentiable function. If is differentiable, show that where, is an unit vector. I cannot seem to get this to work. It pretty much uses only the chain rule(?), but the notation confuses me a lot. How should I go about this?",f :\mathbb{R} \to \mathbb{R} g :\mathbb{R^3} \to \mathbb{R} \partial_u(f \circ g)=f'(g(x))\partial_ug(x)= f'(g(x))(\nabla g(x) \cdot u) u,['multivariable-calculus']
