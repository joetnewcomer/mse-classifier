,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,Probability of passing the 3rd exam,Probability of passing the 3rd exam,,"Let $A_i$ be the event that the student passes the ith exam. The probability of passing the 1st exam is 0.80. If he passes the 1st exam then the chance of passing the second is 0.81 but if fails the first then the chance he passes the second drops to 0.40. If he passes both then his chance of passing the third exam is 0.82. If he only passes one of the first two then the chance of passing the third is 0.60. If he fails both then the chance of passing the 3rd is 0.10. Probability of passing the 1st and 2nd exam: $P(A_1A_2)=P(A_2|A_1)P(A_1)=(0.81)(0.80)=0.648$ Probability of failing the 1st and 2nd exam: $P(A_1^CA_2^C)=P(A_2^C|A_1^C)P(A_1^C)=(1-0.80)(1-0.40)=0.12$ Probability of failing the 1st but passing the 2nd: $P(A_1^CA_2)=P(A_2|A_1^C)P(A_1^C)=(0.40)(0.20)=0.08$ Probability of passing the 3rd exam: $P(A_3)=P(A_3|A_1A_2)P(A_1A_2)+P(A_3|A_1^C \cup A_2^C)P(A_1^C \cup A_2^C)$ I'm struggling with #4, I'm not sure if I have the right idea in my attempt since the $A_i$ 's are not mutually exclusive and if my attempt is correct, I still don't know how to go about finding $P(A_3|A_1^C \cup A_2^C)$ . Please let me know how I could do #4 and if anything needs to be corrected.","Let be the event that the student passes the ith exam. The probability of passing the 1st exam is 0.80. If he passes the 1st exam then the chance of passing the second is 0.81 but if fails the first then the chance he passes the second drops to 0.40. If he passes both then his chance of passing the third exam is 0.82. If he only passes one of the first two then the chance of passing the third is 0.60. If he fails both then the chance of passing the 3rd is 0.10. Probability of passing the 1st and 2nd exam: Probability of failing the 1st and 2nd exam: Probability of failing the 1st but passing the 2nd: Probability of passing the 3rd exam: I'm struggling with #4, I'm not sure if I have the right idea in my attempt since the 's are not mutually exclusive and if my attempt is correct, I still don't know how to go about finding . Please let me know how I could do #4 and if anything needs to be corrected.",A_i P(A_1A_2)=P(A_2|A_1)P(A_1)=(0.81)(0.80)=0.648 P(A_1^CA_2^C)=P(A_2^C|A_1^C)P(A_1^C)=(1-0.80)(1-0.40)=0.12 P(A_1^CA_2)=P(A_2|A_1^C)P(A_1^C)=(0.40)(0.20)=0.08 P(A_3)=P(A_3|A_1A_2)P(A_1A_2)+P(A_3|A_1^C \cup A_2^C)P(A_1^C \cup A_2^C) A_i P(A_3|A_1^C \cup A_2^C),['probability']
1,Formal definition for conditional expectation of $\mathbb{E}[X\mid X>a]$,Formal definition for conditional expectation of,\mathbb{E}[X\mid X>a],"I would like to know why is it true that for any random variable $X$ discrete, continues, or any other case with cumulative distribution function $F_X(x)$ it happens that, for example that the expectation of $X|X>a$ can be written as: $$ \mathbb{E}[X\mid X>a] =\frac{\int_{a}^\infty x\,dF_X(x)}{\mathbb{P}[X>a]}. $$ And the general case, $X\in C\subset \mathbb{R}$ then $$ \mathbb{E}[X\mid X\in C] =\frac{\int_{C} x\,dF_X(x)}{\mathbb{P}[X\in C]}. $$ I'd like to know a formal argument... but all I found is formal definitions such using random variables such as $\mathbb{E}[X|\mathcal{F}]$ , but not any explanation that why is this happening.","I would like to know why is it true that for any random variable discrete, continues, or any other case with cumulative distribution function it happens that, for example that the expectation of can be written as: And the general case, then I'd like to know a formal argument... but all I found is formal definitions such using random variables such as , but not any explanation that why is this happening.","X F_X(x) X|X>a 
\mathbb{E}[X\mid X>a] =\frac{\int_{a}^\infty x\,dF_X(x)}{\mathbb{P}[X>a]}.
 X\in C\subset \mathbb{R} 
\mathbb{E}[X\mid X\in C] =\frac{\int_{C} x\,dF_X(x)}{\mathbb{P}[X\in C]}.
 \mathbb{E}[X|\mathcal{F}]","['probability', 'probability-theory', 'measure-theory']"
2,Birthday problem - expected number of shared birthdays,Birthday problem - expected number of shared birthdays,,"Given $m$ people and an $n$ possible ""days of the year"", what is the expected number of days which 2 or more people share as a birthday (if the distribution of birthdays is iid uniform over the possible days) ? (Alternative formulation: We use $n$ distinct colors to paint $m$ objects. Each object gets colored with a uniformly iid randomly chosen color. What is the number of colors that color at least two objects?) Written more precisely, let $B_i$ be independent uniform random on $S = \{ 1, ..., n \}$ for $i = 1, ..., m$ . What is the expected cardinality of the set of $d \in S$ where there exist $i,j$ not equal with $B_i = B_j = d$ ? This sounds similar to birthday problem - expected number of collisions which asks for the number of collisions, but that question was about the number of people who share birthdays, not the number of days shared. The answer to that question is in the range $[0,m]$ while the answer to my question is in the range $[0,n]$ . Ultimately this is actually a question about hash collisions and prefix collisions in a prefix tree, but I deemed the birthday formulation the most familiar and most likely to be helpful to others searching for it in the future.","Given people and an possible ""days of the year"", what is the expected number of days which 2 or more people share as a birthday (if the distribution of birthdays is iid uniform over the possible days) ? (Alternative formulation: We use distinct colors to paint objects. Each object gets colored with a uniformly iid randomly chosen color. What is the number of colors that color at least two objects?) Written more precisely, let be independent uniform random on for . What is the expected cardinality of the set of where there exist not equal with ? This sounds similar to birthday problem - expected number of collisions which asks for the number of collisions, but that question was about the number of people who share birthdays, not the number of days shared. The answer to that question is in the range while the answer to my question is in the range . Ultimately this is actually a question about hash collisions and prefix collisions in a prefix tree, but I deemed the birthday formulation the most familiar and most likely to be helpful to others searching for it in the future.","m n n m B_i S = \{ 1, ..., n \} i = 1, ..., m d \in S i,j B_i = B_j = d [0,m] [0,n]","['probability', 'birthday']"
3,"$n$ points at random on line segment, average distance between two consecutive points","points at random on line segment, average distance between two consecutive points",n,"I had previously asked the following question from my probability textbook: If two points be taken at random on a finite straight line their average distance apart will be one third of the line. See here: Integrating $\int_0^1 \int_0^1 |x-y|\,\text{d}x\,\text{d}y$ by hand Basically, it boils down to calculating ${\int_0^1 \int_0^1 |x-y|\,\text{d}x\,\text{d}y} = {1\over3}$ . It turns out this question had been asked before a long time ago, see here: Average Distance Between Random Points on a Line Segment Now, my probability textbook also asks the following generalization: If $n$ points be taken at random on a finite line the average distance between any two consecutive points will be one $(n+1)$ th of the line. My question is, how do I go about showing this? How should go about generalizing my previous integral of ${\int_0^1 \int_0^1 |x-y|\,\text{d}x\,\text{d}y}$ ? Any help would be well-appreciated.","I had previously asked the following question from my probability textbook: If two points be taken at random on a finite straight line their average distance apart will be one third of the line. See here: Integrating $\int_0^1 \int_0^1 |x-y|\,\text{d}x\,\text{d}y$ by hand Basically, it boils down to calculating . It turns out this question had been asked before a long time ago, see here: Average Distance Between Random Points on a Line Segment Now, my probability textbook also asks the following generalization: If points be taken at random on a finite line the average distance between any two consecutive points will be one th of the line. My question is, how do I go about showing this? How should go about generalizing my previous integral of ? Any help would be well-appreciated.","{\int_0^1 \int_0^1 |x-y|\,\text{d}x\,\text{d}y} = {1\over3} n (n+1) {\int_0^1 \int_0^1 |x-y|\,\text{d}x\,\text{d}y}","['probability', 'multivariable-calculus', 'contest-math', 'multiple-integral']"
4,"Counting problem with numbered deck of cards, probability one card is exactly double another","Counting problem with numbered deck of cards, probability one card is exactly double another",,"I've recently purchased a book I assume has an incorrect solution to this problem. I wanted to run my solution by you Assume you have a deck of 100 cards with values ranging from 1 to 100 and that you draw two cards at random without replacement. What is probability one card is precisely double that of the other? My solution: Assume first card can be double or second card can be double If first card is double it can be any even number (50 choices) If first card is double, second card has 1/99 probability of being the half-card So for the first card to be double, the probability is: 50 * 1/100 * 1/99 For the second card to be double, we simply double our total probability. So the probability of drawing two cards, where one is double the other, is: 2 * 1/2 * 1/99 or 0.0101 Can someone let me know if I'm correct? The book's solution is double this, saying since order does not matter there are exactly 50 * 2 = 100 ways to draw such a pair. So the desired probability is 100/(100 choose 2) .","I've recently purchased a book I assume has an incorrect solution to this problem. I wanted to run my solution by you Assume you have a deck of 100 cards with values ranging from 1 to 100 and that you draw two cards at random without replacement. What is probability one card is precisely double that of the other? My solution: Assume first card can be double or second card can be double If first card is double it can be any even number (50 choices) If first card is double, second card has 1/99 probability of being the half-card So for the first card to be double, the probability is: 50 * 1/100 * 1/99 For the second card to be double, we simply double our total probability. So the probability of drawing two cards, where one is double the other, is: 2 * 1/2 * 1/99 or 0.0101 Can someone let me know if I'm correct? The book's solution is double this, saying since order does not matter there are exactly 50 * 2 = 100 ways to draw such a pair. So the desired probability is 100/(100 choose 2) .",,"['probability', 'combinatorics']"
5,Proving the $(n+1)$th moment of a continuous random variable,Proving the th moment of a continuous random variable,(n+1),"Let $X$ be a continuous random variable with the probability density function $$ f(x)=\left\{\begin{array}{cc} \frac{x \sin x}{\pi}, & 0<x<\pi \\ 0, & \text { otherwise. } \end{array}\right. $$ Prove that $\mathbb{E}\left(X^{n+1}\right)+(n+1)(n+2) \mathbb{E}\left(X^{n-1}\right)=\pi^{n+1}$ . I've tried computing the first part $E(X^{n+1})$ and here's what I get: Proof Let $k = n+1$ . Notice that $$F_X(x) = \frac{1}{\pi}\int_0^{x} y \sin (y) \,dy = \frac{1}{\pi}(\sin(x) - x \cos(x)) $$ Then, using the identity $E(X^n) = n \int_0^{\infty}x^{n-1}(1-F_X(x)) \,dx$ , $$E(X^k) = k \int_0^{\pi} x^{k-1} (1-Fx(x)) \,dx$$ $$= k \int_0^{\pi} x^{k-1} (1-\frac{1}{\pi}(\sin(x) - x \cos(x)) \,dx $$ $$= k \int_0^{\pi} x^{k-1} - \frac{x^{k-1}}{\pi} \sin(x) - \frac{x^k}{\pi} \cos(x) \,dx $$ But I can't compute the definite integral of $x^k \cos(x) \,dx$ . Well, at least not in the course I'm doing, we are not learning gamma functions. The solutions say to ""use integration by parts twice"" and that's it. Is the way I'm going about this just completely wrong? How else could I prove this?","Let be a continuous random variable with the probability density function Prove that . I've tried computing the first part and here's what I get: Proof Let . Notice that Then, using the identity , But I can't compute the definite integral of . Well, at least not in the course I'm doing, we are not learning gamma functions. The solutions say to ""use integration by parts twice"" and that's it. Is the way I'm going about this just completely wrong? How else could I prove this?","X 
f(x)=\left\{\begin{array}{cc}
\frac{x \sin x}{\pi}, & 0<x<\pi \\
0, & \text { otherwise. }
\end{array}\right.
 \mathbb{E}\left(X^{n+1}\right)+(n+1)(n+2) \mathbb{E}\left(X^{n-1}\right)=\pi^{n+1} E(X^{n+1}) k = n+1 F_X(x) = \frac{1}{\pi}\int_0^{x} y \sin (y) \,dy = \frac{1}{\pi}(\sin(x) - x \cos(x))  E(X^n) = n \int_0^{\infty}x^{n-1}(1-F_X(x)) \,dx E(X^k) = k \int_0^{\pi} x^{k-1} (1-Fx(x)) \,dx = k \int_0^{\pi} x^{k-1} (1-\frac{1}{\pi}(\sin(x) - x \cos(x)) \,dx  = k \int_0^{\pi} x^{k-1} - \frac{x^{k-1}}{\pi} \sin(x) - \frac{x^k}{\pi} \cos(x) \,dx  x^k \cos(x) \,dx","['probability', 'probability-distributions', 'random-variables', 'expected-value']"
6,Variance of a Probability density function,Variance of a Probability density function,,"I'm trying to find the variance of the following random variable with density function $$f(x)=\begin{cases} \frac{3}{2}x(1+x),& \text{ $|x|\leq1$ }  \\   0& \text{  }  o. w \end{cases}$$ First, we find $\displaystyle E(x)=\frac{3}{2} \int_{-1}^{1}x^2(1+x)dx=1$ and $E(x^2)= \displaystyle \frac{3}{2} \int_{-1}^{1}x^3(1+x)dx= \frac{3}{5}.$ Now, variance of the random variable $x$ is defined  by $V(x)=E(x^2)-(E(x))^2= \frac{3}{5}-1\Rightarrow \frac{-2}{5}<0$ . But, variance can't be negative. Where did I made a mistake?","I'm trying to find the variance of the following random variable with density function First, we find and Now, variance of the random variable is defined  by . But, variance can't be negative. Where did I made a mistake?","f(x)=\begin{cases}
\frac{3}{2}x(1+x),& \text{ |x|\leq1 }  \\ 
 0& \text{  }  o. w
\end{cases} \displaystyle E(x)=\frac{3}{2} \int_{-1}^{1}x^2(1+x)dx=1 E(x^2)= \displaystyle \frac{3}{2} \int_{-1}^{1}x^3(1+x)dx= \frac{3}{5}. x V(x)=E(x^2)-(E(x))^2= \frac{3}{5}-1\Rightarrow \frac{-2}{5}<0","['probability', 'probability-distributions', 'random-variables']"
7,"Wrong solution: Given three bins and three balls, what is the possibility that exactly one bin is empty?","Wrong solution: Given three bins and three balls, what is the possibility that exactly one bin is empty?",,"I would like to know why the 'solution' with which I came up is wrong. I gave the correct answer at the bottom. Each ball has probability $\frac{1}{3}$ of going into either bin, and there are a total of $3^3=27$ ways to distribute the balls among the bins. My solution: There are $3$ ways to select the bin for the first ball, $2$ ways to select the bin for the second ball (because it must not go into the bin selected for the first ball), and $2$ ways to select the bin for the third ball (because it must go into either of those selected before). This gives a probability of $\frac{3\cdot 2\cdot 2}{27}=\frac{12}{27}=\frac{4}{9}$ . Correct answer: $\frac{{{3}\choose{2}}(2^3-2)}{27}=\frac{2}{3}$ , because there are ${3}\choose{2}$ ways to select the bins into which the balls should go, $2^3$ ways to put the $3$ balls into these bins, and in $2$ of those cases all balls are in the same bin. As I wrote in the opening, I don't quite see what's wrong with the first attempt.","I would like to know why the 'solution' with which I came up is wrong. I gave the correct answer at the bottom. Each ball has probability of going into either bin, and there are a total of ways to distribute the balls among the bins. My solution: There are ways to select the bin for the first ball, ways to select the bin for the second ball (because it must not go into the bin selected for the first ball), and ways to select the bin for the third ball (because it must go into either of those selected before). This gives a probability of . Correct answer: , because there are ways to select the bins into which the balls should go, ways to put the balls into these bins, and in of those cases all balls are in the same bin. As I wrote in the opening, I don't quite see what's wrong with the first attempt.",\frac{1}{3} 3^3=27 3 2 2 \frac{3\cdot 2\cdot 2}{27}=\frac{12}{27}=\frac{4}{9} \frac{{{3}\choose{2}}(2^3-2)}{27}=\frac{2}{3} {3}\choose{2} 2^3 3 2,"['probability', 'combinatorics', 'permutations', 'combinations']"
8,"1 out of 1000 boats get hit by lightning each year, if you boat an average amount each year for 20 years what is the probability of getting struck?","1 out of 1000 boats get hit by lightning each year, if you boat an average amount each year for 20 years what is the probability of getting struck?",,Having a discussion with my father over this. We found a article from an insurance company that says 1 out of 1000 boats get struck by lightning each year. He says that the probability of getting struck over a 20 year period is 1/50 but I say it is not. Could someone settle our debate and include the math to back it up?,Having a discussion with my father over this. We found a article from an insurance company that says 1 out of 1000 boats get struck by lightning each year. He says that the probability of getting struck over a 20 year period is 1/50 but I say it is not. Could someone settle our debate and include the math to back it up?,,"['probability', 'probability-theory', 'statistics']"
9,Expected square of frog's distance to the centre in the limit of infinite number of jumps: $\lim\limits_{n\to\infty}E[|r_n|^2]$,Expected square of frog's distance to the centre in the limit of infinite number of jumps:,\lim\limits_{n\to\infty}E[|r_n|^2],"Consider a unit circle on a plane and a frog in its centre. The frog makes infinite series of jumps. For the $n^{th}$ jump, the frog chooses a random point $x_n$ on the circle and jumps to the middle of the segment connecting its current position $r_n$ with the point $x_n$ . Find the expected square of the frog's distance to the centre in the limit of an infinite number of jumps: $\lim\limits_{n\to\infty}E[|r_n|^2]$ I'm kind of lost in this problem and don't know where to start. Albeit, I understand a few things, e.g. the area of the unit circle will be $\pi$ and the segment that will be created by $x_n$ and $r_n$ , the centre of the segment would be $\frac{1}{2}$ units from the origin (unit circle). But that's it, any help is appreciated!","Consider a unit circle on a plane and a frog in its centre. The frog makes infinite series of jumps. For the jump, the frog chooses a random point on the circle and jumps to the middle of the segment connecting its current position with the point . Find the expected square of the frog's distance to the centre in the limit of an infinite number of jumps: I'm kind of lost in this problem and don't know where to start. Albeit, I understand a few things, e.g. the area of the unit circle will be and the segment that will be created by and , the centre of the segment would be units from the origin (unit circle). But that's it, any help is appreciated!",n^{th} x_n r_n x_n \lim\limits_{n\to\infty}E[|r_n|^2] \pi x_n r_n \frac{1}{2},"['probability', 'statistics', 'expected-value']"
10,Approximation of $\operatorname{Poisson}(\lambda)$ by Binomial,Approximation of  by Binomial,\operatorname{Poisson}(\lambda),"I don't really understand the approximation of $\operatorname{Poisson}(\lambda)$ distributions by binomial distributions (law of rare events?). For example if I consider $X\sim \operatorname{Poissson}(1)$ , its distribution is not symmetric as drawed in wikipedia (orange curve). However, what I understood is that if we take say $n=1000$ (""big"") and $p=0.001$ (such that $np=1$ ) and we consider the binomial distribution $Y\sim B(n,p)$ we will have that the distribution of $X$ and the distribution of $Y$ are almost the same. But $X$ has a distribution which is not symmetric and $Y$ has a distribution that is symmetric which is confusing me.","I don't really understand the approximation of distributions by binomial distributions (law of rare events?). For example if I consider , its distribution is not symmetric as drawed in wikipedia (orange curve). However, what I understood is that if we take say (""big"") and (such that ) and we consider the binomial distribution we will have that the distribution of and the distribution of are almost the same. But has a distribution which is not symmetric and has a distribution that is symmetric which is confusing me.","\operatorname{Poisson}(\lambda) X\sim \operatorname{Poissson}(1) n=1000 p=0.001 np=1 Y\sim B(n,p) X Y X Y","['probability', 'probability-theory', 'intuition']"
11,"A six digit number is formed randomly using digits $\{1,2,3\}$ with repetitions. Choose the correct option(s):",A six digit number is formed randomly using digits  with repetitions. Choose the correct option(s):,"\{1,2,3\}","A six digit number is formed randomly using digits $\{1,2,3\}$ with repetitions. Choose the correct option(s): A) Probability that all digits are used at least once is $\dfrac{20}{27}$ B) Probability that digit $1$ is used odd number of times and $2$ is used even number of times is $\dfrac{1-3^{-6}}4$ C) Probability that all digits are used as well as odd digits are used odd number of times and even digit is used even number of times is $\dfrac{3^6-2^7+1}{4\cdot3^6}$ Total cases = $3^6$ A) All digits used at least once = Total cases $-$ Cases when one digit is not used Or, would it be All digits used at least once = Total cases $-$ Cases when one digit is not used $-$ Cases when two digits are not used In any case, I am not getting $\frac{20}{27}$ B) $1$ can be used $1$ or $3$ or $5$ times. $2$ can be used $0$ or $2$ or $4$ times. So, favorable cases= $^6C_1(^5C_0+^5C_2+^5C_4)+^6C_3(^3C_0+^3C_2)+^6C_5=182$ , and it matches with the answer. But the answer given in the option is of different format $\dfrac{3^6-1}{4\cdot3^6}$ . Looks like they are subtacting $1$ from the total cases and diving by $4$ to get the favorable case. Why? C) $1$ can be used $1$ or $3$ or $5$ times. $2$ can be used $2$ or $4$ times. So, favorable cases= $^6C_1(^5C_2+^5C_4)+^6C_3(^3C_2)+^6C_5=156$ But as per the option, favorable cases= $150.5$ . Also, looks like they are subtracting $2^7-1$ from total cases and then diving by $4$ , what could be the motivation for this, even if this is wrong?","A six digit number is formed randomly using digits with repetitions. Choose the correct option(s): A) Probability that all digits are used at least once is B) Probability that digit is used odd number of times and is used even number of times is C) Probability that all digits are used as well as odd digits are used odd number of times and even digit is used even number of times is Total cases = A) All digits used at least once = Total cases Cases when one digit is not used Or, would it be All digits used at least once = Total cases Cases when one digit is not used Cases when two digits are not used In any case, I am not getting B) can be used or or times. can be used or or times. So, favorable cases= , and it matches with the answer. But the answer given in the option is of different format . Looks like they are subtacting from the total cases and diving by to get the favorable case. Why? C) can be used or or times. can be used or times. So, favorable cases= But as per the option, favorable cases= . Also, looks like they are subtracting from total cases and then diving by , what could be the motivation for this, even if this is wrong?","\{1,2,3\} \dfrac{20}{27} 1 2 \dfrac{1-3^{-6}}4 \dfrac{3^6-2^7+1}{4\cdot3^6} 3^6 - - - \frac{20}{27} 1 1 3 5 2 0 2 4 ^6C_1(^5C_0+^5C_2+^5C_4)+^6C_3(^3C_0+^3C_2)+^6C_5=182 \dfrac{3^6-1}{4\cdot3^6} 1 4 1 1 3 5 2 2 4 ^6C_1(^5C_2+^5C_4)+^6C_3(^3C_2)+^6C_5=156 150.5 2^7-1 4","['probability', 'combinatorics', 'solution-verification', 'proof-explanation']"
12,$1/E(x)$ vs. $E(1/x)$,vs.,1/E(x) E(1/x),"Question concerning MVUE and geometric distribution, trying to apply Rao-Blackwell Theorem here. We know that the geometric distribution is a regular exponential class with $$Y = \sum x$$ as our sufficient and complete statistic. However $$ E(Y) = \frac{n(1- \theta)}{\theta} $$ is not an unbiased estimator because it does not equal theta. Since 1/E(X) is usually not E(1/X), I tried $$ E(\frac{1}{y}) = \sum\frac{1}{y}\theta(1-\theta)^\frac{1}{y} $$ but that's where I got royally stuck. Is there a way around this summation so that I can get $$ E(Y) = \theta $$","Question concerning MVUE and geometric distribution, trying to apply Rao-Blackwell Theorem here. We know that the geometric distribution is a regular exponential class with as our sufficient and complete statistic. However is not an unbiased estimator because it does not equal theta. Since 1/E(X) is usually not E(1/X), I tried but that's where I got royally stuck. Is there a way around this summation so that I can get",Y = \sum x  E(Y) = \frac{n(1- \theta)}{\theta}   E(\frac{1}{y}) = \sum\frac{1}{y}\theta(1-\theta)^\frac{1}{y}   E(Y) = \theta ,"['probability', 'statistics', 'probability-distributions', 'expected-value']"
13,Multiplying or adding constants within $P(X \leq x)$?,Multiplying or adding constants within ?,P(X \leq x),"Is it always true, that, for a positive constant $c$ , we have $P(X \leq x) = P(c X \leq cx)$ for some continuous random variable $X$ ? Further is it always true that $P(X + c \leq x + c) = P(X \leq x)$ ? If so, what governs the equality?","Is it always true, that, for a positive constant , we have for some continuous random variable ? Further is it always true that ? If so, what governs the equality?",c P(X \leq x) = P(c X \leq cx) X P(X + c \leq x + c) = P(X \leq x),['probability']
14,Problems solving the SDE $dX_t = aX_tdt +\sigma dB_t$. Why don't Ito's lemma work?,Problems solving the SDE . Why don't Ito's lemma work?,dX_t = aX_tdt +\sigma dB_t,"I am trying to solve the SDE below and I am running into some problems. We're given that $X_0=1$ , $a \in R$ , $\sigma > 0.$ $$dX_t = aX_tdt +\sigma dB_t$$ I tried to solve it the following manner. Assuming that $X_t=f(t, B_t)$ and applying Ito's lemma yielded: $$(\frac{\partial f}{\partial t} + \frac{1}{2}\frac{\partial f^2}{\partial^2 x})dt + \frac{\partial f}{\partial x}dB_t$$ Now here is where I want to match ""coefficients"". That is saying $$\frac{\partial f}{\partial x} = \sigma  \implies f=\sigma x + g(t)$$ Entering this into the other ""coefficent"" yields: $$\frac{\partial f}{\partial t} + \frac{1}{2}\frac{\partial f^2}{\partial^2 x} = g'(t) = aX_t = af=a(\sigma x+g(t))$$ Now we've arrived at a seperable ODE for $g(t)$ which when solved would yield the final solution $f(t, B_t)$ . However, this doesn't work, and looking at the solution set I am completely off target. My question is why is this? Why can't I solve this SDE using this method? Where is the flaw in the solution method? The suggested solution method was to study $Y_t= e^{-at}X_t$ . How on earth would you come up with this anstaz by looking purely at the SDE? EDIT (completed tried solution): After having said that $f= \sigma x + g(t)$ I used this fact to obtain that: $$\frac{\partial f}{\partial t} + \frac{1}{2}\frac{\partial f^2}{\partial^2 x}=\frac{\partial f}{\partial t} =g'(t)=a(\sigma x  +g(t))$$ Now I tried to solve that ODE by the follwing processes: $$g'(t)=a(\sigma x+g(t))\iff \int \frac{dg}{a(\sigma x + g(t))} = t + C$$ This yielded further that $g(t)=Ce^{at}-\sigma x$ and so $f=\sigma x + g(t) = Ce^{at}$ . This clearly isn't the solution however.","I am trying to solve the SDE below and I am running into some problems. We're given that , , I tried to solve it the following manner. Assuming that and applying Ito's lemma yielded: Now here is where I want to match ""coefficients"". That is saying Entering this into the other ""coefficent"" yields: Now we've arrived at a seperable ODE for which when solved would yield the final solution . However, this doesn't work, and looking at the solution set I am completely off target. My question is why is this? Why can't I solve this SDE using this method? Where is the flaw in the solution method? The suggested solution method was to study . How on earth would you come up with this anstaz by looking purely at the SDE? EDIT (completed tried solution): After having said that I used this fact to obtain that: Now I tried to solve that ODE by the follwing processes: This yielded further that and so . This clearly isn't the solution however.","X_0=1 a \in R \sigma > 0. dX_t = aX_tdt +\sigma dB_t X_t=f(t, B_t) (\frac{\partial f}{\partial t} + \frac{1}{2}\frac{\partial f^2}{\partial^2 x})dt + \frac{\partial f}{\partial x}dB_t \frac{\partial f}{\partial x} = \sigma  \implies f=\sigma x + g(t) \frac{\partial f}{\partial t} + \frac{1}{2}\frac{\partial f^2}{\partial^2 x} = g'(t) = aX_t = af=a(\sigma x+g(t)) g(t) f(t, B_t) Y_t= e^{-at}X_t f= \sigma x + g(t) \frac{\partial f}{\partial t} + \frac{1}{2}\frac{\partial f^2}{\partial^2 x}=\frac{\partial f}{\partial t} =g'(t)=a(\sigma x  +g(t)) g'(t)=a(\sigma x+g(t))\iff \int \frac{dg}{a(\sigma x + g(t))} = t + C g(t)=Ce^{at}-\sigma x f=\sigma x + g(t) = Ce^{at}","['probability', 'stochastic-processes', 'stochastic-calculus', 'stochastic-integrals', 'stochastic-differential-equations']"
15,Is there a central limit theorem for $L^p$?,Is there a central limit theorem for ?,L^p,let $Y_i$ be a sequence of identically distributed independent random variables with mean 0 and variance 1.  If $X$ is independent and gaussian with the same mean and variance then does $\frac{1}{\sqrt{n}}(\sum_{i=1}^n Y_i)$ converge in $L^p$ to $X$ ?  The central limit theorem usually holds in distribution but I would like to know if there is a stronger convergence where the sum converges to a gaussian random variable.,let be a sequence of identically distributed independent random variables with mean 0 and variance 1.  If is independent and gaussian with the same mean and variance then does converge in to ?  The central limit theorem usually holds in distribution but I would like to know if there is a stronger convergence where the sum converges to a gaussian random variable.,Y_i X \frac{1}{\sqrt{n}}(\sum_{i=1}^n Y_i) L^p X,"['probability', 'probability-theory', 'statistics', 'probability-distributions']"
16,"Let $X, Y, Z \sim N(0, 1)$ and are independent. How to show that $\frac{X + ZY}{\sqrt{1 + Z^2}} \sim N(0, 1)$? [duplicate]",Let  and are independent. How to show that ? [duplicate],"X, Y, Z \sim N(0, 1) \frac{X + ZY}{\sqrt{1 + Z^2}} \sim N(0, 1)","This question already has answers here : Finding distribution of $\frac{X_1+X_2 X_3}{\sqrt{1+X_3^2}}$ (2 answers) Closed 3 years ago . Let $X, Y, Z$ be i.i.d. random variables distributed as $N(0, 1)$ . How to show that $\dfrac{X + ZY}{\sqrt{1 + Z^2}} \sim N(0, 1)$ ? The only solution I see is to progressively find the distributions of First, $ZY$ Then $U := X + ZY$ Then $V := \sqrt{1 + Z^2}$ Then $\dfrac{U}{V}$ But finding these distributions appears to be a nightmare in terms of the number of calculations to be done. And this solution doesn't really explain the intuition behind this problem: how could this expression randomly end up being $N(0, 1)$ -distributed? That makes me wonder if there is a more nice solution. I also think, this is probably a well-known problem, but I couldn't find a solution anywhere. Any help would be greatly appreciated!","This question already has answers here : Finding distribution of $\frac{X_1+X_2 X_3}{\sqrt{1+X_3^2}}$ (2 answers) Closed 3 years ago . Let be i.i.d. random variables distributed as . How to show that ? The only solution I see is to progressively find the distributions of First, Then Then Then But finding these distributions appears to be a nightmare in terms of the number of calculations to be done. And this solution doesn't really explain the intuition behind this problem: how could this expression randomly end up being -distributed? That makes me wonder if there is a more nice solution. I also think, this is probably a well-known problem, but I couldn't find a solution anywhere. Any help would be greatly appreciated!","X, Y, Z N(0, 1) \dfrac{X + ZY}{\sqrt{1 + Z^2}} \sim N(0, 1) ZY U := X + ZY V := \sqrt{1 + Z^2} \dfrac{U}{V} N(0, 1)","['probability', 'normal-distribution']"
17,Probability: Rotating circular board,Probability: Rotating circular board,,"Let there be a circular board that is divided into $2^k$ sectors (similar to a dartboard). Each sector can be red with probability $p$ and green with probability $1- p$ . If we rotate the board, when it comes to rest, I am interested in an event that a red sector covers the 6 o'clock direction. Now let $X$ denote the reward that one gets when the above event occurs. The random variable $X$ takes a value according to the following rule: $X = 1/t$ , when $1 \leq t\leq 2^k$ sectors are red and one of them covers the 6 o'clock direction, i.e., the reward of $1$ is shared among those red sectors. I am interested in the probability mass function of $X$ . When none of the sectors that are red covers the 6 o'clock direction, the reward is $0$ , i.e., $X = 0$ . My attempt : There are $2^k + 1$ combinations depending on how many of the $2^k$ sectors are red, e.g., $0$ , $1$ , $2$ , $\dotsc$ , $2^k$ . Let $p_n$ denote the probability that $n$ sectors are red before rotating the board. Then we have $$p_n = {2^k \choose n}p^{n}(1-p)^{2^k-n}, \,\,\,\, 1 \leq n \leq 2^k.$$ Now, for the case when there are $1 \leq t \leq 2^k$ sectors are red, the probability that a red sector covers the 6 o'clock direction is $$q_t = \frac{t}{2^k}.$$ So, one has $$ X = \begin{cases} 1, \quad \text{w.p.}~~ p_1q_1 \\ \frac{1}{2}, \quad \text{w.p.}~~ p_2q_2\\ \vdots \\ \frac{1}{2^k}, \quad \text{w.p.}~~ p_{2^k}q_{2^k} \\ 0, \quad \text{w.p.}~~(1-p)^{2^k}. \end{cases} $$ Then the probability that a red sector covers the 6 o'clock direction is $$P_{c} = \sum_{n = 1}^{2^k} p_n q_n.$$ The probability that no red sector covers the 6 o'clock direction is $$P_{n,c} = (1-p)^{2^k}.$$ Then the following equation should hold: $$P_{c} + P_{n,c} = 1.$$ But it does not hold true. To check this, let us consider the case of $k = 1$ . In this case, $$P_{c} = \frac{1}{2}2p(1-p) + \frac{2}{2}p^2$$ and $$P_{n,c} = (1-p)^2.$$ Here, $P_c + P_{n,c} \neq 1$ . What am I missing here?","Let there be a circular board that is divided into sectors (similar to a dartboard). Each sector can be red with probability and green with probability . If we rotate the board, when it comes to rest, I am interested in an event that a red sector covers the 6 o'clock direction. Now let denote the reward that one gets when the above event occurs. The random variable takes a value according to the following rule: , when sectors are red and one of them covers the 6 o'clock direction, i.e., the reward of is shared among those red sectors. I am interested in the probability mass function of . When none of the sectors that are red covers the 6 o'clock direction, the reward is , i.e., . My attempt : There are combinations depending on how many of the sectors are red, e.g., , , , , . Let denote the probability that sectors are red before rotating the board. Then we have Now, for the case when there are sectors are red, the probability that a red sector covers the 6 o'clock direction is So, one has Then the probability that a red sector covers the 6 o'clock direction is The probability that no red sector covers the 6 o'clock direction is Then the following equation should hold: But it does not hold true. To check this, let us consider the case of . In this case, and Here, . What am I missing here?","2^k p 1- p X X X = 1/t 1 \leq t\leq 2^k 1 X 0 X = 0 2^k + 1 2^k 0 1 2 \dotsc 2^k p_n n p_n = {2^k \choose n}p^{n}(1-p)^{2^k-n}, \,\,\,\, 1 \leq n \leq 2^k. 1 \leq t \leq 2^k q_t = \frac{t}{2^k}. 
X =
\begin{cases}
1, \quad \text{w.p.}~~ p_1q_1 \\
\frac{1}{2}, \quad \text{w.p.}~~ p_2q_2\\
\vdots \\
\frac{1}{2^k}, \quad \text{w.p.}~~ p_{2^k}q_{2^k} \\
0, \quad \text{w.p.}~~(1-p)^{2^k}.
\end{cases}
 P_{c} = \sum_{n = 1}^{2^k} p_n q_n. P_{n,c} = (1-p)^{2^k}. P_{c} + P_{n,c} = 1. k = 1 P_{c} = \frac{1}{2}2p(1-p) + \frac{2}{2}p^2 P_{n,c} = (1-p)^2. P_c + P_{n,c} \neq 1","['probability', 'combinatorics', 'probability-theory']"
18,Probability St Petersburg Game,Probability St Petersburg Game,,"I am reading a book on probability and there is an interesting chapter on the St Petersburg game - where a coin is flipped until a head is landed and the prize is £2 if there is a head on the first throw, £4 is there is a head on the second flip, £8 for a head on the third etc. The idea of this game is it introduces a paradox since someone should be prepared to pay any amount to play since the expected yield is infinite. However in the book it introduces the notion of changing the rewards to 2,4,6,8.... etc rather than the doubling in the original game. The book contends that even though the possible payouts increase without bound the calculation of the expected value yields a sensible answer of £4. It mentions that this is simple to calculate so omits the calculations. I have pondering this and cant see how the £4 has been arrived at - a entry price of £4 would be a fair price for the game but why? Any help greatly appreciated. I am sure I'm missing something to do with summing to infinity perhaps","I am reading a book on probability and there is an interesting chapter on the St Petersburg game - where a coin is flipped until a head is landed and the prize is £2 if there is a head on the first throw, £4 is there is a head on the second flip, £8 for a head on the third etc. The idea of this game is it introduces a paradox since someone should be prepared to pay any amount to play since the expected yield is infinite. However in the book it introduces the notion of changing the rewards to 2,4,6,8.... etc rather than the doubling in the original game. The book contends that even though the possible payouts increase without bound the calculation of the expected value yields a sensible answer of £4. It mentions that this is simple to calculate so omits the calculations. I have pondering this and cant see how the £4 has been arrived at - a entry price of £4 would be a fair price for the game but why? Any help greatly appreciated. I am sure I'm missing something to do with summing to infinity perhaps",,"['probability', 'probability-theory', 'expected-value', 'average']"
19,"Probability of $X > \max\{Y,Z\}$ exponentials",Probability of  exponentials,"X > \max\{Y,Z\}","Let $X,$ $Y$ and $Z$ be exponentially distributed with $E[X] = 1/\lambda_X$ , $E[Y] = 1/\lambda_Y$ , $E[Z] = 1/\lambda_Z$ respectively. How do I calculate $P(X > \max\{Y,Z\})$ ? I have A, B, C and D for 3 cash boxes in a queueing system. Consider D the one waiting. I want to find the probability D is the last one to leave. The service times of the C.Boxes are indep. Exp with mean 5 (same mean for c.box 1, 2 and 3) That's how I'm trying to solve this: P(D last one) = P(T1 > max{T2,T3})*P(T1 < min{T2,T3}) + P(T2 > max{T1,T3})*P(T2 < min{T1,T3}) + P(T3 > max{T1,T2})*P(T3 < min{T1,T2}) And then: $$(\frac{1/5}{1/5+1/5})^4+(\frac{1/5}{1/5+1/5})^4+(\frac{1/5}{1/5+1/5})^4 = = 3(\frac{1}{2})^4 = 3\frac{1}{16} = \frac{3}{16}$$","Let and be exponentially distributed with , , respectively. How do I calculate ? I have A, B, C and D for 3 cash boxes in a queueing system. Consider D the one waiting. I want to find the probability D is the last one to leave. The service times of the C.Boxes are indep. Exp with mean 5 (same mean for c.box 1, 2 and 3) That's how I'm trying to solve this: P(D last one) = P(T1 > max{T2,T3})*P(T1 < min{T2,T3}) + P(T2 > max{T1,T3})*P(T2 < min{T1,T3}) + P(T3 > max{T1,T2})*P(T3 < min{T1,T2}) And then:","X, Y Z E[X] = 1/\lambda_X E[Y] = 1/\lambda_Y E[Z] = 1/\lambda_Z P(X > \max\{Y,Z\}) (\frac{1/5}{1/5+1/5})^4+(\frac{1/5}{1/5+1/5})^4+(\frac{1/5}{1/5+1/5})^4 = = 3(\frac{1}{2})^4 = 3\frac{1}{16} = \frac{3}{16}","['probability', 'statistics', 'probability-distributions', 'exponential-distribution']"
20,Expected number of tosses to get a head from a coin using integration formulae?,Expected number of tosses to get a head from a coin using integration formulae?,,"I recently started learning Expectation Probability , first of all Any Good resources to study it will be appreciated if any one can share What I have learnt so far the expected  value of some Unknown Variable say $x$ be $ E(x)$ which boils down this equation $ E(x) = \int_{-\infty}^\infty x*P(x) dx $ where P(x) is the Probability of some specific $x$ . So I wanted to find the Expected Number of tosses to Get a  Heads from an Unbiased Coin So I used this equation to solve it $E(Getting Heads) =>  \int_{0}^\infty\dfrac{x}{2^x}  dx$ , **IT IS A PRETTY STANDARD RESULT THAT E(GETTING  HEADS) = 2, But this integral is giving me another answer , which is $\dfrac{1}{\ln^2\left(2\right)}$ Can Anyone tell me where am I going wrong with understanding stuff.","I recently started learning Expectation Probability , first of all Any Good resources to study it will be appreciated if any one can share What I have learnt so far the expected  value of some Unknown Variable say be which boils down this equation where P(x) is the Probability of some specific . So I wanted to find the Expected Number of tosses to Get a  Heads from an Unbiased Coin So I used this equation to solve it , **IT IS A PRETTY STANDARD RESULT THAT E(GETTING  HEADS) = 2, But this integral is giving me another answer , which is Can Anyone tell me where am I going wrong with understanding stuff.",x  E(x)  E(x) = \int_{-\infty}^\infty x*P(x) dx  x E(Getting Heads) =>  \int_{0}^\infty\dfrac{x}{2^x}  dx \dfrac{1}{\ln^2\left(2\right)},"['probability', 'expected-value']"
21,"Throw a coin one million times. What is the expected number of sequences of six tails, if we do not allow overlap?","Throw a coin one million times. What is the expected number of sequences of six tails, if we do not allow overlap?",,"Question itself: Throw a coin one million times. What is the expected number of sequences of six tails, if we do not allow overlap ? I know when overlap is allowed, the answer is (1,000,000-5)/(2^6). Not sure if we can just do (1,000,000-5)/(2^6) divided by 6 if overlap is not allowed? Some clarifications: For example, if part of the sequence is ""one H, nine T, then one H"", we would count 1 sequence of six tails. (When overlap is allowed, we can count three times because each of the first 3 T can start a sequence of six tails; However, this question does not allow overlap, so 9T can only be counted as containing one sequence of six tails) If part of the sequence is ""one H, thirteen T, then one H"", we would count 2 sequences of six tails.","Question itself: Throw a coin one million times. What is the expected number of sequences of six tails, if we do not allow overlap ? I know when overlap is allowed, the answer is (1,000,000-5)/(2^6). Not sure if we can just do (1,000,000-5)/(2^6) divided by 6 if overlap is not allowed? Some clarifications: For example, if part of the sequence is ""one H, nine T, then one H"", we would count 1 sequence of six tails. (When overlap is allowed, we can count three times because each of the first 3 T can start a sequence of six tails; However, this question does not allow overlap, so 9T can only be counted as containing one sequence of six tails) If part of the sequence is ""one H, thirteen T, then one H"", we would count 2 sequences of six tails.",,['probability']
22,Slicing a bar in three pieces - probability,Slicing a bar in three pieces - probability,,I have the following problem: If you split a bar of length 1 in three pieces by choosing two cut points randomly. What is the probability that the lenght of at least one of them is less than 1/5?. I think that $P(a<1/5)=P(1-b<1/5)=P(b-a<1/5)=1/5$ But now I am confused how to use this in the porpouse of compute the total probability. I would bet that 3/5 is the answer (because you either have that a<1/5 or 1-b<1/5 or a-b<1/5) but it is not crystal clear to me.,I have the following problem: If you split a bar of length 1 in three pieces by choosing two cut points randomly. What is the probability that the lenght of at least one of them is less than 1/5?. I think that But now I am confused how to use this in the porpouse of compute the total probability. I would bet that 3/5 is the answer (because you either have that a<1/5 or 1-b<1/5 or a-b<1/5) but it is not crystal clear to me.,P(a<1/5)=P(1-b<1/5)=P(b-a<1/5)=1/5,['probability']
23,Probability that the cards are in AP,Probability that the cards are in AP,,"Box $1$ contains three cards bearing numbers $1, 2, 3$ ; box $2$ contains five cards bearing numbers $1, 2, 3, 4, 5$ ; and box $3$ contains seven cards bearing numbers $1, 2, 3, 4, 5, 6, 7$ . A card is drawn from each of the boxes. Let $x_i$ be the number on the card drawn from the $i$ th box, $i = 1, 2, 3.$ What's the  probability that $x_1 , x_2 , x_3$ are in an arithmetic progression? My attempt : Total cases are: $3\cdot5\cdot7=105$ . Favorable instances are $10$ . $ \{ (1,1,1), (1,2,3), (1,3,5), (1,4,7), (2,2,2), (2,3,4), (2,4,6), (3,3,3), (3,4,5), (3,5,7)\}$ . So, my answer is $\frac{10}{105}$ , but the answer is given as $\frac{11}{105}$ . Which case am I missing?","Box contains three cards bearing numbers ; box contains five cards bearing numbers ; and box contains seven cards bearing numbers . A card is drawn from each of the boxes. Let be the number on the card drawn from the th box, What's the  probability that are in an arithmetic progression? My attempt : Total cases are: . Favorable instances are . . So, my answer is , but the answer is given as . Which case am I missing?","1 1, 2, 3 2 1, 2, 3, 4, 5 3 1, 2, 3, 4, 5, 6, 7 x_i i i = 1, 2, 3. x_1 , x_2 , x_3 3\cdot5\cdot7=105 10  \{ (1,1,1), (1,2,3), (1,3,5), (1,4,7), (2,2,2), (2,3,4), (2,4,6), (3,3,3), (3,4,5), (3,5,7)\} \frac{10}{105} \frac{11}{105}","['probability', 'arithmetic-progressions']"
24,"Why does $\min(X,Y)$ and $|X-Y|$ have the same distribution when $X,Y\sim U(0,1)$?",Why does  and  have the same distribution when ?,"\min(X,Y) |X-Y| X,Y\sim U(0,1)","Say $X,Y \sim U(0,1)$ be two independent uniform random variables, and $T=|X-Y|$ . I would like to find the CDF of $M=\max(X,Y)$ , $L=\min(X,Y)$ , and $T$ . I find the CDF of $M$ and $L$ as $$P(M\leq t ) = P(X\leq t)P(Y \leq t) =t^2$$ $$P(L \leq t) = 1-P(X\geq t ) P(Y\geq t) = 1-(1-t)^2$$ To find the CDF of $T$ , I draw a rectangle with unit length and width, and compute the area within the region $|X-Y| \leq t$ , which turns out to be $1-(1-t)^2$ . My question is how come the CDF of $L$ and $T$ are the same when one is the minimum of two uniform r.vs and the other is the absolute difference of two uniform r.vs? Is there something wrong with my computation? Thanks.","Say be two independent uniform random variables, and . I would like to find the CDF of , , and . I find the CDF of and as To find the CDF of , I draw a rectangle with unit length and width, and compute the area within the region , which turns out to be . My question is how come the CDF of and are the same when one is the minimum of two uniform r.vs and the other is the absolute difference of two uniform r.vs? Is there something wrong with my computation? Thanks.","X,Y \sim U(0,1) T=|X-Y| M=\max(X,Y) L=\min(X,Y) T M L P(M\leq t ) = P(X\leq t)P(Y \leq t) =t^2 P(L \leq t) = 1-P(X\geq t ) P(Y\geq t) = 1-(1-t)^2 T |X-Y| \leq t 1-(1-t)^2 L T","['probability', 'probability-distributions', 'uniform-distribution']"
25,$f\in L^1$ iff $\sum\limits_{i\in\mathbb{N}}2^n\mu(A_n)<\infty$,iff,f\in L^1 \sum\limits_{i\in\mathbb{N}}2^n\mu(A_n)<\infty,"I want to show that the measurable function $f:[0,1]\to\mathbb{R}^+$ is in $L^1([0,1])$ if and only if $$\sum\limits_{i\in\mathbb{N}}2^i\mu(A_i)<\infty$$ for $$A_i:=\{x\in[0,1]\mid 2^i\leq f(x)<2^{i+1}\}.$$ I saw a similar example here but I didn't fully understand what was going on there. If I define the function $$g(x):=\sum_{i\in\mathbb{N}}2^i\chi_{A_i},$$ where $\chi_{A_i}$ is the indicator function, then I basically have to show $$\int_{[0,1]}|f|\,\mathrm{d}\mu<\infty\iff\int_{[0,1]}g\,\mathrm{d}\mu<\infty.$$ I'm not sure how to continue here, but I thought about expressing $f$ in a sum of indicator functions on $A_i$ and then estimating the sum ? Any hints would be appreciated. Thanks :)","I want to show that the measurable function is in if and only if for I saw a similar example here but I didn't fully understand what was going on there. If I define the function where is the indicator function, then I basically have to show I'm not sure how to continue here, but I thought about expressing in a sum of indicator functions on and then estimating the sum ? Any hints would be appreciated. Thanks :)","f:[0,1]\to\mathbb{R}^+ L^1([0,1]) \sum\limits_{i\in\mathbb{N}}2^i\mu(A_i)<\infty A_i:=\{x\in[0,1]\mid 2^i\leq f(x)<2^{i+1}\}. g(x):=\sum_{i\in\mathbb{N}}2^i\chi_{A_i}, \chi_{A_i} \int_{[0,1]}|f|\,\mathrm{d}\mu<\infty\iff\int_{[0,1]}g\,\mathrm{d}\mu<\infty. f A_i","['real-analysis', 'probability', 'measure-theory', 'lebesgue-measure']"
26,"Is it ever possible to ""win"" a 50/50 game with a clear goal in mind.","Is it ever possible to ""win"" a 50/50 game with a clear goal in mind.",,"Lets say I have 750 dollars and want at least 1250 dollars at the end of a 50/50 game where I can bet any possible value. Is there any way in which I can raise my chances of winning? And if there is, how can I define the best initial bet to be doubled? (At first I thought problems like these were easy to solve and there was no possible way to ""win"", but at the same time I have some doubts about it and don't know the mathematical explanation for it.)","Lets say I have 750 dollars and want at least 1250 dollars at the end of a 50/50 game where I can bet any possible value. Is there any way in which I can raise my chances of winning? And if there is, how can I define the best initial bet to be doubled? (At first I thought problems like these were easy to solve and there was no possible way to ""win"", but at the same time I have some doubts about it and don't know the mathematical explanation for it.)",,"['probability', 'binomial-distribution']"
27,Variance of a series of IID's vs a multiple of a random variable,Variance of a series of IID's vs a multiple of a random variable,,"For any random variable, X, Var(aX) = a^2*Var(X), which is easy to demonstrate. Suppose you have a series of IID's, and want to find the variance.  So, in that case for example, Var(X+X+X+X+X) = Var(X) + Var(X) + Var(X) + Var(X) + Var(X) = 5Var(X) since there is no covariance involved.  But isn't Var(X+X+X+X+X) = Var(5X) = 25Var(X)? Follow-up, when doing the variance of a sum of dependent random variables would you add two times every possible pairwise covariance to the individual variances?","For any random variable, X, Var(aX) = a^2*Var(X), which is easy to demonstrate. Suppose you have a series of IID's, and want to find the variance.  So, in that case for example, Var(X+X+X+X+X) = Var(X) + Var(X) + Var(X) + Var(X) + Var(X) = 5Var(X) since there is no covariance involved.  But isn't Var(X+X+X+X+X) = Var(5X) = 25Var(X)? Follow-up, when doing the variance of a sum of dependent random variables would you add two times every possible pairwise covariance to the individual variances?",,['probability']
28,"If $X$ follows an $\operatorname{Exp}(\theta)$, does $1/X$ follow an $\operatorname{Exp}(1/ \theta)$?","If  follows an , does  follow an ?",X \operatorname{Exp}(\theta) 1/X \operatorname{Exp}(1/ \theta),"I heard a teacher say that if $$X \sim \operatorname{Exp}(\theta)$$ then $$\frac{1}{X} \sim \operatorname{Exp}\left(\frac{1}{\theta}\right)$$ I don't trust this teacher because he has given us wrong answers before. I tried to obtain this myself: If $y = g(x) = \frac{1}{x}$ , then $$f_Y(y) = f_X(g^{-1}(y)) \cdot \left|\frac{d}{dy}g^{-1}(y)\right|$$ $$f_Y(y) = \theta \cdot e^{-\theta/y} \cdot \frac{1}{y^2}$$ where $1/y \in (0, +\infty)$ . I don't see an $\operatorname{Exp}(1/\theta)$ here. Am I doing something wrong?","I heard a teacher say that if then I don't trust this teacher because he has given us wrong answers before. I tried to obtain this myself: If , then where . I don't see an here. Am I doing something wrong?","X \sim \operatorname{Exp}(\theta) \frac{1}{X} \sim \operatorname{Exp}\left(\frac{1}{\theta}\right) y = g(x) = \frac{1}{x} f_Y(y) = f_X(g^{-1}(y)) \cdot \left|\frac{d}{dy}g^{-1}(y)\right| f_Y(y) = \theta \cdot e^{-\theta/y} \cdot \frac{1}{y^2} 1/y \in (0, +\infty) \operatorname{Exp}(1/\theta)","['probability', 'probability-distributions', 'exponential-distribution']"
29,Expected number of rolls until all numbers have been rolled an odd number of times (at the same time),Expected number of rolls until all numbers have been rolled an odd number of times (at the same time),,"The title is a reformulation of a game that is played as follows: Before you are six light bulbs (all start turned off). You repeatedly roll a fair six-sided die and each time you roll a number you flip the switch of the corresponding switch, turning it on if it's off, and off if it's on. What is the expected number of rolls it will take until every light bulb is turned on? Note: I am not just asking ""What is the expected number of rolls until each number has been rolled?"", as this question has already been asked and answered many times here.","The title is a reformulation of a game that is played as follows: Before you are six light bulbs (all start turned off). You repeatedly roll a fair six-sided die and each time you roll a number you flip the switch of the corresponding switch, turning it on if it's off, and off if it's on. What is the expected number of rolls it will take until every light bulb is turned on? Note: I am not just asking ""What is the expected number of rolls until each number has been rolled?"", as this question has already been asked and answered many times here.",,"['probability', 'expected-value', 'dice']"
30,Laplace functional for Poisson Process: $E[e^{-\sum_{n=1}^{\infty}f(W_n)}]= e^{-\lambda\int_0^{\infty}(1-e^{-f(t)})dt}$,Laplace functional for Poisson Process:,E[e^{-\sum_{n=1}^{\infty}f(W_n)}]= e^{-\lambda\int_0^{\infty}(1-e^{-f(t)})dt},Let $W_n$ be the $n$ waiting time of a Poisson process. Proof that $$E[e^{-\sum_{n=1}^{\infty}f(W_n)}]= e^{-\lambda\int_0^{\infty}(1-e^{-f(t)})dt}$$ for a measurable postive function $f$ I really don´t know where to start. The first thing that I tried is: $$E[e^{-\sum_{n=1}^{\infty}f(W_n)}]=E[\prod_{n=1}^{\infty}e^{-f(W_n)}]$$ but I don´t know if the last expression is equivalent to $$\prod_{n=1}^{\infty}E[e^{-f(W_n)}]$$ I would really appreciate any hint or suggestion.,Let be the waiting time of a Poisson process. Proof that for a measurable postive function I really don´t know where to start. The first thing that I tried is: but I don´t know if the last expression is equivalent to I would really appreciate any hint or suggestion.,W_n n E[e^{-\sum_{n=1}^{\infty}f(W_n)}]= e^{-\lambda\int_0^{\infty}(1-e^{-f(t)})dt} f E[e^{-\sum_{n=1}^{\infty}f(W_n)}]=E[\prod_{n=1}^{\infty}e^{-f(W_n)}] \prod_{n=1}^{\infty}E[e^{-f(W_n)}],"['probability', 'probability-theory', 'stochastic-processes', 'expected-value', 'poisson-process']"
31,A die is thrown $n$ times. What is the probability that $6$ appears even number of times?,A die is thrown  times. What is the probability that  appears even number of times?,n 6,"A die is thrown $n$ times. What is the probability that $6$ appears even number of times (for the purpose of task $0$ is even number)? The solution from my textbook is: We have two hypotheses $H_1$ that the array starts with 6 and $H_2$ array does not start with 6; $p_n$ is the probability. $p_n=P(H_1)(1-p_{n-1})+P(H_2)p_{n-1}=\frac{1}{6}+\frac{2}{3}p_{n-1}$ . $p_1=\frac{5}{6}$ After solving this recursive relation we get $p_n=\frac{1}{2}(1+(\frac{2}{3})^n)$ . I know how to solve recursion I just don't understand how they got it (I know probabilities of hypothesis). Also, is there any other way to approach this task?","A die is thrown times. What is the probability that appears even number of times (for the purpose of task is even number)? The solution from my textbook is: We have two hypotheses that the array starts with 6 and array does not start with 6; is the probability. . After solving this recursive relation we get . I know how to solve recursion I just don't understand how they got it (I know probabilities of hypothesis). Also, is there any other way to approach this task?",n 6 0 H_1 H_2 p_n p_n=P(H_1)(1-p_{n-1})+P(H_2)p_{n-1}=\frac{1}{6}+\frac{2}{3}p_{n-1} p_1=\frac{5}{6} p_n=\frac{1}{2}(1+(\frac{2}{3})^n),"['probability', 'conditional-probability', 'recursion']"
32,Is P(A and B) = P(B and A)?,Is P(A and B) = P(B and A)?,,I was wondering if the following statement is true and if there are any times in which the following is not true: Is the probability of A and B equal to the probability of B and A? P(A and B) = P(B and A)? Would it be suffice to show that they are equal based on P(A and B) representing the same intersection as P(B and A) on a venn diagram?,I was wondering if the following statement is true and if there are any times in which the following is not true: Is the probability of A and B equal to the probability of B and A? P(A and B) = P(B and A)? Would it be suffice to show that they are equal based on P(A and B) representing the same intersection as P(B and A) on a venn diagram?,,['probability']
33,"If I stop flipping when I've reached equal heads and tails, what's the chance I never stop? [duplicate]","If I stop flipping when I've reached equal heads and tails, what's the chance I never stop? [duplicate]",,This question already has answers here : Proving that $1$- and $2D$ simple symmetric random walks return to the origin with probability $1$ (10 answers) Closed 4 years ago . I toss a balanced coin until the number of heads I get equals the number of tails? What's the chance I never stop? I have tried considering the reverse event and a recursive reasoning but nothing conclusive. A close question has already been asked here but I don't know the Markov formalism that is used.,This question already has answers here : Proving that $1$- and $2D$ simple symmetric random walks return to the origin with probability $1$ (10 answers) Closed 4 years ago . I toss a balanced coin until the number of heads I get equals the number of tails? What's the chance I never stop? I have tried considering the reverse event and a recursive reasoning but nothing conclusive. A close question has already been asked here but I don't know the Markov formalism that is used.,,['probability']
34,Suppose we draw two cards without replacement out of a standard deck of 52 cards,Suppose we draw two cards without replacement out of a standard deck of 52 cards,,"Suppose we draw two cards without replacement out of a standard deck of 52 cards, while each time a card is drawn randomly with the (remaining) cards well-shuffled. Let A be the event that the first card is an Ace. and B be the event that the second card is a spade. Find out if A and B are independent. My attempt:  Intuitively, of course they are not independent because P(B|A)=12/51 if A is the Ace of Spades, but P(B|A)=13/51 if A is not the Ace of Spades. But, teacher wants us to show it systematically, i.e. show $P(A)*P(B)$ is not equal to $P(A)intersectP(B)$ P(A)=1/13 P(B)=51/204 by Total probability theorem How do I find P(AnB)? I know the formula, but P(B|A) takes on two values depending on A.... which is where I am confused.","Suppose we draw two cards without replacement out of a standard deck of 52 cards, while each time a card is drawn randomly with the (remaining) cards well-shuffled. Let A be the event that the first card is an Ace. and B be the event that the second card is a spade. Find out if A and B are independent. My attempt:  Intuitively, of course they are not independent because P(B|A)=12/51 if A is the Ace of Spades, but P(B|A)=13/51 if A is not the Ace of Spades. But, teacher wants us to show it systematically, i.e. show is not equal to P(A)=1/13 P(B)=51/204 by Total probability theorem How do I find P(AnB)? I know the formula, but P(B|A) takes on two values depending on A.... which is where I am confused.",P(A)*P(B) P(A)intersectP(B),['probability']
35,Extraction of elements from a box,Extraction of elements from a box,,"I have a box with $n$ distinct elements and i need to do $n$ extractions with reposition. Let $N$ be the number of different elements that i found through the process I should try to find ${P}[N=k]$ for $k$ ranging between $1$ and $n$ . Here is my work. Let's take $\Omega$ ={ $(x_{1},...,x_{n})\in R^{n}; x_{i}\in {1,...,n}$ }. I have that $\#\Omega$ = $n^{n}$ .  Now I have problem calculating the cardinality of string with exactly $N$ elements. I need this as I would like to express my probability as a quotient of favorable cases over possible cases. I guess that I should start with $\binom{n}{N}$ choosing the different collection of N elements that appear. Now i have to calculate how many different string of n elements I can get knowing that N and only N elements show up. I was trying with something such as counting surjective functions from $n\to N$ . To so i tried do choose N elements from n, permutating them (it should show all the possible function) and letting the n-N elements range randomly. The problem is that my reasoning is wrong as i am clearly counting multiple times the same string, any help? Thanks.","I have a box with distinct elements and i need to do extractions with reposition. Let be the number of different elements that i found through the process I should try to find for ranging between and . Here is my work. Let's take ={ }. I have that = .  Now I have problem calculating the cardinality of string with exactly elements. I need this as I would like to express my probability as a quotient of favorable cases over possible cases. I guess that I should start with choosing the different collection of N elements that appear. Now i have to calculate how many different string of n elements I can get knowing that N and only N elements show up. I was trying with something such as counting surjective functions from . To so i tried do choose N elements from n, permutating them (it should show all the possible function) and letting the n-N elements range randomly. The problem is that my reasoning is wrong as i am clearly counting multiple times the same string, any help? Thanks.","n n N {P}[N=k] k 1 n \Omega (x_{1},...,x_{n})\in R^{n}; x_{i}\in
{1,...,n} \#\Omega n^{n} N \binom{n}{N} n\to N","['probability', 'combinatorics']"
36,Triangle area from uniformly distributed points along a line,Triangle area from uniformly distributed points along a line,,3 random numbers from independent uniform distributions between 0 and 1 are selected. How to calculate the expected triangle area if the points are arranged as in the sketch?,3 random numbers from independent uniform distributions between 0 and 1 are selected. How to calculate the expected triangle area if the points are arranged as in the sketch?,,"['probability', 'triangles', 'geometric-probability']"
37,"What is $P(B_1 > 0, B_2 > 0)$ , where $B_t$ is a Brownian Motion at time $t$?","What is  , where  is a Brownian Motion at time ?","P(B_1 > 0, B_2 > 0) B_t t","The following question is found from this MSE post . For completeness, I restate the problem below. Question:  What is $P(B_1 > 0, B_2 > 0)$ , where $B_t$ is a Brownian Motion at time $t$ ? From that post, the OP calculates the probability as follows: $P(B_1 > 0, B_2 > 0) = P(B_1 > 0, B_2 - B_1 > -B_1) = P(Z_1 > 0, Z_2 > -Z_1) = \frac{3}{8}$ by applying a symmetry argument to the $(Z_1, Z_2) \sim N(0, I_2)$ distribution. I can understand all equalities except the last one which leads to the answer $\frac{3}{8}.$ In other words, I do not understand how symmetry argument to bivariate normal distribution is at play here.","The following question is found from this MSE post . For completeness, I restate the problem below. Question:  What is , where is a Brownian Motion at time ? From that post, the OP calculates the probability as follows: by applying a symmetry argument to the distribution. I can understand all equalities except the last one which leads to the answer In other words, I do not understand how symmetry argument to bivariate normal distribution is at play here.","P(B_1 > 0, B_2 > 0) B_t t P(B_1 > 0, B_2 > 0) = P(B_1 > 0, B_2 - B_1 > -B_1) = P(Z_1 > 0, Z_2 > -Z_1) = \frac{3}{8} (Z_1, Z_2) \sim N(0, I_2) \frac{3}{8}.","['probability', 'probability-theory', 'normal-distribution', 'stochastic-calculus', 'brownian-motion']"
38,"""The Egg:"" Bizarre behavior of the roots of a family of polynomials.","""The Egg:"" Bizarre behavior of the roots of a family of polynomials.",,"In this MO post , I ran into the following family of polynomials: $$f_n(x)=\sum_{m=0}^{n}\prod_{k=0}^{m-1}\frac{x^n-x^k}{x^m-x^k}.$$ In the context of the post, $x$ was a prime number, and $f_n(x)$ counted the number of subspaces of an $n$ -dimensional vector space over $GF(x)$ (which I was using to determine the number of subgroups of an elementary abelian group $E_{x^n}$ ). Anyway, while I was investigating asymptotic behavior of $f_n(x)$ in Mathematica, I got sidetracked and (just for fun) looked at the set of complex roots when I set $f_n(x)=0$ .  For $n=24$ , the plot looked like this: (The real and imaginary axes are from $-1$ to $1$ .) Surprised by the unusual symmetry of the solutions, I made the same plot for a few more values of $n$ .  Note the clearly defined ""tails"" (on the left when even, top and bottom when odd) and ""cusps"" (both sides). You can see that after approximately $n=60$ , the ""circle"" of solutions starts to expand into a band of solutions with a defined outline.  To fully absorb the weirdness of this, I animated the solutions from $n=2$ to $n=112$ . The following is the result: Pretty weird, right!?  Anyhow, here are my questions: First, has anybody ever seen anything at all like this before? What's up with those ""tails?""  They seem to occur only on even $n$ , and they are surely distinguishable from the rest of the solutions. Look how the ""enclosed"" solutions rotate as $n$ increases.  Why does this happen? [Explained in edits.] Anybody have any idea what happens to the solution set as $n\rightarrow \infty$ ? Thanks to @WillSawin, we now know that all the roots are contained in an annulus that converges to the unit circle , which is fantastic.  So, the final step in understanding the limit of the solution sets is figuring out what happens on the unit circle.  We can see from the animation that there are many gaps, particularly around certain roots of unity; however, they do appear to be closing. The natural question is, which points on the unit circle ""are roots in the limit""? In other words, what are the accumulation points of $\{z\left|z\right|^{-1}:z\in\mathbb{C}\text{ and }f_n(z)=0\}$ ? Is the set of accumulation points dense?  @NoahSnyder's heuristic of considering these as a random family of polynomials suggests it should be- at least, almost surely. These are polynomials in $\mathbb{Z}[x]$ .  Can anybody think of a way to rewrite the formula (perhaps recursively?) for the simplified polynomial, with no denominator?  If so, we could use the new formula to prove the series converges to a function on the unit disc, as well as cut computation time in half. [See edits for progress.] Does anybody know a numerical method specifically for finding roots of high degree polynomials?  Or any other way to efficiently compute solution sets for high $n$ ? [Thanks @Hooked!] Thanks everyone.  This may not turn out to be particularly mathematically profound, but it sure is neat . EDIT : Thanks to suggestions in the comments, I cranked up the working precision to maximum and recalculated the animation.  As Hurkyl and mercio suspected, the rotation was indeed a software artifact, and in fact evidently so was the thickening of the solution set.  The new animation looks like this: So, that solves one mystery: the rotation and inflation were caused by tiny roundoff errors in the computation.  With the image clearer, however, I see the behavior of the cusps more clearly.  Is there an explanation for the gradual accumulation of ""cusps"" around the roots of unity?  (Especially 1.) EDIT : Here is an animation $Arg(f_n)$ up to $n=30$ .  I think we can see from this that $f_n$ should converge to some function on the unit disk as $n\rightarrow \infty$ .  I'd love to include higher $n$ , but this was already rather computationally exhausting. Now, I've been tinkering and I may be onto something with respect to point $5$ (i.e. seeking a better formula for $f_n(x)$ ).  The folowing claims aren't proven yet, but I've checked each up to $n=100$ , and they seem inductively consistent.  Here denote $\displaystyle f_n(x)=\sum_{m}a_{n,m}x^m$ , so that $a_{n,m}\in \mathbb{Z}$ are the coefficients in the simplified expansion of $f_n(x)$ . First, I found $\text{deg}(f_n)=\text{deg}(f_{n-1})+\lfloor \frac{n}{2} \rfloor$ .  The solution to this recurrence relation is $$\text{deg}(f_n)=\frac{1}{2}\left({\left\lceil\frac{1-n}{2}\right\rceil}^2 -\left\lceil\frac{1-n}{2}\right\rceil+{\left\lfloor \frac{n}{2} \right\rfloor}^2 + \left\lfloor \frac{n}{2} \right\rfloor\right)=\left\lceil\frac{n^2}{4}\right\rceil.$$ If $f_n(x)$ has $r$ more coefficients than $f_{n-1}(x)$ , the leading $r$ coefficients are the same as the leading $r$ coefficients of $f_{n-2}(x)$ , pairwise. When $n>m$ , $a_{n,m}=a_{n-1,m}+\rho(m)$ , where $\rho(m)$ is the number of integer partitions of $m$ .  (This comes from observation, but I bet an actual proof could follow from some of the formulas here .)  For $n\leq m$ the $\rho(m)$ formula first fails at $n=m=6$ , and not before for some reason.  There is probably a simple correction term I'm not seeing - and whatever that term is, I bet it's what's causing those cusps. Anyhow, with this, we can make almost make a recursive relation for $a_{n,m}$ , $$a_{n,m}= \left\{      \begin{array}{ll}        a_{n-2,m+\left\lceil\frac{n-2}{2}\right\rceil^2-\left\lceil\frac{n}{2}\right\rceil^2} & : \text{deg}(f_{n-1}) < m \leq \text{deg}(f_n)\\        a_{n-1,m}+\rho(m) & : m \leq \text{deg}(f_{n-1})  \text{ and } n > m \\        ? & : m \leq \text{deg}(f_{n-1})  \text{ and } n \leq m      \end{array}    \right. $$ but I can't figure out the last part yet. EDIT : Someone pointed out to me that if we write $\lim_{n\rightarrow\infty}f_n(x)=\sum_{m=0}^\infty b_{m} x^m$ , then it appears that $f_n(x)=\sum_{m=0}^n b_m x^m + O(x^{n+1})$ .  The $b_m$ there seem to me to be relatively well approximated by the $\rho(m)$ formula, considering the correction term only applies for a finite number of recursions. So, if we have the coefficients up to an order of $O(x^{n+1})$ , we can at least prove the polynomials converge on the open unit disk, which the $Arg$ animation suggests is true.  (To be precise, it looks like $f_{2n}$ and $f_{2n+1}$ may have different limit functions, but I suspect the coefficients of both sequences will come from the same recursive formula.)  With this in mind, I put a bounty up for the correction term, since from that all the behavior will probably be explained. EDIT : The limit function proposed by Gottfriend and Aleks has the formal expression $$\lim_{n\rightarrow \infty}f_n(x)=1+\prod_{m=1}^\infty \frac{1}{1-x^m}.$$ I made an $Arg$ plot of $1+\prod_{m=1}^r \frac{1}{1-x^m}$ for up to $r=24$ to see if I could figure out what that ought to ultimately end up looking like, and came up with this: Purely based off the plots, it seems not entirely unlikely that $f_n(x)$ is going to the same place this is, at least inside the unit disc.  Now the question is, how do we determine the solution set at the limit?  I speculate that the unit circle may become a dense combination of zeroes and singularities, with fractal-like concentric ""circles of singularity"" around the roots of unity...  :)","In this MO post , I ran into the following family of polynomials: In the context of the post, was a prime number, and counted the number of subspaces of an -dimensional vector space over (which I was using to determine the number of subgroups of an elementary abelian group ). Anyway, while I was investigating asymptotic behavior of in Mathematica, I got sidetracked and (just for fun) looked at the set of complex roots when I set .  For , the plot looked like this: (The real and imaginary axes are from to .) Surprised by the unusual symmetry of the solutions, I made the same plot for a few more values of .  Note the clearly defined ""tails"" (on the left when even, top and bottom when odd) and ""cusps"" (both sides). You can see that after approximately , the ""circle"" of solutions starts to expand into a band of solutions with a defined outline.  To fully absorb the weirdness of this, I animated the solutions from to . The following is the result: Pretty weird, right!?  Anyhow, here are my questions: First, has anybody ever seen anything at all like this before? What's up with those ""tails?""  They seem to occur only on even , and they are surely distinguishable from the rest of the solutions. Look how the ""enclosed"" solutions rotate as increases.  Why does this happen? [Explained in edits.] Anybody have any idea what happens to the solution set as ? Thanks to @WillSawin, we now know that all the roots are contained in an annulus that converges to the unit circle , which is fantastic.  So, the final step in understanding the limit of the solution sets is figuring out what happens on the unit circle.  We can see from the animation that there are many gaps, particularly around certain roots of unity; however, they do appear to be closing. The natural question is, which points on the unit circle ""are roots in the limit""? In other words, what are the accumulation points of ? Is the set of accumulation points dense?  @NoahSnyder's heuristic of considering these as a random family of polynomials suggests it should be- at least, almost surely. These are polynomials in .  Can anybody think of a way to rewrite the formula (perhaps recursively?) for the simplified polynomial, with no denominator?  If so, we could use the new formula to prove the series converges to a function on the unit disc, as well as cut computation time in half. [See edits for progress.] Does anybody know a numerical method specifically for finding roots of high degree polynomials?  Or any other way to efficiently compute solution sets for high ? [Thanks @Hooked!] Thanks everyone.  This may not turn out to be particularly mathematically profound, but it sure is neat . EDIT : Thanks to suggestions in the comments, I cranked up the working precision to maximum and recalculated the animation.  As Hurkyl and mercio suspected, the rotation was indeed a software artifact, and in fact evidently so was the thickening of the solution set.  The new animation looks like this: So, that solves one mystery: the rotation and inflation were caused by tiny roundoff errors in the computation.  With the image clearer, however, I see the behavior of the cusps more clearly.  Is there an explanation for the gradual accumulation of ""cusps"" around the roots of unity?  (Especially 1.) EDIT : Here is an animation up to .  I think we can see from this that should converge to some function on the unit disk as .  I'd love to include higher , but this was already rather computationally exhausting. Now, I've been tinkering and I may be onto something with respect to point (i.e. seeking a better formula for ).  The folowing claims aren't proven yet, but I've checked each up to , and they seem inductively consistent.  Here denote , so that are the coefficients in the simplified expansion of . First, I found .  The solution to this recurrence relation is If has more coefficients than , the leading coefficients are the same as the leading coefficients of , pairwise. When , , where is the number of integer partitions of .  (This comes from observation, but I bet an actual proof could follow from some of the formulas here .)  For the formula first fails at , and not before for some reason.  There is probably a simple correction term I'm not seeing - and whatever that term is, I bet it's what's causing those cusps. Anyhow, with this, we can make almost make a recursive relation for , but I can't figure out the last part yet. EDIT : Someone pointed out to me that if we write , then it appears that .  The there seem to me to be relatively well approximated by the formula, considering the correction term only applies for a finite number of recursions. So, if we have the coefficients up to an order of , we can at least prove the polynomials converge on the open unit disk, which the animation suggests is true.  (To be precise, it looks like and may have different limit functions, but I suspect the coefficients of both sequences will come from the same recursive formula.)  With this in mind, I put a bounty up for the correction term, since from that all the behavior will probably be explained. EDIT : The limit function proposed by Gottfriend and Aleks has the formal expression I made an plot of for up to to see if I could figure out what that ought to ultimately end up looking like, and came up with this: Purely based off the plots, it seems not entirely unlikely that is going to the same place this is, at least inside the unit disc.  Now the question is, how do we determine the solution set at the limit?  I speculate that the unit circle may become a dense combination of zeroes and singularities, with fractal-like concentric ""circles of singularity"" around the roots of unity...  :)","f_n(x)=\sum_{m=0}^{n}\prod_{k=0}^{m-1}\frac{x^n-x^k}{x^m-x^k}. x f_n(x) n GF(x) E_{x^n} f_n(x) f_n(x)=0 n=24 -1 1 n n=60 n=2 n=112 n n n\rightarrow \infty \{z\left|z\right|^{-1}:z\in\mathbb{C}\text{ and }f_n(z)=0\} \mathbb{Z}[x] n Arg(f_n) n=30 f_n n\rightarrow \infty n 5 f_n(x) n=100 \displaystyle f_n(x)=\sum_{m}a_{n,m}x^m a_{n,m}\in \mathbb{Z} f_n(x) \text{deg}(f_n)=\text{deg}(f_{n-1})+\lfloor \frac{n}{2} \rfloor \text{deg}(f_n)=\frac{1}{2}\left({\left\lceil\frac{1-n}{2}\right\rceil}^2 -\left\lceil\frac{1-n}{2}\right\rceil+{\left\lfloor \frac{n}{2} \right\rfloor}^2 + \left\lfloor \frac{n}{2} \right\rfloor\right)=\left\lceil\frac{n^2}{4}\right\rceil. f_n(x) r f_{n-1}(x) r r f_{n-2}(x) n>m a_{n,m}=a_{n-1,m}+\rho(m) \rho(m) m n\leq m \rho(m) n=m=6 a_{n,m} a_{n,m}= \left\{
     \begin{array}{ll}
       a_{n-2,m+\left\lceil\frac{n-2}{2}\right\rceil^2-\left\lceil\frac{n}{2}\right\rceil^2} & : \text{deg}(f_{n-1}) < m \leq \text{deg}(f_n)\\
       a_{n-1,m}+\rho(m) & : m \leq \text{deg}(f_{n-1})  \text{ and } n > m \\
       ? & : m \leq \text{deg}(f_{n-1})  \text{ and } n \leq m
     \end{array}
   \right.
 \lim_{n\rightarrow\infty}f_n(x)=\sum_{m=0}^\infty b_{m} x^m f_n(x)=\sum_{m=0}^n b_m x^m + O(x^{n+1}) b_m \rho(m) O(x^{n+1}) Arg f_{2n} f_{2n+1} \lim_{n\rightarrow \infty}f_n(x)=1+\prod_{m=1}^\infty \frac{1}{1-x^m}. Arg 1+\prod_{m=1}^r \frac{1}{1-x^m} r=24 f_n(x)","['abstract-algebra', 'complex-analysis', 'algebraic-geometry', 'numerical-methods', 'recreational-mathematics']"
39,Nice examples of groups which are not obviously groups,Nice examples of groups which are not obviously groups,,"I am searching for some groups, where it is not so obvious that they are groups. In the lecture's script there are only examples like $\mathbb{Z}$ under addition and other things like that. I don't think that these examples are helpful to understand the real properties of a group, when only looking to such trivial examples. I am searching for some more exotic examples, like the power set of a set together with the symmetric difference, or an elliptic curve with its group law.","I am searching for some groups, where it is not so obvious that they are groups. In the lecture's script there are only examples like under addition and other things like that. I don't think that these examples are helpful to understand the real properties of a group, when only looking to such trivial examples. I am searching for some more exotic examples, like the power set of a set together with the symmetric difference, or an elliptic curve with its group law.",\mathbb{Z},"['abstract-algebra', 'group-theory', 'examples-counterexamples', 'big-list']"
40,Why are rings called rings?,Why are rings called rings?,,I've done some search in Internet and other sources about this question. Why the name ring to this particular object? Just curiosity. Thanks.,I've done some search in Internet and other sources about this question. Why the name ring to this particular object? Just curiosity. Thanks.,,"['abstract-algebra', 'ring-theory', 'terminology', 'math-history']"
41,How do I sell out with abstract algebra?,How do I sell out with abstract algebra?,,"My plan as an undergraduate was unequivocally to be a pure mathematician, working as an algebraist as a bigshot professor at a bigshot university.  I'm graduating this month, and I didn't get into where I expected to get into.  My letters were great and I'm published, but my GRE was bad and my grades were good but not perfect.  My current plan, I guess, is to start a PhD program at my backup school, and then reapply to the better schools next year. Reality is starting to hit, though, and I'm starting to think about ""selling out.""   I would still love to work in algebra, but I'm not as in love with the Ivory Tower as I was a few years ago, and I don't want to give up my entire life for it.  If the institution isn't going to let me do what I wanted to do, or if I'll never be as talented as I wanted to be, it isn't worth the sacrifice.  In other words, I'd rather be a well-paid applied mathematician in industry than a poor, mediocre pure mathematician at a low-end university. The problem is it seems that most of the applied jobs out there are all about analysis / continuous mathematics, and I am firmly in the algebra / discrete camp. I really do not want to spend my life-solving fluid flow PDEs. I always hear about cryptography as an ""applied algebra"" job, but I'm not particularly crazy about working for the NSA or a telecom (plus crypto can't be the only option). I read some of the answers from Can I use my powers for good? but it's not clear to me which of these suggestions value algebraic thinking.  Many seem very quantitative , rather than structural - is it possible to avoid this in the industry?  Also, I have a lot of debt from a long undergraduate career across several majors, so ""how much"" is unfortunately also a concern.  I don't want to sell out cheap. Are there applied math jobs in industry which focus on structural mathematics reminiscent of abstract algebra, earn an appreciably high salary, and aren't cryptography? How would one best go about pursuing these jobs starting as a recent graduate / first-year graduate student?","My plan as an undergraduate was unequivocally to be a pure mathematician, working as an algebraist as a bigshot professor at a bigshot university.  I'm graduating this month, and I didn't get into where I expected to get into.  My letters were great and I'm published, but my GRE was bad and my grades were good but not perfect.  My current plan, I guess, is to start a PhD program at my backup school, and then reapply to the better schools next year. Reality is starting to hit, though, and I'm starting to think about ""selling out.""   I would still love to work in algebra, but I'm not as in love with the Ivory Tower as I was a few years ago, and I don't want to give up my entire life for it.  If the institution isn't going to let me do what I wanted to do, or if I'll never be as talented as I wanted to be, it isn't worth the sacrifice.  In other words, I'd rather be a well-paid applied mathematician in industry than a poor, mediocre pure mathematician at a low-end university. The problem is it seems that most of the applied jobs out there are all about analysis / continuous mathematics, and I am firmly in the algebra / discrete camp. I really do not want to spend my life-solving fluid flow PDEs. I always hear about cryptography as an ""applied algebra"" job, but I'm not particularly crazy about working for the NSA or a telecom (plus crypto can't be the only option). I read some of the answers from Can I use my powers for good? but it's not clear to me which of these suggestions value algebraic thinking.  Many seem very quantitative , rather than structural - is it possible to avoid this in the industry?  Also, I have a lot of debt from a long undergraduate career across several majors, so ""how much"" is unfortunately also a concern.  I don't want to sell out cheap. Are there applied math jobs in industry which focus on structural mathematics reminiscent of abstract algebra, earn an appreciably high salary, and aren't cryptography? How would one best go about pursuing these jobs starting as a recent graduate / first-year graduate student?",,"['abstract-algebra', 'soft-question', 'applications', 'advice', 'career-development']"
42,Do we have negative prime numbers?,Do we have negative prime numbers?,,"Do we have negative prime numbers? $..., -7, -5, -3, -2, ...$","Do we have negative prime numbers? $..., -7, -5, -3, -2, ...$",,"['abstract-algebra', 'elementary-number-theory']"
43,Can we ascertain that there exists an epimorphism $G\rightarrow H$?,Can we ascertain that there exists an epimorphism ?,G\rightarrow H,"Let $G,H$ be finite groups. Suppose we have an epimorphism $$G\times G\rightarrow H\times H$$  Can we find an epimorphism $G\rightarrow H$?","Let $G,H$ be finite groups. Suppose we have an epimorphism $$G\times G\rightarrow H\times H$$  Can we find an epimorphism $G\rightarrow H$?",,"['group-theory', 'finite-groups', 'abstract-algebra']"
44,An Introduction to Tensors,An Introduction to Tensors,,"As a physics student, I've come across mathematical objects called tensors in several different contexts. Perhaps confusingly, I've also been given both the mathematician's and physicist's definition, which I believe are slightly different. I currently think of them in the following ways, but have a tough time reconciling the different views: An extension/abstraction of scalars, vectors, and matrices in mathematics. A multi-dimensional array of elements. A mapping between vector spaces that represents a co-ordinate independent transformation. In fact, I'm not even sure how correct these three definitions are. Is there a particularly relevant (rigorous, even) definition of tensors and their uses, that might be suitable for a mathematical physicist? Direct answers/explanations, as well as links to good introductory articles, would be much appreciated.","As a physics student, I've come across mathematical objects called tensors in several different contexts. Perhaps confusingly, I've also been given both the mathematician's and physicist's definition, which I believe are slightly different. I currently think of them in the following ways, but have a tough time reconciling the different views: An extension/abstraction of scalars, vectors, and matrices in mathematics. A multi-dimensional array of elements. A mapping between vector spaces that represents a co-ordinate independent transformation. In fact, I'm not even sure how correct these three definitions are. Is there a particularly relevant (rigorous, even) definition of tensors and their uses, that might be suitable for a mathematical physicist? Direct answers/explanations, as well as links to good introductory articles, would be much appreciated.",,"['abstract-algebra', 'tensors']"
45,Does every ring of integers sit inside a ring of integers that has a power basis?,Does every ring of integers sit inside a ring of integers that has a power basis?,,"Given a finite extension of the rationals, $K$ , we know that $K=\mathbb{Q}[\alpha]$ by the primitive element theorem, so every $x \in K$ has the form $$x = a_0 + a_1 \alpha + \cdots + a_n \alpha^n,$$ with $a_i \in \mathbb{Q}$ . However, the ring of integers, $\mathcal{O}_K$ , of $K$ need not have a basis over $\mathbb{Z}$ which consists of $1$ and powers of a single element (a power basis). In fact, there exist number fields which require an arbitrarily large number of elements to form such a basis. Question: Can every ring of integers $\mathcal{O}_K$ that does not have a power basis be extended to a ring of integers $\mathcal{O}_L$ which does have a power basis, for some finite $L/K$ ?","Given a finite extension of the rationals, , we know that by the primitive element theorem, so every has the form with . However, the ring of integers, , of need not have a basis over which consists of and powers of a single element (a power basis). In fact, there exist number fields which require an arbitrarily large number of elements to form such a basis. Question: Can every ring of integers that does not have a power basis be extended to a ring of integers which does have a power basis, for some finite ?","K K=\mathbb{Q}[\alpha] x \in K x = a_0 + a_1 \alpha + \cdots + a_n \alpha^n, a_i \in \mathbb{Q} \mathcal{O}_K K \mathbb{Z} 1 \mathcal{O}_K \mathcal{O}_L L/K","['abstract-algebra', 'number-theory', 'ring-theory', 'algebraic-number-theory']"
46,"What are the differences between rings, groups, and fields?","What are the differences between rings, groups, and fields?",,"Rings, groups, and fields all feel similar. What are the differences between them, both in definition and in how they are used?","Rings, groups, and fields all feel similar. What are the differences between them, both in definition and in how they are used?",,"['terminology', 'abstract-algebra']"
47,The square roots of different primes are linearly independent over the field of rationals,The square roots of different primes are linearly independent over the field of rationals,,"I need to find a way of proving that the square roots of a finite set    of different primes are linearly independent over the field of    rationals. I've tried to solve the problem using elementary algebra  and also using the theory of field extensions, without success. To  prove linear independence of two primes is easy but then my problems  arise. I would be very thankful for an answer to this question.","I need to find a way of proving that the square roots of a finite set    of different primes are linearly independent over the field of    rationals. I've tried to solve the problem using elementary algebra  and also using the theory of field extensions, without success. To  prove linear independence of two primes is easy but then my problems  arise. I would be very thankful for an answer to this question.",,"['abstract-algebra', 'number-theory', 'prime-numbers', 'field-theory']"
48,Example of infinite field of characteristic $p\neq 0$,Example of infinite field of characteristic,p\neq 0,Can you give me an example of infinite field of characteristic $p\neq0$? Thanks.,Can you give me an example of infinite field of characteristic $p\neq0$? Thanks.,,"['abstract-algebra', 'field-theory', 'examples-counterexamples', 'positive-characteristic']"
49,Classification of prime ideals of $\mathbb{Z}[X]$,Classification of prime ideals of,\mathbb{Z}[X],"Let $\mathbb{Z}[X]$ be the ring of polynomials in one variable over $\Bbb Z$ . My question: Is every prime ideal of $\mathbb{Z}[X]$ one of following types? If yes, how would you prove this? $(0)$ . $(f(X))$ , where $f(X)$ is an irreducible polynomial. $(p)$ , where $p$ is a prime number. $(p, f(X))$ , where $p$ is a prime number and $f(X)$ is an irreducible polynomial modulo $p$ .","Let be the ring of polynomials in one variable over . My question: Is every prime ideal of one of following types? If yes, how would you prove this? . , where is an irreducible polynomial. , where is a prime number. , where is a prime number and is an irreducible polynomial modulo .","\mathbb{Z}[X] \Bbb Z \mathbb{Z}[X] (0) (f(X)) f(X) (p) p (p, f(X)) p f(X) p","['abstract-algebra', 'commutative-algebra', 'ideals', 'maximal-and-prime-ideals']"
50,Why is negative times negative = positive?,Why is negative times negative = positive?,,"Someone recently asked me why a negative $\times$ a negative is positive, and why a negative $\times$ a positive is negative, etc. I went ahead and gave them a proof by contradiction like this: Assume $(-x) \cdot (-y) = -xy$ Then divide both sides by $(-x)$ and you get $(-y) = y$ Since we have a contradiction, then our first assumption must be incorrect. I'm guessing I did something wrong here. Since the conclusion of $(-x) \cdot (-y) = (xy)$ is hard to derive from what I wrote. Is there a better way to explain this?  Is my proof incorrect?  Also, what would be an intuitive way to explain the negation concept, if there is one?","Someone recently asked me why a negative a negative is positive, and why a negative a positive is negative, etc. I went ahead and gave them a proof by contradiction like this: Assume Then divide both sides by and you get Since we have a contradiction, then our first assumption must be incorrect. I'm guessing I did something wrong here. Since the conclusion of is hard to derive from what I wrote. Is there a better way to explain this?  Is my proof incorrect?  Also, what would be an intuitive way to explain the negation concept, if there is one?",\times \times (-x) \cdot (-y) = -xy (-x) (-y) = y (-x) \cdot (-y) = (xy),"['abstract-algebra', 'algebra-precalculus', 'arithmetic', 'education', 'faq']"
51,Does $R[x] \cong S[x]$ imply $R \cong S$?,Does  imply ?,R[x] \cong S[x] R \cong S,"This is a very simple question but I believe it's nontrivial. I would like to know if the following is true: If $R$ and $S$ are rings and $R[x]$ and $S[x]$ are isomorphic as rings, then $R$ and $S$ are isomorphic. Thanks! If there isn't a proof (or disproof) of the general result, I would be interested to know if there are particular cases when this claim is true.","This is a very simple question but I believe it's nontrivial. I would like to know if the following is true: If $R$ and $S$ are rings and $R[x]$ and $S[x]$ are isomorphic as rings, then $R$ and $S$ are isomorphic. Thanks! If there isn't a proof (or disproof) of the general result, I would be interested to know if there are particular cases when this claim is true.",,"['abstract-algebra', 'polynomials', 'ring-theory', 'commutative-algebra', 'examples-counterexamples']"
52,How to find the Galois group of a polynomial?,How to find the Galois group of a polynomial?,,"I've been learning about Galois theory recently on my own, and I've been trying to solve tests from my university. Even though I understand all the theorems, I  seem to be having some trouble with the technical stuff. A specific example would be how to find the Galois group of a given polynomial. I know some tricks, and I manage to solve some of those questions, but some not. For example, one of the tests asks to find the Galois group of $x^{4}-4x+2$. I can see that it is irreducible over $\mathbb Q$ (Eisenstein), but I have no clue as to how to find its Galois group over $\mathbb Q$. Can someone tell me how to do this? General techniques concerning this sort of problems are also welcome :). Thanks!","I've been learning about Galois theory recently on my own, and I've been trying to solve tests from my university. Even though I understand all the theorems, I  seem to be having some trouble with the technical stuff. A specific example would be how to find the Galois group of a given polynomial. I know some tricks, and I manage to solve some of those questions, but some not. For example, one of the tests asks to find the Galois group of $x^{4}-4x+2$. I can see that it is irreducible over $\mathbb Q$ (Eisenstein), but I have no clue as to how to find its Galois group over $\mathbb Q$. Can someone tell me how to do this? General techniques concerning this sort of problems are also welcome :). Thanks!",,"['abstract-algebra', 'field-theory', 'galois-theory']"
53,Why is $1$ not a prime number?,Why is  not a prime number?,1,"Why is $1$ not considered a prime number? Or, why is the definition of prime numbers given for integers greater than $1$?","Why is $1$ not considered a prime number? Or, why is the definition of prime numbers given for integers greater than $1$?",,"['abstract-algebra', 'elementary-number-theory', 'ring-theory', 'prime-numbers', 'terminology']"
54,Why are the solutions of polynomial equations so unconstrained over the quaternions?,Why are the solutions of polynomial equations so unconstrained over the quaternions?,,"An $n$th-degree polynomial has at most $n$ distinct zeroes in the complex numbers.  But it may have an uncountable set of zeroes in the quaternions.  For example, $x^2+1$ has two zeroes in $\mathbb C$, but in $\mathbb H$, ${\bf i}\cos x + {\bf j}\sin x$ is a distinct zero of this polynomial for every $x$ in $[0, 2\pi)$, and obviously there are many other zeroes. What is it about $\mathbb H$ that makes its behavior in this regard to be so different from the behavior of $\mathbb R$ and $\mathbb C$?  Is it simply because $\mathbb H$ is four-dimensional rather than two-dimensional? Are there any theorems that say when a ring will behave like $\mathbb H$ and when it will behave like $\mathbb C$? Do all polynomials behave like this in $\mathbb H$? Or is this one unusual?","An $n$th-degree polynomial has at most $n$ distinct zeroes in the complex numbers.  But it may have an uncountable set of zeroes in the quaternions.  For example, $x^2+1$ has two zeroes in $\mathbb C$, but in $\mathbb H$, ${\bf i}\cos x + {\bf j}\sin x$ is a distinct zero of this polynomial for every $x$ in $[0, 2\pi)$, and obviously there are many other zeroes. What is it about $\mathbb H$ that makes its behavior in this regard to be so different from the behavior of $\mathbb R$ and $\mathbb C$?  Is it simply because $\mathbb H$ is four-dimensional rather than two-dimensional? Are there any theorems that say when a ring will behave like $\mathbb H$ and when it will behave like $\mathbb C$? Do all polynomials behave like this in $\mathbb H$? Or is this one unusual?",,"['abstract-algebra', 'polynomials', 'roots', 'quaternions', 'division-algebras']"
55,"If polynomials are almost surjective over a field, is the field algebraically closed?","If polynomials are almost surjective over a field, is the field algebraically closed?",,"Let $K$ be a field.  Say that polynomials are almost surjective over $K$ if for any nonconstant polynomial $f(x)\in K[x]$, the image of the map $f:K\to K$ contains all but finitely many points of $K$.  That is, for all but finitely many $a\in K$, $f(x)-a$ has a root. Clearly polynomials are almost surjective over any finite field, or over any algebraically closed field.  My question is whether the converse holds.  That is: If $K$ is an infinite field and polynomials are almost surjective over $K$, must $K$ be algebraically closed? ( This answer to a similar question gave a simple proof that $\mathbb{C}$ is algebraically closed from the fact that polynomials are almost surjective over $\mathbb{C}$.  However, this proof made heavy use of special properties of $\mathbb{C}$ such as its topology, so it does not generalize to arbitrary fields.)","Let $K$ be a field.  Say that polynomials are almost surjective over $K$ if for any nonconstant polynomial $f(x)\in K[x]$, the image of the map $f:K\to K$ contains all but finitely many points of $K$.  That is, for all but finitely many $a\in K$, $f(x)-a$ has a root. Clearly polynomials are almost surjective over any finite field, or over any algebraically closed field.  My question is whether the converse holds.  That is: If $K$ is an infinite field and polynomials are almost surjective over $K$, must $K$ be algebraically closed? ( This answer to a similar question gave a simple proof that $\mathbb{C}$ is algebraically closed from the fact that polynomials are almost surjective over $\mathbb{C}$.  However, this proof made heavy use of special properties of $\mathbb{C}$ such as its topology, so it does not generalize to arbitrary fields.)",,"['abstract-algebra', 'polynomials', 'field-theory']"
56,Why “characteristic zero” and not “infinite characteristic”?,Why “characteristic zero” and not “infinite characteristic”?,,"The characteristic of a ring (with unity, say) is the smallest positive number $n$ such that $$\underbrace{1 + 1 + \cdots + 1}_{n \text{ times}} = 0,$$ provided such an $n$ exists. Otherwise, we define it to be $0$. But why characteristic zero? Why do we not define it to be $\infty$ instead? Under this alternative definition, the characteristic of a ring is simply the “order” of the additive cyclic group generated by the unit element $1$. My feeling is that there is a precise and convincing explanation for the common convention, but none comes to mind.  I couldn't find the answer in the Wikipedia article either.","The characteristic of a ring (with unity, say) is the smallest positive number $n$ such that $$\underbrace{1 + 1 + \cdots + 1}_{n \text{ times}} = 0,$$ provided such an $n$ exists. Otherwise, we define it to be $0$. But why characteristic zero? Why do we not define it to be $\infty$ instead? Under this alternative definition, the characteristic of a ring is simply the “order” of the additive cyclic group generated by the unit element $1$. My feeling is that there is a precise and convincing explanation for the common convention, but none comes to mind.  I couldn't find the answer in the Wikipedia article either.",,"['abstract-algebra', 'ring-theory', 'terminology', 'intuition']"
57,Normal subgroup of prime index,Normal subgroup of prime index,,Generalizing the case $p=2$ we would like to know if the statement below is true. Let $p$ the smallest prime dividing the order of $G$. If $H$ is a subgroup of $G$ with index $p$ then $H$ is normal.,Generalizing the case $p=2$ we would like to know if the statement below is true. Let $p$ the smallest prime dividing the order of $G$. If $H$ is a subgroup of $G$ with index $p$ then $H$ is normal.,,"['abstract-algebra', 'group-theory', 'finite-groups', 'normal-subgroups']"
58,When to learn category theory?,When to learn category theory?,,"I'm a undergraduate who wishes to learn category theory but I only have basic knowledge of linear algebra and set theory, I've also had a short course on number theory which used some basic concepts about groups and modular arithmetic. Is it too early to start learning category theory? should I wait to take a course on abstract algebra? Is it very important to use category theory facts in a first course in group theory, ring theory, fields and Galois theory, modules and tensor products (each of those is a one semester course), would that make it a 'better' course? I was unsure to learn category theory early but this post Mathematical subjects you wish you learned earlier inspired me to ask you given my background.","I'm a undergraduate who wishes to learn category theory but I only have basic knowledge of linear algebra and set theory, I've also had a short course on number theory which used some basic concepts about groups and modular arithmetic. Is it too early to start learning category theory? should I wait to take a course on abstract algebra? Is it very important to use category theory facts in a first course in group theory, ring theory, fields and Galois theory, modules and tensor products (each of those is a one semester course), would that make it a 'better' course? I was unsure to learn category theory early but this post Mathematical subjects you wish you learned earlier inspired me to ask you given my background.",,"['abstract-algebra', 'category-theory', 'education', 'learning']"
59,"Why are There No ""Triernions"" (3-dimensional analogue of complex numbers / quaternions)? [duplicate]","Why are There No ""Triernions"" (3-dimensional analogue of complex numbers / quaternions)? [duplicate]",,"This question already has answers here : Proving that $\mathbb R^3$ cannot be made into a real division algebra (and that extending complex multiplication would not work) (4 answers) Can there be a set of numbers, which have properties like those of quaternions, but of dimension 3? [duplicate] (1 answer) Closed 8 years ago . The community reviewed whether to reopen this question 2 years ago and left it closed: Original close reason(s) were not resolved Since there are  complex numbers (2 dimensions) and quaternions (4 dimensions), it follows intuitively that there ought to be something in between for 3 dimensions (""triernions""). Yet no one uses these. Why is this?","This question already has answers here : Proving that $\mathbb R^3$ cannot be made into a real division algebra (and that extending complex multiplication would not work) (4 answers) Can there be a set of numbers, which have properties like those of quaternions, but of dimension 3? [duplicate] (1 answer) Closed 8 years ago . The community reviewed whether to reopen this question 2 years ago and left it closed: Original close reason(s) were not resolved Since there are  complex numbers (2 dimensions) and quaternions (4 dimensions), it follows intuitively that there ought to be something in between for 3 dimensions (""triernions""). Yet no one uses these. Why is this?",,"['abstract-algebra', 'complex-numbers', 'quaternions']"
60,Are all algebraic integers with absolute value 1 roots of unity?,Are all algebraic integers with absolute value 1 roots of unity?,,"If we have an algebraic number $\alpha$ with (complex) absolute value $1$ , it does not follow that $\alpha$ is a root of unity (i.e., that $\alpha^n = 1$ for some $n$ ).  For example, $(3/5 + 4/5 i)$ is not a root of unity. But if we assume that $\alpha$ is an algebraic integer with absolute value $1$ , does it follow that $\alpha$ is a root of unity? I know that if all conjugates of $\alpha$ have absolute value $1$ , then $\alpha$ is a root of unity by the argument below: The minimal polynomial of $\alpha$ over $\mathbb{Z}$ is $\prod_{i=1}^d (x-\alpha_i)$ , where the $\alpha_i$ are just the conjugates of $\alpha$ .  Then $\prod_{i=1}^d (x-\alpha_i^n)$ is a polynomial over $\mathbb{Z}$ with $\alpha^n$ as a root.  It also has degree $d$ , and all roots have absolute value $1$ .  But there can only be finitely many such polynomials (since the coefficients are integers with bounded size), so we get that $\alpha^n=\sigma(\alpha)$ for some Galois conjugation $\sigma$ . If $\sigma^m(\alpha) = \alpha$ , then $\alpha^{n^m} = \alpha$ . Thus $\alpha^{n^m - 1} = 1$ .","If we have an algebraic number with (complex) absolute value , it does not follow that is a root of unity (i.e., that for some ).  For example, is not a root of unity. But if we assume that is an algebraic integer with absolute value , does it follow that is a root of unity? I know that if all conjugates of have absolute value , then is a root of unity by the argument below: The minimal polynomial of over is , where the are just the conjugates of .  Then is a polynomial over with as a root.  It also has degree , and all roots have absolute value .  But there can only be finitely many such polynomials (since the coefficients are integers with bounded size), so we get that for some Galois conjugation . If , then . Thus .",\alpha 1 \alpha \alpha^n = 1 n (3/5 + 4/5 i) \alpha 1 \alpha \alpha 1 \alpha \alpha \mathbb{Z} \prod_{i=1}^d (x-\alpha_i) \alpha_i \alpha \prod_{i=1}^d (x-\alpha_i^n) \mathbb{Z} \alpha^n d 1 \alpha^n=\sigma(\alpha) \sigma \sigma^m(\alpha) = \alpha \alpha^{n^m} = \alpha \alpha^{n^m - 1} = 1,"['abstract-algebra', 'algebraic-number-theory']"
61,More than 99% of groups of order less than 2000 are of order 1024?,More than 99% of groups of order less than 2000 are of order 1024?,,"In Algebra: Chapter 0 , the author made a remark (footnote on page 82), saying that more than 99% of groups of order less than 2000 are of order 1024. Is this for real? How can one deduce this result? Is there a nice way or do we just check all finite groups up to isomorphism? Thanks!","In Algebra: Chapter 0 , the author made a remark (footnote on page 82), saying that more than 99% of groups of order less than 2000 are of order 1024. Is this for real? How can one deduce this result? Is there a nice way or do we just check all finite groups up to isomorphism? Thanks!",,"['abstract-algebra', 'group-theory', 'finite-groups', 'p-groups', 'groups-enumeration']"
62,Good abstract algebra books for self study,Good abstract algebra books for self study,,"Last semester I picked up an algebra course at my university, which unfortunately was scheduled during my exams of my major (I'm a computer science major). So I had to self study the material, however, the self written syllabus was not self study friendly (good syllabus overall though). The course was split up into 3 parts, group theory, ring theory and field theory. As a computer science major we only had to study the first 2. Now that I passed the exam for this course I want to study the field theory part ( which covers Galois theory, etc). So, now I want to ask whether any of you know any good books on abstract algebra, which lift off at basic ring theory and continue to more advanced ring theory and to finite fields, Galois theory, ... Please keep in mind that I am not a math major, and that I would like books which are suited for self study (thus a lot of examples and intuition). Thanks in advance!","Last semester I picked up an algebra course at my university, which unfortunately was scheduled during my exams of my major (I'm a computer science major). So I had to self study the material, however, the self written syllabus was not self study friendly (good syllabus overall though). The course was split up into 3 parts, group theory, ring theory and field theory. As a computer science major we only had to study the first 2. Now that I passed the exam for this course I want to study the field theory part ( which covers Galois theory, etc). So, now I want to ask whether any of you know any good books on abstract algebra, which lift off at basic ring theory and continue to more advanced ring theory and to finite fields, Galois theory, ... Please keep in mind that I am not a math major, and that I would like books which are suited for self study (thus a lot of examples and intuition). Thanks in advance!",,"['abstract-algebra', 'reference-request', 'self-learning', 'book-recommendation']"
63,Are there real world applications of finite group theory?,Are there real world applications of finite group theory?,,"I would like to know whether there are examples where finite group theory can be directly applied to solve real world problems outside of mathematics.  (Sufficiently applied mathematics such as cryptography, coding theory, or statistics still count.) Let me clarify: I am not interested in applications of elementary group theory which happen to involve finite groups (e.g. cyclic/dihedral/easy groups as molecular symmetries).  I am interested in applications of topics specifically coming from finite group theory as a discipline, like one might see in Isaacs , Huppert , or Robinson . ""The Schur multiplier has order $2640,$ so we should point the laser that way."" ""Is this computer system secure?""  ""No - Frobenius kernels are nilpotent."" I'm aware of this MO post , but many of the applications listed there are inside mathematics or fall in the ""applications of easy groups"" category.  It is entirely possible that what I'm looking for doesn't exist, and that finite group theory is still an untouchable, pure subject, like number theory in the days of G. H. Hardy.  But perhaps not.  Does anyone know of any applications of the higher level stuff?","I would like to know whether there are examples where finite group theory can be directly applied to solve real world problems outside of mathematics.  (Sufficiently applied mathematics such as cryptography, coding theory, or statistics still count.) Let me clarify: I am not interested in applications of elementary group theory which happen to involve finite groups (e.g. cyclic/dihedral/easy groups as molecular symmetries).  I am interested in applications of topics specifically coming from finite group theory as a discipline, like one might see in Isaacs , Huppert , or Robinson . ""The Schur multiplier has order so we should point the laser that way."" ""Is this computer system secure?""  ""No - Frobenius kernels are nilpotent."" I'm aware of this MO post , but many of the applications listed there are inside mathematics or fall in the ""applications of easy groups"" category.  It is entirely possible that what I'm looking for doesn't exist, and that finite group theory is still an untouchable, pure subject, like number theory in the days of G. H. Hardy.  But perhaps not.  Does anyone know of any applications of the higher level stuff?","2640,","['abstract-algebra', 'group-theory', 'finite-groups', 'big-list', 'applications']"
64,Does commutativity imply Associativity?,Does commutativity imply Associativity?,,Does commutativity imply associativity? I'm asking this because I was trying to think of structures that are commutative but non-associative but couldn't come up with any. Are there any such examples? NOTE: I wasn't sure how to tag this so feel free to retag it.,Does commutativity imply associativity? I'm asking this because I was trying to think of structures that are commutative but non-associative but couldn't come up with any. Are there any such examples? NOTE: I wasn't sure how to tag this so feel free to retag it.,,"['abstract-algebra', 'examples-counterexamples', 'binary-operations', 'associativity']"
65,Ring structure on the Galois group of a finite field,Ring structure on the Galois group of a finite field,,"Let $F$ be a finite field. There is an isomorphism of topological groups $(\mathrm{Gal}(\overline{F}/F),\circ) \cong (\widehat{\mathbb{Z}},+)$ . It follows that the Galois group carries the structure of a topological ring isomorphic to $\widehat{\mathbb{Z}}$ . What does the multiplication $*$ look like, intrinsically? Well, if $\sigma$ is the Frobenius, we have $\sigma^n * \sigma^m = \sigma^{n \cdot m}$ for all $n,m \in \mathbb{Z}$ , and this describes $*$ completely. But is there any way to give an explicit and intrinsic formula for $\alpha * \beta$ if $\alpha,\beta$ are $F$ -automorphisms of $\overline{F}$ ? Also, is there any more conceptual reason why the Galois group carries the structure of a topological ring - without computing the Galois group? Maybe the following is a more precise version of the latter question using Grothendieck's Galois theory : Consider the Galois category $\mathcal{C}$ of finite étale $F$ -algebras together with the fiber functor $\mathcal{C} \to \mathsf{FinSet}$ . The automorphism group is exactly $\pi_1(\mathrm{Spec}(F))=\widehat{\mathbb{Z}}$ . So we may ask: Which additional structure on a Galois category is responsible for the ring structure on its automorphism group? Here is an idea: Grothendieck's main theorem of Galois theory states that $G \mapsto G{-}\mathsf{FinSet}$ is an anti-equivalence of categories from profinite groups to Galois categories (with their fiber functors) -- right? The category of profinite groups has finite products (easy), so there are finite coproducts of Galois categories. But how do we describe these, intrinsically? We have $G{-}\mathsf{FinSet} \sqcup H{-}\mathsf{FinSet} = (G \times H){-}\mathsf{FinSet}$ for example. The connection to the question is as follows: The anti-equivalence above induces an anti-equivalence of monoids with respect to the product. So there is an anti-equivalence of categories between topological rings and comonoids of Galois categories , the latter being equipped with some kind of functor $\mathcal{C} \to \mathcal{C} \sqcup \mathcal{C}$ etc. So this seems to be the additional structure I am looking for. And the original question asks to give an explicit functor for the special case $\mathcal{C} = $ finite étale $F$ -algebras.","Let be a finite field. There is an isomorphism of topological groups . It follows that the Galois group carries the structure of a topological ring isomorphic to . What does the multiplication look like, intrinsically? Well, if is the Frobenius, we have for all , and this describes completely. But is there any way to give an explicit and intrinsic formula for if are -automorphisms of ? Also, is there any more conceptual reason why the Galois group carries the structure of a topological ring - without computing the Galois group? Maybe the following is a more precise version of the latter question using Grothendieck's Galois theory : Consider the Galois category of finite étale -algebras together with the fiber functor . The automorphism group is exactly . So we may ask: Which additional structure on a Galois category is responsible for the ring structure on its automorphism group? Here is an idea: Grothendieck's main theorem of Galois theory states that is an anti-equivalence of categories from profinite groups to Galois categories (with their fiber functors) -- right? The category of profinite groups has finite products (easy), so there are finite coproducts of Galois categories. But how do we describe these, intrinsically? We have for example. The connection to the question is as follows: The anti-equivalence above induces an anti-equivalence of monoids with respect to the product. So there is an anti-equivalence of categories between topological rings and comonoids of Galois categories , the latter being equipped with some kind of functor etc. So this seems to be the additional structure I am looking for. And the original question asks to give an explicit functor for the special case finite étale -algebras.","F (\mathrm{Gal}(\overline{F}/F),\circ) \cong (\widehat{\mathbb{Z}},+) \widehat{\mathbb{Z}} * \sigma \sigma^n * \sigma^m = \sigma^{n \cdot m} n,m \in \mathbb{Z} * \alpha * \beta \alpha,\beta F \overline{F} \mathcal{C} F \mathcal{C} \to \mathsf{FinSet} \pi_1(\mathrm{Spec}(F))=\widehat{\mathbb{Z}} G \mapsto G{-}\mathsf{FinSet} G{-}\mathsf{FinSet} \sqcup H{-}\mathsf{FinSet} = (G \times H){-}\mathsf{FinSet} \mathcal{C} \to \mathcal{C} \sqcup \mathcal{C} \mathcal{C} =  F","['abstract-algebra', 'ring-theory', 'field-theory', 'galois-theory', 'finite-fields']"
66,Finite subgroups of the multiplicative group of a field are cyclic,Finite subgroups of the multiplicative group of a field are cyclic,,"In Grove's book Algebra, Proposition 3.7 at page 94 is the following If $G$ is a finite subgroup of the multiplicative group $F^*$ of a field $F$,   then $G$ is cyclic. He starts the proof by saying ""Since $G$ is the direct product of its Sylow subgroups ..."". But this is only true if the Sylow subgroups of $G$ are all normal. How do we know this?","In Grove's book Algebra, Proposition 3.7 at page 94 is the following If $G$ is a finite subgroup of the multiplicative group $F^*$ of a field $F$,   then $G$ is cyclic. He starts the proof by saying ""Since $G$ is the direct product of its Sylow subgroups ..."". But this is only true if the Sylow subgroups of $G$ are all normal. How do we know this?",,"['abstract-algebra', 'group-theory']"
67,Why would I want to multiply two polynomials?,Why would I want to multiply two polynomials?,,"I'm hoping that this isn't such a basic question that it gets completely laughed off the site, but why would I want to multiply two polynomials together? I flipped through some algebra books and have googled around a bit, and whenever they introduce polynomial multiplication they just say 'Suppose you have two polynomials you wish to multiply', or sometimes it's just as simple as 'find the product'. I even looked for some example story problems, hoping that might let me in on the secret, but no dice. I understand that a polynomial is basically a set of numbers (or, if you'd rather, a mapping of one set of numbers to another), or, in another way of thinking about it, two polynomials are functions, and the product of the two functions is a new function that lets you apply the function once, provided you were planning on applying the original functions to the number and then multiplying the result together. Elementary multiplication can be described as 'add $X$ to itself $Y$ times', where $Y$ is a nice integer number of times. When $Y$ is not a whole number, it doesn't seem to make as much sense. Any ideas?","I'm hoping that this isn't such a basic question that it gets completely laughed off the site, but why would I want to multiply two polynomials together? I flipped through some algebra books and have googled around a bit, and whenever they introduce polynomial multiplication they just say 'Suppose you have two polynomials you wish to multiply', or sometimes it's just as simple as 'find the product'. I even looked for some example story problems, hoping that might let me in on the secret, but no dice. I understand that a polynomial is basically a set of numbers (or, if you'd rather, a mapping of one set of numbers to another), or, in another way of thinking about it, two polynomials are functions, and the product of the two functions is a new function that lets you apply the function once, provided you were planning on applying the original functions to the number and then multiplying the result together. Elementary multiplication can be described as 'add $X$ to itself $Y$ times', where $Y$ is a nice integer number of times. When $Y$ is not a whole number, it doesn't seem to make as much sense. Any ideas?",,"['abstract-algebra', 'algebra-precalculus', 'polynomials', 'soft-question']"
68,"A semigroup $X$ is a group iff for every $g\in X$, $\exists! x\in X$ such that $gxg = g$","A semigroup  is a group iff for every ,  such that",X g\in X \exists! x\in X gxg = g,"The following could have shown up as an exercise in a basic Abstract Algebra text, and if anyone can give me a reference, I will be most grateful. Consider a set $X$ with an associative law of composition, not known to have an identity or inverses. Suppose that for every $g\in X$, there is a unique $x\in X$ with $gxg=g$. Show that $X$ is a group. Note that I’m not asking for a proof (though a really short one would please me!), just some place where this has been published.","The following could have shown up as an exercise in a basic Abstract Algebra text, and if anyone can give me a reference, I will be most grateful. Consider a set $X$ with an associative law of composition, not known to have an identity or inverses. Suppose that for every $g\in X$, there is a unique $x\in X$ with $gxg=g$. Show that $X$ is a group. Note that I’m not asking for a proof (though a really short one would please me!), just some place where this has been published.",,"['abstract-algebra', 'group-theory', 'reference-request', 'semigroups']"
69,How do I prove that $x^p-x+a$ is irreducible in a field with $p$ elements when $a\neq 0$?,How do I prove that  is irreducible in a field with  elements when ?,x^p-x+a p a\neq 0,"Let $p$ be a prime. How do I prove that $x^p-x+a$ is irreducible in a field with $p$ elements when $a\neq 0$? Right now I'm able to prove that it has no roots and that it is separable, but I have not a clue as to how to prove it is irreducible. Any ideas?","Let $p$ be a prime. How do I prove that $x^p-x+a$ is irreducible in a field with $p$ elements when $a\neq 0$? Right now I'm able to prove that it has no roots and that it is separable, but I have not a clue as to how to prove it is irreducible. Any ideas?",,"['abstract-algebra', 'polynomials', 'field-theory', 'irreducible-polynomials', 'positive-characteristic']"
70,"What kind of ""symmetry"" is the symmetric group about?","What kind of ""symmetry"" is the symmetric group about?",,"There are two concepts which are very similar literally in abstract algebra: symmetric group and symmetry group . By definition, the symmetric group on a set is the group consisting of all bijections of the set (all one-to-one and onto functions) from the set to itself with function composition as the group operation. When the set is finite, the group is sometimes denoted as $S_n$. The Dihedral group $D_n$, which is a special case of the symmetry group,  has a very strong geometric intuition about symmetry as the picture shows. I know nothing about the relation between these two concepts but the fact that $D_3$ and $S_3$ are actually the same. For me, symmetric group is more about "" permutations "". And actually its subgroups are also called permutation groups . Here are my questions : What's the relation between these two concepts: ""symmetric group"" and   ""symmetry group""? What kind of "" symmetry "" is the symmetric group about? Where is the name ""symmetric group"" from?","There are two concepts which are very similar literally in abstract algebra: symmetric group and symmetry group . By definition, the symmetric group on a set is the group consisting of all bijections of the set (all one-to-one and onto functions) from the set to itself with function composition as the group operation. When the set is finite, the group is sometimes denoted as $S_n$. The Dihedral group $D_n$, which is a special case of the symmetry group,  has a very strong geometric intuition about symmetry as the picture shows. I know nothing about the relation between these two concepts but the fact that $D_3$ and $S_3$ are actually the same. For me, symmetric group is more about "" permutations "". And actually its subgroups are also called permutation groups . Here are my questions : What's the relation between these two concepts: ""symmetric group"" and   ""symmetry group""? What kind of "" symmetry "" is the symmetric group about? Where is the name ""symmetric group"" from?",,['abstract-algebra']
71,How is a group made up of simple groups?,How is a group made up of simple groups?,,"I've read more than once the analogy between simple groups and prime numbers, stating that any group is built up from simple groups, like any number is built from prime numbers. I've recently started self-studying subgroup series, which is supposed to explain the analogy, but I'm not completely sure I understand how ""any group is made of simple groups"". Given a group $G$ with composition series $$ \{e\}=G_0 \triangleleft G_1\triangleleft \dots \triangleleft G_{r-1} \triangleleft G_r=G$$ then $G$ has associated the simple factor groups $H_{i+1}=G_{i+1}/G_i$. But how is it ""built"" from them? Well, if we have those simple groups $H_i$ then we can say the subnormal subgroups in the composition series can be recovered by taking certain extensions of $H_i$: $$ 1 \to K_i \to G_i \to H_i \to 1$$ where $H_i = G_i/G_{i-1}$, $K_i\simeq G_{i-1}$. Then $G$ is built from some uniquely determined (Jordan-Hölder) simple groups $H_i$ by taking extensions of these groups. Is this description accurate? The question now is: this description seems overly theoretical to me. I don't know how the extensions of $H_i$ look like, and I don't understand how $G$ puts these groups together. Can we describe more explicitly how a group $G$ is made of simple groups? EDIT: I forgot a (not-so-tiny) detail. The previous explanation works for finite groups, or more in general for groups with a composition series. But what about groups which don't admit a composition series? Is it correct to say that they are built from simple groups?","I've read more than once the analogy between simple groups and prime numbers, stating that any group is built up from simple groups, like any number is built from prime numbers. I've recently started self-studying subgroup series, which is supposed to explain the analogy, but I'm not completely sure I understand how ""any group is made of simple groups"". Given a group $G$ with composition series $$ \{e\}=G_0 \triangleleft G_1\triangleleft \dots \triangleleft G_{r-1} \triangleleft G_r=G$$ then $G$ has associated the simple factor groups $H_{i+1}=G_{i+1}/G_i$. But how is it ""built"" from them? Well, if we have those simple groups $H_i$ then we can say the subnormal subgroups in the composition series can be recovered by taking certain extensions of $H_i$: $$ 1 \to K_i \to G_i \to H_i \to 1$$ where $H_i = G_i/G_{i-1}$, $K_i\simeq G_{i-1}$. Then $G$ is built from some uniquely determined (Jordan-Hölder) simple groups $H_i$ by taking extensions of these groups. Is this description accurate? The question now is: this description seems overly theoretical to me. I don't know how the extensions of $H_i$ look like, and I don't understand how $G$ puts these groups together. Can we describe more explicitly how a group $G$ is made of simple groups? EDIT: I forgot a (not-so-tiny) detail. The previous explanation works for finite groups, or more in general for groups with a composition series. But what about groups which don't admit a composition series? Is it correct to say that they are built from simple groups?",,"['abstract-algebra', 'group-theory']"
72,Intuitive meaning of Exact Sequence,Intuitive meaning of Exact Sequence,,"I'm currently learning about exact sequences in grad sch Algebra I course, but I really can't get the intuitive picture of the concept and why it is important at all. Can anyone explain them for me? Thanks in advance.","I'm currently learning about exact sequences in grad sch Algebra I course, but I really can't get the intuitive picture of the concept and why it is important at all. Can anyone explain them for me? Thanks in advance.",,"['abstract-algebra', 'soft-question', 'intuition', 'homological-algebra', 'exact-sequence']"
73,Why can't the Polynomial Ring be a Field?,Why can't the Polynomial Ring be a Field?,,"I'm currently studying Polynomial Rings, but I can't figure out why they are Rings, not Fields. In the definition of a Field, a Set builds a Commutative Group with Addition and Multiplication. This implies an inverse multiple for every Element in the Set. The book doesn't elaborate on this, however. I don't understand why a Polynomial Ring couldn't have an inverse multiplicative for every element (at least in the Whole numbers, and it's already given that it has a neutral element). Could somebody please explain why this can't be so?","I'm currently studying Polynomial Rings, but I can't figure out why they are Rings, not Fields. In the definition of a Field, a Set builds a Commutative Group with Addition and Multiplication. This implies an inverse multiple for every Element in the Set. The book doesn't elaborate on this, however. I don't understand why a Polynomial Ring couldn't have an inverse multiplicative for every element (at least in the Whole numbers, and it's already given that it has a neutral element). Could somebody please explain why this can't be so?",,"['abstract-algebra', 'ring-theory', 'commutative-algebra']"
74,Prove that the set of all algebraic numbers is countable,Prove that the set of all algebraic numbers is countable,,"A complex number $z$ is said to be algebraic if there are integers $a_0, ..., a_n$, not all zero, such that  $a_0z^n+a_1z^{n-1}+...+a_{n-1}z+a_n=0$. Prove that the set of all algebraic numbers is countable.  The Hint is: For every positive integer $N$ there are only finitely many equations with $n+|a_0|+|a_1|+...+|a_n|=N$. Here is a proof I have. The problem though, is that I did not use the hint provided in the text, so maybe this proof is invalid or that there is an alternate (simpler) proof? Please help me out here. Thanks in advance. Proof: The set of integers is countable, we have this following theorem: Let $A$ be a countable set, and let $B_n$ be the set of all n-tuples $(a_1,...,a_n)$, where $a_k \in A, k=1,...,n,$ and the elements $a_1,...,a_n$ need not be distinct. Then $B_n$ is countable. So by this theorem, the set of all $(k+1)$-tuples $(a_0,a_1,...,a_k)$ with $a_0 \neq 0$ is also countable. Let this set be represented by $\mathbb Z ^k$. For each a $a \in \mathbb Z ^k$ consider the polynomial $a_0z^k+a_1z^{k-1}+...+a_k=0$. From the fundamental theorem of algebra, we know that there are exactly $k$ complex roots for this polynomial. We now have a series of nested sets that encompass every possible root for every possible polynomial with integer coefficients. More specifically, we have a countable number of $\mathbb Z^k s$, each containing a countable number of $(k + 1)$-tuples, each of which corresponds with $k$ roots of a $k$-degree polynomial. So our set of complex roots (call it $R$) is a countable union of countable unions of finite sets. This only tells us that $R$ is at most countable: it is either countable or finite. To show that $R$ is not finite, consider the roots for $2$-tuples in $\mathbb Z^1$. Each $2$-tuple of the form $(-1, n)$ corresponds with the polynomial $-z + n = 0$ whose solution is $z = n$. There is clearly a unique solution for each $n \in \mathbb Z$, so $R$ is an infinite set. Because $R$ is also at most countable, this proves that $R$ is countable.","A complex number $z$ is said to be algebraic if there are integers $a_0, ..., a_n$, not all zero, such that  $a_0z^n+a_1z^{n-1}+...+a_{n-1}z+a_n=0$. Prove that the set of all algebraic numbers is countable.  The Hint is: For every positive integer $N$ there are only finitely many equations with $n+|a_0|+|a_1|+...+|a_n|=N$. Here is a proof I have. The problem though, is that I did not use the hint provided in the text, so maybe this proof is invalid or that there is an alternate (simpler) proof? Please help me out here. Thanks in advance. Proof: The set of integers is countable, we have this following theorem: Let $A$ be a countable set, and let $B_n$ be the set of all n-tuples $(a_1,...,a_n)$, where $a_k \in A, k=1,...,n,$ and the elements $a_1,...,a_n$ need not be distinct. Then $B_n$ is countable. So by this theorem, the set of all $(k+1)$-tuples $(a_0,a_1,...,a_k)$ with $a_0 \neq 0$ is also countable. Let this set be represented by $\mathbb Z ^k$. For each a $a \in \mathbb Z ^k$ consider the polynomial $a_0z^k+a_1z^{k-1}+...+a_k=0$. From the fundamental theorem of algebra, we know that there are exactly $k$ complex roots for this polynomial. We now have a series of nested sets that encompass every possible root for every possible polynomial with integer coefficients. More specifically, we have a countable number of $\mathbb Z^k s$, each containing a countable number of $(k + 1)$-tuples, each of which corresponds with $k$ roots of a $k$-degree polynomial. So our set of complex roots (call it $R$) is a countable union of countable unions of finite sets. This only tells us that $R$ is at most countable: it is either countable or finite. To show that $R$ is not finite, consider the roots for $2$-tuples in $\mathbb Z^1$. Each $2$-tuple of the form $(-1, n)$ corresponds with the polynomial $-z + n = 0$ whose solution is $z = n$. There is clearly a unique solution for each $n \in \mathbb Z$, so $R$ is an infinite set. Because $R$ is also at most countable, this proves that $R$ is countable.",,"['abstract-algebra', 'elementary-set-theory', 'algebraic-numbers']"
75,Why are groups more important than semigroups?,Why are groups more important than semigroups?,,"This is an open-ended question, as is probably obvious from the title. I understand that it may not be appreciated and I will try not to ask too many such questions. But this one has been bothering me for quite some time and I'm not entirely certain that it's completely worthless, which is why I've decided to ask it here. Why are groups so immensely more important in mathematics and its applications than semigroups are? I know little of group theory, mathematics and its applications, so I cannot understand how much more important they are. But I know they are because I see how many more people are interested in groups than in semigroups. I wouldn't ask this question if this fact didn't seem a little strange to me. I know that groups are associated with symmetries, or with automorphisms of structures. Cayley's Theorem tells me that every group can be seen as a set (closed with respect to taking compositions and inverses) of automorphisms of a structure with no constants, functions or relations, i.e. a plain set. I know that the automorphisms of a vector space, a module or a group form a group. Obviously, automorphisms are important. Then we have inverse semigroups. These are less popular but this I can understand. They are associated with partial symmetries, by the Wagner-Preston Theorem. Partial functions do seem much less used than functions. But then come semigroups. Just like in the two previous cases, there is an ""embedding theorem"", which says that every semigroup can be embedded in a semigroup of maps from a certain set into itself. And, analogously to the case of groups, the endomorphisms of a vector space, a module or a group form a semigroup. This seems to say that semigroups are to endomorphisms what groups are to automorphisms. The ""conclusion"" would be that $\frac{\mbox{the importance of semigroups}}{\mbox{the importance of endomorphisms}}=\frac{\mbox{the importance of groups}}{\mbox{the importance of automorphisms}}$ Assuming that endomorphisms are about as important as automorphisms, we get that semigroups are about as important as groups. My feeling is that the assumption is correct. I understand that I must be oversimplifying something at some point. But what am I oversimplifying and where? I realize that this is possibly a very dumb question, but my confusion is genuine. Edit: I have realized, after reading your answers and comments, that I made the mistake of using a very vague term without even attempting to define it. The problem seems to be what importance is. Is it popularity or usefulness, or being used a lot, or interesting or being interesting, or something else, or a mixture of many traits? I'm not going to force my understanding of the word now. But if someone still wants to answer this question, perhaps it would be good idea if the answer contained an attempt at clearing this up. (Or maybe not. I'm not sure!) Edit: A question similar to mine is dealt with here .","This is an open-ended question, as is probably obvious from the title. I understand that it may not be appreciated and I will try not to ask too many such questions. But this one has been bothering me for quite some time and I'm not entirely certain that it's completely worthless, which is why I've decided to ask it here. Why are groups so immensely more important in mathematics and its applications than semigroups are? I know little of group theory, mathematics and its applications, so I cannot understand how much more important they are. But I know they are because I see how many more people are interested in groups than in semigroups. I wouldn't ask this question if this fact didn't seem a little strange to me. I know that groups are associated with symmetries, or with automorphisms of structures. Cayley's Theorem tells me that every group can be seen as a set (closed with respect to taking compositions and inverses) of automorphisms of a structure with no constants, functions or relations, i.e. a plain set. I know that the automorphisms of a vector space, a module or a group form a group. Obviously, automorphisms are important. Then we have inverse semigroups. These are less popular but this I can understand. They are associated with partial symmetries, by the Wagner-Preston Theorem. Partial functions do seem much less used than functions. But then come semigroups. Just like in the two previous cases, there is an ""embedding theorem"", which says that every semigroup can be embedded in a semigroup of maps from a certain set into itself. And, analogously to the case of groups, the endomorphisms of a vector space, a module or a group form a semigroup. This seems to say that semigroups are to endomorphisms what groups are to automorphisms. The ""conclusion"" would be that $\frac{\mbox{the importance of semigroups}}{\mbox{the importance of endomorphisms}}=\frac{\mbox{the importance of groups}}{\mbox{the importance of automorphisms}}$ Assuming that endomorphisms are about as important as automorphisms, we get that semigroups are about as important as groups. My feeling is that the assumption is correct. I understand that I must be oversimplifying something at some point. But what am I oversimplifying and where? I realize that this is possibly a very dumb question, but my confusion is genuine. Edit: I have realized, after reading your answers and comments, that I made the mistake of using a very vague term without even attempting to define it. The problem seems to be what importance is. Is it popularity or usefulness, or being used a lot, or interesting or being interesting, or something else, or a mixture of many traits? I'm not going to force my understanding of the word now. But if someone still wants to answer this question, perhaps it would be good idea if the answer contained an attempt at clearing this up. (Or maybe not. I'm not sure!) Edit: A question similar to mine is dealt with here .",,"['abstract-algebra', 'group-theory']"
76,Intuition in algebra?,Intuition in algebra?,,"My algebra background: I've had 2 undergrad semesters of algebra, a reading course in Galois Theory, a graduate course in commutative algebra and one in algebraic geometry, and I've done (most of) MacLane and Birkhoff's Algebra on my own. The problem is that I feel like I still don't have any idea how to do algebra. I do well in my classes and don't have any problem with most exercises in M&B I do. But my process consists pretty much entirely of fiddling around with symbols until I figure out how to apply theorems I know in a completely straightforward way. I'm able to do exercises from a book, but rarely able to prove theorems in the text on my own. This is a complete contrast to how I feel in topology and analysis (not that I really know any topology or analysis), where I have a halfway decent intuition and really think I know why things are true (and moreover, why anyone should care). I'm able to prove theorems. In topology and analysis, I am able to visualize things pretty directly in a way that I can get insight into how things work. For algebra, I have no picture. I've tried learning about Cayley graphs to visualize groups. I think these are neat, but I have yet to successfully apply any insight from them. I hoped learning about algebraic geometry would help me visualize rings. But the geometry in algebraic geometry is dictated entirely by the algebra. So how can you use geometry to help you with the algebra, when you have to do the algebra first to figure out what the geometry looks like? I don't get it. So the question I'm trying to get at is: How do I develop some insight or intuition about algebra? I don't really know what form answers might take; maybe a reading suggestion, or just a general way to look at things. Maybe this isn't a good question, but I'm kind of at the end of my rope with this stuff. A particular user on MathOverflow said he fell in love with algebra the first time he saw the axioms for a group. When I first saw the axioms for a group, I spent the next year trying to figure out why the heck anybody cared about groups (and frankly still only know this in a detached and academic way). So it's possible the only answer is: I'm barking up the wrong tree; algebra isn't for me and I should move on to something that comes more naturally.","My algebra background: I've had 2 undergrad semesters of algebra, a reading course in Galois Theory, a graduate course in commutative algebra and one in algebraic geometry, and I've done (most of) MacLane and Birkhoff's Algebra on my own. The problem is that I feel like I still don't have any idea how to do algebra. I do well in my classes and don't have any problem with most exercises in M&B I do. But my process consists pretty much entirely of fiddling around with symbols until I figure out how to apply theorems I know in a completely straightforward way. I'm able to do exercises from a book, but rarely able to prove theorems in the text on my own. This is a complete contrast to how I feel in topology and analysis (not that I really know any topology or analysis), where I have a halfway decent intuition and really think I know why things are true (and moreover, why anyone should care). I'm able to prove theorems. In topology and analysis, I am able to visualize things pretty directly in a way that I can get insight into how things work. For algebra, I have no picture. I've tried learning about Cayley graphs to visualize groups. I think these are neat, but I have yet to successfully apply any insight from them. I hoped learning about algebraic geometry would help me visualize rings. But the geometry in algebraic geometry is dictated entirely by the algebra. So how can you use geometry to help you with the algebra, when you have to do the algebra first to figure out what the geometry looks like? I don't get it. So the question I'm trying to get at is: How do I develop some insight or intuition about algebra? I don't really know what form answers might take; maybe a reading suggestion, or just a general way to look at things. Maybe this isn't a good question, but I'm kind of at the end of my rope with this stuff. A particular user on MathOverflow said he fell in love with algebra the first time he saw the axioms for a group. When I first saw the axioms for a group, I spent the next year trying to figure out why the heck anybody cared about groups (and frankly still only know this in a detached and academic way). So it's possible the only answer is: I'm barking up the wrong tree; algebra isn't for me and I should move on to something that comes more naturally.",,"['abstract-algebra', 'soft-question', 'intuition']"
77,"Is $\mathbb{Q}(\sqrt{2}, \sqrt{3}) = \mathbb{Q}(\sqrt{2}+\sqrt{3})$?",Is ?,"\mathbb{Q}(\sqrt{2}, \sqrt{3}) = \mathbb{Q}(\sqrt{2}+\sqrt{3})","Is $\mathbb{Q}(\sqrt{2}, \sqrt{3}) = \mathbb{Q}(\sqrt{2}+\sqrt{3})$ ? $$\mathbb{Q}(\sqrt{2},\sqrt{3})=\{a+b\sqrt{2}+c\sqrt{3}+d\sqrt{6} \mid a,b,c,d\in\mathbb{Q}\}$$ $$\mathbb{Q}(\sqrt{2}+\sqrt{3}) = \lbrace a+b(\sqrt{2}+\sqrt{3}) \mid a,b \in \mathbb{Q} \rbrace  $$ So if an element is in $\mathbb Q(\sqrt{2},\sqrt{3})$ , then it is in $\mathbb{Q}(\sqrt{2}+\sqrt{3})$ , because $\sqrt{6} = \sqrt{2}\sqrt{3}$ . How to conclude from there?","Is ? So if an element is in , then it is in , because . How to conclude from there?","\mathbb{Q}(\sqrt{2}, \sqrt{3}) = \mathbb{Q}(\sqrt{2}+\sqrt{3}) \mathbb{Q}(\sqrt{2},\sqrt{3})=\{a+b\sqrt{2}+c\sqrt{3}+d\sqrt{6} \mid a,b,c,d\in\mathbb{Q}\} \mathbb{Q}(\sqrt{2}+\sqrt{3}) = \lbrace a+b(\sqrt{2}+\sqrt{3}) \mid a,b \in \mathbb{Q} \rbrace   \mathbb Q(\sqrt{2},\sqrt{3}) \mathbb{Q}(\sqrt{2}+\sqrt{3}) \sqrt{6} = \sqrt{2}\sqrt{3}","['abstract-algebra', 'field-theory']"
78,"If $G/Z(G)$ is cyclic, then $G$ is abelian","If  is cyclic, then  is abelian",G/Z(G) G,"Continuing my work through Dummit & Foote's ""Abstract Algebra"", 3.1.36 asks the following (which is exactly the same as exercise 5 in this related MSE answer ): Prove that if $G/Z(G)$ is cyclic, then $G$ is abelian. [If $G/Z(G)$ is cyclic with generator $xZ(G)$, show that every element of $G$ can be written in the form $x^az$ for some $a \in \mathbb{Z}$ and some element $z \in Z(G)$] The hint is actually the hardest part for me, as the quotient groups are somewhat abstract. But once I have the hint, I can write: $g, h \in G$ implies that $g = x^{a_1}z_1$ and $h = x^{a_2}z_2$, so \begin{align*}gh &= (x^{a_1}z_1)(x^{a_2}z_2)\\\  &= x^{a_1}x^{a_2}z_1z_2\\\ & = x^{a_1 + a_2}z_2z_1\\\ &= \ldots = (x^{a_2}z_2)(x^{a_1}z_1) = hg. \end{align*} Therefore, $G$ is abelian. 1) Is this right so far? 2) How can I prove the ""hint""?","Continuing my work through Dummit & Foote's ""Abstract Algebra"", 3.1.36 asks the following (which is exactly the same as exercise 5 in this related MSE answer ): Prove that if $G/Z(G)$ is cyclic, then $G$ is abelian. [If $G/Z(G)$ is cyclic with generator $xZ(G)$, show that every element of $G$ can be written in the form $x^az$ for some $a \in \mathbb{Z}$ and some element $z \in Z(G)$] The hint is actually the hardest part for me, as the quotient groups are somewhat abstract. But once I have the hint, I can write: $g, h \in G$ implies that $g = x^{a_1}z_1$ and $h = x^{a_2}z_2$, so \begin{align*}gh &= (x^{a_1}z_1)(x^{a_2}z_2)\\\  &= x^{a_1}x^{a_2}z_1z_2\\\ & = x^{a_1 + a_2}z_2z_1\\\ &= \ldots = (x^{a_2}z_2)(x^{a_1}z_1) = hg. \end{align*} Therefore, $G$ is abelian. 1) Is this right so far? 2) How can I prove the ""hint""?",,"['abstract-algebra', 'group-theory', 'abelian-groups', 'cyclic-groups']"
79,Is Lagrange's theorem the most basic result in finite group theory?,Is Lagrange's theorem the most basic result in finite group theory?,,"Motivated by this question , can one prove that the order of an element in a finite group divides the order of the group without using Lagrange's theorem? (Or, equivalently, that the order of the group is an exponent for every element in the group?) The simplest proof I can think of uses the coset proof of Lagrange's theorem in disguise and goes like this: take $a \in G$ and consider the map $f\colon G \to G$ given by $f(x)=ax$. Consider now the orbits of $f$, that is, the sets $\mathcal{O}(x)=\{ x, f(x), f(f(x)), \dots \}$. Now all orbits have the same number of elements and $|\mathcal{O}(e)| = o(a)$. Hence $o(a)$ divides $|G|$. This proof has perhaps some pedagogical value in introductory courses because it can be generalized in a natural way to non-cyclic subgroups by introducing cosets, leading to the canonical proof of Lagrange's theorem. Has anyone seen a different approach to this result that avoids using Lagrange's theorem? Or is Lagrange's theorem really the most basic result in finite group theory?","Motivated by this question , can one prove that the order of an element in a finite group divides the order of the group without using Lagrange's theorem? (Or, equivalently, that the order of the group is an exponent for every element in the group?) The simplest proof I can think of uses the coset proof of Lagrange's theorem in disguise and goes like this: take $a \in G$ and consider the map $f\colon G \to G$ given by $f(x)=ax$. Consider now the orbits of $f$, that is, the sets $\mathcal{O}(x)=\{ x, f(x), f(f(x)), \dots \}$. Now all orbits have the same number of elements and $|\mathcal{O}(e)| = o(a)$. Hence $o(a)$ divides $|G|$. This proof has perhaps some pedagogical value in introductory courses because it can be generalized in a natural way to non-cyclic subgroups by introducing cosets, leading to the canonical proof of Lagrange's theorem. Has anyone seen a different approach to this result that avoids using Lagrange's theorem? Or is Lagrange's theorem really the most basic result in finite group theory?",,"['abstract-algebra', 'group-theory', 'finite-groups', 'education']"
80,Does every Abelian group admit a ring structure?,Does every Abelian group admit a ring structure?,,"Given some Abelian group $(G, +)$, does there always exist a binary operation $*$ such that $(G, +, *)$ is a ring? That is, $*$ is associative and distributive: \begin{align*} &a * (b * c) = (a*b) * c \\ &a * (b + c) = a * b + a * c \\ &(a + b) * c = a * c + b * c \\ \end{align*} We also might have multiplicative identity $1 \in G$, with $a * 1 = 1 * a = a$ for any $a \in G$. Multiplication may or may not be commutative. Depending on the definition, the answer could be no in the case of the group with one element: then $1 = 0$. But the trivial ring is not a very interesting case. For cyclic groups the statement is certainly true, since $(\mathbb{Z}_n, +, \cdot)$ and $(\mathbb{Z}, +, \cdot)$ are both rings. What about in general? Is there some procedure to give arbitrary abelian groups ring structure?","Given some Abelian group $(G, +)$, does there always exist a binary operation $*$ such that $(G, +, *)$ is a ring? That is, $*$ is associative and distributive: \begin{align*} &a * (b * c) = (a*b) * c \\ &a * (b + c) = a * b + a * c \\ &(a + b) * c = a * c + b * c \\ \end{align*} We also might have multiplicative identity $1 \in G$, with $a * 1 = 1 * a = a$ for any $a \in G$. Multiplication may or may not be commutative. Depending on the definition, the answer could be no in the case of the group with one element: then $1 = 0$. But the trivial ring is not a very interesting case. For cyclic groups the statement is certainly true, since $(\mathbb{Z}_n, +, \cdot)$ and $(\mathbb{Z}, +, \cdot)$ are both rings. What about in general? Is there some procedure to give arbitrary abelian groups ring structure?",,"['abstract-algebra', 'ring-theory']"
81,Quotient ring of Gaussian integers,Quotient ring of Gaussian integers,,"A very basic ring theory question, which I am not able to solve. How does one show that $\mathbb{Z}[i]/(3-i) \cong \mathbb{Z}/10\mathbb{Z}$. Extending the result: $\mathbb{Z}[i]/(a-ib) \cong \mathbb{Z}/(a^{2}+b^{2})\mathbb{Z}$, if $a,b$ are relatively prime. My attempt was to define a map, $\varphi:\mathbb{Z}[i] \to \mathbb{Z}/10\mathbb{Z}$ and show that the kernel is the ideal generated by $\langle{3-i\rangle}$. But I couldn't think of such a map. Anyhow, any ideas would be helpful.","A very basic ring theory question, which I am not able to solve. How does one show that $\mathbb{Z}[i]/(3-i) \cong \mathbb{Z}/10\mathbb{Z}$. Extending the result: $\mathbb{Z}[i]/(a-ib) \cong \mathbb{Z}/(a^{2}+b^{2})\mathbb{Z}$, if $a,b$ are relatively prime. My attempt was to define a map, $\varphi:\mathbb{Z}[i] \to \mathbb{Z}/10\mathbb{Z}$ and show that the kernel is the ideal generated by $\langle{3-i\rangle}$. But I couldn't think of such a map. Anyhow, any ideas would be helpful.",,"['abstract-algebra', 'ring-theory']"
82,How to show that the commutator subgroup is a normal subgroup,How to show that the commutator subgroup is a normal subgroup,,"It is suggested as an exercise in Serge Lang's book ""Algebra"" to show that the commutator subgroup $G^c$ of a group $G$ is a normal subgroup. I'd like to do that but I am afraid I need help, I think the first thing I need to figure out is how a general element in the commutator subgroup looks like, so that I can check that the defining condition for normality is satisfied. That is, supposing for a moment that a general element in $G^c$ is denoted by $g$, I need to show that $aga^{-1} \in G^c,$ for all $a \in G$. But here I get stuck, first because I am unsure how to write a general element in $G^c$ - a simple product in $G^c$ is of the form $xyx^{-1}y^{-1}aba^{-1}b^{-1}$ where $a,b \in G$. I cannot see a way to simplify this - I am sure there is one, but somehow I am blind today. The second thing then is, even if one tries out the conjugation of a simple element like $xyx^{-1}y^{-1}$ in $G^c$, again not simplification offers itself easily I think .. what am I missing ? An alternative would be to find a homomorphism of $G$ whose kernel is precisely $G^c$ - here I tried to think of this as a map $G \times G \to G$ but whatever I cook up is not a homomorphism. Thanks for your hints !!","It is suggested as an exercise in Serge Lang's book ""Algebra"" to show that the commutator subgroup $G^c$ of a group $G$ is a normal subgroup. I'd like to do that but I am afraid I need help, I think the first thing I need to figure out is how a general element in the commutator subgroup looks like, so that I can check that the defining condition for normality is satisfied. That is, supposing for a moment that a general element in $G^c$ is denoted by $g$, I need to show that $aga^{-1} \in G^c,$ for all $a \in G$. But here I get stuck, first because I am unsure how to write a general element in $G^c$ - a simple product in $G^c$ is of the form $xyx^{-1}y^{-1}aba^{-1}b^{-1}$ where $a,b \in G$. I cannot see a way to simplify this - I am sure there is one, but somehow I am blind today. The second thing then is, even if one tries out the conjugation of a simple element like $xyx^{-1}y^{-1}$ in $G^c$, again not simplification offers itself easily I think .. what am I missing ? An alternative would be to find a homomorphism of $G$ whose kernel is precisely $G^c$ - here I tried to think of this as a map $G \times G \to G$ but whatever I cook up is not a homomorphism. Thanks for your hints !!",,"['abstract-algebra', 'group-theory', 'normal-subgroups']"
83,Polynomials irreducible over $\mathbb{Q}$ but reducible over $\mathbb{F}_p$ for every prime $p$,Polynomials irreducible over  but reducible over  for every prime,\mathbb{Q} \mathbb{F}_p p,"Let $f(x) \in \mathbb{Z}[x]$. If we reduce the coefficents of $f(x)$ modulo $p$, where $p$ is prime, we get a polynomial $f^*(x) \in \mathbb{F}_p[x]$. Then if $f^*(x)$ is irreducible and has the same degree as $f(x)$, the polynomial $f(x)$ is irreducible. This is one way to show that a polynomial in $\mathbb{Z}[x]$ is irreducible, but it does not always work. There are polynomials which are irreducible in $\mathbb{Z}[x]$ yet factor in $\mathbb{F}_p[x]$ for every prime $p$. The only examples I know are $x^4 + 1$ and $x^4 - 10x^2 + 1$. I'd like to see more examples, in particular an infinite family of polynomials like this would be interesting. How does one go about finding them? Has anyone ever attempted classifying all polynomials in $\mathbb{Z}[x]$ with this property?","Let $f(x) \in \mathbb{Z}[x]$. If we reduce the coefficents of $f(x)$ modulo $p$, where $p$ is prime, we get a polynomial $f^*(x) \in \mathbb{F}_p[x]$. Then if $f^*(x)$ is irreducible and has the same degree as $f(x)$, the polynomial $f(x)$ is irreducible. This is one way to show that a polynomial in $\mathbb{Z}[x]$ is irreducible, but it does not always work. There are polynomials which are irreducible in $\mathbb{Z}[x]$ yet factor in $\mathbb{F}_p[x]$ for every prime $p$. The only examples I know are $x^4 + 1$ and $x^4 - 10x^2 + 1$. I'd like to see more examples, in particular an infinite family of polynomials like this would be interesting. How does one go about finding them? Has anyone ever attempted classifying all polynomials in $\mathbb{Z}[x]$ with this property?",,"['abstract-algebra', 'polynomials', 'field-theory', 'galois-theory', 'finite-fields']"
84,What is lost when we move from reals to complex numbers? [duplicate],What is lost when we move from reals to complex numbers? [duplicate],,"This question already has answers here : What do we lose passing from the reals to the complex numbers? (2 answers) Closed 6 years ago . As I know when you move to ""bigger"" number systems (such as from complex to quaternions) you lose some properties (e.g. moving from complex to quaternions requires loss of commutativity), but does it hold when you move for example from naturals to integers or from reals to complex and what properties do you lose?","This question already has answers here : What do we lose passing from the reals to the complex numbers? (2 answers) Closed 6 years ago . As I know when you move to ""bigger"" number systems (such as from complex to quaternions) you lose some properties (e.g. moving from complex to quaternions requires loss of commutativity), but does it hold when you move for example from naturals to integers or from reals to complex and what properties do you lose?",,"['abstract-algebra', 'complex-numbers']"
85,Does associativity imply commutativity?,Does associativity imply commutativity?,,"I used to think that commutativity and associativity are two distinct properties. But recently, I started thinking of something which has troubled this idea: $$(1+1)+1 = 1+ (1+1)\implies 2+1=1+2$$ Here using associativity of addition operation, we've shown commutativity. In general, $$\underbrace{(1+1+\dots+1)}_{a \, 1\text{'s }}\ \ + \ \ \underbrace{(1+1+\dots+1)}_{b \, 1\text{'s }}=\underbrace{(1+1+\dots+1)}_{b \, 1\text{'s }}\ \ + \ \ \underbrace{(1+1+\dots+1)}_{a \, 1\text{'s }} \\ \implies a+b=b+a$$ For any natural $a,b$. Hence using only associativity we prove commutativity. That this can be done, is disturbing me too much. Is this really correct? If yes, then are associativity and commutativity closely related? Or is it because of some other property of natural numbers? If yes, then can it be done for other structures as well?","I used to think that commutativity and associativity are two distinct properties. But recently, I started thinking of something which has troubled this idea: $$(1+1)+1 = 1+ (1+1)\implies 2+1=1+2$$ Here using associativity of addition operation, we've shown commutativity. In general, $$\underbrace{(1+1+\dots+1)}_{a \, 1\text{'s }}\ \ + \ \ \underbrace{(1+1+\dots+1)}_{b \, 1\text{'s }}=\underbrace{(1+1+\dots+1)}_{b \, 1\text{'s }}\ \ + \ \ \underbrace{(1+1+\dots+1)}_{a \, 1\text{'s }} \\ \implies a+b=b+a$$ For any natural $a,b$. Hence using only associativity we prove commutativity. That this can be done, is disturbing me too much. Is this really correct? If yes, then are associativity and commutativity closely related? Or is it because of some other property of natural numbers? If yes, then can it be done for other structures as well?",,"['abstract-algebra', 'elementary-number-theory', 'binary-operations', 'associativity']"
86,Does the $32$-inator exist?,Does the -inator exist?,32,"Background It is common popular-math knowledge that as we extend the real numbers to complex numbers, quaternions, octonions, sedenions, $32$ -nions, etc. using the Cayley-Dickson construction , we lose algebraic properties at each step such as commutativity, associativity, alternativity, etc. We can express this phenomenon in a more illustrative way. Consider the following multilinear maps $F_k$ of degree $k$ : Number two: $F_0: [\:] = 1 - (-1) = 2$ . Imaginary part: $F_1: [x] = x - x^*$ . Commutator: $F_2: [x, y] = xy - yx$ . Associator: $F_3: [x, y, z] = (xy)z - x(yz)$ . $16$ -inator: $F_4: [x, y, z, w] = (x(yz))w+(w(yz))x-(xy)(zw)-(wy)(zx)$ . They measure the failure of the algebra to be of characteristic two, Hermitian, commutative, associative and Moufang , respectively. All of them are well-known except for the last one, which I adapted from the quadrilinear map described in section 5 of this paper and essentially comes from a linearized Moufang identity. (Note that depending on the convention, imaginary part, commutator and associator are often defined with an extra factor of $1/2$ , and in the context of complex numbers, imaginary part almost always refers to $F_1$ divided by $2i$ ). Since the maps are multilinear, they are determined by their values at the basis elements $1=e_0, e_1, e_2, e_3, \ldots, e_{2^n-1}$ (under the standard labeling) in the $n$ th Cayley-Dickson algebra over the reals. All the maps above with degree $k \le n$ can then be checked to satisfy the following properties: (A1) $[e_a, e_b, e_c, \ldots] = m e_i$ for some $i$ , where $m\in\{-2,0,2\}$ . (A2) $[e_a, e_b, e_c, \ldots] = 0$ whenever $a, b, c, \ldots <2^{k-1}$ . (A3) $[e_1, e_2, e_4, \ldots, e_{2^{k-1}}] = 2 e_{2^{k}-1}$ . (A4) If $[x, y, \ldots] = z$ , then $[\sigma(x), \sigma(y), \ldots] = \sigma(z)$ for any algebra automorphism $\sigma$ . Properties (A2) and (A3) together imply (rather trivially) that given a Cayley-Dickson algebra $\mathbb{A}$ , $F_k$ vanishes in the next algebra $CD(\mathbb{A})$ if and only if both $F_k$ and $F_{k-1}$ vanish in $\mathbb{A}$ ; for $k < 4$ this fact holds in more generality for any $*$ -algebra (see e.g. here ) and provides a more uniform way to explain why commutativity, associativity, etc. break in sequence as we iterate the Cayley-Dickson process. But rather than talk about algebraic properties, for the purposes of this question I would like to treat the maps themselves as the objects of interest. So let these properties (and multilinearity) be the axioms that define a $2^n$ -inator in a real Cayley-Dickson algebra. The first four maps $F_0, \ldots, F_3$ are in fact uniquely determined by these axioms when $k=n$ . For example, (A3) implies $[\:]=2e_0=2$ ; (A2) and (A3) together with linearity imply $[a e_0+b e_1] = a[e_0]+b[e_1] = a\cdot 0 + b\cdot 2e_1 = 2b e_1$ ; the map $F_2$ in the quaternions is completely fixed by (A2), (A3) and (A4) applied twice, with an automorphism that cyclically permutes the imaginary basis elements and with the involution $e_1 \mapsto e_2, e_2 \mapsto e_1, e_3 \mapsto -e_3$ ; and similarly for $F_3$ in the octonions. I haven't checked whether the $16$ -inator in the sedenions is uniquely recovered from the axioms, but I suspect it's probably nonunique, since the automorphism groups of the sedenions and above are ""small"" relative to the size of the algebras (the groups are all isomorphic to the $14$ -dimensional octonionic automorphism group ( $G_2$ ) times a finite group, while the algebras themselves double their dimension at each step). Nevertheless, it is natural to ask if one can continue the sequence by finding nontrivial examples of $F_k$ for $k>4$ ; my main question deals with the smallest case $k=n=5$ . This paper implies that if a $F_5$ exists, it is necessarily not expressible in terms of multiplication and real constants alone, unlike $F_0, F_2, F_3$ and $F_4$ . There could still be a closed-form expression if we allow the use of conjugation as with $F_1$ , but it looks unlikely to me. The possibility of using non-real constants seems more promising, because the automorphism groups of sedenions and above do not act transitively on imaginary elements of norm $1$ , so some basis elements are ""more special"" than others. In fact, one can define new ""conjugations"" $x^{(8)} = e_{8} x e_{8}$ , $x^{(16)} = e_{16} x e_{16}$ , etc. which can be shown to be invariant under all automorphisms of the algebra; this follows from Lemma 2.1 here . An obvious followup to my main question would be whether the $2^n$ -inators can be expressed in terms of multiplication, real constants, and the conjugations $x^*, x^{(8)}, x^{(16)}, x^{(32)}, \ldots, x^{(2^n)}$ . But in principle it's not guaranteed that $F_n$ will be algebraically expressible at all, if it exists. Question My question is: Is there a $32$ -inator in the $32$ -nions, i.e. a multilinear map $F_5: [x,y,z,w,v]$ satisfying axioms (A1)-(A4)? Followup: can it be expressed in terms of multiplication, real constants and conjugations? Such a map would necessarily be nontrivial by (A3), and would be identically zero in any sedenion subalgebra by (A2) and (A4). Some remarks: In page 12 of the paper I linked above, it is suggested that the possible existence of higher-order maps might be related to projective geometry over the field $\mathbb{F}_2$ . The first problem looks simple enough that it could be solved with a computer search, but the search space seems to be quite large at first glance. The obvious upper bound on possible cases to check, taking into account only multilinearity and (A1), would be $(2\cdot 32 + 1)^{32^5} = 65^{2^{25}} \lesssim 2^{2^{28}}$ . Perhaps some clever argument could reduce this bound to a more manageable number.","Background It is common popular-math knowledge that as we extend the real numbers to complex numbers, quaternions, octonions, sedenions, -nions, etc. using the Cayley-Dickson construction , we lose algebraic properties at each step such as commutativity, associativity, alternativity, etc. We can express this phenomenon in a more illustrative way. Consider the following multilinear maps of degree : Number two: . Imaginary part: . Commutator: . Associator: . -inator: . They measure the failure of the algebra to be of characteristic two, Hermitian, commutative, associative and Moufang , respectively. All of them are well-known except for the last one, which I adapted from the quadrilinear map described in section 5 of this paper and essentially comes from a linearized Moufang identity. (Note that depending on the convention, imaginary part, commutator and associator are often defined with an extra factor of , and in the context of complex numbers, imaginary part almost always refers to divided by ). Since the maps are multilinear, they are determined by their values at the basis elements (under the standard labeling) in the th Cayley-Dickson algebra over the reals. All the maps above with degree can then be checked to satisfy the following properties: (A1) for some , where . (A2) whenever . (A3) . (A4) If , then for any algebra automorphism . Properties (A2) and (A3) together imply (rather trivially) that given a Cayley-Dickson algebra , vanishes in the next algebra if and only if both and vanish in ; for this fact holds in more generality for any -algebra (see e.g. here ) and provides a more uniform way to explain why commutativity, associativity, etc. break in sequence as we iterate the Cayley-Dickson process. But rather than talk about algebraic properties, for the purposes of this question I would like to treat the maps themselves as the objects of interest. So let these properties (and multilinearity) be the axioms that define a -inator in a real Cayley-Dickson algebra. The first four maps are in fact uniquely determined by these axioms when . For example, (A3) implies ; (A2) and (A3) together with linearity imply ; the map in the quaternions is completely fixed by (A2), (A3) and (A4) applied twice, with an automorphism that cyclically permutes the imaginary basis elements and with the involution ; and similarly for in the octonions. I haven't checked whether the -inator in the sedenions is uniquely recovered from the axioms, but I suspect it's probably nonunique, since the automorphism groups of the sedenions and above are ""small"" relative to the size of the algebras (the groups are all isomorphic to the -dimensional octonionic automorphism group ( ) times a finite group, while the algebras themselves double their dimension at each step). Nevertheless, it is natural to ask if one can continue the sequence by finding nontrivial examples of for ; my main question deals with the smallest case . This paper implies that if a exists, it is necessarily not expressible in terms of multiplication and real constants alone, unlike and . There could still be a closed-form expression if we allow the use of conjugation as with , but it looks unlikely to me. The possibility of using non-real constants seems more promising, because the automorphism groups of sedenions and above do not act transitively on imaginary elements of norm , so some basis elements are ""more special"" than others. In fact, one can define new ""conjugations"" , , etc. which can be shown to be invariant under all automorphisms of the algebra; this follows from Lemma 2.1 here . An obvious followup to my main question would be whether the -inators can be expressed in terms of multiplication, real constants, and the conjugations . But in principle it's not guaranteed that will be algebraically expressible at all, if it exists. Question My question is: Is there a -inator in the -nions, i.e. a multilinear map satisfying axioms (A1)-(A4)? Followup: can it be expressed in terms of multiplication, real constants and conjugations? Such a map would necessarily be nontrivial by (A3), and would be identically zero in any sedenion subalgebra by (A2) and (A4). Some remarks: In page 12 of the paper I linked above, it is suggested that the possible existence of higher-order maps might be related to projective geometry over the field . The first problem looks simple enough that it could be solved with a computer search, but the search space seems to be quite large at first glance. The obvious upper bound on possible cases to check, taking into account only multilinearity and (A1), would be . Perhaps some clever argument could reduce this bound to a more manageable number.","32 F_k k F_0: [\:] = 1 - (-1) = 2 F_1: [x] = x - x^* F_2: [x, y] = xy - yx F_3: [x, y, z] = (xy)z - x(yz) 16 F_4: [x, y, z, w] = (x(yz))w+(w(yz))x-(xy)(zw)-(wy)(zx) 1/2 F_1 2i 1=e_0, e_1, e_2, e_3, \ldots, e_{2^n-1} n k \le n [e_a, e_b, e_c, \ldots] = m e_i i m\in\{-2,0,2\} [e_a, e_b, e_c, \ldots] = 0 a, b, c, \ldots <2^{k-1} [e_1, e_2, e_4, \ldots, e_{2^{k-1}}] = 2 e_{2^{k}-1} [x, y, \ldots] = z [\sigma(x), \sigma(y), \ldots] = \sigma(z) \sigma \mathbb{A} F_k CD(\mathbb{A}) F_k F_{k-1} \mathbb{A} k < 4 * 2^n F_0, \ldots, F_3 k=n [\:]=2e_0=2 [a e_0+b e_1] = a[e_0]+b[e_1] = a\cdot 0 + b\cdot 2e_1 = 2b e_1 F_2 e_1 \mapsto e_2, e_2 \mapsto e_1, e_3 \mapsto -e_3 F_3 16 14 G_2 F_k k>4 k=n=5 F_5 F_0, F_2, F_3 F_4 F_1 1 x^{(8)} = e_{8} x e_{8} x^{(16)} = e_{16} x e_{16} 2^n x^*, x^{(8)}, x^{(16)}, x^{(32)}, \ldots, x^{(2^n)} F_n 32 32 F_5: [x,y,z,w,v] \mathbb{F}_2 (2\cdot 32 + 1)^{32^5} = 65^{2^{25}} \lesssim 2^{2^{28}}","['abstract-algebra', 'recreational-mathematics', 'multilinear-algebra', 'octonions', 'sedenions']"
87,Examples of finite nonabelian groups.,Examples of finite nonabelian groups.,,Can anybody provide some examples of finite nonabelian groups which are not symmetric groups or dihedral groups?,Can anybody provide some examples of finite nonabelian groups which are not symmetric groups or dihedral groups?,,"['abstract-algebra', 'group-theory', 'finite-groups', 'examples-counterexamples', 'big-list']"
88,"Why do books titled ""Abstract Algebra"" mostly deal with groups/rings/fields?","Why do books titled ""Abstract Algebra"" mostly deal with groups/rings/fields?",,"As a computer science graduate who had only a basic course in abstract algebra, I want to study some abstract algebra in my free time. I've been looking through some books on the topic, and most seem to 'only' cover groups, rings and fields. Why is this the case? It seems to me you'd want to study simpler  structures like semigroups, too. Especially looking at Wikipedia, there seems to be a huge zoo of different kinds of semigroups.","As a computer science graduate who had only a basic course in abstract algebra, I want to study some abstract algebra in my free time. I've been looking through some books on the topic, and most seem to 'only' cover groups, rings and fields. Why is this the case? It seems to me you'd want to study simpler  structures like semigroups, too. Especially looking at Wikipedia, there seems to be a huge zoo of different kinds of semigroups.",,"['abstract-algebra', 'soft-question', 'reference-request']"
89,"Proof of $(\mathbb{Z}/m\mathbb{Z}) \otimes_\mathbb{Z} (\mathbb{Z} / n \mathbb{Z}) \cong \mathbb{Z}/ \gcd(m,n)\mathbb{Z}$",Proof of,"(\mathbb{Z}/m\mathbb{Z}) \otimes_\mathbb{Z} (\mathbb{Z} / n \mathbb{Z}) \cong \mathbb{Z}/ \gcd(m,n)\mathbb{Z}","I've just started to learn about the tensor product and I want to show: $$(\mathbb{Z}/m\mathbb{Z}) \otimes_\mathbb{Z} (\mathbb{Z} / n \mathbb{Z}) \cong \mathbb{Z}/ \gcd(m,n)\mathbb{Z}.$$ Can you tell me if my proof is right: $\mathbb{Z}/m\mathbb{Z}$ and $\mathbb{Z} / n \mathbb{Z}$ are both finite free $\mathbb{Z}$-modules with the basis consisting of one single element $\{ 1 \}$. So $(\mathbb{Z}/m\mathbb{Z}) \otimes_\mathbb{Z} (\mathbb{Z} / n \mathbb{Z})$ has the basis $\{ 1 \otimes 1 \}$. Therefore, any element in $(\mathbb{Z}/m\mathbb{Z}) \otimes_\mathbb{Z} (\mathbb{Z} / n \mathbb{Z})$  is of the form $(ab) 1 \otimes 1$ and any element in $\mathbb{Z}/ \gcd(m,n)\mathbb{Z}$ is of the form $k 1 = k$ where $k \in \{ 0, \dots , \gcd(n,m) \}$. I would like to construct an isomorphism that maps $ab$ to some $k$. Let this map be $ab (1 \otimes 1) \mapsto ab \bmod \gcd(n,m)$. This is a homomorphism between modules: it maps $0$ to $0$ because it maps the empty sum to the empty sum. It also fulfills $f(a + b) = f(a) + f(b)$ because there is only one element, $a = 1$. It is surjective. So all I need to show is that it is injective. But that is clear too because if $ab \equiv 0 \bmod \gcd(m,n)$  then both $a \equiv 0 \bmod n$ and $b \equiv 0 \bmod m$ so the kernel is trivial. Many thanks for your help!!","I've just started to learn about the tensor product and I want to show: $$(\mathbb{Z}/m\mathbb{Z}) \otimes_\mathbb{Z} (\mathbb{Z} / n \mathbb{Z}) \cong \mathbb{Z}/ \gcd(m,n)\mathbb{Z}.$$ Can you tell me if my proof is right: $\mathbb{Z}/m\mathbb{Z}$ and $\mathbb{Z} / n \mathbb{Z}$ are both finite free $\mathbb{Z}$-modules with the basis consisting of one single element $\{ 1 \}$. So $(\mathbb{Z}/m\mathbb{Z}) \otimes_\mathbb{Z} (\mathbb{Z} / n \mathbb{Z})$ has the basis $\{ 1 \otimes 1 \}$. Therefore, any element in $(\mathbb{Z}/m\mathbb{Z}) \otimes_\mathbb{Z} (\mathbb{Z} / n \mathbb{Z})$  is of the form $(ab) 1 \otimes 1$ and any element in $\mathbb{Z}/ \gcd(m,n)\mathbb{Z}$ is of the form $k 1 = k$ where $k \in \{ 0, \dots , \gcd(n,m) \}$. I would like to construct an isomorphism that maps $ab$ to some $k$. Let this map be $ab (1 \otimes 1) \mapsto ab \bmod \gcd(n,m)$. This is a homomorphism between modules: it maps $0$ to $0$ because it maps the empty sum to the empty sum. It also fulfills $f(a + b) = f(a) + f(b)$ because there is only one element, $a = 1$. It is surjective. So all I need to show is that it is injective. But that is clear too because if $ab \equiv 0 \bmod \gcd(m,n)$  then both $a \equiv 0 \bmod n$ and $b \equiv 0 \bmod m$ so the kernel is trivial. Many thanks for your help!!",,"['abstract-algebra', 'proof-verification', 'modules', 'tensor-products']"
90,How was the Monster's existence originally suspected?,How was the Monster's existence originally suspected?,,"I've read in many places that the Monster group was suspected to exist before it was actually proven to exist, and further that many of its properties were deduced contingent upon existence. For example, in ncatlab's article , The Monster group was predicted to exist by Bernd Fischer and Robert Griess in 1973, as a simple group containing the Fischer groups and some other sporadic simple groups as subquotients. Subsequent work by Fischer, Conway, Norton and Thompson estimated the order of $M$ and discovered other properties and subgroups, assuming that it existed. Or Wikipedia , The Monster was predicted by Bernd Fischer (unpublished) and Robert Griess (1976) in about 1973 as a simple group containing a double cover of Fischer's baby monster group as a centralizer of an involution. Within a few months the order of $M$ was found by Griess using the Thompson order formula [...] Or The Spirit of Moonshine , Its existence is a non-trivial fact: when the original moonshine   conjectures were made, mathematicians suspected its existence, and had been able to work   out its character table, but could not prove it actually existed. They did know that the   dimensions of the smallest irreducible representations would be 1, 196883; and 21296876. It surprises me that this object could have been predicted before being rigorously discovered, due to it being often described as very complicated and highly nonobvious (or at least its construction). Take for instance the description in this AMS review of Moonshine Beyond the Monster : The proof of the moonshine conjectures depends on several coincidences. Even   the existence of the monster seems to be a ﬂuke in any of the known constructions:   these all depend on long, strange calculations that just happen to work for no   obvious reason, and would not have been done if the monster had not already been   suspected to exist. It'd be cool to be acquainted with this part of the story in some more detail at an accessible level, if possible, though I realize it may necessarily involve heavy machinery or convoluted calculations.","I've read in many places that the Monster group was suspected to exist before it was actually proven to exist, and further that many of its properties were deduced contingent upon existence. For example, in ncatlab's article , The Monster group was predicted to exist by Bernd Fischer and Robert Griess in 1973, as a simple group containing the Fischer groups and some other sporadic simple groups as subquotients. Subsequent work by Fischer, Conway, Norton and Thompson estimated the order of $M$ and discovered other properties and subgroups, assuming that it existed. Or Wikipedia , The Monster was predicted by Bernd Fischer (unpublished) and Robert Griess (1976) in about 1973 as a simple group containing a double cover of Fischer's baby monster group as a centralizer of an involution. Within a few months the order of $M$ was found by Griess using the Thompson order formula [...] Or The Spirit of Moonshine , Its existence is a non-trivial fact: when the original moonshine   conjectures were made, mathematicians suspected its existence, and had been able to work   out its character table, but could not prove it actually existed. They did know that the   dimensions of the smallest irreducible representations would be 1, 196883; and 21296876. It surprises me that this object could have been predicted before being rigorously discovered, due to it being often described as very complicated and highly nonobvious (or at least its construction). Take for instance the description in this AMS review of Moonshine Beyond the Monster : The proof of the moonshine conjectures depends on several coincidences. Even   the existence of the monster seems to be a ﬂuke in any of the known constructions:   these all depend on long, strange calculations that just happen to work for no   obvious reason, and would not have been done if the monster had not already been   suspected to exist. It'd be cool to be acquainted with this part of the story in some more detail at an accessible level, if possible, though I realize it may necessarily involve heavy machinery or convoluted calculations.",,"['abstract-algebra', 'group-theory', 'finite-groups', 'math-history']"
91,Is an automorphism of the field of real numbers the identity map?,Is an automorphism of the field of real numbers the identity map?,,"Is an automorphism of the field of real numbers $\mathbb{R}$ the identity map? If yes, how can we prove it? Remark An automorphism of $\mathbb{R}$ may not be continuous.","Is an automorphism of the field of real numbers $\mathbb{R}$ the identity map? If yes, how can we prove it? Remark An automorphism of $\mathbb{R}$ may not be continuous.",,"['abstract-algebra', 'field-theory', 'real-numbers']"
92,What are some mathematical topics that involve adding and multiplying pictures?,What are some mathematical topics that involve adding and multiplying pictures?,,"Let me give you an example of what I mean. Flag algebras are a tool used in extremal graph theory which involve writing inequalities that look like: (It's not too important to my question what this inequality means, but let me give you some context. Informally, the things we're adding and multiplying are probabilities that a random group of vertices in a large graph will induce some specific small subgraph. To make some manipulations rigorously justified, this is not precisely what we mean; instead, they are the limits of such probabilities over a convergent sequence of graphs.) Aside from being potentially useful in solving math problems I'm curious about, I enjoy using, thinking about, and even looking at statements about flag algebras, because these equations and inequalities just look so cool! Instead of multiplying, adding, and comparing letters and numbers, we get to do the same thing to pictures of things. So my question is: what are some other topics in mathematics where we get to do the same thing? Obviously, you can always give any name you like to a variable, like those math problems you see on facebook where cherry plus banana is equal to three times hamburger. I'm not interested in examples like this, because there's nothing special about those variable names. Instead I'm interested in cases satisfying the following conditions: Mathematicians actually working with these objects commonly represent the things they are adding or multiplying or whatever (in general, performing algebraic manipulations on) by pictures. The pictures used to represent these objects are actually helpful for understanding what the objects are. It's okay if it's not adding or multiplying specifically we're doing, as long as we're manipulating the pictures in ways traditionally reserved for numbers or variables. For example, the things represented by pictures could be elements of some algebraic object (group, ring, etc.)","Let me give you an example of what I mean. Flag algebras are a tool used in extremal graph theory which involve writing inequalities that look like: (It's not too important to my question what this inequality means, but let me give you some context. Informally, the things we're adding and multiplying are probabilities that a random group of vertices in a large graph will induce some specific small subgraph. To make some manipulations rigorously justified, this is not precisely what we mean; instead, they are the limits of such probabilities over a convergent sequence of graphs.) Aside from being potentially useful in solving math problems I'm curious about, I enjoy using, thinking about, and even looking at statements about flag algebras, because these equations and inequalities just look so cool! Instead of multiplying, adding, and comparing letters and numbers, we get to do the same thing to pictures of things. So my question is: what are some other topics in mathematics where we get to do the same thing? Obviously, you can always give any name you like to a variable, like those math problems you see on facebook where cherry plus banana is equal to three times hamburger. I'm not interested in examples like this, because there's nothing special about those variable names. Instead I'm interested in cases satisfying the following conditions: Mathematicians actually working with these objects commonly represent the things they are adding or multiplying or whatever (in general, performing algebraic manipulations on) by pictures. The pictures used to represent these objects are actually helpful for understanding what the objects are. It's okay if it's not adding or multiplying specifically we're doing, as long as we're manipulating the pictures in ways traditionally reserved for numbers or variables. For example, the things represented by pictures could be elements of some algebraic object (group, ring, etc.)",,"['abstract-algebra', 'soft-question', 'notation']"
93,What is the difference between a Ring and an Algebra?,What is the difference between a Ring and an Algebra?,,"In mathematics, I want to know what is indeed the difference between a ring and an algebra ?","In mathematics, I want to know what is indeed the difference between a ring and an algebra ?",,"['abstract-algebra', 'ring-theory', 'definition', 'algebras']"
94,"What do prime ideals in $k[x,y]$ look like?",What do prime ideals in  look like?,"k[x,y]","Suppose that $k$ is an algebraically closed field. Then what do the prime ideals in the polynomial ring $k[x,y]$ look like? As far as I know, the maximal ideals of $k[x,y]$ are of the form $(x-a,y-b)$ where $a,b\in k$. What can we say about the prime ideals? Are there similar results? And what about $k[x,y,z], k[x,y,z,w]$ and so on. Would someone be kind enough to give me some hints or referrence on this topic? Thank you very much!","Suppose that $k$ is an algebraically closed field. Then what do the prime ideals in the polynomial ring $k[x,y]$ look like? As far as I know, the maximal ideals of $k[x,y]$ are of the form $(x-a,y-b)$ where $a,b\in k$. What can we say about the prime ideals? Are there similar results? And what about $k[x,y,z], k[x,y,z,w]$ and so on. Would someone be kind enough to give me some hints or referrence on this topic? Thank you very much!",,"['abstract-algebra', 'ring-theory', 'ideals']"
95,Can someone explain the Yoneda Lemma to an applied mathematician?,Can someone explain the Yoneda Lemma to an applied mathematician?,,"I have trouble following the category-theoretic statement and proof of the Yoneda Lemma. Indeed, I followed a category theory course for 4-5 lectures (several years ago now) and felt like I understood everything until we covered the Yoneda Lemma, after which point I lost interest. I guess what I'm asking for are some concrete examples of the Yoneda Lemma in action. For example, how does it apply to specific categories, like a category with one element, or the category Grp or Set ? What results does it generalize? Is there a canonical route to understanding the statement of the Lemma? If you need to assume knowledge, then assume I have a fairly rigorous education in pure/applied mathematics at the undergraduate level but no further.","I have trouble following the category-theoretic statement and proof of the Yoneda Lemma. Indeed, I followed a category theory course for 4-5 lectures (several years ago now) and felt like I understood everything until we covered the Yoneda Lemma, after which point I lost interest. I guess what I'm asking for are some concrete examples of the Yoneda Lemma in action. For example, how does it apply to specific categories, like a category with one element, or the category Grp or Set ? What results does it generalize? Is there a canonical route to understanding the statement of the Lemma? If you need to assume knowledge, then assume I have a fairly rigorous education in pure/applied mathematics at the undergraduate level but no further.",,"['abstract-algebra', 'category-theory', 'intuition']"
96,Intuition on group homomorphisms,Intuition on group homomorphisms,,"So I'm studying for finals now, and came across the idea of homomorphisms again. This is not a new idea for me at all, having seen them in groups, rings, fields ect. However, on reevaluating them I realized suddenly that I really don't understand them on the same level as I thought. While isomorphisms seem to be very natural to think about, I can't visualize what's happening in homomorphisms. At this time, I only have a sketchy idea that the existence of a homomorphism between groups (or between groups and what they're acting on) means that they both somehow combine the same way.. Does someone mind sharing their intuition on the concept? As always, any help is appreciated, thanks","So I'm studying for finals now, and came across the idea of homomorphisms again. This is not a new idea for me at all, having seen them in groups, rings, fields ect. However, on reevaluating them I realized suddenly that I really don't understand them on the same level as I thought. While isomorphisms seem to be very natural to think about, I can't visualize what's happening in homomorphisms. At this time, I only have a sketchy idea that the existence of a homomorphism between groups (or between groups and what they're acting on) means that they both somehow combine the same way.. Does someone mind sharing their intuition on the concept? As always, any help is appreciated, thanks",,"['abstract-algebra', 'group-theory']"
97,Characterizing units in polynomial rings,Characterizing units in polynomial rings,,"I am trying to prove a result, for which I have got one part, but I am not able to get the converse part. Theorem. Let $R$ be a commutative ring with $1$. Then $f(X)=a_{0}+a_{1}X+a_{2}X^{2} + \cdots + a_{n}X^{n}$ is a unit in $R[X]$ if and only if $a_{0}$ is a unit in $R$ and $a_{1},a_{2},\dots,a_{n}$ are all nilpotent in $R$. Proof. Suppose $f(X)=a_{0}+a_{1}X+\cdots +a_{n}X^{n}$ is such that $a_{0}$ is a unit in $R$ and $a_{1},a_{2}, \dots,a_{r}$ are all nilpotent in $R$. Since $R$ is commutative, we get that $a_{1}X,a_{2}X^{2},\cdots,a_{n}X^{n}$ are all nilpotent and hence also their sum is nilpotent. Let $z = \sum a_{i}X^{i}$ then $a_{0}^{-1}z$ is nilpotent and so $1+a_{0}^{-1}z$ is a unit. Thus $f(X)=a_{0}+z=a_{0} \cdot (1+a_{0}^{-1}z)$ is a unit since product of two units in $R[X]$ is a unit. I have not been able to get the converse part and would like to see the proof for the converse part.","I am trying to prove a result, for which I have got one part, but I am not able to get the converse part. Theorem. Let $R$ be a commutative ring with $1$. Then $f(X)=a_{0}+a_{1}X+a_{2}X^{2} + \cdots + a_{n}X^{n}$ is a unit in $R[X]$ if and only if $a_{0}$ is a unit in $R$ and $a_{1},a_{2},\dots,a_{n}$ are all nilpotent in $R$. Proof. Suppose $f(X)=a_{0}+a_{1}X+\cdots +a_{n}X^{n}$ is such that $a_{0}$ is a unit in $R$ and $a_{1},a_{2}, \dots,a_{r}$ are all nilpotent in $R$. Since $R$ is commutative, we get that $a_{1}X,a_{2}X^{2},\cdots,a_{n}X^{n}$ are all nilpotent and hence also their sum is nilpotent. Let $z = \sum a_{i}X^{i}$ then $a_{0}^{-1}z$ is nilpotent and so $1+a_{0}^{-1}z$ is a unit. Thus $f(X)=a_{0}+z=a_{0} \cdot (1+a_{0}^{-1}z)$ is a unit since product of two units in $R[X]$ is a unit. I have not been able to get the converse part and would like to see the proof for the converse part.",,"['abstract-algebra', 'polynomials']"
98,Is there an intuitive reason for a certain operation to be associative?,Is there an intuitive reason for a certain operation to be associative?,,"When I read Pinter's A Book of Abstract Algebra , Exercise 7 on page 25 asks whether the operation $$x*y=\frac{xy}{x+y+1}$$ (defined on the positive real numbers) is associative. At first I considered this to be false, because the expression is so complicated. But when I worked out $(x*y)*z$ and $x*(y*z)$, I found both to be $$\frac{xyz}{xy+yz+zx+x+y+z+1}!$$ Commutativity is easy to see. But associativity can be so counter-intuitive! Can you see this operation is associative without working it out? Are there tricks to do this?","When I read Pinter's A Book of Abstract Algebra , Exercise 7 on page 25 asks whether the operation $$x*y=\frac{xy}{x+y+1}$$ (defined on the positive real numbers) is associative. At first I considered this to be false, because the expression is so complicated. But when I worked out $(x*y)*z$ and $x*(y*z)$, I found both to be $$\frac{xyz}{xy+yz+zx+x+y+z+1}!$$ Commutativity is easy to see. But associativity can be so counter-intuitive! Can you see this operation is associative without working it out? Are there tricks to do this?",,"['abstract-algebra', 'intuition', 'associativity']"
99,Non-associative operations,Non-associative operations,,There are lots of operations that are not commutative. I'm looking for striking counter-examples of operations that are not associative . Or may associativity be genuinely built-in the concept of an operation? May non-associative operations be of genuinely lesser importance? Which role do algebraic structures with non-associative operations play? There's a big gap between commutative and non-commuative algebraic structures (e.g. Abelian vs. non-Abelian groups or categories). Both kinds of algebraic structures are of equal importance. Does the same hold for  assosiative vs. non-associative algebraic structures?,There are lots of operations that are not commutative. I'm looking for striking counter-examples of operations that are not associative . Or may associativity be genuinely built-in the concept of an operation? May non-associative operations be of genuinely lesser importance? Which role do algebraic structures with non-associative operations play? There's a big gap between commutative and non-commuative algebraic structures (e.g. Abelian vs. non-Abelian groups or categories). Both kinds of algebraic structures are of equal importance. Does the same hold for  assosiative vs. non-associative algebraic structures?,,"['abstract-algebra', 'examples-counterexamples', 'big-list']"
