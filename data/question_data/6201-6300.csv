,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,The spectrum of a self-adjoint operator on $\mathcal l^2$,The spectrum of a self-adjoint operator on,\mathcal l^2,"Let $S$ be the unilateral shift operator on $\mathcal l^2$ (which shifts one place to the right) and $S^*$ its adjoint, the backward shift (which shifts one place to the left). I've been trying to find the spectrum of $T=S+S^*$. Since $T$ is self-adjoint, any eigenvalues would be real, and I've shown that no $|\lambda|\ge 2$ can be an eigenvalue. The case $|\lambda|< 2$ corresponds to the roots of $ t^2=\lambda t-1 $ being complex (this is the characteristic polynomial corresponding to the recurrence relation $a_n=\lambda a_{n-1}+a_{n-2}$). These complex roots have absolute value 1, and this implies that $a_n$ does not converge to zero. So I've shown that $T$ has no eigenvalues. But  I'm still looking for an elementary way to find that the spectrum is $[-2,2]$, as answered by Joel below. Although I appreciate Joel's answer below and I'm sure it'll be valuable for many in the community, unfortunately I only know basic Hilbert space theory. Any hints would be very helpful!","Let $S$ be the unilateral shift operator on $\mathcal l^2$ (which shifts one place to the right) and $S^*$ its adjoint, the backward shift (which shifts one place to the left). I've been trying to find the spectrum of $T=S+S^*$. Since $T$ is self-adjoint, any eigenvalues would be real, and I've shown that no $|\lambda|\ge 2$ can be an eigenvalue. The case $|\lambda|< 2$ corresponds to the roots of $ t^2=\lambda t-1 $ being complex (this is the characteristic polynomial corresponding to the recurrence relation $a_n=\lambda a_{n-1}+a_{n-2}$). These complex roots have absolute value 1, and this implies that $a_n$ does not converge to zero. So I've shown that $T$ has no eigenvalues. But  I'm still looking for an elementary way to find that the spectrum is $[-2,2]$, as answered by Joel below. Although I appreciate Joel's answer below and I'm sure it'll be valuable for many in the community, unfortunately I only know basic Hilbert space theory. Any hints would be very helpful!",,"['real-analysis', 'functional-analysis', 'operator-theory', 'hilbert-spaces', 'banach-spaces']"
1,"How to evaluate $\log x$ to high precision ""by hand""","How to evaluate  to high precision ""by hand""",\log x,"I want to prove $$\log 2<\frac{253}{365}.$$ This evaluates to $0.693147\ldots<0.693151\ldots$, so it checks out. (The source of this otherwise obscure numerical problem is in the verification of the Birthday problem .) If you had to do the calculation by hand, what method or series would you use to minimize the number of operations and/or size of the arguments involved in order to get this result? I'm doing a formal computer proof, so it's not exactly by hand, but I still want to minimize the number of evaluations needed to get to this result. One method is by rewriting it into $2<e^{253/365}$ and using the power series; since it is a positive series you know you can stop once you have exceeded $2$. Working this out, it seems you need the terms $n=0,\dots,7$ of the sum, and then it works out to $$2<\sum_{n=0}^7\frac{(253/365)^n}{n!}=\frac{724987549742673918011}{362492763907870312500},$$ which involves rather larger numbers than I'd like. There is also the limit $(1+\frac{253}{365n})^n\to$ $e^{253/365}$, but the convergence on this is not so good; it doesn't get past $2$ until $n=68551$, at which point we are talking about numbers with $507162$ digits. For $\log 2$ there is of course the terribly converging alternate series $\log 2=\sum_{n=1}^\infty\frac{-(-1)^n}n$, which requires $71339$ terms to get the desired bound. This can be improved by pushing the series into its geometrically convergent region as $\log 2=2\log{\sqrt 2}=-2\sum_{n=1}^\infty \frac{(1-\sqrt2)^n}n$, but now there is the added complication of estimating $\sqrt 2$ to sufficient precision. Assuming that $\sqrt 2$ is known exactly, you need to take this series out to $12$ terms, at which point we are verifying $$\frac{1959675656 \sqrt2-2771399891}{1011780}<0\Leftarrow2771399891^2>1959675656^2\cdot 2.$$ What other methods are there to do a calculation like this? Is there a way to use a root-finding method like Newton's to get a strict bound out with fast convergence?","I want to prove $$\log 2<\frac{253}{365}.$$ This evaluates to $0.693147\ldots<0.693151\ldots$, so it checks out. (The source of this otherwise obscure numerical problem is in the verification of the Birthday problem .) If you had to do the calculation by hand, what method or series would you use to minimize the number of operations and/or size of the arguments involved in order to get this result? I'm doing a formal computer proof, so it's not exactly by hand, but I still want to minimize the number of evaluations needed to get to this result. One method is by rewriting it into $2<e^{253/365}$ and using the power series; since it is a positive series you know you can stop once you have exceeded $2$. Working this out, it seems you need the terms $n=0,\dots,7$ of the sum, and then it works out to $$2<\sum_{n=0}^7\frac{(253/365)^n}{n!}=\frac{724987549742673918011}{362492763907870312500},$$ which involves rather larger numbers than I'd like. There is also the limit $(1+\frac{253}{365n})^n\to$ $e^{253/365}$, but the convergence on this is not so good; it doesn't get past $2$ until $n=68551$, at which point we are talking about numbers with $507162$ digits. For $\log 2$ there is of course the terribly converging alternate series $\log 2=\sum_{n=1}^\infty\frac{-(-1)^n}n$, which requires $71339$ terms to get the desired bound. This can be improved by pushing the series into its geometrically convergent region as $\log 2=2\log{\sqrt 2}=-2\sum_{n=1}^\infty \frac{(1-\sqrt2)^n}n$, but now there is the added complication of estimating $\sqrt 2$ to sufficient precision. Assuming that $\sqrt 2$ is known exactly, you need to take this series out to $12$ terms, at which point we are verifying $$\frac{1959675656 \sqrt2-2771399891}{1011780}<0\Leftarrow2771399891^2>1959675656^2\cdot 2.$$ What other methods are there to do a calculation like this? Is there a way to use a root-finding method like Newton's to get a strict bound out with fast convergence?",,"['real-analysis', 'inequality', 'approximation']"
2,$\int_{0}^{\infty}e^{-st}h(t)dt=0 \Rightarrow h(t)=0.$,,\int_{0}^{\infty}e^{-st}h(t)dt=0 \Rightarrow h(t)=0.,"Suppose $h(t)$ is continuous function and $\int_{0}^{\infty}e^{-st}h(t)dt=0 ~\forall~ s>s_{0}$, then prove that $h(t)=0$. I know ""if a function is continuous, non-negative or non-positive, and its integration is zero, then function must be zero"", which is intuitively clear. But here asked question is beyond my knowledge. Here, exponential function is doing some miracle, but how? Would you like to help me?","Suppose $h(t)$ is continuous function and $\int_{0}^{\infty}e^{-st}h(t)dt=0 ~\forall~ s>s_{0}$, then prove that $h(t)=0$. I know ""if a function is continuous, non-negative or non-positive, and its integration is zero, then function must be zero"", which is intuitively clear. But here asked question is beyond my knowledge. Here, exponential function is doing some miracle, but how? Would you like to help me?",,"['real-analysis', 'integration', 'improper-integrals', 'intuition']"
3,Uniformly convergent implies equicontinuous,Uniformly convergent implies equicontinuous,,"I'm trying to prove that if I have a sequence of continuously differentiable functions $f_n$ that converge uniformly on $[a,b]$ , then $\{f_n\}$ is equicontinuous for all $x_0 \in [a, b]$ . My idea is to use uniform convergence to deal with the ""tail"" and then use continuity to deal with the finitely many $f_n$ 's left. But I'm having trouble writing it down.","I'm trying to prove that if I have a sequence of continuously differentiable functions that converge uniformly on , then is equicontinuous for all . My idea is to use uniform convergence to deal with the ""tail"" and then use continuity to deal with the finitely many 's left. But I'm having trouble writing it down.","f_n [a,b] \{f_n\} x_0 \in [a, b] f_n","['real-analysis', 'uniform-convergence', 'sequence-of-function', 'equicontinuity']"
4,$\int_0^\infty \frac{1}{x^2}\left( \left(\sum_{n=1}^\infty\sin\left(\frac{x}{2^n}\right)\right)-\sin(x)\right)\ dx$,,\int_0^\infty \frac{1}{x^2}\left( \left(\sum_{n=1}^\infty\sin\left(\frac{x}{2^n}\right)\right)-\sin(x)\right)\ dx,"While I was working on my stuff, another question suddenly came to mind, the one you see below $$\int_0^\infty \frac{ \left(\sum_{n=1}^\infty\sin\left(\frac{x}{2^n}\right)\right)-\sin(x)}{x^2} \ dx$$ Which way should I look at this integral?","While I was working on my stuff, another question suddenly came to mind, the one you see below $$\int_0^\infty \frac{ \left(\sum_{n=1}^\infty\sin\left(\frac{x}{2^n}\right)\right)-\sin(x)}{x^2} \ dx$$ Which way should I look at this integral?",,"['calculus', 'real-analysis', 'integration', 'definite-integrals', 'improper-integrals']"
5,"If $f$ is continuous on $[a,b)$ and $[b,c]$, then $f$ is Riemann integrable on $[a,c]$.","If  is continuous on  and , then  is Riemann integrable on .","f [a,b) [b,c] f [a,c]","True or False: If $f$ is continuous on $[a, b)$ and on $[b, c]$, then $f$ is Riemann integrable on $[a, c]$. I was unsure if the $)$ in $[a,b)$ completely changed the problem and made it false and I should be looking for a counterexample or if the following attempt worked. Since $f$ is continuous on $[a, b)$, then $f$ is integrable on $[a, b)$. Similarly, $f$ is integrable on $[b, c]$. Therefore by definition, let P$_1$ be a partition of $[a, b]$ s.t. $U(f, P_1) − L(f, P_1) < \frac{\epsilon}{2}$ and $P_2$ be a partition of $[b, c]$ s.t $U(f, P_2) − L(f, P_2) < \frac{\epsilon}{2}$. Therefore, $P = P_1 \cup P_2$ is a partition of $[a, c]$. Since $b$ is the right endpoint of $P_1$ and the left endpoint of $P_2$, then $U(f, P) − L(f, P) = U(f, P_1) + U(f, P_2)−L(f, P_1)−L(f, P_2) = U(f, P_1)−L(f, P_1) +U(f, P_2)−L(f, P_2)< \frac{\epsilon}{2}+ \frac{\epsilon}{2} = \epsilon$. So, $f$ is integrable on $[a, c]$. Does this work? Or, is there a counterexample because $[a,b)$ isn't $[a,b]$?","True or False: If $f$ is continuous on $[a, b)$ and on $[b, c]$, then $f$ is Riemann integrable on $[a, c]$. I was unsure if the $)$ in $[a,b)$ completely changed the problem and made it false and I should be looking for a counterexample or if the following attempt worked. Since $f$ is continuous on $[a, b)$, then $f$ is integrable on $[a, b)$. Similarly, $f$ is integrable on $[b, c]$. Therefore by definition, let P$_1$ be a partition of $[a, b]$ s.t. $U(f, P_1) − L(f, P_1) < \frac{\epsilon}{2}$ and $P_2$ be a partition of $[b, c]$ s.t $U(f, P_2) − L(f, P_2) < \frac{\epsilon}{2}$. Therefore, $P = P_1 \cup P_2$ is a partition of $[a, c]$. Since $b$ is the right endpoint of $P_1$ and the left endpoint of $P_2$, then $U(f, P) − L(f, P) = U(f, P_1) + U(f, P_2)−L(f, P_1)−L(f, P_2) = U(f, P_1)−L(f, P_1) +U(f, P_2)−L(f, P_2)< \frac{\epsilon}{2}+ \frac{\epsilon}{2} = \epsilon$. So, $f$ is integrable on $[a, c]$. Does this work? Or, is there a counterexample because $[a,b)$ isn't $[a,b]$?",,"['real-analysis', 'examples-counterexamples', 'riemann-sum', 'partitions-for-integration']"
6,Rolle theorem proof via intermediate value theorem,Rolle theorem proof via intermediate value theorem,,"I'm wondering if we can use the Intermediate Value Theorem to prove Rolle's Theorem . The hypotheses of Rolle's Theorem are: The function should be continuous on a closed interval $[a,b]$ . The function should be differentiable on the open interval $(a,b)$ . $f(a)=f(b)$ The theorem then shows that there exist $c$ in $(a,b)$ such that $f'(c)=0$ . For I.V.T we need the function to be continuous and $f'(a).f'(b) \leq 0$ . If the conditions for Rolle's Theorem conditions are achieved, does it mean that $f'$ is continuous? For the second condition we have that $$ \frac{f(x)-f(a)}{x-a} .\frac{f(x)-f(b)}{x-b}<0$$ because $$ f(x)-f(a)=f(x)-f(b)$$ and $$ 0<x-a<b-a $$ $$ a-b<x-b<0 $$","I'm wondering if we can use the Intermediate Value Theorem to prove Rolle's Theorem . The hypotheses of Rolle's Theorem are: The function should be continuous on a closed interval . The function should be differentiable on the open interval . The theorem then shows that there exist in such that . For I.V.T we need the function to be continuous and . If the conditions for Rolle's Theorem conditions are achieved, does it mean that is continuous? For the second condition we have that because and","[a,b] (a,b) f(a)=f(b) c (a,b) f'(c)=0 f'(a).f'(b) \leq 0 f'  \frac{f(x)-f(a)}{x-a} .\frac{f(x)-f(b)}{x-b}<0  f(x)-f(a)=f(x)-f(b)  0<x-a<b-a   a-b<x-b<0 ","['real-analysis', 'calculus', 'derivatives', 'continuity', 'rolles-theorem']"
7,"How to compute $\int_0^{\infty} x^{t-1} e^{-x}\ln(x)\,dx$?",How to compute ?,"\int_0^{\infty} x^{t-1} e^{-x}\ln(x)\,dx","I have hit the following integral (in the process of trying to derive a finite-sample correction for the Maximum Likelihood fitting of the Generalized Extreme Value distribution...): $$\int_0^{\infty} x^{t-1}  e^{-x}\ln(x)\,dx$$ It's a sort of cross between a definition of the $\Gamma$ function and an Euler-Mascheroni integral. Help would be greatly appreciated! Thanks in advance.","I have hit the following integral (in the process of trying to derive a finite-sample correction for the Maximum Likelihood fitting of the Generalized Extreme Value distribution...): $$\int_0^{\infty} x^{t-1}  e^{-x}\ln(x)\,dx$$ It's a sort of cross between a definition of the $\Gamma$ function and an Euler-Mascheroni integral. Help would be greatly appreciated! Thanks in advance.",,"['calculus', 'real-analysis', 'integration', 'definite-integrals', 'improper-integrals']"
8,Is a multivariable function continuous iff it is continuous with respect to each variable?,Is a multivariable function continuous iff it is continuous with respect to each variable?,,"I am very uncertain when it comes to understanding the continuity of multivariable functions. If we have, for example, a function $f: \mathbb{R}^{4} \to \mathbb{R}$ , and we denote the four variables $x,y,z,w$ , are the following statements equivalent? i) $f$ is continuous ii) $f|_{x}, f|_{y}, f|_{z}, f|_{w}$ are each continuous Here, $f|_{x}$ stands for the function attained by fixing the variables $w,y,z$ .  I hope this question makes sense.","I am very uncertain when it comes to understanding the continuity of multivariable functions. If we have, for example, a function , and we denote the four variables , are the following statements equivalent? i) is continuous ii) are each continuous Here, stands for the function attained by fixing the variables .  I hope this question makes sense.","f: \mathbb{R}^{4} \to \mathbb{R} x,y,z,w f f|_{x}, f|_{y}, f|_{z}, f|_{w} f|_{x} w,y,z","['real-analysis', 'multivariable-calculus', 'continuity']"
9,Elementary ways to calculate the arc length of the Cantor function (and singular function in general),Elementary ways to calculate the arc length of the Cantor function (and singular function in general),,"Cantor's function: http://en.wikipedia.org/wiki/Cantor_function There is an elementary way to prove that the arc length of the Cantor function is 2? In this article ( http://www.math.helsinki.fi/analysis/seminar/esitelmat/stat0312.pdf ) they use the following result: If $f:[a,b] \rightarrow \mathbb{R}$ is a continuous monotone function, then $f$ is singular if and only if   $$L_a^b = |f(a)-f(b)|+|a-b|$$ But, there is a way for calculate the arc length of singular function without using this property? like using the arc length definition If $X$ is a metric space with metric $d$, then we can define the ''length'' of a curve $\!\,\gamma : [a, b] \rightarrow X$ by $$\text{length} (\gamma)=\sup \left\{ \sum_{i=1}^n d(\gamma(t_i),\gamma(t_{i-1})) : n \in \mathbb{N} \text{ and } a = t_0 < t_1 < \cdots < t_n = b \right\}. $$ where the sup is over all $n$ and all partitions $t_0 < t_1 < \cdots < t_n$ of $[a, b]$.","Cantor's function: http://en.wikipedia.org/wiki/Cantor_function There is an elementary way to prove that the arc length of the Cantor function is 2? In this article ( http://www.math.helsinki.fi/analysis/seminar/esitelmat/stat0312.pdf ) they use the following result: If $f:[a,b] \rightarrow \mathbb{R}$ is a continuous monotone function, then $f$ is singular if and only if   $$L_a^b = |f(a)-f(b)|+|a-b|$$ But, there is a way for calculate the arc length of singular function without using this property? like using the arc length definition If $X$ is a metric space with metric $d$, then we can define the ''length'' of a curve $\!\,\gamma : [a, b] \rightarrow X$ by $$\text{length} (\gamma)=\sup \left\{ \sum_{i=1}^n d(\gamma(t_i),\gamma(t_{i-1})) : n \in \mathbb{N} \text{ and } a = t_0 < t_1 < \cdots < t_n = b \right\}. $$ where the sup is over all $n$ and all partitions $t_0 < t_1 < \cdots < t_n$ of $[a, b]$.",,"['real-analysis', 'arc-length']"
10,Can the proof of fixed point theorems ever be constructive?,Can the proof of fixed point theorems ever be constructive?,,"Overall, Brouwer fixed point theorem and Kakutani fixed theorem are non-constructive. Is there any established paper that demonstrates that there exists constructive proofs that do exactly what these theorems do?","Overall, Brouwer fixed point theorem and Kakutani fixed theorem are non-constructive. Is there any established paper that demonstrates that there exists constructive proofs that do exactly what these theorems do?",,"['real-analysis', 'logic', 'fixed-point-theorems', 'constructive-mathematics']"
11,If $f^3=\rm id$ then it is identity function [duplicate],If  then it is identity function [duplicate],f^3=\rm id,This question already has answers here : 3rd iterate of a continuous function equals identity function (3 answers) Closed 9 years ago . Let $f:\mathbb{R}\to \mathbb{R}$ be a continuous function such that $f^3(x)=f\circ f\circ f(x)=x$ for all $x$. How can I prove $f$ is the identity function?,This question already has answers here : 3rd iterate of a continuous function equals identity function (3 answers) Closed 9 years ago . Let $f:\mathbb{R}\to \mathbb{R}$ be a continuous function such that $f^3(x)=f\circ f\circ f(x)=x$ for all $x$. How can I prove $f$ is the identity function?,,"['real-analysis', 'analysis']"
12,Pointwise converging subsequence of a sequence of functions on countable set,Pointwise converging subsequence of a sequence of functions on countable set,,"Let $A \subset \mathbb R$ be countable and let $f_n: A\to \mathbb R,\,$ $n\in\mathbb N,\,$ be a sequence of functions, such that there exists $M \ge 0$ with $\,\lvert\,f_n(x)\rvert\le M,\,$ for all $n\in\mathbb N$ and $x\in A.\,$ Show that there exists a subsequence $f_{n_k}$ of $f_n$ which converges pointwise, i.e., $\{f_{n_k}(x)\}$ converges for every $x\in A$ . Here is what I have so far: If $A = \{a_1, a_2, \dots \}$ then $f_n(a_1)$ is a bounded sequence hence by Bolzano Weierstrass theorem contains a convergent subsequence $f_{n_{k_1}}$ . By the same argument $f_{n_{k_1}}(a_2)$ contains a convergent subsequence $f_{n_{k_2}}$ . Next I want to define $$f_{n_k} (x) = \lim_{j \to \infty} f_{n_{k_j}}(x),$$ the pointwise limit. Then $f_{n_k}(a_j)$ converges for every $a_j \in A$ (it's clear by how it was defined). Am I done now or am I missing something? Is there anything left to show?","Let be countable and let be a sequence of functions, such that there exists with for all and Show that there exists a subsequence of which converges pointwise, i.e., converges for every . Here is what I have so far: If then is a bounded sequence hence by Bolzano Weierstrass theorem contains a convergent subsequence . By the same argument contains a convergent subsequence . Next I want to define the pointwise limit. Then converges for every (it's clear by how it was defined). Am I done now or am I missing something? Is there anything left to show?","A \subset \mathbb R f_n: A\to \mathbb R,\, n\in\mathbb N,\, M \ge 0 \,\lvert\,f_n(x)\rvert\le M,\, n\in\mathbb N x\in A.\, f_{n_k} f_n \{f_{n_k}(x)\} x\in A A = \{a_1, a_2, \dots \} f_n(a_1) f_{n_{k_1}} f_{n_{k_1}}(a_2) f_{n_{k_2}} f_{n_k} (x) = \lim_{j \to \infty} f_{n_{k_j}}(x), f_{n_k}(a_j) a_j \in A","['real-analysis', 'sequences-and-series', 'limits', 'convergence-divergence']"
13,Prove that the limit of two consecutive fibonacci numbers EXISTS. [duplicate],Prove that the limit of two consecutive fibonacci numbers EXISTS. [duplicate],,"This question already has answers here : How to prove that $\lim \limits_{n\rightarrow \infty} \frac{F_{n+1}}{F_n}=\frac{\sqrt{5}+1}{2}$ (4 answers) Closed 10 years ago . Using the definition of Fibonacci numbers, $F_n=F_{n-1}+F_{n-2}$, I can prove that the limit of $\frac{F_{n+1}}{F_n}$ as $n\to\infty$ is $\phi$ if we assume that the limit exists. How can we prove that the limit does in fact exist? Is there more than one method? I do not think this is a duplicate. Please can someone explicitly show how to split the odd and even terms of this ratio sequence into two sequences- one monotonic increasing and one monotonic decreasing- and given that all ratios are between 1 and 2, show that the limit exists and we do not oscillate forever. Most of the question has been answered. I have shown that there are two subsequences- one increasing and bounded above by 2 and one decreasing, bounded below by 1. Using the fact that the limit exists, I can show it has value $\phi$. But how can I show the limit is the same for both subsequences?!","This question already has answers here : How to prove that $\lim \limits_{n\rightarrow \infty} \frac{F_{n+1}}{F_n}=\frac{\sqrt{5}+1}{2}$ (4 answers) Closed 10 years ago . Using the definition of Fibonacci numbers, $F_n=F_{n-1}+F_{n-2}$, I can prove that the limit of $\frac{F_{n+1}}{F_n}$ as $n\to\infty$ is $\phi$ if we assume that the limit exists. How can we prove that the limit does in fact exist? Is there more than one method? I do not think this is a duplicate. Please can someone explicitly show how to split the odd and even terms of this ratio sequence into two sequences- one monotonic increasing and one monotonic decreasing- and given that all ratios are between 1 and 2, show that the limit exists and we do not oscillate forever. Most of the question has been answered. I have shown that there are two subsequences- one increasing and bounded above by 2 and one decreasing, bounded below by 1. Using the fact that the limit exists, I can show it has value $\phi$. But how can I show the limit is the same for both subsequences?!",,"['real-analysis', 'fibonacci-numbers', 'golden-ratio']"
14,A proof of a theorem of Liouville,A proof of a theorem of Liouville,,"I need some reference for the proof of the following theorem attributed to Liouville: Theorem. Let $f(x):\Omega\longrightarrow \mathbb R^n$ be a $C^2$ function where $\Omega$ is an open subset of $\mathbb R^n$ and assume that $$ \textrm{div}\, f=\sum_{i=1}^n\frac{\partial f_i}{\partial x_i}=0. $$ If $\varphi$ is the flow  of the differential equation $y'=f(y)$ and we consider the homeomorphism $\pi_t:\Omega\longrightarrow\Omega$, such that  $$ x\,\longmapsto\, \pi_t(x):=\varphi(x,t), $$  then the map $\pi_t$ preserves the Lebesgue measure of every measurable subset of $\Omega$. To be more precise, for every $\mu$-measurable subset $D\subseteq \Omega$ we have that $\mu(D)=\mu\big(\pi_t(D)\big)$. The above theorem is very famous and its simpler form applied to Hamiltonian systems is often cited in texts of mechanics. However I need a proof of the general statement. Thanks in advance","I need some reference for the proof of the following theorem attributed to Liouville: Theorem. Let $f(x):\Omega\longrightarrow \mathbb R^n$ be a $C^2$ function where $\Omega$ is an open subset of $\mathbb R^n$ and assume that $$ \textrm{div}\, f=\sum_{i=1}^n\frac{\partial f_i}{\partial x_i}=0. $$ If $\varphi$ is the flow  of the differential equation $y'=f(y)$ and we consider the homeomorphism $\pi_t:\Omega\longrightarrow\Omega$, such that  $$ x\,\longmapsto\, \pi_t(x):=\varphi(x,t), $$  then the map $\pi_t$ preserves the Lebesgue measure of every measurable subset of $\Omega$. To be more precise, for every $\mu$-measurable subset $D\subseteq \Omega$ we have that $\mu(D)=\mu\big(\pi_t(D)\big)$. The above theorem is very famous and its simpler form applied to Hamiltonian systems is often cited in texts of mechanics. However I need a proof of the general statement. Thanks in advance",,"['real-analysis', 'ordinary-differential-equations', 'reference-request', 'multivariable-calculus', 'dynamical-systems']"
15,"$f$ differentiable, $f(x)$ rational if $x$ rational; $f(x)$ irrational if $x$ irrational. Is $f$ a linear function?","differentiable,  rational if  rational;  irrational if  irrational. Is  a linear function?",f f(x) x f(x) x f,Let $f$ be an everywhere differentiable function whose domain consists of all real numbers. Assume that $f(x)$ is rational for rational $x$ and irrational for irrational $x$. Can we conclude that $f$ is a linear function?,Let $f$ be an everywhere differentiable function whose domain consists of all real numbers. Assume that $f(x)$ is rational for rational $x$ and irrational for irrational $x$. Can we conclude that $f$ is a linear function?,,"['real-analysis', 'functions', 'irrational-numbers', 'rational-functions']"
16,Radii of convergence of real analytic functions,Radii of convergence of real analytic functions,,"I am proposing you a self-posed question, but I do not know whether it makes sense or not. Consider an analytic function $f\colon \mathbb R \to \mathbb R$. For every $x$ define $R(x)$ as the radius of convergence of the Taylor series with center in $x$. What can we say about the map $x \mapsto R(x)$? For example, for $e^x, \sin{x}, \cos{x}$ we can take $R \equiv +\infty$. Are there any functions for which the map $x \mapsto R(x)$ is constant? What about the continuity? Is there any characterization of the functions $f$ for which $x \mapsto R(x)$ is continuous? I hope the question is meaningful.","I am proposing you a self-posed question, but I do not know whether it makes sense or not. Consider an analytic function $f\colon \mathbb R \to \mathbb R$. For every $x$ define $R(x)$ as the radius of convergence of the Taylor series with center in $x$. What can we say about the map $x \mapsto R(x)$? For example, for $e^x, \sin{x}, \cos{x}$ we can take $R \equiv +\infty$. Are there any functions for which the map $x \mapsto R(x)$ is constant? What about the continuity? Is there any characterization of the functions $f$ for which $x \mapsto R(x)$ is continuous? I hope the question is meaningful.",,"['real-analysis', 'power-series']"
17,Changing Lebesgue-measurable function to Borel function,Changing Lebesgue-measurable function to Borel function,,"Show that if $f:\mathbb{R}^2\rightarrow\mathbb{R}$ is measurable (with respect to the Lebesgue-measurable sets in $\mathbb{R}^2)$, then there exists a Borel function $g$ such that $f(x)=g(x)$ for almost every $x\in\mathbb{R}^2$ (i.e. for all $x\in\mathbb{R}^2$ except a set of measure zero). I guess ""Borel function"" means Borel-measurable functions. If so, $g$ Borel-measurable would mean that $g^{-1}(A)$ is a Borel set in $\mathbb{R}^2$ for all Borel $A\in\mathbb{R}$. And $f$ Lebesgue-measurable would mean that $f^{-1}(A)$ is a Lebesgue-measurable set in $\mathbb{R}^2$ for all Borel $A\in\mathbb{R}$. Given this setting, it is hard to see how to proceed. I am allowed to change the value of $f(x)$ for a subset of measure zero in $\mathbb{R}^2$, and I want to get a Borel-measurable function. How can I do that?","Show that if $f:\mathbb{R}^2\rightarrow\mathbb{R}$ is measurable (with respect to the Lebesgue-measurable sets in $\mathbb{R}^2)$, then there exists a Borel function $g$ such that $f(x)=g(x)$ for almost every $x\in\mathbb{R}^2$ (i.e. for all $x\in\mathbb{R}^2$ except a set of measure zero). I guess ""Borel function"" means Borel-measurable functions. If so, $g$ Borel-measurable would mean that $g^{-1}(A)$ is a Borel set in $\mathbb{R}^2$ for all Borel $A\in\mathbb{R}$. And $f$ Lebesgue-measurable would mean that $f^{-1}(A)$ is a Lebesgue-measurable set in $\mathbb{R}^2$ for all Borel $A\in\mathbb{R}$. Given this setting, it is hard to see how to proceed. I am allowed to change the value of $f(x)$ for a subset of measure zero in $\mathbb{R}^2$, and I want to get a Borel-measurable function. How can I do that?",,"['real-analysis', 'measure-theory']"
18,Proof of Rudin theorem 2.34 compact subsets of metric spaces are closed,Proof of Rudin theorem 2.34 compact subsets of metric spaces are closed,,"Rudin writes at a certain point in the proof: $\textbf {Since $K$ is compact}$, there are finitely many points $q_1,...,q_n$ in $K$ such that: $K \subset W_{q1} \cup W_{q2} \cup ... \cup W_{qn}$ where the $\textbf{$W_{qi}$'s are neighborhoods}$ of a point $q\in K$ Well the implication in bold is my problem, that's definitely not what the definition of a compact set states. The definition says that for every open cover we can get an open finite subcover but we can't assume that they're neighborhoods right?? How does he make this jump?? Thanks for the help!","Rudin writes at a certain point in the proof: $\textbf {Since $K$ is compact}$, there are finitely many points $q_1,...,q_n$ in $K$ such that: $K \subset W_{q1} \cup W_{q2} \cup ... \cup W_{qn}$ where the $\textbf{$W_{qi}$'s are neighborhoods}$ of a point $q\in K$ Well the implication in bold is my problem, that's definitely not what the definition of a compact set states. The definition says that for every open cover we can get an open finite subcover but we can't assume that they're neighborhoods right?? How does he make this jump?? Thanks for the help!",,"['real-analysis', 'general-topology']"
19,Prove that some set is compact directly from definition,Prove that some set is compact directly from definition,,"Let $A$ be a subset of $R$ which consist of $0$ and the numbers $\frac{1}{n}$, for $n=1,2,3,\dots$. I want to prove that $K$ is compact directly from the definition of compact. So, given any open cover of $A$, I should be able to find a finite subcover. Proving a set is compact is much difficult than proving not compact. I have find a process of finding a finite sub cover for every open cover which means I need to find some common property of every open cover.","Let $A$ be a subset of $R$ which consist of $0$ and the numbers $\frac{1}{n}$, for $n=1,2,3,\dots$. I want to prove that $K$ is compact directly from the definition of compact. So, given any open cover of $A$, I should be able to find a finite subcover. Proving a set is compact is much difficult than proving not compact. I have find a process of finding a finite sub cover for every open cover which means I need to find some common property of every open cover.",,"['real-analysis', 'metric-spaces']"
20,Sum of prime factors,Sum of prime factors,,"If $x$ is an odd prime, is it true that the sum of the prime factors of $x + 1$ is less than $x$? If so, then this would give a nice way of constructing a ""jumpy"" sequence that converges to $0$, namely, let the $n$th term be $1$ divided by the sum of the prime factors of $n + 1$. Then if $n + 1$ is an odd prime, then the $n$th term is less than the $n + 1$th term, so that we cannot say, ""We're always getting closer to $0$, every step of the way."", but the sequence does in fact converge to $0$. This illustrates why the rigorous definition of convergence is needed: convergence of bounded monotonic sequences can be handled off-handedly, but not so the general case. edit (4.Sep.2013, CST, MERCA): In fact, non-monotonic convergence to zero is a familiar physical fact, and there are some expressions that capture this notion. Here are three: ""flash-in-the-pan"" ""dead-cat bounce"" ""death-rattle"" Can anyone come up with any others? Calculus teachers could perhaps make use of these expressions in motivating the formal definition of limit.","If $x$ is an odd prime, is it true that the sum of the prime factors of $x + 1$ is less than $x$? If so, then this would give a nice way of constructing a ""jumpy"" sequence that converges to $0$, namely, let the $n$th term be $1$ divided by the sum of the prime factors of $n + 1$. Then if $n + 1$ is an odd prime, then the $n$th term is less than the $n + 1$th term, so that we cannot say, ""We're always getting closer to $0$, every step of the way."", but the sequence does in fact converge to $0$. This illustrates why the rigorous definition of convergence is needed: convergence of bounded monotonic sequences can be handled off-handedly, but not so the general case. edit (4.Sep.2013, CST, MERCA): In fact, non-monotonic convergence to zero is a familiar physical fact, and there are some expressions that capture this notion. Here are three: ""flash-in-the-pan"" ""dead-cat bounce"" ""death-rattle"" Can anyone come up with any others? Calculus teachers could perhaps make use of these expressions in motivating the formal definition of limit.",,[]
21,Newton's method — for which initial guesses does it converge?,Newton's method — for which initial guesses does it converge?,,"We've got a function: $ f : \Bbb R \to \Bbb R$ defined by $f(x) = x^3 - 9$ . Let $x^* $ be its root, which means $ f(x^*) = 0$ . We want to find approximation for $x^*$ using a Newton's method. There are two questions I don't know how to answer: We choose an initial guess: $x_0 = 2,5$ . Does it lineary converge for such $x_0$ ? And quadratically? And, most of all, I'm interested how to find range of $x_0 $ for which this method converges. How to do it? Can we show that $|x_3 - x^*| < 2^{-15}$ ? Is there a posiibility to do it without computing $x_3$ , which is not that easy without a calculator?","We've got a function: defined by . Let be its root, which means . We want to find approximation for using a Newton's method. There are two questions I don't know how to answer: We choose an initial guess: . Does it lineary converge for such ? And quadratically? And, most of all, I'm interested how to find range of for which this method converges. How to do it? Can we show that ? Is there a posiibility to do it without computing , which is not that easy without a calculator?"," f : \Bbb R \to \Bbb R f(x) = x^3 - 9 x^*   f(x^*) = 0 x^* x_0 = 2,5 x_0 x_0  |x_3 - x^*| < 2^{-15} x_3","['real-analysis', 'numerical-methods', 'dynamical-systems', 'newton-raphson', 'basins-of-attraction']"
22,Why are scattered sets G-delta?,Why are scattered sets G-delta?,,"I am looking for an elementary proof of the following - The problem appears in a real analysis text of A. Bruckner: A subset $S$ of $\mathbb{R}$ is called scattered if every non empty subset $X \subseteq S$ has an isolated point. Show that every scattered set is equal to a countable intersection of open sets. I'd really appreciate a detailed proof, if possible. Thanks.","I am looking for an elementary proof of the following - The problem appears in a real analysis text of A. Bruckner: A subset $S$ of $\mathbb{R}$ is called scattered if every non empty subset $X \subseteq S$ has an isolated point. Show that every scattered set is equal to a countable intersection of open sets. I'd really appreciate a detailed proof, if possible. Thanks.",,"['real-analysis', 'general-topology']"
23,"How to show that Vitali set can't be nowhere dense in $[0,1)$",How to show that Vitali set can't be nowhere dense in,"[0,1)","I saw a comment mentioning that it can be shown ""a Vitali set cannot be nowhere dense, nor even meager"" by Baire category theorem. But I don't know how. In particular, assuming $\bf{AC}$, the cardinality of a Vitali set is $\mathfrak c$ (continuum). How to represent it as the union of a countable collection of closed sets with empty interior?","I saw a comment mentioning that it can be shown ""a Vitali set cannot be nowhere dense, nor even meager"" by Baire category theorem. But I don't know how. In particular, assuming $\bf{AC}$, the cardinality of a Vitali set is $\mathfrak c$ (continuum). How to represent it as the union of a countable collection of closed sets with empty interior?",,"['real-analysis', 'measure-theory']"
24,"How do I show there are no elementary function solutions for the differential equation $f''(x)=f(\sqrt{x}), x>0$?",How do I show there are no elementary function solutions for the differential equation ?,"f''(x)=f(\sqrt{x}), x>0","How do I show there are no elementary function solutions for the differential equation $f''(x)=f(\sqrt{x}), x>0$ in the $C^2(0,\infty)$ space solutions?","How do I show there are no elementary function solutions for the differential equation $f''(x)=f(\sqrt{x}), x>0$ in the $C^2(0,\infty)$ space solutions?",,"['calculus', 'real-analysis', 'analysis', 'ordinary-differential-equations']"
25,Help understanding Rudin's proof of the chain rule,Help understanding Rudin's proof of the chain rule,,"The first step of the proof of the chain rule in Rudin's Principles of Mathematical Analysis (Theorem 5.5, page 105) is as follows Theorem. Suppose $f$ is continuous on $[a,b]$ , $f'(x)$ exists at some point $x\in[a,b]$ , $g$ is defined on an interval $I$ which contains the range of $f$ , and $g$ is differentiable at the point $f(x)$ . If $$h(t)=g(f(t))\quad (a\leq t\leq b)$$ then $h$ is differentiable at $x$ , and $$h'(x)=g'(f(x))f'(x)$$ Proof. Let $y=f(x)$ . By the definition of the derivative, we have $$f(t)-f(x)=(t-x)[f'(x)+u(t)]$$ $$ g(s)-g(y)=(s-y)[g'(y)+v(s)]$$ where $t\in[a,b]$ , $s\in I$ , and $u(t)\rightarrow 0$ as $t \rightarrow x$ , $v(s) \rightarrow 0$ as $s\rightarrow y$ . [...] I think I can follow the rest from here, but I don't understand this manipulation. The definition of the derivative gives $$f'(x)=\lim_{t\rightarrow x} \frac{f(t)-f(x)}{t-x}$$ I can sort of see what's going on—it's a little like we're multiplying both sides of the equation by $t-x$ and $u(t)$ is there to make doing that make sense but I can't figure out how.","The first step of the proof of the chain rule in Rudin's Principles of Mathematical Analysis (Theorem 5.5, page 105) is as follows Theorem. Suppose is continuous on , exists at some point , is defined on an interval which contains the range of , and is differentiable at the point . If then is differentiable at , and Proof. Let . By the definition of the derivative, we have where , , and as , as . [...] I think I can follow the rest from here, but I don't understand this manipulation. The definition of the derivative gives I can sort of see what's going on—it's a little like we're multiplying both sides of the equation by and is there to make doing that make sense but I can't figure out how.","f [a,b] f'(x) x\in[a,b] g I f g f(x) h(t)=g(f(t))\quad (a\leq t\leq b) h x h'(x)=g'(f(x))f'(x) y=f(x) f(t)-f(x)=(t-x)[f'(x)+u(t)]  g(s)-g(y)=(s-y)[g'(y)+v(s)] t\in[a,b] s\in I u(t)\rightarrow 0 t \rightarrow x v(s) \rightarrow 0 s\rightarrow y f'(x)=\lim_{t\rightarrow x} \frac{f(t)-f(x)}{t-x} t-x u(t)","['calculus', 'real-analysis', 'limits', 'derivatives']"
26,Fixed point of $\cos(\sin(x))$,Fixed point of,\cos(\sin(x)),"I can show that $\cos(\sin(x))$ is a contraction on $\mathbb{R}$ and hence by the Contraction Mapping Theorem it will have a unique fixed point. But what is the process for finding this fixed point? This is in the context of metric spaces, I know in numerical analysis it can be done trivially with fixed point iteration. Is there a method of finding it analytically?","I can show that $\cos(\sin(x))$ is a contraction on $\mathbb{R}$ and hence by the Contraction Mapping Theorem it will have a unique fixed point. But what is the process for finding this fixed point? This is in the context of metric spaces, I know in numerical analysis it can be done trivially with fixed point iteration. Is there a method of finding it analytically?",,"['real-analysis', 'general-topology', 'metric-spaces']"
27,Proving $\mathbb{R}$ is uncountable using Dedekind cuts?,Proving  is uncountable using Dedekind cuts?,\mathbb{R},"I'm familiar with several proofs that the real numbers are uncountable (Cantor's initial proof, a proof by diagonalization, etc.). However, I've never seen a proof that the reals are uncountable that proceeds by showing that the set of Dedekind cuts of the rationals are uncountable.  I'm aware that the set of all subsets of the rationals is uncountable, but not all of these sets are Dedekind cuts. Is there a simple proof of the uncountability of $\mathbb{R}$ that works by showing the set of Dedekind cuts is uncountable? Thanks!","I'm familiar with several proofs that the real numbers are uncountable (Cantor's initial proof, a proof by diagonalization, etc.). However, I've never seen a proof that the reals are uncountable that proceeds by showing that the set of Dedekind cuts of the rationals are uncountable.  I'm aware that the set of all subsets of the rationals is uncountable, but not all of these sets are Dedekind cuts. Is there a simple proof of the uncountability of $\mathbb{R}$ that works by showing the set of Dedekind cuts is uncountable? Thanks!",,"['real-analysis', 'cardinals']"
28,Uniform convergence of subsequences implying uniform convergence,Uniform convergence of subsequences implying uniform convergence,,"Suppose ${f_n}$ are uniformly bounded and equicontinuous on some closed interval $[a,b]$. Therefore, by Arzela-Ascoli we know that $f_n$ has a uniformly convergent subsequence. But we can also apply Arzela-Ascoli to the subsequences of $f_n$ to get that every subsequence of $f_n$ has a subsequence that converges uniformly. And we have some extra information that says each of the subsequences of subsequences converges uniformly to the same thing. How do you conclude that $f_n$ converges uniformly from this?","Suppose ${f_n}$ are uniformly bounded and equicontinuous on some closed interval $[a,b]$. Therefore, by Arzela-Ascoli we know that $f_n$ has a uniformly convergent subsequence. But we can also apply Arzela-Ascoli to the subsequences of $f_n$ to get that every subsequence of $f_n$ has a subsequence that converges uniformly. And we have some extra information that says each of the subsequences of subsequences converges uniformly to the same thing. How do you conclude that $f_n$ converges uniformly from this?",,['real-analysis']
29,A young limit $\lim_{n\to\infty} \frac{{(n+1)}^{n+1}}{n^n} - \frac{{n}^{n}}{{(n-1)}^{n-1}} =e$,A young limit,\lim_{n\to\infty} \frac{{(n+1)}^{n+1}}{n^n} - \frac{{n}^{n}}{{(n-1)}^{n-1}} =e,"These days I saw a really interesting limit as I was reading more information on Napier's  constant here : http://mathworld.wolfram.com/e.html . It seems a pretty young limit since it appears under the name and year ""Brothers and Knox 1998"". It's also new for me and I'd like to know more approaching ways for it. $$\lim_{n\to\infty} \frac{{(n+1)}^{n+1}}{n^n} - \frac{{n}^{n}}{{(n-1)}^{n-1}} =e$$ A possible way to go is to use the first converse of the Stolz–Cesàro theorem, but since $\lim_{n\to\infty}\frac{b_{n}}{b_{n+1}}=1$ we can at most check the given result because the theorem may work or not in this case. For the case when $\lim_{n\to\infty}\frac{b_{n}}{b_{n+1}}=1$ it is required more research in order to be sure that we can safely apply the first converse theorem. I'll make  an update as soon  as things are clarified such that I may turn it into a rigorous proof. $$\lim_{n\to\infty} \frac{{(n+1)}^{n+1}}{n^n} - \frac{{n}^{n}}{{(n-1)}^{n-1}}=$$ $$\lim_{n\to\infty} f(n+1) - f(n)=$$ By the first converse of the Stolz–Cesàro theorem we have $$\lim_{n\to\infty} \frac{f(n+1) - f(n)}{n+1-n}=$$ $$\lim_{n\to\infty} \frac{f(n)}{n}=$$ $$\lim_{n\to\infty} \frac{n^n}{(n-1)^{n-1}}\cdot \frac{1}{n}=$$ $$\lim_{n\to\infty} \frac{n^{n-1}}{(n-1)^{n-1}}=\left(1+\frac{1}{n-1}\right)^{n-1}=e.$$ What else can we do here? Thanks.","These days I saw a really interesting limit as I was reading more information on Napier's  constant here : http://mathworld.wolfram.com/e.html . It seems a pretty young limit since it appears under the name and year ""Brothers and Knox 1998"". It's also new for me and I'd like to know more approaching ways for it. $$\lim_{n\to\infty} \frac{{(n+1)}^{n+1}}{n^n} - \frac{{n}^{n}}{{(n-1)}^{n-1}} =e$$ A possible way to go is to use the first converse of the Stolz–Cesàro theorem, but since $\lim_{n\to\infty}\frac{b_{n}}{b_{n+1}}=1$ we can at most check the given result because the theorem may work or not in this case. For the case when $\lim_{n\to\infty}\frac{b_{n}}{b_{n+1}}=1$ it is required more research in order to be sure that we can safely apply the first converse theorem. I'll make  an update as soon  as things are clarified such that I may turn it into a rigorous proof. $$\lim_{n\to\infty} \frac{{(n+1)}^{n+1}}{n^n} - \frac{{n}^{n}}{{(n-1)}^{n-1}}=$$ $$\lim_{n\to\infty} f(n+1) - f(n)=$$ By the first converse of the Stolz–Cesàro theorem we have $$\lim_{n\to\infty} \frac{f(n+1) - f(n)}{n+1-n}=$$ $$\lim_{n\to\infty} \frac{f(n)}{n}=$$ $$\lim_{n\to\infty} \frac{n^n}{(n-1)^{n-1}}\cdot \frac{1}{n}=$$ $$\lim_{n\to\infty} \frac{n^{n-1}}{(n-1)^{n-1}}=\left(1+\frac{1}{n-1}\right)^{n-1}=e.$$ What else can we do here? Thanks.",,"['calculus', 'real-analysis', 'sequences-and-series', 'limits']"
30,Proof of uniform continuity of $\frac{1}{x}$,Proof of uniform continuity of,\frac{1}{x},"Show that the function $f(x) = \frac{1}{x}$ is not uniformly continuous on the interval $(0,\infty)$ but is uniformly continuous on any interval of the form $(\mu, \infty)$ if $\mu > 0$. My Work Referring to the definition of uniform continuity, I have that $f$ is unif. cts. if for each $\epsilon > 0$ there is a $\delta > 0$ so that for all $x, c$ in the domain of $f$ $|x - c| \le \delta \ \Rightarrow \ |f(x) - f(c) | \le \epsilon$. From this definition, it is clear that if $f$ is uniformly continuous, it will be uniformly continuous on its domain, $(-\infty, \infty) \backslash \{0\}$. So for $\mu > 0$, $(\mu, \infty) \subset \mathrm{Dom}\,(f)$. Additionally, $f$ cannot be unif. cts on $(0, \infty)$ because $0 \notin \mathrm{Dom}\, (f)$. (Sorry about the longwindedness) Now to find the $\delta$: \begin{align*} |f(x) - f(c)| = \left|\frac{1}{x} - \frac{1}{c}\right| &= \left|\frac{x - c}{cx}\right| \\ \text{since }x\text{ is within }\delta\text{ of }c \ \Rightarrow \ &\le \frac{\delta}{|cx|}\\ x, c>0 \ \Rightarrow \ &= \frac{\delta}{cx} \end{align*} This is where I am stuck. Should I use that $x \le c + \delta$, or should I break this up into two cases, one where $cx < 1$ and one where $cx \ge 1$? Edit (due to Brian M. Scott) It was pointed out that $0 \notin (0, \infty)$ so my above argument is senseless.","Show that the function $f(x) = \frac{1}{x}$ is not uniformly continuous on the interval $(0,\infty)$ but is uniformly continuous on any interval of the form $(\mu, \infty)$ if $\mu > 0$. My Work Referring to the definition of uniform continuity, I have that $f$ is unif. cts. if for each $\epsilon > 0$ there is a $\delta > 0$ so that for all $x, c$ in the domain of $f$ $|x - c| \le \delta \ \Rightarrow \ |f(x) - f(c) | \le \epsilon$. From this definition, it is clear that if $f$ is uniformly continuous, it will be uniformly continuous on its domain, $(-\infty, \infty) \backslash \{0\}$. So for $\mu > 0$, $(\mu, \infty) \subset \mathrm{Dom}\,(f)$. Additionally, $f$ cannot be unif. cts on $(0, \infty)$ because $0 \notin \mathrm{Dom}\, (f)$. (Sorry about the longwindedness) Now to find the $\delta$: \begin{align*} |f(x) - f(c)| = \left|\frac{1}{x} - \frac{1}{c}\right| &= \left|\frac{x - c}{cx}\right| \\ \text{since }x\text{ is within }\delta\text{ of }c \ \Rightarrow \ &\le \frac{\delta}{|cx|}\\ x, c>0 \ \Rightarrow \ &= \frac{\delta}{cx} \end{align*} This is where I am stuck. Should I use that $x \le c + \delta$, or should I break this up into two cases, one where $cx < 1$ and one where $cx \ge 1$? Edit (due to Brian M. Scott) It was pointed out that $0 \notin (0, \infty)$ so my above argument is senseless.",,['real-analysis']
31,About the $p$ summable sequences,About the  summable sequences,p,How to prove that $\ell^p \setminus \cup_{1\leq q <p}\ell^q$ is not empty? Here $\ell^p=L^p(\mathbb{N})$.,How to prove that $\ell^p \setminus \cup_{1\leq q <p}\ell^q$ is not empty? Here $\ell^p=L^p(\mathbb{N})$.,,"['real-analysis', 'functional-analysis']"
32,Show that this polynomial is positive,Show that this polynomial is positive,,"Consider the following polynomial in two variables : $$ Q(k,x)=27x^6 - 144kx^4 + 80k^2x^3 + 240k^2x^2 - 192k^3x + (64k^4 - 128k^3) $$ Then for any integer $k \geq 5$, the polynomial $Q(k,.)$ (in one variable $x$) seems to be always positive (i.e, $Q(k,x) >0$ for any real number $x$).   Prove or disprove.","Consider the following polynomial in two variables : $$ Q(k,x)=27x^6 - 144kx^4 + 80k^2x^3 + 240k^2x^2 - 192k^3x + (64k^4 - 128k^3) $$ Then for any integer $k \geq 5$, the polynomial $Q(k,.)$ (in one variable $x$) seems to be always positive (i.e, $Q(k,x) >0$ for any real number $x$).   Prove or disprove.",,"['real-analysis', 'polynomials']"
33,"Is there a one-one onto continuous function $f:[0,1]\rightarrow[0,1]^2$?",Is there a one-one onto continuous function ?,"f:[0,1]\rightarrow[0,1]^2","Is there a one-one onto continuous function $f:[0,1]\rightarrow[0,1]^2$? I was trying to prove that there is no such function, but failed. Any suggestions?","Is there a one-one onto continuous function $f:[0,1]\rightarrow[0,1]^2$? I was trying to prove that there is no such function, but failed. Any suggestions?",,"['real-analysis', 'functions']"
34,"For bounded sequences, does convergence of the Abel means imply that for the Cesàro means?","For bounded sequences, does convergence of the Abel means imply that for the Cesàro means?",,"See the title. This is true if the sequence is nonnegative; some Tauberian theorems which I was able to find give some more general sufficient conditions. I would like to know if this is true for arbitrary bounded sequences. Recall that for a sequence $(a_n)$ with natural indices $n$, the Cesàro means are $\frac1{N}\sum\limits_{n=1}^N a_n$, and the Abel means are $(1-r)\sum\limits_n r^n a_n$.","See the title. This is true if the sequence is nonnegative; some Tauberian theorems which I was able to find give some more general sufficient conditions. I would like to know if this is true for arbitrary bounded sequences. Recall that for a sequence $(a_n)$ with natural indices $n$, the Cesàro means are $\frac1{N}\sum\limits_{n=1}^N a_n$, and the Abel means are $(1-r)\sum\limits_n r^n a_n$.",,"['real-analysis', 'sequences-and-series', 'divergent-series']"
35,differentiation,differentiation,,"I don't have any idea how to start solving this problem. Any help please? Problem: Suppose that a differentiable function $f:\mathbb R \to \mathbb R$ and its derivative $f'$ have no common zeros. Prove that $f$ has only finitely many zeros in $[0,1]$.","I don't have any idea how to start solving this problem. Any help please? Problem: Suppose that a differentiable function $f:\mathbb R \to \mathbb R$ and its derivative $f'$ have no common zeros. Prove that $f$ has only finitely many zeros in $[0,1]$.",,"['calculus', 'real-analysis']"
36,Are there more Lebesgue measurable or more non Lebesgue measurable functions?,Are there more Lebesgue measurable or more non Lebesgue measurable functions?,,Are there more Lebesgue measurable or more non Lebesgue measurable functions? Does anybody see how to answer this. Please do tell.,Are there more Lebesgue measurable or more non Lebesgue measurable functions? Does anybody see how to answer this. Please do tell.,,"['real-analysis', 'measure-theory']"
37,Closure of the Laplacian in $L^2(\mathbb R^n)$,Closure of the Laplacian in,L^2(\mathbb R^n),"Consider the Laplacian $\Delta$ as an operator on $L^2(\mathbb R^n)$, densely defined on the subspace $C^\infty_0(\mathbb R^n)$. Is the domain of the closure of the Laplacian, in the sense described here: https://en.wikipedia.org/wiki/Unbounded_operator#Closed_linear_operators , equal exactly to: $$\{u \in L^2(\mathbb R^n) | \Delta u \in L^2(\mathbb R^n)\}$$ (where $\Delta$ here means in the distributional sense)? Does any of the above spaces (which I hope are equal) in turn exactly equal the Sobolev space $W^{2,2}(\mathbb R^n)$, or is $W^{2,2}$ actually a strictly smaller space? Does any of the above spaces equal the Friedrichs extension ?","Consider the Laplacian $\Delta$ as an operator on $L^2(\mathbb R^n)$, densely defined on the subspace $C^\infty_0(\mathbb R^n)$. Is the domain of the closure of the Laplacian, in the sense described here: https://en.wikipedia.org/wiki/Unbounded_operator#Closed_linear_operators , equal exactly to: $$\{u \in L^2(\mathbb R^n) | \Delta u \in L^2(\mathbb R^n)\}$$ (where $\Delta$ here means in the distributional sense)? Does any of the above spaces (which I hope are equal) in turn exactly equal the Sobolev space $W^{2,2}(\mathbb R^n)$, or is $W^{2,2}$ actually a strictly smaller space? Does any of the above spaces equal the Friedrichs extension ?",,"['real-analysis', 'functional-analysis', 'laplacian']"
38,"Given a real function $g$ satisfying certain conditions, can we construct a convex $h$ with $h \le g$?","Given a real function  satisfying certain conditions, can we construct a convex  with ?",g h h \le g,"The following is Exercise 8 from Chapter 3 of Rudin's Real and Complex Analysis (not a homework problem, just for fun). Let $g$ be a positive function on $(0, 1)$ such that $g(x) \to \infty$ as $x \to 0$. Does there exist a convex function $h$ on $(0, 1)$ such that $h \le g$ and $h(x) \to \infty$ as $x \to 0$? This seems true, but I can't show it. All I've been able to do is to reduce the problem to $g$ being a monotone step function or a strictly monotone piecewise-linear function but this doesn't seem to get me anywhere. I'm not sure how to use the property of $g$ to construct the explicit $h$.","The following is Exercise 8 from Chapter 3 of Rudin's Real and Complex Analysis (not a homework problem, just for fun). Let $g$ be a positive function on $(0, 1)$ such that $g(x) \to \infty$ as $x \to 0$. Does there exist a convex function $h$ on $(0, 1)$ such that $h \le g$ and $h(x) \to \infty$ as $x \to 0$? This seems true, but I can't show it. All I've been able to do is to reduce the problem to $g$ being a monotone step function or a strictly monotone piecewise-linear function but this doesn't seem to get me anywhere. I'm not sure how to use the property of $g$ to construct the explicit $h$.",,['real-analysis']
39,Uniform convergence problem,Uniform convergence problem,,"I encountered this problem while studying for an analysis exam. Here is a related question I asked some days ago. The problem is as follows: Suppose $a_n$ is a decreasing sequence of positive real numbers and that$$\sum_{n = 0}^{\infty}{a_n \sin{(nx)}}$$ converges uniformly on $\mathbb{R}$, show that $$\lim_{n \to \infty}{(n a_n)} = 0.$$ Any tip or solution is welcome, and also avoid using Fourier series, because they haven't been introduced in the book so it can be solved without using them.","I encountered this problem while studying for an analysis exam. Here is a related question I asked some days ago. The problem is as follows: Suppose $a_n$ is a decreasing sequence of positive real numbers and that$$\sum_{n = 0}^{\infty}{a_n \sin{(nx)}}$$ converges uniformly on $\mathbb{R}$, show that $$\lim_{n \to \infty}{(n a_n)} = 0.$$ Any tip or solution is welcome, and also avoid using Fourier series, because they haven't been introduced in the book so it can be solved without using them.",,"['real-analysis', 'sequences-and-series', 'convergence-divergence']"
40,Q is dense in R and Completions,Q is dense in R and Completions,,"What is the relation between the fact that $\mathbb{Q}$ is dense in $\mathbb{R}$ and the fact that the completion of $\mathbb{Q}$ is $\mathbb{R}$? Or in general, that is if $A$ is dense in a metric space $B$, what's the relation between the completion of $A$ and $B$?","What is the relation between the fact that $\mathbb{Q}$ is dense in $\mathbb{R}$ and the fact that the completion of $\mathbb{Q}$ is $\mathbb{R}$? Or in general, that is if $A$ is dense in a metric space $B$, what's the relation between the completion of $A$ and $B$?",,"['real-analysis', 'general-topology']"
41,Fubini's theorem problem,Fubini's theorem problem,,"Let $f$ be a non-negative measurable function in $\mathbb{R}$. Suppose that $$\iint_{\mathbb{R}^2}{f(4x)f(x-3y)\,dxdy}=2\,.$$ Calculate $\int_{-\infty}^{\infty}{f(x)\,dx}$. My first thought was to change the order of integration so that I integrate in $y$ first, since there's only a single $y$ in the integrand... but I'm not sure how/if that even helps me. Then the more I thought about it, the less clear it was to me that Fubini's theorem even applies as it's written. Somehow I need a function of two variables. So should I set $g(x,y) = f(4x)f(x-3y)$ and do something with that? At least Fubini's theorem applies for $g(x,y)$, since we know it's integrable on $\mathbb{R}^2$.    .... Maybe? I'm just pretty lost on this, so any help you could offer would be great. Thanks!","Let $f$ be a non-negative measurable function in $\mathbb{R}$. Suppose that $$\iint_{\mathbb{R}^2}{f(4x)f(x-3y)\,dxdy}=2\,.$$ Calculate $\int_{-\infty}^{\infty}{f(x)\,dx}$. My first thought was to change the order of integration so that I integrate in $y$ first, since there's only a single $y$ in the integrand... but I'm not sure how/if that even helps me. Then the more I thought about it, the less clear it was to me that Fubini's theorem even applies as it's written. Somehow I need a function of two variables. So should I set $g(x,y) = f(4x)f(x-3y)$ and do something with that? At least Fubini's theorem applies for $g(x,y)$, since we know it's integrable on $\mathbb{R}^2$.    .... Maybe? I'm just pretty lost on this, so any help you could offer would be great. Thanks!",,['real-analysis']
42,An interesting series,An interesting series,,"$\sum_{n=1}^{\infty} \frac{\varphi(n)}{n}$ where $\varphi(n)$ is 1 if the variable $\text n$ has the number $\text 7$ in its typical base-$\text10$ representation, and $\text0$ otherwise. I am supposed to find out if this series converges or diverges. I think it diverges, and here is why. We can see that there is a series whose partial sums are always below our series, but which diverges. Compare some of the terms of each sequence $\frac{1}{7} > \frac{1}{8}$ $\frac{1}{70} > \frac{1}{80}$ $\frac{1}{71} > \frac{1}{80}$ $\frac{1}{72} > \frac{1}{80}$ $\text ... $ $\frac{1}{79} > \frac{1}{80}$ $\text ... $ $\frac{1}{700} > \frac{1}{800}$ $\text ... $ And continue in this way. Obviously some terms are left out of the sequence on the left, which is fine since our sequence of terms on the left is already greater than the right side. Notice the right side can be grouped into $\frac{1}{8} + \frac{1}{8} + ... $ because we will have $10$ $\frac{1}{80}$s, $100$ $\frac{1}{800}$s, etc etc. Thus we are adding up infinitely many 1/8s. This is similar to the idea of the divergence of the harmonic series. So, my conclusion is that it diverges. A bunch of other students in my real analysis class have come to the conclusion that is, in fact, convergent, and launched into a detailed verbal explanation about comparison with a geometric series that I couldn't follow without seeing their work. Is my reasoning, like they suspect, flawed? I can't see how. Sorry about the poor format, I'm new to TeX and couldn't figure out how to format a piecewise function (it was telling me a my \left delimiter wasn't recognized).","$\sum_{n=1}^{\infty} \frac{\varphi(n)}{n}$ where $\varphi(n)$ is 1 if the variable $\text n$ has the number $\text 7$ in its typical base-$\text10$ representation, and $\text0$ otherwise. I am supposed to find out if this series converges or diverges. I think it diverges, and here is why. We can see that there is a series whose partial sums are always below our series, but which diverges. Compare some of the terms of each sequence $\frac{1}{7} > \frac{1}{8}$ $\frac{1}{70} > \frac{1}{80}$ $\frac{1}{71} > \frac{1}{80}$ $\frac{1}{72} > \frac{1}{80}$ $\text ... $ $\frac{1}{79} > \frac{1}{80}$ $\text ... $ $\frac{1}{700} > \frac{1}{800}$ $\text ... $ And continue in this way. Obviously some terms are left out of the sequence on the left, which is fine since our sequence of terms on the left is already greater than the right side. Notice the right side can be grouped into $\frac{1}{8} + \frac{1}{8} + ... $ because we will have $10$ $\frac{1}{80}$s, $100$ $\frac{1}{800}$s, etc etc. Thus we are adding up infinitely many 1/8s. This is similar to the idea of the divergence of the harmonic series. So, my conclusion is that it diverges. A bunch of other students in my real analysis class have come to the conclusion that is, in fact, convergent, and launched into a detailed verbal explanation about comparison with a geometric series that I couldn't follow without seeing their work. Is my reasoning, like they suspect, flawed? I can't see how. Sorry about the poor format, I'm new to TeX and couldn't figure out how to format a piecewise function (it was telling me a my \left delimiter wasn't recognized).",,['real-analysis']
43,Does a solution for this integral even exists? $I=\int_0^\frac{\pi}{2}\ln(a+\sin(x))dx$,Does a solution for this integral even exists?,I=\int_0^\frac{\pi}{2}\ln(a+\sin(x))dx,"Some time ago I computed $\displaystyle \int_0^\frac{\pi}{2} \ln(1+\sin(x))dx$ and $\displaystyle \int_0^\frac{\pi}{2} \ln(1-\sin(x))dx$ , which suggested to tackle the more general case: $$I=\int_0^\frac{\pi}{2}\ln(a+\sin(x))dx$$ with $a>0$ . I wasn't able to solve it. I approached it whith Feynman technique: consider $I$ as $I(a)$ , and so $\frac{dI}{da}=\displaystyle \int_0^\frac{\pi}{2}\dfrac{1}{a+\sin(x)}dx$ , which evaluates with standard calculus to $\frac{2}{\sqrt{a^2-1}}\arctan{\sqrt{\frac{a-1}{a+1}}}$ . Unfortunately seems like this result cannot be integrated with respect to $a$ , so I got stuck. Anyone knows a solution? Notes: Clearly $I=\displaystyle \int_0^\frac{\pi}{2} \ln(a+\sin(x))dx=\int_0^\frac{\pi}{2} \ln(a+\cos(x))dx$ . Here are the results I got for the first two integrals if it can help: $$\int_0^\frac{\pi}{2} \ln(1+\sin(x))dx=2G-\frac{\pi}{2}\ln(2)\hspace{1cm}\int_0^\frac{\pi}{2} \ln(1-\sin(x))dx=-2G-\frac{\pi}{2}\ln(2)$$ where G is Catalan constant.","Some time ago I computed and , which suggested to tackle the more general case: with . I wasn't able to solve it. I approached it whith Feynman technique: consider as , and so , which evaluates with standard calculus to . Unfortunately seems like this result cannot be integrated with respect to , so I got stuck. Anyone knows a solution? Notes: Clearly . Here are the results I got for the first two integrals if it can help: where G is Catalan constant.",\displaystyle \int_0^\frac{\pi}{2} \ln(1+\sin(x))dx \displaystyle \int_0^\frac{\pi}{2} \ln(1-\sin(x))dx I=\int_0^\frac{\pi}{2}\ln(a+\sin(x))dx a>0 I I(a) \frac{dI}{da}=\displaystyle \int_0^\frac{\pi}{2}\dfrac{1}{a+\sin(x)}dx \frac{2}{\sqrt{a^2-1}}\arctan{\sqrt{\frac{a-1}{a+1}}} a I=\displaystyle \int_0^\frac{\pi}{2} \ln(a+\sin(x))dx=\int_0^\frac{\pi}{2} \ln(a+\cos(x))dx \int_0^\frac{\pi}{2} \ln(1+\sin(x))dx=2G-\frac{\pi}{2}\ln(2)\hspace{1cm}\int_0^\frac{\pi}{2} \ln(1-\sin(x))dx=-2G-\frac{\pi}{2}\ln(2),"['real-analysis', 'calculus', 'integration', 'definite-integrals', 'trigonometric-integrals']"
44,Evaluate $\int_{0}^{\infty}x\left ( \operatorname{Ci}(x)^2 +\operatorname{si}(x)^2\right )\operatorname{Ci}(x)\text{d}x$,Evaluate,\int_{0}^{\infty}x\left ( \operatorname{Ci}(x)^2 +\operatorname{si}(x)^2\right )\operatorname{Ci}(x)\text{d}x,"Evaluate $$ I_1=\int_{0}^{\infty}x\left ( \operatorname{Ci}(x)^2 +\operatorname{si}(x)^2\right )\operatorname{Ci}(x)\text{d}x. $$ Where $\operatorname{Ci}(x)=-\int_{x}^{\infty}\frac{\cos(t)}{t}\text{d}t,\operatorname{si}(x)=-\int_{x}^{\infty}\frac{\sin(t)}{t}\text{d}t$ are cosine integral function and (modified) sine integral function respectively. By integrating numerically, one gives $I_1=0$ . My evaluation is a bit complex, so I would like to see a canonical one. Thanks for replying. A related version: $$ I_2=\int_{0}^{\infty}x\left ( \operatorname{Ci}(x)^2 +\operatorname{si}(x)^2\right )\operatorname{si}(x)\text{d}x=\frac\pi2\left(1-2\ln(2)\right). $$ A sophisticated version: $$ I_3=\int_{0}^{\infty}\left ( \operatorname{Ci}(x)^2+\operatorname{si}(x)^2\right )^2\left (   \operatorname{Ci}(x)^2+\operatorname{si}(x)^2+3\pi\operatorname{si}(x)\right ) \text{d}x =6\pi^3\operatorname{Li}_2\left ( \frac{1}{4}  \right ) +12\pi^3\ln(2)^2. $$ Where $\operatorname{Li}_2(z)=\sum_{k=1}^{\infty}\frac{z^k}{k^2}$ is the dilogarithm.","Evaluate Where are cosine integral function and (modified) sine integral function respectively. By integrating numerically, one gives . My evaluation is a bit complex, so I would like to see a canonical one. Thanks for replying. A related version: A sophisticated version: Where is the dilogarithm.","
I_1=\int_{0}^{\infty}x\left ( \operatorname{Ci}(x)^2
+\operatorname{si}(x)^2\right )\operatorname{Ci}(x)\text{d}x.
 \operatorname{Ci}(x)=-\int_{x}^{\infty}\frac{\cos(t)}{t}\text{d}t,\operatorname{si}(x)=-\int_{x}^{\infty}\frac{\sin(t)}{t}\text{d}t I_1=0 
I_2=\int_{0}^{\infty}x\left ( \operatorname{Ci}(x)^2
+\operatorname{si}(x)^2\right )\operatorname{si}(x)\text{d}x=\frac\pi2\left(1-2\ln(2)\right).
 
I_3=\int_{0}^{\infty}\left ( \operatorname{Ci}(x)^2+\operatorname{si}(x)^2\right )^2\left ( 
 \operatorname{Ci}(x)^2+\operatorname{si}(x)^2+3\pi\operatorname{si}(x)\right ) \text{d}x
=6\pi^3\operatorname{Li}_2\left ( \frac{1}{4}  \right ) +12\pi^3\ln(2)^2.
 \operatorname{Li}_2(z)=\sum_{k=1}^{\infty}\frac{z^k}{k^2}","['real-analysis', 'calculus', 'integration', 'definite-integrals', 'special-functions']"
45,Find the smallest possible value for $x^2-3x+2y^2+4y+2$,Find the smallest possible value for,x^2-3x+2y^2+4y+2,"Find the smallest possible value for $x^2-3x+2y^2+4y+2$ . I know this: $x^2-3x+2y^2+4y+2=(x-\frac{3}{2})^2+2(y+1)^2-\frac{9}{4}$ so the smallest value is $-\frac{9}{4}$ . Now we also know this: $x^2-3x+2y^2+4y+2=(x-1)^2-1-x+2(y+1)^2=(*)(x-1)^2+2(y+1)^2-(1+x)$ My question is: why we cannot say that the smallest value possible is $-(1+x)$ when $(x-1)^2=0$ (i.e. when $x=1$ ) and $(y+1)^2=0$ , and because $x=1$ so the smallest value is $-2$ ?","Find the smallest possible value for . I know this: so the smallest value is . Now we also know this: My question is: why we cannot say that the smallest value possible is when (i.e. when ) and , and because so the smallest value is ?",x^2-3x+2y^2+4y+2 x^2-3x+2y^2+4y+2=(x-\frac{3}{2})^2+2(y+1)^2-\frac{9}{4} -\frac{9}{4} x^2-3x+2y^2+4y+2=(x-1)^2-1-x+2(y+1)^2=(*)(x-1)^2+2(y+1)^2-(1+x) -(1+x) (x-1)^2=0 x=1 (y+1)^2=0 x=1 -2,"['real-analysis', 'calculus', 'algebra-precalculus', 'analysis']"
46,"What (if anything) is wrong with this ""proof"" of $\frac{d}{dx}e^x=e^x$?","What (if anything) is wrong with this ""proof"" of ?",\frac{d}{dx}e^x=e^x,"I'm in physics and am not super adept at developing completely rigorous proofs. I was curious to prove that $\frac{d}{dx}e^x=e^x$ for myself and I came up with the following proof, but I can't find anyone else doing the proof in precisely this way, which makes me suspect that there is some subtle issue with it. I'd like to understand what that issue is or if it is, in fact, a valid proof. I begin with the limit definition of $e$ $$e\equiv\lim_{h\to0}(1+h)^{\frac{1}{h}}$$ The definition of the derivative (obviously): $$\frac{df}{dx}\equiv\lim_{h\to0}\frac{f(x+h)-f(x)}{h}$$ Then evaluate these as most people do: $$\frac{d}{dx}e^x=\lim_{h\to0}\frac{e^{x+h}-e^x}{h}$$ $$=\lim_{h\to0}\frac{e^{x}e^{h}-e^{x}}{h}$$ $$=e^x\lim_{h\to0}\frac{e^h-1}{h}$$ Plugging in the definition of $e$ : $$=e^x\lim_{h\to0}\frac{((1+h)^{\frac{1}{h}})^h-1}{h}$$ $$=e^x\lim_{h\to0}\frac{(1+h)-1}{h}=e^x\lim_{h\to0}(1)=e^x(1)=e^x$$ The only thing I can think that may be wrong with this proof is if it is not valid to fold the limit that comes from the derivative and the limit that comes from the definition of $e$ into the same limit index $h$ , but if that is the case, I don't see why.","I'm in physics and am not super adept at developing completely rigorous proofs. I was curious to prove that for myself and I came up with the following proof, but I can't find anyone else doing the proof in precisely this way, which makes me suspect that there is some subtle issue with it. I'd like to understand what that issue is or if it is, in fact, a valid proof. I begin with the limit definition of The definition of the derivative (obviously): Then evaluate these as most people do: Plugging in the definition of : The only thing I can think that may be wrong with this proof is if it is not valid to fold the limit that comes from the derivative and the limit that comes from the definition of into the same limit index , but if that is the case, I don't see why.",\frac{d}{dx}e^x=e^x e e\equiv\lim_{h\to0}(1+h)^{\frac{1}{h}} \frac{df}{dx}\equiv\lim_{h\to0}\frac{f(x+h)-f(x)}{h} \frac{d}{dx}e^x=\lim_{h\to0}\frac{e^{x+h}-e^x}{h} =\lim_{h\to0}\frac{e^{x}e^{h}-e^{x}}{h} =e^x\lim_{h\to0}\frac{e^h-1}{h} e =e^x\lim_{h\to0}\frac{((1+h)^{\frac{1}{h}})^h-1}{h} =e^x\lim_{h\to0}\frac{(1+h)-1}{h}=e^x\lim_{h\to0}(1)=e^x(1)=e^x e h,"['real-analysis', 'calculus', 'derivatives', 'solution-verification']"
47,"If balls are replaced with rectangles, does Lebesgue's differentiation theorem hold?","If balls are replaced with rectangles, does Lebesgue's differentiation theorem hold?",,"I know that the Lebesgue's differentiation theorem states that for $f\in L^1(\mathbb R^n)$ then $$\lim_{r\to 0^+}\frac1{B_r(x)}\int_{B_r(x)}f$$ exists and coincides with $f(x)$ a.e. I am wondering whether balls can be replaced with rectangles. To be precise, if $f\in L^1(\mathbb R^n)$ , does $$\lim_{r_1,\dots,r_n\to 0^+}\frac1{r_1\cdots r_n}\int_{[x_1-r_1/2,x_1+r_1/2]\times\cdots\times[x_n-r_n/2,x_n+r_n/2]}f$$ exist and coincide with $f(x)$ a.e. I have found the following more general results of Lebesgue's differentiation, but it seems to be not applicable in our current case. So I guess the answer is no. But I cannot find any example. Let $\mathcal {V}$ be any family with the property that (i) for some $c > 0$ , each set $U$ from the family is contained in a ball $B$ with $ |U|\geq c\cdot |B|$ ; (ii) every point $x\in\mathbb R^n$ is contained in arbitrarily small sets from ${\mathcal {V}}$ . Then the analogous limit exists and coincides with $f(x)$ a.e. as the sets $U$ shrink to $x$ . So, for example, if we let $Q_r(x)$ denotes the cube centered at $x$ with diameter $r$ , then $$\lim_{r\to 0^+}\frac1{Q_r(x)}\int_{Q_r(x)}f.$$ exists and coincides with $f(x)$ a.e. Thanks!","I know that the Lebesgue's differentiation theorem states that for then exists and coincides with a.e. I am wondering whether balls can be replaced with rectangles. To be precise, if , does exist and coincide with a.e. I have found the following more general results of Lebesgue's differentiation, but it seems to be not applicable in our current case. So I guess the answer is no. But I cannot find any example. Let be any family with the property that (i) for some , each set from the family is contained in a ball with ; (ii) every point is contained in arbitrarily small sets from . Then the analogous limit exists and coincides with a.e. as the sets shrink to . So, for example, if we let denotes the cube centered at with diameter , then exists and coincides with a.e. Thanks!","f\in L^1(\mathbb R^n) \lim_{r\to 0^+}\frac1{B_r(x)}\int_{B_r(x)}f f(x) f\in L^1(\mathbb R^n) \lim_{r_1,\dots,r_n\to 0^+}\frac1{r_1\cdots r_n}\int_{[x_1-r_1/2,x_1+r_1/2]\times\cdots\times[x_n-r_n/2,x_n+r_n/2]}f f(x) \mathcal {V} c > 0 U B  |U|\geq c\cdot |B| x\in\mathbb R^n {\mathcal {V}} f(x) U x Q_r(x) x r \lim_{r\to 0^+}\frac1{Q_r(x)}\int_{Q_r(x)}f. f(x)","['real-analysis', 'analysis', 'measure-theory', 'lebesgue-measure']"
48,Is this operator between $\ell^{25}$ and $\ell^{12}$ continuous?,Is this operator between  and  continuous?,\ell^{25} \ell^{12},"Problem: Let us define $\ell^p$ as the space of sequences $(x_n)_{n \in \mathbb{N}}$ such that $\sum\limits_{n \in \mathbb{N}}|x_n|^p < +\infty$ with the usual norm $$\|x\|_p = \big( \sum\limits_{n \in \mathbb{N}}|x_n|^p \big)^{\frac 1 p}$$ Define $T:\ell^{25} \to \ell^{12}$ in the following way: $$T(x_1,\dots,x_n, \dots) = (x_1^{2018},\dots,x_n^{2018}, \dots)$$ I am asking if the map $T$ is continuous. Attempt: Let us fix $x \in \ell^{25}$ and let us study what happens for $y \in \ell^{25}$ with $\|y-x\|_p \leq 1$ . We have that $|y_n-x_n| \leq 1$ for all $n \in \mathbb{N}$ and thus $|x_n^{2018}-y_n^{2018}| \leq |x_n - y_n| K$ for $K > 0$ depending only on $\|x\|_p$ which has been fixed. This is because $a^n-b^n = (a-b)p(a, b)$ where $p$ is a polynomial on $a$ and $b$ . Thus $\|Tx-Ty\|_{12}^{12} \leq K^{12} \|x-y\|_{12}^{12}$ but this does not help me to conclude because we cannot control the $\mathcal{l}^{12}$ with the $\mathcal{l}^{25}$ norm. Trivial remark: the operator is not linear and thus we have no hope to prove that $T$ is Lipschitz continuous.",Problem: Let us define as the space of sequences such that with the usual norm Define in the following way: I am asking if the map is continuous. Attempt: Let us fix and let us study what happens for with . We have that for all and thus for depending only on which has been fixed. This is because where is a polynomial on and . Thus but this does not help me to conclude because we cannot control the with the norm. Trivial remark: the operator is not linear and thus we have no hope to prove that is Lipschitz continuous.,"\ell^p (x_n)_{n \in \mathbb{N}} \sum\limits_{n \in \mathbb{N}}|x_n|^p < +\infty \|x\|_p = \big( \sum\limits_{n \in \mathbb{N}}|x_n|^p \big)^{\frac 1 p} T:\ell^{25} \to \ell^{12} T(x_1,\dots,x_n, \dots) = (x_1^{2018},\dots,x_n^{2018}, \dots) T x \in \ell^{25} y \in \ell^{25} \|y-x\|_p \leq 1 |y_n-x_n| \leq 1 n \in \mathbb{N} |x_n^{2018}-y_n^{2018}| \leq |x_n - y_n| K K > 0 \|x\|_p a^n-b^n = (a-b)p(a, b) p a b \|Tx-Ty\|_{12}^{12} \leq K^{12} \|x-y\|_{12}^{12} \mathcal{l}^{12} \mathcal{l}^{25} T","['real-analysis', 'functional-analysis', 'lp-spaces']"
49,$\lim\limits_{x\to\infty}xf'\left(x\right)=1\Rightarrow\lim\limits_{x\to\infty}f\left(x\right)=\infty$ [closed],[closed],\lim\limits_{x\to\infty}xf'\left(x\right)=1\Rightarrow\lim\limits_{x\to\infty}f\left(x\right)=\infty,"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 2 years ago . Improve this question Let $f:\left(0,\infty\right)\longrightarrow\mathbb{R}$ differentiable on $\left(0,\infty\right)$ and $\lim\limits_{x\to\infty}xf'\left(x\right)=1$ . Show that: $\lim\limits_{x\to\infty}f\left(x\right)=\infty$ . I get that for a sufficently large x: $\frac{1}{2x}<f'\left(x\right)<\frac{3}{2x}$ . But I'm not sure how to proceed.","Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 2 years ago . Improve this question Let differentiable on and . Show that: . I get that for a sufficently large x: . But I'm not sure how to proceed.","f:\left(0,\infty\right)\longrightarrow\mathbb{R} \left(0,\infty\right) \lim\limits_{x\to\infty}xf'\left(x\right)=1 \lim\limits_{x\to\infty}f\left(x\right)=\infty \frac{1}{2x}<f'\left(x\right)<\frac{3}{2x}","['real-analysis', 'calculus', 'analysis']"
50,Why do we define Measurable functions by their pre-image?,Why do we define Measurable functions by their pre-image?,,"Let $(X,\mathcal{M})$ and $(Y,\mathcal{N})$ be measurable spaces. We call the function $f:X\to Y$ to be $(\mathcal{M},\mathcal{N})$ -measurable if $f^{-1}(E)\in \mathcal{M}$ for all $E\in \mathcal{N}$ . Similarly, when we are trying to define ""nearness"" in functions we call a function to be continuous if the preimage of the open set is open. There might be other types of similar definitions (if so, I'd appreciate more examples!), but my question is why we define the particular function always based on the preimage? Thanks in advance!","Let and be measurable spaces. We call the function to be -measurable if for all . Similarly, when we are trying to define ""nearness"" in functions we call a function to be continuous if the preimage of the open set is open. There might be other types of similar definitions (if so, I'd appreciate more examples!), but my question is why we define the particular function always based on the preimage? Thanks in advance!","(X,\mathcal{M}) (Y,\mathcal{N}) f:X\to Y (\mathcal{M},\mathcal{N}) f^{-1}(E)\in \mathcal{M} E\in \mathcal{N}","['real-analysis', 'measure-theory']"
51,Show that there is no differentiable function $f :\Bbb R \to\Bbb R$ such that $f(0) = 1$ and $f'(x) ≥ (f(x))^2\space\forall x\in\Bbb R$,Show that there is no differentiable function  such that  and,f :\Bbb R \to\Bbb R f(0) = 1 f'(x) ≥ (f(x))^2\space\forall x\in\Bbb R,"Show that there is no differentiable function $f :\Bbb R \to\Bbb R$ such that $f(0) = 1$ and $f'(x) ≥ (f(x))^2\space\forall x\in\Bbb R$ . My attempt: Suppose there is a differentiable function such that the above statement holds true. Then, as the function is differentiable so it will satisfy the mean value theorem. $$f(x)-f(0)=f'(c)(x-0),x>0,c \in (0,x)$$ $$f(x)-1=f'(c)x$$ $$f(x)-1 \ge (f(c))^2 x$$ $$f(x)-(f(c))^2 x -1 \ge 0$$ If we let $h(x) = f(x)-(f(c))^2.x-1$ then $h'(x) \ge 0$ then $f'(x)-(f(c))^2 \ge 0$ . How do I proceed after this and obtain a contradiction.","Show that there is no differentiable function such that and . My attempt: Suppose there is a differentiable function such that the above statement holds true. Then, as the function is differentiable so it will satisfy the mean value theorem. If we let then then . How do I proceed after this and obtain a contradiction.","f :\Bbb R \to\Bbb R f(0) = 1 f'(x) ≥ (f(x))^2\space\forall x\in\Bbb R f(x)-f(0)=f'(c)(x-0),x>0,c \in (0,x) f(x)-1=f'(c)x f(x)-1 \ge (f(c))^2 x f(x)-(f(c))^2 x -1 \ge 0 h(x) = f(x)-(f(c))^2.x-1 h'(x) \ge 0 f'(x)-(f(c))^2 \ge 0","['real-analysis', 'analysis', 'problem-solving']"
52,Closed form of the sum $\sum _{n=1}^{\infty }\:\left(\frac{1}{x^2-n^2\pi ^2}\right)$,Closed form of the sum,\sum _{n=1}^{\infty }\:\left(\frac{1}{x^2-n^2\pi ^2}\right),Came across this in an old textbook and I'm struggling to simplify this in any way. I tried to integrate it and write it as a product: $\int y\:=\:\frac{1}{2x}\left(\log\prod \:_{n=1}^{\infty }\left(x^2-n^2\pi ^2\right)\right)$ but that doesn't seem to help and neither did my attempt at partial fraction decomposition. Any help would be appreciated!,Came across this in an old textbook and I'm struggling to simplify this in any way. I tried to integrate it and write it as a product: but that doesn't seem to help and neither did my attempt at partial fraction decomposition. Any help would be appreciated!,\int y\:=\:\frac{1}{2x}\left(\log\prod \:_{n=1}^{\infty }\left(x^2-n^2\pi ^2\right)\right),"['real-analysis', 'sequences-and-series', 'infinite-product']"
53,"Under what conditions is it true that if $f(x) \sim g(x)$ as $x \rightarrow x_0$, then $f'(x) \sim g'(x)$ as $x \rightarrow x_0$?","Under what conditions is it true that if  as , then  as ?",f(x) \sim g(x) x \rightarrow x_0 f'(x) \sim g'(x) x \rightarrow x_0,"So we say that two functions $f(x)$ and $g(x)$ of a real variable are asymptotic to each other as $x \rightarrow x_0$ ( $x_0$ need not be finite) if $$ \lim_{x \rightarrow x_0} \frac{f(x)}{g(x)} =1,$$ and in that case we write $f(x) \sim g(x)$ as $x \rightarrow x_0$ . It is also not true generally that if $f(x) \sim g(x)$ , then $f'(x) \sim g'(x)$ . To see this, consider the cases $f(x) = x + \sin(x)$ and $g(x) = x$ . In this cases it is true that $x + \sin(x) \sim x$ as $x \rightarrow \infty$ , yet it is not true $1 + \cos(x) \sim 1$ as $x \rightarrow \infty$ (in fact the latter limit does not even exist). My question is, under what conditions on $f(x)$ and $g(x)$ can we say that it is true that if $f(x) \sim g(x)$ , then $f'(x) \sim g'(x)$ as $x \rightarrow x_0$ .","So we say that two functions and of a real variable are asymptotic to each other as ( need not be finite) if and in that case we write as . It is also not true generally that if , then . To see this, consider the cases and . In this cases it is true that as , yet it is not true as (in fact the latter limit does not even exist). My question is, under what conditions on and can we say that it is true that if , then as .","f(x) g(x) x \rightarrow x_0 x_0  \lim_{x \rightarrow x_0} \frac{f(x)}{g(x)} =1, f(x) \sim g(x) x \rightarrow x_0 f(x) \sim g(x) f'(x) \sim g'(x) f(x) = x + \sin(x) g(x) = x x + \sin(x) \sim x x \rightarrow \infty 1 + \cos(x) \sim 1 x \rightarrow \infty f(x) g(x) f(x) \sim g(x) f'(x) \sim g'(x) x \rightarrow x_0","['real-analysis', 'calculus', 'asymptotics']"
54,The integral of $\mathrm{e}^{-\frac{1}{\sin^2 x}}$,The integral of,\mathrm{e}^{-\frac{1}{\sin^2 x}},"I have tried playing around an integral $$ \int \mathrm{e}^{-\frac{1}{\sin^2 x}} \,\mathrm{d}x . $$ I know this integral does not have a closed form (Wolfram Alpha and many more resources!), but I appreciate an approximation that holds as close as possible to this integral. The Taylor series is not accurate. It can be assumed that the bounds of the integral are $0$ and $x$ , where $0<x<\frac{\pi}{2}$ . Appendix Thanks to the @robjohn's useful answer, I have simulated his three approaches (in the original order presented) for $0<x<\frac{\pi}{2}$ in the following figure: The blue solid line denotes the integral which should be approximated and the other curves are three closed forms for that. Unfortunately, there is a bad divergence between the approximations and the integral after about $x=0.9$ .","I have tried playing around an integral I know this integral does not have a closed form (Wolfram Alpha and many more resources!), but I appreciate an approximation that holds as close as possible to this integral. The Taylor series is not accurate. It can be assumed that the bounds of the integral are and , where . Appendix Thanks to the @robjohn's useful answer, I have simulated his three approaches (in the original order presented) for in the following figure: The blue solid line denotes the integral which should be approximated and the other curves are three closed forms for that. Unfortunately, there is a bad divergence between the approximations and the integral after about .","
\int \mathrm{e}^{-\frac{1}{\sin^2 x}} \,\mathrm{d}x
.
 0 x 0<x<\frac{\pi}{2} 0<x<\frac{\pi}{2} x=0.9","['real-analysis', 'integration', 'analysis', 'improper-integrals']"
55,Limit $\lim_{x\rightarrow0}\frac{1}{x}\int_{0}^{\sin x}\sin \frac{1}{t} \cos t^{2}\mathrm{d}t$,Limit,\lim_{x\rightarrow0}\frac{1}{x}\int_{0}^{\sin x}\sin \frac{1}{t} \cos t^{2}\mathrm{d}t,"Determine whether or not the limit below exists. $$\lim_{x\rightarrow0}\frac{1}{x}\int_{0}^{\sin x}\sin \frac{1}{t} \cos t^{2}\mathrm{d}t$$ I tried to use the Mean value theorem integrals to prove the limit exists, but it does not exist for $\lim\limits_{x\rightarrow0}\sin \frac{1}{x}$ . So I guessed the limit does not exist and used the Cauchy principle to prove it, but I failed. Any idea will be helpful.","Determine whether or not the limit below exists. I tried to use the Mean value theorem integrals to prove the limit exists, but it does not exist for . So I guessed the limit does not exist and used the Cauchy principle to prove it, but I failed. Any idea will be helpful.",\lim_{x\rightarrow0}\frac{1}{x}\int_{0}^{\sin x}\sin \frac{1}{t} \cos t^{2}\mathrm{d}t \lim\limits_{x\rightarrow0}\sin \frac{1}{x},"['real-analysis', 'integration', 'limits']"
56,Prove that $\lim_{n\to\infty}n^2\int_0^{\frac{1}{n}}x^{x+1}dx=\frac{1}{2}.$,Prove that,\lim_{n\to\infty}n^2\int_0^{\frac{1}{n}}x^{x+1}dx=\frac{1}{2}.,"Question: Prove that $$\lim_{n\to\infty}n^2\int_0^{\frac{1}{n}}x^{x+1}dx=\frac{1}{2}.$$ Solution: Let $$I_n:=n^2\int_0^{\frac{1}{n}}x^{x+1}dx, \forall \in\mathbb{N}.$$ Substituting $nx=t$ in $I_n$ , we have $$I_n=n\int_0^1\left(\frac{t}{n}\right)^{1+\frac{t}{n}}dt.$$ Now for all $0\le t\le 1$ and for all $n\in\mathbb{N}, n+t\le n+1\implies 1+\frac{t}{n}\le1+\frac{1}{n}.$ This implies that for all $0\le t\le 1$ and for all $n\in\mathbb{N}$ , we have $$\left(\frac{t}{n}\right)^{1+\frac{t}{n}}\ge \left(\frac{t}{n}\right)^{1+\frac{1}{n}}.$$ Therefore, for all $n\in\mathbb{N},$ $$\int_0^1\left(\frac{t}{n}\right)^{1+\frac{t}{n}}dt\ge \int_0^1\left(\frac{t}{n}\right)^{1+\frac{1}{n}}dt=n^{-\left(1+\frac{1}{n}\right)}\frac{n}{2n+1}.$$ This implies that $$I_n\ge n^{-\frac{1}{n}}\frac{n}{2n+1},\forall n\in\mathbb{N}.$$ Next note that for all $0\le t\le 1$ and for all $n\in\mathbb{N}$ , $1+\frac{t}{n}>1$ , which implies that $\left(\frac{t}{n}\right)^{1+\frac{t}{n}}<\frac{t}{n}.$ Therefore, $$\int_0^1\left(\frac{t}{n}\right)^{1+\frac{t}{n}}dt<\int_0^1\left(\frac{t}{n}\right)dt=\frac{1}{2n}.$$ This implies that $$I_n<\frac{1}{2},\forall n\in\mathbb{N}.$$ Thus, for all $n\in\mathbb{N}$ , we have $$n^{-\frac{1}{n}}\frac{n}{2n+1}\le I_n<\frac{1}{2}.$$ Now since $$\lim_{n\to\infty}n^{-\frac{1}{n}}\frac{n}{2n+1}=\frac{1}{2},$$ therefore by Sandwich theorem we can conclude that $$\lim_{n\to\infty}I_n=\frac{1}{2}.$$ Is this solution correct and rigorous enough and is there any other way to solve the problem?","Question: Prove that Solution: Let Substituting in , we have Now for all and for all This implies that for all and for all , we have Therefore, for all This implies that Next note that for all and for all , , which implies that Therefore, This implies that Thus, for all , we have Now since therefore by Sandwich theorem we can conclude that Is this solution correct and rigorous enough and is there any other way to solve the problem?","\lim_{n\to\infty}n^2\int_0^{\frac{1}{n}}x^{x+1}dx=\frac{1}{2}. I_n:=n^2\int_0^{\frac{1}{n}}x^{x+1}dx, \forall \in\mathbb{N}. nx=t I_n I_n=n\int_0^1\left(\frac{t}{n}\right)^{1+\frac{t}{n}}dt. 0\le t\le 1 n\in\mathbb{N}, n+t\le n+1\implies 1+\frac{t}{n}\le1+\frac{1}{n}. 0\le t\le 1 n\in\mathbb{N} \left(\frac{t}{n}\right)^{1+\frac{t}{n}}\ge \left(\frac{t}{n}\right)^{1+\frac{1}{n}}. n\in\mathbb{N}, \int_0^1\left(\frac{t}{n}\right)^{1+\frac{t}{n}}dt\ge \int_0^1\left(\frac{t}{n}\right)^{1+\frac{1}{n}}dt=n^{-\left(1+\frac{1}{n}\right)}\frac{n}{2n+1}. I_n\ge n^{-\frac{1}{n}}\frac{n}{2n+1},\forall n\in\mathbb{N}. 0\le t\le 1 n\in\mathbb{N} 1+\frac{t}{n}>1 \left(\frac{t}{n}\right)^{1+\frac{t}{n}}<\frac{t}{n}. \int_0^1\left(\frac{t}{n}\right)^{1+\frac{t}{n}}dt<\int_0^1\left(\frac{t}{n}\right)dt=\frac{1}{2n}. I_n<\frac{1}{2},\forall n\in\mathbb{N}. n\in\mathbb{N} n^{-\frac{1}{n}}\frac{n}{2n+1}\le I_n<\frac{1}{2}. \lim_{n\to\infty}n^{-\frac{1}{n}}\frac{n}{2n+1}=\frac{1}{2}, \lim_{n\to\infty}I_n=\frac{1}{2}.","['real-analysis', 'integration', 'definite-integrals', 'solution-verification']"
57,"If$|f(x)-f(y)|\le (x-y)^2$, prove that $f$ is constant","If, prove that  is constant",|f(x)-f(y)|\le (x-y)^2 f,"(Baby Rudin Chapter 5 Exercise 1) Let $f$ be defined for all real $x$ , and suppose that \begin{equation}\tag{1}     |f(x)-f(y)|\le (x-y)^2 \end{equation} Prove that $f$ is constant. My attempt: Let $f$ be defined for all real-valued inputs. Let $x \in \mathbb{R}$ and $y \in \mathbb{R} \smallsetminus \{ x \}$ , and suppose that (1) holds. Then, we have: \begin{align*}         \left| \dfrac{f(x)-f(y)}{x-y}\right| \le (x-y)     \end{align*} As $x\to y,  \lim\limits_{x \to y}\left| \dfrac{f(x)-f(y)}{x-y}\right| \le 0$ . Since it cannot be that $\left|f'(y)\right| < 0$ , we have that $\left|f'(y)\right| = 0 \implies f'(y) = 0$ . Can someone please read over my proof and let me know if it is correct?","(Baby Rudin Chapter 5 Exercise 1) Let be defined for all real , and suppose that Prove that is constant. My attempt: Let be defined for all real-valued inputs. Let and , and suppose that (1) holds. Then, we have: As . Since it cannot be that , we have that . Can someone please read over my proof and let me know if it is correct?","f x \begin{equation}\tag{1}
    |f(x)-f(y)|\le (x-y)^2
\end{equation} f f x \in \mathbb{R} y \in \mathbb{R} \smallsetminus \{ x \} \begin{align*}
        \left| \dfrac{f(x)-f(y)}{x-y}\right| \le (x-y)
    \end{align*} x\to y,  \lim\limits_{x \to y}\left| \dfrac{f(x)-f(y)}{x-y}\right| \le 0 \left|f'(y)\right| < 0 \left|f'(y)\right| = 0 \implies f'(y) = 0","['real-analysis', 'solution-verification']"
58,"Prove that $\exists a, b\in(0,1)$ such that $\int_0^{a} xf(x)dx=0\text{ and }\int_0^bxf(x)dx=\frac{b^2f(b)}{2}.$",Prove that  such that,"\exists a, b\in(0,1) \int_0^{a} xf(x)dx=0\text{ and }\int_0^bxf(x)dx=\frac{b^2f(b)}{2}.","Question: Let $f:[0,1]\to\mathbb{R}$ be a continuous function such that $f(0)=0$ and $$\int_0^1f(x)dx=0.$$ Prove that $\exists a, b\in(0,1)$ such that $$\int_0^{a} xf(x)dx=0\text{ and }\int_0^bxf(x)dx=\frac{b^2f(b)}{2}.$$ My approach: Let $g:[0,1]\to\mathbb{R}$ be such that $$g(x)=x\int_0^xf(t)dt-\int_0^xtf(t)dt, \forall x\in[0,1].$$ By the first fundamental theorem of calculus we can conclude that $g$ is differentiable on $[0,1]$ and $$g'(x)=\int_0^xf(t)dt, \forall x\in[0,1].$$ Also, observe that $g(0)=0$ and $g(1)=-\int_0^1tf(t)dt$ . Thus, by applying MVT to the function $g$ on the interval $[0,1]$ , we can conclude that, $\exists c\in(0,1)$ such that $$g'(c)=\int_0^cf(t)dt=-\int_0^1tf(t)dt.$$ Observe that clearly three cases are possible, i.e, either $$\int_0^cf(t)dt<0\text{ or }\int_0^cf(t)dt=0\text{ or }\int_0^cf(t)dt>0.$$ Now let $h:[0,1]\to\mathbb{R}$ be such that $$h(x)=\int_0^xtf(t)dt, \forall x\in[0,1].$$ Please note that the part highlighted below is wrong, but still I have included it to just demonstrate my thinking process, as it might be of some help to others trying out this problem. Observe that if $\int_0^cf(t)dt<0$ , then, $h(1)>0$ . This also implies that $\exists$ an open interval $(d,e)\in[0,c]$ , such that $f(t)<0, \forall t\in(d,e)$ . Now select any point $c_1\in(d,e)$ . Applying MVT to the function $h$ on the interval $[0,c_1]$ , we can conclude that $\exists c_2\in(0,c_1)$ such that $$h'(c_2).c_1=f(c_2).c_2.c_1=h(c_1)-h(0)=h(c_1)<0.$$ Now $h(c_1)<0$ and $h(1)>0$ . Thus, by IVT we can conclude that $\exists a\in(c_1,1)\subseteq(0,1)$ , such that $$h(a)=\int_0^af(t)dt=0.$$ A similar reasoning for the case when $\int_0^cf(t)dt>0,$ shows that $\exists a\in(0,1)$ , such that $$h(a)=\int_0^af(t)dt=0.$$ Now finally if $\int_0^cf(t)dt=0$ , then we will have $h(1)=0$ . Now if $f$ is identically equal to $0$ on $[0,c]$ , then clearly $tf(t)=0, \forall t\in[0,c]\implies h(x)=0, \forall x\in[0,c].$ Thus choosing any point $x\in(0,c]$ and setting it as $a$ , we will have $h(a)=0$ and we will be done in that case. Now if $f$ acquires both positive and negative values on $[0,c]$ , then we can conclude that $\exists c_1,c_2\in(0,c)$ , such that $f(c_1)>0$ and $f(c_2)<0$ . Also, let us assume WLOG that $c_2>c_1$ . I have not been able to make any significant approach other than this. Can someone help me out with this problem? Please note that a solution using integration by parts might not be possible, since $f$ is not a differentiable function.","Question: Let be a continuous function such that and Prove that such that My approach: Let be such that By the first fundamental theorem of calculus we can conclude that is differentiable on and Also, observe that and . Thus, by applying MVT to the function on the interval , we can conclude that, such that Observe that clearly three cases are possible, i.e, either Now let be such that Please note that the part highlighted below is wrong, but still I have included it to just demonstrate my thinking process, as it might be of some help to others trying out this problem. Observe that if , then, . This also implies that an open interval , such that . Now select any point . Applying MVT to the function on the interval , we can conclude that such that Now and . Thus, by IVT we can conclude that , such that A similar reasoning for the case when shows that , such that Now finally if , then we will have . Now if is identically equal to on , then clearly Thus choosing any point and setting it as , we will have and we will be done in that case. Now if acquires both positive and negative values on , then we can conclude that , such that and . Also, let us assume WLOG that . I have not been able to make any significant approach other than this. Can someone help me out with this problem? Please note that a solution using integration by parts might not be possible, since is not a differentiable function.","f:[0,1]\to\mathbb{R} f(0)=0 \int_0^1f(x)dx=0. \exists a, b\in(0,1) \int_0^{a} xf(x)dx=0\text{ and }\int_0^bxf(x)dx=\frac{b^2f(b)}{2}. g:[0,1]\to\mathbb{R} g(x)=x\int_0^xf(t)dt-\int_0^xtf(t)dt, \forall x\in[0,1]. g [0,1] g'(x)=\int_0^xf(t)dt, \forall x\in[0,1]. g(0)=0 g(1)=-\int_0^1tf(t)dt g [0,1] \exists c\in(0,1) g'(c)=\int_0^cf(t)dt=-\int_0^1tf(t)dt. \int_0^cf(t)dt<0\text{ or }\int_0^cf(t)dt=0\text{ or }\int_0^cf(t)dt>0. h:[0,1]\to\mathbb{R} h(x)=\int_0^xtf(t)dt, \forall x\in[0,1]. \int_0^cf(t)dt<0 h(1)>0 \exists (d,e)\in[0,c] f(t)<0, \forall t\in(d,e) c_1\in(d,e) h [0,c_1] \exists c_2\in(0,c_1) h'(c_2).c_1=f(c_2).c_2.c_1=h(c_1)-h(0)=h(c_1)<0. h(c_1)<0 h(1)>0 \exists a\in(c_1,1)\subseteq(0,1) h(a)=\int_0^af(t)dt=0. \int_0^cf(t)dt>0, \exists a\in(0,1) h(a)=\int_0^af(t)dt=0. \int_0^cf(t)dt=0 h(1)=0 f 0 [0,c] tf(t)=0, \forall t\in[0,c]\implies h(x)=0, \forall x\in[0,c]. x\in(0,c] a h(a)=0 f [0,c] \exists c_1,c_2\in(0,c) f(c_1)>0 f(c_2)<0 c_2>c_1 f","['real-analysis', 'integration']"
59,Experiences with Folland's Real Analysis Textbook,Experiences with Folland's Real Analysis Textbook,,I am thinking of self studying the first six chapters of Folland's Real Analysis: Modern techniques and Their Applications . I had read the first six chapters of Baby Rudin in the first real analysis course I had taken and would love to hear what people think of Folland's book for a second real analysis course. Has anyone read this book as an undergraduate? Is it too challenging for an undergraduate student? Bonus: Does anyone have any other suggestions for a different textbook that can be used in a  second semester of real analysis? I've read about Spivak's book but I don't think a physics-based analysis course is relevant to me (I want to pursue graduate-level statistics in a couple years). Edit: I would love to learn some measure theory.,I am thinking of self studying the first six chapters of Folland's Real Analysis: Modern techniques and Their Applications . I had read the first six chapters of Baby Rudin in the first real analysis course I had taken and would love to hear what people think of Folland's book for a second real analysis course. Has anyone read this book as an undergraduate? Is it too challenging for an undergraduate student? Bonus: Does anyone have any other suggestions for a different textbook that can be used in a  second semester of real analysis? I've read about Spivak's book but I don't think a physics-based analysis course is relevant to me (I want to pursue graduate-level statistics in a couple years). Edit: I would love to learn some measure theory.,,"['real-analysis', 'book-recommendation']"
60,Derivative of the Determinant of the Jacobian Matrix,Derivative of the Determinant of the Jacobian Matrix,,"Let $f:\mathbb{R}^n\to \mathbb{R}^n$ be a smooth vector field, with flow $\phi_t$ taking values in $\mathbb{R}^d$ i.e $\partial_t \phi_t= f(\phi_t)$ , and $\phi_0=id$ . Let $J_t(x) := \det \Big(D_x(\phi_t(x))\Big)$ be the determinant of the Jacobian matrix $D_x(\phi_t(x))$ . Is it really obvious that  : $$ \partial_t \det \Big(D_x(\phi_t(x))\Big)=\text{div}\Big(f(\phi_t(x)) \Big)J_t(x) ?$$ To prove this does one have to write out the entire formula for the detemrinant of an $n\times n$ matrix?","Let be a smooth vector field, with flow taking values in i.e , and . Let be the determinant of the Jacobian matrix . Is it really obvious that  : To prove this does one have to write out the entire formula for the detemrinant of an matrix?",f:\mathbb{R}^n\to \mathbb{R}^n \phi_t \mathbb{R}^d \partial_t \phi_t= f(\phi_t) \phi_0=id J_t(x) := \det \Big(D_x(\phi_t(x))\Big) D_x(\phi_t(x))  \partial_t \det \Big(D_x(\phi_t(x))\Big)=\text{div}\Big(f(\phi_t(x)) \Big)J_t(x) ? n\times n,"['real-analysis', 'calculus', 'multivariable-calculus', 'jacobian', 'divergence-operator']"
61,"Does Rudin's definition of ""bounded"" overlook the empty set?","Does Rudin's definition of ""bounded"" overlook the empty set?",,"In Definition $2.18$ of Baby Rudin, Rudin defines boundedness for metric spaces as follows: given a metric space $X$ , and a set $E \subset X$ , we say $E$ is bounded if there exists a real number $M$ and a point $q \in X$ such that $d(p,q) < M$ for all $p \in E$ . Per this definition, if $E=X=\varnothing$ , then $E$ is unbounded. The reason is that ""there exists a point $q \in X$ "" is a false statement if $X = \varnothing$ . Should this be regarded as a minor mistake/typo in Baby Rudin? Or are there particular reasons the empty set should be considered unbounded? If we use the alternative definition that a set $E$ is bounded if there exists a real number $M$ such that for all $p,q \in E$ , $d(p,q) < M$ , then this coincides with the above definition for nonempty sets, but also always considers the empty set bounded.","In Definition of Baby Rudin, Rudin defines boundedness for metric spaces as follows: given a metric space , and a set , we say is bounded if there exists a real number and a point such that for all . Per this definition, if , then is unbounded. The reason is that ""there exists a point "" is a false statement if . Should this be regarded as a minor mistake/typo in Baby Rudin? Or are there particular reasons the empty set should be considered unbounded? If we use the alternative definition that a set is bounded if there exists a real number such that for all , , then this coincides with the above definition for nonempty sets, but also always considers the empty set bounded.","2.18 X E \subset X E M q \in X d(p,q) < M p \in E E=X=\varnothing E q \in X X = \varnothing E M p,q \in E d(p,q) < M",['real-analysis']
62,Integral $\int_0^{\pi/2}x\arctan\left(\tfrac{1}{\sqrt3}+\tfrac{2}{\sqrt3}\tan x\right)dx$,Integral,\int_0^{\pi/2}x\arctan\left(\tfrac{1}{\sqrt3}+\tfrac{2}{\sqrt3}\tan x\right)dx,"Evaluate the integral $$P=\int_0^{\pi/2}x\arctan\left(\tfrac{1}{\sqrt3}+\tfrac{2}{\sqrt3}\tan x\right)dx.$$ Context: I started trying to evaluate the integral $$J=\int_0^\infty \frac{\arctan(x)^2}{x^2+x+1}dx,$$ and the integral $P$ is part of the process. At first, I tried $x\mapsto 1/x$ , but it just ended up showing that $$J=\frac{\pi^2}{4}\int_0^\infty \frac{dx}{x^2+x+1}-\pi\int_0^\infty \frac{\arctan x}{x^2+x+1}dx+J,$$ which is of no use. Next, I tried integration by parts, using $$\int\frac{dx}{x^2+x+1}=\frac{2}{\sqrt3}\arctan\frac{2x+1}{\sqrt3},$$ so that $$J=\frac{\pi^3}{4\sqrt3}-\frac{4}{\sqrt3}\int_0^\infty\arctan(x)\arctan\left(\tfrac1{\sqrt3}+\tfrac{2}{\sqrt3}x\right)\frac{dx}{1+x^2}.$$ Then with $x\mapsto \tan x$ we have $$J=\frac{\pi^3}{4\sqrt3}-\frac{4}{\sqrt3}P.$$ Theoretically, integration by parts if possible from this point, as Wolfram provides an awful closed form for the anti-derivative of $\arctan\left(\tfrac{1}{\sqrt3}+\tfrac{2}{\sqrt3}\tan x\right)$ , but I do not think this is really that realistic of an approach. Is there a better way to evaluate the integral $P$ ?","Evaluate the integral Context: I started trying to evaluate the integral and the integral is part of the process. At first, I tried , but it just ended up showing that which is of no use. Next, I tried integration by parts, using so that Then with we have Theoretically, integration by parts if possible from this point, as Wolfram provides an awful closed form for the anti-derivative of , but I do not think this is really that realistic of an approach. Is there a better way to evaluate the integral ?","P=\int_0^{\pi/2}x\arctan\left(\tfrac{1}{\sqrt3}+\tfrac{2}{\sqrt3}\tan x\right)dx. J=\int_0^\infty \frac{\arctan(x)^2}{x^2+x+1}dx, P x\mapsto 1/x J=\frac{\pi^2}{4}\int_0^\infty \frac{dx}{x^2+x+1}-\pi\int_0^\infty \frac{\arctan x}{x^2+x+1}dx+J, \int\frac{dx}{x^2+x+1}=\frac{2}{\sqrt3}\arctan\frac{2x+1}{\sqrt3}, J=\frac{\pi^3}{4\sqrt3}-\frac{4}{\sqrt3}\int_0^\infty\arctan(x)\arctan\left(\tfrac1{\sqrt3}+\tfrac{2}{\sqrt3}x\right)\frac{dx}{1+x^2}. x\mapsto \tan x J=\frac{\pi^3}{4\sqrt3}-\frac{4}{\sqrt3}P. \arctan\left(\tfrac{1}{\sqrt3}+\tfrac{2}{\sqrt3}\tan x\right) P","['real-analysis', 'integration', 'definite-integrals', 'closed-form', 'trigonometric-integrals']"
63,Non standard solution to $f(x) = \frac{1}{2}\Big(f(\frac{x}{2}) + f(\frac{1+x}{2})\Big)$,Non standard solution to,f(x) = \frac{1}{2}\Big(f(\frac{x}{2}) + f(\frac{1+x}{2})\Big),"Final update: on 11/29/2019: I have worked on this a bit more, and wrote an article summarizing all the main findings. You can read it here . This functional equation appears in the following context. Let $\alpha\in[0,1]$ be an irrational number (called seed ) and consider the sequence $x_n=\{2^n \alpha\}$ . Here the brackets represent the fractional part function. In particular, $\lfloor 2x_n\rfloor$ is the $n$ -th digit of $\alpha$ in base $2$ . The values $x_n$ are distributed in a certain way due to the ergodicity of the underlying process. The density associated with this distribution is the function $f$ , and for the immense majority of seeds $\alpha$ that density is uniform on $[0, 1]$ , that is, $f(x) = 1, x \in [0, 1]$ . Such seeds $\alpha$ producing the uniform density are sometimes called normal numbers; their digit distribution is also uniform. However, the functional equation $f(x) = \frac{1}{2}\Big(f(\frac{x}{2}) + f(\frac{1+x}{2})\Big)$ may have plenty of other solutions. Such solutions are called non-standard solutions. Can you find a seed $\alpha$ producing a non-standard solution, with an explicit form for $f$ ? Maybe a step-wise uniform function? The set of seeds producing non-standard solutions is known to have Lebesgue measure zero, but there are infinitely many such seeds. All rational seeds $\alpha$ work, but they produce a discrete distribution. Thus their density is of the discrete type. Here however, I am interested in a continuous function $f$ , even if it has infinitely many points of discontinuity (that is, a function $f$ continuous almost everywhere: the set of discontinuity points has Lebesgue measure zero.) Update I am looking for a function $f$ that is a density on $[0, 1]$ , so there are additional constraints here: $\int_0^1 f(x)\,dx = 1$ and $f(x) \geq 0$ . However, note that if $f$ is a solution, then $cf$ is also a solution regardless of the constant $c$ . So any solution can be normalized to integrate to one. Also, $cf+d$ is also a solution ( $c, d$ constants). Second update Below is a density satisfying all the requirements. Actually, the plot below represents its percentile distribution. It was produced with a seed $\alpha$ built as follows: its $n$ -th binary digit is $1$ if $\mbox{Rand}(n) < 0.75$ , and $0$ otherwise, using a pseudo random number generator. Note that $P._{25} = 0.5$ and corresponds to a dip ( $P._{25}$ denotes the $25$ -th percentile.) Dips are everywhere, only the big ones are visible. By contrast, the percentile distribution for the uniform case (if you replace $0.75$ by $0.50$ in $\mbox{Rand}(n) < 0.75$ ) is a straight line, with no dips. Note: I eventually answered my question, see the second answer.","Final update: on 11/29/2019: I have worked on this a bit more, and wrote an article summarizing all the main findings. You can read it here . This functional equation appears in the following context. Let be an irrational number (called seed ) and consider the sequence . Here the brackets represent the fractional part function. In particular, is the -th digit of in base . The values are distributed in a certain way due to the ergodicity of the underlying process. The density associated with this distribution is the function , and for the immense majority of seeds that density is uniform on , that is, . Such seeds producing the uniform density are sometimes called normal numbers; their digit distribution is also uniform. However, the functional equation may have plenty of other solutions. Such solutions are called non-standard solutions. Can you find a seed producing a non-standard solution, with an explicit form for ? Maybe a step-wise uniform function? The set of seeds producing non-standard solutions is known to have Lebesgue measure zero, but there are infinitely many such seeds. All rational seeds work, but they produce a discrete distribution. Thus their density is of the discrete type. Here however, I am interested in a continuous function , even if it has infinitely many points of discontinuity (that is, a function continuous almost everywhere: the set of discontinuity points has Lebesgue measure zero.) Update I am looking for a function that is a density on , so there are additional constraints here: and . However, note that if is a solution, then is also a solution regardless of the constant . So any solution can be normalized to integrate to one. Also, is also a solution ( constants). Second update Below is a density satisfying all the requirements. Actually, the plot below represents its percentile distribution. It was produced with a seed built as follows: its -th binary digit is if , and otherwise, using a pseudo random number generator. Note that and corresponds to a dip ( denotes the -th percentile.) Dips are everywhere, only the big ones are visible. By contrast, the percentile distribution for the uniform case (if you replace by in ) is a straight line, with no dips. Note: I eventually answered my question, see the second answer.","\alpha\in[0,1] x_n=\{2^n \alpha\} \lfloor 2x_n\rfloor n \alpha 2 x_n f \alpha [0, 1] f(x) = 1, x \in [0, 1] \alpha f(x) = \frac{1}{2}\Big(f(\frac{x}{2}) + f(\frac{1+x}{2})\Big) \alpha f \alpha f f f [0, 1] \int_0^1 f(x)\,dx = 1 f(x) \geq 0 f cf c cf+d c, d \alpha n 1 \mbox{Rand}(n) < 0.75 0 P._{25} = 0.5 P._{25} 25 0.75 0.50 \mbox{Rand}(n) < 0.75","['real-analysis', 'sequences-and-series', 'irrational-numbers', 'density-function']"
64,"If $f \in C[a,b]$ has $\int_{a}^{b}f(x)x^{n}dx=0$ for all $n\in \mathbb{N}$, then $f=0$.","If  has  for all , then .","f \in C[a,b] \int_{a}^{b}f(x)x^{n}dx=0 n\in \mathbb{N} f=0","Let $f \in C[a,b]$ with $$\int_{a}^{b}f(x)x^{n}dx=0$$ for all $n\in \mathbb{N}$ . Prove $f=0$ . I got the intuition to prove this with induction over $n \in \mathbb{N}$ , for $n=0$ ,   I have $\int_{a}^{b}f(x)dx=0$ . So how I got that $f=0$ ? Also, how I end up the proof? Any help will be appreciated. Thanks","Let with for all . Prove . I got the intuition to prove this with induction over , for ,   I have . So how I got that ? Also, how I end up the proof? Any help will be appreciated. Thanks","f \in C[a,b] \int_{a}^{b}f(x)x^{n}dx=0 n\in \mathbb{N} f=0 n \in \mathbb{N} n=0 \int_{a}^{b}f(x)dx=0 f=0","['real-analysis', 'calculus', 'integration', 'functional-analysis']"
65,What is the higher-order derivative test in multivariable calculus?,What is the higher-order derivative test in multivariable calculus?,,"In single-variable calculus, the second-derivative test states that if $x$ is a real number such that $f'(x)=0$, then: If $f''(x)>0$, then $f$ has a local minimum at $x$. If $f''(x)<0$, then $f$ has a local maximum at $x$. If $f''(x)=0$, then the text is inconclusive. But there's no need to despair if the second-derivative test is inconclusive, because there is the higher-order derivative test.  It states that if $x$ is a real number such that $f'(x)=0$, and $n$ is the smallest natural number such that $f^{(n)}(x)\neq 0$, then: If $n$ is even and $f^{(n)}>0$, then $f$ has a local minimum at $x$. If $n$ is even and $f^{(n)}<0$, then $f$ has a local manimum at $x$. If $n$ is odd, then $f$ has an inflection point at $x$. Similarly, in multivariable calculus the second-derivative test states that if $(x,y)$  is an ordered pair such that $\nabla f(x,y) = 0$, then: If $D(x,y)>0$ and $f_{xx}(x,y)>0$, then $f$ has a local minimum at $(x,y)$. If $D(x,y)>0$ and $f_{xx}(x,y)<0$, then $f$ has a local maximum at $(x,y)$. If $D(x,y)<0$, then $f$ has a saddle point at $(x,y)$. If $D(x,y)=0$, then the test is inconclusive. where $D(x,y)=f_{xx}(x,y)f_{yy}(x,y)-(f_{xy}(x,y))^2$ is the determinant of the Hessian matrix of $f$ evaluated at $(x,y)$. My question is, what do you do if this test is inconclusive?  What is the analogue of the higher-order derivative test in multivariable calculus?","In single-variable calculus, the second-derivative test states that if $x$ is a real number such that $f'(x)=0$, then: If $f''(x)>0$, then $f$ has a local minimum at $x$. If $f''(x)<0$, then $f$ has a local maximum at $x$. If $f''(x)=0$, then the text is inconclusive. But there's no need to despair if the second-derivative test is inconclusive, because there is the higher-order derivative test.  It states that if $x$ is a real number such that $f'(x)=0$, and $n$ is the smallest natural number such that $f^{(n)}(x)\neq 0$, then: If $n$ is even and $f^{(n)}>0$, then $f$ has a local minimum at $x$. If $n$ is even and $f^{(n)}<0$, then $f$ has a local manimum at $x$. If $n$ is odd, then $f$ has an inflection point at $x$. Similarly, in multivariable calculus the second-derivative test states that if $(x,y)$  is an ordered pair such that $\nabla f(x,y) = 0$, then: If $D(x,y)>0$ and $f_{xx}(x,y)>0$, then $f$ has a local minimum at $(x,y)$. If $D(x,y)>0$ and $f_{xx}(x,y)<0$, then $f$ has a local maximum at $(x,y)$. If $D(x,y)<0$, then $f$ has a saddle point at $(x,y)$. If $D(x,y)=0$, then the test is inconclusive. where $D(x,y)=f_{xx}(x,y)f_{yy}(x,y)-(f_{xy}(x,y))^2$ is the determinant of the Hessian matrix of $f$ evaluated at $(x,y)$. My question is, what do you do if this test is inconclusive?  What is the analogue of the higher-order derivative test in multivariable calculus?",,"['calculus', 'real-analysis', 'multivariable-calculus', 'hessian-matrix']"
66,Find the Limit of $‎\prod‎_{n=1}^{‎\infty}\frac{(1+‎\frac{1}{n}‎)^n}{(1+‎\frac{1}{n+x})^{n+x}}$,Find the Limit of,‎\prod‎_{n=1}^{‎\infty}\frac{(1+‎\frac{1}{n}‎)^n}{(1+‎\frac{1}{n+x})^{n+x}},‎Consider the following productions‎ ‎$$‎‎‎‎‎\prod‎_{n=1}^{‎\infty}\frac{1+‎\frac{1}{n}‎}{1+‎\frac{1}{n+x}}$$ and $$‎‎‎‎‎\prod‎_{n=1}^{‎\infty}\frac{(1+‎\frac{1}{n}‎)^n}{(1+‎\frac{1}{n+x})^{n+x}}$$ I know that the above are convergent. Can anyone find the limit of these products? ‎,‎Consider the following productions‎ ‎$$‎‎‎‎‎\prod‎_{n=1}^{‎\infty}\frac{1+‎\frac{1}{n}‎}{1+‎\frac{1}{n+x}}$$ and $$‎‎‎‎‎\prod‎_{n=1}^{‎\infty}\frac{(1+‎\frac{1}{n}‎)^n}{(1+‎\frac{1}{n+x})^{n+x}}$$ I know that the above are convergent. Can anyone find the limit of these products? ‎,,"['calculus', 'real-analysis', 'sequences-and-series', 'special-functions']"
67,Use of partitions of unity,Use of partitions of unity,,"So, I'm working through introduction to smooth manifolds, and I've seen the fact that partitions of unity exist . They seem like extremely useful theoretical tools, but I haven't seen them used yet. I'd like some nice examples as to where they can be used, and why they're ""special"" in some sense (I don't have a good intuition as to why you need a smooth manifold to exhibit this partition). What's the intuition with partitions of unity? asks the intuition of this object. I am after uses. Non-Theoretical Applications of Partitions of Unity asks about, well, applied uses. I want something along the lines of ""what is the most dazzling thing you can do with this tool"" - Something like the awesome uses of orthogonalization, or the spectral theorem you would see in linear algebra.","So, I'm working through introduction to smooth manifolds, and I've seen the fact that partitions of unity exist . They seem like extremely useful theoretical tools, but I haven't seen them used yet. I'd like some nice examples as to where they can be used, and why they're ""special"" in some sense (I don't have a good intuition as to why you need a smooth manifold to exhibit this partition). What's the intuition with partitions of unity? asks the intuition of this object. I am after uses. Non-Theoretical Applications of Partitions of Unity asks about, well, applied uses. I want something along the lines of ""what is the most dazzling thing you can do with this tool"" - Something like the awesome uses of orthogonalization, or the spectral theorem you would see in linear algebra.",,"['real-analysis', 'differential-geometry', 'differential-topology']"
68,Find all $f$ that satisfies $f:\mathbb{R}\rightarrow\mathbb{R};f(x+y)+f(x)f(y)=(1+x)f(y)+(1+y)f(x)+f(xy)$,Find all  that satisfies,f f:\mathbb{R}\rightarrow\mathbb{R};f(x+y)+f(x)f(y)=(1+x)f(y)+(1+y)f(x)+f(xy),"Find all $f$ that satisfies: $1, ~f:\mathbb{R}\rightarrow\mathbb{R};\\ 2,\forall x,y\in\mathbb{R},f(x+y)+f(x)f(y)=(1+x)f(y)+(1+y)f(x)+f(xy); $ Maybe we can prove it's derivable or it's a linear function. Any idea?","Find all $f$ that satisfies: $1, ~f:\mathbb{R}\rightarrow\mathbb{R};\\ 2,\forall x,y\in\mathbb{R},f(x+y)+f(x)f(y)=(1+x)f(y)+(1+y)f(x)+f(xy); $ Maybe we can prove it's derivable or it's a linear function. Any idea?",,"['real-analysis', 'functional-equations']"
69,On the Euler sum $\sum \limits_{n=1}^{\infty} \frac{H_n^{(4)} H_n^2}{n^6}$,On the Euler sum,\sum \limits_{n=1}^{\infty} \frac{H_n^{(4)} H_n^2}{n^6},"Here is an Euler sum I ran into. $$\mathcal{S} = \sum_{n=1}^{\infty} \frac{\mathcal{H}_n^{(4)} \mathcal{H}_n^2}{n^6}$$ where $\mathcal{H}_n^{(s)}$ is the generalised harmonic number of order $s$ . I have no idea to what this evaluates neither do I have the appropriate techniques to begin cracking it. We have seen quite plenty Euler sums but I don't think we have seen something like this before. Correct me if I am wrong! Any help? Edit: Maybe this link ( inverse sin ) is helpful for an integral approach. Update (by Editor): By stuffle relations of Multiple Zeta Values the result is: > $$S=\small -\frac{3}{20} \pi ^4 \zeta(6,2)-\frac{5}{3} \pi ^2 \zeta(8,2)+\frac{143}{4} \zeta(10,2)-6\zeta(8,2,1,1)+\frac{\zeta (3)^4}{3}-\frac{23 \pi ^6 \zeta (3)^2}{1620}-\frac{8}{45} \pi ^4 \zeta (5) \zeta (3)-\frac{31}{3} \pi ^2 \zeta (7) \zeta (3)+\frac{2531 \zeta (9) \zeta (3)}{18}-\frac{9 \pi ^2 \zeta (5)^2}{2}+\frac{1115 \zeta (5) \zeta (7)}{8}-\frac{964213 \pi ^{12}}{8756748000}$$",Here is an Euler sum I ran into. where is the generalised harmonic number of order . I have no idea to what this evaluates neither do I have the appropriate techniques to begin cracking it. We have seen quite plenty Euler sums but I don't think we have seen something like this before. Correct me if I am wrong! Any help? Edit: Maybe this link ( inverse sin ) is helpful for an integral approach. Update (by Editor): By stuffle relations of Multiple Zeta Values the result is: >,"\mathcal{S} = \sum_{n=1}^{\infty} \frac{\mathcal{H}_n^{(4)} \mathcal{H}_n^2}{n^6} \mathcal{H}_n^{(s)} s S=\small -\frac{3}{20} \pi ^4 \zeta(6,2)-\frac{5}{3} \pi ^2 \zeta(8,2)+\frac{143}{4} \zeta(10,2)-6\zeta(8,2,1,1)+\frac{\zeta (3)^4}{3}-\frac{23 \pi ^6 \zeta (3)^2}{1620}-\frac{8}{45} \pi ^4 \zeta (5) \zeta (3)-\frac{31}{3} \pi ^2 \zeta (7) \zeta (3)+\frac{2531 \zeta (9) \zeta (3)}{18}-\frac{9 \pi ^2 \zeta (5)^2}{2}+\frac{1115 \zeta (5) \zeta (7)}{8}-\frac{964213 \pi ^{12}}{8756748000}","['real-analysis', 'sequences-and-series', 'complex-analysis']"
70,"Is f(x,y) integrable? Question 3-7 from Spivak's Calculus on Manifolds","Is f(x,y) integrable? Question 3-7 from Spivak's Calculus on Manifolds",,"I am trying to work my through the exercises in Spivak's Calculus on Manifolds. I am currently working on the exercises in Chapter 3 which deals with Integration. I am having trouble with the following question: Let: \begin{equation}   f(x,y)=\begin{cases}     0, & \text{if $x$ is irrational}.\\     0, & \text{if $x$ is rational, $y$ is irrational}. \\     1/q, & \text{if $x$ is rational, $y=p/q$ in lowest terms}.   \end{cases} \end{equation} Show that $f$ is integrable  on $A = [0,1] \times [0,1]$ and $\int_A f = 0$. I was thinking of trying to prove that this set is Jordan Measurable and that it's Jordan measure is zero and that it is therefore Riemann Integrable but I am not sure how to do this or if it is even the best way to solve this problem. If I could show that $f$ is continuous on $A$ up to a set of Jordan Measure $0$, then $f$ would be integrable but again, I'm not sure I can do this or if its even appropriate for this problem. Any assistance that anyone could provide would be greatly appreciated. Thank you.","I am trying to work my through the exercises in Spivak's Calculus on Manifolds. I am currently working on the exercises in Chapter 3 which deals with Integration. I am having trouble with the following question: Let: \begin{equation}   f(x,y)=\begin{cases}     0, & \text{if $x$ is irrational}.\\     0, & \text{if $x$ is rational, $y$ is irrational}. \\     1/q, & \text{if $x$ is rational, $y=p/q$ in lowest terms}.   \end{cases} \end{equation} Show that $f$ is integrable  on $A = [0,1] \times [0,1]$ and $\int_A f = 0$. I was thinking of trying to prove that this set is Jordan Measurable and that it's Jordan measure is zero and that it is therefore Riemann Integrable but I am not sure how to do this or if it is even the best way to solve this problem. If I could show that $f$ is continuous on $A$ up to a set of Jordan Measure $0$, then $f$ would be integrable but again, I'm not sure I can do this or if its even appropriate for this problem. Any assistance that anyone could provide would be greatly appreciated. Thank you.",,"['real-analysis', 'integration', 'multivariable-calculus', 'riemann-integration']"
71,Why can't the ratio test be used for geometric series?,Why can't the ratio test be used for geometric series?,,"The ratio test says that, for $a_k\neq 0$, if $$\lim_{k\to\infty}\left|\frac{a_{k+1}}{a_k}\right|=L$$ exists, then if $0\leq L <1$, then $\sum_k a_k$ converges. If $L>1$, it diverges. The notes I'm reading say that it's inadmissible to use the ratio test to test for convergence of a geometric series. I can't see why this should be the case. Say we have some geometric series $\sum_kar^k$. Then $$\lim_{k\to\infty}\left|\frac{a_{k+1}}{a_k}\right|=\lim_{k\to\infty}\frac{\left|ar^{k+1}\right|}{\left|ar^k\right|}=|r|.$$ So the ratio test tells us that the geometric series converges for $|r|<1$, and diverges for $|r|>1$, which is exactly what we get by using the formula $$\sum_{k=1}^n ar^k=a\left(\frac{1-r^{n+1}}{1-r}\right).$$ What is an example that demonstrates why the ratio test is inadmissible for a geometric series?","The ratio test says that, for $a_k\neq 0$, if $$\lim_{k\to\infty}\left|\frac{a_{k+1}}{a_k}\right|=L$$ exists, then if $0\leq L <1$, then $\sum_k a_k$ converges. If $L>1$, it diverges. The notes I'm reading say that it's inadmissible to use the ratio test to test for convergence of a geometric series. I can't see why this should be the case. Say we have some geometric series $\sum_kar^k$. Then $$\lim_{k\to\infty}\left|\frac{a_{k+1}}{a_k}\right|=\lim_{k\to\infty}\frac{\left|ar^{k+1}\right|}{\left|ar^k\right|}=|r|.$$ So the ratio test tells us that the geometric series converges for $|r|<1$, and diverges for $|r|>1$, which is exactly what we get by using the formula $$\sum_{k=1}^n ar^k=a\left(\frac{1-r^{n+1}}{1-r}\right).$$ What is an example that demonstrates why the ratio test is inadmissible for a geometric series?",,"['real-analysis', 'sequences-and-series', 'analysis', 'power-series', 'geometric-series']"
72,"if $f:[0,1] \to \mathbb{R}$ is increasing, show that $f$ is the pointwise limit of a sequence of continuous functions over $[0,1]$ [duplicate]","if  is increasing, show that  is the pointwise limit of a sequence of continuous functions over  [duplicate]","f:[0,1] \to \mathbb{R} f [0,1]","This question already has an answer here : Increasing functions are Baire one (1 answer) Closed 6 years ago . if $f:[0,1] \to \mathbb{R}$ is increasing, show that $f$ is the pointwise limit of a sequence of continuous  functions over $[0,1]$ Intuitively this makes sense but I am having trouble with showing why there would be a sequence of continuous functions converging pointwise to $f$. Clearly there is a sequence converging pointwise to $f$, I can set: $\forall n \in \mathbb{N}, f_n = f$. How to prove there is at least one which is made up of continuous functions $f_n, \forall n \in \mathbb{N}$ over $[0,1]$ I can't quite figure out the argument.","This question already has an answer here : Increasing functions are Baire one (1 answer) Closed 6 years ago . if $f:[0,1] \to \mathbb{R}$ is increasing, show that $f$ is the pointwise limit of a sequence of continuous  functions over $[0,1]$ Intuitively this makes sense but I am having trouble with showing why there would be a sequence of continuous functions converging pointwise to $f$. Clearly there is a sequence converging pointwise to $f$, I can set: $\forall n \in \mathbb{N}, f_n = f$. How to prove there is at least one which is made up of continuous functions $f_n, \forall n \in \mathbb{N}$ over $[0,1]$ I can't quite figure out the argument.",,['real-analysis']
73,Compact iff all continuous functions are uniformly cont.,Compact iff all continuous functions are uniformly cont.,,"I wonder if someone can help me with this problem: Let $(X,d)$ be a connected metric space such that all continuous functions $f:(X,d) \to \mathbb{R}$ are uniformly continuous. Show that $(X,d)$ is compact. A hint is to work with the counter positive and assume that $(X,d)$ is not totally bounded and to use Urysohn functions. I've been trying to tackle it from different directions and one idea I had was to pick  a sequence $\{x_n\}_{n\in \mathbb{N}}$ such that $x_n \in X \backslash \cup_{k=1}^{n-1} B(x_k,\epsilon)$ and define $f_n(x)=\frac{d(x,X \backslash \mathcal{U}_n )}{d(x,X \backslash \mathcal{U}_n )+ d(x,\overline{B(x_n,\epsilon)} }$ where $\mathcal{U}_n$ is an open set containing $\overline{B(x_n,\epsilon)}$ s.t  $d( \overline{B(x_n, \epsilon)} , X \backslash \mathcal{U}_n ) <\frac{1}{n}$ and put $f(x)=\sum_{n=1}^{\infty} f_n(x)$. Would this work, is there any better way to do it? EDIT I found the following results that I think might help solving the case: Def. A space $X$ is said to be compactly generated if it satisfies: A set $A$ is open in $X$ if $A\cap C$ is open in $C$ for each compact subspace $C$ of $X$. Lemma: A space $X$ is compactly generated if it satisfies the first axiom of countability. So in particular a metric space is compactly generated(if I'm not mistaken). Lemma: If $X$ is compactly generated, then a function $f: X \to Y$ is continuous if for each compact subspace $C$ of $X$ the restricted function $\left.f\right|_C$ is continuous. So if I get this right it boils down to showing that $f(x)= \sum_{n=1}^{\infty} \frac{d(x,X \backslash \mathcal{U}_n )}{d(x,X \backslash \mathcal{U}_n )+ d(x,\overline{B(x_n,\epsilon)} }$ is continuous for each compact subset $C$. Take an arbitrary compact set $C \subset X$. Since $C$ is compact and $\{x_n \}_{n\in \mathbb{N}}$ is a diverging sequence $C$ can contain at most finitely many elements of $\{x_n \}_{n\in \mathbb{N}}$. Then  $\left.f\right|_C(x) \sum_{i=1}^{m} \frac{d(x,X \backslash \mathcal{U}_{n_i} )}{d(x,X \backslash \mathcal{U}_{n_i} )+ d(x,\overline{B(x_{n_i},\epsilon)} }$ which is continuous since it's a finite combination of continuous functions. This implies that $f(x)$ is continuous. However for any $\delta >0$ one can find an $n \geq 1 : \frac{1}{n} < \delta$. Then since $d(X \backslash \mathcal{U}_{n},\overline{B(x_{n},\epsilon)})< \frac{1}{n}$ one can pick $x_0 \in X \backslash \mathcal{U}_{n}$ and $x_1 \in  \overline{B(x_{n},\epsilon)}$ such that $d(x_0,x_1)<\frac{1}{n} < \delta$. But since $x_0\in X \backslash \mathcal{U}_{n}, f(x_0)=0$ while $x_1 \in \overline{B(x_{n},\epsilon)}$ so $f(x_1)=1$. I haven't used the fact that $X$ is connected, and that makes me a little uneasy. If someone could check if this looks correct I would be very grateful!","I wonder if someone can help me with this problem: Let $(X,d)$ be a connected metric space such that all continuous functions $f:(X,d) \to \mathbb{R}$ are uniformly continuous. Show that $(X,d)$ is compact. A hint is to work with the counter positive and assume that $(X,d)$ is not totally bounded and to use Urysohn functions. I've been trying to tackle it from different directions and one idea I had was to pick  a sequence $\{x_n\}_{n\in \mathbb{N}}$ such that $x_n \in X \backslash \cup_{k=1}^{n-1} B(x_k,\epsilon)$ and define $f_n(x)=\frac{d(x,X \backslash \mathcal{U}_n )}{d(x,X \backslash \mathcal{U}_n )+ d(x,\overline{B(x_n,\epsilon)} }$ where $\mathcal{U}_n$ is an open set containing $\overline{B(x_n,\epsilon)}$ s.t  $d( \overline{B(x_n, \epsilon)} , X \backslash \mathcal{U}_n ) <\frac{1}{n}$ and put $f(x)=\sum_{n=1}^{\infty} f_n(x)$. Would this work, is there any better way to do it? EDIT I found the following results that I think might help solving the case: Def. A space $X$ is said to be compactly generated if it satisfies: A set $A$ is open in $X$ if $A\cap C$ is open in $C$ for each compact subspace $C$ of $X$. Lemma: A space $X$ is compactly generated if it satisfies the first axiom of countability. So in particular a metric space is compactly generated(if I'm not mistaken). Lemma: If $X$ is compactly generated, then a function $f: X \to Y$ is continuous if for each compact subspace $C$ of $X$ the restricted function $\left.f\right|_C$ is continuous. So if I get this right it boils down to showing that $f(x)= \sum_{n=1}^{\infty} \frac{d(x,X \backslash \mathcal{U}_n )}{d(x,X \backslash \mathcal{U}_n )+ d(x,\overline{B(x_n,\epsilon)} }$ is continuous for each compact subset $C$. Take an arbitrary compact set $C \subset X$. Since $C$ is compact and $\{x_n \}_{n\in \mathbb{N}}$ is a diverging sequence $C$ can contain at most finitely many elements of $\{x_n \}_{n\in \mathbb{N}}$. Then  $\left.f\right|_C(x) \sum_{i=1}^{m} \frac{d(x,X \backslash \mathcal{U}_{n_i} )}{d(x,X \backslash \mathcal{U}_{n_i} )+ d(x,\overline{B(x_{n_i},\epsilon)} }$ which is continuous since it's a finite combination of continuous functions. This implies that $f(x)$ is continuous. However for any $\delta >0$ one can find an $n \geq 1 : \frac{1}{n} < \delta$. Then since $d(X \backslash \mathcal{U}_{n},\overline{B(x_{n},\epsilon)})< \frac{1}{n}$ one can pick $x_0 \in X \backslash \mathcal{U}_{n}$ and $x_1 \in  \overline{B(x_{n},\epsilon)}$ such that $d(x_0,x_1)<\frac{1}{n} < \delta$. But since $x_0\in X \backslash \mathcal{U}_{n}, f(x_0)=0$ while $x_1 \in \overline{B(x_{n},\epsilon)}$ so $f(x_1)=1$. I haven't used the fact that $X$ is connected, and that makes me a little uneasy. If someone could check if this looks correct I would be very grateful!",,"['real-analysis', 'general-topology', 'metric-spaces', 'uniform-continuity']"
74,Uniform convergence and lengths,Uniform convergence and lengths,,"Consider the following sequence of piece-wise smooth functions $f_n:[0,2]\rightarrow \mathbb{R}$ (below drawn first two functions by their graphs, and is contunued then in natural way.) The function $f:[0,2]\rightarrow \mathbb{R}$, $f(x)=0$ for all $x$ is the limit of this sequence. Q. 1 Note that the lengths of graph of each $f_n$ is $2\sqrt{2}$, but the length of limiting function is not $2\sqrt{2}$; why this happens? (Such problem appears as a puzzle shown below picture in some book, but I was looking it through a sequence of functions, it convergence etc.) Here sequence $f_n$ converges to $f$ uniformly; but still the length of their graphs do not converges to length of graph of limit of $f_n$. This raises following question: Q If $f_n:[a,b]\rightarrow \mathbb{R}$ is a sequence of piecewise smmoth functions, and converging uniformly to $f:[a,b]\rightarrow \mathbb{R}$, then under what more conditions on $f_n$, we can guarantee the convergence of lengths of $f_n$ to length of $f$?","Consider the following sequence of piece-wise smooth functions $f_n:[0,2]\rightarrow \mathbb{R}$ (below drawn first two functions by their graphs, and is contunued then in natural way.) The function $f:[0,2]\rightarrow \mathbb{R}$, $f(x)=0$ for all $x$ is the limit of this sequence. Q. 1 Note that the lengths of graph of each $f_n$ is $2\sqrt{2}$, but the length of limiting function is not $2\sqrt{2}$; why this happens? (Such problem appears as a puzzle shown below picture in some book, but I was looking it through a sequence of functions, it convergence etc.) Here sequence $f_n$ converges to $f$ uniformly; but still the length of their graphs do not converges to length of graph of limit of $f_n$. This raises following question: Q If $f_n:[a,b]\rightarrow \mathbb{R}$ is a sequence of piecewise smmoth functions, and converging uniformly to $f:[a,b]\rightarrow \mathbb{R}$, then under what more conditions on $f_n$, we can guarantee the convergence of lengths of $f_n$ to length of $f$?",,"['calculus', 'real-analysis', 'convergence-divergence', 'uniform-convergence']"
75,"Is there a ""simple"" way of proving Stirlings formula? [duplicate]","Is there a ""simple"" way of proving Stirlings formula? [duplicate]",,"This question already has answers here : Stirling's formula: proof? (13 answers) Closed 7 years ago . Is there any way to derive Stirlings formula that only requires some undergraduate knowledge of calculus, real analysis and perhaps some identitets involving the gamma function, maybe Wallis product, and things along those lines? If not, and I know this is a rather vague question, what is the simplest but still sufficiently rigorous way of deriving it? I had a look at Stirling's formula: proof? but the comments seems quite messy. Wikipedia was not particularly helpful either since I have not learned about Laplace's method, Bernoulli numbers or the trapezoidal rule.","This question already has answers here : Stirling's formula: proof? (13 answers) Closed 7 years ago . Is there any way to derive Stirlings formula that only requires some undergraduate knowledge of calculus, real analysis and perhaps some identitets involving the gamma function, maybe Wallis product, and things along those lines? If not, and I know this is a rather vague question, what is the simplest but still sufficiently rigorous way of deriving it? I had a look at Stirling's formula: proof? but the comments seems quite messy. Wikipedia was not particularly helpful either since I have not learned about Laplace's method, Bernoulli numbers or the trapezoidal rule.",,"['calculus', 'real-analysis', 'analysis', 'approximation', 'gamma-function']"
76,"Does absolute continuity of $f$ on $[\epsilon,1]$ and continuity at $f=0$ imply absolute continuity on $[0,1]$?",Does absolute continuity of  on  and continuity at  imply absolute continuity on ?,"f [\epsilon,1] f=0 [0,1]","$\textbf{Question}$: Let $f$ be absolutely continuous on the interval $[\epsilon, 1]$ for $0<\epsilon<1$. Does the continuity of $f$ at 0 imply that $f$ is absolutely continuous on $[0,1]$? What if f is also of bounded variation on $[0,1]$? $\textbf{Attempt}$: My thoughts are that $f$ is NOT absolutely continuous on $[0,1]$. The definition from my textbook states that a function $F$ defined on $[a,b]$ is absolutely continuous if for any $\epsilon>0$ there exists $\delta >0$ so that $\sum_{k=1}^{N}|F(b_{k})-F(a_{k})|<\epsilon$ whenever $\sum_{k=1}^{N}(b_{k}-a_{k})<\delta$ and the intervals $(a_{k},b_{k})$ are disjoint. So, I think that the point $0$ is not included in this definition - $f$ is differentiable a.e. which does not necessarily include the boundary at the point 0, even if the point itself exists. Is this correct? My guess is then that the bounded variation assumption will make $F$ absolutely continuous on $[0,1]$ but I am not sure why.","$\textbf{Question}$: Let $f$ be absolutely continuous on the interval $[\epsilon, 1]$ for $0<\epsilon<1$. Does the continuity of $f$ at 0 imply that $f$ is absolutely continuous on $[0,1]$? What if f is also of bounded variation on $[0,1]$? $\textbf{Attempt}$: My thoughts are that $f$ is NOT absolutely continuous on $[0,1]$. The definition from my textbook states that a function $F$ defined on $[a,b]$ is absolutely continuous if for any $\epsilon>0$ there exists $\delta >0$ so that $\sum_{k=1}^{N}|F(b_{k})-F(a_{k})|<\epsilon$ whenever $\sum_{k=1}^{N}(b_{k}-a_{k})<\delta$ and the intervals $(a_{k},b_{k})$ are disjoint. So, I think that the point $0$ is not included in this definition - $f$ is differentiable a.e. which does not necessarily include the boundary at the point 0, even if the point itself exists. Is this correct? My guess is then that the bounded variation assumption will make $F$ absolutely continuous on $[0,1]$ but I am not sure why.",,"['real-analysis', 'measure-theory']"
77,Union of two countable sets is countable [Proof],Union of two countable sets is countable [Proof],,"Theorem : If $A$ and $B$ are both countable sets, then their union $A\cup B$ is also countable. I am trying to prove this theorem in the following manner: Since $A$ is a countable set, there exists a bijective function such that $f:\mathbb{N}\to A$. Similarly, there exists a bijective function $g:\mathbb{N}\to B$. Now define $h:\mathbb{N}\to A\cup B$ such that: $$h(n)=\begin{cases}        f(\frac{n+1}{2})&\text{, n is odd}\\       g(n/2) & \text{, n is even} \\    \end{cases}$$ So in essence, $h(1)=f(1)$, $h(2)=g(1)$, $h(3)=f(2)$ and so on. Now we have to show that h is a bijection. h(n) is one-one : Proof: If  $h(n_1)=h(n_2)$ then, if $n_1$ and $n_2$ are both either odd or even, we get $n_1=n_2$. But if, suppose $n_1$ is odd and $n_2$ is even, this implies that: $$f\left(\frac{n_1+1}{2}\right)=g\left(\frac{n_2}{2}\right)$$ How can one deduce from this equality that $n_1=n_2$? I tried to think about this and realized that if $A\cap B=\phi$ then this case is impossible as it would imply that there is a common element in both sets. On the other hand, if we assume that $A\cap B\neq \phi$, then either $f\left(\frac{n_1+1}{2}\right)\in A\cup B$ or $g\left(\frac{n_2}{2}\right)\in A\cup B$....Beyond this I'm clueless. Edit : Solution by the author-","Theorem : If $A$ and $B$ are both countable sets, then their union $A\cup B$ is also countable. I am trying to prove this theorem in the following manner: Since $A$ is a countable set, there exists a bijective function such that $f:\mathbb{N}\to A$. Similarly, there exists a bijective function $g:\mathbb{N}\to B$. Now define $h:\mathbb{N}\to A\cup B$ such that: $$h(n)=\begin{cases}        f(\frac{n+1}{2})&\text{, n is odd}\\       g(n/2) & \text{, n is even} \\    \end{cases}$$ So in essence, $h(1)=f(1)$, $h(2)=g(1)$, $h(3)=f(2)$ and so on. Now we have to show that h is a bijection. h(n) is one-one : Proof: If  $h(n_1)=h(n_2)$ then, if $n_1$ and $n_2$ are both either odd or even, we get $n_1=n_2$. But if, suppose $n_1$ is odd and $n_2$ is even, this implies that: $$f\left(\frac{n_1+1}{2}\right)=g\left(\frac{n_2}{2}\right)$$ How can one deduce from this equality that $n_1=n_2$? I tried to think about this and realized that if $A\cap B=\phi$ then this case is impossible as it would imply that there is a common element in both sets. On the other hand, if we assume that $A\cap B\neq \phi$, then either $f\left(\frac{n_1+1}{2}\right)\in A\cup B$ or $g\left(\frac{n_2}{2}\right)\in A\cup B$....Beyond this I'm clueless. Edit : Solution by the author-",,"['real-analysis', 'proof-verification']"
78,Limit of $x_n^3/n^2$ when $x_{n+1}=x_n+ 1/\sqrt {x_n}$ with $x_0 \gt 0$,Limit of  when  with,x_n^3/n^2 x_{n+1}=x_n+ 1/\sqrt {x_n} x_0 \gt 0,"Let $(x_n)_{n \ge 0}$  a sequence of real numbers with $x_0 \gt 0$ and   $x_{n+1}=x_n+ \frac {1}{\sqrt {x_n}}$. Check the existence and find $$L=\lim_{n \rightarrow \infty} \frac {x_n^3} {n^2}$$ Because $x_n$ is increasing, there is $\lim_{n \rightarrow \infty} {x_n}=l$ and cannot be finite (otherwise $l=l + \frac 1 {\sqrt {l}}$ impossible) therefore $L$ is indeterminate of $\frac {\infty}{\infty}$ form.","Let $(x_n)_{n \ge 0}$  a sequence of real numbers with $x_0 \gt 0$ and   $x_{n+1}=x_n+ \frac {1}{\sqrt {x_n}}$. Check the existence and find $$L=\lim_{n \rightarrow \infty} \frac {x_n^3} {n^2}$$ Because $x_n$ is increasing, there is $\lim_{n \rightarrow \infty} {x_n}=l$ and cannot be finite (otherwise $l=l + \frac 1 {\sqrt {l}}$ impossible) therefore $L$ is indeterminate of $\frac {\infty}{\infty}$ form.",,"['real-analysis', 'sequences-and-series']"
79,Why $\sin(x)+\sin(\pi x)$ is not periodic?,Why  is not periodic?,\sin(x)+\sin(\pi x),"Why $\sin(x)+\sin(\pi x)$ is not periodic? There is an answer here which tries to explain it, but I somehow do not get it. If we assume that $T>0$ is a period of $\sin(x)+\sin(\pi x)$, then $$\sin(x)+\sin(\pi x)=\sin(x+T)+\sin(\pi (x +T))$$ Apparently one needs to differentiate the equation above two times to get: $$\sin(x)+\pi^2 \sin(\pi x)=\sin(x+T)+ \pi^2 \sin(\pi (x +T))$$ and then what?","Why $\sin(x)+\sin(\pi x)$ is not periodic? There is an answer here which tries to explain it, but I somehow do not get it. If we assume that $T>0$ is a period of $\sin(x)+\sin(\pi x)$, then $$\sin(x)+\sin(\pi x)=\sin(x+T)+\sin(\pi (x +T))$$ Apparently one needs to differentiate the equation above two times to get: $$\sin(x)+\pi^2 \sin(\pi x)=\sin(x+T)+ \pi^2 \sin(\pi (x +T))$$ and then what?",,"['real-analysis', 'trigonometry', 'periodic-functions']"
80,Graph of a continuous function has measure zero,Graph of a continuous function has measure zero,,"I need help to solve the following problem: Let $f: \mathbb{R}^n \rightarrow \mathbb{R}$ be a continuous function. Prove that the graph $G(f)=\{(x,f(x)):x\in\mathbb{R}^n\}$ has measure zero in $\mathbb{R}^{n+1}$. I suppose that I have to use that f es uniformly continuous, but I don't know what rectangle which sum of volumes is less than $\varepsilon > 0$ should I take.","I need help to solve the following problem: Let $f: \mathbb{R}^n \rightarrow \mathbb{R}$ be a continuous function. Prove that the graph $G(f)=\{(x,f(x)):x\in\mathbb{R}^n\}$ has measure zero in $\mathbb{R}^{n+1}$. I suppose that I have to use that f es uniformly continuous, but I don't know what rectangle which sum of volumes is less than $\varepsilon > 0$ should I take.",,['real-analysis']
81,Explicit characterization of dual of $H^1$,Explicit characterization of dual of,H^1,"Let's start by some well-known facts: $H^1(\mathbb{R})$ is a Hilbert space, hence there holds the Riesz representation theorem, stating that any linear functional on it can be represented as $L = \langle \cdot , v\rangle$, where $v \in H^1(\mathbb{R})$, and $\langle \cdot, \cdot \rangle$ is the pairing in $H^1(\mathbb{R})$: $$ \langle u, v \rangle = \int_{\mathbb{R}} (u_x v_x + uv) d x. $$ Here, subscript indicates derivative. Let $w \in L^2(\mathbb{R})$. Since the $H^1$-norm is stronger than the $L^2$-norm, the functional $v \mapsto L(v) := \int_{\mathbb{R}^2} w(x) v(x) d x$ is a continuous linear functional on $H^1(\mathbb{R})$. My question is: is it possible to give the representative explicitly? In other words, find $z \in H^1(\mathbb{R})$ such that $L(v) = \langle v,z\rangle$. I tried mimicking the proof of the Riesz theorem but it did not help. I am sure there is something really easy I don't see, but I seem to have some misconception here. Any help would be appreciated.","Let's start by some well-known facts: $H^1(\mathbb{R})$ is a Hilbert space, hence there holds the Riesz representation theorem, stating that any linear functional on it can be represented as $L = \langle \cdot , v\rangle$, where $v \in H^1(\mathbb{R})$, and $\langle \cdot, \cdot \rangle$ is the pairing in $H^1(\mathbb{R})$: $$ \langle u, v \rangle = \int_{\mathbb{R}} (u_x v_x + uv) d x. $$ Here, subscript indicates derivative. Let $w \in L^2(\mathbb{R})$. Since the $H^1$-norm is stronger than the $L^2$-norm, the functional $v \mapsto L(v) := \int_{\mathbb{R}^2} w(x) v(x) d x$ is a continuous linear functional on $H^1(\mathbb{R})$. My question is: is it possible to give the representative explicitly? In other words, find $z \in H^1(\mathbb{R})$ such that $L(v) = \langle v,z\rangle$. I tried mimicking the proof of the Riesz theorem but it did not help. I am sure there is something really easy I don't see, but I seem to have some misconception here. Any help would be appreciated.",,"['real-analysis', 'functional-analysis', 'hilbert-spaces', 'sobolev-spaces']"
82,How to evaluate $\int_{0}^{\infty }\frac{x^{4}e^{-2x^{2}}}{\left ( 1+x^{2} \right )^{4}}\mathrm{d}x$ [closed],How to evaluate  [closed],\int_{0}^{\infty }\frac{x^{4}e^{-2x^{2}}}{\left ( 1+x^{2} \right )^{4}}\mathrm{d}x,"Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 8 years ago . Improve this question How to evaluate the following integral $$\int_{0}^{\infty }\frac{x^{4}e^{-2x^{2}}}{\left ( 1+x^{2} \right )^{4}}\mathrm{d}x$$ Can we solve it without using complex analysis method?","Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 8 years ago . Improve this question How to evaluate the following integral $$\int_{0}^{\infty }\frac{x^{4}e^{-2x^{2}}}{\left ( 1+x^{2} \right )^{4}}\mathrm{d}x$$ Can we solve it without using complex analysis method?",,"['calculus', 'real-analysis', 'integration', 'analysis']"
83,Easy question that I can't do: Find all continuous functions satisfying $\left\|\int_0^1 f\right\|= \int_0^1 \|f\|$,Easy question that I can't do: Find all continuous functions satisfying,\left\|\int_0^1 f\right\|= \int_0^1 \|f\|,"I am trying to find all continuous functions $f: [0,1] \to \mathbb R^n$ satisfying: $$\left\|\int_0^1 f\right\|= \int_0^1 \|f\|$$ Any hints? I have tried lots of things, none of which have worked. I have found plenty of functions that work, but have no idea how to describe the entire set. I have tried writing the integral as Riemann sums and using the analogy of the vector triangle inequality, which wasn't helpful. I have tried looking at specific norms to get some necessary conditions, but didn't get far with that either. For $\mathbb R^1$ a necessary condition would be that $f$ is always positive or always negative.","I am trying to find all continuous functions $f: [0,1] \to \mathbb R^n$ satisfying: $$\left\|\int_0^1 f\right\|= \int_0^1 \|f\|$$ Any hints? I have tried lots of things, none of which have worked. I have found plenty of functions that work, but have no idea how to describe the entire set. I have tried writing the integral as Riemann sums and using the analogy of the vector triangle inequality, which wasn't helpful. I have tried looking at specific norms to get some necessary conditions, but didn't get far with that either. For $\mathbb R^1$ a necessary condition would be that $f$ is always positive or always negative.",,"['real-analysis', 'definite-integrals']"
84,"Is there a function that's continuous and has all directional derivatives as a linear function of direction, but still fails to be differentiable?","Is there a function that's continuous and has all directional derivatives as a linear function of direction, but still fails to be differentiable?",,"There are many standard examples of functions $f:\mathbb{R}^2\to\mathbb{R}$ that possess all directional derivatives at a point and yet fail to differentiable or even continuous there. The most common counterexamples are not even continuous, e.g. $$f(x,y)=\begin{cases}\frac{x^2y}{x^2+y^4}&(x,y)\neq(0,0)\\0&(x,y)=(0,0)\end{cases}$$ This example illustrates the easiest type of nondifferentiability given the existence of all directional derivatives, but I'm interested in more sophisticated pathologies -- how far we can go in improving the situation before we finally hit differentiability. The natural next step would be to impose continuity and yet still fail to achieve differentiability. A good example here is $$f(x,y)=\sqrt[3]{x^2y}$$ This function is continuous at the origin, unlike the last example, and all its directional derivatives exist there, but it still fails to be differentiable. The easiest way to show this is to note that the partials are zero, but there are directional derivatives $D_u$ that are nonzero, so the directional derivative $D_u$ cannot be given by a linear function of the direction $u$ for all $u$. The problem is that the directional derivative isn't a linear function of the direction. The next step would be to ask for both continuity and all-directional-derivatives-exist-as-a-linear-function-of-direction and yet still fail to achieve differentiability. The example given in this answer is not continuous, so it shows only that the second condition alone doesn't imply differentiability. What if we impose continuity? I'm almost certain there is such a counterexample, but I can't think of one, nor could I find one in a quick glance at Courant, Apostol, or Munkres. Showing nondifferentiability here would be harder, since the standard arguments, discontinuity and the failure of the directional derivative operator to be linear, aren't available. Presumably the trick will be to have something go wrong along a nonlinear path even though everything goes right linearly.","There are many standard examples of functions $f:\mathbb{R}^2\to\mathbb{R}$ that possess all directional derivatives at a point and yet fail to differentiable or even continuous there. The most common counterexamples are not even continuous, e.g. $$f(x,y)=\begin{cases}\frac{x^2y}{x^2+y^4}&(x,y)\neq(0,0)\\0&(x,y)=(0,0)\end{cases}$$ This example illustrates the easiest type of nondifferentiability given the existence of all directional derivatives, but I'm interested in more sophisticated pathologies -- how far we can go in improving the situation before we finally hit differentiability. The natural next step would be to impose continuity and yet still fail to achieve differentiability. A good example here is $$f(x,y)=\sqrt[3]{x^2y}$$ This function is continuous at the origin, unlike the last example, and all its directional derivatives exist there, but it still fails to be differentiable. The easiest way to show this is to note that the partials are zero, but there are directional derivatives $D_u$ that are nonzero, so the directional derivative $D_u$ cannot be given by a linear function of the direction $u$ for all $u$. The problem is that the directional derivative isn't a linear function of the direction. The next step would be to ask for both continuity and all-directional-derivatives-exist-as-a-linear-function-of-direction and yet still fail to achieve differentiability. The example given in this answer is not continuous, so it shows only that the second condition alone doesn't imply differentiability. What if we impose continuity? I'm almost certain there is such a counterexample, but I can't think of one, nor could I find one in a quick glance at Courant, Apostol, or Munkres. Showing nondifferentiability here would be harder, since the standard arguments, discontinuity and the failure of the directional derivative operator to be linear, aren't available. Presumably the trick will be to have something go wrong along a nonlinear path even though everything goes right linearly.",,"['real-analysis', 'multivariable-calculus', 'examples-counterexamples']"
85,A question about existence of derivative of function at Zero,A question about existence of derivative of function at Zero,,"Assume that $f:\mathbb{R}\to\mathbb{R}$ is continuous and differentiable everywhere but at $0$. If $\displaystyle\lim_{x\to0} f'(x) = L$ exists, then does it follow that $f'(0)$ exists? Prove or disprove. I think it has to be true. I know that by definition $\displaystyle f'(0)=\lim_{h\to0}\frac{f(h)-f(0)}{h}$, but I could not able to further steps from here.  could you please help me out.","Assume that $f:\mathbb{R}\to\mathbb{R}$ is continuous and differentiable everywhere but at $0$. If $\displaystyle\lim_{x\to0} f'(x) = L$ exists, then does it follow that $f'(0)$ exists? Prove or disprove. I think it has to be true. I know that by definition $\displaystyle f'(0)=\lim_{h\to0}\frac{f(h)-f(0)}{h}$, but I could not able to further steps from here.  could you please help me out.",,"['calculus', 'real-analysis', 'limits', 'derivatives']"
86,Transforming a set of equations from one set of variables to another?,Transforming a set of equations from one set of variables to another?,,"Assume $\theta_1,\dots,\theta_n$ are $n$ positive numbers such that $$\theta_1+\dots+\theta_n=1$$ Define $y_{ij}=\frac{\theta_i}{\theta_j}$ for all $i,j \in \{1,\dots,n\}$. Is there a way to transform the above equation in terms of $y_{ij}$.","Assume $\theta_1,\dots,\theta_n$ are $n$ positive numbers such that $$\theta_1+\dots+\theta_n=1$$ Define $y_{ij}=\frac{\theta_i}{\theta_j}$ for all $i,j \in \{1,\dots,n\}$. Is there a way to transform the above equation in terms of $y_{ij}$.",,"['calculus', 'real-analysis']"
87,Rudin's PMA: Exercise 2.24,Rudin's PMA: Exercise 2.24,,"I have some difficulties solving the following exercise (Rudin's Principles of Mathematical Analysis (PMA), 2.24) Let $ X $ be a metric space in which every infinite subset has a limit point. Prove that $ X $ is separable. In order to solve this I try to find a connected set which contains an infinite subset that has no limit point. So, the interval $ [0 \dots 1] $ is a connected set, but it is a compact one also. According to PMA's theorem 2.37 any infinte subset $E$ of a compact set $K$ has a limit point in $K$ . It appears that I can't find a subset to contradict exercise 2.24. I know that there is a hint in the book, I try not to read it. Thnks","I have some difficulties solving the following exercise (Rudin's Principles of Mathematical Analysis (PMA), 2.24) Let be a metric space in which every infinite subset has a limit point. Prove that is separable. In order to solve this I try to find a connected set which contains an infinite subset that has no limit point. So, the interval is a connected set, but it is a compact one also. According to PMA's theorem 2.37 any infinte subset of a compact set has a limit point in . It appears that I can't find a subset to contradict exercise 2.24. I know that there is a hint in the book, I try not to read it. Thnks", X   X   [0 \dots 1]  E K K,['real-analysis']
88,Asymptotic of Inverse Function,Asymptotic of Inverse Function,,"Suppose we choose a positive constant $c$ and let $f_c(x)=\frac12x^2+cx^{3/2}$. I would like to get an asymptotic estimate for the function $f_c^{-1}(x)$ as $x\rightarrow\infty$. I assume it will be something of the form $f_c^{-1}(x)=\sqrt{2x}+O(g_c(x))$ for some function $g_c(x)$ of order less than $\sqrt x$, but I'm not sure how to get a nice estimate for $g_c(x)$.","Suppose we choose a positive constant $c$ and let $f_c(x)=\frac12x^2+cx^{3/2}$. I would like to get an asymptotic estimate for the function $f_c^{-1}(x)$ as $x\rightarrow\infty$. I assume it will be something of the form $f_c^{-1}(x)=\sqrt{2x}+O(g_c(x))$ for some function $g_c(x)$ of order less than $\sqrt x$, but I'm not sure how to get a nice estimate for $g_c(x)$.",,"['real-analysis', 'functions', 'asymptotics']"
89,Composition of almost everywhere continuous functions $\mathbb{R}\rightarrow\mathbb{R}$,Composition of almost everywhere continuous functions,\mathbb{R}\rightarrow\mathbb{R},"If $f,g:\mathbb{R}\rightarrow\mathbb{R}$ are continuous it is well known that the composition $f\circ g$ is also continuous. But what happens if we assume that $f,g$ are only almost everywhere continuous? Is the composition also almost everywhere continuous? The question Composition of almost everywhere differentiable functions makes me believe the answer is negative, but I am unable to provide a counterexample for the case $f,g:\mathbb{R}\rightarrow \mathbb{R}$.","If $f,g:\mathbb{R}\rightarrow\mathbb{R}$ are continuous it is well known that the composition $f\circ g$ is also continuous. But what happens if we assume that $f,g$ are only almost everywhere continuous? Is the composition also almost everywhere continuous? The question Composition of almost everywhere differentiable functions makes me believe the answer is negative, but I am unable to provide a counterexample for the case $f,g:\mathbb{R}\rightarrow \mathbb{R}$.",,"['real-analysis', 'measure-theory']"
90,In what sense does $\sum_{k=0}^{\infty} 2^{2k} = - {1 \over 3}$?,In what sense does ?,\sum_{k=0}^{\infty} 2^{2k} = - {1 \over 3},"In The Road to Reality Penrose remarks on an identity written down by Euler which is ""obviously wrong"" and yet correct ""on some deeper level"". He makes reference to the series again when discussing renormalisation. The partial series don't look much better at $\sqrt{-4}$ than they do at 2. So the answer to making use of it can't be simply to look at $\Bbb C$ . In what sense is this divergent series good? Edit: Sorry, I mean in what sense other than as a formal power series. Edit 2: Penrose remarks that the lower = left & right parts of the plot are ""inaccessible"" to the partial series. So maybe a way to rephrase the question is, how can one access them?","In The Road to Reality Penrose remarks on an identity written down by Euler which is ""obviously wrong"" and yet correct ""on some deeper level"". He makes reference to the series again when discussing renormalisation. The partial series don't look much better at than they do at 2. So the answer to making use of it can't be simply to look at . In what sense is this divergent series good? Edit: Sorry, I mean in what sense other than as a formal power series. Edit 2: Penrose remarks that the lower = left & right parts of the plot are ""inaccessible"" to the partial series. So maybe a way to rephrase the question is, how can one access them?",\sqrt{-4} \Bbb C,"['real-analysis', 'summation', 'divergent-series']"
91,Eigenvalues of Fourier Transform on Schwartz Functions,Eigenvalues of Fourier Transform on Schwartz Functions,,"Find all the eigenvalues of the Fourier transform $\hat{f}$(viewed as an operator acting on the class of Schwartz functions $S(R)$), i.e. all values $\lambda \in \mathbb{C}$ such that there exists a non-zero function $f \in S(R)$ with $\hat{f}= \lambda f$. Hint: the corresponding eigenfunctions may be found in the form $P(x)e^{−πx^2}$ , where $P(x)$ is a suitable polynomial of degree at most three. It could be verified that $e^{−πx^2}$ is an eigenfunction with eigenvalue $\lambda = 1$. How do I find the remaining? Why should $P(x)$ be atmost degree 3.","Find all the eigenvalues of the Fourier transform $\hat{f}$(viewed as an operator acting on the class of Schwartz functions $S(R)$), i.e. all values $\lambda \in \mathbb{C}$ such that there exists a non-zero function $f \in S(R)$ with $\hat{f}= \lambda f$. Hint: the corresponding eigenfunctions may be found in the form $P(x)e^{−πx^2}$ , where $P(x)$ is a suitable polynomial of degree at most three. It could be verified that $e^{−πx^2}$ is an eigenfunction with eigenvalue $\lambda = 1$. How do I find the remaining? Why should $P(x)$ be atmost degree 3.",,"['real-analysis', 'fourier-analysis']"
92,"If a continuous function is nonzero at a point $a$, there is a ball around $a$ in which it has the same sign as $f(a)$","If a continuous function is nonzero at a point , there is a ball around  in which it has the same sign as",a a f(a),"Let $f$ be a scalar field continuous at an interior point a of a set $S\in \mathbb{R}$.   If $f(a)\ne 0$, prove that there is an $n$-ball $B(a)$ in which $f$ has the same sign as $f(a)$. The above statement is what I want to prove. I am not sure whether I have to use epsilon/delta to prove it. Please help me.","Let $f$ be a scalar field continuous at an interior point a of a set $S\in \mathbb{R}$.   If $f(a)\ne 0$, prove that there is an $n$-ball $B(a)$ in which $f$ has the same sign as $f(a)$. The above statement is what I want to prove. I am not sure whether I have to use epsilon/delta to prove it. Please help me.",,"['calculus', 'real-analysis', 'general-topology', 'multivariable-calculus', 'epsilon-delta']"
93,"Show that there is no non-zero polynomial $P(u,v)$ in two variables with real coefficients such that $P(x, \cos x) = 0$ holds for all real $x$",Show that there is no non-zero polynomial  in two variables with real coefficients such that  holds for all real,"P(u,v) P(x, \cos x) = 0 x","I came across the following real analysis problem while reviewing, and I am genuinely stuck on this one: Show that there is no non-zero polynomial $P(u,v)$ in two variables with real coefficients such that $P(x, \cos x) = 0 $ holds for all real $x$. I just don't have any intuition about the non-existence of such a $P$. Am I missing something obvious? Starting out on the problem, why would you expect the above statement to be true?","I came across the following real analysis problem while reviewing, and I am genuinely stuck on this one: Show that there is no non-zero polynomial $P(u,v)$ in two variables with real coefficients such that $P(x, \cos x) = 0 $ holds for all real $x$. I just don't have any intuition about the non-existence of such a $P$. Am I missing something obvious? Starting out on the problem, why would you expect the above statement to be true?",,['real-analysis']
94,Find the closed form of the gamma function related series,Find the closed form of the gamma function related series,,"I see a possible way of computing  the series by using integrals, but I wonder if it possible to avoid the use of them, to get a neat evaluation by only using series. Compute $$\sum_{n=1}^{\infty} (-1)^{n+1}\log\left(\frac{\displaystyle \Gamma\left(\frac{n+2}{2}\right)\displaystyle\Gamma\left(\frac{n}{2}\right)}{\left(\displaystyle\Gamma\left(\frac{n+1}{2}\right)\right)^2}\right)$$ And here is a supplementary question $$\sum_{n=1}^{\infty} (-1)^{n+1}\psi^{(0)}(\alpha n)\log\left(\frac{\displaystyle \Gamma\left(\frac{n+2}{2}\right)\displaystyle\Gamma\left(\frac{n}{2}\right)}{\left(\displaystyle\Gamma\left(\frac{n+1}{2}\right)\right)^2}\right),\space \alpha >0$$","I see a possible way of computing  the series by using integrals, but I wonder if it possible to avoid the use of them, to get a neat evaluation by only using series. Compute $$\sum_{n=1}^{\infty} (-1)^{n+1}\log\left(\frac{\displaystyle \Gamma\left(\frac{n+2}{2}\right)\displaystyle\Gamma\left(\frac{n}{2}\right)}{\left(\displaystyle\Gamma\left(\frac{n+1}{2}\right)\right)^2}\right)$$ And here is a supplementary question $$\sum_{n=1}^{\infty} (-1)^{n+1}\psi^{(0)}(\alpha n)\log\left(\frac{\displaystyle \Gamma\left(\frac{n+2}{2}\right)\displaystyle\Gamma\left(\frac{n}{2}\right)}{\left(\displaystyle\Gamma\left(\frac{n+1}{2}\right)\right)^2}\right),\space \alpha >0$$",,"['calculus', 'real-analysis', 'integration', 'sequences-and-series']"
95,How to construct a smooth curve whose range is dense in $\mathbb R^2$?,How to construct a smooth curve whose range is dense in ?,\mathbb R^2,"How to construct a smooth curve $f: \mathbb R \to \mathbb R^2$   whose range is dense in $\mathbb R^2$? Space-filling curves are well-known, but they cannot be smooth.   The image of a smooth function  $f: \mathbb R \to \mathbb R^2$ is a countable union of curves of finite length, so it has zero area. Here, a smooth function is a function which has derivatives of all orders. Roughly, this curve has no sharp corners.","How to construct a smooth curve $f: \mathbb R \to \mathbb R^2$   whose range is dense in $\mathbb R^2$? Space-filling curves are well-known, but they cannot be smooth.   The image of a smooth function  $f: \mathbb R \to \mathbb R^2$ is a countable union of curves of finite length, so it has zero area. Here, a smooth function is a function which has derivatives of all orders. Roughly, this curve has no sharp corners.",,"['real-analysis', 'general-topology', 'examples-counterexamples']"
96,Modulus of continuity properties and uniform continuity.,Modulus of continuity properties and uniform continuity.,,"Let  $f:[a,b]\rightarrow \mathbb{R}$ bounded and $\omega(f,r)=\sup\{|f(x)-f(y)| \colon x,y \in [a,b], \ |x-y|<r\}$ (called modulus of continuity of $f$ EDIT note: the original question is in spanish, I'm not sure if the ""modulus of continuity"" is a proper translation from ""módulo de continuidad"") Show that the following properties of $\omega(f,r)$ are satisfied: if  $0<r_1 < r_2$  then $\omega(f,r_1)<\omega(f,r_2)$ $\omega(f,r_1+ r_2) \leq \omega(f,r_1) + \omega(f,r_2)$ $f$  is uniformly continuous if and only if $\lim_{r \searrow 0}\omega(f,r)=0$ If $\lambda>0$ then $\omega(f,\lambda r)< (1+\lambda)\omega(f,r)$ So far I have proved only the first one: Let $\gamma(r)=\{|f(x)-f(y)|\colon x,y \in [a,b], \ |x-y|<r\}$ Then $\gamma(r_1)\subseteq \gamma(r_2)$ Then $\sup\gamma(r_1) \leq \sup \gamma(r_2)$ (*) $\therefore \omega(f,r_1)\leq \omega(f,r_2)$ (*) Let $A,B \subseteq \mathbb{R}$ bounded above, such that $A \subseteq B$, we know that $\sup B$ is an upper bound of $A$ and by definition, $\sup A$ is the smallest upper bound of $A$ $\therefore \sup A \leq \sup B$. For the second I am trying to use a triangle inequality but I'm having a hard time incorporating it inside $\omega(f,r)$ if that makes any sense. I'm not exactly sure how I should start. For this one I tried using the definition of uniform continuity using sequences, but I'm not sure I'm writing it properly: $\forall \varepsilon >0$ $\exists \ (X_n)_{n=1}^\infty ,(Y_n)_{n=1}^\infty $ sequences in $[a,b]$, such that $|X_n-Y_n|<\frac{1}{n} \Rightarrow$  $|f(X_n)-f(Y_n)|<\varepsilon$ I don't know how to start with this one Any corrections, comments, suggestions are highly appreciated.","Let  $f:[a,b]\rightarrow \mathbb{R}$ bounded and $\omega(f,r)=\sup\{|f(x)-f(y)| \colon x,y \in [a,b], \ |x-y|<r\}$ (called modulus of continuity of $f$ EDIT note: the original question is in spanish, I'm not sure if the ""modulus of continuity"" is a proper translation from ""módulo de continuidad"") Show that the following properties of $\omega(f,r)$ are satisfied: if  $0<r_1 < r_2$  then $\omega(f,r_1)<\omega(f,r_2)$ $\omega(f,r_1+ r_2) \leq \omega(f,r_1) + \omega(f,r_2)$ $f$  is uniformly continuous if and only if $\lim_{r \searrow 0}\omega(f,r)=0$ If $\lambda>0$ then $\omega(f,\lambda r)< (1+\lambda)\omega(f,r)$ So far I have proved only the first one: Let $\gamma(r)=\{|f(x)-f(y)|\colon x,y \in [a,b], \ |x-y|<r\}$ Then $\gamma(r_1)\subseteq \gamma(r_2)$ Then $\sup\gamma(r_1) \leq \sup \gamma(r_2)$ (*) $\therefore \omega(f,r_1)\leq \omega(f,r_2)$ (*) Let $A,B \subseteq \mathbb{R}$ bounded above, such that $A \subseteq B$, we know that $\sup B$ is an upper bound of $A$ and by definition, $\sup A$ is the smallest upper bound of $A$ $\therefore \sup A \leq \sup B$. For the second I am trying to use a triangle inequality but I'm having a hard time incorporating it inside $\omega(f,r)$ if that makes any sense. I'm not exactly sure how I should start. For this one I tried using the definition of uniform continuity using sequences, but I'm not sure I'm writing it properly: $\forall \varepsilon >0$ $\exists \ (X_n)_{n=1}^\infty ,(Y_n)_{n=1}^\infty $ sequences in $[a,b]$, such that $|X_n-Y_n|<\frac{1}{n} \Rightarrow$  $|f(X_n)-f(Y_n)|<\varepsilon$ I don't know how to start with this one Any corrections, comments, suggestions are highly appreciated.",,"['real-analysis', 'functional-analysis', 'uniform-continuity']"
97,"Continuity of function where $f(x+y) = f(x)f(y) ~~\forall x, y \in \mathbb{R}$.",Continuity of function where .,"f(x+y) = f(x)f(y) ~~\forall x, y \in \mathbb{R}","Let $f:\mathbb{R}\rightarrow \mathbb{R}$ be a function which satisfies $f(x+y) = f(x)f(y)  ~~\forall x, y \in \mathbb{R}$ is continuous at $x=0$, then it is continuous at every point of $\mathbb{R}$. So we know $\forall \epsilon > 0 ~~\exists \delta > 0$ such that $|x-0|<\delta \implies |f(x)-f(0)|<\epsilon$ and we want to show that given any $\epsilon > 0 ~~\exists \delta > 0$ such that $|x-y|<\delta \implies |f(x)-f(y)|<\epsilon$. Now I see that the 'trick' that we can use is replacing $f(x)$ with $f(x)f(0)$ but I still cannot seem to finish the proof. Any advice?","Let $f:\mathbb{R}\rightarrow \mathbb{R}$ be a function which satisfies $f(x+y) = f(x)f(y)  ~~\forall x, y \in \mathbb{R}$ is continuous at $x=0$, then it is continuous at every point of $\mathbb{R}$. So we know $\forall \epsilon > 0 ~~\exists \delta > 0$ such that $|x-0|<\delta \implies |f(x)-f(0)|<\epsilon$ and we want to show that given any $\epsilon > 0 ~~\exists \delta > 0$ such that $|x-y|<\delta \implies |f(x)-f(y)|<\epsilon$. Now I see that the 'trick' that we can use is replacing $f(x)$ with $f(x)f(0)$ but I still cannot seem to finish the proof. Any advice?",,"['calculus', 'real-analysis', 'functional-equations']"
98,Uniform convergence of $xe^{-nx}$,Uniform convergence of,xe^{-nx},"Does the sequence $(f_n)$ on $[0, \infty)$ given by $ f_n(x) = > xe^{-nx} $ converge uniformly? This is from Bartle's Elements of Real Analysis . I've already proven that the sequence is convergent (not uniformly) to the zero fucntion, $f$. But I'm guessing the answer to this question is ""No"" because $x$ is present in every approximation I come up with hence the choice of $m \in \Bbb N$ such that $\forall \epsilon \gt 0(n \ge m \implies |f_n(x) - f(x)| \lt \epsilon)$ depends on $x$. But I wish to prove this by presenting a subsequence $(f_k)$ and a corresponding subsequence $(x_k) \subseteq [0, \infty)$ such that $ |f_k(x) - f(x)| $ is not less than some positive number $\epsilon_{\circ}$. Any hints would be appreciated. I am not allowed to use the series expansions of $e^x$. Just the definition $e^x = \lim \left({1 + \frac x n}\right)^n$ and maybe the logarithm-exponential relationships. PS: The next exercise on the list is on the uniform convergence of $(x^2e^{-nx})$","Does the sequence $(f_n)$ on $[0, \infty)$ given by $ f_n(x) = > xe^{-nx} $ converge uniformly? This is from Bartle's Elements of Real Analysis . I've already proven that the sequence is convergent (not uniformly) to the zero fucntion, $f$. But I'm guessing the answer to this question is ""No"" because $x$ is present in every approximation I come up with hence the choice of $m \in \Bbb N$ such that $\forall \epsilon \gt 0(n \ge m \implies |f_n(x) - f(x)| \lt \epsilon)$ depends on $x$. But I wish to prove this by presenting a subsequence $(f_k)$ and a corresponding subsequence $(x_k) \subseteq [0, \infty)$ such that $ |f_k(x) - f(x)| $ is not less than some positive number $\epsilon_{\circ}$. Any hints would be appreciated. I am not allowed to use the series expansions of $e^x$. Just the definition $e^x = \lim \left({1 + \frac x n}\right)^n$ and maybe the logarithm-exponential relationships. PS: The next exercise on the list is on the uniform convergence of $(x^2e^{-nx})$",,"['real-analysis', 'sequences-and-series', 'self-learning']"
99,Multiple differentiability from Taylor expansion,Multiple differentiability from Taylor expansion,,"Let $f\colon\mathbb{R}\to\mathbb{R}$ be a real function, and let $0\leq n\leq+\infty$ .  We make the following assumption: For every $a \in\mathbb{R}$ and for $k=n$ (resp., in the case $n=+\infty$ : for any $k\geq 0$ ), there exist real numbers $c_0(a),\ldots,c_k(a)$ such that $$f(x) = c_0(a) + c_1(a)\,(x-a) + \frac{1}{2}c_2(a)\,(x-a)^2 + \cdots + \frac{1}{k!}c_k(a)\,(x-a)^k + o((x-a)^k)$$ where, as usual, $o((x-a)^k)$ means $(x-a)^k\,\varepsilon_{a,k}(x)$ for some function $\varepsilon_{a,k}$ tending to $0$ when $x \to a$ . In other words, we assume that $f$ has a power expansion of order $k$ with $o$ error term at (every) $a$ .  Note that no assumption is made on uniformity of the $o$ error term when $a$ varies (e.g., we do not assume that $\varepsilon_{a,k}(x)$ is bounded by some function of $x-a$ ): we only assume that for every $a$ there exists an expansion of order $k$ as above, nothing more. Naturally, the $c_i(a)$ are uniquely determined, we have $c_0 = f$ (that is, $c_0(a) = f(a)$ for every $a$ ) and $f$ is continuous; and moreover, as soon as $n\geq 1$ , clearly, $f$ is differentiable with derivative $f' = c_1$ . We cannot deduce that $f$ is twice differentiable, or even $C^1$ , from the above hypothesis alone, no matter how large $n$ is.  The simple example of $f(x) = x^{n+1} \sin(x^{-n})$ provides a counterexample (it is $o(x^n)$ at $0$ and analytic everywhere else, so it has a power expansion of order $n$ everywhere, yet it is easily seen that it is not even $C^1$ at $0$ ); a slightly more complicated counterexample works for $n=\infty$ . Now here is my question.  Let us make the following additional assumption (which is not satisfied for the above counterexample): For each $0\leq k\leq n$ , the function $c_k$ (that is, $a\mapsto c_k(a)$ ) is continuous. (In particular, if $n\geq 1$ , it is now clear that $f$ is $C^1$ .) Can I conclude from both assumptions that $f$ is $C^n$ ?  (Or, if not, can I conclude something non-trivial?)","Let be a real function, and let .  We make the following assumption: For every and for (resp., in the case : for any ), there exist real numbers such that where, as usual, means for some function tending to when . In other words, we assume that has a power expansion of order with error term at (every) .  Note that no assumption is made on uniformity of the error term when varies (e.g., we do not assume that is bounded by some function of ): we only assume that for every there exists an expansion of order as above, nothing more. Naturally, the are uniquely determined, we have (that is, for every ) and is continuous; and moreover, as soon as , clearly, is differentiable with derivative . We cannot deduce that is twice differentiable, or even , from the above hypothesis alone, no matter how large is.  The simple example of provides a counterexample (it is at and analytic everywhere else, so it has a power expansion of order everywhere, yet it is easily seen that it is not even at ); a slightly more complicated counterexample works for . Now here is my question.  Let us make the following additional assumption (which is not satisfied for the above counterexample): For each , the function (that is, ) is continuous. (In particular, if , it is now clear that is .) Can I conclude from both assumptions that is ?  (Or, if not, can I conclude something non-trivial?)","f\colon\mathbb{R}\to\mathbb{R} 0\leq n\leq+\infty a \in\mathbb{R} k=n n=+\infty k\geq 0 c_0(a),\ldots,c_k(a) f(x) = c_0(a) + c_1(a)\,(x-a) + \frac{1}{2}c_2(a)\,(x-a)^2 + \cdots + \frac{1}{k!}c_k(a)\,(x-a)^k + o((x-a)^k) o((x-a)^k) (x-a)^k\,\varepsilon_{a,k}(x) \varepsilon_{a,k} 0 x \to a f k o a o a \varepsilon_{a,k}(x) x-a a k c_i(a) c_0 = f c_0(a) = f(a) a f n\geq 1 f f' = c_1 f C^1 n f(x) = x^{n+1} \sin(x^{-n}) o(x^n) 0 n C^1 0 n=\infty 0\leq k\leq n c_k a\mapsto c_k(a) n\geq 1 f C^1 f C^n","['real-analysis', 'taylor-expansion']"
