,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,"Double integral $\int_{[0,1]^2} \frac{f(x)-f(y)}{x-y} dx\,dy$ for regular function $f$",Double integral  for regular function,"\int_{[0,1]^2} \frac{f(x)-f(y)}{x-y} dx\,dy f","Let $f$ be a sufficiently regular function on the unit square. (For example, say $f\in C^1$ .) Then, is there a way to simplify the integral $$\int_{[0,1]^2} \frac{f(x)-f(y)}{x-y} dx\,dy ?$$ If $f\in C^1$ , then the integrand is bounded and therefore the integral is well-defined.","Let be a sufficiently regular function on the unit square. (For example, say .) Then, is there a way to simplify the integral If , then the integrand is bounded and therefore the integral is well-defined.","f f\in C^1 \int_{[0,1]^2} \frac{f(x)-f(y)}{x-y} dx\,dy ? f\in C^1","['integration', 'multivariable-calculus', 'multiple-integral']"
1,Converting cartesian to polar double integral,Converting cartesian to polar double integral,,"Convert the following integral to polar coordinates. You do not need to evaluate. $$\int_{-3}^3 \int_{x}^{\sqrt{9-x^2}} x^2y dy dx$$ My work : I plotted the limits and I don't understand the bounded region due to $y=x$ , but still I got like this which is wrong I know I solved the integral it should be $\frac{-81}{5}$ but the integral in the polar coordinates I obtained is incorrect $$  \int\limits_{\pi/4}^{\pi}\int\limits_{0}^{3}r^4\cos^2 \theta \sin \theta dr d\theta+ \int\limits_{\pi}^{5\pi/4}\int\limits_{-3/\cos \theta}^{-3\sqrt2}r^4\cos^2 \theta \sin \theta dr d\theta$$ Can anyone help me recorrecting it the answer below is not complete, and it is definitely not the double of the answer ???","Convert the following integral to polar coordinates. You do not need to evaluate. My work : I plotted the limits and I don't understand the bounded region due to , but still I got like this which is wrong I know I solved the integral it should be but the integral in the polar coordinates I obtained is incorrect Can anyone help me recorrecting it the answer below is not complete, and it is definitely not the double of the answer ???",\int_{-3}^3 \int_{x}^{\sqrt{9-x^2}} x^2y dy dx y=x \frac{-81}{5}   \int\limits_{\pi/4}^{\pi}\int\limits_{0}^{3}r^4\cos^2 \theta \sin \theta dr d\theta+ \int\limits_{\pi}^{5\pi/4}\int\limits_{-3/\cos \theta}^{-3\sqrt2}r^4\cos^2 \theta \sin \theta dr d\theta,"['calculus', 'integration', 'multivariable-calculus', 'area', 'polar-coordinates']"
2,Prove the following inequality $\sum_{i<j<k}\frac{a_ia_ja_k}{(n-2)(n-1)n}\le \bigg(\sum_{i<j}\frac{a_ia_j}{(n-1)n}\bigg)^2+\frac{1}{12}$,Prove the following inequality,\sum_{i<j<k}\frac{a_ia_ja_k}{(n-2)(n-1)n}\le \bigg(\sum_{i<j}\frac{a_ia_j}{(n-1)n}\bigg)^2+\frac{1}{12},"For positive integer $n \ge 3$ , prove the following inequality $$\sum_{i<j<k}\frac{a_ia_ja_k}{(n-2)(n-1)n}\le \bigg(\sum_{i<j}\frac{a_ia_j}{(n-1)n}\bigg)^2+\frac{1}{12}$$ where $a_1+a_2+\cdots +a_n=0$ I noticed that $$(n-2)(n-1)n=6{n \choose 3}$$ and $$(n-1)n=2{n \choose 2}$$ After many arithmetics and research I got: $$(n-1)\sqrt[3]{\sum_{i<j<k}\frac{a_ia_ja_k}{n \choose 3}}+\sqrt{\frac{\sum a_i^2}{n}} \le \sum a_i =0$$ Does it help? Maybe after plugging them into the initial expression, it comes down to apply some famous inequalities that I don't know. Any help is greatly appreciated.","For positive integer , prove the following inequality where I noticed that and After many arithmetics and research I got: Does it help? Maybe after plugging them into the initial expression, it comes down to apply some famous inequalities that I don't know. Any help is greatly appreciated.",n \ge 3 \sum_{i<j<k}\frac{a_ia_ja_k}{(n-2)(n-1)n}\le \bigg(\sum_{i<j}\frac{a_ia_j}{(n-1)n}\bigg)^2+\frac{1}{12} a_1+a_2+\cdots +a_n=0 (n-2)(n-1)n=6{n \choose 3} (n-1)n=2{n \choose 2} (n-1)\sqrt[3]{\sum_{i<j<k}\frac{a_ia_ja_k}{n \choose 3}}+\sqrt{\frac{\sum a_i^2}{n}} \le \sum a_i =0,"['multivariable-calculus', 'inequality', 'summation', 'symmetric-polynomials', 'uvw']"
3,Non-Strict Saddle Point vs Local Minima,Non-Strict Saddle Point vs Local Minima,,"While going through Escaping Saddle Points Efficiently , I came across the definition of Strict Saddle Point. They define a stationary point to be a strict saddle if at least one of the eigenvalue of the Hessian Matrix is negative. This implies a non-strict saddle point will have all eigenvalues of the Hessian Matrix greater than or equal to zero. But, isn't this a sufficient and necessary condition for local minima? What is the difference between a non-strict saddle point and a local-minima?","While going through Escaping Saddle Points Efficiently , I came across the definition of Strict Saddle Point. They define a stationary point to be a strict saddle if at least one of the eigenvalue of the Hessian Matrix is negative. This implies a non-strict saddle point will have all eigenvalues of the Hessian Matrix greater than or equal to zero. But, isn't this a sufficient and necessary condition for local minima? What is the difference between a non-strict saddle point and a local-minima?",,"['linear-algebra', 'multivariable-calculus', 'optimization']"
4,Find the double integral $\int_{0}^1 \int_{y}^1 e^{x^2} \ dx \ dy $ [duplicate],Find the double integral  [duplicate],\int_{0}^1 \int_{y}^1 e^{x^2} \ dx \ dy ,"This question already has an answer here : Evaluate: $ \int_0^1 \int_y^1 e^{x^2 }dxdy$ (1 answer) Closed 4 years ago . I need assistance to solve the double integral $$ \int_{0}^1 \int_{y}^1 e^{x^2} \ dx \ dy.  $$ So far I have done some poor attempts to rewrite the integral $\int e^{x^2} \ dx$ where I let $t= e^{x^2}, \ $ which lead to $\ln(t)={x^2}$ , differentiating then gives $\frac{dt}{t}\ = 2x \ dx$ . After some back and forth I end up with the not so nice expression $\int e^{x^2} \ dx=\frac{1}{2} \int\left(\frac{1}{\ln(t)}\right)^{1/2} \ dt$ . The answer to the double integral is $$ \int_{0}^1 \int_{y}^1 e^{x^2} \ dx \ dy=\frac{1}{2}(e-1),  $$ but I would like some one to give me a hint on how to reach the conclusion. Any assistance is appreciated./Pablo","This question already has an answer here : Evaluate: $ \int_0^1 \int_y^1 e^{x^2 }dxdy$ (1 answer) Closed 4 years ago . I need assistance to solve the double integral So far I have done some poor attempts to rewrite the integral where I let which lead to , differentiating then gives . After some back and forth I end up with the not so nice expression . The answer to the double integral is but I would like some one to give me a hint on how to reach the conclusion. Any assistance is appreciated./Pablo","
\int_{0}^1 \int_{y}^1 e^{x^2} \ dx \ dy. 
 \int e^{x^2} \ dx t= e^{x^2}, \  \ln(t)={x^2} \frac{dt}{t}\ = 2x \ dx \int e^{x^2} \ dx=\frac{1}{2} \int\left(\frac{1}{\ln(t)}\right)^{1/2} \ dt 
\int_{0}^1 \int_{y}^1 e^{x^2} \ dx \ dy=\frac{1}{2}(e-1), 
","['calculus', 'integration', 'multivariable-calculus', 'definite-integrals']"
5,Evaluate using Stokes' Theorem,Evaluate using Stokes' Theorem,,"To evaluate $\oint_{C} -y^3dx+x^3dy+z^3dz,$ where $C$ is the intersection of cylinder $x^2 + y^2 =1$ and plane $x+y+z=1$ . The orientation of $C$ is counter-clockwise motion in the $xy$ plane. Now I have computed $\nabla \times\mathbf{F} = \left(0,0,3\left(x^2+y^2\right)\right).$ I am having difficulty finding out the curve $C$ of intersection and also I am confused about projection on the $xy$ plane. Should I take part inside $x+y=1 $ only or part between $x+y=1$ and $x^2+y^2=1?$",To evaluate where is the intersection of cylinder and plane . The orientation of is counter-clockwise motion in the plane. Now I have computed I am having difficulty finding out the curve of intersection and also I am confused about projection on the plane. Should I take part inside only or part between and,"\oint_{C} -y^3dx+x^3dy+z^3dz, C x^2 + y^2 =1 x+y+z=1 C xy \nabla \times\mathbf{F} = \left(0,0,3\left(x^2+y^2\right)\right). C xy x+y=1  x+y=1 x^2+y^2=1?","['integration', 'multivariable-calculus', 'vector-analysis']"
6,Calculating iterated integral using Sagemath,Calculating iterated integral using Sagemath,,"I would like to calculate the integral over the following domain (with order $x,y,z$ ) using Sagemath $$0 \le z \le 3, \max\{ 0,\frac{z-1}{2}\} \le y \le 1, \max \{y,z-2y\} \le x \le 1. \tag{1} \label{1}$$ This is equivalent to finding the integral (with order $z,y,x$ ) $$\int_{0}^{1} \int_{0}^{x}\int_{0}^{x+2y}  \mathrm dz\,\mathrm dy\,\mathrm dx=2/3.$$ However, when I tried to write this onto Sagemath as follow integrate(integrate(integrate(1,x,max(z-2*y,y),1),y,max(0,(z-1)/2),1),z,0,3) I obtained a wrong answer of $3/2$ . I tried to break $\eqref{1}$ down into smaller domain (see here ), I then got the correct answer. var(""x y z"") X=integrate(integrate(integrate(1,x,z-2*y,1),y,0,z/3),z,0,1) Y=integrate(integrate(integrate(1,x,y,1),y,z/3,1),z,0,1) Z=integrate(integrate(integrate(1,x,y,1),y,z/3,1),z,1,3) W=integrate(integrate(integrate(1,x,z-2*y,1),y,(z-1)/2,z/3),z,1,3) C=X+Y+Z+W I would like to ask why is it so? How does Sagemath deal with $\max$ function in this case?","I would like to calculate the integral over the following domain (with order ) using Sagemath This is equivalent to finding the integral (with order ) However, when I tried to write this onto Sagemath as follow integrate(integrate(integrate(1,x,max(z-2*y,y),1),y,max(0,(z-1)/2),1),z,0,3) I obtained a wrong answer of . I tried to break down into smaller domain (see here ), I then got the correct answer. var(""x y z"") X=integrate(integrate(integrate(1,x,z-2*y,1),y,0,z/3),z,0,1) Y=integrate(integrate(integrate(1,x,y,1),y,z/3,1),z,0,1) Z=integrate(integrate(integrate(1,x,y,1),y,z/3,1),z,1,3) W=integrate(integrate(integrate(1,x,z-2*y,1),y,(z-1)/2,z/3),z,1,3) C=X+Y+Z+W I would like to ask why is it so? How does Sagemath deal with function in this case?","x,y,z 0 \le z \le 3, \max\{ 0,\frac{z-1}{2}\} \le y \le 1, \max \{y,z-2y\} \le x \le 1. \tag{1} \label{1} z,y,x \int_{0}^{1} \int_{0}^{x}\int_{0}^{x+2y}  \mathrm dz\,\mathrm dy\,\mathrm dx=2/3. 3/2 \eqref{1} \max","['multivariable-calculus', 'sagemath']"
7,Bounded Gradient implies Lipschitz proof with the mean value theorem,Bounded Gradient implies Lipschitz proof with the mean value theorem,,"Let $f:\mathbb{R}^n \to \mathbb{R}$ with $|| \nabla f(x)|| \leq M$ (say it is the Euclidean norm), then f is Lipschitz. I have seen proofs that do this for the case where $f:\mathbb{R} \to \mathbb{R}$ by applying the mean value theorem. I am wondering if there is a proof available that shows how the mean value theorem is applied to the problem for a function from $f:\mathbb{R}^n \to \mathbb{R}$ ?","Let with (say it is the Euclidean norm), then f is Lipschitz. I have seen proofs that do this for the case where by applying the mean value theorem. I am wondering if there is a proof available that shows how the mean value theorem is applied to the problem for a function from ?",f:\mathbb{R}^n \to \mathbb{R} || \nabla f(x)|| \leq M f:\mathbb{R} \to \mathbb{R} f:\mathbb{R}^n \to \mathbb{R},"['multivariable-calculus', 'lipschitz-functions']"
8,What's the precise definition of coordinates in Euclidean space?,What's the precise definition of coordinates in Euclidean space?,,"I have a loose understanding of what coordinates are, but not something rigorous or concrete. For example take the statement of this result below When the author says let $x = (x^1, \dots x^n)$ denote the ""standard coordinates"" on $U$ and $y = (y^1, \dots, y^m)$ those on $\widetilde{U}$ what precisely does the author mean? For example there is a very clear and rigorous definition of what a basis for a vector space is, but I can't seem to find that for coordinates. The closest thing I know to a definition for coordinates is the following: If I have a smooth manifold say $M$ of dimension $n$ and a smooth chart $(U, \phi)$ , then for any $p \in U$ we have $\phi(p) = (x^1(p), \dots, x^n(p))$ where the $x^i : U \to \mathbb{R}$ are the component functions of the homeomorphism $\phi$ and the collection of component functions $(x^1, \dots, x^n)$ are called local coordinates on $M$ . However this notion doesn't seem to apply to the corollary above, because it seems ""coordinates"" take on a different meaning above, because for example $$\frac{\partial G^i}{\partial y^k}$$ (in the statement of the result above) doesn't make sense since $y^k$ interpreted this way would actually be a function and it's meaningless to take the partial derivative of a function with respect to another function. Furthermore the only definition of the partial derivative of a function that I'm familiar with is the following Definition: Let $U \subseteq \mathbb{R}^m$ be an open set and let $f : U \to \mathbb{R}$ . The $j$ -th partial derivative of $f$ at $a$ is defined to be the directional derivative of $f$ at $a$ with respect to the basis vector $e_j = (0, \dots, 1, \dots, 0)$ (where $1$ is in the $j$ -th position) provided the derivative exists $$\frac{\partial f}{\partial x^j} = \lim_{t \to 0} \frac{f(a +te_j) - f(a)}{t}$$ and in this definition above I just think of $x^j$ as a reminder that I'm taking the directional derivative with respect to the $e_j$ basis vector on $\mathbb{R}^m$ . How do I reconcile the definition above with what the authors mean by standard coordinates? Basically the question I'm asking is, what does ""let $x = (x^1, \dots, x^n)$ denote the standard coordinates on $U$ "" mean? Or more generally if $V$ is an open subset of $\mathbb{R}^k$ what would ""let $z = (z^1, \dots, z^k)$ denote the coordinates on $V$ "" mean?","I have a loose understanding of what coordinates are, but not something rigorous or concrete. For example take the statement of this result below When the author says let denote the ""standard coordinates"" on and those on what precisely does the author mean? For example there is a very clear and rigorous definition of what a basis for a vector space is, but I can't seem to find that for coordinates. The closest thing I know to a definition for coordinates is the following: If I have a smooth manifold say of dimension and a smooth chart , then for any we have where the are the component functions of the homeomorphism and the collection of component functions are called local coordinates on . However this notion doesn't seem to apply to the corollary above, because it seems ""coordinates"" take on a different meaning above, because for example (in the statement of the result above) doesn't make sense since interpreted this way would actually be a function and it's meaningless to take the partial derivative of a function with respect to another function. Furthermore the only definition of the partial derivative of a function that I'm familiar with is the following Definition: Let be an open set and let . The -th partial derivative of at is defined to be the directional derivative of at with respect to the basis vector (where is in the -th position) provided the derivative exists and in this definition above I just think of as a reminder that I'm taking the directional derivative with respect to the basis vector on . How do I reconcile the definition above with what the authors mean by standard coordinates? Basically the question I'm asking is, what does ""let denote the standard coordinates on "" mean? Or more generally if is an open subset of what would ""let denote the coordinates on "" mean?","x = (x^1, \dots x^n) U y = (y^1, \dots, y^m) \widetilde{U} M n (U, \phi) p \in U \phi(p) = (x^1(p), \dots, x^n(p)) x^i : U \to \mathbb{R} \phi (x^1, \dots, x^n) M \frac{\partial G^i}{\partial y^k} y^k U \subseteq \mathbb{R}^m f : U \to \mathbb{R} j f a f a e_j = (0, \dots, 1, \dots, 0) 1 j \frac{\partial f}{\partial x^j} = \lim_{t \to 0} \frac{f(a +te_j) - f(a)}{t} x^j e_j \mathbb{R}^m x = (x^1, \dots, x^n) U V \mathbb{R}^k z = (z^1, \dots, z^k) V","['multivariable-calculus', 'differential-geometry', 'euclidean-geometry']"
9,Does the improper integral $\int_{\mathbb{R}^2} e^{-xy}\sin(x)$ converge?,Does the improper integral  converge?,\int_{\mathbb{R}^2} e^{-xy}\sin(x),"Does the improper integral $\large\int_{\mathbb{R}^2} e^{-xy}\sin(x)\mathrm{d}x\mathrm{d}y$ converge? I know the when the integral is only on $\mathbb{R}^2_+ \times \mathbb{R}^2_+$ then it converges to $\frac{\pi}{2}$ . However here the boundaries are different. Also it is not a positive function so I need to show it on the absolute value. It means I need to show that $$\int_{\mathbb{R}^2} e^{-xy}|\sin(x)|\mathrm{d}x\mathrm{d}y$$ converges. I'm pretty sure it doesn't converge, since I can't bound it from the right by a converge-bale function (I tried with $e^{-xy}$ , also, Wolfram Alpha said so too). However I can't think of a function that won't converge and will bound it from the left. Help would be appreciated.","Does the improper integral converge? I know the when the integral is only on then it converges to . However here the boundaries are different. Also it is not a positive function so I need to show it on the absolute value. It means I need to show that converges. I'm pretty sure it doesn't converge, since I can't bound it from the right by a converge-bale function (I tried with , also, Wolfram Alpha said so too). However I can't think of a function that won't converge and will bound it from the left. Help would be appreciated.",\large\int_{\mathbb{R}^2} e^{-xy}\sin(x)\mathrm{d}x\mathrm{d}y \mathbb{R}^2_+ \times \mathbb{R}^2_+ \frac{\pi}{2} \int_{\mathbb{R}^2} e^{-xy}|\sin(x)|\mathrm{d}x\mathrm{d}y e^{-xy},"['integration', 'multivariable-calculus', 'convergence-divergence', 'improper-integrals']"
10,"Is $ f(x,y) = \frac{|x|^{5/2} y}{x^4 + y^2} $ differentiable at $(0,0)$?",Is  differentiable at ?," f(x,y) = \frac{|x|^{5/2} y}{x^4 + y^2}  (0,0)","Let $ f: \mathbb{R}^2 \rightarrow \mathbb{R} $ be a function defined as : $$ \begin{cases}  f(x,y) = \frac{|x|^{5/2} y}{x^4 + y^2} , &  (x,y) \not= (0,0) \\  f(0,0) = 0  \end{cases}  $$ Compute $ \frac{df}{dx}(0,0) $ and $\frac{df}{dy}(0,0)$ . Is $f$ differentiable at $(0,0)$ ? I have problem with question 2. For $f$ to be differentiable, the partial derivatives must exist and be continuous. They exist because we have: $ \frac{df}{dx}(0,0) = \lim_{x \to 0} \frac{f(x,0) - f(0,0)}{x} = 0$ $ \frac{df}{dy}(0,0) = \lim_{y \to 0} \frac{f(0,y) - f(0,0)}{y} = 0$ but I cannot compute the partial derivatives because of the absolute value. How to know if the partial derivatives are continuous at $(0,0)$ ?","Let be a function defined as : Compute and . Is differentiable at ? I have problem with question 2. For to be differentiable, the partial derivatives must exist and be continuous. They exist because we have: but I cannot compute the partial derivatives because of the absolute value. How to know if the partial derivatives are continuous at ?"," f: \mathbb{R}^2 \rightarrow \mathbb{R}  
\begin{cases} 
f(x,y) = \frac{|x|^{5/2} y}{x^4 + y^2} , &  (x,y) \not= (0,0) \\ 
f(0,0) = 0 
\end{cases} 
  \frac{df}{dx}(0,0)  \frac{df}{dy}(0,0) f (0,0) f  \frac{df}{dx}(0,0) = \lim_{x \to 0} \frac{f(x,0) - f(0,0)}{x} = 0  \frac{df}{dy}(0,0) = \lim_{y \to 0} \frac{f(0,y) - f(0,0)}{y} = 0 (0,0)","['real-analysis', 'multivariable-calculus']"
11,PDE: Find all Functions $u \in C^1(\mathbb{R}^2)$ that satisfy $x_2 u_{x_1} - x_1 u_{x_2} = 0$,PDE: Find all Functions  that satisfy,u \in C^1(\mathbb{R}^2) x_2 u_{x_1} - x_1 u_{x_2} = 0,Can anyone help me with this problem? It's the start of the PDE lecture so there should be some kind of basic way of figuring out all solutions to the PDE. I tried re-writing it as $u_{x_1} = \frac{x_1}{x_2} u_{x_2}$ and tried to integrate it but I didn't really come up with anything I can work with.,Can anyone help me with this problem? It's the start of the PDE lecture so there should be some kind of basic way of figuring out all solutions to the PDE. I tried re-writing it as and tried to integrate it but I didn't really come up with anything I can work with.,u_{x_1} = \frac{x_1}{x_2} u_{x_2},"['real-analysis', 'multivariable-calculus', 'partial-differential-equations']"
12,Finding the minimum or maximum of a bivariate function when $f_{xx}\times f_{yy}-f_{xy}^2=0$.,Finding the minimum or maximum of a bivariate function when .,f_{xx}\times f_{yy}-f_{xy}^2=0,I see that when $f_{xx}\times f_{yy}-f_{xy}^2<0$ then it is a saddle point. Also when $f_{xx}\times f_{yy}-f_{xy}^2>0$ then it is a minima or maxima. What is exactly happening when $f_{xx}\times f_{yy}-f_{xy}^2=0$ ?,I see that when then it is a saddle point. Also when then it is a minima or maxima. What is exactly happening when ?,f_{xx}\times f_{yy}-f_{xy}^2<0 f_{xx}\times f_{yy}-f_{xy}^2>0 f_{xx}\times f_{yy}-f_{xy}^2=0,"['multivariable-calculus', 'optimization', 'maxima-minima']"
13,Is there an easy way to compute the jacobian of a normalized vector?,Is there an easy way to compute the jacobian of a normalized vector?,,If $x \in \mathbb{R}^3$ I want to compute the jacobian of the following function $$ f(x) = \frac{x}{\lVert x \rVert } $$ If I proceed I get a matrix whose elements are $$ a_{ij} = \begin{cases} \frac{1}{\lVert x \rVert} - \frac{x_i^2}{\lVert x \rVert^3} & i = j \\ -\frac{x_i x_j}{\lVert x \rVert^3} &i \neq j \end{cases} $$ Is this the most compact form? The derivation is based on the product rule componentwise.,If $x \in \mathbb{R}^3$ I want to compute the jacobian of the following function $$ f(x) = \frac{x}{\lVert x \rVert } $$ If I proceed I get a matrix whose elements are $$ a_{ij} = \begin{cases} \frac{1}{\lVert x \rVert} - \frac{x_i^2}{\lVert x \rVert^3} & i = j \\ -\frac{x_i x_j}{\lVert x \rVert^3} &i \neq j \end{cases} $$ Is this the most compact form? The derivation is based on the product rule componentwise.,,"['multivariable-calculus', 'jacobian']"
14,How to generalize the concept of differentiation to higher dimensions?,How to generalize the concept of differentiation to higher dimensions?,,"I am studying the chapter 'Several Variables' from Rudin's ""Principles of Mathematical Analysis"". There I found a new conception of derivative which generalizes the notion of derivatives to higher dimensions. He explained it as follows which I write here in my own languange of understanding $:$ Suppose $f:(a,b) \longrightarrow \mathbb R$ be differentiable function. Then for each $x \in (a,b)$, $f'(x)$ exists. Instead of viewing $f'(x)$ for some $x \in (a,b)$ as a real number we may treat it as a linear transformation from $\mathbb R$ into $\mathbb R$. Because for every real number $\alpha$ we can associate a linear operator $T_{\alpha}$ on $\mathbb R$ defined by $T_{\alpha} (x) = \alpha x$, $x \in \mathbb R$. Conversely, any linear operator $T$ on $\mathbb R$ can be defined as $T(x)= \alpha x$ for some $\alpha \in \mathbb R$ (we may take $\alpha=T(1)$). Hence there exist a $1-1$ correspondence from $\mathbb R$ onto $L(\mathbb R)$. Also it can be easily checked that this kind of correspondence is linear. So it's an isomorphism. Therefore we may identify $f'(x)$ by the linear operator $T_{f'(x)}$ on $\mathbb R$ defined by $T_{f'(x)} (h) = f'(x) h$, $h \in \mathbb R$. Similarly if $f : (a,b) \longrightarrow \mathbb R^m$ is a differentiable function we then find an isomorphism from $\mathbb R^m$ onto $L(\mathbb R, \mathbb R^m)$ and hence instead of identifying $f'(x)$ for some $x \in \mathbb R$ as a vector in $\mathbb R^m$ we may simply identify it as a linear transformation $T_{f'(x)}$ from $\mathbb R$ to $\mathbb R^m$ defined by $T_{f'(x)} (h) = f'(x) h$, $h \in \mathbb R$. But for higher dimensions I fail to relate this concept. Suppose $f : E \subset_{open} \mathbb R^n \longrightarrow \mathbb R^m$ is differentiable on $E$ then how can I find the $1-1$ correspondence which is linear? I have thought about it for at least half an hour but couldn't find any satisfactory answer. Please help me at this point. Thank you in advance.","I am studying the chapter 'Several Variables' from Rudin's ""Principles of Mathematical Analysis"". There I found a new conception of derivative which generalizes the notion of derivatives to higher dimensions. He explained it as follows which I write here in my own languange of understanding $:$ Suppose $f:(a,b) \longrightarrow \mathbb R$ be differentiable function. Then for each $x \in (a,b)$, $f'(x)$ exists. Instead of viewing $f'(x)$ for some $x \in (a,b)$ as a real number we may treat it as a linear transformation from $\mathbb R$ into $\mathbb R$. Because for every real number $\alpha$ we can associate a linear operator $T_{\alpha}$ on $\mathbb R$ defined by $T_{\alpha} (x) = \alpha x$, $x \in \mathbb R$. Conversely, any linear operator $T$ on $\mathbb R$ can be defined as $T(x)= \alpha x$ for some $\alpha \in \mathbb R$ (we may take $\alpha=T(1)$). Hence there exist a $1-1$ correspondence from $\mathbb R$ onto $L(\mathbb R)$. Also it can be easily checked that this kind of correspondence is linear. So it's an isomorphism. Therefore we may identify $f'(x)$ by the linear operator $T_{f'(x)}$ on $\mathbb R$ defined by $T_{f'(x)} (h) = f'(x) h$, $h \in \mathbb R$. Similarly if $f : (a,b) \longrightarrow \mathbb R^m$ is a differentiable function we then find an isomorphism from $\mathbb R^m$ onto $L(\mathbb R, \mathbb R^m)$ and hence instead of identifying $f'(x)$ for some $x \in \mathbb R$ as a vector in $\mathbb R^m$ we may simply identify it as a linear transformation $T_{f'(x)}$ from $\mathbb R$ to $\mathbb R^m$ defined by $T_{f'(x)} (h) = f'(x) h$, $h \in \mathbb R$. But for higher dimensions I fail to relate this concept. Suppose $f : E \subset_{open} \mathbb R^n \longrightarrow \mathbb R^m$ is differentiable on $E$ then how can I find the $1-1$ correspondence which is linear? I have thought about it for at least half an hour but couldn't find any satisfactory answer. Please help me at this point. Thank you in advance.",,"['multivariable-calculus', 'derivatives']"
15,Deriving the weak form for linear elasticity equation,Deriving the weak form for linear elasticity equation,,"Consider the BVP from linear elasticity: \begin{align*} -\mu \Delta\textbf{u} - (\lambda + \mu) \nabla(\nabla \cdot \textbf{u})) &= \textbf{f}, \text{ in } \Omega \subset\mathbb{R}^2, \\ \textbf{u} &= \textbf{g}_D, \text{ on } \partial \Omega. \end{align*} First, we multiply by a test function and integrate both sides, \begin{align*} \int_{\Omega}( -\mu \textbf{v} \Delta \textbf{u} - (\lambda + \mu) \textbf{v} \nabla (\nabla\cdot \textbf{u})))dX = \int_{\Omega} \textbf{v}\textbf{f} dX. \end{align*} Now I am not sure what the best way to proceed is. I think I should break everything up into components, but I am not confident. I think I will end up using the divergence theorem (twice?). Would anyone be able to help me work out the details? I think the $\Delta \textbf{u}$ term should be simple, but I have no clue what identities to use for $\nabla (\nabla \cdot \textbf{u})$. I appreciate any help.","Consider the BVP from linear elasticity: \begin{align*} -\mu \Delta\textbf{u} - (\lambda + \mu) \nabla(\nabla \cdot \textbf{u})) &= \textbf{f}, \text{ in } \Omega \subset\mathbb{R}^2, \\ \textbf{u} &= \textbf{g}_D, \text{ on } \partial \Omega. \end{align*} First, we multiply by a test function and integrate both sides, \begin{align*} \int_{\Omega}( -\mu \textbf{v} \Delta \textbf{u} - (\lambda + \mu) \textbf{v} \nabla (\nabla\cdot \textbf{u})))dX = \int_{\Omega} \textbf{v}\textbf{f} dX. \end{align*} Now I am not sure what the best way to proceed is. I think I should break everything up into components, but I am not confident. I think I will end up using the divergence theorem (twice?). Would anyone be able to help me work out the details? I think the $\Delta \textbf{u}$ term should be simple, but I have no clue what identities to use for $\nabla (\nabla \cdot \textbf{u})$. I appreciate any help.",,"['multivariable-calculus', 'partial-differential-equations', 'calculus-of-variations', 'finite-element-method']"
16,query on maximum minimum of multi-variable function,query on maximum minimum of multi-variable function,,"Q.How do we find out the sum of absolute maximum and absolute minimum values of $f(x,y)=(x+y)^2-(x+y)+1$ on a unit square $\{(x,y):0<x<1,0<y<1\}$?  Using the method of $rt-s^2$ its value is zero. So now how do I search for the Max and main value?","Q.How do we find out the sum of absolute maximum and absolute minimum values of $f(x,y)=(x+y)^2-(x+y)+1$ on a unit square $\{(x,y):0<x<1,0<y<1\}$?  Using the method of $rt-s^2$ its value is zero. So now how do I search for the Max and main value?",,['multivariable-calculus']
17,Integration by substitution: follow-up,Integration by substitution: follow-up,,"This is a follow-up question to the one I ask here .  Gono's answer indicates that the following is true: Suppose that $f:I \to \Bbb R$ is continuous and $\varphi: [a,b] \to I$ is continuously differentiable.    Then it holds $$ \int _{{a}}^{{b}}f(\varphi (t))\cdot \varphi '(t)\,{\mathrm  {d}}t=\int _{{\varphi (a)}}^{{\varphi (b)}}f(x)\,{\mathrm  {d}}x $$   This holds regardless of whether $\varphi$ is injective I had been under the impression that this was only guaranteed to be valid if $\varphi$ is injective, since injectivity is one of the conditions given in a multivariate setting .  In fact, I thought that I had a counterexample in my question.  Note, however, that with the example I gave, the $f(\varphi(t))\varphi'(t)$ resulting from my substitution failed to coincide with the integrand $t^4$ over $[-1,1]$. So, here's my question: is the above statement true?  Is there a corresponding statement that holds in the multivariate generalization regardless of whether $\varphi$ is injective?","This is a follow-up question to the one I ask here .  Gono's answer indicates that the following is true: Suppose that $f:I \to \Bbb R$ is continuous and $\varphi: [a,b] \to I$ is continuously differentiable.    Then it holds $$ \int _{{a}}^{{b}}f(\varphi (t))\cdot \varphi '(t)\,{\mathrm  {d}}t=\int _{{\varphi (a)}}^{{\varphi (b)}}f(x)\,{\mathrm  {d}}x $$   This holds regardless of whether $\varphi$ is injective I had been under the impression that this was only guaranteed to be valid if $\varphi$ is injective, since injectivity is one of the conditions given in a multivariate setting .  In fact, I thought that I had a counterexample in my question.  Note, however, that with the example I gave, the $f(\varphi(t))\varphi'(t)$ resulting from my substitution failed to coincide with the integrand $t^4$ over $[-1,1]$. So, here's my question: is the above statement true?  Is there a corresponding statement that holds in the multivariate generalization regardless of whether $\varphi$ is injective?",,"['calculus', 'integration', 'multivariable-calculus']"
18,Surface integral over a sphere of inverse of distance,Surface integral over a sphere of inverse of distance,,"Let $S$ be a sphere in $\mathbb{R}^3$ of radius $r$ centered at the origin and $x_0\not\in S$. Let $f:\mathbb{R}^3\to\mathbb{R}$ be given by $f(x)=\Vert x-x_0\Vert$. I'm asked to compute the (surface) integral $$ \int_S fdS $$ I think I have to separate this in the cases $\Vert x_0\Vert>r$ and $\Vert x_0\Vert<r$. For the former, we could use the divergence theorem and for the latter, try to show some kind of invariance and reduce the problem to the case where  we have to integrate over a small sphere around $x_0$. However, I haven't been able to develop this ideas as I can't find suitable vector fields $F$ to work with them. Any kind of help or suggestions are greatly appreciated.","Let $S$ be a sphere in $\mathbb{R}^3$ of radius $r$ centered at the origin and $x_0\not\in S$. Let $f:\mathbb{R}^3\to\mathbb{R}$ be given by $f(x)=\Vert x-x_0\Vert$. I'm asked to compute the (surface) integral $$ \int_S fdS $$ I think I have to separate this in the cases $\Vert x_0\Vert>r$ and $\Vert x_0\Vert<r$. For the former, we could use the divergence theorem and for the latter, try to show some kind of invariance and reduce the problem to the case where  we have to integrate over a small sphere around $x_0$. However, I haven't been able to develop this ideas as I can't find suitable vector fields $F$ to work with them. Any kind of help or suggestions are greatly appreciated.",,"['multivariable-calculus', 'surface-integrals']"
19,differentiable function with all partial derivatives equal,differentiable function with all partial derivatives equal,,"Let $n\ge 3$ and $f\colon\mathbb{R}^n\setminus \overline{B}(0,1)\to\mathbb{R}$ - differentiable with all partial derivatives equal. Prove that there exists differentiable $g\colon\mathbb{R}\to\mathbb{R}$ such that $f(x)=g(x_1+\dots +x_n)$ for all $x=(x_1,\dots,x_n)\in \mathbb{R}^n\setminus \overline{B}(0,1)$. Is it true for $n=2$ also?","Let $n\ge 3$ and $f\colon\mathbb{R}^n\setminus \overline{B}(0,1)\to\mathbb{R}$ - differentiable with all partial derivatives equal. Prove that there exists differentiable $g\colon\mathbb{R}\to\mathbb{R}$ such that $f(x)=g(x_1+\dots +x_n)$ for all $x=(x_1,\dots,x_n)\in \mathbb{R}^n\setminus \overline{B}(0,1)$. Is it true for $n=2$ also?",,"['multivariable-calculus', 'partial-derivative']"
20,Good way of remembering Green's Theorem,Good way of remembering Green's Theorem,,"I am referring to Green's Theorem as follows: $$\int_C P\,dx+Q\,dy=\int\int_R (Q_x-P_y)\,dA.$$ Is there a way to remember it more easily, preferably by a intuitive way? The problem is that I can remember the ""form"" easily enough (by seeing that it is an analogue of the fundamental theorem of Calculus for double integrals), but I may memorize it wrongly as: $$\int P\, dx+Q\,dy=\int\int_R (P_x+Q_y)\,dA$$ for instance. Thanks for any tips.","I am referring to Green's Theorem as follows: $$\int_C P\,dx+Q\,dy=\int\int_R (Q_x-P_y)\,dA.$$ Is there a way to remember it more easily, preferably by a intuitive way? The problem is that I can remember the ""form"" easily enough (by seeing that it is an analogue of the fundamental theorem of Calculus for double integrals), but I may memorize it wrongly as: $$\int P\, dx+Q\,dy=\int\int_R (P_x+Q_y)\,dA$$ for instance. Thanks for any tips.",,"['calculus', 'multivariable-calculus']"
21,"Some questions about the normal vector and Jacobian factor in surface integrals,","Some questions about the normal vector and Jacobian factor in surface integrals,",,"I have some short questions on some lingering confusing concepts, specific to surface integrals: a) Is the surface integral in the Divergence and Stokes's Theorem the same thing? Both require a unit-normal vector and a surface area component, $\vec n$dS, one is clearly a ""flux integral"", but is the one in Stoke's Theorem also a flux integral? b) In both theorems, the surface integrals require unit-normal vectors in the integrand.  But I remember working through some surface integrals where one shouldn't normalize the normal vector obtained -- I don't remember the example concretely now.  Why does this happen?  Why keep the magnitude of the normal vector... sometimes ?  I also have used a formula in the past, but not often: $\vec n$ = $<-f_x,-f_y,1$> - why doesn't this formula show up more often in surface integral problems?  Is this formula for a normal vector $\vec n$ = $<-f_x,-f_y,1$>...normalized / required to be normalized? c) And finally, why do some surface integrals have this factor in its $ds$ surface area component, $\large||\frac{d\vec v}{d\theta}\times\frac{d\vec v} {d\phi}||$,say, for integrating over a surface that is a sphere.  Does this factor just play the role of a ""Jacobian"", but for 3-D surfaces?  And then perhaps in flat surfaces in 3-D space, e.g., a plane disk in space, such a ""Jacobian"" factor is computed differently?  We know the Jacobian to be the determinant of the matrix of partial derivatives, e.g., yielding the variable ""r"" in polar and spherical coordinates integration.  But $\large||\frac{d\vec v}{d\theta}\times\frac{d\vec v} {d\phi}||$ is the magnitude of a determinant (cross-product of partial derivative vectors), though. Thanks so much in advance,","I have some short questions on some lingering confusing concepts, specific to surface integrals: a) Is the surface integral in the Divergence and Stokes's Theorem the same thing? Both require a unit-normal vector and a surface area component, $\vec n$dS, one is clearly a ""flux integral"", but is the one in Stoke's Theorem also a flux integral? b) In both theorems, the surface integrals require unit-normal vectors in the integrand.  But I remember working through some surface integrals where one shouldn't normalize the normal vector obtained -- I don't remember the example concretely now.  Why does this happen?  Why keep the magnitude of the normal vector... sometimes ?  I also have used a formula in the past, but not often: $\vec n$ = $<-f_x,-f_y,1$> - why doesn't this formula show up more often in surface integral problems?  Is this formula for a normal vector $\vec n$ = $<-f_x,-f_y,1$>...normalized / required to be normalized? c) And finally, why do some surface integrals have this factor in its $ds$ surface area component, $\large||\frac{d\vec v}{d\theta}\times\frac{d\vec v} {d\phi}||$,say, for integrating over a surface that is a sphere.  Does this factor just play the role of a ""Jacobian"", but for 3-D surfaces?  And then perhaps in flat surfaces in 3-D space, e.g., a plane disk in space, such a ""Jacobian"" factor is computed differently?  We know the Jacobian to be the determinant of the matrix of partial derivatives, e.g., yielding the variable ""r"" in polar and spherical coordinates integration.  But $\large||\frac{d\vec v}{d\theta}\times\frac{d\vec v} {d\phi}||$ is the magnitude of a determinant (cross-product of partial derivative vectors), though. Thanks so much in advance,",,"['real-analysis', 'integration', 'multivariable-calculus', 'surface-integrals', 'stokes-theorem']"
22,A high-level reason that $(a \times b) \cdot ((b \times c) \times (c \times a)) = (a \cdot (b \times c))^2$,A high-level reason that,(a \times b) \cdot ((b \times c) \times (c \times a)) = (a \cdot (b \times c))^2,"I can do the algebra to prove this identity: $$(\mathbf{a} \times \mathbf{b}) \cdot ((\mathbf{b} \times \mathbf{c}) \times (\mathbf{c} \times \mathbf{a})) = (\mathbf{a} \cdot (\mathbf{b} \times \mathbf{c}))^2$$ in particular, by appeal to the ``BAC -- CAB'' identity mentioned on Wikipedia plus some simplifications. Is there a geometric or linear-algebraic interpretation of this? I know that the scalar triple product is the volume of the paralellopiped formed by its components, and it seems this should be related, but I'm not sure how. Where would volume squared come in?","I can do the algebra to prove this identity: $$(\mathbf{a} \times \mathbf{b}) \cdot ((\mathbf{b} \times \mathbf{c}) \times (\mathbf{c} \times \mathbf{a})) = (\mathbf{a} \cdot (\mathbf{b} \times \mathbf{c}))^2$$ in particular, by appeal to the ``BAC -- CAB'' identity mentioned on Wikipedia plus some simplifications. Is there a geometric or linear-algebraic interpretation of this? I know that the scalar triple product is the volume of the paralellopiped formed by its components, and it seems this should be related, but I'm not sure how. Where would volume squared come in?",,"['multivariable-calculus', 'vectors', 'cross-product']"
23,index notation for $(A \times \nabla) \times B$,index notation for,(A \times \nabla) \times B,"I can't figure out this one, been trying for hours but always get wrong answer. Simplify with index notation $(A\times \nabla) \times B$ Can anyone just show me how to do it once? I think i will get the hang of it if i just see it.","I can't figure out this one, been trying for hours but always get wrong answer. Simplify with index notation Can anyone just show me how to do it once? I think i will get the hang of it if i just see it.",(A\times \nabla) \times B,['calculus']
24,How to intuitively arrive at the total derivative limit and the jacobian matrix?,How to intuitively arrive at the total derivative limit and the jacobian matrix?,,"I'm following this PDF and I need to understand how to arrive at the definition of total derivative geometrically. For now, what I understand is that, from the original definition of derivative: $$\lim_{h\to 0} \frac{f(p+h)-f(x)}{h} = m \implies \lim_{h\to 0}\frac{f(x+p)-f(x)-mh}{h} = 0$$ Which is the same as $$\lim_{|h|\to 0}\frac{|f(x+p)-f(x)-mh|}{|h|} = 0$$ and the advantage of this is that we can know define the existence of the derivative, but now for a multivariable function $f$ from $\mathbb R^m \to \mathbb R^n$. Besides not giving any geometrial interpretation, I quite accept this result. But then, the PDF starts just defining what the total derivative would be. I need an intuitive way to arrive at this, and then, a way to start from the limit definition of the total derivative and arrive at the jacobian matrix. Also, how it was historically derivated for the first time?","I'm following this PDF and I need to understand how to arrive at the definition of total derivative geometrically. For now, what I understand is that, from the original definition of derivative: $$\lim_{h\to 0} \frac{f(p+h)-f(x)}{h} = m \implies \lim_{h\to 0}\frac{f(x+p)-f(x)-mh}{h} = 0$$ Which is the same as $$\lim_{|h|\to 0}\frac{|f(x+p)-f(x)-mh|}{|h|} = 0$$ and the advantage of this is that we can know define the existence of the derivative, but now for a multivariable function $f$ from $\mathbb R^m \to \mathbb R^n$. Besides not giving any geometrial interpretation, I quite accept this result. But then, the PDF starts just defining what the total derivative would be. I need an intuitive way to arrive at this, and then, a way to start from the limit definition of the total derivative and arrive at the jacobian matrix. Also, how it was historically derivated for the first time?",,"['calculus', 'real-analysis', 'multivariable-calculus']"
25,Application of Green's Theorem when undefined at origin,Application of Green's Theorem when undefined at origin,,"Problem: Let $P={-y \over x^2+y^2}$ and $Q={x \over x^2+y^2}$ for $(x,y)\ne(0,0)$. Show that $\oint_{\partial \Omega}(Pdx + Qdy)=2\pi$ if $\Omega$ is any open set containing $(0,0)$ and with a sufficiently regular boundary. Working: Clearly, we cannot immediately apply Green's Theorem, because $P$ and $Q$ are not continuous at $(0,0)$. So, we can create a new region $\Omega_\epsilon$ which is $\Omega$ with a disc of radius $\epsilon$ centered at the origin excised from it. We then note ${\partial Q \over \partial x} - {\partial P \over \partial y} = 0$ and apply Green's Theorem over $\Omega_\epsilon$. Furthermore, $\oint_C(Pdx + Qdy)=2\pi$ if $C$ is any positively oriented circle centered at the origin. I get the general scheme of how to approach this problem, however I am unsure of how to argue it in a rigorous manner.","Problem: Let $P={-y \over x^2+y^2}$ and $Q={x \over x^2+y^2}$ for $(x,y)\ne(0,0)$. Show that $\oint_{\partial \Omega}(Pdx + Qdy)=2\pi$ if $\Omega$ is any open set containing $(0,0)$ and with a sufficiently regular boundary. Working: Clearly, we cannot immediately apply Green's Theorem, because $P$ and $Q$ are not continuous at $(0,0)$. So, we can create a new region $\Omega_\epsilon$ which is $\Omega$ with a disc of radius $\epsilon$ centered at the origin excised from it. We then note ${\partial Q \over \partial x} - {\partial P \over \partial y} = 0$ and apply Green's Theorem over $\Omega_\epsilon$. Furthermore, $\oint_C(Pdx + Qdy)=2\pi$ if $C$ is any positively oriented circle centered at the origin. I get the general scheme of how to approach this problem, however I am unsure of how to argue it in a rigorous manner.",,['real-analysis']
26,To find the extreme values of function,To find the extreme values of function,,"can anyone just help me with the below stated problem: Show that: $1.)$ $\text{sin}(x)+\text{sin}(y)+\text{sin}(x+y)$ , $x,y\in [0,\pi/2]$ has a global maximum $3\sqrt3/2$ at $(\pi/3,\pi/3)$ and global minimum at $(0,0)$ . $2.)$ $f(x,yz)=xyz(1-x-y-z)$ has a maximum at $(1/4,1/4,1/4).$ $3.)$ $f(x,yz)=(x+z)^2+(y+z)^2+xyz$ has no extrema. $4.)$ maximum value of $\text{cos}~a+\text{cos}~b+\text{cos}~c$ where $a,b,c$ are angle of triangle.... I could just have an idea of solving $4.)$ by taking $c=\pi-(a+b)$ and then maximizing the function: $\text{cos}~a+\text{cos}~b+\text{cos}~(a+b)$ where $a,b\lt \pi$ and then take $\partial_af(a,b)=0$ and $\partial_b{f(a,b)}=0$ .... any hint of how to solve the rest of them...... Please help...","can anyone just help me with the below stated problem: Show that: , has a global maximum at and global minimum at . has a maximum at has no extrema. maximum value of where are angle of triangle.... I could just have an idea of solving by taking and then maximizing the function: where and then take and .... any hint of how to solve the rest of them...... Please help...","1.) \text{sin}(x)+\text{sin}(y)+\text{sin}(x+y) x,y\in [0,\pi/2] 3\sqrt3/2 (\pi/3,\pi/3) (0,0) 2.) f(x,yz)=xyz(1-x-y-z) (1/4,1/4,1/4). 3.) f(x,yz)=(x+z)^2+(y+z)^2+xyz 4.) \text{cos}~a+\text{cos}~b+\text{cos}~c a,b,c 4.) c=\pi-(a+b) \text{cos}~a+\text{cos}~b+\text{cos}~(a+b) a,b\lt \pi \partial_af(a,b)=0 \partial_b{f(a,b)}=0","['calculus', 'multivariable-calculus', 'partial-derivative']"
27,cross product in cylindrical coordinates,cross product in cylindrical coordinates,,"Hi i know this is a really really simple question but it has me confused. I want to calculate the cross product of two vectors $$ \vec a \times \vec r. $$ The vectors are given by $$ \vec a= a\hat z,\quad \vec r= x\hat x +y\hat y+z\hat z. $$ The vector $\vec r$ is the radius vector in cartesian coordinates. My problem is: I want to calculate the cross product in cylindrical coordinates, so I need to write $\vec r$ in this coordinate system. The cross product in cartesian coordinates is $$ \vec a \times \vec r=-a y\hat x+ax\hat y, $$ however how can we do this in cylindrical coordinates?  Thank you","Hi i know this is a really really simple question but it has me confused. I want to calculate the cross product of two vectors $$ \vec a \times \vec r. $$ The vectors are given by $$ \vec a= a\hat z,\quad \vec r= x\hat x +y\hat y+z\hat z. $$ The vector $\vec r$ is the radius vector in cartesian coordinates. My problem is: I want to calculate the cross product in cylindrical coordinates, so I need to write $\vec r$ in this coordinate system. The cross product in cartesian coordinates is $$ \vec a \times \vec r=-a y\hat x+ax\hat y, $$ however how can we do this in cylindrical coordinates?  Thank you",,"['multivariable-calculus', 'vector-analysis', 'cross-product']"
28,Lagrange multipliers from hell,Lagrange multipliers from hell,,"I was asked to solve this question, decided to try and solve it with lagrange multipliers as I see no other way: ""Find the closest and furthest points on the circle made from the intersection of the ball $(x-1)^2+(y-2)^2+(z-3)^2=9$ and the plane $x-2z=0$ from the point $(0,0)$"". What I did: the distance for any point $(x,y,z)$ from the origin is $d(x,y,z)=\sqrt{x^2+y^2+z^2}$. so using lagrange multipliers we have: $d(x,y,z)=\sqrt{x^2+y^2+z^2}$ $C_1(x,y,z)=(x-1)^2+(y-2)^2+(z-3)^2-9$ $C_2(x,y,z)=x-2z$ $L(x,y,z) = d-\lambda_1C_1-\lambda_2C_2 $ meaning: $L(x,y,z)=\sqrt{x^2+y^2+z^2}-\lambda_1[(x-1)^2+(y-2)^2+(z-3)^2-9]-\lambda_2(x-2z)$ Let's derive and solve when derivatives are zero: $\frac{\partial L}{\partial x}= \frac{x}{\sqrt{x^2+y^2+z^2}}-2\lambda_1(x-1)-\lambda_2=0$ $\frac{\partial L}{\partial y} = \frac{y}{\sqrt{x^2+y^2+z^2}}-2\lambda_1(y-2)=0$ $\frac{\partial L}{\partial z} = \frac{z}{\sqrt{x^2+y^2+z^2}}-2\lambda_1(z-3)+2\lambda_2=0$ $\frac{\partial L}{\partial \lambda_1} = -(x-1)^2-(y-2)^2-(z-3)^2+9=0$ $\frac{\partial L}{\partial \lambda_2} = 2z-x=0$ Solving this monstrous system seems very unlikely, and very difficult, and not how the question is meant to be solved. am I missing something?","I was asked to solve this question, decided to try and solve it with lagrange multipliers as I see no other way: ""Find the closest and furthest points on the circle made from the intersection of the ball $(x-1)^2+(y-2)^2+(z-3)^2=9$ and the plane $x-2z=0$ from the point $(0,0)$"". What I did: the distance for any point $(x,y,z)$ from the origin is $d(x,y,z)=\sqrt{x^2+y^2+z^2}$. so using lagrange multipliers we have: $d(x,y,z)=\sqrt{x^2+y^2+z^2}$ $C_1(x,y,z)=(x-1)^2+(y-2)^2+(z-3)^2-9$ $C_2(x,y,z)=x-2z$ $L(x,y,z) = d-\lambda_1C_1-\lambda_2C_2 $ meaning: $L(x,y,z)=\sqrt{x^2+y^2+z^2}-\lambda_1[(x-1)^2+(y-2)^2+(z-3)^2-9]-\lambda_2(x-2z)$ Let's derive and solve when derivatives are zero: $\frac{\partial L}{\partial x}= \frac{x}{\sqrt{x^2+y^2+z^2}}-2\lambda_1(x-1)-\lambda_2=0$ $\frac{\partial L}{\partial y} = \frac{y}{\sqrt{x^2+y^2+z^2}}-2\lambda_1(y-2)=0$ $\frac{\partial L}{\partial z} = \frac{z}{\sqrt{x^2+y^2+z^2}}-2\lambda_1(z-3)+2\lambda_2=0$ $\frac{\partial L}{\partial \lambda_1} = -(x-1)^2-(y-2)^2-(z-3)^2+9=0$ $\frac{\partial L}{\partial \lambda_2} = 2z-x=0$ Solving this monstrous system seems very unlikely, and very difficult, and not how the question is meant to be solved. am I missing something?",,"['calculus', 'multivariable-calculus', 'derivatives', 'lagrange-multiplier']"
29,Shortest distance between two curves,Shortest distance between two curves,,"Let $C_1= \{ (x, y) \in \mathrm{R}^2 : y = x^2 +1 \}$ and $C_2= \{ (x, y) \in \mathrm{R}^2 : x = y^2 +1 \}$, find the points which minimize distance between $C_1$ and $C_2$. What I tried is: we know that a generic point for $C_1$ is $(x, x^2 + 1)$, while a generic point for $C_2$ is $(y^2 + 1, y)$. It is pretty clear that the two curves don't intersect, so the distance between a generic pair is: $$||(x, x^2+1) - (y^2 + 1, y)||$$ Which for extreme finding is the same as $$f(x, y) = ||(x - y^2 - 1, x^2+1 - y)||^2$$ Trying to find the critical points means we have to find out what the gradient is: $$\nabla f(x, y) = (4x^3 - 2y^2 - 4yx +6x - 2, 4y^3 - 2x^2 -4yx +6y -2)$$ Hence, I'm looking for the roots of: $$2x^3 - 2y^3 -y^2 +x^2 +3x-3y = 0$$ It is pretty clear that $(x, x) : x \in R$ is always a solution. However, I'm not sure there aren't any other roots. Is there any other method or some other way to solve this problem?","Let $C_1= \{ (x, y) \in \mathrm{R}^2 : y = x^2 +1 \}$ and $C_2= \{ (x, y) \in \mathrm{R}^2 : x = y^2 +1 \}$, find the points which minimize distance between $C_1$ and $C_2$. What I tried is: we know that a generic point for $C_1$ is $(x, x^2 + 1)$, while a generic point for $C_2$ is $(y^2 + 1, y)$. It is pretty clear that the two curves don't intersect, so the distance between a generic pair is: $$||(x, x^2+1) - (y^2 + 1, y)||$$ Which for extreme finding is the same as $$f(x, y) = ||(x - y^2 - 1, x^2+1 - y)||^2$$ Trying to find the critical points means we have to find out what the gradient is: $$\nabla f(x, y) = (4x^3 - 2y^2 - 4yx +6x - 2, 4y^3 - 2x^2 -4yx +6y -2)$$ Hence, I'm looking for the roots of: $$2x^3 - 2y^3 -y^2 +x^2 +3x-3y = 0$$ It is pretty clear that $(x, x) : x \in R$ is always a solution. However, I'm not sure there aren't any other roots. Is there any other method or some other way to solve this problem?",,"['multivariable-calculus', 'optimization']"
30,what is the relationship between vector spaces and rings?,what is the relationship between vector spaces and rings?,,Can you show me an example to show how vector and scalar multiplication works with rings would be really helpful.,Can you show me an example to show how vector and scalar multiplication works with rings would be really helpful.,,"['abstract-algebra', 'multivariable-calculus', 'ring-theory', 'vector-spaces']"
31,Advantages to continuity at a point,Advantages to continuity at a point,,"A scalar field $f : \mathbb{R}^n \to \mathbb{R}$ is said to be continuous at a point $\boldsymbol{a}$ if $$ \lim_{\boldsymbol{x} \to \boldsymbol{a}} f(\boldsymbol{x}) = f(\boldsymbol{a}) $$ So in other words, $f$ has to be defined at $\boldsymbol{a}$ and also has to have a limit at $\boldsymbol{a}$. But isolated points are also defined to be continuous. It seems to me like, given this definition, there isn't really an advantage to having a function continuous at a point, and continuity is only useful on an set or interval, because of the following: $f$ can be continuous at a point but not differentiable (e.g. $f(\boldsymbol{x}) = \|\boldsymbol{x}\|$ at $\boldsymbol{0}$) $f$ can be continuous at a point but the limit doesn't exist (e.g. $f(\boldsymbol{x}) = \sqrt{-\|\boldsymbol{x}\|}$ at $\boldsymbol{0}$ if $f(\boldsymbol{x}) \subset \mathbb{R}$) $f$ can be continuous at a point but the first-order partials don't exist So if $f$ is continuous at a point $\boldsymbol{a}$, is there anything we can say about $f$ at $\boldsymbol{a}$? Or is it just a nice-to-have?","A scalar field $f : \mathbb{R}^n \to \mathbb{R}$ is said to be continuous at a point $\boldsymbol{a}$ if $$ \lim_{\boldsymbol{x} \to \boldsymbol{a}} f(\boldsymbol{x}) = f(\boldsymbol{a}) $$ So in other words, $f$ has to be defined at $\boldsymbol{a}$ and also has to have a limit at $\boldsymbol{a}$. But isolated points are also defined to be continuous. It seems to me like, given this definition, there isn't really an advantage to having a function continuous at a point, and continuity is only useful on an set or interval, because of the following: $f$ can be continuous at a point but not differentiable (e.g. $f(\boldsymbol{x}) = \|\boldsymbol{x}\|$ at $\boldsymbol{0}$) $f$ can be continuous at a point but the limit doesn't exist (e.g. $f(\boldsymbol{x}) = \sqrt{-\|\boldsymbol{x}\|}$ at $\boldsymbol{0}$ if $f(\boldsymbol{x}) \subset \mathbb{R}$) $f$ can be continuous at a point but the first-order partials don't exist So if $f$ is continuous at a point $\boldsymbol{a}$, is there anything we can say about $f$ at $\boldsymbol{a}$? Or is it just a nice-to-have?",,"['multivariable-calculus', 'continuity']"
32,Classifying extrema in the 3d plane,Classifying extrema in the 3d plane,,"I have a function $$f(x,y) = x^4 + x^2 - 6xy + 3y^2$$ for all $x$ and $y$ in $R^2$ so $f_x = 4x^3 + 2x - 6y$ $f_y = -6x + 6y$ $f_{xx} = 12x^2 + 2$ $f_{yy} = 6$ I'm looking for the extrema here and attempting to classify them. Solving the first derivatives set equal to 0, I get the points $(0,0,0); (1,1,-1); (-1,-1,-1)$ Plugging them into the second derivatives, I get positive values for all the points. Wolfram Alpha tells me that only (1,1) and (-1,-1) are global mininma. Why is (0,0) not considered a local min?","I have a function $$f(x,y) = x^4 + x^2 - 6xy + 3y^2$$ for all $x$ and $y$ in $R^2$ so $f_x = 4x^3 + 2x - 6y$ $f_y = -6x + 6y$ $f_{xx} = 12x^2 + 2$ $f_{yy} = 6$ I'm looking for the extrema here and attempting to classify them. Solving the first derivatives set equal to 0, I get the points $(0,0,0); (1,1,-1); (-1,-1,-1)$ Plugging them into the second derivatives, I get positive values for all the points. Wolfram Alpha tells me that only (1,1) and (-1,-1) are global mininma. Why is (0,0) not considered a local min?",,"['multivariable-calculus', 'optimization']"
33,Is this really a typo?,Is this really a typo?,,"Let $U \subseteq \mathbb R^n$ and $F: U \to \mathbb R^m$ a function with coordinate functions $f_i$. My notes say that: If $F$ is differentiable on $U$ the Jacobian of $F$ is defined at each point in $U$, its $nm$ entries are functions on $U$. These functions need not be continuous on $U$; they are continuous on $U$ if and only if $F$ is of class $C^1$. My problem is with the only if part of last part of the statement: it should be $C^k$ with  $k \ge 1$, right? (I'm merely asking because there is this nagging doubt I have that it might not be a typo after all.)","Let $U \subseteq \mathbb R^n$ and $F: U \to \mathbb R^m$ a function with coordinate functions $f_i$. My notes say that: If $F$ is differentiable on $U$ the Jacobian of $F$ is defined at each point in $U$, its $nm$ entries are functions on $U$. These functions need not be continuous on $U$; they are continuous on $U$ if and only if $F$ is of class $C^1$. My problem is with the only if part of last part of the statement: it should be $C^k$ with  $k \ge 1$, right? (I'm merely asking because there is this nagging doubt I have that it might not be a typo after all.)",,"['differential-geometry', 'multivariable-calculus', 'derivatives']"
34,How to evaluate double integrals over a region?,How to evaluate double integrals over a region?,,"Evaluate the double integral $\iint_D(1/x)dA$, where D is the region bounded by the circles $x^2+y^2=1$ and $x^2+y^2=2x$ Alright so first I converted to polar coordinates: $$ x^2 + y^2 = 1 \ \Rightarrow \ r = 1 \ \ , \ \ x^2 + y^2 = 2x \ \Rightarrow \ r^2 = 2r \cos θ \ \Rightarrow \ r = 2 \cos θ \ .  $$ Points of intersection: $ 2 \cos θ = 1 \ \Rightarrow \ θ = ±π/3 \ , $ $ 2 \cos θ > 1 $ for θ in (-π/3, π/3). So, $$ \int \int_D \ (1/x) \ \ dA  \ \ = \ \ \int_{-π/3}^{π/3} \ \int_1^{2 \cos θ} \ \frac{1}{r \cos θ} \ \ r dr \  dθ  $$ $$ = \ \ \int_{-π/3}^{π/3} \ \int_1^{2 \cos θ} \ \sec θ \ \ dr \  dθ \ \  = \ \ \int_{-π/3}^{π/3} \ (2 \cos θ - 1) \sec θ \ \ dθ $$ $$ = \ \ 2 \ \int_0^{π/3} \ (2 - \sec θ) \ \ dθ \ \ , $$ (since the integrand is even) $$ = \ \ 2 \ (2 θ \ - \ \ln |\sec θ + \tan θ| \ ) \vert_0^{π/3} \ \ = \ \ \frac{4π}{3} \ - \ 2 \ln(2 + √3) \ \ .  $$ I'm not sure this is right. Could someone look over it?","Evaluate the double integral $\iint_D(1/x)dA$, where D is the region bounded by the circles $x^2+y^2=1$ and $x^2+y^2=2x$ Alright so first I converted to polar coordinates: $$ x^2 + y^2 = 1 \ \Rightarrow \ r = 1 \ \ , \ \ x^2 + y^2 = 2x \ \Rightarrow \ r^2 = 2r \cos θ \ \Rightarrow \ r = 2 \cos θ \ .  $$ Points of intersection: $ 2 \cos θ = 1 \ \Rightarrow \ θ = ±π/3 \ , $ $ 2 \cos θ > 1 $ for θ in (-π/3, π/3). So, $$ \int \int_D \ (1/x) \ \ dA  \ \ = \ \ \int_{-π/3}^{π/3} \ \int_1^{2 \cos θ} \ \frac{1}{r \cos θ} \ \ r dr \  dθ  $$ $$ = \ \ \int_{-π/3}^{π/3} \ \int_1^{2 \cos θ} \ \sec θ \ \ dr \  dθ \ \  = \ \ \int_{-π/3}^{π/3} \ (2 \cos θ - 1) \sec θ \ \ dθ $$ $$ = \ \ 2 \ \int_0^{π/3} \ (2 - \sec θ) \ \ dθ \ \ , $$ (since the integrand is even) $$ = \ \ 2 \ (2 θ \ - \ \ln |\sec θ + \tan θ| \ ) \vert_0^{π/3} \ \ = \ \ \frac{4π}{3} \ - \ 2 \ln(2 + √3) \ \ .  $$ I'm not sure this is right. Could someone look over it?",,['multivariable-calculus']
35,Change of variables to a desirable form,Change of variables to a desirable form,,"Suppose we have smooth function $\varphi: \mathbb{R}^n \to \mathbb{R}$ and let $x_0 \in \mathbb{R}^n$ be a non-degenerate critical point. That is, $$\begin{equation} \varphi'(x_0) = \nabla \varphi(x_0) = 0, \quad \text{and} \quad \text{det }(\varphi''(x_0)) \neq 0 \end{equation}$$ where $\varphi ''(x)$ stands for the Hessian matrix of second partial derivatives. Suppose that $(r,n-r)$ is the signature of $\varphi '' (x_0)$ (this means that the number of postive and negative eigenvalues is $r$ and $n - r$ respectively). I am currently trying to understand a proof for the Morse Lemma, in which it is taken for granted that I see the following statement as trivial: Given the above assumptions, after a translation and a linear change of coordinates, we may assume that $x_0 = 0$ and that $$\begin{equation} \varphi(x) = \frac{1}{2}(x^2_1 + \cdots + x_r^2 - x^2_{r + 1} - \cdots - x_n^2) + \mathcal{O}(|x|^3) \quad x \to 0 \end{equation} $$ Why is this so? I understand that the translation $y(x) = x - x_0$ gives me the result that $y(x_0) = 0$. Then I think I need to apply a linear transformation that somehow involves the Hessian $\varphi^''(x_0)$, but I need to diagonalize and rescale it, that is I need to transform to a basis of its eigenvectors that are scaled by the inverses of the eigenvalues, so that I get the $\pm \frac{1}{2}$ - coefficients. Then I think I need to use Taylor's expansion? I can use the fact that the gradient at $x_0$ is zero. But then I would still need the value $\varphi(0)$ in the above equation$\ldots$ here I guess the text I am using might have a typo. If I could get some feedback on whether the above rough reasoning goes into the right direction that would be a huge help! Many thanks!","Suppose we have smooth function $\varphi: \mathbb{R}^n \to \mathbb{R}$ and let $x_0 \in \mathbb{R}^n$ be a non-degenerate critical point. That is, $$\begin{equation} \varphi'(x_0) = \nabla \varphi(x_0) = 0, \quad \text{and} \quad \text{det }(\varphi''(x_0)) \neq 0 \end{equation}$$ where $\varphi ''(x)$ stands for the Hessian matrix of second partial derivatives. Suppose that $(r,n-r)$ is the signature of $\varphi '' (x_0)$ (this means that the number of postive and negative eigenvalues is $r$ and $n - r$ respectively). I am currently trying to understand a proof for the Morse Lemma, in which it is taken for granted that I see the following statement as trivial: Given the above assumptions, after a translation and a linear change of coordinates, we may assume that $x_0 = 0$ and that $$\begin{equation} \varphi(x) = \frac{1}{2}(x^2_1 + \cdots + x_r^2 - x^2_{r + 1} - \cdots - x_n^2) + \mathcal{O}(|x|^3) \quad x \to 0 \end{equation} $$ Why is this so? I understand that the translation $y(x) = x - x_0$ gives me the result that $y(x_0) = 0$. Then I think I need to apply a linear transformation that somehow involves the Hessian $\varphi^''(x_0)$, but I need to diagonalize and rescale it, that is I need to transform to a basis of its eigenvectors that are scaled by the inverses of the eigenvalues, so that I get the $\pm \frac{1}{2}$ - coefficients. Then I think I need to use Taylor's expansion? I can use the fact that the gradient at $x_0$ is zero. But then I would still need the value $\varphi(0)$ in the above equation$\ldots$ here I guess the text I am using might have a typo. If I could get some feedback on whether the above rough reasoning goes into the right direction that would be a huge help! Many thanks!",,"['calculus', 'real-analysis', 'differential-geometry', 'multivariable-calculus']"
36,An harmonic radial function in $\mathbb{R}^2$,An harmonic radial function in,\mathbb{R}^2,"I'm taking multivariable-calculus, and I got the following question: A function $f$ in n variables is called harmonic if $\sum_{i = 1}^{n}{\frac{\partial ^2 f}{\partial x_{i}^2}} = 0$. Is there a non-constant, radial harmonic function in $\mathbb{R}^2$? I found an almost-identical question here (PDF file) (number three), and I'm guessing that the answer is no. One explanation I could think of, is that if there was such function, it would contradict the mean value property at the origin. However, we didn't learn about the mean value property (or about harmonic functions in general), so I'm not sure if I'm correct and either way I can't use it. I feel like there is something very simple I'm missing. Ideas? Thanks!","I'm taking multivariable-calculus, and I got the following question: A function $f$ in n variables is called harmonic if $\sum_{i = 1}^{n}{\frac{\partial ^2 f}{\partial x_{i}^2}} = 0$. Is there a non-constant, radial harmonic function in $\mathbb{R}^2$? I found an almost-identical question here (PDF file) (number three), and I'm guessing that the answer is no. One explanation I could think of, is that if there was such function, it would contradict the mean value property at the origin. However, we didn't learn about the mean value property (or about harmonic functions in general), so I'm not sure if I'm correct and either way I can't use it. I feel like there is something very simple I'm missing. Ideas? Thanks!",,['multivariable-calculus']
37,Basic Question on Gradients,Basic Question on Gradients,,"I am having trouble understanding how the gradient of a scalar field is the direction along the $x$-$y$ plane that yields the maximum inclination. Sure it takes into account the partial derivatives along both the x and y axes but does that mean ""maximum inclination""? How do the partial derivatives come into the gradient vector field? A basic intuition would be great. Thank You in advance.","I am having trouble understanding how the gradient of a scalar field is the direction along the $x$-$y$ plane that yields the maximum inclination. Sure it takes into account the partial derivatives along both the x and y axes but does that mean ""maximum inclination""? How do the partial derivatives come into the gradient vector field? A basic intuition would be great. Thank You in advance.",,[]
38,Calculating Triple Integral using Cylindrical Coordinates,Calculating Triple Integral using Cylindrical Coordinates,,"I'm given $ E $ is located in $ x^2 + y^2 = (z-1)^2 $ and between $z = 0$ and $z=2$ . I used level curves to graph this out, and as I see it is a circular cone. First, I set up my region, $$ E = \Big\{(r,\theta,z) | 0 \leq r \leq 1, 0 \leq \theta \leq 2\pi, 0 \leq z \leq 2\Big\}.$$ I then set up my integral as $$ \int_{0}^{2\pi}\int_{0}^{2}\int_{0}^{1} rdrdzd\theta $$ This resulted in $2\pi$ , which was wrong. So then I changed the bounds of my $z$ by solving the equation for $z$ , where $z = 1 \pm \sqrt{x^2+y^2} = 1 \pm r $ so that resulted in this integral: $$  \int_{0}^{2\pi}\int_{0}^{1}\int_{1-r}^{1+r} rdzdrd\theta $$ which resulted in $\frac{4}{3}\pi$ , which was wrong as well. What am I doing wrong? The only thing I can think of is to integrate on the equation $z = 1 + \sqrt{x^2+y^2}$ but I am not sure that is right either.","I'm given is located in and between and . I used level curves to graph this out, and as I see it is a circular cone. First, I set up my region, I then set up my integral as This resulted in , which was wrong. So then I changed the bounds of my by solving the equation for , where so that resulted in this integral: which resulted in , which was wrong as well. What am I doing wrong? The only thing I can think of is to integrate on the equation but I am not sure that is right either."," E   x^2 + y^2 = (z-1)^2  z = 0 z=2  E = \Big\{(r,\theta,z) | 0 \leq r \leq 1, 0 \leq \theta \leq 2\pi, 0 \leq z \leq 2\Big\}.  \int_{0}^{2\pi}\int_{0}^{2}\int_{0}^{1} rdrdzd\theta  2\pi z z z = 1 \pm \sqrt{x^2+y^2} = 1 \pm r    \int_{0}^{2\pi}\int_{0}^{1}\int_{1-r}^{1+r} rdzdrd\theta  \frac{4}{3}\pi z = 1 + \sqrt{x^2+y^2}","['multivariable-calculus', 'multiple-integral', 'cylindrical-coordinates']"
39,Computing $\iint_S (y+z)\mathrm{d}y\mathrm{d}z+(z+x)\mathrm{d}z\mathrm{d}x+(x+y)\mathrm{d}x\mathrm{d}y$,Computing,\iint_S (y+z)\mathrm{d}y\mathrm{d}z+(z+x)\mathrm{d}z\mathrm{d}x+(x+y)\mathrm{d}x\mathrm{d}y,"Computing $$I=\iint_S (y+z)\mathrm{d}y\mathrm{d}z+(z+x)\mathrm{d}z\mathrm{d}x+(x+y)\mathrm{d}x\mathrm{d}y,$$ where $S$ is the upper side of the plane $x+y+z=1$ located inside the interior of the sphere $x^2+y^2+z^2 \leq 1$ . I have tried two methods: First, I attempted to close the surface and use Divergence Theorem, but the integral over the sphere surface was also difficult to calculate. Second, I directly projected the integral onto the $xy$ -plane, but the projected surface became an ellipse, and I am not familiar with handling such cases. I wonder if there are any other solutions available. I found a stupid mistake in my second solution. I've corrected it and wrote an answer.","Computing where is the upper side of the plane located inside the interior of the sphere . I have tried two methods: First, I attempted to close the surface and use Divergence Theorem, but the integral over the sphere surface was also difficult to calculate. Second, I directly projected the integral onto the -plane, but the projected surface became an ellipse, and I am not familiar with handling such cases. I wonder if there are any other solutions available. I found a stupid mistake in my second solution. I've corrected it and wrote an answer.","I=\iint_S (y+z)\mathrm{d}y\mathrm{d}z+(z+x)\mathrm{d}z\mathrm{d}x+(x+y)\mathrm{d}x\mathrm{d}y, S x+y+z=1 x^2+y^2+z^2 \leq 1 xy","['multivariable-calculus', 'surface-integrals']"
40,Implicit function theorem: from local to global,Implicit function theorem: from local to global,,"Suppose that we have $F(x,y)=0$ satisfying the usual hypotheses of the IFT at $(0,0)$ , but such that not only $F_y(0,0)\neq 0$ , but $F(x,y)=0$ and $F_y(x,y)\neq 0$ for all $x\in U\ni0$ , where the open interval $U\subset\mathbb{R}$ . Then instead of having only a unique local function $y(x):(-\varepsilon,\varepsilon)\longrightarrow V'$ such that $F(x,y(x))=0$ for some $V'\subset\mathbb{R}$ , we should be able to assert the existence of a unique global function $y^*(x):U\longrightarrow V''$ such that $F(x,y^*(x))=0$ . The way I was thinking of this was the following: say we have $y=y_0$ , $\varepsilon=\varepsilon_0$ on $(-\varepsilon_0,\varepsilon_0)$ . Then we can consider $F(\varepsilon_0,y_0(\varepsilon_0))=0$ , $F_y(\varepsilon_0,y_0(\varepsilon_0))\neq 0$ and find another uniques $y_1:(\varepsilon_0-\varepsilon_1,\varepsilon_0+\varepsilon_1)\longrightarrow V'''$ and on the overlap by uniqueness $y_0\equiv y_1$ . So defining $y^*$ by cases as $y_0$ on $(\varepsilon_0,\varepsilon_0)$ and $y_1$ on $[\varepsilon_0,\varepsilon_1)$ we have, repeating this inductively, a $y_*$ globally defined not only on $(-\varepsilon_0,\varepsilon_1)$ , but on $U$ . However this can be only if the sequence $\varepsilon_i$ does not decrease too fast and the boundary values $y_i(\varepsilon_i)$ satisfy $F_y(\varepsilon_i,y_i(\varepsilon_i))\neq 0$ and $F(\varepsilon_i,y_i(\varepsilon_i))=0$ at each step. The second of these issues is taken care of by the hypothesis that $F(x,y)=0$ for every $x\in U$ , since this ensures that if $F(\varepsilon_i,y_i(\varepsilon_i))\neq0$ for some $i$ , we have covered the whole of $U$ (I am thinking only about its right half, the reasoning extends symmetrically). My question is: what are usual hypotheses which on the other hand ensure that the $\varepsilon_i$ do not decrease too fast? Are the ones given above already enough? Is it possible to devise one that ensures this? I think perhaps by contradiction one can show that the $\varepsilon_i$ can only stop at the boundary of $U$ , since if, say, they converge at some $-u<b<u$ , where $U=(-u,u)$ for example. Then one can take $b$ and apply the IFT there, extending the unique solution a little further, against the hypothesis that it could not be done. Is this the right idea? Thank you all for any help.","Suppose that we have satisfying the usual hypotheses of the IFT at , but such that not only , but and for all , where the open interval . Then instead of having only a unique local function such that for some , we should be able to assert the existence of a unique global function such that . The way I was thinking of this was the following: say we have , on . Then we can consider , and find another uniques and on the overlap by uniqueness . So defining by cases as on and on we have, repeating this inductively, a globally defined not only on , but on . However this can be only if the sequence does not decrease too fast and the boundary values satisfy and at each step. The second of these issues is taken care of by the hypothesis that for every , since this ensures that if for some , we have covered the whole of (I am thinking only about its right half, the reasoning extends symmetrically). My question is: what are usual hypotheses which on the other hand ensure that the do not decrease too fast? Are the ones given above already enough? Is it possible to devise one that ensures this? I think perhaps by contradiction one can show that the can only stop at the boundary of , since if, say, they converge at some , where for example. Then one can take and apply the IFT there, extending the unique solution a little further, against the hypothesis that it could not be done. Is this the right idea? Thank you all for any help.","F(x,y)=0 (0,0) F_y(0,0)\neq 0 F(x,y)=0 F_y(x,y)\neq 0 x\in U\ni0 U\subset\mathbb{R} y(x):(-\varepsilon,\varepsilon)\longrightarrow V' F(x,y(x))=0 V'\subset\mathbb{R} y^*(x):U\longrightarrow V'' F(x,y^*(x))=0 y=y_0 \varepsilon=\varepsilon_0 (-\varepsilon_0,\varepsilon_0) F(\varepsilon_0,y_0(\varepsilon_0))=0 F_y(\varepsilon_0,y_0(\varepsilon_0))\neq 0 y_1:(\varepsilon_0-\varepsilon_1,\varepsilon_0+\varepsilon_1)\longrightarrow V''' y_0\equiv y_1 y^* y_0 (\varepsilon_0,\varepsilon_0) y_1 [\varepsilon_0,\varepsilon_1) y_* (-\varepsilon_0,\varepsilon_1) U \varepsilon_i y_i(\varepsilon_i) F_y(\varepsilon_i,y_i(\varepsilon_i))\neq 0 F(\varepsilon_i,y_i(\varepsilon_i))=0 F(x,y)=0 x\in U F(\varepsilon_i,y_i(\varepsilon_i))\neq0 i U \varepsilon_i \varepsilon_i U -u<b<u U=(-u,u) b","['real-analysis', 'calculus', 'multivariable-calculus', 'implicit-function-theorem', 'implicit-function']"
41,jacobian on a line,jacobian on a line,,"How does the interpretation of the Jacobian as the stretch factor apply when looking at a function on a subset. Say my function is projection of the plane to the x-axis, $(x,y)\mapsto (x,0)$ . As a function from the plane to plane it collapses volumes so the Jacobian is 0 like I expect, $|\begin{pmatrix}1&0\\0&0\end{pmatrix}|=0.$ But what if this projection is viewed as a function from the line $y=x$ ? It is one-to-one here. It looks like a segment of volume/length $\sqrt{2}$ maps to one of volume/length $1$ , so I would expect a ""Jacobian-like"" object to be $1/\sqrt{2}$ . What is the Jacobian like object? From reading on the Internet I have the impression this has something to do with differential forms which I have not studied, only multivariable calculus, and I already graduated. Is it possible to understand, at least with simple things like lines, without that theory?","How does the interpretation of the Jacobian as the stretch factor apply when looking at a function on a subset. Say my function is projection of the plane to the x-axis, . As a function from the plane to plane it collapses volumes so the Jacobian is 0 like I expect, But what if this projection is viewed as a function from the line ? It is one-to-one here. It looks like a segment of volume/length maps to one of volume/length , so I would expect a ""Jacobian-like"" object to be . What is the Jacobian like object? From reading on the Internet I have the impression this has something to do with differential forms which I have not studied, only multivariable calculus, and I already graduated. Is it possible to understand, at least with simple things like lines, without that theory?","(x,y)\mapsto (x,0) |\begin{pmatrix}1&0\\0&0\end{pmatrix}|=0. y=x \sqrt{2} 1 1/\sqrt{2}","['linear-algebra', 'multivariable-calculus', 'projection']"
42,Solving triple integral with cylindrical coordinates,Solving triple integral with cylindrical coordinates,,"We are told to evaluate the triple integral: $$\iiint_E z dV$$ where $E$ is bounded by $x=4y^2+4z^2$ and $x=4$ . My attempt: First I noticed that this represents a paraboloid on the x axis so I thought to use cylindrical coordinates (however as the paraboloid was centered around x I wasnt sure whether to let $z=r\cos\theta$ , $y=r\sin\theta$ or the other way around?) $$z=r\cos\theta$$ $$y=r\sin\theta$$ $$0<r<\frac{\sqrt x}{2}$$ $$0<\theta<2\pi$$ $$0<x<4$$ and our integral becomes $$\int_0^4\int_0^{2\pi}\int_0^{\frac{\sqrt x}{2}} r\cos\theta r dr d\theta dx=0$$ However my textbook James Stewart Calculus gives me $\frac{16\pi}{3}$ . Where have I gone wrong?","We are told to evaluate the triple integral: where is bounded by and . My attempt: First I noticed that this represents a paraboloid on the x axis so I thought to use cylindrical coordinates (however as the paraboloid was centered around x I wasnt sure whether to let , or the other way around?) and our integral becomes However my textbook James Stewart Calculus gives me . Where have I gone wrong?",\iiint_E z dV E x=4y^2+4z^2 x=4 z=r\cos\theta y=r\sin\theta z=r\cos\theta y=r\sin\theta 0<r<\frac{\sqrt x}{2} 0<\theta<2\pi 0<x<4 \int_0^4\int_0^{2\pi}\int_0^{\frac{\sqrt x}{2}} r\cos\theta r dr d\theta dx=0 \frac{16\pi}{3},"['integration', 'multivariable-calculus', 'cylindrical-coordinates']"
43,Proving that $\ln\frac{1-x}{\sqrt{1-2\delta(1-x)}-x}$ is convex with respect to $x$,Proving that  is convex with respect to,\ln\frac{1-x}{\sqrt{1-2\delta(1-x)}-x} x,"I am working on an optimisation problem and I am trying to prove that a binary function is convex for one of its independent variables. The function is: $$f(x,\delta)=\ln \frac{1-x}{\sqrt{1-2\delta(1-x)}-x},$$ where $x \in [0,1]$ and $\delta \in \left[0,\dfrac{1}{2}\right]$ . I want to prove that $f(x,\delta)$ is a convex function of $x$ . Actually, I have learned that it is true using Mathematica by graphing it, except at the point that $x=1$ . However, I do not know how to prove it mathematically. I have already tried to prove it by calculating $\dfrac{\partial^2 f}{\partial x^2}$ , which is $$-\frac{1}{(-1+x)^2}+\frac{\left(-1+\frac{\delta}{\sqrt{\smash[b]{1-2(1-x) \delta}}}\right)^2}{(-x+\sqrt{1-2(1-x) \delta})^2}+\frac{\delta^2}{(1-2(1-x) \delta)^{3 / 2}(-x+\sqrt{1-2(1-x) \delta})}.$$ However, I cannot prove that it is always larger than $0$ . I hope someone can give me some help. Thanks in advance!","I am working on an optimisation problem and I am trying to prove that a binary function is convex for one of its independent variables. The function is: where and . I want to prove that is a convex function of . Actually, I have learned that it is true using Mathematica by graphing it, except at the point that . However, I do not know how to prove it mathematically. I have already tried to prove it by calculating , which is However, I cannot prove that it is always larger than . I hope someone can give me some help. Thanks in advance!","f(x,\delta)=\ln \frac{1-x}{\sqrt{1-2\delta(1-x)}-x}, x \in [0,1] \delta \in \left[0,\dfrac{1}{2}\right] f(x,\delta) x x=1 \dfrac{\partial^2 f}{\partial x^2} -\frac{1}{(-1+x)^2}+\frac{\left(-1+\frac{\delta}{\sqrt{\smash[b]{1-2(1-x) \delta}}}\right)^2}{(-x+\sqrt{1-2(1-x) \delta})^2}+\frac{\delta^2}{(1-2(1-x) \delta)^{3 / 2}(-x+\sqrt{1-2(1-x) \delta})}. 0","['multivariable-calculus', 'convex-analysis']"
44,Evaluate the following humongous expression,Evaluate the following humongous expression,,PROBLEM : Evaluate $$\left(\frac{\displaystyle\sum_{n=-\infty}^{\infty}\frac{1}{1+n^2}}{\operatorname{coth}(\pi)}\right)^2$$ CONTEXT : I saw a very interesting and yet intimidating question on the internet: Find the value of $$\frac{16\displaystyle\int_0^\pi\int_0^1x^2\cdot\operatorname{sin}(y)\:\:dxdy\:\:\left(\frac{\displaystyle\sum_{n=-\infty}^{\infty}\frac{1}{1+n^2}}{\operatorname{coth}(\pi)}\right)^2}{\displaystyle\sum_{n=1}^{\infty}\frac{1}{n^2}}+5$$ I just know or rather heard that (though I don't know the proof) $$\sum_{n=1}^{\infty}\frac{1}{n^2}=\frac{{\pi}^2}{6}$$ and (I calculated it) $$16\displaystyle\int_0^\pi\int_0^1x^2\cdot\operatorname{sin}(y)\:\:dxdy=\frac{32}{3}$$ but I can't calculate the value of the expression written in the big brackets. Any help is greatly appreciated.,PROBLEM : Evaluate CONTEXT : I saw a very interesting and yet intimidating question on the internet: Find the value of I just know or rather heard that (though I don't know the proof) and (I calculated it) but I can't calculate the value of the expression written in the big brackets. Any help is greatly appreciated.,\left(\frac{\displaystyle\sum_{n=-\infty}^{\infty}\frac{1}{1+n^2}}{\operatorname{coth}(\pi)}\right)^2 \frac{16\displaystyle\int_0^\pi\int_0^1x^2\cdot\operatorname{sin}(y)\:\:dxdy\:\:\left(\frac{\displaystyle\sum_{n=-\infty}^{\infty}\frac{1}{1+n^2}}{\operatorname{coth}(\pi)}\right)^2}{\displaystyle\sum_{n=1}^{\infty}\frac{1}{n^2}}+5 \sum_{n=1}^{\infty}\frac{1}{n^2}=\frac{{\pi}^2}{6} 16\displaystyle\int_0^\pi\int_0^1x^2\cdot\operatorname{sin}(y)\:\:dxdy=\frac{32}{3},"['real-analysis', 'integration', 'multivariable-calculus', 'definite-integrals', 'summation']"
45,Convert the double integral $\int_{-T/2}^{T/2}\int_{-T/2}^{T/2}g(\alpha-\beta)d\alpha d\beta$ to a single integral with respect to $\tau=\alpha-\beta$,Convert the double integral  to a single integral with respect to,\int_{-T/2}^{T/2}\int_{-T/2}^{T/2}g(\alpha-\beta)d\alpha d\beta \tau=\alpha-\beta,"Question I am self-studying signal processing and ran into the following double integral: $$\int_{-T/2}^{T/2}\int_{-T/2}^{T/2}g(\alpha-\beta)d\alpha d\beta$$ The YouTube video I am watching says this integral can be converted to a single integral using variable substitution with $\tau=\alpha-\beta$ . $$ \int_{-T/2}^{T/2}\int_{-T/2}^{T/2}g(\alpha-\beta)d\alpha d\beta=T\int_{-T}^T g(\tau)\left(1-\frac{|\tau|}{T}\right)d\tau $$ but I'm having trouble proving it. Attempt Following this question , I tried variable substitution with $\tau=\alpha-\beta$ and $\phi=\alpha+\beta$ . Then \begin{align*} \alpha&=\tau+\beta\\ \beta&=\alpha-\tau\\ \alpha&=\phi-\beta\\ \beta&=\phi-\alpha \end{align*} so the Jacobian is $$ \frac{\partial (\alpha,\beta)}{\partial (\tau,\phi)}= \left|\begin{matrix} 1 & -1\\ 1 & 1 \end{matrix}\right|=2 $$ If I'm doing this correctly, the new integral would be $$ \int_{-T}^{T}\int_{-T}^{T}g(\tau)(2)d\tau d\phi $$ But this doesn't seem to be correct. What is the best way to approach this integral?","Question I am self-studying signal processing and ran into the following double integral: The YouTube video I am watching says this integral can be converted to a single integral using variable substitution with . but I'm having trouble proving it. Attempt Following this question , I tried variable substitution with and . Then so the Jacobian is If I'm doing this correctly, the new integral would be But this doesn't seem to be correct. What is the best way to approach this integral?","\int_{-T/2}^{T/2}\int_{-T/2}^{T/2}g(\alpha-\beta)d\alpha d\beta \tau=\alpha-\beta 
\int_{-T/2}^{T/2}\int_{-T/2}^{T/2}g(\alpha-\beta)d\alpha d\beta=T\int_{-T}^T g(\tau)\left(1-\frac{|\tau|}{T}\right)d\tau
 \tau=\alpha-\beta \phi=\alpha+\beta \begin{align*}
\alpha&=\tau+\beta\\
\beta&=\alpha-\tau\\
\alpha&=\phi-\beta\\
\beta&=\phi-\alpha
\end{align*} 
\frac{\partial (\alpha,\beta)}{\partial (\tau,\phi)}=
\left|\begin{matrix}
1 & -1\\
1 & 1
\end{matrix}\right|=2
 
\int_{-T}^{T}\int_{-T}^{T}g(\tau)(2)d\tau d\phi
","['integration', 'multivariable-calculus', 'substitution']"
46,A problematic integral,A problematic integral,,"I want to evaluate the integral $$I=\int_0^\infty\frac{\ln x\ln(1+x)}{x(1+x^2)}\,dx$$ This is a part of one of the solutions of my previous posts here . The user who posted this hasn't visited the site from over a month, so I thought it would be better to ask it as a post instead of a comment on the original solution. The OP suggests to using a parameter. So I tried doing that. $$I(a)=\int_0^\infty\frac{\ln x\ln(1+ax)}{x(x^2+1)}\,dx$$ Differentiating $$\begin{align}I'(a)&=\int_0^\infty\frac{\ln x}{(1+ax)(x^2+1)}\,dx\\&=\int_0^1\frac{\ln x}{(x^2+1)(ax+1)}\,dx+\int_1^\infty\frac{\ln x}{(ax+1)(x^2+1)}\,dx\\&=\int_0^1\frac{\ln x}{(ax+1)(x^2+1)}-\frac{x\ln x}{(x+a)(x^2+1)}\,dx\\&=\frac1{a^2+1}\int_0^1\frac{a^2\ln x}{ax+1}-\frac{2ax\ln x}{x^2+1}+\frac{a\ln x}{x+a}\,dx\\I'(a)&=\frac{\pi^2a}{24(a^2+1)}+\frac1{a^2+1}\int_0^1\frac{a^2\ln x}{ax+1}+\frac{a\ln x}{x+a}\,dx\end{align}$$ Now integrating from $0$ to $1$ , $$\begin{align}I&=\frac{\pi^2}{48}\ln 2+\int_0^1\ln x\int_0^1\frac{a^2}{(a^2+1)(ax+1)}+\frac a{(x+a)(a^2+1)}\,da\,dx\\&=\frac{\pi^2}{48}\ln2+\int_0^1\frac{\ln x}{x^2+1}\int_0^1\frac{2ax}{a^2+1}+\frac1{ax+1}-\frac x{a+x}\,da\,dx\\I&=\frac{\pi^2}{48}\ln2+\ln2\int_0^1\frac{x\ln x}{x^2+1}\,dx+\int_0^1\frac{\ln x\ln(1+x)}{x(x^2+1)}\,dx-\int_0^1\frac{x\ln x\ln(1+x)}{1+x^2}\,dx+\int_0^1\frac{x\ln^2x}{x^2+1}\,dx\end{align}$$ Now solving the first and last integral and using the already proved relation $$\int_0^1\frac{\ln x\ln(1+x)}{x(1+x^2)}\,dx=-\frac34\zeta(3)-\int_0^1\frac{x\ln x\ln(1+x)}{1+x^2}\,dx$$ we get $$I=-2\int_0^1\frac{x\ln x\ln(1+x)}{1+x^2}\,dx-\frac9{16}\zeta(3)$$ This doesn't help as this is what I was solving for in the first place. Is there a better way to solve for $I$ ? Or maybe there is some possible manipulation in my solution that can potentially give a closed form. Any help would be welcomed. Please also check the original post for further clarification. Probably the final edit: I've found that if we represent my last two integrals after differentiation using dilogarithm and use the identity $$\text{Li}_2(z)+\text{Li}_2(z^{-1})=-\frac{\pi^2}6-\frac{\ln^2(-z)}2$$ we can easily solve it. No need to do manipulations using double integrals.","I want to evaluate the integral This is a part of one of the solutions of my previous posts here . The user who posted this hasn't visited the site from over a month, so I thought it would be better to ask it as a post instead of a comment on the original solution. The OP suggests to using a parameter. So I tried doing that. Differentiating Now integrating from to , Now solving the first and last integral and using the already proved relation we get This doesn't help as this is what I was solving for in the first place. Is there a better way to solve for ? Or maybe there is some possible manipulation in my solution that can potentially give a closed form. Any help would be welcomed. Please also check the original post for further clarification. Probably the final edit: I've found that if we represent my last two integrals after differentiation using dilogarithm and use the identity we can easily solve it. No need to do manipulations using double integrals.","I=\int_0^\infty\frac{\ln x\ln(1+x)}{x(1+x^2)}\,dx I(a)=\int_0^\infty\frac{\ln x\ln(1+ax)}{x(x^2+1)}\,dx \begin{align}I'(a)&=\int_0^\infty\frac{\ln x}{(1+ax)(x^2+1)}\,dx\\&=\int_0^1\frac{\ln x}{(x^2+1)(ax+1)}\,dx+\int_1^\infty\frac{\ln x}{(ax+1)(x^2+1)}\,dx\\&=\int_0^1\frac{\ln x}{(ax+1)(x^2+1)}-\frac{x\ln x}{(x+a)(x^2+1)}\,dx\\&=\frac1{a^2+1}\int_0^1\frac{a^2\ln x}{ax+1}-\frac{2ax\ln x}{x^2+1}+\frac{a\ln x}{x+a}\,dx\\I'(a)&=\frac{\pi^2a}{24(a^2+1)}+\frac1{a^2+1}\int_0^1\frac{a^2\ln x}{ax+1}+\frac{a\ln x}{x+a}\,dx\end{align} 0 1 \begin{align}I&=\frac{\pi^2}{48}\ln 2+\int_0^1\ln x\int_0^1\frac{a^2}{(a^2+1)(ax+1)}+\frac a{(x+a)(a^2+1)}\,da\,dx\\&=\frac{\pi^2}{48}\ln2+\int_0^1\frac{\ln x}{x^2+1}\int_0^1\frac{2ax}{a^2+1}+\frac1{ax+1}-\frac x{a+x}\,da\,dx\\I&=\frac{\pi^2}{48}\ln2+\ln2\int_0^1\frac{x\ln x}{x^2+1}\,dx+\int_0^1\frac{\ln x\ln(1+x)}{x(x^2+1)}\,dx-\int_0^1\frac{x\ln x\ln(1+x)}{1+x^2}\,dx+\int_0^1\frac{x\ln^2x}{x^2+1}\,dx\end{align} \int_0^1\frac{\ln x\ln(1+x)}{x(1+x^2)}\,dx=-\frac34\zeta(3)-\int_0^1\frac{x\ln x\ln(1+x)}{1+x^2}\,dx I=-2\int_0^1\frac{x\ln x\ln(1+x)}{1+x^2}\,dx-\frac9{16}\zeta(3) I \text{Li}_2(z)+\text{Li}_2(z^{-1})=-\frac{\pi^2}6-\frac{\ln^2(-z)}2","['calculus', 'integration', 'multivariable-calculus']"
47,Deriving Green's theorem,Deriving Green's theorem,,"The reasoning leading to Green's theorem in my course makes a step I don't understand, with no justification. We have a function $P:R\to \mathbb{R}$ that has a partial derivative with respect to $y$ over $R$ . We're computing $\displaystyle \iint \limits _R\dfrac{\partial P}{\partial y}(x,y)\,dA$ . Say $R$ is regular with respect to the $x$ -axis. We can easily compute that it's equal to $\displaystyle \int \limits _a^bP(x,f_2(x))\,dx-\int \limits _a^bP(x,f_1(x))\,dx$ for $a\leq x\leq b$ . These are line integrals along the curves $y=f_1(x)$ and $y=f_2(x)$ . We'll call them respectively $C_1$ and $C_2$ . So our original double integral becomes $\displaystyle \int \limits _{C_2}P(x,y)\,dx-\int \limits _{C_1}P(x,y)\,dx$ . The next step is the one I don't understand: my course states that this is equal to $\displaystyle -\oint \limits _{C^+}P(x,y)\,dx$ . Why?","The reasoning leading to Green's theorem in my course makes a step I don't understand, with no justification. We have a function that has a partial derivative with respect to over . We're computing . Say is regular with respect to the -axis. We can easily compute that it's equal to for . These are line integrals along the curves and . We'll call them respectively and . So our original double integral becomes . The next step is the one I don't understand: my course states that this is equal to . Why?","P:R\to \mathbb{R} y R \displaystyle \iint \limits _R\dfrac{\partial P}{\partial y}(x,y)\,dA R x \displaystyle \int \limits _a^bP(x,f_2(x))\,dx-\int \limits _a^bP(x,f_1(x))\,dx a\leq x\leq b y=f_1(x) y=f_2(x) C_1 C_2 \displaystyle \int \limits _{C_2}P(x,y)\,dx-\int \limits _{C_1}P(x,y)\,dx \displaystyle -\oint \limits _{C^+}P(x,y)\,dx","['calculus', 'multivariable-calculus', 'line-integrals', 'greens-theorem']"
48,Computing $I=\iint_D\sqrt{\frac{1-x^2-y^2}{1+x^2+y^2}}dxdy$,Computing,I=\iint_D\sqrt{\frac{1-x^2-y^2}{1+x^2+y^2}}dxdy,"Compute $$I=\iint_D\sqrt{\frac{1-x^2-y^2}{1+x^2+y^2}}dxdy,$$ where $D=\{(x,y)\in\Bbb R^2\mid x^2+y^2\le 1, x\ge 0, y\ge 0\}.$ Source: Berman, task 3537 My attempt: I switched to polar coordinates $$\begin{cases}x=r\cos\varphi\\y=r\sin\varphi\end{cases}, 0\le\varphi\le \pi/2,0\le r\le 1$$ $$\begin{aligned}I&=\int_0^{\pi/2}\int_0^1\sqrt{\frac{1-r^2}{1+r^2}}rdrd\varphi\\&=\frac\pi2\int_0^1\sqrt{\frac{1-r^2}{1+r^2}}rdr\\&=\begin{bmatrix}t&=r^2\\dt&=2rdr\end{bmatrix}\\&=\frac\pi4\int_0^1\sqrt{\frac{1-t}{1+t}}dt\\&=\begin{bmatrix}s=\sqrt{\frac{1-t}{1+t}}\implies(1+t)s^2=1-t\implies s^2+ts^2=1-t\implies(1+s^2)t=1-s^2\implies t=\frac{1-s^2}{1+s^2}\\\implies dt=-\frac{4sds}{(1+s^2)^2}\end{bmatrix}\\&=-\frac\pi4\int_1^0\frac{4s}{(1+s^2)^2}ds\\&=\frac\pi2\int_0^1\frac{2sds}{{(1+s^2)}^2}\\&=\begin{bmatrix}v=1+s^2\\dv=2sds\end{bmatrix}\\&=\frac\pi2\int_1^2\frac{dv}{v^2}\\&=-\frac\pi2\frac1v\Big|_1^2\\&=\frac\pi4\end{aligned}$$ The solution in the book is $\frac{\pi(\pi-2)}8.$ What did I do wrong?","Compute where Source: Berman, task 3537 My attempt: I switched to polar coordinates The solution in the book is What did I do wrong?","I=\iint_D\sqrt{\frac{1-x^2-y^2}{1+x^2+y^2}}dxdy, D=\{(x,y)\in\Bbb R^2\mid x^2+y^2\le 1, x\ge 0, y\ge 0\}. \begin{cases}x=r\cos\varphi\\y=r\sin\varphi\end{cases}, 0\le\varphi\le \pi/2,0\le r\le 1 \begin{aligned}I&=\int_0^{\pi/2}\int_0^1\sqrt{\frac{1-r^2}{1+r^2}}rdrd\varphi\\&=\frac\pi2\int_0^1\sqrt{\frac{1-r^2}{1+r^2}}rdr\\&=\begin{bmatrix}t&=r^2\\dt&=2rdr\end{bmatrix}\\&=\frac\pi4\int_0^1\sqrt{\frac{1-t}{1+t}}dt\\&=\begin{bmatrix}s=\sqrt{\frac{1-t}{1+t}}\implies(1+t)s^2=1-t\implies s^2+ts^2=1-t\implies(1+s^2)t=1-s^2\implies t=\frac{1-s^2}{1+s^2}\\\implies dt=-\frac{4sds}{(1+s^2)^2}\end{bmatrix}\\&=-\frac\pi4\int_1^0\frac{4s}{(1+s^2)^2}ds\\&=\frac\pi2\int_0^1\frac{2sds}{{(1+s^2)}^2}\\&=\begin{bmatrix}v=1+s^2\\dv=2sds\end{bmatrix}\\&=\frac\pi2\int_1^2\frac{dv}{v^2}\\&=-\frac\pi2\frac1v\Big|_1^2\\&=\frac\pi4\end{aligned} \frac{\pi(\pi-2)}8.","['calculus', 'integration', 'multivariable-calculus', 'definite-integrals']"
49,Solid enclosed by the paraboloid $\frac{y^2}{b^2}+\frac{z^2}{c^2}=2\frac{x}a$ and the plane $x=a.$,Solid enclosed by the paraboloid  and the plane,\frac{y^2}{b^2}+\frac{z^2}{c^2}=2\frac{x}a x=a.,"Compute the volume of the solid enclosed by the paraboloid $$\frac{y^2}{b^2}+\frac{z^2}{c^2}=2\frac{x}a$$ and the plane $x=a.$ My attempt: I considered slices of the solid parallel with the plane $x=a$ and summed up their areas. $$\begin{aligned}y&=br\sqrt{\frac2a}\sin\varphi\\z&=cr\sqrt{\frac2a}\cos\varphi,\\\varphi&\in[0,2\pi],\\r&\in[0,\sqrt x],\\x&\in[0,a]\\\text{Jacobian  : } J_\psi&={2bc r}a\\\int_0^a\int_0^{2\pi}\int_0^{\sqrt x}\frac{2bcr}adrd\varphi dx&=\frac{2bc}a2\pi\int_0^a\frac{x}2dx\\&=\frac{2bc}a2\pi\frac12\int_0^axdx\\&=\frac{2bc}a\pi\frac{a^2}2\\&=abc\pi\end{aligned}$$ Is this right and if so, is there any other efficient method?","Compute the volume of the solid enclosed by the paraboloid and the plane My attempt: I considered slices of the solid parallel with the plane and summed up their areas. Is this right and if so, is there any other efficient method?","\frac{y^2}{b^2}+\frac{z^2}{c^2}=2\frac{x}a x=a. x=a \begin{aligned}y&=br\sqrt{\frac2a}\sin\varphi\\z&=cr\sqrt{\frac2a}\cos\varphi,\\\varphi&\in[0,2\pi],\\r&\in[0,\sqrt x],\\x&\in[0,a]\\\text{Jacobian 
: } J_\psi&={2bc r}a\\\int_0^a\int_0^{2\pi}\int_0^{\sqrt x}\frac{2bcr}adrd\varphi dx&=\frac{2bc}a2\pi\int_0^a\frac{x}2dx\\&=\frac{2bc}a2\pi\frac12\int_0^axdx\\&=\frac{2bc}a\pi\frac{a^2}2\\&=abc\pi\end{aligned}","['integration', 'multivariable-calculus', 'solution-verification']"
50,Alternative concepts for tangent spaces of smooth manifolds and derivatives of smooth maps,Alternative concepts for tangent spaces of smooth manifolds and derivatives of smooth maps,,"The derivative of a smooth  map $f : U \to V$ between open subsets $U \subset \mathbb R^m, V \subset \mathbb R^n$ at $p \in U$ is a linear map $df_p : \mathbb R^m \to \mathbb R^n$ which is charcterized by the well-known property $\lim_{h \to 0} \dfrac{\lVert (f(p +h) - (f(p) + df_p(h))\rVert}{\lVert h \rVert} = 0$ . This concept relies on the linear structures of domain and range of $f$ which are given locally around $p$ and $f(p)$ . For a smooth  map $f : M \to N$ from an $m$ -manifold $M$ to an $n$ -manifold $N$ this not work because we do not have (in general) canonical linear structures around $p$ and $f(p)$ . We can of course choose charts $\phi : U \to U'$ on $M$ around $p$ and $\psi : V \to V'$ on $N$ around $f(p)$ and get a ""localized derivative"" $$d_{(\phi,\psi)} f_p  = d(\psi \circ f \circ \phi^{-1})_{\phi(p)} .$$ But this depends on the choice of local Euclidean coordinates around $p$ and $f(p)$ so that $d_{(\phi,\psi)} f_p$ does not seem to be a good concept of a derivative of $f$ at $p$ . Indeed the derivative of $f$ at $p$ is usually understood as a linear map $df_p : T_pM \to T_{f(p)}N$ between tangent spaces. The tangent space $T_pM$ is defined either as the set of derivations $C^\infty(M) \to \mathbb R$ at $p$ or as the set of ""derivatives $u'(0)$ of curves $u : J \to M$ through $p$ "". Such $u$ is a smooth map defined on an open interval $J \subset \mathbb  R$ with $0 \in J$ and $u(0) = p$ and the derivative (whatever its definition may be) is taken at $t =0$ . The approach via derivations is very abstract; in my opinion the approach via curves is more intuitive and better for motivational purposes. However, it involves a ""preliminary"" concept of derivative using the following fact: Although we do not yet have an interpretation of $u'(0)$ we can at least say what it means that two curves $u,v$ have the same derivative at $0$ : This can be characterized by the requirement that $(\phi \circ u)'(0) = (\phi \circ v)'(0)$ for all charts $\phi : U \to U'$ around $p$ . Note that this equation holds for some chart, then it holds for all charts. Having the same derivative at $0$ is an equivalence relation for curves through $p$ and one defines $u'(0) = [u]$ = equivalence class of $u$ . In my eyes this is a strange interpretation of ""derivative"", but formally it does make sense. A similar phenomenon occurs in the context of the cotangent space $T^*_pM$ . One approach is to define it as the set of equivalence classes of maps $f \in C^\infty(M)$ with respect to the equivalence relation of having the same derivative at $p$ which is defined via chart $\phi : U \to U'$ around $p$ by considering $d(f \circ \psi^{-1})_{\psi^{-1}(p)}$ . Some authors write $df_p = [f]$ (e.g. Hitchin p. 17 ; see also Hitchin's definition of tangent space and tangent vectors ). My questions: The same thing could be done for smooth maps $f : M \to N$ by considering equivalence classes of smooth maps $M \to N$ with respect to the equivalence relation of having the same derivative at $p$ , the latter being defined via charts. That is, $df_p = [f]$ . Does this occur anywhere in the literature and does it have any use? Is there an alternative approach to define $T_pM$ (and also $T^*_pM$ ) not in terms of equivalence classes of curves, but in a more persuavive form? A vague idea would be that the set of derivatives of curves through a point $p$ of an open subset of $\mathbb R^m$ is nothing else than $\mathbb R^m$ itself. So why not take $T_pM = (\mathbb R^m,\phi)$ , where $\phi : U \to U'$ is a fixed chart around $p$ ? This would involve the axiom of choice to assign charts $\phi$ to points $p$ , but isn't it okay?","The derivative of a smooth  map between open subsets at is a linear map which is charcterized by the well-known property . This concept relies on the linear structures of domain and range of which are given locally around and . For a smooth  map from an -manifold to an -manifold this not work because we do not have (in general) canonical linear structures around and . We can of course choose charts on around and on around and get a ""localized derivative"" But this depends on the choice of local Euclidean coordinates around and so that does not seem to be a good concept of a derivative of at . Indeed the derivative of at is usually understood as a linear map between tangent spaces. The tangent space is defined either as the set of derivations at or as the set of ""derivatives of curves through "". Such is a smooth map defined on an open interval with and and the derivative (whatever its definition may be) is taken at . The approach via derivations is very abstract; in my opinion the approach via curves is more intuitive and better for motivational purposes. However, it involves a ""preliminary"" concept of derivative using the following fact: Although we do not yet have an interpretation of we can at least say what it means that two curves have the same derivative at : This can be characterized by the requirement that for all charts around . Note that this equation holds for some chart, then it holds for all charts. Having the same derivative at is an equivalence relation for curves through and one defines = equivalence class of . In my eyes this is a strange interpretation of ""derivative"", but formally it does make sense. A similar phenomenon occurs in the context of the cotangent space . One approach is to define it as the set of equivalence classes of maps with respect to the equivalence relation of having the same derivative at which is defined via chart around by considering . Some authors write (e.g. Hitchin p. 17 ; see also Hitchin's definition of tangent space and tangent vectors ). My questions: The same thing could be done for smooth maps by considering equivalence classes of smooth maps with respect to the equivalence relation of having the same derivative at , the latter being defined via charts. That is, . Does this occur anywhere in the literature and does it have any use? Is there an alternative approach to define (and also ) not in terms of equivalence classes of curves, but in a more persuavive form? A vague idea would be that the set of derivatives of curves through a point of an open subset of is nothing else than itself. So why not take , where is a fixed chart around ? This would involve the axiom of choice to assign charts to points , but isn't it okay?","f : U \to V U \subset \mathbb R^m, V \subset \mathbb R^n p \in U df_p : \mathbb R^m \to \mathbb R^n \lim_{h \to 0} \dfrac{\lVert (f(p +h) - (f(p) + df_p(h))\rVert}{\lVert h \rVert} = 0 f p f(p) f : M \to N m M n N p f(p) \phi : U \to U' M p \psi : V \to V' N f(p) d_{(\phi,\psi)} f_p  = d(\psi \circ f \circ \phi^{-1})_{\phi(p)} . p f(p) d_{(\phi,\psi)} f_p f p f p df_p : T_pM \to T_{f(p)}N T_pM C^\infty(M) \to \mathbb R p u'(0) u : J \to M p u J \subset \mathbb  R 0 \in J u(0) = p t =0 u'(0) u,v 0 (\phi \circ u)'(0) = (\phi \circ v)'(0) \phi : U \to U' p 0 p u'(0) = [u] u T^*_pM f \in C^\infty(M) p \phi : U \to U' p d(f \circ \psi^{-1})_{\psi^{-1}(p)} df_p = [f] f : M \to N M \to N p df_p = [f] T_pM T^*_pM p \mathbb R^m \mathbb R^m T_pM = (\mathbb R^m,\phi) \phi : U \to U' p \phi p","['multivariable-calculus', 'derivatives', 'differential-geometry', 'reference-request', 'tangent-spaces']"
51,Changing order of integration in triple integral,Changing order of integration in triple integral,,"Change the order of integration in the triple integral $$\int_0^1 \int_0^x \int_0^y f(x,y,z) \,dz ~dy~dx$$ to $dz~dx~dy$ Attempt: Here, since $z$ remains in the same place I think it is enough to consider only what happens in the $xy$ plane. So the bounds will become $0 \leq y \leq 1$ , $y \leq x \leq 1$ and $0 \leq z \leq y$ . However, my intuition tells me that this is not correct since if we change the order of $x$ and $y$ then this will affect $z$ as well. So is my intuition correct? If yes then how can I approach this problem?","Change the order of integration in the triple integral to Attempt: Here, since remains in the same place I think it is enough to consider only what happens in the plane. So the bounds will become , and . However, my intuition tells me that this is not correct since if we change the order of and then this will affect as well. So is my intuition correct? If yes then how can I approach this problem?","\int_0^1 \int_0^x \int_0^y f(x,y,z) \,dz ~dy~dx dz~dx~dy z xy 0 \leq y \leq 1 y \leq x \leq 1 0 \leq z \leq y x y z","['calculus', 'integration', 'multivariable-calculus']"
52,Conditional inequality $2a^3+b^3≥3$,Conditional inequality,2a^3+b^3≥3,"Non-negative $a$ and $b$ such that $a^5+a^5b^5=2$ . How then do I prove the following inequality $2a^3+b^3≥3$ ? So, we can try using the Lagrange multiplier method: Let $f(a, b)=2 a^{3}+b^{3}+\lambda(a^{5}+a^{5} b^{5}-2), \quad a, b \geq 0$ $$\tag1 \frac{\partial f}{\partial a}=6 a^{2}+\lambda(5 a^{4}+5 a^{4} b^{5})=0 \ldots$$ $$\tag2 \frac{\partial f}{\partial b}=3 b^{2}+\lambda(5 a^{5} b^{4})=0 \ldots$$ $$\left\{\begin{array}{c} 6+\lambda(5 a^{2}+5 a^{2} b^{5})=0 \\ 3+\lambda(5 a^{3} b^{4})=0 \end{array}\right. $$ $$ \lambda=-\frac{6}{5 a^{2}+5 a^{2} b^{5}}=-\frac{3}{5 a^{3} b^{4}} $$ Multiply by $a^3$ , $\,2 a^{6} b^{4}=a^{5}+a^{5} b^{5}=2$ . $$ a^{6} b^{4}=1,\, a^{3} b^{2}=1 \Rightarrow b=\frac{1}{a^{\frac{3}{2}}}\quad a, b \geq 0 $$ $$ \begin{gathered} a^{5}+a^{5} b^{5}=2 \Rightarrow a^{5}+\frac{a^{5}}{a^{\frac{15}{2}}}=2 \Rightarrow a^{5}+\frac{1}{a^{\frac{5}{2}}}=2 \Rightarrow a^{5}-2 a^{\frac{5}{2}}+1=0 \\ \Rightarrow\left(a^{\frac{5}{2}}-1\right)^{2}=0 \Rightarrow a=1 \end{gathered} $$ And the minimum is at $(a,b)=(1,1)$ . I'm not sure I solved this inequality correctly, I would like to see a more beautiful way.","Non-negative and such that . How then do I prove the following inequality ? So, we can try using the Lagrange multiplier method: Let Multiply by , . And the minimum is at . I'm not sure I solved this inequality correctly, I would like to see a more beautiful way.","a b a^5+a^5b^5=2 2a^3+b^3≥3 f(a, b)=2 a^{3}+b^{3}+\lambda(a^{5}+a^{5} b^{5}-2), \quad a, b \geq 0 \tag1 \frac{\partial f}{\partial a}=6 a^{2}+\lambda(5 a^{4}+5 a^{4} b^{5})=0 \ldots \tag2 \frac{\partial f}{\partial b}=3 b^{2}+\lambda(5 a^{5} b^{4})=0 \ldots \left\{\begin{array}{c} 6+\lambda(5 a^{2}+5 a^{2} b^{5})=0 \\ 3+\lambda(5 a^{3} b^{4})=0 \end{array}\right.   \lambda=-\frac{6}{5 a^{2}+5 a^{2} b^{5}}=-\frac{3}{5 a^{3} b^{4}}  a^3 \,2 a^{6} b^{4}=a^{5}+a^{5} b^{5}=2  a^{6} b^{4}=1,\, a^{3} b^{2}=1 \Rightarrow b=\frac{1}{a^{\frac{3}{2}}}\quad a, b \geq 0   \begin{gathered} a^{5}+a^{5} b^{5}=2 \Rightarrow a^{5}+\frac{a^{5}}{a^{\frac{15}{2}}}=2 \Rightarrow a^{5}+\frac{1}{a^{\frac{5}{2}}}=2 \Rightarrow a^{5}-2 a^{\frac{5}{2}}+1=0 \\ \Rightarrow\left(a^{\frac{5}{2}}-1\right)^{2}=0 \Rightarrow a=1 \end{gathered}  (a,b)=(1,1)","['multivariable-calculus', 'inequality']"
53,Finding the tangent hyperplane to a function at a given point,Finding the tangent hyperplane to a function at a given point,,"Given a function $f : \mathbb{R}^n \to \mathbb{R}$ and a point $p$ in the domain, how would you find the equation of the tangent hyperplane to the surface at point $p$ ? We know that if $h \to 0$ , then the Jacobian matrix $J_f(p)h = 0$ . Can we use this to find the equation of the hyperplane? Maybe something along the lines of $J_f(p) \cdot [x_i - p_i]^{\text{T}} = c$ ?","Given a function and a point in the domain, how would you find the equation of the tangent hyperplane to the surface at point ? We know that if , then the Jacobian matrix . Can we use this to find the equation of the hyperplane? Maybe something along the lines of ?",f : \mathbb{R}^n \to \mathbb{R} p p h \to 0 J_f(p)h = 0 J_f(p) \cdot [x_i - p_i]^{\text{T}} = c,"['multivariable-calculus', 'jacobian']"
54,Is it possible for the set of critical points of a differentiable function $f$ to be union of the diagonal lines defined by $y= \pm x$,Is it possible for the set of critical points of a differentiable function  to be union of the diagonal lines defined by,f y= \pm x,"Let $f: \mathbb{R}^2 ⟶ \mathbb{R}$ be a differentiable function. Let $S$ denote the set of critical points of $f$ . Is it possible for $S$ to be the union of $y = x$ and $y = -x$ ? I recently stumbled upon a textbook question asking if $S$ could be the union of the $x$ and $y$ axes, which led me to wonder about $y= \pm x$ . The way I thought about the question was that the partials with respect to $x$ and $y$ must be $0$ for this to occur. Hence, $$\frac{\partial f}{\partial x} = 0, \frac{\partial f}{\partial y} = 0$$ Moreover, these partials can only be $0$ when $y = x$ or $y = -x$ . This ""or"" is the part where I get confused - my original thought was for the partials to be $$\frac{\partial f}{\partial x} = x - y, \frac{\partial f}{\partial y} = x + y$$ But this leads to an intersection, not a union. Any guidance is greatly appreciated!","Let be a differentiable function. Let denote the set of critical points of . Is it possible for to be the union of and ? I recently stumbled upon a textbook question asking if could be the union of the and axes, which led me to wonder about . The way I thought about the question was that the partials with respect to and must be for this to occur. Hence, Moreover, these partials can only be when or . This ""or"" is the part where I get confused - my original thought was for the partials to be But this leads to an intersection, not a union. Any guidance is greatly appreciated!","f: \mathbb{R}^2 ⟶ \mathbb{R} S f S y = x y = -x S x y y= \pm x x y 0 \frac{\partial f}{\partial x} = 0, \frac{\partial f}{\partial y} = 0 0 y = x y = -x \frac{\partial f}{\partial x} = x - y, \frac{\partial f}{\partial y} = x + y","['real-analysis', 'calculus']"
55,Definition of directional derivative: Why does it work?,Definition of directional derivative: Why does it work?,,"The definition of the directional derivative in my textbook is $$ \nabla_{\vec{v}} f = \lim\limits_{h\to 0}\frac{f(\vec{x} + h \vec{v} )-f(\vec{x})}{h} $$ with $\vec{x} = (x_1, x_2)$ and $\vec{v} = (v_1, v_2)$ (I first want to consider the two variable case). However, before looking at this definition I tried to come up with it on my own: Say I'm given a function $f(x_1, x_2)$ and wanted to evaluate the directional derivative for a vector $\vec{v}$ for $x_1 = 0$ and $x_2 = 0$ . Similar to the one-variable case I would consider the slope of the secant $\Delta f$ $$ \Delta f = \frac{f(x_1 + v_1, x_2 + v_2) -f(x_1, x_2)}{\Vert \vec{v}\Vert} $$ so when introducing some real number $h > 0$ which will allow us to make the change in $(x_1, x_2)$ arbitrary small we can  consider the limit $$ \nabla_{\vec{v}} f = \lim\limits_{h\to 0}\frac{f(\vec{x} + h \vec{v} )-f(\vec{x})}{h\Vert\vec{v}\Vert} $$ whereby $\Vert\cdot\Vert$ denotes the standard $\mathbb{R}^2$ norm. Is the latter definition also correct? I doubt it, because if $$ \nabla_{\vec{v}} f = \lim\limits_{h\to 0}\frac{f(\vec{x} + h \vec{v} )-f(\vec{x})}{h} = L \neq  0  $$ then $$ \nabla_{\vec{v}} f = \lim\limits_{h\to 0}\frac{f(\vec{x} + h \vec{v} )-f(\vec{x})}{h\Vert\vec{v}\Vert} = \frac{1}{\Vert \vec{v}\Vert} \lim\limits_{h\to 0}\frac{f(\vec{x} + h \vec{v} )-f(\vec{x})}{h} = \frac{1}{\Vert \vec{v}\Vert} L \neq L. $$ I hope someone can clarify this for me. Edit: I know that the first definition is more general because  it doesn't require the existence of a  norm but I still don't see why it is correct. Also, in my attempt I don't assume that $\Vert\vec{v}\Vert = 1$ because I just think of it as the length of the vector. Why is this incorrect?","The definition of the directional derivative in my textbook is with and (I first want to consider the two variable case). However, before looking at this definition I tried to come up with it on my own: Say I'm given a function and wanted to evaluate the directional derivative for a vector for and . Similar to the one-variable case I would consider the slope of the secant so when introducing some real number which will allow us to make the change in arbitrary small we can  consider the limit whereby denotes the standard norm. Is the latter definition also correct? I doubt it, because if then I hope someone can clarify this for me. Edit: I know that the first definition is more general because  it doesn't require the existence of a  norm but I still don't see why it is correct. Also, in my attempt I don't assume that because I just think of it as the length of the vector. Why is this incorrect?","
\nabla_{\vec{v}} f = \lim\limits_{h\to 0}\frac{f(\vec{x} + h \vec{v} )-f(\vec{x})}{h}
 \vec{x} = (x_1, x_2) \vec{v} = (v_1, v_2) f(x_1, x_2) \vec{v} x_1 = 0 x_2 = 0 \Delta f 
\Delta f = \frac{f(x_1 + v_1, x_2 + v_2) -f(x_1, x_2)}{\Vert \vec{v}\Vert}
 h > 0 (x_1, x_2) 
\nabla_{\vec{v}} f = \lim\limits_{h\to 0}\frac{f(\vec{x} + h \vec{v} )-f(\vec{x})}{h\Vert\vec{v}\Vert}
 \Vert\cdot\Vert \mathbb{R}^2 
\nabla_{\vec{v}} f = \lim\limits_{h\to 0}\frac{f(\vec{x} + h \vec{v} )-f(\vec{x})}{h} = L \neq  0 
 
\nabla_{\vec{v}} f = \lim\limits_{h\to 0}\frac{f(\vec{x} + h \vec{v} )-f(\vec{x})}{h\Vert\vec{v}\Vert} = \frac{1}{\Vert \vec{v}\Vert} \lim\limits_{h\to 0}\frac{f(\vec{x} + h \vec{v} )-f(\vec{x})}{h} = \frac{1}{\Vert \vec{v}\Vert} L \neq L.
 \Vert\vec{v}\Vert = 1","['multivariable-calculus', 'derivatives', 'multivalued-functions']"
56,"What goes wrong in my computation of $\iiint _S 1 dz dy dx$ where $S=\{(x,y,z)\mid ax+by+cz=0, 1\leq x,y,z \leq N\}$?",What goes wrong in my computation of  where ?,"\iiint _S 1 dz dy dx S=\{(x,y,z)\mid ax+by+cz=0, 1\leq x,y,z \leq N\}","I am trying to calculate this triple integral but I don't really know if it is correct. I don't have much experience with triple integrals. Here it is: $\iiint _S 1 dz dy dx$ where $S$ is the set $\{(x,y,z)\mid ax+by+cz=0, 1\leq x,y,z \leq N\}$ . Here $a$ , $b$ and $c$ are fixed non-zero reals, $N$ is a fixed positive real. It seems that $z$ should run from $1$ to $-(ax+by)/c$ and we can let $x$ and $y$ run from $1$ to $N$ , my reasoning being that if $x$ and $y$ are fixed, $z$ gets automatically fixed. Then our integral transforms into $$\iiint _S 1 dz dy dx=\int_1^N \int_1^N \int_1^{-(ax+by)/c} 1 dz dy dx=\int_1^N \int_1^N -(ax+by+c)/c  \mathrm{d}x dy$$ which is equal to $$\int_1^N-(ax^2/2+bxy+cx)|^{N}_1 dy=\int_1^N \left(-(aN^2/2+bNy+cN)+a/2+by+c\right)dy=-aN^3/2-bN^3/2-cN^2+aN/2+bN^2/2+cN+aN^2+bN/2+cN-a/2-b/2-c.$$ I feel like i am making an error somewhere, but I am not sure. Thanks!","I am trying to calculate this triple integral but I don't really know if it is correct. I don't have much experience with triple integrals. Here it is: where is the set . Here , and are fixed non-zero reals, is a fixed positive real. It seems that should run from to and we can let and run from to , my reasoning being that if and are fixed, gets automatically fixed. Then our integral transforms into which is equal to I feel like i am making an error somewhere, but I am not sure. Thanks!","\iiint _S 1 dz dy dx S \{(x,y,z)\mid ax+by+cz=0, 1\leq x,y,z \leq N\} a b c N z 1 -(ax+by)/c x y 1 N x y z \iiint _S 1 dz dy dx=\int_1^N \int_1^N \int_1^{-(ax+by)/c} 1 dz dy dx=\int_1^N \int_1^N -(ax+by+c)/c  \mathrm{d}x dy \int_1^N-(ax^2/2+bxy+cx)|^{N}_1 dy=\int_1^N \left(-(aN^2/2+bNy+cN)+a/2+by+c\right)dy=-aN^3/2-bN^3/2-cN^2+aN/2+bN^2/2+cN+aN^2+bN/2+cN-a/2-b/2-c.","['calculus', 'multivariable-calculus', 'solution-verification']"
57,"Laplace equation in polar coordinates, using matrices","Laplace equation in polar coordinates, using matrices",,"I saw that the method shown below could be used to derive the Laplace equation for polar coordinates using less calculations. \begin{aligned} &\nabla^{2} u=\frac{\partial^{2} u}{\partial r^{2}}+\frac{1}{r} \frac{\partial u}{\partial r}+\frac{1}{r^{2}} \frac{\partial^{2} u}{\partial \theta^{2}}\\ &\left(\begin{array}{c} \frac{\partial}{\partial r} \\ \frac{\partial}{\partial \theta} \end{array}\right)=\left(\begin{array}{ll} \frac{\partial x}{\partial r} & \frac{\partial y}{\partial r} \\ \frac{\partial x}{\partial \theta} & \frac{\partial x}{\partial \theta} \end{array}\right)\left(\begin{array}{c} \frac{\partial}{\partial x} \\ \frac{\partial}{\partial y} \end{array}\right) \quad \longrightarrow \quad\left(\begin{array}{c} \frac{\partial}{\partial x} \\ \frac{\partial}{\partial y} \end{array}\right)=\left(\begin{array}{ll} \frac{\partial x}{\partial r} & \frac{\partial y}{\partial r} \\ \frac{\partial x}{\partial \theta} & \frac{\partial x}{\partial \theta} \end{array}\right)^{-1}\left(\begin{array}{c} \frac{\partial}{\partial r} \\ \frac{\partial}{\partial \theta} \end{array}\right)\\ &\nabla^{2}=\left(\begin{array}{c} \frac{\partial}{\partial x} \\ \frac{\partial}{\partial y} \end{array}\right) \cdot\left(\begin{array}{c} \frac{\partial}{\partial x} \\ \frac{\partial}{\partial y} \end{array}\right) \end{aligned} Even after computing the following matrix: \begin{pmatrix}\frac{\partial x}{\partial r} & \frac{\partial y}{\partial r} \\ \frac{\partial x}{\partial \theta} & \frac{\partial x}{\partial \theta} \end{pmatrix} (using $x=r\cos\theta, y=r\sin\theta$ ), I still do not know how the Laplacian $\nabla^{2} u=\frac{\partial^{2} u}{\partial r^{2}}+\frac{1}{r} \frac{\partial u}{\partial r}+\frac{1}{r^{2}} \frac{\partial^{2} u}{\partial \theta^{2}}$ is gotten. What especially confuses me is the dot product and how the $2\times1$ operator gets applied to a $2 \times2$ matrix.","I saw that the method shown below could be used to derive the Laplace equation for polar coordinates using less calculations. Even after computing the following matrix: (using ), I still do not know how the Laplacian is gotten. What especially confuses me is the dot product and how the operator gets applied to a matrix.","\begin{aligned}
&\nabla^{2} u=\frac{\partial^{2} u}{\partial r^{2}}+\frac{1}{r} \frac{\partial u}{\partial r}+\frac{1}{r^{2}} \frac{\partial^{2} u}{\partial \theta^{2}}\\
&\left(\begin{array}{c}
\frac{\partial}{\partial r} \\
\frac{\partial}{\partial \theta}
\end{array}\right)=\left(\begin{array}{ll}
\frac{\partial x}{\partial r} & \frac{\partial y}{\partial r} \\
\frac{\partial x}{\partial \theta} & \frac{\partial x}{\partial \theta}
\end{array}\right)\left(\begin{array}{c}
\frac{\partial}{\partial x} \\
\frac{\partial}{\partial y}
\end{array}\right) \quad \longrightarrow \quad\left(\begin{array}{c}
\frac{\partial}{\partial x} \\
\frac{\partial}{\partial y}
\end{array}\right)=\left(\begin{array}{ll}
\frac{\partial x}{\partial r} & \frac{\partial y}{\partial r} \\
\frac{\partial x}{\partial \theta} & \frac{\partial x}{\partial \theta}
\end{array}\right)^{-1}\left(\begin{array}{c}
\frac{\partial}{\partial r} \\
\frac{\partial}{\partial \theta}
\end{array}\right)\\
&\nabla^{2}=\left(\begin{array}{c}
\frac{\partial}{\partial x} \\
\frac{\partial}{\partial y}
\end{array}\right) \cdot\left(\begin{array}{c}
\frac{\partial}{\partial x} \\
\frac{\partial}{\partial y}
\end{array}\right)
\end{aligned} \begin{pmatrix}\frac{\partial x}{\partial r} & \frac{\partial y}{\partial r} \\
\frac{\partial x}{\partial \theta} & \frac{\partial x}{\partial \theta}
\end{pmatrix} x=r\cos\theta, y=r\sin\theta \nabla^{2} u=\frac{\partial^{2} u}{\partial r^{2}}+\frac{1}{r} \frac{\partial u}{\partial r}+\frac{1}{r^{2}} \frac{\partial^{2} u}{\partial \theta^{2}} 2\times1 2 \times2","['linear-algebra', 'multivariable-calculus', 'partial-differential-equations', 'partial-derivative', 'laplacian']"
58,Advantages of Each Coordinate System,Advantages of Each Coordinate System,,"I am currently learning about the spherical coordinate system in class, but I do not know its advantages or even if it is advantageous in using this coordinate system over another. I am very comfortable in using the rectangular coordinate system and the cylindrical coordinate system (polar coordinate system but just in 3D), as the rectangular coordinate system is just the cartesian coordinate system with another dimension and the cylindrical coordinate system is just the polar coordinate system with an additional dimension. However, the concept of spherical coordinates come out of nowhere (that I know of) and I am unable to see its advantages. For example, if wanting to calculate an integral in the first octant, you can just restrict to $x>0$ , $y>0$ , and $z>0$ for the rectangular coordinate system. And for cylindrical coordinates, you can restrict $z>0$ , $0<\theta<\frac{\pi}{2}$ , and $r$ its corresponding boundary conditions. My question is are there ever cases when using spherical coordinates are more intuitive than using cylindrical or rectangular coordinates?","I am currently learning about the spherical coordinate system in class, but I do not know its advantages or even if it is advantageous in using this coordinate system over another. I am very comfortable in using the rectangular coordinate system and the cylindrical coordinate system (polar coordinate system but just in 3D), as the rectangular coordinate system is just the cartesian coordinate system with another dimension and the cylindrical coordinate system is just the polar coordinate system with an additional dimension. However, the concept of spherical coordinates come out of nowhere (that I know of) and I am unable to see its advantages. For example, if wanting to calculate an integral in the first octant, you can just restrict to , , and for the rectangular coordinate system. And for cylindrical coordinates, you can restrict , , and its corresponding boundary conditions. My question is are there ever cases when using spherical coordinates are more intuitive than using cylindrical or rectangular coordinates?",x>0 y>0 z>0 z>0 0<\theta<\frac{\pi}{2} r,"['multivariable-calculus', 'coordinate-systems', 'spherical-coordinates', 'cylindrical-coordinates']"
59,Intuitive meaning of Diffeomorphism,Intuitive meaning of Diffeomorphism,,"Let $U\subset\mathbb{R}^n$ , $V\subset\mathbb{R}^m$ and a bijection $f:U\to V$ is a diffeomorphism if $f$ and $f^{-1}$ are differentiable. I would like to know the intuitive meaning of two open sets being diffeomorphic. For example , if two spaces are homeomorphic , these spaces share the same topological properties. And we have a clear intuitive idea of two spaces being homeomorphs, like the classic relationship between a donut and a mug. Is there a similar intuitive idea for diffeomorphism? Edit: The proprieties that homeomorphism preserves is, for example, if one of them is compact, then the other is as well; if one of them is connected, then the other is as well; if one of them is Hausdorff, then the other is as well; their homotopy and homology groups will coincide. So, what are the properties preserved by diffeomorphism? I quoted homeomorphism, to indicate what I meant by an intuitive idea.","Let , and a bijection is a diffeomorphism if and are differentiable. I would like to know the intuitive meaning of two open sets being diffeomorphic. For example , if two spaces are homeomorphic , these spaces share the same topological properties. And we have a clear intuitive idea of two spaces being homeomorphs, like the classic relationship between a donut and a mug. Is there a similar intuitive idea for diffeomorphism? Edit: The proprieties that homeomorphism preserves is, for example, if one of them is compact, then the other is as well; if one of them is connected, then the other is as well; if one of them is Hausdorff, then the other is as well; their homotopy and homology groups will coincide. So, what are the properties preserved by diffeomorphism? I quoted homeomorphism, to indicate what I meant by an intuitive idea.",U\subset\mathbb{R}^n V\subset\mathbb{R}^m f:U\to V f f^{-1},"['multivariable-calculus', 'differential-geometry', 'intuition', 'diffeomorphism']"
60,Volume Enclosed Between a Surface and a Plane,Volume Enclosed Between a Surface and a Plane,,"This is a problem that I came up with that has been driving me crazy. I have not been able to find a solution nor a person who could direct me to one. The problem is as follows; Find the volume of the region enclosed by the surfaces on [ $-\pi..\pi, -\pi..\pi]$ (These are meant to describe the x and y value ranges, excuse my lack of knowledge on formal math notation) $$F(x,y)=cos(x)+cos(y)$$ $$G(x,y)=1/2$$ I need to specific, however, that I do not want to find the net volume of this region as 1-These are trigonometric functions with an infinite domain and therefore never reach a truly enclosed area and 2-Selecting a square region as I have will result in an answer of 0. The volume I want to find in specific is the one enclosed by the plane G(x,y) and F(x,y) where F(x,y) lies above the plane. Below are some pictures to illustrate this. This problem is a bit above my current math knowledge, but I am really itching for a solution/walkthrough on the problem. The biggest obstacle for me is determining how to describe the boundary of the region I need to integrate over. Any help would be much appreciated and I can answer any questions as I know my explanation isn't perfectly clear/formalized.","This is a problem that I came up with that has been driving me crazy. I have not been able to find a solution nor a person who could direct me to one. The problem is as follows; Find the volume of the region enclosed by the surfaces on [ (These are meant to describe the x and y value ranges, excuse my lack of knowledge on formal math notation) I need to specific, however, that I do not want to find the net volume of this region as 1-These are trigonometric functions with an infinite domain and therefore never reach a truly enclosed area and 2-Selecting a square region as I have will result in an answer of 0. The volume I want to find in specific is the one enclosed by the plane G(x,y) and F(x,y) where F(x,y) lies above the plane. Below are some pictures to illustrate this. This problem is a bit above my current math knowledge, but I am really itching for a solution/walkthrough on the problem. The biggest obstacle for me is determining how to describe the boundary of the region I need to integrate over. Any help would be much appreciated and I can answer any questions as I know my explanation isn't perfectly clear/formalized.","-\pi..\pi, -\pi..\pi] F(x,y)=cos(x)+cos(y) G(x,y)=1/2","['calculus', 'integration', 'multivariable-calculus', 'volume']"
61,Find minimal value of $\left(2-x\right)\left(2-y\right)\left(2-z\right)$,Find minimal value of,\left(2-x\right)\left(2-y\right)\left(2-z\right),"Let $x,y,z>0$ such that $x^2+y^2+z^2=3$ . Find minimal value of $$\left(2-x\right)\left(2-y\right)\left(2-z\right)$$ I thought the equality occurs at $x = y = z = 1$ (then it is easy), but the fact is $x = y = \frac{1}{3}; z = \frac{5}{3}$ . So I just thought of using $uvw$ , but I am not allowed to use it during my exam. Because of the equality I cannot use AMGM, Cauchy-Schwarz, etc. I tried to use Mixing-Variables, but I failed. Please help.","Let such that . Find minimal value of I thought the equality occurs at (then it is easy), but the fact is . So I just thought of using , but I am not allowed to use it during my exam. Because of the equality I cannot use AMGM, Cauchy-Schwarz, etc. I tried to use Mixing-Variables, but I failed. Please help.","x,y,z>0 x^2+y^2+z^2=3 \left(2-x\right)\left(2-y\right)\left(2-z\right) x = y = z = 1 x = y = \frac{1}{3}; z = \frac{5}{3} uvw","['multivariable-calculus', 'inequality', 'optimization', 'roots', 'symmetric-polynomials']"
62,Multivariable implicit function theorem proof,Multivariable implicit function theorem proof,,"I am trying to understand the proof of the implicit function theorem for multivariable functions. If I have a function $F(x,y,z) = 0$ with the assumption that $z = f(x,y)$ and we want to find $\frac{\partial z}{\partial x}$ , then taking the derivative with respect to x (partial derivative must be used because F is a multivariable function) on both sides gives: $$\frac{\partial F}{\partial x} = 0 \tag{1}\label{1}$$ The left side can be evaluated using the chain rule: $$\frac{\partial F}{\partial x} + \frac{\partial F}{\partial z} \frac{\partial z}{\partial x} = 0 \tag{2}\label{2}$$ The thing that I am having trouble understanding is that the term $\frac{\partial F}{\partial x}$ appears in both equations (1) and (2), so if I write something like $$\frac{\partial F}{\partial x} = \frac{\partial F}{\partial x} + \frac{\partial F}{\partial z} \frac{\partial z}{\partial x} = 0 \tag{3}\label{3}$$ Subtracting $\frac{\partial F}{\partial x}$ from each side gives: $$\frac{\partial F}{\partial z} \frac{\partial z}{\partial x} = 0$$ which is incorrect. Which step am I messing up?","I am trying to understand the proof of the implicit function theorem for multivariable functions. If I have a function with the assumption that and we want to find , then taking the derivative with respect to x (partial derivative must be used because F is a multivariable function) on both sides gives: The left side can be evaluated using the chain rule: The thing that I am having trouble understanding is that the term appears in both equations (1) and (2), so if I write something like Subtracting from each side gives: which is incorrect. Which step am I messing up?","F(x,y,z) = 0 z = f(x,y) \frac{\partial z}{\partial x} \frac{\partial F}{\partial x} = 0 \tag{1}\label{1} \frac{\partial F}{\partial x} + \frac{\partial F}{\partial z} \frac{\partial z}{\partial x} = 0 \tag{2}\label{2} \frac{\partial F}{\partial x} \frac{\partial F}{\partial x} = \frac{\partial F}{\partial x} + \frac{\partial F}{\partial z} \frac{\partial z}{\partial x} = 0 \tag{3}\label{3} \frac{\partial F}{\partial x} \frac{\partial F}{\partial z} \frac{\partial z}{\partial x} = 0","['calculus', 'multivariable-calculus', 'implicit-function-theorem']"
63,Limits of integration on double integrals,Limits of integration on double integrals,,"I was given this problem: Find an integral equal to the volume of the solid bounded by $z=4-2y,z=0,x=y^4,x=1$ and evaluate. I understand how to evaluate once my double integral is set up, but I do not know how to find my limits of integration. I am assuming that my function will be $z=4-2y$ and that using this I should be able to find my limits of integration. I can say that $0=4-2y$ which means that $y=2$ . I can then plug that into $x=y^4$ and get $1\leq x\leq 16$ which may be correct, but I still am missing the limits of integration for y. Am I thinking about this problem correctly? How can I go about solving this?","I was given this problem: Find an integral equal to the volume of the solid bounded by and evaluate. I understand how to evaluate once my double integral is set up, but I do not know how to find my limits of integration. I am assuming that my function will be and that using this I should be able to find my limits of integration. I can say that which means that . I can then plug that into and get which may be correct, but I still am missing the limits of integration for y. Am I thinking about this problem correctly? How can I go about solving this?","z=4-2y,z=0,x=y^4,x=1 z=4-2y 0=4-2y y=2 x=y^4 1\leq x\leq 16","['integration', 'multivariable-calculus', 'definite-integrals', 'multiple-integral']"
64,How do I begin with this integral? [Polar coordinates & line integrals],How do I begin with this integral? [Polar coordinates & line integrals],,"Calculate the line integral of the scalar field over the curve L: $$ \int_L(x+y)\,ds $$ with $L$ the right loop of $r^2=2\cos(2\theta)$ I've been going at it for over 3 hours now. With several people in a discord for math that were trying to help me but we can't seem to get the solution that should be 2sqrt(2). Can someone help me get to the solution? :) I tried the following: x = r.cos(θ) and y = r.sin(θ) and my $ds= \sqrt{r²\:+\:r'²}d\theta   =\sqrt{\frac{2}{cos(2\theta)}} .d\theta$ from there I got my boundaries as θ = $\frac{\pi }{4}$ -> r = 0 and as θ = 0 -> r = $\sqrt{2}$ from here I got that the integral should be the following: $\int _Lr\left(cos\theta +sin\theta \right)\:\cdot \sqrt{\frac{2}{cos\left(2\theta \right)}}\:.d\theta$ Now the issue is that I don't know what I have to fill in for the boudnaries of my integral. And even if this is correct what I'm doing.",Calculate the line integral of the scalar field over the curve L: with the right loop of I've been going at it for over 3 hours now. With several people in a discord for math that were trying to help me but we can't seem to get the solution that should be 2sqrt(2). Can someone help me get to the solution? :) I tried the following: x = r.cos(θ) and y = r.sin(θ) and my from there I got my boundaries as θ = -> r = 0 and as θ = 0 -> r = from here I got that the integral should be the following: Now the issue is that I don't know what I have to fill in for the boudnaries of my integral. And even if this is correct what I'm doing.,"
\int_L(x+y)\,ds
 L r^2=2\cos(2\theta) ds= \sqrt{r²\:+\:r'²}d\theta 
 =\sqrt{\frac{2}{cos(2\theta)}} .d\theta \frac{\pi }{4} \sqrt{2} \int _Lr\left(cos\theta +sin\theta \right)\:\cdot \sqrt{\frac{2}{cos\left(2\theta \right)}}\:.d\theta","['calculus', 'multivariable-calculus', 'vector-analysis']"
65,Partial derivative of integral of multi variable function,Partial derivative of integral of multi variable function,,"This is the problem I am working on. Here, I find it hard to calculate $\nabla{\times}\mathbf{H}$ . For example, to calculate $(\frac{\partial H_z}{\partial y}-\frac{\partial H_y}{\partial z})\mathbf{i}$ , I am not sure how to continue after $$\frac{\partial H_y}{\partial z}=\frac{\partial} {\partial z}\left({\int_{x_0}^x \! G_z(x’, y, z) \, \mathrm{d}x’}\right)$$ . I think the core problem here is that $x$ is taken as variable in the integral while $z$ was taken as variable in the partial derivative. What should I do to solve it? Thanks.","This is the problem I am working on. Here, I find it hard to calculate . For example, to calculate , I am not sure how to continue after . I think the core problem here is that is taken as variable in the integral while was taken as variable in the partial derivative. What should I do to solve it? Thanks.","\nabla{\times}\mathbf{H} (\frac{\partial H_z}{\partial y}-\frac{\partial H_y}{\partial z})\mathbf{i} \frac{\partial H_y}{\partial z}=\frac{\partial} {\partial z}\left({\int_{x_0}^x \! G_z(x’, y, z) \, \mathrm{d}x’}\right) x z","['multivariable-calculus', 'partial-derivative']"
66,Integrating using surface and volume elements,Integrating using surface and volume elements,,"As a Physics Degree undergraduate, I have been forced countless times to use a certain method to integrate over 3D surfaces and volumes, which my lecturers like to call integration through surface and volume elements . I can't stand this method, nor I could ever understand how to do it, especially because it is not mathematically rigorous. However, at most cases, I'm not able to avoid it. For example - given the electric field of a ring of radius $r$ lying on $z=0$ at the point $(0,0,z_0)$ , I am required to find the electric field of a disk of radius $R$ lying on $z=0$ at the point $(0,0,z_0)$ . In order to do that, I have to use the aforesaid method - integrate the electric field I'm given with respect to the length element $dr$ , from $r=0$ to $r=R$ . Of course, doing that would also require to translate the charge density of a length element, to a charge density of a surface element, assuming they're both unifrom. But - Physics is not my problem here - but the math. And that's why I came here. I tried to understand using this method, but it sometimes works - and sometimes doesn't. I would be glad to know where I'm right, and where I'm wrong. $(\star)$ Important : The angle $\theta$ in Examples 1,2 is the angle of the polar coordinates . In Examples 3,4, it is the polar angle of the spherical coordinates (meaning it is not the azimuthal one). $(\star)$ I will denote $\color{green}{Good}$ in green and $\color{red}{Bad}$ in red. Lowercase will be integration variables, and uppercase would be given parameters. Example 1: Calculating the area of an empty cylinder of radius R and height H A. With respect to $dz$ Given a perimeter of a ring $2\pi R$ , the area of a ring with an infinitesimal height $dz$ would be given by $2\pi Rz$ . And then: $$S=\int\limits_{0}^{H}2\pi Rz\ dz=\color{green}{2\pi RH}$$ A correct answer, gladly. B. With respect to $d\theta$ We know that if we sliced the cylinder vertically, rotating with the angle $\theta$ , we would get lines of height $H$ each, multiplied by an infinitesimal width $Rd\theta$ . Thus, the surface element would be given by $HRd\theta$ . And then: $$S=\int\limits_{0}^{2\pi}HR\ d\theta=\color{green}{2\pi RH}$$ Again - a good answer. But: this is where things are going to get ugly. Example 2: Calculating the volume of a cylinder of radius R and height H A. With respect to $dr$ We would want to sum cylinders with infinitesimal widths $dr$ , thus the volume element would be given by $2\pi H rdr$ (the perimeter of a ring of radius $r$ multiplied by the infinitesimal width $dr$ and height $H$ ). And then: $$V=\int\limits_{0}^{R}2\pi Hr\ dr=\color{green}{\pi HR^2}$$ This is of course correct, but: B. With respect to $d\theta$ We would want to sum the exact same slices we described at B. of Example 1 , but now they would also have a width of $R$ . Meaning: the volume element would be given by $HR^2d\theta$ (since every rectangle is of dimensions $H \times R$ , and we multiply each by an infinitesimal width $Rd\theta$ ). And now: $$V=\int\limits_{0}^{2\pi}HR^2\ d\theta=\color{red}{2\pi HR^2}$$ This is bad. I would show  you now 2 more examples - in the case of a sphere and a ball. It doesn't work there either. Example 3: Calculating the area of a sphere of radius R With respect to $d\theta$ Given a ring of radius $r$ , it can be easily checked to see that, geometrically, $r$ would be given by $R\sin\theta$ . The infinitesimal width of such disk, would be now $Rd\theta$ , thus the surface element would be given by $2\pi R^2\sin\theta d\theta$ . Therefore: $$V=\int\limits_{0}^{\pi}2\pi R^2\sin\theta \ d\theta=\color{green}{4\pi R^2}$$ Getting optimistic, let's try to calculate the volume of the ball. Example 4: Calculating the volume of a ball of radius R A. With respect to $dr$ We would want to sum spheres, of radius $r$ and infinitesimal width $dr$ each. Thus, the volume element would be given by $4\pi r^2 dr$ , and then: $$V=\int\limits_{0}^{R}4\pi r^2 \ dr = \color{green}{\frac{4}{3}\pi R^3}$$ But unfortunately: B. With respect to $d\theta$ Going again like B. of Example 3 , we would want to sum the exact same rings, but now they would be disks with the infinitesimal width $Rd\theta$ . The volume element would be given by $\pi (R\sin\theta)^2 Rd\theta$ , which leads us to: $$V=\int\limits_{0}^{\pi}\pi R^3 \sin^2\theta\ d\theta=\color{red}{\frac{1}{2}\pi^2 R^3}$$ I tried using the other elements too: $d\varphi$ , for example. the azimuthal angle, which is much more complicated, and also tried other shapes like a cone and even paraboloid. But it just won't work right. It works sometimes - and that's not enough for me, unfortunately. I put many efforts to this post, in order to show you my way of thinking, because that's how I had been taught to do this. But maybe it is not right (it feels like it, for sure). Thank you very much for reading all this, and I would be very glad to hear your thoughts. P.S. : I wish I could add pictures, but I don't know any programs that I can use to draw them.","As a Physics Degree undergraduate, I have been forced countless times to use a certain method to integrate over 3D surfaces and volumes, which my lecturers like to call integration through surface and volume elements . I can't stand this method, nor I could ever understand how to do it, especially because it is not mathematically rigorous. However, at most cases, I'm not able to avoid it. For example - given the electric field of a ring of radius lying on at the point , I am required to find the electric field of a disk of radius lying on at the point . In order to do that, I have to use the aforesaid method - integrate the electric field I'm given with respect to the length element , from to . Of course, doing that would also require to translate the charge density of a length element, to a charge density of a surface element, assuming they're both unifrom. But - Physics is not my problem here - but the math. And that's why I came here. I tried to understand using this method, but it sometimes works - and sometimes doesn't. I would be glad to know where I'm right, and where I'm wrong. Important : The angle in Examples 1,2 is the angle of the polar coordinates . In Examples 3,4, it is the polar angle of the spherical coordinates (meaning it is not the azimuthal one). I will denote in green and in red. Lowercase will be integration variables, and uppercase would be given parameters. Example 1: Calculating the area of an empty cylinder of radius R and height H A. With respect to Given a perimeter of a ring , the area of a ring with an infinitesimal height would be given by . And then: A correct answer, gladly. B. With respect to We know that if we sliced the cylinder vertically, rotating with the angle , we would get lines of height each, multiplied by an infinitesimal width . Thus, the surface element would be given by . And then: Again - a good answer. But: this is where things are going to get ugly. Example 2: Calculating the volume of a cylinder of radius R and height H A. With respect to We would want to sum cylinders with infinitesimal widths , thus the volume element would be given by (the perimeter of a ring of radius multiplied by the infinitesimal width and height ). And then: This is of course correct, but: B. With respect to We would want to sum the exact same slices we described at B. of Example 1 , but now they would also have a width of . Meaning: the volume element would be given by (since every rectangle is of dimensions , and we multiply each by an infinitesimal width ). And now: This is bad. I would show  you now 2 more examples - in the case of a sphere and a ball. It doesn't work there either. Example 3: Calculating the area of a sphere of radius R With respect to Given a ring of radius , it can be easily checked to see that, geometrically, would be given by . The infinitesimal width of such disk, would be now , thus the surface element would be given by . Therefore: Getting optimistic, let's try to calculate the volume of the ball. Example 4: Calculating the volume of a ball of radius R A. With respect to We would want to sum spheres, of radius and infinitesimal width each. Thus, the volume element would be given by , and then: But unfortunately: B. With respect to Going again like B. of Example 3 , we would want to sum the exact same rings, but now they would be disks with the infinitesimal width . The volume element would be given by , which leads us to: I tried using the other elements too: , for example. the azimuthal angle, which is much more complicated, and also tried other shapes like a cone and even paraboloid. But it just won't work right. It works sometimes - and that's not enough for me, unfortunately. I put many efforts to this post, in order to show you my way of thinking, because that's how I had been taught to do this. But maybe it is not right (it feels like it, for sure). Thank you very much for reading all this, and I would be very glad to hear your thoughts. P.S. : I wish I could add pictures, but I don't know any programs that I can use to draw them.","r z=0 (0,0,z_0) R z=0 (0,0,z_0) dr r=0 r=R (\star) \theta (\star) \color{green}{Good} \color{red}{Bad} dz 2\pi R dz 2\pi Rz S=\int\limits_{0}^{H}2\pi Rz\ dz=\color{green}{2\pi RH} d\theta \theta H Rd\theta HRd\theta S=\int\limits_{0}^{2\pi}HR\ d\theta=\color{green}{2\pi RH} dr dr 2\pi H rdr r dr H V=\int\limits_{0}^{R}2\pi Hr\ dr=\color{green}{\pi HR^2} d\theta R HR^2d\theta H \times R Rd\theta V=\int\limits_{0}^{2\pi}HR^2\ d\theta=\color{red}{2\pi HR^2} d\theta r r R\sin\theta Rd\theta 2\pi R^2\sin\theta d\theta V=\int\limits_{0}^{\pi}2\pi R^2\sin\theta \ d\theta=\color{green}{4\pi R^2} dr r dr 4\pi r^2 dr V=\int\limits_{0}^{R}4\pi r^2 \ dr = \color{green}{\frac{4}{3}\pi R^3} d\theta Rd\theta \pi (R\sin\theta)^2 Rd\theta V=\int\limits_{0}^{\pi}\pi R^3 \sin^2\theta\ d\theta=\color{red}{\frac{1}{2}\pi^2 R^3} d\varphi","['integration', 'multivariable-calculus', 'physics']"
67,How should Matrix Calculus be thought of with respect to Vector and Multi-variable Calculus?,How should Matrix Calculus be thought of with respect to Vector and Multi-variable Calculus?,,"My question boils down to identifying the ways in which Matrix Calculus is distinct from Vector and Multi-variable Calculus, and how they overlap. Is Matrix Calculus simply a notation on top of these other types of Calculus, or does it actually contain new theorems/lemmas/results separate from the other two? The subject of Matrix Calculus does not seem to be a subject of courses at universities and I struggle to find resources to actually learn about it.","My question boils down to identifying the ways in which Matrix Calculus is distinct from Vector and Multi-variable Calculus, and how they overlap. Is Matrix Calculus simply a notation on top of these other types of Calculus, or does it actually contain new theorems/lemmas/results separate from the other two? The subject of Matrix Calculus does not seem to be a subject of courses at universities and I struggle to find resources to actually learn about it.",,"['calculus', 'linear-algebra', 'multivariable-calculus']"
68,"Transform coordinate system for the gradient of a function at a specific x, y, z value.","Transform coordinate system for the gradient of a function at a specific x, y, z value.",,"I have the gradient of a function $f(x,y,z)$ at a specific value of $x, y,$ and $z$ in the vector form: $$ \nabla f(x,y,z)\Bigr|_{\substack{x=x_1\\y=y_1\\z=z_1}} =\begin{pmatrix} \frac{\partial{f}}{\partial{x}}\\ \frac{\partial{f}}{\partial{y}}\\ \frac{\partial{f}}{\partial{z}} \end{pmatrix}_{\substack{x=x_1\\y=y_1\\z=z_1}} $$ where I know the numerical value of each derivative but I don't know the explicit form of the function $f$ . I need to transform this gradient vector into a new coordinate system of three vectors, $\vec{p} ,\vec{q},$ and $\vec{r}$ , defined by: $$ \vec{p}=\begin{pmatrix} x_p\\ y_p\\ z_p \end{pmatrix}; \,\, \vec{q}=\begin{pmatrix} x_q\\ y_q\\ z_q \end{pmatrix}; \,\, \vec{r}=\begin{pmatrix} x_r\\ y_r\\ z_r \end{pmatrix} $$ So I need to transform the vector $\nabla \vec{f}(x,y,z)$ to $\nabla \vec{f}(p,q,r)$ (or the equivalent unit vectors), for that specific value of variables. Something like: $$ \nabla f(p,q,r)\Bigr|_{\substack{x=x_1\\y=y_1\\z=z_1}} =\begin{pmatrix} \frac{\partial{f}}{\partial{p}}\\ \frac{\partial{f}}{\partial{q}}\\ \frac{\partial{f}}{\partial{r}} \end{pmatrix}_{\substack{x=x_1\\y=y_1\\z=z_1}} $$ For any regular vector I know I can construct a transformation matrix from vectors $\vec{p} ,\vec{q},$ and $\vec{r}$ , and multiply the original vector. But given that vector $\nabla \vec{f}(x,y,z)$ comes from a gradient, I'm not sure if there are any special precautions I have to take to account for the chain rule. Is it as simple as a regular coordinate transformation? PS: Along similar lines, how can I transform the hessian of $f$ in $x,y,z$ coordinates ( $f_{xy}$ : matrix of second derivatives), into a hessian of $f$ in $p,q,r$ coordinates, when I only have the hessian at a specific value of $x,y,$ and $z$ ? Thanks!","I have the gradient of a function at a specific value of and in the vector form: where I know the numerical value of each derivative but I don't know the explicit form of the function . I need to transform this gradient vector into a new coordinate system of three vectors, and , defined by: So I need to transform the vector to (or the equivalent unit vectors), for that specific value of variables. Something like: For any regular vector I know I can construct a transformation matrix from vectors and , and multiply the original vector. But given that vector comes from a gradient, I'm not sure if there are any special precautions I have to take to account for the chain rule. Is it as simple as a regular coordinate transformation? PS: Along similar lines, how can I transform the hessian of in coordinates ( : matrix of second derivatives), into a hessian of in coordinates, when I only have the hessian at a specific value of and ? Thanks!","f(x,y,z) x, y, z 
\nabla f(x,y,z)\Bigr|_{\substack{x=x_1\\y=y_1\\z=z_1}} =\begin{pmatrix}
\frac{\partial{f}}{\partial{x}}\\
\frac{\partial{f}}{\partial{y}}\\
\frac{\partial{f}}{\partial{z}}
\end{pmatrix}_{\substack{x=x_1\\y=y_1\\z=z_1}}
 f \vec{p} ,\vec{q}, \vec{r} 
\vec{p}=\begin{pmatrix}
x_p\\
y_p\\
z_p
\end{pmatrix}; \,\,
\vec{q}=\begin{pmatrix}
x_q\\
y_q\\
z_q
\end{pmatrix}; \,\,
\vec{r}=\begin{pmatrix}
x_r\\
y_r\\
z_r
\end{pmatrix}
 \nabla \vec{f}(x,y,z) \nabla \vec{f}(p,q,r) 
\nabla f(p,q,r)\Bigr|_{\substack{x=x_1\\y=y_1\\z=z_1}} =\begin{pmatrix}
\frac{\partial{f}}{\partial{p}}\\
\frac{\partial{f}}{\partial{q}}\\
\frac{\partial{f}}{\partial{r}}
\end{pmatrix}_{\substack{x=x_1\\y=y_1\\z=z_1}}
 \vec{p} ,\vec{q}, \vec{r} \nabla \vec{f}(x,y,z) f x,y,z f_{xy} f p,q,r x,y, z","['multivariable-calculus', 'vectors']"
69,Spivak's Notation on Linear Transformation,Spivak's Notation on Linear Transformation,,"At the bottom of page 3 of Spivak's Calculus on Manifolds Book the author mentions a linear transformation of a basis: $T:\bf{R^n}\rightarrow \bf{R}^m$ is a  matrix $A$ with $m$ rows and $n$ columns. But the transformation formulae of the individual basis, $e_i$ , is confusing: $T(e_i) = \sum_{j=1}^m a_{ji}e_j$ and the coefficients of $T(e_j)$ are the columns of A. To take a simple example if A is a 3*2 matrix $\begin{bmatrix}3&1\\2&3\\1&5\end{bmatrix}$ and we want to transform a $\bf{R}^2 \rightarrow \bf{R}^3 $ then $T(e_1)$ should be $\begin{equation} \begin{bmatrix}3&1\\2&3\\1&5\end{bmatrix} * \begin{bmatrix}1\\0\end{bmatrix} \end{equation}$ which is product of the rows of A and the first basis column vector giving the column vector $\begin{bmatrix}3\\2\\1\end{bmatrix}$ (similarly for $e_2$ ), but then the formulae for this should be $T(e_i) = \sum_{j=1}^{n} a_{ij}*e_j$ not what the author has written out. What am I missing?","At the bottom of page 3 of Spivak's Calculus on Manifolds Book the author mentions a linear transformation of a basis: is a  matrix with rows and columns. But the transformation formulae of the individual basis, , is confusing: and the coefficients of are the columns of A. To take a simple example if A is a 3*2 matrix and we want to transform a then should be which is product of the rows of A and the first basis column vector giving the column vector (similarly for ), but then the formulae for this should be not what the author has written out. What am I missing?",T:\bf{R^n}\rightarrow \bf{R}^m A m n e_i T(e_i) = \sum_{j=1}^m a_{ji}e_j T(e_j) \begin{bmatrix}3&1\\2&3\\1&5\end{bmatrix} \bf{R}^2 \rightarrow \bf{R}^3  T(e_1) \begin{equation} \begin{bmatrix}3&1\\2&3\\1&5\end{bmatrix} * \begin{bmatrix}1\\0\end{bmatrix} \end{equation} \begin{bmatrix}3\\2\\1\end{bmatrix} e_2 T(e_i) = \sum_{j=1}^{n} a_{ij}*e_j,"['multivariable-calculus', 'manifolds']"
70,Finding the error in an argument,Finding the error in an argument,,"If $z=f(x,y)$ and $y=x^2$ , then by the chain rule $\frac{\partial z}{\partial x}=\frac{\partial z}{\partial x}\frac{\partial x}{\partial x}+\frac{\partial z}{\partial y}\frac{\partial y}{\partial x}=\frac{\partial z}{\partial x}+2x\frac{\partial z}{\partial y}$ Therefore $2x\frac{\partial z}{\partial y}=0$ and $\frac{\partial z}{\partial y}=0$ What is wrong with this argument? I have a feeling that 1.) $x$ and $y$ do not have partial derivatives because they are single-variable, and 2.) $\frac{\partial z}{\partial y}$ cannot be zero, because $y=x^2$ and therefore the derivative of any $y$ term exists. How is my reasoning? I am pretty confused by this question.","If and , then by the chain rule Therefore and What is wrong with this argument? I have a feeling that 1.) and do not have partial derivatives because they are single-variable, and 2.) cannot be zero, because and therefore the derivative of any term exists. How is my reasoning? I am pretty confused by this question.","z=f(x,y) y=x^2 \frac{\partial z}{\partial x}=\frac{\partial z}{\partial x}\frac{\partial x}{\partial x}+\frac{\partial z}{\partial y}\frac{\partial y}{\partial x}=\frac{\partial z}{\partial x}+2x\frac{\partial z}{\partial y} 2x\frac{\partial z}{\partial y}=0 \frac{\partial z}{\partial y}=0 x y \frac{\partial z}{\partial y} y=x^2 y","['calculus', 'multivariable-calculus', 'partial-derivative']"
71,"$\int_{[0,1]} \int_{[0,1]} \frac {x^2-y^2}{(x^2+y^2)^2}\,dx\,dy$ and $\int_{[0,1]} \int_{[0,1]} \frac {x^2-y^2}{(x^2+y^2)^2}\,dy\,dx$",and,"\int_{[0,1]} \int_{[0,1]} \frac {x^2-y^2}{(x^2+y^2)^2}\,dx\,dy \int_{[0,1]} \int_{[0,1]} \frac {x^2-y^2}{(x^2+y^2)^2}\,dy\,dx","I am facing problem in calculating $$I_1=\int_{[0,1]} \int_{[0,1]} \frac {x^2-y^2}{(x^2+y^2)^2} \,dx\,dy$$ and $$I_2=\int_{[0,1]} \int_{[0,1]} \frac {x^2-y^2}{(x^2+y^2)^2} \,dy\,dx$$ after substituting $x=r\cos \theta$ and $y=r\sin \theta$ we have $I_1=\int_{[-\pi,\pi]} \int_{[0,1]}\frac{\cos(2\theta)}{r^2}r\,dr\, d\theta$ so I am getting $\infty \times 0=0$ So I am not getting $\pi/4$ and $-\pi/4$ resp. Maybe there is a silly point I am missing. Please help.",I am facing problem in calculating and after substituting and we have so I am getting So I am not getting and resp. Maybe there is a silly point I am missing. Please help.,"I_1=\int_{[0,1]} \int_{[0,1]} \frac {x^2-y^2}{(x^2+y^2)^2} \,dx\,dy I_2=\int_{[0,1]} \int_{[0,1]} \frac {x^2-y^2}{(x^2+y^2)^2} \,dy\,dx x=r\cos \theta y=r\sin \theta I_1=\int_{[-\pi,\pi]} \int_{[0,1]}\frac{\cos(2\theta)}{r^2}r\,dr\, d\theta \infty \times 0=0 \pi/4 -\pi/4","['calculus', 'integration', 'multivariable-calculus', 'definite-integrals']"
72,Directional derivatives for vector-valued functions,Directional derivatives for vector-valued functions,,"Do we only calculate directional derivatives for scalar-valued functions? Is it not possible to calculate directional derivatives for vector-valued functions? How about using the vector of directional derivatives of the components of the given vector function? Would there be any useful physical or geometric meaning? For a specific (randomly chosen) example, if $\vec v(x,y,z)$ is given by $$ \vec v(x,y,z)= \begin{bmatrix} x^3+y^2+z\\  ze^x\\ xyz-9xz\\  \end{bmatrix} $$ how can we interpret the directional derivative of $\vec v$ at the point $(1,2,3)$ in the direction of the vector $\vec u=2i+3j-5k$ ?","Do we only calculate directional derivatives for scalar-valued functions? Is it not possible to calculate directional derivatives for vector-valued functions? How about using the vector of directional derivatives of the components of the given vector function? Would there be any useful physical or geometric meaning? For a specific (randomly chosen) example, if is given by how can we interpret the directional derivative of at the point in the direction of the vector ?","\vec v(x,y,z) 
\vec v(x,y,z)=
\begin{bmatrix}
x^3+y^2+z\\ 
ze^x\\
xyz-9xz\\ 
\end{bmatrix}
 \vec v (1,2,3) \vec u=2i+3j-5k",['multivariable-calculus']
73,"Given a function $f:\mathbb{R}^2 \to \mathbb{R}^2$, to find its inverse near a given point","Given a function , to find its inverse near a given point",f:\mathbb{R}^2 \to \mathbb{R}^2,"Let $f:\mathbb{R}^2 \to \mathbb{R}^2$ be a function given by $$f\left(x,y\right)=\left(x-y,xy\right),\,\, \left(x,y\right) \in \mathbb{R}^2$$ Question : What is the inverse of $f$ near the point $\left(2,-3\right)$ ? Upon checking the conditions, inverse function theorem gives me the existence of $f^{-1}$ , that $f^{-1}$ is $C^1$ under appropriate condition and an explicit form of the derivative of $f^{-1}$ . However, I do not understand how to calculate an explicit form of $f^{-1}$ using inverse function theorem . By direct calculation, I find : $$f^{-1}\left(u,v\right)=\left(\frac{2v}{-u\pm\sqrt{u^2+4v}},\frac{-u\pm\sqrt{u^2+4v}}{2}\right)$$ Two issues : $1$ . It appears to be a one-to-two(!) map. $2$ . At the point $\left(2,-3\right)$ , both the arguments are complex numbers! Any help would be much appreciated.","Let be a function given by Question : What is the inverse of near the point ? Upon checking the conditions, inverse function theorem gives me the existence of , that is under appropriate condition and an explicit form of the derivative of . However, I do not understand how to calculate an explicit form of using inverse function theorem . By direct calculation, I find : Two issues : . It appears to be a one-to-two(!) map. . At the point , both the arguments are complex numbers! Any help would be much appreciated.","f:\mathbb{R}^2 \to \mathbb{R}^2 f\left(x,y\right)=\left(x-y,xy\right),\,\, \left(x,y\right) \in \mathbb{R}^2 f \left(2,-3\right) f^{-1} f^{-1} C^1 f^{-1} f^{-1} f^{-1}\left(u,v\right)=\left(\frac{2v}{-u\pm\sqrt{u^2+4v}},\frac{-u\pm\sqrt{u^2+4v}}{2}\right) 1 2 \left(2,-3\right)","['real-analysis', 'multivariable-calculus', 'inverse-function-theorem']"
74,Roots of $2x^3-4x+1$,Roots of,2x^3-4x+1,"I'm having difficulty getting the solution to the cubic equation $2x^3-4x+1=0$ and from http://www2.trinity.unimelb.edu.au/~rbroekst/MathX/Cubic%20Formula.pdf it claims that the general solution to $Ax^3+Bx^2+Cx+D=0$ is $$p+q+r=-B/A$$ $$pq+qr+rp=C/A$$ $$pqr=-D/A$$ where $p,q,r$ are the roots. I also tried http://www2.trinity.unimelb.edu.au/~rbroekst/MathX/Cubic%20Formula.pdf and very carefully followed their technique (which looks at first glance different but must obviously be equivalent) however it didn't line up with what I got from Wolframalpha.  So I'm just looking to see where I messed up or how others would solve this. Here's my attempt: $$p+q+r=0$$ $$pq+qr+rp=-2$$ $$pqr=-1/2$$ From the first we get that $$p=-q-r$$ then from the second $$p(q+r) = -2-qr$$ then from the two of these we get $$-p^2 = -2-qr$$ equivalently $$0=p^3 -2p+1/2$$ Wait!  What?!  A cubic to solve a cubic..... cubics all the way down!?  Obviously not.... so if it's possible to solve this apart from numerical techniques, I would be really interested in such an answer.  Thank you","I'm having difficulty getting the solution to the cubic equation and from http://www2.trinity.unimelb.edu.au/~rbroekst/MathX/Cubic%20Formula.pdf it claims that the general solution to is where are the roots. I also tried http://www2.trinity.unimelb.edu.au/~rbroekst/MathX/Cubic%20Formula.pdf and very carefully followed their technique (which looks at first glance different but must obviously be equivalent) however it didn't line up with what I got from Wolframalpha.  So I'm just looking to see where I messed up or how others would solve this. Here's my attempt: From the first we get that then from the second then from the two of these we get equivalently Wait!  What?!  A cubic to solve a cubic..... cubics all the way down!?  Obviously not.... so if it's possible to solve this apart from numerical techniques, I would be really interested in such an answer.  Thank you","2x^3-4x+1=0 Ax^3+Bx^2+Cx+D=0 p+q+r=-B/A pq+qr+rp=C/A pqr=-D/A p,q,r p+q+r=0 pq+qr+rp=-2 pqr=-1/2 p=-q-r p(q+r) = -2-qr -p^2 = -2-qr 0=p^3 -2p+1/2","['multivariable-calculus', 'roots', 'substitution', 'inverse-function', 'cubics']"
75,"Gradient and Hessian of $x x^T$ w.r.t. $x$, where $x \in \mathbb{R}^{n \times 1}$,?","Gradient and Hessian of  w.r.t. , where ,?",x x^T x x \in \mathbb{R}^{n \times 1},"Question: Can we find the gradient and Hessian of $x x^T$ w.r.t. $x$, where $x \in \mathbb{R}^{n \times 1}$ ? EDIT:  If we can, may I know how to compute that? Thank you.","Question: Can we find the gradient and Hessian of $x x^T$ w.r.t. $x$, where $x \in \mathbb{R}^{n \times 1}$ ? EDIT:  If we can, may I know how to compute that? Thank you.",,"['linear-algebra', 'multivariable-calculus', 'derivatives', 'matrix-calculus', 'hessian-matrix']"
76,Deriving the Taylor expansion $f(x+p) = f(x) + \nabla f(x+tp)^Tp$,Deriving the Taylor expansion,f(x+p) = f(x) + \nabla f(x+tp)^Tp,"I'm trying to derive the Taylor formula: $$f(x+p) = f(x) + \nabla f(x+tp)^Tp$$ For that I think tha I just need to use the formula for one variable taylor  expansion and follow like here: https://math.stackexchange.com/a/222217/166180 . This answer kinda explains the formula I need but for an infinite expansion. I need a finite expansion which guarantees there's a $t$ in which the expansion is exact. I couldn't find a specific taylor theorem like this, so I'm trying to derive this forula and I think it has to do with the mean value theorem: $$f'(c) = \frac{f(x)-f(a)}{x-a}$$ for some $c\in(a,x)$ so $f'(c)x-f'(c)a = f(x)-f(a) \implies f(x) = f(a) + f'(c)x -f'(c)a$ It kinda looks like something I want. UPDATE: Take $a=0$ to get $f(x) = f(0) + f'(c)x$ Call $f$ as $\Phi$ to get: $$(1) = \Phi(t) = \Phi(0) + \Phi'(c)t$$ for some $c\in(0,t)$ If we take $\Phi(t) = \phi(x + pt)$ so $$\Phi'(c) = \lim_{a\to c}\frac{\phi(x+ct)-\phi(x+at)}{c-a} = \ ?$$ I need $\Phi'(c)$ to finish $(1)$ by writing everything in terms of $\phi$ and hopefully achieve the formula with the gradient. PS : how would I achieve the second order expansion $$f(x+p) = f(x) + \nabla f(x)^Tp + \frac{1}{2}p^T\nabla^2f(x+tp)p$$ ? I can't think of a second order version of the mean value theorem.","I'm trying to derive the Taylor formula: $$f(x+p) = f(x) + \nabla f(x+tp)^Tp$$ For that I think tha I just need to use the formula for one variable taylor  expansion and follow like here: https://math.stackexchange.com/a/222217/166180 . This answer kinda explains the formula I need but for an infinite expansion. I need a finite expansion which guarantees there's a $t$ in which the expansion is exact. I couldn't find a specific taylor theorem like this, so I'm trying to derive this forula and I think it has to do with the mean value theorem: $$f'(c) = \frac{f(x)-f(a)}{x-a}$$ for some $c\in(a,x)$ so $f'(c)x-f'(c)a = f(x)-f(a) \implies f(x) = f(a) + f'(c)x -f'(c)a$ It kinda looks like something I want. UPDATE: Take $a=0$ to get $f(x) = f(0) + f'(c)x$ Call $f$ as $\Phi$ to get: $$(1) = \Phi(t) = \Phi(0) + \Phi'(c)t$$ for some $c\in(0,t)$ If we take $\Phi(t) = \phi(x + pt)$ so $$\Phi'(c) = \lim_{a\to c}\frac{\phi(x+ct)-\phi(x+at)}{c-a} = \ ?$$ I need $\Phi'(c)$ to finish $(1)$ by writing everything in terms of $\phi$ and hopefully achieve the formula with the gradient. PS : how would I achieve the second order expansion $$f(x+p) = f(x) + \nabla f(x)^Tp + \frac{1}{2}p^T\nabla^2f(x+tp)p$$ ? I can't think of a second order version of the mean value theorem.",,"['multivariable-calculus', 'derivatives', 'taylor-expansion']"
77,Taylor's formula with remainder for vector-valued functions,Taylor's formula with remainder for vector-valued functions,,Let $f: \mathbb{R}^n \to  \mathbb{R}^n $. Does there exist a generalization of Taylor's Theorem with Lagrange Remainder for such a vector-valued function?,Let $f: \mathbb{R}^n \to  \mathbb{R}^n $. Does there exist a generalization of Taylor's Theorem with Lagrange Remainder for such a vector-valued function?,,"['calculus', 'real-analysis', 'multivariable-calculus', 'taylor-expansion', 'vector-analysis']"
78,How do you do change of variables for triple integrals?,How do you do change of variables for triple integrals?,,"I am evaluating the function over the following bounds.$$\int_0^2\int_0^{\sqrt{4-x^2}}\int_0^{\sqrt{4-x^2-y^2}}z\sqrt{4-x^2-y^2}\,\mathrm dz\,\mathrm dy\,\mathrm dx$$ I'm not sure how to combine triple integrals and change of variables. Can someone run me through the steps for this problem? thanks","I am evaluating the function over the following bounds.$$\int_0^2\int_0^{\sqrt{4-x^2}}\int_0^{\sqrt{4-x^2-y^2}}z\sqrt{4-x^2-y^2}\,\mathrm dz\,\mathrm dy\,\mathrm dx$$ I'm not sure how to combine triple integrals and change of variables. Can someone run me through the steps for this problem? thanks",,"['integration', 'multivariable-calculus', 'change-of-variable']"
79,Compute $\iiint_R 6z \ dV.$,Compute,\iiint_R 6z \ dV.,"Integrate the function $$f(x,y,z)=6z$$ over the tetrahedral   $$R=\{(x,y,z):x\geq0, \ y\geq 0, \ z\geq 0, \ 5x+y+z \leq 5\}.$$ This tetrahedral can obtain hegiths in $z$-axis from 0 to 5 in the first octant. Drawing this out, I get that the bounds are \begin{array}{lcl} 0 \leq x \leq  1 \\ 0 \leq y \leq 5-5x \\ 0 \leq z \leq -5x-y+5 \end{array} So $$\iiint_R 6z \ dV=\int_0^1\int_0^{5-5x}\int_0^{-5x-y+5}6z \ dzdydx=\frac{125}{4}.$$ Can anyone confirm this is correct and check for any improvement?","Integrate the function $$f(x,y,z)=6z$$ over the tetrahedral   $$R=\{(x,y,z):x\geq0, \ y\geq 0, \ z\geq 0, \ 5x+y+z \leq 5\}.$$ This tetrahedral can obtain hegiths in $z$-axis from 0 to 5 in the first octant. Drawing this out, I get that the bounds are \begin{array}{lcl} 0 \leq x \leq  1 \\ 0 \leq y \leq 5-5x \\ 0 \leq z \leq -5x-y+5 \end{array} So $$\iiint_R 6z \ dV=\int_0^1\int_0^{5-5x}\int_0^{-5x-y+5}6z \ dzdydx=\frac{125}{4}.$$ Can anyone confirm this is correct and check for any improvement?",,"['multivariable-calculus', 'proof-verification']"
80,"$\int_0^1 \int_0^{1-y} \cos\Big( \frac{x-y}{x+y} \Big) \, dx dy$ [duplicate]",[duplicate],"\int_0^1 \int_0^{1-y} \cos\Big( \frac{x-y}{x+y} \Big) \, dx dy","This question already has answers here : How to get the interval after change of variables? (2 answers) Closed 6 years ago . Reviewing old homework sets for a class and I came across this integral: $$\displaystyle \int_0^1 \int_0^{1-y} \cos\Big( \frac{x-y}{x+y} \Big) \; dx dy,$$ which the question suggests to evaluate using a change of coordinates; however, I haven't a clue where to begin to identify a useful change of coordinates. I tried $u = x-y$ and $v = x+y$, but then wasn't sure how I'd convert the domain of integration. After that, I looked to the given limits for inspiration and noticed that $0<x<1-y$ could be rewritten $y < x+y < 1$, so tried $u = x+y$ and $v = y$, which yielded $$\displaystyle \int_0^1 \int_y^{1} \cos\Big( \frac{ u - 2y }{u} \Big) \; du\,dy,$$ but that doesn't seem any simpler than the original, to me. Any advice would be appreciated!","This question already has answers here : How to get the interval after change of variables? (2 answers) Closed 6 years ago . Reviewing old homework sets for a class and I came across this integral: $$\displaystyle \int_0^1 \int_0^{1-y} \cos\Big( \frac{x-y}{x+y} \Big) \; dx dy,$$ which the question suggests to evaluate using a change of coordinates; however, I haven't a clue where to begin to identify a useful change of coordinates. I tried $u = x-y$ and $v = x+y$, but then wasn't sure how I'd convert the domain of integration. After that, I looked to the given limits for inspiration and noticed that $0<x<1-y$ could be rewritten $y < x+y < 1$, so tried $u = x+y$ and $v = y$, which yielded $$\displaystyle \int_0^1 \int_y^{1} \cos\Big( \frac{ u - 2y }{u} \Big) \; du\,dy,$$ but that doesn't seem any simpler than the original, to me. Any advice would be appreciated!",,"['multivariable-calculus', 'definite-integrals', 'change-of-variable']"
81,What is the sound of one argument permuting?,What is the sound of one argument permuting?,,"A function $M:\left(\mathbb{R}^{n}\right)^{k}\to\mathbb{R}$, written $M\left[\mathfrak{a}_{1},\dots,\mathfrak{a}_{k}\right]$; where $\mathfrak{a}_{i}\in\mathbb{R}^{n}$ is said to be k-multilinear on $\mathbb{R}^{n}$ if it is linear in each of its arguuments. It is said to be alternating if $$M\left[\dots,\mathfrak{a}_{i}\dots,\mathfrak{a}_{i},\dots\right]=0.$$ That is, if any pair of arguuments are equal. Or, equivalently, if interchanging a pair of arguments reverses the the arithmetic singn of the function. That is, if  $$M\left[\dots,\mathfrak{a}_{i}\dots,\mathfrak{a}_{j},\dots\right]=-M\left[\dots,\mathfrak{a}_{j}\dots,\mathfrak{a}_{i},\dots\right].$$ For example, $D:\left(\mathbb{R}^{n}\right)^{n}\to\mathbb{R},$ the determinant of an $n\times n$ matrix written as a function the column vectors $$D\left[\mathfrak{a}_{1},\dots,\mathfrak{a}_{n}\right]=\left|\begin{bmatrix}a_{\cdot1}^{1} & \dots & a_{\cdot n}^{1}\\ \vdots & \ddots & \vdots\\ a_{\cdot1}^{n} & \dots & a_{\cdot n}^{n} \end{bmatrix}\right|,$$ is such a function. Edwards condescends: Notice that every linear function on $\mathbb{R}^{n}$ is automatically alternating. How do I interchange or make equal a pair of arguments in the function $L\left[\mathfrak{a}\right]\in\mathbb{R}$? Or, why should I conclude that a ""1-multilinear function"" is althernating?","A function $M:\left(\mathbb{R}^{n}\right)^{k}\to\mathbb{R}$, written $M\left[\mathfrak{a}_{1},\dots,\mathfrak{a}_{k}\right]$; where $\mathfrak{a}_{i}\in\mathbb{R}^{n}$ is said to be k-multilinear on $\mathbb{R}^{n}$ if it is linear in each of its arguuments. It is said to be alternating if $$M\left[\dots,\mathfrak{a}_{i}\dots,\mathfrak{a}_{i},\dots\right]=0.$$ That is, if any pair of arguuments are equal. Or, equivalently, if interchanging a pair of arguments reverses the the arithmetic singn of the function. That is, if  $$M\left[\dots,\mathfrak{a}_{i}\dots,\mathfrak{a}_{j},\dots\right]=-M\left[\dots,\mathfrak{a}_{j}\dots,\mathfrak{a}_{i},\dots\right].$$ For example, $D:\left(\mathbb{R}^{n}\right)^{n}\to\mathbb{R},$ the determinant of an $n\times n$ matrix written as a function the column vectors $$D\left[\mathfrak{a}_{1},\dots,\mathfrak{a}_{n}\right]=\left|\begin{bmatrix}a_{\cdot1}^{1} & \dots & a_{\cdot n}^{1}\\ \vdots & \ddots & \vdots\\ a_{\cdot1}^{n} & \dots & a_{\cdot n}^{n} \end{bmatrix}\right|,$$ is such a function. Edwards condescends: Notice that every linear function on $\mathbb{R}^{n}$ is automatically alternating. How do I interchange or make equal a pair of arguments in the function $L\left[\mathfrak{a}\right]\in\mathbb{R}$? Or, why should I conclude that a ""1-multilinear function"" is althernating?",,"['linear-algebra', 'multivariable-calculus', 'logic', 'definition', 'determinant']"
82,Why is continuity of partials necessary for Jacobian to be the derivative?,Why is continuity of partials necessary for Jacobian to be the derivative?,,"Jut having the partials is not enough, but partials being continuous   is enough to guarantee that the Jacobian is the derivative. My question is: ""Why is it necessary for the partial derivatives to be continuous? Isn't just the existence of partials sufficient?"" According to what I know, the Jacobian is a just a matrix consisting of the partial derivatives.","Jut having the partials is not enough, but partials being continuous   is enough to guarantee that the Jacobian is the derivative. My question is: ""Why is it necessary for the partial derivatives to be continuous? Isn't just the existence of partials sufficient?"" According to what I know, the Jacobian is a just a matrix consisting of the partial derivatives.",,"['multivariable-calculus', 'derivatives']"
83,Why does Fubini's theorem not hold here?,Why does Fubini's theorem not hold here?,,I have shown that $$\int_0^1\int_0^1 \frac{x^2-y^2}{(x^2+y^2)^2}dx  dy=\frac{\pi}{4}$$ and that $$\int_0^1\int_0^1 \frac{x^2-y^2}{(x^2+y^2)^2}dy   dx =-\frac{\pi}{4}$$ Shouldn't these two Integrals be equal according to fubini's theorem? Why do we get two different results?,I have shown that $$\int_0^1\int_0^1 \frac{x^2-y^2}{(x^2+y^2)^2}dx  dy=\frac{\pi}{4}$$ and that $$\int_0^1\int_0^1 \frac{x^2-y^2}{(x^2+y^2)^2}dy   dx =-\frac{\pi}{4}$$ Shouldn't these two Integrals be equal according to fubini's theorem? Why do we get two different results?,,"['calculus', 'multivariable-calculus', 'definite-integrals']"
84,Gradient of function composed with linear transformation,Gradient of function composed with linear transformation,,"In one dimension we have the following easy result, $\frac{d}{dx} f(ax) = af'(ax)$ for any constant $a$. In higher dimensions, we have two ""reasonable"" candidates for $\frac{d}{d\mathbf{x}}f(A\mathbf{x})$, namely $A \nabla f(A \mathbf{x})$ and $A^T \nabla f(A \mathbf{x})$. By bashing out the algebra I can verify that the correct answer is the second option, but can an experienced mathematician explain why it's obviously the second one?","In one dimension we have the following easy result, $\frac{d}{dx} f(ax) = af'(ax)$ for any constant $a$. In higher dimensions, we have two ""reasonable"" candidates for $\frac{d}{d\mathbf{x}}f(A\mathbf{x})$, namely $A \nabla f(A \mathbf{x})$ and $A^T \nabla f(A \mathbf{x})$. By bashing out the algebra I can verify that the correct answer is the second option, but can an experienced mathematician explain why it's obviously the second one?",,"['multivariable-calculus', 'differential-geometry']"
85,Matrix derivative of scalar function involving matrix square root,Matrix derivative of scalar function involving matrix square root,,"Let $X$ be a positive definite matrix with positive definite matrix square root $X^{1/2}$. Define $$y = \text{trace}(AX^{1/2})$$ some known matrix $A$. What is ${\partial y}/{\partial X}$ ? I tried using this set of notes together with the square root formula from here to evaluate it in the $2 \times 2$ case using index notation, but there must be a better way? Especially to generalize it to the $n \times n$ case. I would guess it is something like $$ \frac{\partial y}{\partial X} = \frac{1}{2} A^T X^{-1/2}$$ where $X^{-1/2}$ is the square root of $X^{-1}$.","Let $X$ be a positive definite matrix with positive definite matrix square root $X^{1/2}$. Define $$y = \text{trace}(AX^{1/2})$$ some known matrix $A$. What is ${\partial y}/{\partial X}$ ? I tried using this set of notes together with the square root formula from here to evaluate it in the $2 \times 2$ case using index notation, but there must be a better way? Especially to generalize it to the $n \times n$ case. I would guess it is something like $$ \frac{\partial y}{\partial X} = \frac{1}{2} A^T X^{-1/2}$$ where $X^{-1/2}$ is the square root of $X^{-1}$.",,"['multivariable-calculus', 'matrix-calculus']"
86,Gradient of a homogenous function,Gradient of a homogenous function,,"I am having trouble constructing a proof for this preposition. I am not sure if I am misunderstanding the meaning of a homogenous functions, but either way I get stuck in my proof. Let $k$ be an integer. A function $f : \mathbb{R^n} \to \mathbb{R}$ is called homogenous of degree $k$ if $f(\lambda x) = \lambda^k f(x)$ for all $\lambda \in \mathbb{R}$ and $x \in \mathbb{R^n}$. Prove that if $f$ is homogenous of degree $k$ then $x \cdot \nabla f(x) = kf(x)$. Here is my go at it. Proof. (=>) Suppose $x \cdot \nabla f(x)$. Then  $$x \cdot \nabla f(x)$$  $$= x \cdot [\frac{\partial f}{\partial x_1}, ..., \frac{\partial f}{ \partial x_n}]$$ $$ = [x_1, ..., x_n] \cdot [\frac{\partial f}{\partial x_1}, ..., \frac{\partial f}{\partial x_n}]$$ $$ = \frac{\partial f}{\partial x_1} x_1 +  ... + \frac{\partial f}{\partial x_n} x_n $$ But I feel I've gone off course now. Any ideas / hints? Thanks!","I am having trouble constructing a proof for this preposition. I am not sure if I am misunderstanding the meaning of a homogenous functions, but either way I get stuck in my proof. Let $k$ be an integer. A function $f : \mathbb{R^n} \to \mathbb{R}$ is called homogenous of degree $k$ if $f(\lambda x) = \lambda^k f(x)$ for all $\lambda \in \mathbb{R}$ and $x \in \mathbb{R^n}$. Prove that if $f$ is homogenous of degree $k$ then $x \cdot \nabla f(x) = kf(x)$. Here is my go at it. Proof. (=>) Suppose $x \cdot \nabla f(x)$. Then  $$x \cdot \nabla f(x)$$  $$= x \cdot [\frac{\partial f}{\partial x_1}, ..., \frac{\partial f}{ \partial x_n}]$$ $$ = [x_1, ..., x_n] \cdot [\frac{\partial f}{\partial x_1}, ..., \frac{\partial f}{\partial x_n}]$$ $$ = \frac{\partial f}{\partial x_1} x_1 +  ... + \frac{\partial f}{\partial x_n} x_n $$ But I feel I've gone off course now. Any ideas / hints? Thanks!",,"['real-analysis', 'multivariable-calculus']"
87,Surface area of paraboloid $z=x^2+y^2$ and below $z=9$,Surface area of paraboloid  and below,z=x^2+y^2 z=9,"I have attempted to convert to cylindrical coordinates and have gotten to the equation r*sqrt(4r^2+1) but I am unsure of how to set up the triple integral limits from here. If this is correct so far, how do you find the limits for each integral, if not then how would you go about solving it? Thanks. Also, I know the answer is pi/6(37^(3/2)*-1) which is about 117.319","I have attempted to convert to cylindrical coordinates and have gotten to the equation r*sqrt(4r^2+1) but I am unsure of how to set up the triple integral limits from here. If this is correct so far, how do you find the limits for each integral, if not then how would you go about solving it? Thanks. Also, I know the answer is pi/6(37^(3/2)*-1) which is about 117.319",,"['calculus', 'multivariable-calculus']"
88,"Is there a ""clean"" way of parametrizing the intersection of the sphere and saddle?","Is there a ""clean"" way of parametrizing the intersection of the sphere and saddle?",,"I was playing around, trying to find a parametrization for the intersection of the two surfaces, $$x^2+y^2+z^2=1$$ $$z=x^2-y^2,$$ but I wasn't able to get anything nice-looking. Any suggestions, or is this a hopeless endeavor? EDIT: Thanks, Thomas Andrews. With the corrected formulas, $$ a(t) = \sqrt{\frac{2}{1+\sqrt{1+4\cos^2 2t}}} $$ $$ (x(t),y(t),z(t)) = (a(t)\cos t, a(t)\sin t, a(t)^2 \cos2t), $$ we get a very nice picture!","I was playing around, trying to find a parametrization for the intersection of the two surfaces, $$x^2+y^2+z^2=1$$ $$z=x^2-y^2,$$ but I wasn't able to get anything nice-looking. Any suggestions, or is this a hopeless endeavor? EDIT: Thanks, Thomas Andrews. With the corrected formulas, $$ a(t) = \sqrt{\frac{2}{1+\sqrt{1+4\cos^2 2t}}} $$ $$ (x(t),y(t),z(t)) = (a(t)\cos t, a(t)\sin t, a(t)^2 \cos2t), $$ we get a very nice picture!",,"['multivariable-calculus', 'parametric']"
89,How to show that $ \int_0^x\left[\int_0^uf(t)dt \right] du = \int_0^xf(u)(x-u)du$?,How to show that ?, \int_0^x\left[\int_0^uf(t)dt \right] du = \int_0^xf(u)(x-u)du,"I have been asked to show that  $$  \int_0^x\left[\int_0^uf(t)dt \right] du = \int_0^xf(u)(x-u)du. $$  But it has not been specified whether or not $f$ is continuous or if it has an anti-derivative. I have shown this is true if $f$ does have an anti-derivative but can't find a way to show it's true otherwise. Is this statement true even if $f$ does not have an anti-derivative or does it become nonsense? I appreciate any help. My proof for when $f$ has an anti-derivative, $F$ \begin{align}  \int_0^xf(u)(x-u)du &= \left[F(u)(x-u)\right]_0^x + \int_0^xF(u)du \\ & = -F(0)x + \int_0^xF(u)du \\ & = \int_0^x(F(u) - F(0))du \\ & = \int_0^x\left[\int_0^uf(t)dt\right]du  \end{align}","I have been asked to show that  $$  \int_0^x\left[\int_0^uf(t)dt \right] du = \int_0^xf(u)(x-u)du. $$  But it has not been specified whether or not $f$ is continuous or if it has an anti-derivative. I have shown this is true if $f$ does have an anti-derivative but can't find a way to show it's true otherwise. Is this statement true even if $f$ does not have an anti-derivative or does it become nonsense? I appreciate any help. My proof for when $f$ has an anti-derivative, $F$ \begin{align}  \int_0^xf(u)(x-u)du &= \left[F(u)(x-u)\right]_0^x + \int_0^xF(u)du \\ & = -F(0)x + \int_0^xF(u)du \\ & = \int_0^x(F(u) - F(0))du \\ & = \int_0^x\left[\int_0^uf(t)dt\right]du  \end{align}",,"['calculus', 'multivariable-calculus']"
90,How to solve simultaneous inequalities?,How to solve simultaneous inequalities?,,"I am doing multivariable calculus, and specifically double integrals. I am facing difficulties finding the domain of the integal, however i am given the following equations: $$1 ≤ 2x+y ≤ 2$$ $$0 ≤ x-2y ≤ 1$$ Through these two equations i am supposed to find the area of integrals for each of the variables i.e $x$ and $y$ I set them as simultaneous inequalities but it doesn't seem to help because i get the boundary for $x$ to be $2/5 ≤ x ≤ 1 $ and for $y$ to be $4/5 ≤ y ≤ 0 $ which is obviously WRONG because how can the lowest boundary for possibly have higher value than that in the higher. Guys, I really appreciate you help, it means the world to me.","I am doing multivariable calculus, and specifically double integrals. I am facing difficulties finding the domain of the integal, however i am given the following equations: $$1 ≤ 2x+y ≤ 2$$ $$0 ≤ x-2y ≤ 1$$ Through these two equations i am supposed to find the area of integrals for each of the variables i.e $x$ and $y$ I set them as simultaneous inequalities but it doesn't seem to help because i get the boundary for $x$ to be $2/5 ≤ x ≤ 1 $ and for $y$ to be $4/5 ≤ y ≤ 0 $ which is obviously WRONG because how can the lowest boundary for possibly have higher value than that in the higher. Guys, I really appreciate you help, it means the world to me.",,"['multivariable-calculus', 'systems-of-equations']"
91,Evaluate the line integral of a parabola,Evaluate the line integral of a parabola,,"How can I evaluate : $$\int_{C} y \;dx + x^2 \; dy$$ where $C$ is the parabola define by $$y=4x-x^2 \quad \text{from } \; (4,0) \; \text{ to } \; (1,3).$$ Do I need to parameterize the parabola?","How can I evaluate : $$\int_{C} y \;dx + x^2 \; dy$$ where $C$ is the parabola define by $$y=4x-x^2 \quad \text{from } \; (4,0) \; \text{ to } \; (1,3).$$ Do I need to parameterize the parabola?",,"['calculus', 'integration', 'multivariable-calculus', 'line-integrals']"
92,I don't understand why we represent functions $f:I \subseteq \Bbb R \to \Bbb R^2$ the way we do.,I don't understand why we represent functions  the way we do.,f:I \subseteq \Bbb R \to \Bbb R^2,"I don't understand why we represent functions $f:I \subseteq \Bbb R \to \Bbb R^2$ the way we do: doing an analogy with how we represent functions from $\Bbb R$ to $\Bbb R$ or from $\Bbb R^2 \to \Bbb R$, my impression would be to represent functions like $f$ in the format: $$(t,f_1(t),f_2(t))$$ But instead we grab all the points $(f_1(t),f_2(t))$, that is, the image of $f$ and we plot that in $\Bbb R^2$, and we call $f$ a parametrization. As an example of this, let $f(t)=(\cos t, \sin t)$. If we represented this by plotting the points $(t,\cos t, \sin t)$ we'd get a helix. But instead we 'splash' that helix into the wall and we get the parametrization of a circle.","I don't understand why we represent functions $f:I \subseteq \Bbb R \to \Bbb R^2$ the way we do: doing an analogy with how we represent functions from $\Bbb R$ to $\Bbb R$ or from $\Bbb R^2 \to \Bbb R$, my impression would be to represent functions like $f$ in the format: $$(t,f_1(t),f_2(t))$$ But instead we grab all the points $(f_1(t),f_2(t))$, that is, the image of $f$ and we plot that in $\Bbb R^2$, and we call $f$ a parametrization. As an example of this, let $f(t)=(\cos t, \sin t)$. If we represented this by plotting the points $(t,\cos t, \sin t)$ we'd get a helix. But instead we 'splash' that helix into the wall and we get the parametrization of a circle.",,"['multivariable-calculus', 'functions', 'parametrization']"
93,"Local coordinates near point such that $X= \partial_1$ is relative to those coordinates, vector field with isolated s.t. coordinates do not exist?","Local coordinates near point such that  is relative to those coordinates, vector field with isolated s.t. coordinates do not exist?",X= \partial_1,"Let $M$ be a finite-dimensional smooth manifold, and let $X$ be a smooth vector field on $M$. Let $X(p) \neq 0$ for some $p \in M$. How do I show that I can find local coordinates near $p$ such that $X = \partial_1$ relative to these coordinates? What is an example of a vector field with an isolated zero for which such coordinates do not exist?","Let $M$ be a finite-dimensional smooth manifold, and let $X$ be a smooth vector field on $M$. Let $X(p) \neq 0$ for some $p \in M$. How do I show that I can find local coordinates near $p$ such that $X = \partial_1$ relative to these coordinates? What is an example of a vector field with an isolated zero for which such coordinates do not exist?",,"['multivariable-calculus', 'differential-geometry']"
94,Mean value theorem for vector valued multivariable function,Mean value theorem for vector valued multivariable function,,"In the general situation of $f:S\to \mathbb R^m$ where $S\subset \mathbb R^n$. There is a form of the mean value theorem: $a\cdot (f(y)-f(x))=a\cdot (f'(z)(y-x))$ which requires a vector $a$ and dot products. In Tom Apostol's Mathematical Analysis (Second Edition), page No. 355, I found that after choosing $a$ to be a unit vector and using Cauchy-Scwarz inequality, they have written $\parallel f(y)-f(x)\parallel\leq \parallel f'(z)\parallel \parallel y-x\parallel$. But how have they got rid of $a$ in the left hand side. If I choose the unit vector in the direction of the vector $f(y)-f(x)$, then it is possible, but how will it follow for an arbitrary unit vector? Please help!","In the general situation of $f:S\to \mathbb R^m$ where $S\subset \mathbb R^n$. There is a form of the mean value theorem: $a\cdot (f(y)-f(x))=a\cdot (f'(z)(y-x))$ which requires a vector $a$ and dot products. In Tom Apostol's Mathematical Analysis (Second Edition), page No. 355, I found that after choosing $a$ to be a unit vector and using Cauchy-Scwarz inequality, they have written $\parallel f(y)-f(x)\parallel\leq \parallel f'(z)\parallel \parallel y-x\parallel$. But how have they got rid of $a$ in the left hand side. If I choose the unit vector in the direction of the vector $f(y)-f(x)$, then it is possible, but how will it follow for an arbitrary unit vector? Please help!",,"['calculus', 'real-analysis', 'multivariable-calculus']"
95,Volume of the region outside of a cylinder and inside a sphere,Volume of the region outside of a cylinder and inside a sphere,,The cylinder is $x^2 +y^2 = 1$ and the sphere is $x^2 + y^2 + z^2 = 4$. I have to find the volume of the region outside the cylinder and inside the sphere. The triple spherical integral for this problem is (from the answer key) $$\int _0^{2\pi }\int _{\frac{\pi }{6}}^{\frac{5\pi }{6}}\int _{csc\phi }^2\:\rho ^2sin\phi \:d\rho \:d\phi \:d\theta $$ What is confusing me here is that there's some space at the endcaps of the cylinder that is not being accounted for. Why is this the case?,The cylinder is $x^2 +y^2 = 1$ and the sphere is $x^2 + y^2 + z^2 = 4$. I have to find the volume of the region outside the cylinder and inside the sphere. The triple spherical integral for this problem is (from the answer key) $$\int _0^{2\pi }\int _{\frac{\pi }{6}}^{\frac{5\pi }{6}}\int _{csc\phi }^2\:\rho ^2sin\phi \:d\rho \:d\phi \:d\theta $$ What is confusing me here is that there's some space at the endcaps of the cylinder that is not being accounted for. Why is this the case?,,"['integration', 'multivariable-calculus', 'spherical-coordinates']"
96,Solving $\int_{0}^{1}\int_{0}^{1-y} \sin\frac{x-y}{x+y}\mathrm dx\mathrm dy$,Solving,\int_{0}^{1}\int_{0}^{1-y} \sin\frac{x-y}{x+y}\mathrm dx\mathrm dy,How would I go about solving the following double integral? $\int_{0}^{1}\int_{0}^{1-y} \sin\frac{x-y}{x+y}\mathrm dx\mathrm dy$ I am absolutely clueless on what to do with that sine.,How would I go about solving the following double integral? $\int_{0}^{1}\int_{0}^{1-y} \sin\frac{x-y}{x+y}\mathrm dx\mathrm dy$ I am absolutely clueless on what to do with that sine.,,"['integration', 'multivariable-calculus']"
97,How can vector field simultaneously be a function and also an operator that acts on a function?,How can vector field simultaneously be a function and also an operator that acts on a function?,,"In elementary calculus we have definition: A vector field is a function that assigns a vector to each point in $\mathbb{R}^2$ or $\mathbb{R}^3$ i.e. F(x,y) = P(x,y) $\hat i$ + Q(x,y,) $\hat j$ In differential geometry we have notation: $vf$ where $v$ = $v_k \partial^k$ so $vf$ = $v_k \partial^k f$ more precisely $v : C^\infty(M) \to C^\infty(M) $ In the former case, a vector field is defined as $F(x,y)$ , in the latter case, vector field is an operator $v = v_k \partial^k$ I am very new to differential geometry. Is there any reconciliation between the two concepts?","In elementary calculus we have definition: A vector field is a function that assigns a vector to each point in or i.e. F(x,y) = P(x,y) + Q(x,y,) In differential geometry we have notation: where = so = more precisely In the former case, a vector field is defined as , in the latter case, vector field is an operator I am very new to differential geometry. Is there any reconciliation between the two concepts?","\mathbb{R}^2 \mathbb{R}^3 \hat i \hat j vf v v_k \partial^k vf v_k \partial^k f v : C^\infty(M) \to C^\infty(M)  F(x,y) v = v_k \partial^k","['multivariable-calculus', 'differential-geometry', 'vector-fields']"
98,integrate this double integral by any method you can. [closed],integrate this double integral by any method you can. [closed],,"Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 9 years ago . Improve this question I'm having trouble with this double integral: $$\int_0^2\int_0^{2-x} \exp\left(\frac{x−y}{x+y}\right)\text dy\,\text dx$$","Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 9 years ago . Improve this question I'm having trouble with this double integral: $$\int_0^2\int_0^{2-x} \exp\left(\frac{x−y}{x+y}\right)\text dy\,\text dx$$",,"['integration', 'multivariable-calculus', 'definite-integrals']"
99,Green's Theorem; computing a double integral,Green's Theorem; computing a double integral,,"This is the last part of an exercise in Apostol Vol. II.  (p.385, 1 (e), to be precise.)  No doubt there's a trick I'm missing, because evaluating the double integral over the region involved seems unduly complicated. We are supposed to use Green's Theorem to evaluate the line integral $\oint_CPdx + Qdy$ (where $C$ is traversed counterclockwise), with $P = y^2$, $Q = x$, and $C$ described by the parametric equations $x = 2\cos^3t$, $y = 2\sin^3t$ where $t$ ranges from $0$ to $2\pi$. We have $\frac{\partial Q}{\partial x} = 1$, $\frac{\partial P}{\partial y} = 2y$.  So what we want to evaluate is \begin{align} \iint\limits_R (1 - 2y)dydx \end{align} where $R$ is the interior of the the region bounded by $C$.  Trying to evaluate by iterated integration gives us \begin{align} \int_{-2}^2\left[\int_{-2\cos^3\left[\arcsin\left(\sqrt[3]{\frac{x}{2}}\right)\right]}^{2\cos^3\left[\arcsin\left(\sqrt[3]{\frac{x}{2}}\right)\right]}(1-2y)dy\right]dx & = \int_{-2}^24\cos^3\left[\arcsin\left(\sqrt[3]{\frac{x}{2}}\right)\right]dx \end{align} and, I mean, give me a break.  There's got to be a better way, right?","This is the last part of an exercise in Apostol Vol. II.  (p.385, 1 (e), to be precise.)  No doubt there's a trick I'm missing, because evaluating the double integral over the region involved seems unduly complicated. We are supposed to use Green's Theorem to evaluate the line integral $\oint_CPdx + Qdy$ (where $C$ is traversed counterclockwise), with $P = y^2$, $Q = x$, and $C$ described by the parametric equations $x = 2\cos^3t$, $y = 2\sin^3t$ where $t$ ranges from $0$ to $2\pi$. We have $\frac{\partial Q}{\partial x} = 1$, $\frac{\partial P}{\partial y} = 2y$.  So what we want to evaluate is \begin{align} \iint\limits_R (1 - 2y)dydx \end{align} where $R$ is the interior of the the region bounded by $C$.  Trying to evaluate by iterated integration gives us \begin{align} \int_{-2}^2\left[\int_{-2\cos^3\left[\arcsin\left(\sqrt[3]{\frac{x}{2}}\right)\right]}^{2\cos^3\left[\arcsin\left(\sqrt[3]{\frac{x}{2}}\right)\right]}(1-2y)dy\right]dx & = \int_{-2}^24\cos^3\left[\arcsin\left(\sqrt[3]{\frac{x}{2}}\right)\right]dx \end{align} and, I mean, give me a break.  There's got to be a better way, right?",,['multivariable-calculus']
