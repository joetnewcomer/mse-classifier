{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7b2154f8-78cd-4524-b4e1-027ec4a35a17",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import ast\n",
    "import contractions\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from gensim.models import Word2Vec, KeyedVectors\n",
    "\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import hamming_loss, f1_score, precision_score, recall_score, accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "30ad2992-7bb0-4cda-8f8d-ad48c2a94ede",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    }
   ],
   "source": [
    "# Make sure tensorflow recognizes my GPU\n",
    "print(tf.config.list_physical_devices('GPU'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0af418b1-3485-40bf-9873-b22e5d64f0b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import data\n",
    "raw_data = pd.read_csv('../../data/parsed_data.csv').fillna('')\n",
    "raw_data['tags'] = raw_data['tags'].apply(ast.literal_eval)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61860789-7d55-4b6d-8367-f06077c4ccfa",
   "metadata": {},
   "source": [
    "### Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c29262ba-3673-4c0f-ac60-317d433591b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize body text\n",
    "def tokenize_sentences(text):\n",
    "    expanded_text = contractions.fix(text)\n",
    "    sentences = sent_tokenize(expanded_text)\n",
    "    tokenized_sentences = [word_tokenize(sentence) for sentence in sentences]\n",
    "    recombined_sentences = []\n",
    "\n",
    "    # Recombine the tokens that NLTK split (as well as convert them to lowercase)\n",
    "    for sentence in tokenized_sentences:\n",
    "        recombined_sentence = []\n",
    "        skip_counter = 0\n",
    "        for i, token in enumerate(sentence):\n",
    "            if skip_counter:\n",
    "                skip_counter -= 1\n",
    "                continue\n",
    "            if token == '<' and i + 2 < len(sentence) and sentence[i + 2] == '>':\n",
    "                recombined_sentence.append('<' + sentence[i + 1] + '>')\n",
    "                skip_counter = 2\n",
    "            else:\n",
    "                recombined_sentence.append(token.lower())\n",
    "        recombined_sentences.append(recombined_sentence)\n",
    "    return recombined_sentences\n",
    "raw_data['tokenized_body_text'] = raw_data['body_text'].apply(tokenize_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e310b880-8046-4d8b-b006-124629c30b01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Word2Vec on text data\n",
    "tokenized_sentences = [sentence for sentences in raw_data['tokenized_body_text'] for sentence in sentences]\n",
    "\n",
    "word2vec_model_100 = Word2Vec(sentences=tokenized_sentences, vector_size=100, window=5, min_count=1, workers=4, epochs=10)\n",
    "word2vec_model_100.save(\"embeddings/custom_word2vec_100.model\")\n",
    "\n",
    "word2vec_model_300 = Word2Vec(sentences=tokenized_sentences, vector_size=300, window=5, min_count=1, workers=4, epochs=10)\n",
    "word2vec_model_300.save(\"embeddings/custom_word2vec_300.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9ec0df1a-3b2c-43bb-ade5-73e8cce6b8db",
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec_model_100 = Word2Vec.load('embeddings/custom_word2vec_100.model')\n",
    "word2vec_model_300 = Word2Vec.load('embeddings/custom_word2vec_300.model')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3684ad8b-06a2-4dbc-9788-05b520f49d21",
   "metadata": {},
   "source": [
    "### Data Prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "27eb3f48-1298-4921-b2a5-a0637fab037d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get rid of all tags I didn't intentionally scrape\n",
    "rel_tags = [\n",
    "    'real-analysis',\n",
    "    'calculus',\n",
    "    'linear-algebra',\n",
    "    'probability',\n",
    "    'abstract-algebra',\n",
    "    'integration',\n",
    "    'sequences-and-series',\n",
    "    'combinatorics',\n",
    "    'general-topology',\n",
    "    'matrices',\n",
    "    'functional-analysis',\n",
    "    'complex-analysis',\n",
    "    'geometry',\n",
    "    'group-theory',\n",
    "    'algebra-precalculus',\n",
    "    'probability-theory',\n",
    "    'ordinary-differential-equations',\n",
    "    'limits',\n",
    "    'analysis',\n",
    "    'number-theory',\n",
    "    'measure-theory',\n",
    "    'statistics',\n",
    "    'multivariable-calculus',\n",
    "    'functions',\n",
    "    'derivatives',\n",
    "    'differential-geometry',\n",
    "    'discrete-mathematics',\n",
    "    'trigonometry',\n",
    "    'algebraic-geometry',\n",
    "    'elementary-set-theory'\n",
    "]\n",
    "raw_data['tags'] = raw_data['tags'].apply(lambda x: [tag for tag in x if tag in rel_tags])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "97e17a14-6ce0-4f17-99b8-7ae77322ac5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of data tensor: (235312, 500)\n",
      "Shape of label tensor: (235312, 30)\n"
     ]
    }
   ],
   "source": [
    "# Data prep\n",
    "tokenized_texts = [sum(question, []) for question in raw_data['tokenized_body_text'].tolist()]\n",
    "labels = raw_data['tags'].tolist()\n",
    "\n",
    "# Create word-to-index mapping\n",
    "word_index = {}\n",
    "index = 1\n",
    "for question in tokenized_texts:\n",
    "    for token in question:\n",
    "        if token not in word_index:\n",
    "            word_index[token] = index\n",
    "            index += 1\n",
    "\n",
    "# Convert tokens to sequences of integers\n",
    "sequences = [[word_index[token] for token in question] for question in tokenized_texts]\n",
    "\n",
    "# Define maximum sequence length\n",
    "MAX_SEQ_LEN = 500\n",
    "\n",
    "# Pad sequences\n",
    "data = pad_sequences(sequences, maxlen=MAX_SEQ_LEN)\n",
    "\n",
    "# Encode labels\n",
    "mlb = MultiLabelBinarizer()\n",
    "categorical_labels = mlb.fit_transform(labels)\n",
    "\n",
    "# Check the shapes\n",
    "print(f'Shape of data tensor: {data.shape}')\n",
    "print(f'Shape of label tensor: {categorical_labels.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "847a5f51-0ef4-4e86-83e8-8fd693db6230",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data shape: (188249, 500)\n",
      "Validation data shape: (23531, 500)\n",
      "Test data shape: (23532, 500)\n"
     ]
    }
   ],
   "source": [
    "# Split the data into train (80%), validation (10%), and test (10%)\n",
    "data_train, data_temp, labels_train, labels_temp = train_test_split(data, categorical_labels, test_size=0.2, random_state=42)\n",
    "data_val, data_test, labels_val, labels_test = train_test_split(data_temp, labels_temp, test_size=0.5, random_state=42)\n",
    "\n",
    "# Check the shapes\n",
    "print(f'Training data shape: {data_train.shape}')\n",
    "print(f'Validation data shape: {data_val.shape}')\n",
    "print(f'Test data shape: {data_test.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a7f8705-603c-4e18-b3d2-df0db66e7d32",
   "metadata": {},
   "source": [
    "### Baseline Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ae1701bd-7f6c-4170-a6a0-5c110e68f533",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create embedding matrix\n",
    "embedding_dim = word2vec_model_300.vector_size\n",
    "embedding_matrix = np.zeros((len(word_index) + 1, embedding_dim))\n",
    "\n",
    "for word, i in word_index.items():\n",
    "    if word in word2vec_model_300.wv:\n",
    "        embedding_matrix[i] = word2vec_model_300.wv[word]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f0d746d2-1548-4fbd-bf8b-fb829ba69f8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom metric\n",
    "def micro_precision(y_true, y_pred):\n",
    "    y_true = tf.cast(y_true, tf.float32)\n",
    "    y_pred = tf.round(y_pred)\n",
    "    y_pred = tf.cast(y_pred, tf.float32)\n",
    "    \n",
    "    true_positives = K.sum(K.cast(y_true * y_pred, 'float'))\n",
    "    predicted_positives = K.sum(K.cast(y_pred, 'float'))\n",
    "    \n",
    "    precision = true_positives / (predicted_positives + K.epsilon())\n",
    "    return precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8c5f7bba-b68f-4eaf-869e-aa4f3b8505bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def micro_f1_score(y_true, y_pred):\n",
    "    y_true = tf.cast(y_true, tf.float32)\n",
    "    y_pred = tf.round(y_pred)\n",
    "    y_pred = tf.cast(y_pred, tf.float32)\n",
    "    \n",
    "    tp = K.sum(K.cast(y_true * y_pred, 'float'))\n",
    "    fp = K.sum(K.cast((1 - y_true) * y_pred, 'float'))\n",
    "    fn = K.sum(K.cast(y_true * (1 - y_pred), 'float'))\n",
    "    \n",
    "    precision = tp / (tp + fp + K.epsilon())\n",
    "    recall = tp / (tp + fn + K.epsilon())\n",
    "    \n",
    "    f1 = 2 * (precision * recall) / (precision + recall + K.epsilon())\n",
    "    return f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8c705686-ba0f-4d21-98fd-e37ed97a77b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(model, y_true):\n",
    "    # Test\n",
    "    predictions = model.predict(data_val)\n",
    "    predictions = np.round(predictions)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    hamming = hamming_loss(y_true, predictions)\n",
    "    f1 = f1_score(y_true, predictions, average='micro')\n",
    "    precision = precision_score(y_true, predictions, average='micro')\n",
    "    recall = recall_score(y_true, predictions, average='micro')\n",
    "    subset_acc = accuracy_score(y_true, predictions)\n",
    "    \n",
    "    # Print results\n",
    "    print(f'Hamming Loss: {hamming}')\n",
    "    print(f'F1 Score: {f1}')\n",
    "    print(f'Precision: {precision}')\n",
    "    print(f'Recall: {recall}')\n",
    "    print(f'Subset Accuracy: {subset_acc}')\n",
    "\n",
    "    # Return dictionary with results\n",
    "    results = {'hamming':hamming, 'f1':f1, 'precision':precision, 'recall':recall, 'subset_acc':subset_acc}\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "25e4d6c3-a1b0-496a-a7ca-955d2d370614",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n",
      "1471/1471 [==============================] - 94s 63ms/step - loss: 0.1362 - micro_precision: 0.5651 - micro_f1_score: 0.3251 - val_loss: 0.1034 - val_micro_precision: 0.7502 - val_micro_f1_score: 0.5451\n",
      "Epoch 2/4\n",
      "1471/1471 [==============================] - 90s 61ms/step - loss: 0.0981 - micro_precision: 0.7568 - micro_f1_score: 0.5786 - val_loss: 0.0974 - val_micro_precision: 0.7529 - val_micro_f1_score: 0.5845\n",
      "Epoch 3/4\n",
      "1471/1471 [==============================] - 90s 61ms/step - loss: 0.0920 - micro_precision: 0.7645 - micro_f1_score: 0.6107 - val_loss: 0.0952 - val_micro_precision: 0.7476 - val_micro_f1_score: 0.6023\n",
      "Epoch 4/4\n",
      "1471/1471 [==============================] - 90s 61ms/step - loss: 0.0866 - micro_precision: 0.7763 - micro_f1_score: 0.6375 - val_loss: 0.0950 - val_micro_precision: 0.7483 - val_micro_f1_score: 0.6049\n"
     ]
    }
   ],
   "source": [
    "# Build base model\n",
    "base_custom_model = Sequential()\n",
    "base_custom_model.add(Embedding(input_dim = len(word_index) + 1,\n",
    "                    output_dim = 300,\n",
    "                    weights=[embedding_matrix],\n",
    "                    trainable=True))\n",
    "base_custom_model.add(LSTM(units=128, return_sequences=False))\n",
    "base_custom_model.add(Dense(len(mlb.classes_), activation='sigmoid'))\n",
    "\n",
    "\n",
    "# Compile the model\n",
    "base_custom_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=[micro_precision, micro_f1_score])\n",
    "\n",
    "# Create callback to store intermediate models\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    filepath='weights/base_custom_embedding/model_epoch_{epoch:02d}.weights.h5',\n",
    "    save_weights_only=True,\n",
    "    save_best_only=False,\n",
    "    save_freq='epoch'  # Save every epoch\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "with tf.device('/GPU:0'):\n",
    "    base_custom_history = base_custom_model.fit(data_train, labels_train, epochs=4, batch_size=128, validation_data=(data_val, labels_val), callbacks=[checkpoint_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0fa00973-1a56-41f0-bffb-8074893bc580",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "736/736 [==============================] - 8s 11ms/step\n",
      "Hamming Loss: 0.034163443967532195\n",
      "F1 Score: 0.6050084347413073\n",
      "Precision: 0.7483186127542338\n",
      "Recall: 0.507766323024055\n",
      "Subset Accuracy: 0.3714249288173048\n"
     ]
    }
   ],
   "source": [
    "base_custom_results = test_model(base_custom_model, labels_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a7670539-5160-4f6d-bb0f-3d63401f841a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Google news embedding matrix\n",
    "path = 'embeddings/GoogleNews-vectors-negative300.bin'\n",
    "google_word2vec_model = KeyedVectors.load_word2vec_format(path, binary=True)\n",
    "\n",
    "embedding_dim = google_word2vec_model.vector_size\n",
    "google_embedding_matrix = np.zeros((len(word_index) + 1, embedding_dim))\n",
    "\n",
    "for word, i in word_index.items():\n",
    "    if word in google_word2vec_model:\n",
    "        google_embedding_matrix[i] = google_word2vec_model[word]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6d0517b7-bea2-4dfd-abbc-7ff57c195595",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n",
      "1471/1471 [==============================] - 95s 64ms/step - loss: 0.1974 - micro_precision: 3.5655e-04 - micro_f1_score: 4.0793e-04 - val_loss: 0.1671 - val_micro_precision: 0.0054 - val_micro_f1_score: 5.2510e-05\n",
      "Epoch 2/4\n",
      "1471/1471 [==============================] - 93s 63ms/step - loss: 0.1283 - micro_precision: 0.6637 - micro_f1_score: 0.3151 - val_loss: 0.1113 - val_micro_precision: 0.7429 - val_micro_f1_score: 0.5015\n",
      "Epoch 3/4\n",
      "1471/1471 [==============================] - 92s 62ms/step - loss: 0.1010 - micro_precision: 0.7570 - micro_f1_score: 0.5662 - val_loss: 0.1001 - val_micro_precision: 0.7564 - val_micro_f1_score: 0.5791\n",
      "Epoch 4/4\n",
      "1471/1471 [==============================] - 90s 61ms/step - loss: 0.0913 - micro_precision: 0.7703 - micro_f1_score: 0.6203 - val_loss: 0.0981 - val_micro_precision: 0.7343 - val_micro_f1_score: 0.6022\n"
     ]
    }
   ],
   "source": [
    "# Build base model\n",
    "base_google_model = Sequential()\n",
    "base_google_model.add(Embedding(input_dim = len(word_index) + 1,\n",
    "                    output_dim = 300,\n",
    "                    weights=[google_embedding_matrix],\n",
    "                    trainable=True))\n",
    "base_google_model.add(LSTM(units=128, return_sequences=False))\n",
    "base_google_model.add(Dense(len(mlb.classes_), activation='sigmoid'))\n",
    "\n",
    "# Compile the model\n",
    "base_google_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=[micro_precision, micro_f1_score])\n",
    "\n",
    "# Create callback to store intermediate models\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    filepath='weights/base_google_embedding/model_epoch_{epoch:02d}.weights.h5',\n",
    "    save_weights_only=True,\n",
    "    save_best_only=False,\n",
    "    save_freq='epoch'  # Save every epoch\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "google_history = base_google_model.fit(data_train, labels_train, epochs=4, batch_size=128, validation_data=(data_val, labels_val), callbacks=[checkpoint_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "38de2827-9a92-4c10-8178-86d32cfb27d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "736/736 [==============================] - 8s 11ms/step\n",
      "Hamming Loss: 0.03473007238678056\n",
      "F1 Score: 0.602454962624248\n",
      "Precision: 0.7343848829854522\n",
      "Recall: 0.5107079037800687\n",
      "Subset Accuracy: 0.37176490586885386\n"
     ]
    }
   ],
   "source": [
    "base_google_results = test_model(base_google_model, labels_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef331221-4458-4f59-adf2-0e6a968c8b91",
   "metadata": {},
   "source": [
    "### Expanded Model and Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5744ec5f-ee51-40b5-b516-de959637b168",
   "metadata": {},
   "source": [
    "Here, I'm going to add an extra LSTM layer as well as two dropout layers for regularization. From here, I'll adjust hyperparameters. I'll be using the custom embedding matrix as initial weights since they seemed to perform slightly better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "078237cc-5a26-4c40-bef6-4f6170fa3925",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to create arbitrary model\n",
    "def create_model(lstm_units, dropout_rate, learning_rate):\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(input_dim=len(word_index) + 1,\n",
    "                        output_dim=300,\n",
    "                        weights=[embedding_matrix],\n",
    "                        trainable=False))\n",
    "    model.add(LSTM(units=lstm_units, return_sequences=True))\n",
    "    model.add(Dropout(dropout_rate))\n",
    "    model.add(LSTM(units=lstm_units, return_sequences=False))\n",
    "    model.add(Dropout(dropout_rate))\n",
    "    model.add(Dense(len(mlb.classes_), activation='sigmoid'))\n",
    "\n",
    "    optimizer = Adam(learning_rate=learning_rate)\n",
    "    model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=[micro_precision, micro_f1_score])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "76c9b934-d044-4c05-b910-06d0e0249b75",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with batch_size=32, lstm_units=64, dropout_rate=0.3, learning_rate=0.001\n",
      "Epoch 1/10\n",
      "5883/5883 [==============================] - 232s 38ms/step - loss: 0.1448 - micro_precision: 0.5380 - micro_f1_score: 0.2622 - val_loss: 0.1075 - val_micro_precision: 0.7595 - val_micro_f1_score: 0.5015\n",
      "Epoch 2/10\n",
      "5883/5883 [==============================] - 225s 38ms/step - loss: 0.1082 - micro_precision: 0.7413 - micro_f1_score: 0.5211 - val_loss: 0.0991 - val_micro_precision: 0.7691 - val_micro_f1_score: 0.5596\n",
      "Epoch 3/10\n",
      "5883/5883 [==============================] - 230s 39ms/step - loss: 0.1026 - micro_precision: 0.7550 - micro_f1_score: 0.5583 - val_loss: 0.0964 - val_micro_precision: 0.7760 - val_micro_f1_score: 0.5732\n",
      "Epoch 4/10\n",
      "5883/5883 [==============================] - 229s 39ms/step - loss: 0.0998 - micro_precision: 0.7594 - micro_f1_score: 0.5747 - val_loss: 0.0949 - val_micro_precision: 0.7702 - val_micro_f1_score: 0.5868\n",
      "Best val_f1 so far: 0.5867583155632019 with hyperparams: {'batch_size': 32, 'lstm_units': 64, 'dropout_rate': 0.3, 'learning_rate': 0.001}\n",
      "Training with batch_size=32, lstm_units=64, dropout_rate=0.3, learning_rate=0.0001\n",
      "Epoch 1/10\n",
      "5883/5883 [==============================] - 232s 39ms/step - loss: 0.2020 - micro_precision: 0.1398 - micro_f1_score: 0.0131 - val_loss: 0.1554 - val_micro_precision: 0.5067 - val_micro_f1_score: 0.0334\n",
      "Epoch 2/10\n",
      "5883/5883 [==============================] - 226s 38ms/step - loss: 0.1481 - micro_precision: 0.5919 - micro_f1_score: 0.1825 - val_loss: 0.1278 - val_micro_precision: 0.7258 - val_micro_f1_score: 0.2804\n",
      "Epoch 3/10\n",
      "5883/5883 [==============================] - 223s 38ms/step - loss: 0.1310 - micro_precision: 0.6637 - micro_f1_score: 0.3343 - val_loss: 0.1173 - val_micro_precision: 0.7440 - val_micro_f1_score: 0.4054\n",
      "Epoch 4/10\n",
      "5883/5883 [==============================] - 225s 38ms/step - loss: 0.1218 - micro_precision: 0.6977 - micro_f1_score: 0.4168 - val_loss: 0.1109 - val_micro_precision: 0.7588 - val_micro_f1_score: 0.4761\n",
      "Best val_f1 so far: 0.4760870337486267 with hyperparams: {'batch_size': 32, 'lstm_units': 64, 'dropout_rate': 0.3, 'learning_rate': 0.001}\n",
      "Training with batch_size=32, lstm_units=64, dropout_rate=0.5, learning_rate=0.001\n",
      "Epoch 1/10\n",
      "5883/5883 [==============================] - 226s 38ms/step - loss: 0.1568 - micro_precision: 0.4880 - micro_f1_score: 0.1820 - val_loss: 0.1157 - val_micro_precision: 0.7725 - val_micro_f1_score: 0.4027\n",
      "Epoch 2/10\n",
      "5883/5883 [==============================] - 230s 39ms/step - loss: 0.1186 - micro_precision: 0.7341 - micro_f1_score: 0.4411 - val_loss: 0.1033 - val_micro_precision: 0.7885 - val_micro_f1_score: 0.5057\n",
      "Epoch 3/10\n",
      "5883/5883 [==============================] - 225s 38ms/step - loss: 0.1115 - micro_precision: 0.7501 - micro_f1_score: 0.4985 - val_loss: 0.1002 - val_micro_precision: 0.7850 - val_micro_f1_score: 0.5351\n",
      "Epoch 4/10\n",
      "5883/5883 [==============================] - 225s 38ms/step - loss: 0.1084 - micro_precision: 0.7554 - micro_f1_score: 0.5164 - val_loss: 0.0991 - val_micro_precision: 0.7862 - val_micro_f1_score: 0.5453\n",
      "Best val_f1 so far: 0.5452804565429688 with hyperparams: {'batch_size': 32, 'lstm_units': 64, 'dropout_rate': 0.3, 'learning_rate': 0.001}\n",
      "Training with batch_size=32, lstm_units=64, dropout_rate=0.5, learning_rate=0.0001\n",
      "Epoch 1/10\n",
      "5883/5883 [==============================] - 228s 38ms/step - loss: 0.2110 - micro_precision: 0.1280 - micro_f1_score: 0.0135 - val_loss: 0.1573 - val_micro_precision: 0.0000e+00 - val_micro_f1_score: 0.0000e+00\n",
      "Epoch 2/10\n",
      "5883/5883 [==============================] - 226s 38ms/step - loss: 0.1566 - micro_precision: 0.5280 - micro_f1_score: 0.1489 - val_loss: 0.1308 - val_micro_precision: 0.7365 - val_micro_f1_score: 0.2282\n",
      "Epoch 3/10\n",
      "5883/5883 [==============================] - 223s 38ms/step - loss: 0.1386 - micro_precision: 0.6424 - micro_f1_score: 0.2808 - val_loss: 0.1195 - val_micro_precision: 0.7606 - val_micro_f1_score: 0.3653\n",
      "Epoch 4/10\n",
      "5883/5883 [==============================] - 223s 38ms/step - loss: 0.1292 - micro_precision: 0.6875 - micro_f1_score: 0.3537 - val_loss: 0.1132 - val_micro_precision: 0.7730 - val_micro_f1_score: 0.4168\n",
      "Best val_f1 so far: 0.4168027937412262 with hyperparams: {'batch_size': 32, 'lstm_units': 64, 'dropout_rate': 0.3, 'learning_rate': 0.001}\n",
      "Training with batch_size=32, lstm_units=64, dropout_rate=0.7, learning_rate=0.001\n",
      "Epoch 1/10\n",
      "5883/5883 [==============================] - 229s 39ms/step - loss: 0.1736 - micro_precision: 0.3387 - micro_f1_score: 0.0710 - val_loss: 0.1254 - val_micro_precision: 0.7762 - val_micro_f1_score: 0.2448\n",
      "Epoch 2/10\n",
      "5883/5883 [==============================] - 228s 39ms/step - loss: 0.1347 - micro_precision: 0.6989 - micro_f1_score: 0.2782 - val_loss: 0.1144 - val_micro_precision: 0.8030 - val_micro_f1_score: 0.3597\n",
      "Epoch 3/10\n",
      "5883/5883 [==============================] - 227s 39ms/step - loss: 0.1270 - micro_precision: 0.7219 - micro_f1_score: 0.3536 - val_loss: 0.1090 - val_micro_precision: 0.7971 - val_micro_f1_score: 0.4387\n",
      "Epoch 4/10\n",
      "5883/5883 [==============================] - 227s 39ms/step - loss: 0.1235 - micro_precision: 0.7291 - micro_f1_score: 0.3866 - val_loss: 0.1073 - val_micro_precision: 0.7848 - val_micro_f1_score: 0.4684\n",
      "Best val_f1 so far: 0.46842435002326965 with hyperparams: {'batch_size': 32, 'lstm_units': 64, 'dropout_rate': 0.3, 'learning_rate': 0.001}\n",
      "Training with batch_size=32, lstm_units=64, dropout_rate=0.7, learning_rate=0.0001\n",
      "Epoch 1/10\n",
      "5883/5883 [==============================] - 227s 38ms/step - loss: 0.2324 - micro_precision: 0.0437 - micro_f1_score: 0.0089 - val_loss: 0.1757 - val_micro_precision: 0.0000e+00 - val_micro_f1_score: 0.0000e+00\n",
      "Epoch 2/10\n",
      "5883/5883 [==============================] - 224s 38ms/step - loss: 0.1773 - micro_precision: 0.3826 - micro_f1_score: 0.0500 - val_loss: 0.1432 - val_micro_precision: 0.6903 - val_micro_f1_score: 0.0921\n",
      "Epoch 3/10\n",
      "5883/5883 [==============================] - 225s 38ms/step - loss: 0.1558 - micro_precision: 0.5884 - micro_f1_score: 0.1415 - val_loss: 0.1295 - val_micro_precision: 0.7103 - val_micro_f1_score: 0.2030\n",
      "Epoch 4/10\n",
      "5883/5883 [==============================] - 229s 39ms/step - loss: 0.1455 - micro_precision: 0.6442 - micro_f1_score: 0.1979 - val_loss: 0.1235 - val_micro_precision: 0.7537 - val_micro_f1_score: 0.2719\n",
      "Best val_f1 so far: 0.271894246339798 with hyperparams: {'batch_size': 32, 'lstm_units': 64, 'dropout_rate': 0.3, 'learning_rate': 0.001}\n",
      "Training with batch_size=32, lstm_units=128, dropout_rate=0.3, learning_rate=0.001\n",
      "Epoch 1/10\n",
      "5883/5883 [==============================] - 344s 58ms/step - loss: 0.1332 - micro_precision: 0.5958 - micro_f1_score: 0.3442 - val_loss: 0.1033 - val_micro_precision: 0.7559 - val_micro_f1_score: 0.5336\n",
      "Epoch 2/10\n",
      "5883/5883 [==============================] - 339s 58ms/step - loss: 0.1020 - micro_precision: 0.7473 - micro_f1_score: 0.5623 - val_loss: 0.0959 - val_micro_precision: 0.7620 - val_micro_f1_score: 0.5867\n",
      "Epoch 3/10\n",
      "5883/5883 [==============================] - 337s 57ms/step - loss: 0.0966 - micro_precision: 0.7558 - micro_f1_score: 0.5920 - val_loss: 0.0931 - val_micro_precision: 0.7616 - val_micro_f1_score: 0.6036\n",
      "Epoch 4/10\n",
      "5883/5883 [==============================] - 339s 58ms/step - loss: 0.0937 - micro_precision: 0.7619 - micro_f1_score: 0.6062 - val_loss: 0.0927 - val_micro_precision: 0.7634 - val_micro_f1_score: 0.6065\n",
      "Best val_f1 so far: 0.6065341234207153 with hyperparams: {'batch_size': 32, 'lstm_units': 128, 'dropout_rate': 0.3, 'learning_rate': 0.001}\n",
      "Training with batch_size=32, lstm_units=128, dropout_rate=0.3, learning_rate=0.0001\n",
      "Epoch 1/10\n",
      "5883/5883 [==============================] - 343s 58ms/step - loss: 0.1657 - micro_precision: 0.4182 - micro_f1_score: 0.1636 - val_loss: 0.1222 - val_micro_precision: 0.7097 - val_micro_f1_score: 0.4048\n",
      "Epoch 2/10\n",
      "5883/5883 [==============================] - 337s 57ms/step - loss: 0.1197 - micro_precision: 0.6951 - micro_f1_score: 0.4538 - val_loss: 0.1082 - val_micro_precision: 0.7350 - val_micro_f1_score: 0.5153\n",
      "Epoch 3/10\n",
      "5883/5883 [==============================] - 339s 58ms/step - loss: 0.1094 - micro_precision: 0.7264 - micro_f1_score: 0.5189 - val_loss: 0.1023 - val_micro_precision: 0.7615 - val_micro_f1_score: 0.5420\n",
      "Epoch 4/10\n",
      "5883/5883 [==============================] - 337s 57ms/step - loss: 0.1041 - micro_precision: 0.7385 - micro_f1_score: 0.5485 - val_loss: 0.0991 - val_micro_precision: 0.7515 - val_micro_f1_score: 0.5702\n",
      "Best val_f1 so far: 0.570151686668396 with hyperparams: {'batch_size': 32, 'lstm_units': 128, 'dropout_rate': 0.3, 'learning_rate': 0.001}\n",
      "Training with batch_size=32, lstm_units=128, dropout_rate=0.5, learning_rate=0.001\n",
      "Epoch 1/10\n",
      "5883/5883 [==============================] - 344s 58ms/step - loss: 0.1375 - micro_precision: 0.5913 - micro_f1_score: 0.3291 - val_loss: 0.1049 - val_micro_precision: 0.7576 - val_micro_f1_score: 0.5206\n",
      "Epoch 2/10\n",
      "5883/5883 [==============================] - 336s 57ms/step - loss: 0.1072 - micro_precision: 0.7427 - micro_f1_score: 0.5366 - val_loss: 0.0978 - val_micro_precision: 0.7792 - val_micro_f1_score: 0.5603\n",
      "Epoch 3/10\n",
      "5883/5883 [==============================] - 335s 57ms/step - loss: 0.1019 - micro_precision: 0.7540 - micro_f1_score: 0.5673 - val_loss: 0.0947 - val_micro_precision: 0.7644 - val_micro_f1_score: 0.5925\n",
      "Epoch 4/10\n",
      "5883/5883 [==============================] - 335s 57ms/step - loss: 0.0992 - micro_precision: 0.7584 - micro_f1_score: 0.5801 - val_loss: 0.0943 - val_micro_precision: 0.7728 - val_micro_f1_score: 0.5874\n",
      "Best val_f1 so far: 0.5924867391586304 with hyperparams: {'batch_size': 32, 'lstm_units': 128, 'dropout_rate': 0.3, 'learning_rate': 0.001}\n",
      "Training with batch_size=32, lstm_units=128, dropout_rate=0.5, learning_rate=0.0001\n",
      "Epoch 1/10\n",
      "5883/5883 [==============================] - 341s 58ms/step - loss: 0.1781 - micro_precision: 0.3774 - micro_f1_score: 0.1384 - val_loss: 0.1248 - val_micro_precision: 0.7291 - val_micro_f1_score: 0.3523\n",
      "Epoch 2/10\n",
      "5883/5883 [==============================] - 337s 57ms/step - loss: 0.1258 - micro_precision: 0.6801 - micro_f1_score: 0.4117 - val_loss: 0.1090 - val_micro_precision: 0.7508 - val_micro_f1_score: 0.4920\n",
      "Epoch 3/10\n",
      "5883/5883 [==============================] - 334s 57ms/step - loss: 0.1135 - micro_precision: 0.7232 - micro_f1_score: 0.4971 - val_loss: 0.1026 - val_micro_precision: 0.7534 - val_micro_f1_score: 0.5418\n",
      "Epoch 4/10\n",
      "5883/5883 [==============================] - 333s 57ms/step - loss: 0.1079 - micro_precision: 0.7375 - micro_f1_score: 0.5313 - val_loss: 0.0997 - val_micro_precision: 0.7650 - val_micro_f1_score: 0.5519\n",
      "Best val_f1 so far: 0.551878035068512 with hyperparams: {'batch_size': 32, 'lstm_units': 128, 'dropout_rate': 0.3, 'learning_rate': 0.001}\n",
      "Training with batch_size=32, lstm_units=128, dropout_rate=0.7, learning_rate=0.001\n",
      "Epoch 1/10\n",
      "5883/5883 [==============================] - 337s 57ms/step - loss: 0.1515 - micro_precision: 0.5289 - micro_f1_score: 0.2315 - val_loss: 0.1109 - val_micro_precision: 0.7829 - val_micro_f1_score: 0.4349\n",
      "Epoch 2/10\n",
      "5883/5883 [==============================] - 333s 57ms/step - loss: 0.1180 - micro_precision: 0.7355 - micro_f1_score: 0.4581 - val_loss: 0.1035 - val_micro_precision: 0.7834 - val_micro_f1_score: 0.5019\n",
      "Epoch 3/10\n",
      "5883/5883 [==============================] - 332s 56ms/step - loss: 0.1128 - micro_precision: 0.7476 - micro_f1_score: 0.4930 - val_loss: 0.1021 - val_micro_precision: 0.7703 - val_micro_f1_score: 0.5340\n",
      "Epoch 4/10\n",
      "5883/5883 [==============================] - 331s 56ms/step - loss: 0.1100 - micro_precision: 0.7532 - micro_f1_score: 0.5101 - val_loss: 0.1001 - val_micro_precision: 0.7840 - val_micro_f1_score: 0.5447\n",
      "Best val_f1 so far: 0.5447073578834534 with hyperparams: {'batch_size': 32, 'lstm_units': 128, 'dropout_rate': 0.3, 'learning_rate': 0.001}\n",
      "Training with batch_size=32, lstm_units=128, dropout_rate=0.7, learning_rate=0.0001\n",
      "Epoch 1/10\n",
      "5883/5883 [==============================] - 336s 57ms/step - loss: 0.2012 - micro_precision: 0.2702 - micro_f1_score: 0.0611 - val_loss: 0.1391 - val_micro_precision: 0.7075 - val_micro_f1_score: 0.1745\n",
      "Epoch 2/10\n",
      "5883/5883 [==============================] - 331s 56ms/step - loss: 0.1422 - micro_precision: 0.6185 - micro_f1_score: 0.2806 - val_loss: 0.1162 - val_micro_precision: 0.7678 - val_micro_f1_score: 0.4032\n",
      "Epoch 3/10\n",
      "5883/5883 [==============================] - 331s 56ms/step - loss: 0.1257 - micro_precision: 0.6975 - micro_f1_score: 0.4057 - val_loss: 0.1083 - val_micro_precision: 0.7748 - val_micro_f1_score: 0.4738\n",
      "Epoch 4/10\n",
      "5883/5883 [==============================] - 320s 54ms/step - loss: 0.1185 - micro_precision: 0.7230 - micro_f1_score: 0.4619 - val_loss: 0.1050 - val_micro_precision: 0.7802 - val_micro_f1_score: 0.5018\n",
      "Best val_f1 so far: 0.5018205046653748 with hyperparams: {'batch_size': 32, 'lstm_units': 128, 'dropout_rate': 0.3, 'learning_rate': 0.001}\n",
      "Training with batch_size=32, lstm_units=256, dropout_rate=0.3, learning_rate=0.001\n",
      "Epoch 1/10\n",
      "5883/5883 [==============================] - 387s 65ms/step - loss: 0.1234 - micro_precision: 0.6372 - micro_f1_score: 0.4159 - val_loss: 0.1015 - val_micro_precision: 0.7484 - val_micro_f1_score: 0.5538\n",
      "Epoch 2/10\n",
      "5883/5883 [==============================] - 387s 66ms/step - loss: 0.1003 - micro_precision: 0.7452 - micro_f1_score: 0.5725 - val_loss: 0.0958 - val_micro_precision: 0.7563 - val_micro_f1_score: 0.5868\n",
      "Epoch 3/10\n",
      "5883/5883 [==============================] - 382s 65ms/step - loss: 0.0949 - micro_precision: 0.7561 - micro_f1_score: 0.5995 - val_loss: 0.0931 - val_micro_precision: 0.7553 - val_micro_f1_score: 0.6071\n",
      "Epoch 4/10\n",
      "5883/5883 [==============================] - 385s 65ms/step - loss: 0.0917 - micro_precision: 0.7637 - micro_f1_score: 0.6153 - val_loss: 0.0917 - val_micro_precision: 0.7551 - val_micro_f1_score: 0.6156\n",
      "Best val_f1 so far: 0.6156092882156372 with hyperparams: {'batch_size': 32, 'lstm_units': 256, 'dropout_rate': 0.3, 'learning_rate': 0.001}\n",
      "Training with batch_size=32, lstm_units=256, dropout_rate=0.3, learning_rate=0.0001\n",
      "Epoch 1/10\n",
      "5883/5883 [==============================] - 383s 65ms/step - loss: 0.1432 - micro_precision: 0.5459 - micro_f1_score: 0.3111 - val_loss: 0.1093 - val_micro_precision: 0.7246 - val_micro_f1_score: 0.5069\n",
      "Epoch 2/10\n",
      "5883/5883 [==============================] - 384s 65ms/step - loss: 0.1063 - micro_precision: 0.7261 - micro_f1_score: 0.5376 - val_loss: 0.0999 - val_micro_precision: 0.7451 - val_micro_f1_score: 0.5684\n",
      "Epoch 3/10\n",
      "5883/5883 [==============================] - 385s 65ms/step - loss: 0.0989 - micro_precision: 0.7433 - micro_f1_score: 0.5776 - val_loss: 0.0960 - val_micro_precision: 0.7558 - val_micro_f1_score: 0.5874\n",
      "Epoch 4/10\n",
      "5883/5883 [==============================] - 382s 65ms/step - loss: 0.0947 - micro_precision: 0.7534 - micro_f1_score: 0.5989 - val_loss: 0.0938 - val_micro_precision: 0.7580 - val_micro_f1_score: 0.6032\n",
      "Best val_f1 so far: 0.6031637787818909 with hyperparams: {'batch_size': 32, 'lstm_units': 256, 'dropout_rate': 0.3, 'learning_rate': 0.001}\n",
      "Training with batch_size=32, lstm_units=256, dropout_rate=0.5, learning_rate=0.001\n",
      "Epoch 1/10\n",
      "5883/5883 [==============================] - 384s 65ms/step - loss: 0.1298 - micro_precision: 0.6195 - micro_f1_score: 0.3803 - val_loss: 0.1030 - val_micro_precision: 0.7426 - val_micro_f1_score: 0.5456\n",
      "Epoch 2/10\n",
      "5883/5883 [==============================] - 384s 65ms/step - loss: 0.1024 - micro_precision: 0.7437 - micro_f1_score: 0.5627 - val_loss: 0.0959 - val_micro_precision: 0.7676 - val_micro_f1_score: 0.5800\n",
      "Epoch 3/10\n",
      "5883/5883 [==============================] - 383s 65ms/step - loss: 0.0975 - micro_precision: 0.7553 - micro_f1_score: 0.5899 - val_loss: 0.0932 - val_micro_precision: 0.7714 - val_micro_f1_score: 0.5955\n",
      "Epoch 4/10\n",
      "5883/5883 [==============================] - 385s 65ms/step - loss: 0.0951 - micro_precision: 0.7602 - micro_f1_score: 0.6018 - val_loss: 0.0930 - val_micro_precision: 0.7657 - val_micro_f1_score: 0.6010\n",
      "Best val_f1 so far: 0.6009848713874817 with hyperparams: {'batch_size': 32, 'lstm_units': 256, 'dropout_rate': 0.3, 'learning_rate': 0.001}\n",
      "Training with batch_size=32, lstm_units=256, dropout_rate=0.5, learning_rate=0.0001\n",
      "Epoch 1/10\n",
      "5883/5883 [==============================] - 384s 65ms/step - loss: 0.1544 - micro_precision: 0.4834 - micro_f1_score: 0.2524 - val_loss: 0.1114 - val_micro_precision: 0.7436 - val_micro_f1_score: 0.4730\n",
      "Epoch 2/10\n",
      "5883/5883 [==============================] - 383s 65ms/step - loss: 0.1109 - micro_precision: 0.7159 - micro_f1_score: 0.5154 - val_loss: 0.1011 - val_micro_precision: 0.7564 - val_micro_f1_score: 0.5485\n",
      "Epoch 3/10\n",
      "5883/5883 [==============================] - 379s 64ms/step - loss: 0.1025 - micro_precision: 0.7389 - micro_f1_score: 0.5602 - val_loss: 0.0977 - val_micro_precision: 0.7585 - val_micro_f1_score: 0.5715\n",
      "Epoch 4/10\n",
      "5883/5883 [==============================] - 380s 65ms/step - loss: 0.0980 - micro_precision: 0.7488 - micro_f1_score: 0.5849 - val_loss: 0.0953 - val_micro_precision: 0.7643 - val_micro_f1_score: 0.5854\n",
      "Best val_f1 so far: 0.5854374170303345 with hyperparams: {'batch_size': 32, 'lstm_units': 256, 'dropout_rate': 0.3, 'learning_rate': 0.001}\n",
      "Training with batch_size=32, lstm_units=256, dropout_rate=0.7, learning_rate=0.001\n",
      "Epoch 1/10\n",
      "5883/5883 [==============================] - 386s 65ms/step - loss: 0.1427 - micro_precision: 0.5557 - micro_f1_score: 0.2944 - val_loss: 0.1067 - val_micro_precision: 0.7527 - val_micro_f1_score: 0.5084\n",
      "Epoch 2/10\n",
      "5883/5883 [==============================] - 379s 64ms/step - loss: 0.1101 - micro_precision: 0.7373 - micro_f1_score: 0.5198 - val_loss: 0.0995 - val_micro_precision: 0.7764 - val_micro_f1_score: 0.5485\n",
      "Epoch 3/10\n",
      "5883/5883 [==============================] - 379s 64ms/step - loss: 0.1049 - micro_precision: 0.7489 - micro_f1_score: 0.5488 - val_loss: 0.0968 - val_micro_precision: 0.7674 - val_micro_f1_score: 0.5761\n",
      "Epoch 4/10\n",
      "5883/5883 [==============================] - 380s 65ms/step - loss: 0.1023 - micro_precision: 0.7560 - micro_f1_score: 0.5634 - val_loss: 0.0954 - val_micro_precision: 0.7648 - val_micro_f1_score: 0.5895\n",
      "Best val_f1 so far: 0.5894966721534729 with hyperparams: {'batch_size': 32, 'lstm_units': 256, 'dropout_rate': 0.3, 'learning_rate': 0.001}\n",
      "Training with batch_size=32, lstm_units=256, dropout_rate=0.7, learning_rate=0.0001\n",
      "Epoch 1/10\n",
      "5883/5883 [==============================] - 382s 65ms/step - loss: 0.1674 - micro_precision: 0.4369 - micro_f1_score: 0.2029 - val_loss: 0.1151 - val_micro_precision: 0.7322 - val_micro_f1_score: 0.4485\n",
      "Epoch 2/10\n",
      "5883/5883 [==============================] - 376s 64ms/step - loss: 0.1178 - micro_precision: 0.7007 - micro_f1_score: 0.4790 - val_loss: 0.1030 - val_micro_precision: 0.7601 - val_micro_f1_score: 0.5396\n",
      "Epoch 3/10\n",
      "5883/5883 [==============================] - 372s 63ms/step - loss: 0.1077 - micro_precision: 0.7340 - micro_f1_score: 0.5391 - val_loss: 0.0989 - val_micro_precision: 0.7697 - val_micro_f1_score: 0.5610\n",
      "Epoch 4/10\n",
      "5883/5883 [==============================] - 374s 64ms/step - loss: 0.1031 - micro_precision: 0.7463 - micro_f1_score: 0.5653 - val_loss: 0.0964 - val_micro_precision: 0.7638 - val_micro_f1_score: 0.5849\n",
      "Best val_f1 so far: 0.5848938226699829 with hyperparams: {'batch_size': 32, 'lstm_units': 256, 'dropout_rate': 0.3, 'learning_rate': 0.001}\n",
      "Training with batch_size=64, lstm_units=64, dropout_rate=0.3, learning_rate=0.001\n",
      "Epoch 1/10\n",
      "2942/2942 [==============================] - 121s 41ms/step - loss: 0.1599 - micro_precision: 0.4360 - micro_f1_score: 0.1826 - val_loss: 0.1137 - val_micro_precision: 0.7518 - val_micro_f1_score: 0.4623\n",
      "Epoch 2/10\n",
      "2942/2942 [==============================] - 118s 40ms/step - loss: 0.1124 - micro_precision: 0.7297 - micro_f1_score: 0.4902 - val_loss: 0.1024 - val_micro_precision: 0.7795 - val_micro_f1_score: 0.5255\n",
      "Epoch 3/10\n",
      "2942/2942 [==============================] - 116s 39ms/step - loss: 0.1047 - micro_precision: 0.7491 - micro_f1_score: 0.5448 - val_loss: 0.0979 - val_micro_precision: 0.7762 - val_micro_f1_score: 0.5601\n",
      "Epoch 4/10\n",
      "2942/2942 [==============================] - 115s 39ms/step - loss: 0.1010 - micro_precision: 0.7561 - micro_f1_score: 0.5660 - val_loss: 0.0960 - val_micro_precision: 0.7721 - val_micro_f1_score: 0.5782\n",
      "Best val_f1 so far: 0.5781558156013489 with hyperparams: {'batch_size': 32, 'lstm_units': 256, 'dropout_rate': 0.3, 'learning_rate': 0.001}\n",
      "Training with batch_size=64, lstm_units=64, dropout_rate=0.3, learning_rate=0.0001\n",
      "Epoch 1/10\n",
      "2942/2942 [==============================] - 120s 40ms/step - loss: 0.2211 - micro_precision: 0.0454 - micro_f1_score: 0.0040 - val_loss: 0.1774 - val_micro_precision: 0.0000e+00 - val_micro_f1_score: 0.0000e+00\n",
      "Epoch 2/10\n",
      "2942/2942 [==============================] - 116s 39ms/step - loss: 0.1663 - micro_precision: 0.4478 - micro_f1_score: 0.0642 - val_loss: 0.1405 - val_micro_precision: 0.7128 - val_micro_f1_score: 0.0871\n",
      "Epoch 3/10\n",
      "2942/2942 [==============================] - 114s 39ms/step - loss: 0.1412 - micro_precision: 0.6195 - micro_f1_score: 0.2478 - val_loss: 0.1251 - val_micro_precision: 0.7469 - val_micro_f1_score: 0.3093\n",
      "Epoch 4/10\n",
      "2942/2942 [==============================] - 117s 40ms/step - loss: 0.1302 - micro_precision: 0.6651 - micro_f1_score: 0.3522 - val_loss: 0.1175 - val_micro_precision: 0.7539 - val_micro_f1_score: 0.4167\n",
      "Best val_f1 so far: 0.41671517491340637 with hyperparams: {'batch_size': 32, 'lstm_units': 256, 'dropout_rate': 0.3, 'learning_rate': 0.001}\n",
      "Training with batch_size=64, lstm_units=64, dropout_rate=0.5, learning_rate=0.001\n",
      "Epoch 1/10\n",
      "2942/2942 [==============================] - 120s 40ms/step - loss: 0.1661 - micro_precision: 0.4125 - micro_f1_score: 0.1379 - val_loss: 0.1169 - val_micro_precision: 0.7729 - val_micro_f1_score: 0.3689\n",
      "Epoch 2/10\n",
      "2942/2942 [==============================] - 115s 39ms/step - loss: 0.1213 - micro_precision: 0.7217 - micro_f1_score: 0.4240 - val_loss: 0.1054 - val_micro_precision: 0.7936 - val_micro_f1_score: 0.4822\n",
      "Epoch 3/10\n",
      "2942/2942 [==============================] - 116s 40ms/step - loss: 0.1133 - micro_precision: 0.7441 - micro_f1_score: 0.4887 - val_loss: 0.1021 - val_micro_precision: 0.7843 - val_micro_f1_score: 0.5190\n",
      "Epoch 4/10\n",
      "2942/2942 [==============================] - 115s 39ms/step - loss: 0.1097 - micro_precision: 0.7510 - micro_f1_score: 0.5103 - val_loss: 0.0995 - val_micro_precision: 0.7942 - val_micro_f1_score: 0.5344\n",
      "Best val_f1 so far: 0.5344350337982178 with hyperparams: {'batch_size': 32, 'lstm_units': 256, 'dropout_rate': 0.3, 'learning_rate': 0.001}\n",
      "Training with batch_size=64, lstm_units=64, dropout_rate=0.5, learning_rate=0.0001\n",
      "Epoch 1/10\n",
      "2942/2942 [==============================] - 118s 40ms/step - loss: 0.2342 - micro_precision: 0.0590 - micro_f1_score: 0.0084 - val_loss: 0.1790 - val_micro_precision: 0.0000e+00 - val_micro_f1_score: 0.0000e+00\n",
      "Epoch 2/10\n",
      "2942/2942 [==============================] - 114s 39ms/step - loss: 0.1745 - micro_precision: 0.4195 - micro_f1_score: 0.0688 - val_loss: 0.1426 - val_micro_precision: 0.6688 - val_micro_f1_score: 0.0981\n",
      "Epoch 3/10\n",
      "2942/2942 [==============================] - 114s 39ms/step - loss: 0.1492 - micro_precision: 0.5877 - micro_f1_score: 0.2178 - val_loss: 0.1268 - val_micro_precision: 0.7606 - val_micro_f1_score: 0.2634\n",
      "Epoch 4/10\n",
      "2942/2942 [==============================] - 115s 39ms/step - loss: 0.1372 - micro_precision: 0.6458 - micro_f1_score: 0.3070 - val_loss: 0.1186 - val_micro_precision: 0.7631 - val_micro_f1_score: 0.3707\n",
      "Best val_f1 so far: 0.3706950843334198 with hyperparams: {'batch_size': 32, 'lstm_units': 256, 'dropout_rate': 0.3, 'learning_rate': 0.001}\n",
      "Training with batch_size=64, lstm_units=64, dropout_rate=0.7, learning_rate=0.001\n",
      "Epoch 1/10\n",
      "2942/2942 [==============================] - 119s 40ms/step - loss: 0.1948 - micro_precision: 0.2132 - micro_f1_score: 0.0269 - val_loss: 0.1404 - val_micro_precision: 0.7327 - val_micro_f1_score: 0.1228\n",
      "Epoch 2/10\n",
      "2942/2942 [==============================] - 115s 39ms/step - loss: 0.1436 - micro_precision: 0.6610 - micro_f1_score: 0.1948 - val_loss: 0.1219 - val_micro_precision: 0.7852 - val_micro_f1_score: 0.2776\n",
      "Epoch 3/10\n",
      "2942/2942 [==============================] - 114s 39ms/step - loss: 0.1327 - micro_precision: 0.7069 - micro_f1_score: 0.2983 - val_loss: 0.1135 - val_micro_precision: 0.7847 - val_micro_f1_score: 0.3888\n",
      "Epoch 4/10\n",
      "2942/2942 [==============================] - 114s 39ms/step - loss: 0.1265 - micro_precision: 0.7214 - micro_f1_score: 0.3600 - val_loss: 0.1077 - val_micro_precision: 0.7905 - val_micro_f1_score: 0.4532\n",
      "Best val_f1 so far: 0.4532148241996765 with hyperparams: {'batch_size': 32, 'lstm_units': 256, 'dropout_rate': 0.3, 'learning_rate': 0.001}\n",
      "Training with batch_size=64, lstm_units=64, dropout_rate=0.7, learning_rate=0.0001\n",
      "Epoch 1/10\n",
      "2942/2942 [==============================] - 118s 40ms/step - loss: 0.2564 - micro_precision: 0.0455 - micro_f1_score: 0.0151 - val_loss: 0.1955 - val_micro_precision: 0.0000e+00 - val_micro_f1_score: 0.0000e+00\n",
      "Epoch 2/10\n",
      "2942/2942 [==============================] - 115s 39ms/step - loss: 0.1978 - micro_precision: 0.2290 - micro_f1_score: 0.0124 - val_loss: 0.1638 - val_micro_precision: 0.0000e+00 - val_micro_f1_score: 0.0000e+00\n",
      "Epoch 3/10\n",
      "2942/2942 [==============================] - 115s 39ms/step - loss: 0.1737 - micro_precision: 0.5255 - micro_f1_score: 0.0705 - val_loss: 0.1444 - val_micro_precision: 0.6981 - val_micro_f1_score: 0.0910\n",
      "Epoch 4/10\n",
      "2942/2942 [==============================] - 116s 39ms/step - loss: 0.1584 - micro_precision: 0.5755 - micro_f1_score: 0.1293 - val_loss: 0.1333 - val_micro_precision: 0.7168 - val_micro_f1_score: 0.1315\n",
      "Best val_f1 so far: 0.13145413994789124 with hyperparams: {'batch_size': 32, 'lstm_units': 256, 'dropout_rate': 0.3, 'learning_rate': 0.001}\n",
      "Training with batch_size=64, lstm_units=128, dropout_rate=0.3, learning_rate=0.001\n",
      "Epoch 1/10\n",
      "2942/2942 [==============================] - 177s 60ms/step - loss: 0.1439 - micro_precision: 0.5246 - micro_f1_score: 0.2794 - val_loss: 0.1083 - val_micro_precision: 0.7505 - val_micro_f1_score: 0.5024\n",
      "Epoch 2/10\n",
      "2942/2942 [==============================] - 180s 61ms/step - loss: 0.1061 - micro_precision: 0.7394 - micro_f1_score: 0.5378 - val_loss: 0.0984 - val_micro_precision: 0.7578 - val_micro_f1_score: 0.5726\n",
      "Epoch 3/10\n",
      "2942/2942 [==============================] - 174s 59ms/step - loss: 0.0991 - micro_precision: 0.7522 - micro_f1_score: 0.5782 - val_loss: 0.0953 - val_micro_precision: 0.7673 - val_micro_f1_score: 0.5844\n",
      "Epoch 4/10\n",
      "2942/2942 [==============================] - 175s 60ms/step - loss: 0.0954 - micro_precision: 0.7587 - micro_f1_score: 0.5983 - val_loss: 0.0935 - val_micro_precision: 0.7536 - val_micro_f1_score: 0.6096\n",
      "Best val_f1 so far: 0.609558641910553 with hyperparams: {'batch_size': 32, 'lstm_units': 256, 'dropout_rate': 0.3, 'learning_rate': 0.001}\n",
      "Training with batch_size=64, lstm_units=128, dropout_rate=0.3, learning_rate=0.0001\n",
      "Epoch 1/10\n",
      "2942/2942 [==============================] - 179s 60ms/step - loss: 0.1895 - micro_precision: 0.2923 - micro_f1_score: 0.0693 - val_loss: 0.1337 - val_micro_precision: 0.7345 - val_micro_f1_score: 0.2494\n",
      "Epoch 2/10\n",
      "2942/2942 [==============================] - 173s 59ms/step - loss: 0.1288 - micro_precision: 0.6727 - micro_f1_score: 0.3809 - val_loss: 0.1129 - val_micro_precision: 0.7465 - val_micro_f1_score: 0.4725\n",
      "Epoch 3/10\n",
      "2942/2942 [==============================] - 177s 60ms/step - loss: 0.1149 - micro_precision: 0.7107 - micro_f1_score: 0.4867 - val_loss: 0.1059 - val_micro_precision: 0.7437 - val_micro_f1_score: 0.5250\n",
      "Epoch 4/10\n",
      "2942/2942 [==============================] - 176s 60ms/step - loss: 0.1085 - micro_precision: 0.7244 - micro_f1_score: 0.5236 - val_loss: 0.1022 - val_micro_precision: 0.7491 - val_micro_f1_score: 0.5502\n",
      "Best val_f1 so far: 0.5501781702041626 with hyperparams: {'batch_size': 32, 'lstm_units': 256, 'dropout_rate': 0.3, 'learning_rate': 0.001}\n",
      "Training with batch_size=64, lstm_units=128, dropout_rate=0.5, learning_rate=0.001\n",
      "Epoch 1/10\n",
      "2942/2942 [==============================] - 177s 59ms/step - loss: 0.1513 - micro_precision: 0.5053 - micro_f1_score: 0.2322 - val_loss: 0.1095 - val_micro_precision: 0.7529 - val_micro_f1_score: 0.4849\n",
      "Epoch 2/10\n",
      "2942/2942 [==============================] - 177s 60ms/step - loss: 0.1099 - micro_precision: 0.7383 - micro_f1_score: 0.5155 - val_loss: 0.1001 - val_micro_precision: 0.7702 - val_micro_f1_score: 0.5512\n",
      "Epoch 3/10\n",
      "2942/2942 [==============================] - 173s 59ms/step - loss: 0.1034 - micro_precision: 0.7499 - micro_f1_score: 0.5560 - val_loss: 0.0969 - val_micro_precision: 0.7779 - val_micro_f1_score: 0.5695\n",
      "Epoch 4/10\n",
      "2942/2942 [==============================] - 173s 59ms/step - loss: 0.1003 - micro_precision: 0.7578 - micro_f1_score: 0.5748 - val_loss: 0.0952 - val_micro_precision: 0.7681 - val_micro_f1_score: 0.5890\n",
      "Best val_f1 so far: 0.5890129804611206 with hyperparams: {'batch_size': 32, 'lstm_units': 256, 'dropout_rate': 0.3, 'learning_rate': 0.001}\n",
      "Training with batch_size=64, lstm_units=128, dropout_rate=0.5, learning_rate=0.0001\n",
      "Epoch 1/10\n",
      "2942/2942 [==============================] - 176s 59ms/step - loss: 0.1970 - micro_precision: 0.2846 - micro_f1_score: 0.0627 - val_loss: 0.1391 - val_micro_precision: 0.6935 - val_micro_f1_score: 0.1743\n",
      "Epoch 2/10\n",
      "2942/2942 [==============================] - 171s 58ms/step - loss: 0.1389 - micro_precision: 0.6238 - micro_f1_score: 0.3129 - val_loss: 0.1174 - val_micro_precision: 0.7501 - val_micro_f1_score: 0.4040\n",
      "Epoch 3/10\n",
      "2942/2942 [==============================] - 175s 60ms/step - loss: 0.1228 - micro_precision: 0.6909 - micro_f1_score: 0.4345 - val_loss: 0.1086 - val_micro_precision: 0.7687 - val_micro_f1_score: 0.4850\n",
      "Epoch 4/10\n",
      "2942/2942 [==============================] - 172s 58ms/step - loss: 0.1151 - micro_precision: 0.7153 - micro_f1_score: 0.4847 - val_loss: 0.1046 - val_micro_precision: 0.7748 - val_micro_f1_score: 0.5113\n",
      "Best val_f1 so far: 0.5112666487693787 with hyperparams: {'batch_size': 32, 'lstm_units': 256, 'dropout_rate': 0.3, 'learning_rate': 0.001}\n",
      "Training with batch_size=64, lstm_units=128, dropout_rate=0.7, learning_rate=0.001\n",
      "Epoch 1/10\n",
      "2942/2942 [==============================] - 174s 59ms/step - loss: 0.1586 - micro_precision: 0.4961 - micro_f1_score: 0.1992 - val_loss: 0.1140 - val_micro_precision: 0.7679 - val_micro_f1_score: 0.4244\n",
      "Epoch 2/10\n",
      "2942/2942 [==============================] - 175s 60ms/step - loss: 0.1196 - micro_precision: 0.7327 - micro_f1_score: 0.4494 - val_loss: 0.1040 - val_micro_precision: 0.7928 - val_micro_f1_score: 0.4937\n",
      "Epoch 3/10\n",
      "2942/2942 [==============================] - 170s 58ms/step - loss: 0.1131 - micro_precision: 0.7469 - micro_f1_score: 0.4958 - val_loss: 0.1012 - val_micro_precision: 0.7798 - val_micro_f1_score: 0.5340\n",
      "Epoch 4/10\n",
      "2942/2942 [==============================] - 174s 59ms/step - loss: 0.1096 - micro_precision: 0.7532 - micro_f1_score: 0.5157 - val_loss: 0.0998 - val_micro_precision: 0.7969 - val_micro_f1_score: 0.5326\n",
      "Best val_f1 so far: 0.5340368151664734 with hyperparams: {'batch_size': 32, 'lstm_units': 256, 'dropout_rate': 0.3, 'learning_rate': 0.001}\n",
      "Training with batch_size=64, lstm_units=128, dropout_rate=0.7, learning_rate=0.0001\n",
      "Epoch 1/10\n",
      "2942/2942 [==============================] - 179s 60ms/step - loss: 0.2215 - micro_precision: 0.1667 - micro_f1_score: 0.0223 - val_loss: 0.1566 - val_micro_precision: 0.3899 - val_micro_f1_score: 0.0132\n",
      "Epoch 2/10\n",
      "2942/2942 [==============================] - 170s 58ms/step - loss: 0.1569 - micro_precision: 0.5354 - micro_f1_score: 0.1921 - val_loss: 0.1254 - val_micro_precision: 0.7370 - val_micro_f1_score: 0.3148\n",
      "Epoch 3/10\n",
      "2942/2942 [==============================] - 175s 59ms/step - loss: 0.1358 - micro_precision: 0.6441 - micro_f1_score: 0.3306 - val_loss: 0.1145 - val_micro_precision: 0.7661 - val_micro_f1_score: 0.4216\n",
      "Epoch 4/10\n",
      "2942/2942 [==============================] - 171s 58ms/step - loss: 0.1256 - micro_precision: 0.6962 - micro_f1_score: 0.4079 - val_loss: 0.1093 - val_micro_precision: 0.7573 - val_micro_f1_score: 0.4822\n",
      "Best val_f1 so far: 0.48223868012428284 with hyperparams: {'batch_size': 32, 'lstm_units': 256, 'dropout_rate': 0.3, 'learning_rate': 0.001}\n",
      "Training with batch_size=64, lstm_units=256, dropout_rate=0.3, learning_rate=0.001\n",
      "Epoch 1/10\n",
      "2942/2942 [==============================] - 213s 72ms/step - loss: 0.1273 - micro_precision: 0.6201 - micro_f1_score: 0.3953 - val_loss: 0.1027 - val_micro_precision: 0.7426 - val_micro_f1_score: 0.5494\n",
      "Epoch 2/10\n",
      "2942/2942 [==============================] - 210s 71ms/step - loss: 0.0997 - micro_precision: 0.7461 - micro_f1_score: 0.5752 - val_loss: 0.0955 - val_micro_precision: 0.7558 - val_micro_f1_score: 0.5944\n",
      "Epoch 3/10\n",
      "2942/2942 [==============================] - 211s 72ms/step - loss: 0.0942 - micro_precision: 0.7561 - micro_f1_score: 0.6028 - val_loss: 0.0930 - val_micro_precision: 0.7429 - val_micro_f1_score: 0.6163\n",
      "Epoch 4/10\n",
      "2942/2942 [==============================] - 211s 72ms/step - loss: 0.0907 - micro_precision: 0.7639 - micro_f1_score: 0.6205 - val_loss: 0.0913 - val_micro_precision: 0.7542 - val_micro_f1_score: 0.6167\n",
      "Best val_f1 so far: 0.6167498826980591 with hyperparams: {'batch_size': 64, 'lstm_units': 256, 'dropout_rate': 0.3, 'learning_rate': 0.001}\n",
      "Training with batch_size=64, lstm_units=256, dropout_rate=0.3, learning_rate=0.0001\n",
      "Epoch 1/10\n",
      "2942/2942 [==============================] - 214s 72ms/step - loss: 0.1555 - micro_precision: 0.4848 - micro_f1_score: 0.2492 - val_loss: 0.1148 - val_micro_precision: 0.7087 - val_micro_f1_score: 0.4731\n",
      "Epoch 2/10\n",
      "2942/2942 [==============================] - 212s 72ms/step - loss: 0.1111 - micro_precision: 0.7118 - micro_f1_score: 0.5121 - val_loss: 0.1030 - val_micro_precision: 0.7408 - val_micro_f1_score: 0.5475\n",
      "Epoch 3/10\n",
      "2942/2942 [==============================] - 211s 72ms/step - loss: 0.1029 - micro_precision: 0.7326 - micro_f1_score: 0.5577 - val_loss: 0.0989 - val_micro_precision: 0.7459 - val_micro_f1_score: 0.5767\n",
      "Epoch 4/10\n",
      "2942/2942 [==============================] - 209s 71ms/step - loss: 0.0984 - micro_precision: 0.7434 - micro_f1_score: 0.5815 - val_loss: 0.0962 - val_micro_precision: 0.7576 - val_micro_f1_score: 0.5826\n",
      "Best val_f1 so far: 0.5826347470283508 with hyperparams: {'batch_size': 64, 'lstm_units': 256, 'dropout_rate': 0.3, 'learning_rate': 0.001}\n",
      "Training with batch_size=64, lstm_units=256, dropout_rate=0.5, learning_rate=0.001\n",
      "Epoch 1/10\n",
      "2942/2942 [==============================] - 213s 72ms/step - loss: 0.1339 - micro_precision: 0.6013 - micro_f1_score: 0.3528 - val_loss: 0.1035 - val_micro_precision: 0.7423 - val_micro_f1_score: 0.5426\n",
      "Epoch 2/10\n",
      "2942/2942 [==============================] - 213s 73ms/step - loss: 0.1035 - micro_precision: 0.7415 - micro_f1_score: 0.5559 - val_loss: 0.0964 - val_micro_precision: 0.7542 - val_micro_f1_score: 0.5841\n",
      "Epoch 3/10\n",
      "2942/2942 [==============================] - 212s 72ms/step - loss: 0.0976 - micro_precision: 0.7532 - micro_f1_score: 0.5886 - val_loss: 0.0940 - val_micro_precision: 0.7491 - val_micro_f1_score: 0.6030\n",
      "Epoch 4/10\n",
      "2942/2942 [==============================] - 214s 73ms/step - loss: 0.0943 - micro_precision: 0.7607 - micro_f1_score: 0.6058 - val_loss: 0.0923 - val_micro_precision: 0.7665 - val_micro_f1_score: 0.6089\n",
      "Best val_f1 so far: 0.6089484095573425 with hyperparams: {'batch_size': 64, 'lstm_units': 256, 'dropout_rate': 0.3, 'learning_rate': 0.001}\n",
      "Training with batch_size=64, lstm_units=256, dropout_rate=0.5, learning_rate=0.0001\n",
      "Epoch 1/10\n",
      "2942/2942 [==============================] - 212s 72ms/step - loss: 0.1637 - micro_precision: 0.4549 - micro_f1_score: 0.2189 - val_loss: 0.1151 - val_micro_precision: 0.7256 - val_micro_f1_score: 0.4625\n",
      "Epoch 2/10\n",
      "2942/2942 [==============================] - 209s 71ms/step - loss: 0.1155 - micro_precision: 0.6999 - micro_f1_score: 0.4928 - val_loss: 0.1038 - val_micro_precision: 0.7456 - val_micro_f1_score: 0.5403\n",
      "Epoch 3/10\n",
      "2942/2942 [==============================] - 212s 72ms/step - loss: 0.1059 - micro_precision: 0.7288 - micro_f1_score: 0.5449 - val_loss: 0.0990 - val_micro_precision: 0.7531 - val_micro_f1_score: 0.5698\n",
      "Epoch 4/10\n",
      "2942/2942 [==============================] - 213s 72ms/step - loss: 0.1011 - micro_precision: 0.7424 - micro_f1_score: 0.5710 - val_loss: 0.0965 - val_micro_precision: 0.7623 - val_micro_f1_score: 0.5819\n",
      "Best val_f1 so far: 0.5819200873374939 with hyperparams: {'batch_size': 64, 'lstm_units': 256, 'dropout_rate': 0.3, 'learning_rate': 0.001}\n",
      "Training with batch_size=64, lstm_units=256, dropout_rate=0.7, learning_rate=0.001\n",
      "Epoch 1/10\n",
      "2942/2942 [==============================] - 215s 72ms/step - loss: 0.1436 - micro_precision: 0.5533 - micro_f1_score: 0.3028 - val_loss: 0.1052 - val_micro_precision: 0.7606 - val_micro_f1_score: 0.5168\n",
      "Epoch 2/10\n",
      "2942/2942 [==============================] - 210s 71ms/step - loss: 0.1094 - micro_precision: 0.7384 - micro_f1_score: 0.5272 - val_loss: 0.0988 - val_micro_precision: 0.7652 - val_micro_f1_score: 0.5629\n",
      "Epoch 3/10\n",
      "2942/2942 [==============================] - 211s 72ms/step - loss: 0.1039 - micro_precision: 0.7510 - micro_f1_score: 0.5596 - val_loss: 0.0964 - val_micro_precision: 0.7622 - val_micro_f1_score: 0.5826\n",
      "Epoch 4/10\n",
      "2942/2942 [==============================] - 211s 72ms/step - loss: 0.1008 - micro_precision: 0.7572 - micro_f1_score: 0.5760 - val_loss: 0.0953 - val_micro_precision: 0.7781 - val_micro_f1_score: 0.5829\n",
      "Best val_f1 so far: 0.5829261541366577 with hyperparams: {'batch_size': 64, 'lstm_units': 256, 'dropout_rate': 0.3, 'learning_rate': 0.001}\n",
      "Training with batch_size=64, lstm_units=256, dropout_rate=0.7, learning_rate=0.0001\n",
      "Epoch 1/10\n",
      "2942/2942 [==============================] - 214s 72ms/step - loss: 0.1859 - micro_precision: 0.3428 - micro_f1_score: 0.1220 - val_loss: 0.1257 - val_micro_precision: 0.7348 - val_micro_f1_score: 0.3236\n",
      "Epoch 2/10\n",
      "2942/2942 [==============================] - 210s 71ms/step - loss: 0.1273 - micro_precision: 0.6635 - micro_f1_score: 0.4216 - val_loss: 0.1078 - val_micro_precision: 0.7634 - val_micro_f1_score: 0.4938\n",
      "Epoch 3/10\n",
      "2942/2942 [==============================] - 209s 71ms/step - loss: 0.1138 - micro_precision: 0.7132 - micro_f1_score: 0.5044 - val_loss: 0.1025 - val_micro_precision: 0.7590 - val_micro_f1_score: 0.5391\n",
      "Epoch 4/10\n",
      "2942/2942 [==============================] - 209s 71ms/step - loss: 0.1077 - micro_precision: 0.7338 - micro_f1_score: 0.5391 - val_loss: 0.0989 - val_micro_precision: 0.7655 - val_micro_f1_score: 0.5650\n",
      "Best val_f1 so far: 0.565029501914978 with hyperparams: {'batch_size': 64, 'lstm_units': 256, 'dropout_rate': 0.3, 'learning_rate': 0.001}\n",
      "Training with batch_size=128, lstm_units=64, dropout_rate=0.3, learning_rate=0.001\n",
      "Epoch 1/10\n",
      "1471/1471 [==============================] - 85s 57ms/step - loss: 0.1862 - micro_precision: 0.2188 - micro_f1_score: 0.0387 - val_loss: 0.1325 - val_micro_precision: 0.7044 - val_micro_f1_score: 0.2038\n",
      "Epoch 2/10\n",
      "1471/1471 [==============================] - 80s 54ms/step - loss: 0.1236 - micro_precision: 0.7027 - micro_f1_score: 0.3937 - val_loss: 0.1077 - val_micro_precision: 0.7716 - val_micro_f1_score: 0.4894\n",
      "Epoch 3/10\n",
      "1471/1471 [==============================] - 82s 56ms/step - loss: 0.1095 - micro_precision: 0.7390 - micro_f1_score: 0.5144 - val_loss: 0.1011 - val_micro_precision: 0.7650 - val_micro_f1_score: 0.5502\n",
      "Epoch 4/10\n",
      "1471/1471 [==============================] - 79s 54ms/step - loss: 0.1043 - micro_precision: 0.7499 - micro_f1_score: 0.5493 - val_loss: 0.0981 - val_micro_precision: 0.7715 - val_micro_f1_score: 0.5646\n",
      "Best val_f1 so far: 0.5646498203277588 with hyperparams: {'batch_size': 64, 'lstm_units': 256, 'dropout_rate': 0.3, 'learning_rate': 0.001}\n",
      "Training with batch_size=128, lstm_units=64, dropout_rate=0.3, learning_rate=0.0001\n",
      "Epoch 1/10\n",
      "1471/1471 [==============================] - 85s 56ms/step - loss: 0.2447 - micro_precision: 0.0298 - micro_f1_score: 0.0080 - val_loss: 0.1945 - val_micro_precision: 0.0000e+00 - val_micro_f1_score: 0.0000e+00\n",
      "Epoch 2/10\n",
      "1471/1471 [==============================] - 80s 54ms/step - loss: 0.1861 - micro_precision: 0.2902 - micro_f1_score: 0.0143 - val_loss: 0.1613 - val_micro_precision: 0.0000e+00 - val_micro_f1_score: 0.0000e+00\n",
      "Epoch 3/10\n",
      "1471/1471 [==============================] - 82s 55ms/step - loss: 0.1601 - micro_precision: 0.5460 - micro_f1_score: 0.1118 - val_loss: 0.1420 - val_micro_precision: 0.7172 - val_micro_f1_score: 0.0931\n",
      "Epoch 4/10\n",
      "1471/1471 [==============================] - 80s 54ms/step - loss: 0.1450 - micro_precision: 0.6039 - micro_f1_score: 0.2215 - val_loss: 0.1301 - val_micro_precision: 0.7259 - val_micro_f1_score: 0.2443\n",
      "Best val_f1 so far: 0.24425862729549408 with hyperparams: {'batch_size': 64, 'lstm_units': 256, 'dropout_rate': 0.3, 'learning_rate': 0.001}\n",
      "Training with batch_size=128, lstm_units=64, dropout_rate=0.5, learning_rate=0.001\n",
      "Epoch 1/10\n",
      "1471/1471 [==============================] - 84s 56ms/step - loss: 0.1878 - micro_precision: 0.2925 - micro_f1_score: 0.0651 - val_loss: 0.1280 - val_micro_precision: 0.7464 - val_micro_f1_score: 0.2109\n",
      "Epoch 2/10\n",
      "1471/1471 [==============================] - 80s 54ms/step - loss: 0.1294 - micro_precision: 0.6852 - micro_f1_score: 0.3520 - val_loss: 0.1089 - val_micro_precision: 0.7708 - val_micro_f1_score: 0.4823\n",
      "Epoch 3/10\n",
      "1471/1471 [==============================] - 81s 55ms/step - loss: 0.1173 - micro_precision: 0.7329 - micro_f1_score: 0.4612 - val_loss: 0.1034 - val_micro_precision: 0.7830 - val_micro_f1_score: 0.5117\n",
      "Epoch 4/10\n",
      "1471/1471 [==============================] - 81s 55ms/step - loss: 0.1121 - micro_precision: 0.7460 - micro_f1_score: 0.4969 - val_loss: 0.1015 - val_micro_precision: 0.7747 - val_micro_f1_score: 0.5310\n",
      "Best val_f1 so far: 0.530978798866272 with hyperparams: {'batch_size': 64, 'lstm_units': 256, 'dropout_rate': 0.3, 'learning_rate': 0.001}\n",
      "Training with batch_size=128, lstm_units=64, dropout_rate=0.5, learning_rate=0.0001\n",
      "Epoch 1/10\n",
      "1471/1471 [==============================] - 83s 55ms/step - loss: 0.2580 - micro_precision: 0.0549 - micro_f1_score: 0.0144 - val_loss: 0.1957 - val_micro_precision: 0.0000e+00 - val_micro_f1_score: 0.0000e+00\n",
      "Epoch 2/10\n",
      "1471/1471 [==============================] - 81s 55ms/step - loss: 0.1947 - micro_precision: 0.2508 - micro_f1_score: 0.0168 - val_loss: 0.1600 - val_micro_precision: 0.0054 - val_micro_f1_score: 5.4621e-05\n",
      "Epoch 3/10\n",
      "1471/1471 [==============================] - 80s 54ms/step - loss: 0.1662 - micro_precision: 0.5132 - micro_f1_score: 0.1193 - val_loss: 0.1406 - val_micro_precision: 0.7356 - val_micro_f1_score: 0.1234\n",
      "Epoch 4/10\n",
      "1471/1471 [==============================] - 81s 55ms/step - loss: 0.1510 - micro_precision: 0.5848 - micro_f1_score: 0.2105 - val_loss: 0.1299 - val_micro_precision: 0.7438 - val_micro_f1_score: 0.2263\n",
      "Best val_f1 so far: 0.22631721198558807 with hyperparams: {'batch_size': 64, 'lstm_units': 256, 'dropout_rate': 0.3, 'learning_rate': 0.001}\n",
      "Training with batch_size=128, lstm_units=64, dropout_rate=0.7, learning_rate=0.001\n",
      "Epoch 1/10\n",
      "1471/1471 [==============================] - 82s 55ms/step - loss: 0.2069 - micro_precision: 0.1651 - micro_f1_score: 0.0106 - val_loss: 0.1508 - val_micro_precision: 0.6713 - val_micro_f1_score: 0.0319\n",
      "Epoch 2/10\n",
      "1471/1471 [==============================] - 82s 56ms/step - loss: 0.1511 - micro_precision: 0.6181 - micro_f1_score: 0.1499 - val_loss: 0.1226 - val_micro_precision: 0.7778 - val_micro_f1_score: 0.2802\n",
      "Epoch 3/10\n",
      "1471/1471 [==============================] - 80s 54ms/step - loss: 0.1349 - micro_precision: 0.7047 - micro_f1_score: 0.2829 - val_loss: 0.1130 - val_micro_precision: 0.7899 - val_micro_f1_score: 0.3882\n",
      "Epoch 4/10\n",
      "1471/1471 [==============================] - 82s 56ms/step - loss: 0.1280 - micro_precision: 0.7254 - micro_f1_score: 0.3547 - val_loss: 0.1097 - val_micro_precision: 0.8091 - val_micro_f1_score: 0.4067\n",
      "Best val_f1 so far: 0.40674057602882385 with hyperparams: {'batch_size': 64, 'lstm_units': 256, 'dropout_rate': 0.3, 'learning_rate': 0.001}\n",
      "Training with batch_size=128, lstm_units=64, dropout_rate=0.7, learning_rate=0.0001\n",
      "Epoch 1/10\n",
      "1471/1471 [==============================] - 85s 55ms/step - loss: 0.2866 - micro_precision: 0.0542 - micro_f1_score: 0.0267 - val_loss: 0.2014 - val_micro_precision: 0.0000e+00 - val_micro_f1_score: 0.0000e+00\n",
      "Epoch 2/10\n",
      "1471/1471 [==============================] - 82s 56ms/step - loss: 0.2200 - micro_precision: 0.0551 - micro_f1_score: 0.0011 - val_loss: 0.1923 - val_micro_precision: 0.0000e+00 - val_micro_f1_score: 0.0000e+00\n",
      "Epoch 3/10\n",
      "1471/1471 [==============================] - 80s 55ms/step - loss: 0.2014 - micro_precision: 0.1358 - micro_f1_score: 0.0038 - val_loss: 0.1719 - val_micro_precision: 0.0000e+00 - val_micro_f1_score: 0.0000e+00\n",
      "Epoch 4/10\n",
      "1471/1471 [==============================] - 82s 56ms/step - loss: 0.1828 - micro_precision: 0.3797 - micro_f1_score: 0.0343 - val_loss: 0.1520 - val_micro_precision: 0.0000e+00 - val_micro_f1_score: 0.0000e+00\n",
      "Best val_f1 so far: 0.0 with hyperparams: {'batch_size': 64, 'lstm_units': 256, 'dropout_rate': 0.3, 'learning_rate': 0.001}\n",
      "Training with batch_size=128, lstm_units=128, dropout_rate=0.3, learning_rate=0.001\n",
      "Epoch 1/10\n",
      "1471/1471 [==============================] - 95s 63ms/step - loss: 0.1569 - micro_precision: 0.4700 - micro_f1_score: 0.2039 - val_loss: 0.1111 - val_micro_precision: 0.7469 - val_micro_f1_score: 0.4870\n",
      "Epoch 2/10\n",
      "1471/1471 [==============================] - 92s 62ms/step - loss: 0.1076 - micro_precision: 0.7360 - micro_f1_score: 0.5275 - val_loss: 0.0993 - val_micro_precision: 0.7574 - val_micro_f1_score: 0.5651\n",
      "Epoch 3/10\n",
      "1471/1471 [==============================] - 94s 64ms/step - loss: 0.0996 - micro_precision: 0.7507 - micro_f1_score: 0.5756 - val_loss: 0.0951 - val_micro_precision: 0.7691 - val_micro_f1_score: 0.5884\n",
      "Epoch 4/10\n",
      "1471/1471 [==============================] - 92s 62ms/step - loss: 0.0956 - micro_precision: 0.7576 - micro_f1_score: 0.5970 - val_loss: 0.0935 - val_micro_precision: 0.7571 - val_micro_f1_score: 0.6065\n",
      "Best val_f1 so far: 0.606495201587677 with hyperparams: {'batch_size': 64, 'lstm_units': 256, 'dropout_rate': 0.3, 'learning_rate': 0.001}\n",
      "Training with batch_size=128, lstm_units=128, dropout_rate=0.3, learning_rate=0.0001\n",
      "Epoch 1/10\n",
      "1471/1471 [==============================] - 96s 64ms/step - loss: 0.2129 - micro_precision: 0.1422 - micro_f1_score: 0.0116 - val_loss: 0.1600 - val_micro_precision: 0.3338 - val_micro_f1_score: 0.0051\n",
      "Epoch 2/10\n",
      "1471/1471 [==============================] - 92s 63ms/step - loss: 0.1477 - micro_precision: 0.6012 - micro_f1_score: 0.2255 - val_loss: 0.1260 - val_micro_precision: 0.7372 - val_micro_f1_score: 0.3563\n",
      "Epoch 3/10\n",
      "1471/1471 [==============================] - 91s 62ms/step - loss: 0.1266 - micro_precision: 0.6759 - micro_f1_score: 0.4034 - val_loss: 0.1141 - val_micro_precision: 0.7467 - val_micro_f1_score: 0.4607\n",
      "Epoch 4/10\n",
      "1471/1471 [==============================] - 94s 64ms/step - loss: 0.1174 - micro_precision: 0.7023 - micro_f1_score: 0.4708 - val_loss: 0.1090 - val_micro_precision: 0.7389 - val_micro_f1_score: 0.5146\n",
      "Best val_f1 so far: 0.5146135091781616 with hyperparams: {'batch_size': 64, 'lstm_units': 256, 'dropout_rate': 0.3, 'learning_rate': 0.001}\n",
      "Training with batch_size=128, lstm_units=128, dropout_rate=0.5, learning_rate=0.001\n",
      "Epoch 1/10\n",
      "1471/1471 [==============================] - 94s 63ms/step - loss: 0.1576 - micro_precision: 0.4702 - micro_f1_score: 0.2184 - val_loss: 0.1103 - val_micro_precision: 0.7444 - val_micro_f1_score: 0.4854\n",
      "Epoch 2/10\n",
      "1471/1471 [==============================] - 91s 62ms/step - loss: 0.1116 - micro_precision: 0.7298 - micro_f1_score: 0.5080 - val_loss: 0.1013 - val_micro_precision: 0.7506 - val_micro_f1_score: 0.5543\n",
      "Epoch 3/10\n",
      "1471/1471 [==============================] - 97s 66ms/step - loss: 0.1045 - micro_precision: 0.7455 - micro_f1_score: 0.5518 - val_loss: 0.0981 - val_micro_precision: 0.7657 - val_micro_f1_score: 0.5733\n",
      "Epoch 4/10\n",
      "1471/1471 [==============================] - 100s 68ms/step - loss: 0.1007 - micro_precision: 0.7531 - micro_f1_score: 0.5741 - val_loss: 0.0958 - val_micro_precision: 0.7728 - val_micro_f1_score: 0.5846\n",
      "Best val_f1 so far: 0.5845743417739868 with hyperparams: {'batch_size': 64, 'lstm_units': 256, 'dropout_rate': 0.3, 'learning_rate': 0.001}\n",
      "Training with batch_size=128, lstm_units=128, dropout_rate=0.5, learning_rate=0.0001\n",
      "Epoch 1/10\n",
      "1471/1471 [==============================] - 103s 69ms/step - loss: 0.2276 - micro_precision: 0.1547 - micro_f1_score: 0.0142 - val_loss: 0.1637 - val_micro_precision: 0.0000e+00 - val_micro_f1_score: 0.0000e+00\n",
      "Epoch 2/10\n",
      "1471/1471 [==============================] - 99s 67ms/step - loss: 0.1556 - micro_precision: 0.5408 - micro_f1_score: 0.2038 - val_loss: 0.1267 - val_micro_precision: 0.7255 - val_micro_f1_score: 0.3171\n",
      "Epoch 3/10\n",
      "1471/1471 [==============================] - 99s 67ms/step - loss: 0.1338 - micro_precision: 0.6378 - micro_f1_score: 0.3604 - val_loss: 0.1152 - val_micro_precision: 0.7553 - val_micro_f1_score: 0.4355\n",
      "Epoch 4/10\n",
      "1471/1471 [==============================] - 99s 68ms/step - loss: 0.1229 - micro_precision: 0.6858 - micro_f1_score: 0.4397 - val_loss: 0.1094 - val_micro_precision: 0.7502 - val_micro_f1_score: 0.4922\n",
      "Best val_f1 so far: 0.4921984076499939 with hyperparams: {'batch_size': 64, 'lstm_units': 256, 'dropout_rate': 0.3, 'learning_rate': 0.001}\n",
      "Training with batch_size=128, lstm_units=128, dropout_rate=0.7, learning_rate=0.001\n",
      "Epoch 1/10\n",
      "1471/1471 [==============================] - 102s 68ms/step - loss: 0.1713 - micro_precision: 0.4251 - micro_f1_score: 0.1546 - val_loss: 0.1162 - val_micro_precision: 0.7470 - val_micro_f1_score: 0.4091\n",
      "Epoch 2/10\n",
      "1471/1471 [==============================] - 100s 68ms/step - loss: 0.1215 - micro_precision: 0.7235 - micro_f1_score: 0.4331 - val_loss: 0.1045 - val_micro_precision: 0.7764 - val_micro_f1_score: 0.5021\n",
      "Epoch 3/10\n",
      "1471/1471 [==============================] - 99s 67ms/step - loss: 0.1139 - micro_precision: 0.7442 - micro_f1_score: 0.4942 - val_loss: 0.1012 - val_micro_precision: 0.7928 - val_micro_f1_score: 0.5245\n",
      "Epoch 4/10\n",
      "1471/1471 [==============================] - 101s 69ms/step - loss: 0.1100 - micro_precision: 0.7508 - micro_f1_score: 0.5189 - val_loss: 0.0996 - val_micro_precision: 0.7845 - val_micro_f1_score: 0.5432\n",
      "Best val_f1 so far: 0.5431967377662659 with hyperparams: {'batch_size': 64, 'lstm_units': 256, 'dropout_rate': 0.3, 'learning_rate': 0.001}\n",
      "Training with batch_size=128, lstm_units=128, dropout_rate=0.7, learning_rate=0.0001\n",
      "Epoch 1/10\n",
      "1471/1471 [==============================] - 103s 69ms/step - loss: 0.2486 - micro_precision: 0.0818 - micro_f1_score: 0.0136 - val_loss: 0.1842 - val_micro_precision: 0.0000e+00 - val_micro_f1_score: 0.0000e+00\n",
      "Epoch 2/10\n",
      "1471/1471 [==============================] - 100s 68ms/step - loss: 0.1772 - micro_precision: 0.4292 - micro_f1_score: 0.1094 - val_loss: 0.1373 - val_micro_precision: 0.7451 - val_micro_f1_score: 0.1647\n",
      "Epoch 3/10\n",
      "1471/1471 [==============================] - 100s 68ms/step - loss: 0.1482 - micro_precision: 0.5994 - micro_f1_score: 0.2664 - val_loss: 0.1211 - val_micro_precision: 0.7537 - val_micro_f1_score: 0.3559\n",
      "Epoch 4/10\n",
      "1471/1471 [==============================] - 100s 68ms/step - loss: 0.1348 - micro_precision: 0.6584 - micro_f1_score: 0.3529 - val_loss: 0.1142 - val_micro_precision: 0.7630 - val_micro_f1_score: 0.4368\n",
      "Best val_f1 so far: 0.4367997944355011 with hyperparams: {'batch_size': 64, 'lstm_units': 256, 'dropout_rate': 0.3, 'learning_rate': 0.001}\n",
      "Training with batch_size=128, lstm_units=256, dropout_rate=0.3, learning_rate=0.001\n",
      "Epoch 1/10\n",
      "1471/1471 [==============================] - 135s 91ms/step - loss: 0.1388 - micro_precision: 0.5468 - micro_f1_score: 0.3199 - val_loss: 0.1038 - val_micro_precision: 0.7272 - val_micro_f1_score: 0.5508\n",
      "Epoch 2/10\n",
      "1471/1471 [==============================] - 133s 90ms/step - loss: 0.1016 - micro_precision: 0.7411 - micro_f1_score: 0.5643 - val_loss: 0.0979 - val_micro_precision: 0.7461 - val_micro_f1_score: 0.5815\n",
      "Epoch 3/10\n",
      "1471/1471 [==============================] - 133s 90ms/step - loss: 0.0958 - micro_precision: 0.7546 - micro_f1_score: 0.5965 - val_loss: 0.0943 - val_micro_precision: 0.7520 - val_micro_f1_score: 0.5981\n",
      "Epoch 4/10\n",
      "1471/1471 [==============================] - 133s 90ms/step - loss: 0.0923 - micro_precision: 0.7616 - micro_f1_score: 0.6127 - val_loss: 0.0932 - val_micro_precision: 0.7504 - val_micro_f1_score: 0.6071\n",
      "Best val_f1 so far: 0.6070702075958252 with hyperparams: {'batch_size': 64, 'lstm_units': 256, 'dropout_rate': 0.3, 'learning_rate': 0.001}\n",
      "Training with batch_size=128, lstm_units=256, dropout_rate=0.3, learning_rate=0.0001\n",
      "Epoch 1/10\n",
      "1471/1471 [==============================] - 136s 92ms/step - loss: 0.1767 - micro_precision: 0.3592 - micro_f1_score: 0.1289 - val_loss: 0.1253 - val_micro_precision: 0.7108 - val_micro_f1_score: 0.3678\n",
      "Epoch 2/10\n",
      "1471/1471 [==============================] - 133s 90ms/step - loss: 0.1214 - micro_precision: 0.6830 - micro_f1_score: 0.4449 - val_loss: 0.1095 - val_micro_precision: 0.7363 - val_micro_f1_score: 0.4958\n",
      "Epoch 3/10\n",
      "1471/1471 [==============================] - 131s 89ms/step - loss: 0.1096 - micro_precision: 0.7159 - micro_f1_score: 0.5212 - val_loss: 0.1026 - val_micro_precision: 0.7373 - val_micro_f1_score: 0.5496\n",
      "Epoch 4/10\n",
      "1471/1471 [==============================] - 133s 90ms/step - loss: 0.1037 - micro_precision: 0.7294 - micro_f1_score: 0.5526 - val_loss: 0.0993 - val_micro_precision: 0.7394 - val_micro_f1_score: 0.5722\n",
      "Best val_f1 so far: 0.5722013115882874 with hyperparams: {'batch_size': 64, 'lstm_units': 256, 'dropout_rate': 0.3, 'learning_rate': 0.001}\n",
      "Training with batch_size=128, lstm_units=256, dropout_rate=0.5, learning_rate=0.001\n",
      "Epoch 1/10\n",
      "1471/1471 [==============================] - 136s 91ms/step - loss: 0.1418 - micro_precision: 0.5442 - micro_f1_score: 0.3065 - val_loss: 0.1061 - val_micro_precision: 0.7401 - val_micro_f1_score: 0.5230\n",
      "Epoch 2/10\n",
      "1471/1471 [==============================] - 132s 90ms/step - loss: 0.1055 - micro_precision: 0.7364 - micro_f1_score: 0.5450 - val_loss: 0.0982 - val_micro_precision: 0.7562 - val_micro_f1_score: 0.5718\n",
      "Epoch 3/10\n",
      "1471/1471 [==============================] - 131s 89ms/step - loss: 0.0986 - micro_precision: 0.7504 - micro_f1_score: 0.5841 - val_loss: 0.0948 - val_micro_precision: 0.7588 - val_micro_f1_score: 0.5962\n",
      "Epoch 4/10\n",
      "1471/1471 [==============================] - 132s 90ms/step - loss: 0.0948 - micro_precision: 0.7588 - micro_f1_score: 0.6042 - val_loss: 0.0932 - val_micro_precision: 0.7453 - val_micro_f1_score: 0.6148\n",
      "Best val_f1 so far: 0.6147745847702026 with hyperparams: {'batch_size': 64, 'lstm_units': 256, 'dropout_rate': 0.3, 'learning_rate': 0.001}\n",
      "Training with batch_size=128, lstm_units=256, dropout_rate=0.5, learning_rate=0.0001\n",
      "Epoch 1/10\n",
      "1471/1471 [==============================] - 135s 91ms/step - loss: 0.1886 - micro_precision: 0.3159 - micro_f1_score: 0.1010 - val_loss: 0.1280 - val_micro_precision: 0.7126 - val_micro_f1_score: 0.3412\n",
      "Epoch 2/10\n",
      "1471/1471 [==============================] - 131s 89ms/step - loss: 0.1271 - micro_precision: 0.6557 - micro_f1_score: 0.4183 - val_loss: 0.1098 - val_micro_precision: 0.7249 - val_micro_f1_score: 0.5074\n",
      "Epoch 3/10\n",
      "1471/1471 [==============================] - 133s 90ms/step - loss: 0.1135 - micro_precision: 0.7031 - micro_f1_score: 0.5048 - val_loss: 0.1030 - val_micro_precision: 0.7477 - val_micro_f1_score: 0.5408\n",
      "Epoch 4/10\n",
      "1471/1471 [==============================] - 133s 91ms/step - loss: 0.1071 - micro_precision: 0.7219 - micro_f1_score: 0.5402 - val_loss: 0.1009 - val_micro_precision: 0.7462 - val_micro_f1_score: 0.5615\n",
      "Best val_f1 so far: 0.5614652037620544 with hyperparams: {'batch_size': 64, 'lstm_units': 256, 'dropout_rate': 0.3, 'learning_rate': 0.001}\n",
      "Training with batch_size=128, lstm_units=256, dropout_rate=0.7, learning_rate=0.001\n",
      "Epoch 1/10\n",
      "1471/1471 [==============================] - 135s 91ms/step - loss: 0.1524 - micro_precision: 0.4988 - micro_f1_score: 0.2607 - val_loss: 0.1077 - val_micro_precision: 0.7336 - val_micro_f1_score: 0.5125\n",
      "Epoch 2/10\n",
      "1471/1471 [==============================] - 133s 90ms/step - loss: 0.1100 - micro_precision: 0.7357 - micro_f1_score: 0.5230 - val_loss: 0.0993 - val_micro_precision: 0.7518 - val_micro_f1_score: 0.5726\n",
      "Epoch 3/10\n",
      "1471/1471 [==============================] - 132s 89ms/step - loss: 0.1036 - micro_precision: 0.7493 - micro_f1_score: 0.5614 - val_loss: 0.0967 - val_micro_precision: 0.7572 - val_micro_f1_score: 0.5858\n",
      "Epoch 4/10\n",
      "1471/1471 [==============================] - 133s 90ms/step - loss: 0.1001 - micro_precision: 0.7565 - micro_f1_score: 0.5810 - val_loss: 0.0953 - val_micro_precision: 0.7639 - val_micro_f1_score: 0.5922\n",
      "Best val_f1 so far: 0.592246413230896 with hyperparams: {'batch_size': 64, 'lstm_units': 256, 'dropout_rate': 0.3, 'learning_rate': 0.001}\n",
      "Training with batch_size=128, lstm_units=256, dropout_rate=0.7, learning_rate=0.0001\n",
      "Epoch 1/10\n",
      "1471/1471 [==============================] - 138s 93ms/step - loss: 0.2087 - micro_precision: 0.2346 - micro_f1_score: 0.0551 - val_loss: 0.1401 - val_micro_precision: 0.6892 - val_micro_f1_score: 0.1662\n",
      "Epoch 2/10\n",
      "1471/1471 [==============================] - 137s 93ms/step - loss: 0.1403 - micro_precision: 0.6018 - micro_f1_score: 0.3392 - val_loss: 0.1150 - val_micro_precision: 0.7457 - val_micro_f1_score: 0.4446\n",
      "Epoch 3/10\n",
      "1471/1471 [==============================] - 287s 195ms/step - loss: 0.1222 - micro_precision: 0.6837 - micro_f1_score: 0.4593 - val_loss: 0.1067 - val_micro_precision: 0.7580 - val_micro_f1_score: 0.5097\n",
      "Epoch 4/10\n",
      "1471/1471 [==============================] - 277s 189ms/step - loss: 0.1142 - micro_precision: 0.7119 - micro_f1_score: 0.5054 - val_loss: 0.1032 - val_micro_precision: 0.7690 - val_micro_f1_score: 0.5289\n",
      "Best val_f1 so far: 0.5289055705070496 with hyperparams: {'batch_size': 64, 'lstm_units': 256, 'dropout_rate': 0.3, 'learning_rate': 0.001}\n",
      "Best hyperparameters: {'batch_size': 64, 'lstm_units': 256, 'dropout_rate': 0.3, 'learning_rate': 0.001}\n"
     ]
    }
   ],
   "source": [
    "# Define a grid of hyperparameters to search\n",
    "batch_sizes = [32, 64, 128]\n",
    "lstm_units = [64, 128, 256]\n",
    "dropout_rates = [0.3, 0.5, 0.7]\n",
    "learning_rates = [1e-3, 1e-4]\n",
    "\n",
    "best_val_f1 = 0\n",
    "best_hyperparams = {}\n",
    "\n",
    "for batch_size in batch_sizes:\n",
    "    for units in lstm_units:\n",
    "        for dropout_rate in dropout_rates:\n",
    "            for learning_rate in learning_rates:\n",
    "                print(f'Training with batch_size={batch_size}, lstm_units={units}, dropout_rate={dropout_rate}, learning_rate={learning_rate}')\n",
    "                model = create_model(units, dropout_rate, learning_rate)\n",
    "                \n",
    "                # Define callbacks\n",
    "                checkpoint_callback = ModelCheckpoint(\n",
    "                    filepath=f'weights/hyperparams/expanded_model_bs{batch_size}_units{units}_dr{dropout_rate}_lr{learning_rate}_epoch{{epoch:02d}}.weights.h5',\n",
    "                    save_weights_only=True,\n",
    "                    save_best_only=True,\n",
    "                    monitor='val_micro_f1_score',\n",
    "                    mode='min',\n",
    "                    save_freq='epoch'\n",
    "                )\n",
    "\n",
    "                early_stopping = EarlyStopping(monitor='val_micro_f1_score', patience=3, restore_best_weights=True)\n",
    "\n",
    "                history = model.fit(\n",
    "                    data_train, labels_train,\n",
    "                    epochs=10,\n",
    "                    batch_size=batch_size,\n",
    "                    validation_data=(data_val, labels_val),\n",
    "                    callbacks=[checkpoint_callback, early_stopping]\n",
    "                )\n",
    "\n",
    "                val_f1 = max(history.history['val_micro_f1_score'])\n",
    "                if val_f1 > best_val_f1:\n",
    "                    best_val_f1 = val_f1\n",
    "                    best_hyperparams = {\n",
    "                        'batch_size': batch_size,\n",
    "                        'lstm_units': units,\n",
    "                        'dropout_rate': dropout_rate,\n",
    "                        'learning_rate': learning_rate\n",
    "                    }\n",
    "                tf.keras.backend.clear_session()\n",
    "                print(f'Best val_f1 so far: {best_val_f1} with hyperparams: {best_hyperparams}')\n",
    "\n",
    "print(f'Best hyperparameters: {best_hyperparams}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3d6ba1f-deeb-4d62-87e0-9b307b7c48a2",
   "metadata": {},
   "source": [
    "### Final Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "921dead6-459b-4d48-84f0-a67b4685d988",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "3310/3310 [==============================] - 700s 211ms/step - loss: 0.1297 - micro_precision: 0.5995 - micro_f1_score: 0.3757\n",
      "Epoch 2/25\n",
      "3310/3310 [==============================] - 866s 262ms/step - loss: 0.0992 - micro_precision: 0.7453 - micro_f1_score: 0.5778\n",
      "Epoch 3/25\n",
      "3310/3310 [==============================] - 573s 173ms/step - loss: 0.0936 - micro_precision: 0.7565 - micro_f1_score: 0.6062\n",
      "Epoch 4/25\n",
      "3310/3310 [==============================] - 525s 159ms/step - loss: 0.0904 - micro_precision: 0.7630 - micro_f1_score: 0.6221\n",
      "Epoch 5/25\n",
      "3310/3310 [==============================] - 498s 150ms/step - loss: 0.0880 - micro_precision: 0.7675 - micro_f1_score: 0.6328\n",
      "Epoch 6/25\n",
      "3310/3310 [==============================] - 1003s 303ms/step - loss: 0.0862 - micro_precision: 0.7717 - micro_f1_score: 0.6419\n",
      "Epoch 7/25\n",
      "3310/3310 [==============================] - 494s 149ms/step - loss: 0.0843 - micro_precision: 0.7758 - micro_f1_score: 0.6495\n",
      "Epoch 8/25\n",
      "3310/3310 [==============================] - 508s 153ms/step - loss: 0.0826 - micro_precision: 0.7795 - micro_f1_score: 0.6569\n",
      "Epoch 9/25\n",
      "3310/3310 [==============================] - 478s 145ms/step - loss: 0.0812 - micro_precision: 0.7825 - micro_f1_score: 0.6623\n",
      "Epoch 10/25\n",
      "3310/3310 [==============================] - 486s 147ms/step - loss: 0.0798 - micro_precision: 0.7852 - micro_f1_score: 0.6679\n",
      "Epoch 11/25\n",
      "3310/3310 [==============================] - 475s 143ms/step - loss: 0.0786 - micro_precision: 0.7882 - micro_f1_score: 0.6738\n",
      "Epoch 12/25\n",
      "3310/3310 [==============================] - 481s 145ms/step - loss: 0.0775 - micro_precision: 0.7908 - micro_f1_score: 0.6781\n",
      "Epoch 13/25\n",
      "3310/3310 [==============================] - 506s 153ms/step - loss: 0.0765 - micro_precision: 0.7932 - micro_f1_score: 0.6820\n",
      "Epoch 14/25\n",
      "3310/3310 [==============================] - 460s 139ms/step - loss: 0.0755 - micro_precision: 0.7958 - micro_f1_score: 0.6868\n",
      "Epoch 15/25\n",
      "3310/3310 [==============================] - 389s 118ms/step - loss: 0.0747 - micro_precision: 0.7973 - micro_f1_score: 0.6895\n",
      "Epoch 16/25\n",
      "3310/3310 [==============================] - 501s 151ms/step - loss: 0.0740 - micro_precision: 0.7983 - micro_f1_score: 0.6929\n",
      "Epoch 17/25\n",
      "3310/3310 [==============================] - 556s 168ms/step - loss: 0.0734 - micro_precision: 0.8003 - micro_f1_score: 0.6954\n",
      "Epoch 18/25\n",
      "3310/3310 [==============================] - 506s 153ms/step - loss: 0.0727 - micro_precision: 0.8018 - micro_f1_score: 0.6984\n",
      "Epoch 19/25\n",
      "3310/3310 [==============================] - 516s 156ms/step - loss: 0.0721 - micro_precision: 0.8031 - micro_f1_score: 0.7008\n",
      "Epoch 20/25\n",
      "3310/3310 [==============================] - 473s 143ms/step - loss: 0.0717 - micro_precision: 0.8042 - micro_f1_score: 0.7025\n",
      "Epoch 21/25\n",
      "3310/3310 [==============================] - 478s 144ms/step - loss: 0.0711 - micro_precision: 0.8048 - micro_f1_score: 0.7044\n",
      "Epoch 22/25\n",
      "3310/3310 [==============================] - 491s 148ms/step - loss: 0.0707 - micro_precision: 0.8058 - micro_f1_score: 0.7064\n",
      "Epoch 23/25\n",
      "3310/3310 [==============================] - 561s 169ms/step - loss: 0.0703 - micro_precision: 0.8078 - micro_f1_score: 0.7085\n",
      "Epoch 24/25\n",
      "3310/3310 [==============================] - 501s 151ms/step - loss: 0.0699 - micro_precision: 0.8078 - micro_f1_score: 0.7096\n",
      "Epoch 25/25\n",
      "3310/3310 [==============================] - 721s 218ms/step - loss: 0.0696 - micro_precision: 0.8085 - micro_f1_score: 0.7117\n"
     ]
    }
   ],
   "source": [
    "final_model = create_model(256, 0.3, 0.001)\n",
    "\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    filepath='weights/final_model/final_model_epoch_{epoch:02d}.weights.h5',\n",
    "    save_weights_only=True,\n",
    "    save_best_only=False,\n",
    "    save_freq='epoch'  # Save every epoch\n",
    ")\n",
    "\n",
    "history = final_model.fit(\n",
    "                np.concatenate([data_train, data_val]), np.concatenate([labels_train, labels_val]),\n",
    "                epochs=25,\n",
    "                batch_size=64,\n",
    "                callbacks=[checkpoint_callback]\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "be77d326-0a7b-4510-b7db-a30d5a7eb3ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "736/736 [==============================] - 89s 120ms/step\n",
      "Hamming Loss: 0.03299620375092073\n",
      "F1 Score: 0.6400024727227769\n",
      "Precision: 0.7268578649910485\n",
      "Recall: 0.571688892570198\n",
      "Subset Accuracy: 0.41199218086010536\n"
     ]
    }
   ],
   "source": [
    "predictions = final_model.predict(data_test)\n",
    "predictions = np.round(predictions)\n",
    "\n",
    "# Calculate metrics\n",
    "hamming = hamming_loss(labels_test, predictions)\n",
    "f1 = f1_score(labels_test, predictions, average='micro')\n",
    "precision = precision_score(labels_test, predictions, average='micro')\n",
    "recall = recall_score(labels_test, predictions, average='micro')\n",
    "subset_acc = accuracy_score(labels_test, predictions)\n",
    "\n",
    "# Print results\n",
    "print(f'Hamming Loss: {hamming}')\n",
    "print(f'F1 Score: {f1}')\n",
    "print(f'Precision: {precision}')\n",
    "print(f'Recall: {recall}')\n",
    "print(f'Subset Accuracy: {subset_acc}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6078fb39-de5b-4854-812c-21e687dd0f33",
   "metadata": {},
   "source": [
    "### Figs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "145cd669-e196-467b-823e-42f7e53ccadb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "736/736 [==============================] - 8s 11ms/step\n",
      "Hamming Loss: 0.034163443967532195\n",
      "F1 Score: 0.6050084347413073\n",
      "Precision: 0.7483186127542338\n",
      "Recall: 0.507766323024055\n",
      "Subset Accuracy: 0.3714249288173048\n"
     ]
    }
   ],
   "source": [
    "# Baseline custom embedding performance\n",
    "base_custom_model = Sequential()\n",
    "base_custom_model.add(Embedding(input_dim = len(word_index) + 1,\n",
    "                    output_dim = 300,\n",
    "                    weights=[embedding_matrix],\n",
    "                    trainable=True))\n",
    "base_custom_model.add(LSTM(units=128, return_sequences=False))\n",
    "base_custom_model.add(Dense(len(mlb.classes_), activation='sigmoid'))\n",
    "\n",
    "base_custom_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=[micro_precision, micro_f1_score])\n",
    "\n",
    "base_custom_model.load_weights('weights/base_custom_embedding/model_epoch_04.weights.h5')\n",
    "\n",
    "base_custom_results = test_model(base_custom_model, labels_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7c724fc9-1e94-4d8f-998f-96d43c9fe63a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "736/736 [==============================] - 8s 10ms/step\n",
      "Hamming Loss: 0.03473007238678056\n",
      "F1 Score: 0.602454962624248\n",
      "Precision: 0.7343848829854522\n",
      "Recall: 0.5107079037800687\n",
      "Subset Accuracy: 0.37176490586885386\n"
     ]
    }
   ],
   "source": [
    "# Baseline google embedding performance\n",
    "base_google_model = Sequential()\n",
    "base_google_model.add(Embedding(input_dim = len(word_index) + 1,\n",
    "                    output_dim = 300,\n",
    "                    weights=[google_embedding_matrix],\n",
    "                    trainable=True))\n",
    "base_google_model.add(LSTM(units=128, return_sequences=False))\n",
    "base_google_model.add(Dense(len(mlb.classes_), activation='sigmoid'))\n",
    "\n",
    "# Compile the model\n",
    "base_google_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=[micro_precision, micro_f1_score])\n",
    "\n",
    "base_google_model.load_weights('weights/base_google_embedding/model_epoch_04.weights.h5')\n",
    "\n",
    "base_google_results = test_model(base_google_model, labels_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7a107b41-0dbe-447b-a7ae-74cc7987ddb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final model performance\n",
    "final_model = create_model(256, 0.3, 0.001)\n",
    "final_model.load_weights('weights/final_model/final_model_epoch_25.weights.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "74104c3a-1b05-41d5-8b77-9b01a0b1d0b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "736/736 [==============================] - 17s 23ms/step\n",
      "Hamming Loss: 0.03299620375092073\n",
      "F1 Score: 0.6400024727227769\n",
      "Precision: 0.7268578649910485\n",
      "Recall: 0.571688892570198\n",
      "Subset Accuracy: 0.41199218086010536\n"
     ]
    }
   ],
   "source": [
    "# Test\n",
    "predictions = final_model.predict(data_test)\n",
    "predictions = np.round(predictions)\n",
    "\n",
    "# Calculate metrics\n",
    "hamming = hamming_loss(labels_test, predictions)\n",
    "f1 = f1_score(labels_test, predictions, average='micro')\n",
    "precision = precision_score(labels_test, predictions, average='micro')\n",
    "recall = recall_score(labels_test, predictions, average='micro')\n",
    "subset_acc = accuracy_score(labels_test, predictions)\n",
    "\n",
    "# Print results\n",
    "print(f'Hamming Loss: {hamming}')\n",
    "print(f'F1 Score: {f1}')\n",
    "print(f'Precision: {precision}')\n",
    "print(f'Recall: {recall}')\n",
    "print(f'Subset Accuracy: {subset_acc}')\n",
    "\n",
    "final_results = {'hamming':hamming, 'f1':f1, 'precision':precision, 'recall':recall, 'subset_acc':subset_acc}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "c686900e-88ef-4dc0-ba50-e41fe7473a66",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\joetn\\AppData\\Local\\Temp\\ipykernel_20784\\314804146.py:16: UserWarning: The palette list has more values (6) than needed (3), which may not be intended.\n",
      "  ax = sns.barplot(x='Metric', y='value', hue='Model', data=pd.melt(df[df['Metric'] != 'Hamming Loss'], id_vars='Metric', var_name='Model', value_name='value'), palette=palette)\n",
      "C:\\Users\\joetn\\AppData\\Local\\Temp\\ipykernel_20784\\314804146.py:22: UserWarning: The palette list has more values (6) than needed (3), which may not be intended.\n",
      "  ax2 = sns.barplot(x='Metric', y='value', hue='Model', data=pd.melt(df[df['Metric'] == 'Hamming Loss'], id_vars='Metric', var_name='Model', value_name='value'), ax=ax2, dodge=True, palette=palette)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAABMkAAAK7CAYAAADoathkAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8fJSN1AAAACXBIWXMAAA9hAAAPYQGoP6dpAACT+ElEQVR4nOzdeVRVZf///9cRZBAUEwycUBwScShvcEAjtQxFyz6pRXY7D3dEakqOkTlkWWqGVmCahlopleVdRimVmgOVAw4plZV60hsksBRFQOX8/ujn+XYEFRA8wX4+1tprua/93td+b6S17LWuvbfJYrFYBAAAAAAAABhYFXs3AAAAAAAAANgbIRkAAAAAAAAMj5AMAAAAAAAAhkdIBgAAAAAAAMMjJAMAAAAAAIDhEZIBAAAAAADA8AjJAAAAAAAAYHiEZAAAAAAAADA8QjIAAAAAAAAYHiEZAAD/cPHx8TKZTDKZTNq8eXOh4xaLRU2bNpXJZFLXrl3L9Nomk0kzZswo8XlHjx6VyWRSfHx8seoub1WqVJGnp6d69eql5OTk0jV9Da+++qqaNm0qJycnmUwm/fnnn2V+DaPYvHmz9e/tan/Pd999t0wmkxo1alSqa7z77ruKiYkp0TnF/d0DAAC4EiEZAAAVRPXq1bVs2bJC41u2bNEvv/yi6tWr26GrsjFmzBglJydr69atmjNnjvbt26du3bopJSWlzK6xd+9ejR07Vt26ddNXX32l5OTkCv0z+6e42u/lkSNHtHnzZtWoUaPUc5cmJKtTp46Sk5PVu3fvUl8XAAAYEyEZAAAVRHh4uNauXaszZ87YjC9btkzBwcHy9fW1U2c3ztfXVx07dlTnzp31n//8R6tWrVJeXp5iY2NveO6cnBxJ0sGDByVJo0aN0p133qmOHTvKwcGhTOY2svDwcG3btk2HDx+2GV++fLnq1aunzp0735Q+Ll26pLy8PDk7O6tjx46qXbv2TbkuAACoPAjJAACoIAYMGCBJWr16tXXs9OnTWrt2rYYPH17kOadOnVJkZKTq1asnJycnNW7cWNHR0crLy7OpO3PmjEaNGiVPT0+5u7urZ8+e+umnn4qc8/Dhw3r00Ud16623ytnZWS1atNDrr79eRnf5l44dO0qSjh07Zh374osvdM8996hGjRqqVq2aOnfurC+//NLmvBkzZshkMmnPnj3q37+/brnlFjVp0kRdu3bVwIEDJUkdOnSQyWTS0KFDrectX75ct99+u1xcXFSrVi09+OCDSk1NtZl76NChcnd314EDBxQaGqrq1avrnnvukfTXY6mjR4/WW2+9pebNm8vV1VVBQUH65ptvZLFYNG/ePPn5+cnd3V133323fv75Z5u5k5KS9MADD6h+/fpycXFR06ZN9dhjjykzM7PI+zt48KAGDBggDw8PeXt7a/jw4Tp9+rRNbUFBgV599VXdcccdcnV1Vc2aNdWxY0d9/PHHNnUJCQkKDg6Wm5ub3N3d1aNHjxKt4Lv33nvVoEEDLV++3ObaK1as0JAhQ1SlSuF/blosFsXGxlp7u+WWW9S/f3/9+uuv1pquXbvq008/1bFjx2weyZX+3yOVc+fO1ezZs+Xn5ydnZ2dt2rTpqo9b/vDDDxowYIC8vb3l7OwsX19fDR482PrfQk5OjiZMmCA/Pz/r70FQUJDNf28AAKByIyQDAKCCqFGjhvr3728TRqxevVpVqlRReHh4ofrc3Fx169ZNK1euVFRUlD799FMNHDhQc+fOVd++fa11FotF//d//6dVq1bpqaee0kcffaSOHTsqLCys0JyHDh1Su3bt9P333+vll1/W+vXr1bt3b40dO1YzZ84ss3u9HCJdXg309ttvKzQ0VDVq1NCKFSv03nvvqVatWurRo0ehoEyS+vbtq6ZNm+r999/X4sWLFRsbq2eeeUaS9NZbbyk5OVnTpk2TJM2ZM0cjRoxQy5Yt9eGHH2rhwoXav3+/goODC62Oys/PV58+fXT33Xfrv//9r809r1+/Xm+++aZefPFFrV69WtnZ2erdu7eeeuopbd++Xa+99pqWLFmiQ4cOqV+/frJYLNZzf/nlFwUHBysuLk4bN27Us88+q2+//VZ33nmnLly4UOj++vXrp9tuu01r167VlClT9O6772r8+PE2NUOHDtWTTz6pdu3aKSEhQWvWrFGfPn109OhRa80LL7ygAQMGKCAgQO+9955WrVql7OxshYSE6NChQ8X6u6pSpYqGDh2qlStX6tKlS5KkjRs36vjx4xo2bFiR5zz22GMaN26cunfvrnXr1ik2NlYHDx5Up06ddPLkSUlSbGysOnfuLB8fHyUnJ1u3v1u0aJG++uorzZ8/X5999pn8/f2LvN6+ffvUrl07ffPNN5o1a5Y+++wzzZkzR3l5ecrPz5ckRUVFKS4uTmPHjtXnn3+uVatW6aGHHlJWVlaxfg4AAKASsAAAgH+0t956yyLJsnPnTsumTZsskizff/+9xWKxWNq1a2cZOnSoxWKxWFq2bGnp0qWL9bzFixdbJFnee+89m/leeukliyTLxo0bLRaLxfLZZ59ZJFkWLlxoU/f8889bJFmmT59uHevRo4elfv36ltOnT9vUjh492uLi4mI5deqUxWKxWI4cOWKRZHnrrbeueW+X61566SXLhQsXLLm5uZbdu3db2rVrZ5Fk+fTTTy3nzp2z1KpVy3L//ffbnHvp0iXL7bffbmnfvr11bPr06RZJlmefffaaP8fL/vjjD4urq6ulV69eNrVms9ni7OxsefTRR61jQ4YMsUiyLF++vNDckiw+Pj6Ws2fPWsfWrVtnkWS54447LAUFBdbxmJgYiyTL/v37i/yZFBQUWC5cuGA5duyYRZLlv//9b6H7mzt3rs05kZGRFhcXF+t1vv76a4skS3R0dJHXuHyPjo6OljFjxtiMZ2dnW3x8fCwPP/zwVc+1WCzW38X333/f8uuvv1pMJpNl/fr1FovFYnnooYcsXbt2tVgsFkvv3r0tDRs2tJ6XnJxskWR5+eWXbeb77bffLK6urpZJkyZZx64897LLvzdNmjSx5OfnF3ns7797d999t6VmzZqWjIyMq95Pq1atLP/3f/93zXsGAACVGyvJAACoQLp06aImTZpo+fLlOnDggHbu3HnVRy2/+uorubm5qX///jbjlx8zvLwCa9OmTZKkf//73zZ1jz76qM1+bm6uvvzySz344IOqVq2aLl68aN169eql3NxcffPNN6W6r8mTJ6tq1apycXFRYGCgzGaz3njjDfXq1Us7duzQqVOnNGTIEJtrFhQUqGfPntq5c6fOnTtnM1+/fv2Kdd3k5GSdP3/e5tFLSWrQoIHuvvvuIlepXW3ubt26yc3NzbrfokULSVJYWJj1McG/j//9UdKMjAxFRESoQYMGcnR0VNWqVdWwYUNJKvTYpyT16dPHZr9NmzbKzc1VRkaGJOmzzz6TJD3xxBNF37ikDRs26OLFixo8eLDNz9XFxUVdunQp8kuqV+Pn56euXbtq+fLlysrK0n//+9+r/l6uX79eJpNJAwcOtLmuj4+Pbr/99hJdt0+fPqpateo1a3JycrRlyxY9/PDD13xPWfv27fXZZ59pypQp2rx5s86fP1/sPgAAQOXgaO8GAABA8ZlMJg0bNkyLFi1Sbm6ubrvtNoWEhBRZm5WVJR8fH5uARpJuvfVWOTo6Wh8jy8rKkqOjozw9PW3qfHx8Cs138eJFvfrqq3r11VeLvOaV79AqrieffFIDBw5UlSpVVLNmTfn5+Vn7vvz43ZVh39+dOnXKJqCqU6dOsa57+WdQVH3dunWVlJRkM1atWrWrfq2xVq1aNvtOTk7XHM/NzZX01/u7QkND9b///U/Tpk1T69at5ebmpoKCAnXs2LHIsObKvytnZ2dJstb+/vvvcnBwKPR3+HeXf67t2rUr8nhR7xK7lhEjRmjYsGFasGCBXF1dr/r3dfLkSVksFnl7exd5vHHjxsW+ZnH+nv/44w9dunRJ9evXv2bdokWLVL9+fSUkJOill16Si4uLevTooXnz5qlZs2bF7gkAAFRchGQAAFQwQ4cO1bPPPqvFixfr+eefv2qdp6envv32W1ksFpugLCMjQxcvXpSXl5e17uLFi8rKyrIJX9LT023mu+WWW+Tg4KBBgwZddYWSn59fqe6pfv36CgoKKvLY5T5fffVV6wv9r3Rl4HJlMHg1l+83LS2t0LH//e9/1muXdN6S+P7777Vv3z7Fx8dryJAh1vErX+5fErVr19alS5eUnp5+1SDp8r198MEH1lVrN6Jv37564okn9OKLL2rUqFFydXW96nVNJpO2bt1qDff+rqixqynO30etWrXk4OCg48ePX7POzc1NM2fO1MyZM3Xy5EnrqrL7779fP/zwQ7F7AgAAFRePWwIAUMHUq1dPEydO1P33328Tqlzpnnvu0dmzZ7Vu3Tqb8ZUrV1qPS389JihJ77zzjk3du+++a7NfrVo1devWTSkpKWrTpo2CgoIKbVeucCoLnTt3Vs2aNXXo0KEirxkUFGRdnVVSwcHBcnV11dtvv20zfvz4cX311VfWn1F5uhz0XBkOvfHGG6We8/JHF+Li4q5a06NHDzk6OuqXX3656s+1JFxdXfXss8/q/vvv1+OPP37Vuvvuu08Wi0UnTpwo8pqtW7e21jo7O9/wY4+urq7q0qWL3n///WKvdPT29tbQoUM1YMAA/fjjj8rJybmhHgAAQMXASjIAACqgF1988bo1gwcP1uuvv64hQ4bo6NGjat26tbZt26YXXnhBvXr1Uvfu3SVJoaGhuuuuuzRp0iSdO3dOQUFB2r59u1atWlVozoULF+rOO+9USEiIHn/8cTVq1EjZ2dn6+eef9cknn+irr74q83t1d3fXq6++qiFDhujUqVPq37+/br31Vv3+++/at2+ffv/992uGQddSs2ZNTZs2TU8//bQGDx6sAQMGKCsrSzNnzpSLi4umT59exndTmL+/v5o0aaIpU6bIYrGoVq1a+uSTTwo96lkSISEhGjRokGbPnq2TJ0/qvvvuk7Ozs1JSUlStWjWNGTNGjRo10qxZsxQdHa1ff/1VPXv21C233KKTJ0/qu+++s66sKomoqChFRUVds6Zz5876z3/+o2HDhmnXrl2666675ObmprS0NG3btk2tW7e2hmytW7fWhx9+qLi4OAUGBqpKlSolDu8kacGCBbrzzjvVoUMHTZkyRU2bNtXJkyf18ccf64033lD16tXVoUMH3XfffWrTpo1uueUWpaamatWqVQoODla1atVKfE0AAFDxEJIBAFBJubi4aNOmTYqOjta8efP0+++/q169epowYYJN+FOlShV9/PHHioqK0ty5c5Wfn6/OnTsrMTFR/v7+NnMGBARoz549eu655/TMM88oIyNDNWvWVLNmzdSrV69yu5eBAwfK19dXc+fO1WOPPabs7GzdeuutuuOOOwq9dL+kpk6dqltvvVWLFi1SQkKCXF1d1bVrV73wwgs35V1UVatW1SeffKInn3xSjz32mBwdHdW9e3d98cUX8vX1LfW88fHx+te//qVly5YpPj5erq6uCggI0NNPP22tmTp1qgICArRw4UKtXr1aeXl58vHxUbt27RQREVEWt1ekN954Qx07dtQbb7yh2NhYFRQUqG7duurcubPat29vrXvyySd18OBBPf300zp9+rQsFossFkuJr3f77bfru+++0/Tp0zV16lRlZ2fLx8dHd999t3UV4t13362PP/5Yr7zyinJyclSvXj0NHjxY0dHRZXbfAADgn81kKc2/NAAAAAAAAIBKhHeSAQAAAAAAwPAIyQAAAAAAAGB4hGQAAAAAAAAwPEIyAAAAAAAAGB4hGQAAAAAAAAyPkAwAAAAAAACG52jvBm62ixcvKiUlRd7e3qpShYwQAAAAAABUDgUFBTp58qTatm0rR0fDRT43zHA/sZSUFLVv397ebQAAAAAAAJSL7777Tu3atbN3GxWO4UIyb29vSX/9wtSpU8fO3QAAAAAAAJSNtLQ0tW/f3pp9oGQMF5JdfsSyTp06ql+/vp27AQAAAAAAKFu8Xqp0+KkBAAAAAADA8AjJAAAAAAAAYHiEZAAAAAAAADA8w72TrDgsFosuXryoS5cu2bsVVCIODg5ydHSUyWSydysAAAAAAOAKhGRXyM/PV1pamnJycuzdCiqhatWqqU6dOnJycrJ3KwAAAAAA4G8Iyf6moKBAR44ckYODg+rWrSsnJydW/aBMWCwW5efn6/fff9eRI0fUrFkzvjYCAAAAAMA/CCHZ3+Tn56ugoEANGjRQtWrV7N0OKhlXV1dVrVpVx44dU35+vlxcXOzdEgAAAAAA+P+xlKUIrPBBeeF3CwAAAACAfyb+jx0AAAAAAACGR0gGAAAAAAAAwyMkQ5nbvHmzTCaT/vzzz2Kf06hRI8XExJRbTwAAAAAAANdCSGZAQ4cOlclkUkRERKFjkZGRMplMGjp06M1vDAAAAAAAwE4IyQyqQYMGWrNmjc6fP28dy83N1erVq+Xr62vHzgAAAAAAAG4+QjKD+te//iVfX199+OGH1rEPP/xQDRo0UNu2ba1jeXl5Gjt2rG699Va5uLjozjvv1M6dO23mSkxM1G233SZXV1d169ZNR48eLXS9HTt26K677pKrq6saNGigsWPH6ty5c+V2fwAAAAAAACVBSGZgw4YN01tvvWXdX758uYYPH25TM2nSJK1du1YrVqzQnj171LRpU/Xo0UOnTp2SJP3222/q27evevXqpb1792rkyJGaMmWKzRwHDhxQjx491LdvX+3fv18JCQnatm2bRo8eXf43CQAAAAAAUAyEZAY2aNAgbdu2TUePHtWxY8e0fft2DRw40Hr83LlziouL07x58xQWFqaAgAAtXbpUrq6uWrZsmSQpLi5OjRs31iuvvKLmzZvr3//+d6H3mc2bN0+PPvqoxo0bp2bNmqlTp05atGiRVq5cqdzc3Jt5ywAAAAAAAEVytHcDsB8vLy/17t1bK1askMViUe/eveXl5WU9/ssvv+jChQvq3Lmzdaxq1apq3769UlNTJUmpqanq2LGjTCaTtSY4ONjmOrt379bPP/+sd955xzpmsVhUUFCgI0eOqEWLFuV1iwAAAAAAAMVCSGZww4cPtz72+Prrr9scs1gskmQTgF0evzx2ueZaCgoK9Nhjj2ns2LGFjvGRAAAAAAAA8E/A45YG17NnT+Xn5ys/P189evSwOda0aVM5OTlp27Zt1rELFy5o165d1tVfAQEB+uabb2zOu3L/X//6lw4ePKimTZsW2pycnMrpzgAAAAAAQHHExsbKz89PLi4uCgwM1NatW69Zv2XLFgUGBsrFxUWNGzfW4sWLbY5/+OGHCgoKUs2aNeXm5qY77rhDq1atsqmZMWOGTCaTzebj41Pm91YShGQG5+DgoNTUVKWmpsrBwcHmmJubmx5//HFNnDhRn3/+uQ4dOqRRo0YpJydHI0aMkCRFRETol19+UVRUlH788Ue9++67io+Pt5ln8uTJSk5O1hNPPKG9e/fq8OHD+vjjjzVmzJibdZsAAAAAAKAICQkJGjdunKKjo5WSkqKQkBCFhYXJbDYXWX/kyBH16tVLISEhSklJ0dNPP62xY8dq7dq11ppatWopOjpaycnJ2r9/v4YNG6Zhw4Zpw4YNNnO1bNlSaWlp1u3AgQPleq/Xw+OWUI0aNa567MUXX1RBQYEGDRqk7OxsBQUFacOGDbrlllsk/fW45Nq1azV+/HjFxsaqffv2euGFF2y+ktmmTRtt2bJF0dHRCgkJkcViUZMmTRQeHl7u9wYAAAAAgNFkZ2frzJkz1n1nZ2c5OzsXWbtgwQKNGDFCI0eOlCTFxMRow4YNiouL05w5cwrVL168WL6+voqJiZEktWjRQrt27dL8+fPVr18/SVLXrl1tznnyySe1YsUKbdu2zeYpNkdHR7uvHvs7k6U4L5WqRI4fP64GDRrot99+U/369W2O5ebm6siRI9YlhkBZ43cMAAAAAFBeLmceV5o+fbpmzJhRaDw/P1/VqlXT+++/rwcffNA6/uSTT2rv3r3asmVLoXPuuusutW3bVgsXLrSOffTRR3r44YeVk5OjqlWr2tRbLBZ99dVX6tOnj9atW6d7771X0l+PW86bN08eHh5ydnZWhw4d9MILL6hx48alvf0bxkoyAAAAAACASuTQoUOqV6+edf9qq8gyMzN16dIleXt724x7e3srPT29yHPS09OLrL948aIyMzNVp04dSdLp06dVr1495eXlycHBQbGxsdaATJI6dOiglStX6rbbbtPJkyc1e/ZsderUSQcPHpSnp2ep7vtGEZIBAAAAAABUItWrV7/mq5WuZDKZbPYtFkuhsevVXzlevXp17d27V2fPntWXX36pqKgoNW7c2PooZlhYmLW2devWCg4OVpMmTbRixQpFRUUVu/eyREgGAAAAAABgQF5eXnJwcCi0aiwjI6PQarHLfHx8iqx3dHS0WQFWpUoVNW3aVJJ0xx13KDU1VXPmzCn0vrLL3Nzc1Lp1ax0+fPgG7ujGEJIBAOzCbDYrMzPT3m2UOy8vL/n6+tq7DQAAUEnwbyiUJScnJwUGBiopKcnmnWRJSUl64IEHijwnODhYn3zyic3Yxo0bFRQUVOh9ZH9nsViUl5d31eN5eXlKTU1VSEhICe+i7BCSAQBuOrPZrOb+/so9f97erZQ7F1dX/fjDD/wjDwAA3DD+DYXyEBUVpUGDBikoKEjBwcFasmSJzGazIiIiJElTp07ViRMntHLlSklSRESEXnvtNUVFRWnUqFFKTk7WsmXLtHr1auucc+bMUVBQkJo0aaL8/HwlJiZq5cqViouLs9ZMmDBB999/v3x9fZWRkaHZs2frzJkzGjJkyM39AfwNIRkA4KbLzMxU7vnz8uvdTy6eXvZup9zkZmXqyKdrlZmZyT/wAADADbv8b6gWnfvJzaO2vdspN+dO/67U7fwb6mYJDw9XVlaWZs2apbS0NLVq1UqJiYlq2LChJCktLU1ms9la7+fnp8TERI0fP16vv/666tatq0WLFqlfv37WmnPnzikyMlLHjx+Xq6ur/P399fbbbys8PNxac/z4cQ0YMECZmZmqXbu2OnbsqG+++cZ6XXsgJAMA2I2Lp5fcfOrauw0AAIAKxc2jtqp78m8olJ3IyEhFRkYWeSw+Pr7QWJcuXbRnz56rzjd79mzNnj37mtdcs2ZNiXq8GarYuwEAAAAAAADA3gjJAAAAAAAAYHg8bllM99056aZeb/22uWUyT9euXXXHHXcoJiamTOaTpBkzZmjdunXau3dvmc0JAAAAAABgT4RklcTQoUO1YsWKQuPffvutWrRoYYeOAAAAAAAAKg5CskqkZ8+eeuutt2zGateuLQcHBzt1BAAAAAAAUDHwTrJKxNnZWT4+PjbbPffco3HjxllrGjVqpBdeeEHDhw9X9erV5evrqyVLltjMM3nyZN12222qVq2aGjdurGnTpunChQs3+W4AAAAAAABuHlaSGdDLL7+s5557Tk8//bQ++OADPf7447rrrrvk7+8vSapevbri4+NVt25dHThwQKNGjVL16tU1adLNfS8bAAAAAMC4UlNT7d1CufPy8pKvr6+928D/j5CsElm/fr3c3d2t+2FhYUXW9erVS5GRkZL+WjX2yiuvaPPmzdaQ7JlnnrHWNmrUSE899ZQSEhIIyYCbxGw2KzMz095tlCsj/IMHAAAApZN3PlsymTRw4EB7t1LuXFxd9eMPPxCU/UMQklUi3bp1U1xcnHXfzc1NAwYMKFTXpk0b659NJpN8fHyUkZFhHfvggw8UExOjn3/+WWfPntXFixdVo0aN8m0egKS/ArLm/v7KPX/e3q0AAAAAdnExP1eyWOTXu59cPL3s3U65yc3K1JFP1yozM5OQ7B+CkKwScXNzU9OmTa9bV7VqVZt9k8mkgoICSdI333yjRx55RDNnzlSPHj3k4eGhNWvW6OWXXy6XngHYyszMVO7582rRuZ/cPGrbu51yk3niJx3d95W92wAAAMA/mIunl9x86tq7DRgIIRlsbN++XQ0bNlR0dLR17NixY3bsCDAmN4/aqu5Zef9BcO707/ZuAQAAAABsEJLBRtOmTWU2m7VmzRq1a9dOn376qT766CN7twUAAAAAAFCuCMmKaf22ufZu4aZ44IEHNH78eI0ePVp5eXnq3bu3pk2bphkzZti7NQAAAAAAgHJDSFZJxMfHFzm+efNmm/2jR48Wqtm7d6/N/ty5czV3rm0oOG7cOOufZ8yYQWgGu+CrjwAAAACA8kJIBqBCMJvNat7cX7m5fPURAAAAAFD2CMkAVAiZmZnKzT2vO5r1lns1T3u3U24yTv2qn37bZu82AAAAAMBwCMkAVCju1Tzl4e5t7zbKzdmcLHu3AAAAAACGVMXeDQAAAAAAAAD2RkgGAAAAAAAAwyMkAwAAAAAAgOERkgEAAAAAAMDwCMkAAAAAAABgeIRkAAAAAAAAMDxHezdQUdw9+Lmber2vVk67qdcrb3fddZciIiL06KOP2ruVIsXHx2vcuHH6888/i1Wfl5enZs2a6aOPPlJgYGD5NgcAAAAAAModK8kqiaFDh8pkMlk3T09P9ezZU/v377d3a1q/fr3S09P1yCOP2IynpKQoPDxcderUkbOzsxo2bKj77rtPn3zyiSwWi526LR5nZ2dNmDBBkydPtncrAAAAAACgDLCSrBLp2bOn3nrrLUlSenq6nnnmGd13330ym8127WvRokUaNmyYqlT5f5nsf//7Xz388MPq3r27VqxYoSZNmigrK0v79+/XM888o5CQENWsWdN+TRfDv//9b02cOFGpqalq0aKFvdsBAAAADM1sNiszM9PebZSr1NRUe7cAVGqEZJWIs7OzfHx8JEk+Pj6aPHmy7rrrLv3++++qXbu2JGny5Mn66KOPdPz4cfn4+Ojf//63nn32WVWtWlWStG/fPo0bN067du2SyWRSs2bN9MYbbygoKEiStGPHDk2ZMkU7d+6Ul5eXHnzwQc2ZM0dubm5F9pSZmakvvvhCr7zyinXs3LlzGjFihHr37q0PP/zQOt6kSRO1b99eI0eOtFlJtmXLFk2cOFH79u1TrVq1NGTIEM2ePVuOjn/9+ubl5WnixIlas2aNzpw5o6CgIL3yyitq166ddY6PP/5YTz31lI4fP66OHTtq6NChGjp0qP7444+rhnGffPKJZsyYoYMHD6pu3boaMmSIoqOjrdf19PRUp06dtHr1as2aNatEf1cAAAAAyo7ZbFbz5v7KzT1v71YAVGCEZJXU2bNn9c4776hp06by9PS0jlevXl3x8fGqW7euDhw4oFGjRql69eqaNGmSpL9WR7Vt21ZxcXFycHDQ3r17rQHagQMH1KNHDz333HNatmyZfv/9d40ePVqjR4+2rmC70rZt21StWjWblVYbN25UVlaW9ZpFMZlMkqQTJ06oV69eGjp0qFauXKkffvhBo0aNkouLi2bMmCFJmjRpktauXasVK1aoYcOGmjt3rnr06KGff/5ZtWrV0tGjR9W/f389+eSTGjlypFJSUjRhwoRr/vw2bNiggQMHatGiRQoJCdEvv/yi//znP5Kk6dOnW+vat2+vrVu3XnMuAAAAAOUrMzNTubnndUez3nKv5nn9EyqojFO/6qffttm7DaDSIiSrRNavXy93d3dJf63WqlOnjtavX2/zmOMzzzxj/XOjRo301FNPKSEhwRpYmc1mTZw4Uf7+/pKkZs2aWevnzZunRx99VOPGjbMeW7Rokbp06aK4uDi5uLgU6uno0aPy9va26eGnn36SJDVv3tw6tnPnTnXr1s26v2bNGt13332KjY1VgwYN9Nprr8lkMsnf31//+9//NHnyZD377LM6f/684uLiFB8fr7CwMEnS0qVLlZSUpGXLlmnixIlavHixmjdvrnnz5lmv+/333+v555+/6s/y+eef15QpUzRkyBBJUuPGjfXcc89p0qRJNiFZvXr1dPTo0avOAwAAAODmca/mKQ93b3u3UW7O5mTZuwWgUiMkq0S6deumuLg4SdKpU6cUGxursLAwfffdd2rYsKEk6YMPPlBMTIx+/vlnnT17VhcvXlSNGjWsc0RFRWnkyJFatWqVunfvroceekhNmjSRJO3evVs///yz3nnnHWu9xWJRQUGBjhw5UuR7uc6fP19keHalNm3aaO/evZL+Ct8uXrwo6a9n7oODg60ryySpc+fOOnv2rI4fP64///xTFy5cUOfOna3Hq1atqvbt21uf1//xxx9tHr2U/loBdi27d+/Wzp07bYK0S5cuKTc3Vzk5OapWrZokydXVVTk5Ode9PwAAAAAA8M/G1y0rETc3NzVt2lRNmzZV+/bttWzZMp07d05Lly6VJH3zzTd65JFHFBYWpvXr1yslJUXR0dHKz8+3znH5HVy9e/fWV199pYCAAH300UeSpIKCAj322GPau3evddu3b58OHz5sDdKu5OXlpT/++MNm7PLqtB9//NE65uzsbO397ywWi01AdnlM+uuRzL//+WrnXWuOqykoKNDMmTNt7vXAgQM6fPiwTeh36tQp6/veAAAAAABAxWX3kCw2NlZ+fn5ycXFRYGDgNd/vNHToUJlMpkJby5Ytb2LHFYfJZFKVKlV0/vxfL6/cvn27GjZsqOjoaAUFBalZs2Y6duxYofNuu+02jR8/Xhs3blTfvn2t7xv717/+pYMHD1rDrL9vTk5ORfbQtm1bpaen2wRloaGhqlWrll566aXr3kNAQIB27NhhE2rt2LFD1atXV7169azX3rbt/z2Xf+HCBe3atcu6ss3f3187d+60mXfXrl3XvO6//vUv/fjjj0Xe698fHf3+++/Vtm3b694HAAAAAAD4Z7NrSJaQkKBx48YpOjpaKSkpCgkJUVhYmMxmc5H1CxcuVFpamnX77bffVKtWLT300EM3ufN/pry8PKWnpys9PV2pqakaM2aMzp49q/vvv1+S1LRpU5nNZq1Zs0a//PKLFi1aZF0lJv31aOTo0aO1efNmHTt2TNu3b9fOnTutYdPkyZOVnJysJ554Qnv37tXhw4f18ccfa8yYMVftqW3btqpdu7a2b99uHXN3d9ebb76pTz/9VL1799aGDRv066+/av/+/Zo7d64kycHBQZIUGRmp3377TWPGjNEPP/yg//73v5o+fbqioqJUpUoVubm56fHHH9fEiRP1+eef69ChQxo1apRycnI0YsQISdJjjz2mH374QZMnT9ZPP/2k9957T/Hx8ZIKr0C77Nlnn9XKlSutK+tSU1OVkJBg8043Sdq6datCQ0NL8tcEAAAAAAD+gez6TrIFCxZoxIgRGjlypCQpJiZGGzZsUFxcnObMmVOo3sPDQx4eHtb9devW6Y8//tCwYcPKvdevVk4r92vcqM8//1x16tSR9NdXLP39/fX++++ra9eukqQHHnhA48eP1+jRo5WXl6fevXtr2rRp1q9EOjg4KCsrS4MHD9bJkyfl5eWlvn37aubMmZL+em/Yli1bFB0drZCQEFksFjVp0kTh4eFX7cnBwUHDhw/XO++8o/vuu886/uCDD2rHjh166aWXNHjwYJ06dUoeHh4KCgqyvrRf+uvF+ImJiZo4caJuv/121apVSyNGjLAJq1588UUVFBRo0KBBys7OVlBQkDZs2KBbbrlFkuTn56cPPvhATz31lBYuXKjg4GBFR0fr8ccfl7Ozc5F99+jRQ+vXr9esWbM0d+5cVa1aVf7+/tbfVUlKTk7W6dOn1b9//xL+TQEAAAAAgH8au4Vk+fn52r17t6ZMmWIzHhoaqh07dhRrjmXLlql79+7Wl9IXJS8vT3l5edb97Ozs0jX8DxcfH29dHXUtc+fOta7Wuuzy1yqdnJy0evXqa57frl07bdy4sUS9jRs3Ti1bttSxY8ds/q6CgoL0/vvvX/f8Ll266LvvvrvqcRcXFy1atEiLFi26ak2fPn3Up08f6/7zzz+v+vXrW98vNnToUA0dOtTmnB49eqhHjx5XnXPBggWaOHGiXF1dr3sPAAAAAADgn81uj1tmZmbq0qVL8va2/Tyvt7e30tPTr3t+WlqaPvvsM5uVPUWZM2eOdQWah4eHAgICbqhvlJy3t7eWLVt21cdob4bY2Fjt3LlTv/76q1atWqV58+ZpyJAhpZ4vLy9Pt99+u8aPH1+GXQIAAAAAAHux6+OW0rW/Sngt8fHxqlmzpv7v//7vmnVTp05VVFSUdf/EiRMEZXbwwAMP2PX6hw8f1uzZs3Xq1Cn5+vrqqaee0tSpU0s9n7Ozc6H3kwEAAAAAgIrLbiGZl5eXHBwcCq0ay8jIKLS67EoWi0XLly/XoEGDrvpVxcucnZ1t3jt15syZ0jeNCuuVV17RK6+8Yu82AAAAAADAP5TdHrd0cnJSYGCgkpKSbMaTkpLUqVOna567ZcsW/fzzz9avFwIAAAAAAAA3wq6PW0ZFRWnQoEEKCgpScHCwlixZIrPZrIiICEl/PSp54sQJrVy50ua8ZcuWqUOHDmrVqpU92gYAAAAAAEAlY9eQLDw8XFlZWZo1a5bS0tLUqlUrJSYmWr+AmJaWVuhl76dPn9batWu1cOFCe7QMAAAAAACASsjuL+6PjIxUZGRkkcfi4+MLjXl4eCgnJ6ecuwIAAAAAAICR2O2dZAAAAAAAAMA/BSEZAAAAAAAADM/uj1tWFO0mz7qp19v50rM39Xrl7a677lJERIQeffRRe7dSbtq1a6epU6eqb9++9m4FAAAAAACUECvJKomhQ4fKZDJZN09PT/Xs2VP79++3d2tav3690tPT9cgjj9iMp6Sk6KGHHpK3t7dcXFx02223adSoUfrpp5/K5Lrx8fGqWbNmmcxVHNOmTdOUKVNUUFBw064JAAAAAADKBiFZJdKzZ0+lpaUpLS1NX375pRwdHXXffffZuy0tWrRIw4YNU5Uq/+/Xbf369erYsaPy8vL0zjvvKDU1VatWrZKHh4emTZtmx25Lr3fv3jp9+rQ2bNhg71YAAAAAAEAJEZJVIs7OzvLx8ZGPj4/uuOMOTZ48Wb/99pt+//13a83kyZN12223qVq1amrcuLGmTZumCxcuWI/v27dP3bp1U/Xq1VWjRg0FBgZq165d1uM7duzQXXfdJVdXVzVo0EBjx47VuXPnrtpTZmamvvjiC/Xp08c6lpOTo2HDhqlXr176+OOP1b17d/n5+alDhw6aP3++3njjDUlFrwRbt26dTCbTdfvdvHmzhg0bptOnT1tX182YMUOS9Mcff2jw4MG65ZZbVK1aNYWFhenw4cPWOS9fd/369WrevLmqVaum/v3769y5c1qxYoUaNWqkW265RWPGjNGlS5es5zk4OKhXr15avXp1Mf/GAAAAAADAPwUhWSV19uxZvfPOO2ratKk8PT2t49WrV1d8fLwOHTqkhQsXaunSpXrllVesx//973+rfv362rlzp3bv3q0pU6aoatWqkqQDBw6oR48e6tu3r/bv36+EhARt27ZNo0ePvmof27ZtU7Vq1dSiRQvr2IYNG5SZmalJkyYVeU5JHpG8Wr+dOnVSTEyMatSoYV1dN2HCBEl/PZq6a9cuffzxx0pOTpbFYlGvXr1swsKcnBwtWrRIa9as0eeff67Nmzerb9++SkxMVGJiolatWqUlS5bogw8+sOmnffv22rp1a7H7BwAAAAAA/wy8uL8SWb9+vdzd3SVJ586dU506dbR+/XqbxxyfeeYZ658bNWqkp556SgkJCdbAymw2a+LEifL395ckNWvWzFo/b948Pfrooxo3bpz12KJFi9SlSxfFxcXJxcWlUE9Hjx6Vt7e3TQ+XV21dvsaNuFa/Hh4eMplM8vHxsbn2xx9/rO3bt6tTp06SpHfeeUcNGjTQunXr9NBDD0mSLly4oLi4ODVp0kSS1L9/f61atUonT56Uu7u7AgIC1K1bN23atEnh4eHW+evVqyez2ayCggKbewYAAAAAAP9s/F98JdKtWzft3btXe/fu1bfffqvQ0FCFhYXp2LFj1poPPvhAd955p3x8fOTu7q5p06bJbDZbj0dFRWnkyJHq3r27XnzxRf3yyy/WY7t371Z8fLzc3d2tW48ePVRQUKAjR44U2dP58+cLhWcWi6XM7vla/RYlNTVVjo6O6tChg3XM09NTzZs3V2pqqnWsWrVq1oBMkry9vdWoUSNrCHl5LCMjw2Z+V1dXFRQUKC8v70ZvDQAAAAAA3ESEZJWIm5ubmjZtqqZNm6p9+/ZatmyZzp07p6VLl0qSvvnmGz3yyCMKCwvT+vXrlZKSoujoaOXn51vnmDFjhg4ePKjevXvrq6++UkBAgD766CNJUkFBgR577DFrELd3717t27dPhw8ftgmU/s7Ly0t//PGHzdhtt90mSfrhhx+ueT9VqlQpFKj9/ZHI6/VblKsFdBaLxeZdZ5cfMb3MZDIVOXbllyxPnTqlatWqydXV9eo3BgAAAAAA/nEIySoxk8mkKlWq6Pz585Kk7du3q2HDhoqOjlZQUJCaNWtms8rssttuu03jx4/Xxo0b1bdvX7311luSpH/96186ePCgNYj7++bk5FRkD23btlV6erpNUBYaGiovLy/NnTu3yHP+/PNPSVLt2rWVnZ1t82GAvXv3FrtfJycnmxfrS1JAQIAuXryob7/91jqWlZWln376yea9aaX1/fff61//+tcNzwMAAAAAAG4uQrJKJC8vT+np6UpPT1dqaqrGjBmjs2fP6v7775ckNW3aVGazWWvWrNEvv/yiRYsW2ay6On/+vEaPHq3Nmzfr2LFj2r59u3bu3GkNjyZPnqzk5GQ98cQT2rt3r/X9XmPGjLlqT23btlXt2rW1fft265ibm5vefPNNffrpp+rTp4+++OILHT16VLt27dKkSZMUEREhSerQoYOqVaump59+Wj///LPeffddxcfHF7vfRo0a6ezZs/ryyy+VmZmpnJwcNWvWTA888IBGjRqlbdu2ad++fRo4cKDq1aunBx544Ib/DrZu3arQ0NAbngcAAAAAANxcvLi/mHa+9Ky9W7iuzz//XHXq1JH011cs/f399f7776tr166SpAceeEDjx4/X6NGjlZeXp969e2vatGmaMWOGJMnBwUFZWVkaPHiwTp48KS8vL/Xt21czZ86UJLVp00ZbtmxRdHS0QkJCZLFY1KRJE5sX11/JwcFBw4cP1zvvvKP77rvPOv7AAw9ox44dmjNnjh599FGdOXNGDRo00N13363Zs2dLkmrVqqW3335bEydO1JIlS9S9e3fNmDFD//nPf4rVb6dOnRQREaHw8HBlZWVp+vTpmjFjht566y09+eSTuu+++5Sfn6+77rpLiYmJhR6nLKkTJ05ox44devvtt29oHgAAAAAAcPOZLGX5FvUK4Pjx42rQoIF+++031a9f3+ZYbm6ujhw5Ij8/vyK/1IjSOXnypFq2bKndu3erYcOG9m6n3EycOFGnT5/WkiVLrlrD71jp7dmzR4GBgbrz9sHycPe2dzvl5kTGIe09/KmCekWoumdde7dTbtJ/3afU7WvVYvBjcvOpvPd5Lv1/Sl35hnbv3s2j2AAAlCP+rVi58G/F0rtW5oHr43FLlDtvb28tW7bM5iualdGtt96q5557zt5tAAAAAACAUuBxS9wUZfG+r3+6iRMn2rsFAAAAAABQSqwkAwAAAAAAgOERkgEAAAAAAMDwCMmKYLBvGeAm4ncLAAAAAIB/JkKyv6lataokKScnx86doLK6/Lt1+XcNAAAAAAD8M/Di/r9xcHBQzZo1lZGRIUmqVq2aTCaTnbtCZWCxWJSTk6OMjAzVrFlTDg4O9m4JAAAAAAD8DSHZFXx8fCTJGpQBZalmzZrW3zEAAAAAAPDPQUh2BZPJpDp16ujWW2/VhQsX7N0OKpGqVauyggwAAAAAgH8oQrKrcHBwINAAAAAAAAAwCF7cDwAAAAAAAMMjJAMAAAAAAIDhEZIBAAAAAADA8AjJAAAAAAAAYHiEZAAAAAAAADA8QjIAAAAAAAAYHiEZAAAAAAAADI+QDAAAAAAAwMBiY2Pl5+cnFxcXBQYGauvWrdes37JliwIDA+Xi4qLGjRtr8eLFNsc//PBDBQUFqWbNmnJzc9Mdd9yhVatW3fB1yxshGQAAAAAAgEElJCRo3Lhxio6OVkpKikJCQhQWFiaz2Vxk/ZEjR9SrVy+FhIQoJSVFTz/9tMaOHau1a9daa2rVqqXo6GglJydr//79GjZsmIYNG6YNGzaU+ro3AyEZAAAAAACAQS1YsEAjRozQyJEj1aJFC8XExKhBgwaKi4srsn7x4sXy9fVVTEyMWrRooZEjR2r48OGaP3++taZr16568MEH1aJFCzVp0kRPPvmk2rRpo23btpX6ujcDIRkAAAAAAEAlkp2drTNnzli3vLy8Iuvy8/O1e/duhYaG2oyHhoZqx44dRZ6TnJxcqL5Hjx7atWuXLly4UKjeYrHoyy+/1I8//qi77rqr1Ne9GQjJAAAAAAAAKpGAgAB5eHhYtzlz5hRZl5mZqUuXLsnb29tm3NvbW+np6UWek56eXmT9xYsXlZmZaR07ffq03N3d5eTkpN69e+vVV1/VvffeW+rr3gyOdrsyAAAAAAAAytyhQ4dUr149676zs/M1600mk82+xWIpNHa9+ivHq1evrr179+rs2bP68ssvFRUVpcaNG6tr166lvm55IyQDAAAAAACoRKpXr64aNWpct87Ly0sODg6FVm9lZGQUWuV1mY+PT5H1jo6O8vT0tI5VqVJFTZs2lSTdcccdSk1N1Zw5c9S1a9dSXfdm4HFLAAAAAAAAA3JyclJgYKCSkpJsxpOSktSpU6cizwkODi5Uv3HjRgUFBalq1apXvZbFYrG+G600170ZWElWhsxms83zt5WVl5eXfH197d0GAAAAAAC4QVFRURo0aJCCgoIUHBysJUuWyGw2KyIiQpI0depUnThxQitXrpQkRURE6LXXXlNUVJRGjRql5ORkLVu2TKtXr7bOOWfOHAUFBalJkybKz89XYmKiVq5cafPlyutd1x4IycqI2WxW8+b+ys09b+9Wyp2Li6t+/PEHgjIAAAAAACq48PBwZWVladasWUpLS1OrVq2UmJiohg0bSpLS0tJkNput9X5+fkpMTNT48eP1+uuvq27dulq0aJH69etnrTl37pwiIyN1/Phxubq6yt/fX2+//bbCw8OLfV17ICQrI5mZmcrNPa87mvWWezXP659QQZ3NydLew58qMzOTkAwAAAAAgEogMjJSkZGRRR6Lj48vNNalSxft2bPnqvPNnj1bs2fPvqHr2gMhWRlzr+YpD3f7vWQOAAAAAAAAJceL+wEAAAAAAGB4hGQAAAAAAAAwPEIyAAAAAAAAGB4hGQAAAAAAAAyPkAwAAAAAAACGR0gGAAAAAAAAwyMkAwAAAAAAgOERkgEAAAAAAMDwCMkAAAAAAABgeIRkAAAAAAAAMDxCMgAAAAAAABgeIRkAAAAAAAAMj5AMAAAAAAAAhkdIBgAAAAAAAMMjJAMAAAAAAIDhEZIBAAAAAADA8AjJAAAAAAAAYHiEZAAAAAAAADA8QjIAAAAAAAAYHiEZAAAAAAAADI+QDAAAAAAAAIZHSAYAAAAAAADDIyQDAAAAAACA4TnauwHgn8psNiszM9PebZQ7Ly8v+fr62rsNAAAAAADsyu4hWWxsrObNm6e0tDS1bNlSMTExCgkJuWp9Xl6eZs2apbffflvp6emqX7++oqOjNXz48JvYNSo7s9ms5v7+yj1/3t6tlDsXV1f9+MMPBGUAAAAAAEOza0iWkJCgcePGKTY2Vp07d9Ybb7yhsLAwHTp06Kr/w/7www/r5MmTWrZsmZo2baqMjAxdvHjxJneO1NRUe7dQrlJTU5V7/rz8eveTi6eXvdspN7lZmTry6VplZmYSkgEAAAAADM2uIdmCBQs0YsQIjRw5UpIUExOjDRs2KC4uTnPmzClU//nnn2vLli369ddfVatWLUlSo0aNbmbLhpebf1aSSQMHDrR3KzeFi6eX3Hzq2rsNAAAAAABQzuwWkuXn52v37t2aMmWKzXhoaKh27NhR5Dkff/yxgoKCNHfuXK1atUpubm7q06ePnnvuObm6uhZ5Tl5envLy8qz72dnZZXcTBnTxYp4ki1p07ic3j9r2bqfcZJ74SUf3fWXvNgAAAAAAwE1it5AsMzNTly5dkre3t824t7e30tPTizzn119/1bZt2+Ti4qKPPvpImZmZioyM1KlTp7R8+fIiz5kzZ45mzpxZ5v0bnZtHbVX3rLwrrM6d/t3eLQAAAAAAgJuoir0bMJlMNvsWi6XQ2GUFBQUymUx655131L59e/Xq1UsLFixQfHy8zl/lBetTp07V6dOnrduhQ4fK/B4AAAAAAABQsdktJPPy8pKDg0OhVWMZGRmFVpddVqdOHdWrV08eHh7WsRYtWshisej48eNFnuPs7KwaNWpYt+rVq5fdTQAAAAAAAKBSsFtI5uTkpMDAQCUlJdmMJyUlqVOnTkWe07lzZ/3vf//T2bNnrWM//fSTqlSpovr165drvwAAAAAAAKi87Pq4ZVRUlN58800tX75cqampGj9+vMxmsyIiIiT99ajk4MGDrfWPPvqoPD09NWzYMB06dEhff/21Jk6cqOHDh1/1xf0AAAAAAADA9djtxf2SFB4erqysLM2aNUtpaWlq1aqVEhMT1bBhQ0lSWlqazGaztd7d3V1JSUkaM2aMgoKC5OnpqYcfflizZ8+21y0AAAAAAACgErBrSCZJkZGRioyMLPJYfHx8oTF/f/9Cj2gCAAAAAAAAN8LuX7cEAAAAAAAA7I2QDAAAAAAAAIZHSAYAAAAAAADDIyQDAAAAAACA4RGSAQAAAAAAwPAIyQAAAAAAAGB4hGQAAAAAAAAwPEIyAAAAAAAAGB4hGQAAAAAAAAyPkAwAAAAAAACGR0gGAAAAAAAAwyMkAwAAAAAAgOERkgEAAAAAAMDwCMkAAAAAAABgeIRkAAAAAAAAMDxHezcAAAD+GcxmszIzM+3dRrnz8vKSr6+vvdsAAADAPwwhGQAAkNlsVvPm/srNPW/vVsqdi4urfvzxB4IyAAAA2CAkAwAAyszMVG7ued3RrLfcq3nau51yczYnS3sPf6rMzExCMgAAANggJAMAAFbu1Tzl4e5t7zYAAACAm44X9wMAAAAAAMDwCMkAAAAAAABgeIRkAAAAAAAAMDxCMgAAAAAAABgeIRkAAAAAAAAMj5AMAAAAAAAAhkdIBgAAAAAAAMMjJAMAAAAAAIDhEZIBAAAAAADA8AjJAAAAAAAAYHiEZAAAAAAAADA8QjIAAAAAAAAYHiEZAAAAAAAADI+QDAAAAAAAAIZHSAYAAAAAAADDIyQDAAAAAACA4RGSAQAAAAAAwPAIyQAAAAAAAGB4hGQAAAAAAAAwPEIyAAAAAAAAA4uNjZWfn59cXFwUGBiorVu3XrN+y5YtCgwMlIuLixo3bqzFixfbHF+6dKlCQkJ0yy236JZbblH37t313Xff2dTMmDFDJpPJZvPx8SnzeysJQjIAAAAAAACDSkhI0Lhx4xQdHa2UlBSFhIQoLCxMZrO5yPojR46oV69eCgkJUUpKip5++mmNHTtWa9eutdZs3rxZAwYM0KZNm5ScnCxfX1+FhobqxIkTNnO1bNlSaWlp1u3AgQPleq/X42jXqwMAAAAAAKBMZWdn68yZM9Z9Z2dnOTs7F1m7YMECjRgxQiNHjpQkxcTEaMOGDYqLi9OcOXMK1S9evFi+vr6KiYmRJLVo0UK7du3S/Pnz1a9fP0nSO++8Y3PO0qVL9cEHH+jLL7/U4MGDreOOjo52Xz32d6wkAwAAAAAAqEQCAgLk4eFh3YoKuyQpPz9fu3fvVmhoqM14aGioduzYUeQ5ycnJhep79OihXbt26cKFC0Wek5OTowsXLqhWrVo244cPH1bdunXl5+enRx55RL/++mtxb7FcsJIMAAAAAACgEjl06JDq1atn3b/aKrLMzExdunRJ3t7eNuPe3t5KT08v8pz09PQi6y9evKjMzEzVqVOn0DlTpkxRvXr11L17d+tYhw4dtHLlSt122206efKkZs+erU6dOungwYPy9PQs9r2WJUIyAAAAAACASqR69eqqUaNGsetNJpPNvsViKTR2vfqixiVp7ty5Wr16tTZv3iwXFxfreFhYmPXPrVu3VnBwsJo0aaIVK1YoKiqq2L2XJUIyAAAAAAAAA/Ly8pKDg0OhVWMZGRmFVotd5uPjU2S9o6NjoRVg8+fP1wsvvKAvvvhCbdq0uWYvbm5uat26tQ4fPlyKOykbvJMMAAAAAADAgJycnBQYGKikpCSb8aSkJHXq1KnIc4KDgwvVb9y4UUFBQapatap1bN68eXruuef0+eefKygo6Lq95OXlKTU1tcjHNW8WQjIAAAAAAACDioqK0ptvvqnly5crNTVV48ePl9lsVkREhCRp6tSpNl+kjIiI0LFjxxQVFaXU1FQtX75cy5Yt04QJE6w1c+fO1TPPPKPly5erUaNGSk9PV3p6us6ePWutmTBhgrZs2aIjR47o22+/Vf/+/XXmzBkNGTLk5t38FXjcEgAAAAAAwKDCw8OVlZWlWbNmKS0tTa1atVJiYqIaNmwoSUpLS5PZbLbW+/n5KTExUePHj9frr7+uunXratGiRerXr5+1JjY2Vvn5+erfv7/NtaZPn64ZM2ZIko4fP64BAwYoMzNTtWvXVseOHfXNN99Yr2sPhGQAAAAAAAAGFhkZqcjIyCKPxcfHFxrr0qWL9uzZc9X5jh49et1rrlmzprjt3TQ8bgkAAAAAAADDIyQDAAAAAACA4RGSAQAAAAAAwPAIyQAAAAAAAGB4hGQAAAAAAAAwPEIyAAAAAAAAGB4hGQAAAAAAAAyPkAwAAAAAAACGR0gGAAAAAAAAwyMkAwAAAAAAgOERkgEAAAAAAMDwCMkAAAAAAABgeIRkAAAAAAAAMDxHezcAAABws6Wmptq7hXLn5eUlX19fe7cBAABQYRCSAQAAw8jNPyvJpIEDB9q7lXLn4uqqH3/4gaAMAACgmAjJAACAYVy8mCfJohad+8nNo7a92yk3507/rtTta5WZmUlIBgAAUEyEZAAAwHDcPGqrumdde7dR7nisFAAAoPgIyQAAACqZvPPZkonHSgEAAEqCkAwAAKCSuZifK1ks8uvdTy6eXvZup9zkZmXqyKc8VgoAAMqG3UOy2NhYzZs3T2lpaWrZsqViYmIUEhJSZO3mzZvVrVu3QuOpqany9/cv71YBAAAqFBdPL7n5VP7HSgEAAMpCFXtePCEhQePGjVN0dLRSUlIUEhKisLAwmc3ma573448/Ki0tzbo1a9bsJnUMAAAAAACAysiuIdmCBQs0YsQIjRw5Ui1atFBMTIwaNGiguLi4a5536623ysfHx7o5ODjcpI4BAAAAAABQGdktJMvPz9fu3bsVGhpqMx4aGqodO3Zc89y2bduqTp06uueee7Rp06Zr1ubl5enMmTPWLTs7+4Z7BwAAAAAAQOVit5AsMzNTly5dkre3t824t7e30tPTizynTp06WrJkidauXasPP/xQzZs31z333KOvv/76qteZM2eOPDw8rFtAQECZ3gcAAAAAAAAqPru/uN9kMtnsWyyWQmOXNW/eXM2bN7fuBwcH67ffftP8+fN11113FXnO1KlTFRUVZd0/ceIEQRkAAAAAAABs2G0lmZeXlxwcHAqtGsvIyCi0uuxaOnbsqMOHD1/1uLOzs2rUqGHdqlevXuqeAQAAAAAAUDnZLSRzcnJSYGCgkpKSbMaTkpLUqVOnYs+TkpKiOnXqlHV7AAAAAAAAMBC7Pm4ZFRWlQYMGKSgoSMHBwVqyZInMZrMiIiIk/fWo5IkTJ7Ry5UpJUkxMjBo1aqSWLVsqPz9fb7/9ttauXau1a9fa8zYAAAAAAABQwdk1JAsPD1dWVpZmzZqltLQ0tWrVSomJiWrYsKEkKS0tTWaz2Vqfn5+vCRMm6MSJE3J1dVXLli316aefqlevXva6BQAAAAAAAFQCdn9xf2RkpCIjI4s8Fh8fb7M/adIkTZo06SZ0BQAAAAAAACOx2zvJAAAAAAAAgH8KQjIAAAAAAAAYHiEZAAAAAAAADI+QDAAAAAAAAIZHSAYAAAAAAADDIyQDAAAAAACA4RGSAQAAAAAAwPAIyQAAAAAAAGB4hGQAAAAAAAAwPEIyAAAAAAAAGB4hGQAAAAAAAAyPkAwAAAAAAACGR0gGAAAAAAAAwyMkAwAAAAAAgOERkgEAAAAAAMDwCMkAAAAAAABgeIRkAAAAAAAAMDxCMgAAAAAAABgeIRkAAAAAAAAMj5AMAAAAAAAAhkdIBgAAAAAAAMMjJAMAAAAAAIDhEZIBAAAAAADA8AjJAAAAAAAAYHiEZAAAAAAAADA8QjIAAAAAAAAYHiEZAAAAAAAADI+QDAAAAAAAABXK+fPnlZOTY90/duyYYmJitHHjxlLPSUgGAAAAAACACuWBBx7QypUrJUl//vmnOnTooJdfflkPPPCA4uLiSjUnIRkAAAAAAAAqlD179igkJESS9MEHH8jb21vHjh3TypUrtWjRolLNSUgGAAAAAACACiUnJ0fVq1eXJG3cuFF9+/ZVlSpV1LFjRx07dqxUcxKSAQAAAAAAoEJp2rSp1q1bp99++00bNmxQaGioJCkjI0M1atQo1ZyEZAAAAAAAAKhQnn32WU2YMEGNGjVShw4dFBwcLOmvVWVt27Yt1ZyOZdkgAAAAAAAAUN769++vO++8U2lpabr99tut4/fcc48efPDBUs1JSAYAAAAAAIAKx8fHRz4+PpKkM2fO6KuvvlLz5s3l7+9fqvl43BIAAAAAAAAVysMPP6zXXntNknT+/HkFBQXp4YcfVps2bbR27dpSzUlIBgAAAAAAgArl66+/VkhIiCTpo48+ksVi0Z9//qlFixZp9uzZpZqTkAwAAAAAAAAVyunTp1WrVi1J0ueff65+/fqpWrVq6t27tw4fPlyqOQnJAAAAAAAAUKE0aNBAycnJOnfunD7//HOFhoZKkv744w+5uLiUak5e3A8AAAAAAIAKZdy4cfr3v/8td3d3NWzYUF27dpX012OYrVu3LtWchGQAAAAAAACoUCIjI9W+fXv99ttvuvfee1Wlyl8PSzZu3LjU7yQjJAMAAAAAAECFExQUpKCgIFksFlksFplMJvXu3bvU8/FOMgAAAAAAAFQ4K1euVOvWreXq6ipXV1e1adNGq1atKvV8hGQAAAAAAAAGFhsbKz8/P7m4uCgwMFBbt269Zv2WLVsUGBgoFxcXNW7cWIsXL7Y5vnTpUoWEhOiWW27RLbfcou7du+u777674ev+3YIFC/T444+rV69eeu+995SQkKCePXsqIiJCr7zySrHn+TtCMgAAAAAAAINKSEjQuHHjFB0drZSUFIWEhCgsLExms7nI+iNHjqhXr14KCQlRSkqKnn76aY0dO1Zr16611mzevFkDBgzQpk2blJycLF9fX4WGhurEiROlvu6VXn31VcXFxemll15Snz599MADD2ju3LmKjY3VokWLSvWzICQDAAAAAAAwqAULFmjEiBEaOXKkWrRooZiYGDVo0EBxcXFF1i9evFi+vr6KiYlRixYtNHLkSA0fPlzz58+31rzzzjuKjIzUHXfcIX9/fy1dulQFBQX68ssvS33dK6WlpalTp06Fxjt16qS0tLQS/hT+QkgGAAAAAABQiWRnZ+vMmTPWLS8vr8i6/Px87d69W6GhoTbjoaGh2rFjR5HnJCcnF6rv0aOHdu3apQsXLhR5Tk5Oji5cuKBatWqV+rpXatq0qd57771C4wkJCWrWrFmx5rgSX7cEAAAAAACoRAICAmz2p0+frhkzZhSqy8zM1KVLl+Tt7W0z7u3trfT09CLnTk9PL7L+4sWLyszMVJ06dQqdM2XKFNWrV0/du3cv9XWvNHPmTIWHh+vrr79W586dZTKZtG3bNn355ZdFhmfFQUgGAAAAAABQiRw6dEj16tWz7js7O1+z3mQy2exbLJZCY9erL2pckubOnavVq1dr8+bNcnFxuaHr/l2/fv307bff6pVXXtG6detksVgUEBCg7777Tm3bti3WHFciJAMAAAAAAKhEqlevrho1aly3zsvLSw4ODoVWb2VkZBRa5XWZj49PkfWOjo7y9PS0GZ8/f75eeOEFffHFF2rTps0NXbcogYGBevvtt23GTp48qVmzZunZZ58t9jyX8U4yAAAAAAAAA3JyclJgYKCSkpJsxpOSkop8Kb4kBQcHF6rfuHGjgoKCVLVqVevYvHnz9Nxzz+nzzz9XUFDQDV+3uNLT0zVz5sxSnctKMgAAAAAAAIOKiorSoEGDFBQUpODgYC1ZskRms1kRERGSpKlTp+rEiRNauXKlJCkiIkKvvfaaoqKiNGrUKCUnJ2vZsmVavXq1dc65c+dq2rRpevfdd9WoUSPrijF3d3e5u7sX67r2QEgGAAAAAABgUOHh4crKytKsWbOUlpamVq1aKTExUQ0bNpQkpaWlyWw2W+v9/PyUmJio8ePH6/XXX1fdunW1aNEi9evXz1oTGxur/Px89e/f3+Zaf/+AwPWuaw+EZAAAAAAAAAYWGRmpyMjIIo/Fx8cXGuvSpYv27Nlz1fmOHj16w9e1B0IyAAAAAAAAVAhRUVHXPP7777+Xem5CMgAAAAAAAFQIKSkp16256667SjU3IRkAAAAAAAAqhE2bNpXb3FXKbWYAAAAAAACggiAkAwAAAAAAgOERkgEAAAAAAMDwbigky8/P148//qiLFy+WVT8AAAAAAADATVeqF/fn5ORozJgxWrFihSTpp59+UuPGjTV27FjVrVtXU6ZMKdMmAQAAgIrObDYrMzPT3m2UOy8vL/n6+tq7DQAASqxUIdnUqVO1b98+bd68WT179rSOd+/eXdOnTyckAwAAAP7GbDareXN/5eaet3cr5c7FxVU//vgDQRkAoFzt37+/yHGTySQXFxf5+vrK2dm5RHOWKiRbt26dEhIS1LFjR5lMJut4QECAfvnll9JMCQAAAFRamZmZys09rzua9ZZ7NU97t1NuzuZkae/hT5WZmUlIBgAoV3fccYdNJnWlqlWrKjw8XG+88YZcXFyKNWepQrLff/9dt956a6Hxc+fOXbNBAAAAwMjcq3nKw93b3m0AAFDhffTRR5o8ebImTpyo9u3by2KxaOfOnXr55Zc1ffp0Xbx4UVOmTNEzzzyj+fPnF2vOUoVk7dq106effqoxY8ZIkjUYW7p0qYKDg0szJQAAAAAAAFAszz//vBYuXKgePXpYx9q0aaP69etr2rRp+u677+Tm5qannnqqfEOyOXPmqGfPnjp06JAuXryohQsX6uDBg0pOTtaWLVtKMyUAAAAAAABQLAcOHFDDhg0LjTds2FAHDhyQ9NcjmWlpacWes0ppGunUqZN27NihnJwcNWnSRBs3bpS3t7eSk5MVGBhYorliY2Pl5+cnFxcXBQYGauvWrcU6b/v27XJ0dNQdd9xRijsAAAAAAABAReXv768XX3xR+fn51rELFy7oxRdflL+/vyTpxIkT8vYu/msOSryS7MKFC/rPf/6jadOmacWKFSU93UZCQoLGjRun2NhYde7cWW+88YbCwsJ06NCha77o8/Tp0xo8eLDuuecenTx58oZ6AAAAAAAAQMXy+uuvq0+fPqpfv77atGkjk8mk/fv369KlS1q/fr0k6ddff1VkZGSx5yzxSrKqVavqo48+KulpRVqwYIFGjBihkSNHqkWLFoqJiVGDBg0UFxd3zfMee+wxPfroo7z/DAAAAAAAwIA6deqko0ePatasWWrTpo1atWqlWbNm6ciRI+rYsaMkadCgQZo4cWKx5yzVO8kefPBBrVu3TlFRUaU5XZKUn5+v3bt3a8qUKTbjoaGh2rFjx1XPe+utt/TLL7/o7bff1uzZs697nby8POXl5Vn3s7OzS90zAAAAAAAA/hnc3d0VERFRZvOVKiRr2rSpnnvuOe3YsUOBgYFyc3OzOT527NjrzpGZmalLly4VejbU29tb6enpRZ5z+PBhTZkyRVu3bpWjY/FanzNnjmbOnFmsWgAAAAAAAFQMP/30kzZv3qyMjAwVFBTYHHv22WdLPF+pQrI333xTNWvW1O7du7V7926bYyaTqVgh2d/r/85isRQak6RLly7p0Ucf1cyZM3XbbbcVe/6pU6farHg7ceKEAgICin0+AAAAAAAA/lmWLl2qxx9/XF5eXvLx8bHJkkwm080LyY4cOVKa02x4eXnJwcGh0KqxjIyMIr88kJ2drV27diklJUWjR4+WJBUUFMhiscjR0VEbN27U3XffXeg8Z2dnOTs7W/fPnDlzw70DAAAAAADAfmbPnq3nn39ekydPLrM5S/zi/itZLBZZLJYSn+fk5KTAwEAlJSXZjCclJalTp06F6mvUqKEDBw5o79691i0iIkLNmzfX3r171aFDh1LfAwAAAAAAACqOP/74Qw899FCZzlnqkGzlypVq3bq1XF1d5erqqjZt2mjVqlUlmiMqKkpvvvmmli9frtTUVI0fP15ms9n60rWpU6dq8ODBfzVapYpatWpls916661ycXFRq1atCr0XDQAAAAAAAJXTQw89pI0bN5bpnKV63HLBggWaNm2aRo8erc6dO8tisWj79u2KiIhQZmamxo8fX6x5wsPDlZWVpVmzZiktLU2tWrVSYmKiGjZsKElKS0uT2WwuTYsAAAAAAACopJo2bapp06bpm2++UevWrVW1alWb4yV5X/5lpQrJXn31VcXFxVlXeUnSAw88oJYtW2rGjBnFDskkKTIyUpGRkUUei4+Pv+a5M2bM0IwZM4p9LQAAAAAAAFR8S5Yskbu7u7Zs2aItW7bYHCvpRyUvK1VIlpaWVuR7wzp16qS0tLTSTAkAAAAAAAAUS1l8VPJKpXonWdOmTfXee+8VGk9ISFCzZs1uuCkAAAAAAADgZirVSrKZM2cqPDxcX3/9tTp37iyTyaRt27bpyy+/LDI8AwAAAAAAAG5EVFSUnnvuObm5uSkqKuqatQsWLCjx/KUKyfr166dvv/1Wr7zyitatWyeLxaKAgAB99913atu2bWmmBAAAAAAAAK4qJSVFFy5csP75akwmU6nmL1VIJkmBgYF6++23S3s6AAAAAAAAUGybNm0q8s9lpVTvJEtMTNSGDRsKjW/YsEGfffbZDTcFAAAAAAAA3EylWkk2ZcoUvfjii4XGLRaLpkyZorCwsBtuDAAAAAAAAChKbm6uXn31VW3atEkZGRkqKCiwOb5nz54Sz1mqkOzw4cMKCAgoNO7v76+ff/65NFMCAAAAAAAAxTJ8+HAlJSWpf//+at++fanfQ/Z3pQrJPDw89Ouvv6pRo0Y24z///LPc3NxuuCkAAAAAAADgaj799FMlJiaqc+fOZTZnqd5J1qdPH40bN06//PKLdeznn3/WU089pT59+pRZcwAAAAAAAMCV6tWrp+rVq5fpnKUKyebNmyc3Nzf5+/vLz89Pfn5+8vf3l6enp+bPn1+mDQIAAAAAAAB/9/LLL2vy5Mk6duxYmc1Z6sctd+zYoaSkJO3bt0+urq66/fbbFRISUmaNAQAAAAAAAEUJCgpSbm6uGjdurGrVqqlq1ao2x0+dOlXiOUsUkn377bc6deqUwsLCZDKZFBoaqrS0NE2fPl05OTn6v//7P7366qtydnYucSMAAAAAAABAcQwYMEAnTpzQCy+8IG9v75v/4v4ZM2aoa9euCgsLkyQdOHBAo0aN0pAhQ9SiRQvNmzdPdevW1YwZM264MQAAAAAAAKAoO3bsUHJysm6//fYym7NEIdnevXv13HPPWffXrFmj9u3ba+nSpZKkBg0aaPr06YRkAAAAgIGlpqbau4Vy5+XlJV9fX3u3AQCG5e/vr/Pnz5fpnCUKyf744w95e3tb97ds2aKePXta99u1a6fffvut7LoDAAAAUGHk5p+VZNLAgQPt3Uq5c3Z21tq1a1WnTh17t1KuCAMB/FO9+OKLeuqpp/T888+rdevWhd5JVqNGjRLPWaKQzNvbW0eOHFGDBg2Un5+vPXv2aObMmdbj2dnZhZoCAAAAYAwXL+ZJsqhF535y86ht73bKzZ8Zx/Tz7s9133332buVcufi6qoff/iBoAzAP87lRVv33HOPzbjFYpHJZNKlS5dKPGeJQrKePXtqypQpeumll7Ru3TpVq1bN5ouW+/fvV5MmTUrcBAAAAIDKw82jtqp71rV3G+Xm3OnfJYtFfr37ycXTy97tlJvcrEwd+XStMjMzCckA/ONs2rSpzOcsUUg2e/Zs9e3bV126dJG7u7tWrFghJycn6/Hly5crNDS0zJsEAAAAgH8aF08vuflU3jAQAP7JunTpUuZzligkq127trZu3arTp0/L3d1dDg4ONsfff/99ubu7l2mDAAAAAAAAwJVyc3O1f/9+ZWRkqKCgwOZYnz59SjxfiUKyyzw8PIocr1WrVmmmAwAAAAAAAIrt888/1+DBg5WZmVnoWGnfSValLBoDAAAAAAAAbpbRo0froYceUlpamgoKCmy20gRkEiEZAAAAAAAAKpiMjAxFRUXJ29u7zOYkJAMAAAAAAECF0r9/f23evLlM5yzVO8kAAAAAAAAAe3nttdf00EMPaevWrWrdurWqVq1qc3zs2LElnpOQDAAAAAAAABXKu+++qw0bNsjV1VWbN2+WyWSyHjOZTIRkAAAAAAAAqPyeeeYZzZo1S1OmTFGVKmXzNjHeSQYAAAAAAIAKJT8/X+Hh4WUWkEmEZAAAAAAAAKhghgwZooSEhDKdk8ctAQAAAAAAUKFcunRJc+fO1YYNG9SmTZtCL+5fsGBBieckJAMAAAAAAECFcuDAAbVt21aS9P3339sc+/tL/EuCkAwAAAAAAAAVyqZNm8p8Tt5JBgAAAAAAAMNjJRkAAAAAAAAqnJ07d+r999+X2WxWfn6+zbEPP/ywxPOxkgwAAAAAAAAVypo1a9S5c2cdOnRIH330kS5cuKBDhw7pq6++koeHR6nmJCQDAAAAAABAhfLCCy/olVde0fr16+Xk5KSFCxcqNTVVDz/8sHx9fUs1JyEZAAAAAAAAKpRffvlFvXv3liQ5Ozvr3LlzMplMGj9+vJYsWVKqOQnJAAAAAAAAUKHUqlVL2dnZkqR69erp+++/lyT9+eefysnJKdWcvLgfAAAAAAAAFUpISIiSkpLUunVrPfzww3ryySf11VdfKSkpSffcc0+p5iQkAwAAAAAAQIXy2muvKTc3V5I0depUVa1aVdu2bVPfvn01bdq0Us1JSAYAAAAAAIAKpVatWtY/V6lSRZMmTdKkSZNuaE5CMgAAAAAAAFQIZ86cKVZdjRo1Sjw3IRkAAAAAAAAqhJo1a8pkMl31uMVikclk0qVLl0o8NyEZAAAAAAAAKoRNmzZZ/2yxWNSrVy+9+eabqlev3g3PTUgGAAAAAACACqFLly42+w4ODurYsaMaN258w3NXueEZAAAAAAAAgAqOkAwAAAAAAACGR0gGAAAAAACACutaL/IvCd5JBgAAAAAAgAqhb9++Nvu5ubmKiIiQm5ubzfiHH35Y4rlZSQYAAAAAAGBgsbGx8vPzk4uLiwIDA7V169Zr1m/ZskWBgYFycXFR48aNtXjxYpvjBw8eVL9+/dSoUSOZTCbFxMQUmmPGjBkymUw2m4+Pz3V79fDwsNkGDhyounXrFhovDVaSAQAAAAAAGFRCQoLGjRun2NhYde7cWW+88YbCwsJ06NAh+fr6Fqo/cuSIevXqpVGjRuntt9/W9u3bFRkZqdq1a6tfv36SpJycHDVu3FgPPfSQxo8ff9Vrt2zZUl988YV138HB4br9vvXWW6W4y+IhJAMAAAAAADCoBQsWaMSIERo5cqQkKSYmRhs2bFBcXJzmzJlTqH7x4sXy9fW1rg5r0aKFdu3apfnz51tDsnbt2qldu3aSpClTplz12o6OjsVaPXaz8LglAAAAAABAJZKdna0zZ85Yt7y8vCLr8vPztXv3boWGhtqMh4aGaseOHUWek5ycXKi+R48e2rVrly5cuFCiPg8fPqy6devKz89PjzzyiH799dcSnV/WCMkAAAAAAAAqkYCAAJv3cxW1IkySMjMzdenSJXl7e9uMe3t7Kz09vchz0tPTi6y/ePGiMjMzi91jhw4dtHLlSm3YsEFLly5Venq6OnXqpKysrGLPUdZ43BIAAAAAAKASOXTokOrVq2fdd3Z2vma9yWSy2bdYLIXGrldf1Pi1hIWFWf/cunVrBQcHq0mTJlqxYoWioqKKPU9ZIiQDAAAAAACoRKpXr64aNWpct87Ly0sODg6FVo1lZGQUWi12mY+PT5H1jo6O8vT0LHXPbm5uat26tQ4fPlzqOW4Uj1sCAAAAAAAYkJOTkwIDA5WUlGQznpSUpE6dOhV5TnBwcKH6jRs3KigoSFWrVi11L3l5eUpNTVWdOnVKPceNIiQDAAAAAAAwqKioKL355ptavny5UlNTNX78eJnNZkVEREiSpk6dqsGDB1vrIyIidOzYMUVFRSk1NVXLly/XsmXLNGHCBGtNfn6+9u7dq7179yo/P18nTpzQ3r179fPPP1trJkyYoC1btujIkSP69ttv1b9/f505c0ZDhgy5eTd/BR63BAAAAAAAMKjw8HBlZWVp1qxZSktLU6tWrZSYmKiGDRtKktLS0mQ2m631fn5+SkxM1Pjx4/X666+rbt26WrRokfr162et+d///qe2bdta9+fPn6/58+erS5cu2rx5syTp+PHjGjBggDIzM1W7dm117NhR33zzjfW69kBIBgAAAAAAYGCRkZGKjIws8lh8fHyhsS5dumjPnj1Xna9Ro0bWl/lfzZo1a0rU483A45YAAAAAAAAwPEIyAAAAAAAAGB4hGQAAAAAAAAyPkAwAAAAAAACGR0gGAAAAAAAAwyMkAwAAAAAAgOERkgEAAAAAAMDwCMkAAAAAAABgeHYPyWJjY+Xn5ycXFxcFBgZq69atV63dtm2bOnfuLE9PT7m6usrf31+vvPLKTewWAAAAAAAAlZGjPS+ekJCgcePGKTY2Vp07d9Ybb7yhsLAwHTp0SL6+voXq3dzcNHr0aLVp00Zubm7atm2bHnvsMbm5uek///mPHe4AAAAAAAAAlYFdV5ItWLBAI0aM0MiRI9WiRQvFxMSoQYMGiouLK7K+bdu2GjBggFq2bKlGjRpp4MCB6tGjxzVXnwEAAAAAAADXY7eQLD8/X7t371ZoaKjNeGhoqHbs2FGsOVJSUrRjxw516dLlqjV5eXk6c+aMdcvOzr6hvgEAAAAAAFD52C0ky8zM1KVLl+Tt7W0z7u3trfT09GueW79+fTk7OysoKEhPPPGERo4cedXaOXPmyMPDw7oFBASUSf8AAAAAAACoPOz+4n6TyWSzb7FYCo1daevWrdq1a5cWL16smJgYrV69+qq1U6dO1enTp63boUOHyqRvAAAAAAAAVB52e3G/l5eXHBwcCq0ay8jIKLS67Ep+fn6SpNatW+vkyZOaMWOGBgwYUGSts7OznJ2drftnzpy5wc4BAAAAAABQ2dhtJZmTk5MCAwOVlJRkM56UlKROnToVex6LxaK8vLyybg8AAAAAAAAGYreVZJIUFRWlQYMGKSgoSMHBwVqyZInMZrMiIiIk/fWo5IkTJ7Ry5UpJ0uuvvy5fX1/5+/tLkrZt26b58+drzJgxdrsHAAAAAAAAVHx2DcnCw8OVlZWlWbNmKS0tTa1atVJiYqIaNmwoSUpLS5PZbLbWFxQUaOrUqTpy5IgcHR3VpEkTvfjii3rsscfsdQsAAAAAAACoBOwakklSZGSkIiMjizwWHx9vsz9mzBhWjQEAAAAAAKDM2f3rlgAAAAAAAIC9EZIBAAAAAADA8AjJAAAAAAAAYHiEZAAAAAAAADA8QjIAAAAAAAAYHiEZAAAAAAAADI+QDAAAAAAAAIZHSAYAAAAAAADDIyQDAAAAAACA4RGSAQAAAAAAwPAIyQAAAAAAAGB4hGQAAAAAAAAwPEIyAAAAAAAAGB4hGQAAAAAAAAyPkAwAAAAAAACGR0gGAAAAAAAAwyMkAwAAAAAAgOERkgEAAAAAAMDwCMkAAAAAAABgeIRkAAAAAAAAMDxCMgAAAAAAABgeIRkAAAAAAAAMj5AMAAAAAAAAhkdIBgAAAAAAAMMjJAMAAAAAAIDhEZIBAAAAAADA8AjJAAAAAAAAYHiEZAAAAAAAADA8QjIAAAAAAAAYHiEZAAAAAAAADI+QDAAAAAAAAIZHSAYAAAAAAADDIyQDAAAAAACA4RGSAQAAAAAAwPAIyQAAAAAAAGB4hGQAAAAAAAAwPEIyAAAAAAAAGB4hGQAAAAAAAAyPkAwAAAAAAACGR0gGAAAAAAAAwyMkAwAAAAAAgOERkgEAAAAAAMDwCMkAAAAAAABgeIRkAAAAAAAAMDxCMgAAAAAAABgeIRkAAAAAAAAMj5AMAAAAAAAAhkdIBgAAAAAAAMMjJAMAAAAAAIDhEZIBAAAAAADA8AjJAAAAAAAAYHiEZAAAAAAAADA8QjIAAAAAAAAYHiEZAAAAAAAADI+QDAAAAAAAAIZHSAYAAAAAAGBgsbGx8vPzk4uLiwIDA7V169Zr1m/ZskWBgYFycXFR48aNtXjxYpvjBw8eVL9+/dSoUSOZTCbFxMSUyXXLGyEZAAAAAACAQSUkJGjcuHGKjo5WSkqKQkJCFBYWJrPZXGT9kSNH1KtXL4WEhCglJUVPP/20xo4dq7Vr11prcnJy1LhxY7344ovy8fEpk+veDIRkAAAAAAAABrVgwQKNGDFCI0eOVIsWLRQTE6MGDRooLi6uyPrFixfL19dXMTExatGihUaOHKnhw4dr/vz51pp27dpp3rx5euSRR+Ts7Fwm170ZCMkAAAAAAAAqkezsbJ05c8a65eXlFVmXn5+v3bt3KzQ01GY8NDRUO3bsKPKc5OTkQvU9evTQrl27dOHChWL1V5rr3gyEZAAAAAAAAJVIQECAPDw8rNucOXOKrMvMzNSlS5fk7e1tM+7t7a309PQiz0lPTy+y/uLFi8rMzCxWf6W57s3gaLcrAwAAAAAAoMwdOnRI9erVs+5f7ZHHy0wmk82+xWIpNHa9+qLGr6ek1y1vhGQAAAAAAACVSPXq1VWjRo3r1nl5ecnBwaHQ6q2MjIxCq7wu8/HxKbLe0dFRnp6exeqvNNe9GXjcEgAAAAAAwICcnJwUGBiopKQkm/GkpCR16tSpyHOCg4ML1W/cuFFBQUGqWrVquV33ZmAlGQAAAAAAgEFFRUVp0KBBCgoKUnBwsJYsWSKz2ayIiAhJ0tSpU3XixAmtXLlSkhQREaHXXntNUVFRGjVqlJKTk7Vs2TKtXr3aOmd+fr4OHTpk/fOJEye0d+9eubu7q2nTpsW6rj0QkgEAAAAAABhUeHi4srKyNGvWLKWlpalVq1ZKTExUw4YNJUlpaWkym83Wej8/PyUmJmr8+PF6/fXXVbduXS1atEj9+vWz1vzvf/9T27Ztrfvz58/X/Pnz1aVLF23evLlY17UHQjIAAAAAAAADi4yMVGRkZJHH4uPjC4116dJFe/bsuep8jRo1sr7Mv7TXtQfeSQYAAAAAAADDs3tIFhsbKz8/P7m4uCgwMFBbt269au2HH36oe++9V7Vr11aNGjUUHBysDRs23MRuAQAAAAAAUBnZNSRLSEjQuHHjFB0drZSUFIWEhCgsLMzmWde/+/rrr3XvvfcqMTFRu3fvVrdu3XT//fcrJSXlJncOAAAAAACAysSuIdmCBQs0YsQIjRw5Ui1atFBMTIwaNGiguLi4IutjYmI0adIktWvXTs2aNdMLL7ygZs2a6ZNPPrnJnQMAAAAAAKAysVtIlp+fr927dys0NNRmPDQ0VDt27CjWHAUFBcrOzlatWrWuWpOXl6czZ85Yt+zs7BvqGwAAAAAAAJWP3UKyzMxMXbp0Sd7e3jbj3t7eSk9PL9YcL7/8ss6dO6eHH374qjVz5syRh4eHdQsICLihvgEAAAAAAFD52P3F/SaTyWbfYrEUGivK6tWrNWPGDCUkJOjWW2+9at3UqVN1+vRp63bo0KEb7hkAAAAAAACVi6O9Luzl5SUHB4dCq8YyMjIKrS67UkJCgkaMGKH3339f3bt3v2ats7OznJ2drftnzpwpfdMAAAAAAAColOy2kszJyUmBgYFKSkqyGU9KSlKnTp2uet7q1as1dOhQvfvuu+rdu3d5twkAAAAAAAADsNtKMkmKiorSoEGDFBQUpODgYC1ZskRms1kRERGS/npU8sSJE1q5cqWkvwKywYMHa+HCherYsaN1FZqrq6s8PDzsdh8AAAAAAACo2OwakoWHhysrK0uzZs1SWlqaWrVqpcTERDVs2FCSlJaWJrPZbK1/4403dPHiRT3xxBN64oknrONDhgxRfHz8zW4fAAAAAAAAlYRdQzJJioyMVGRkZJHHrgy+Nm/eXP4NAQAAAAAAwHDs/nVLAAAAAAAAwN4IyQAAAAAAAGB4hGQAAAAAAAAwPEIyAAAAAAAAGB4hGQAAAAAAAAyPkAwAAAAAAACGR0gGAAAAAAAAwyMkAwAAAAAAgOERkgEAAAAAAMDwCMkAAAAAAABgeIRkAAAAAAAAMDxCMgAAAAAAABgeIRkAAAAAAAAMj5AMAAAAAAAAhkdIBgAAAAAAAMMjJAMAAAAAAIDhEZIBAAAAAADA8AjJAAAAAAAAYHiEZAAAAAAAADA8QjIAAAAAAAAYHiEZAAAAAAAADI+QDAAAAAAAAIZHSAYAAAAAAADDIyQDAAAAAACA4RGSAQAAAAAAwPAIyQAAAAAAAGB4hGQAAAAAAAAwPEIyAAAAAAAAGB4hGQAAAAAAAAyPkAwAAAAAAACGR0gGAAAAAAAAwyMkAwAAAAAAgOERkgEAAAAAAMDwCMkAAAAAAABgeIRkAAAAAAAAMDxCMgAAAAAAABgeIRkAAAAAAAAMj5AMAAAAAAAAhkdIBgAAAAAAAMMjJAMAAAAAAIDhEZIBAAAAAADA8AjJAAAAAAAAYHiEZAAAAAAAADA8QjIAAAAAAAAYHiEZAAAAAAAADI+QDAAAAAAAAIZHSAYAAAAAAADDIyQDAAAAAACA4RGSAQAAAAAAwPAIyQAAAAAAAGB4hGQAAAAAAAAwPEIyAAAAAAAAGB4hGQAAAAAAAAyPkAwAAAAAAACGR0gGAAAAAAAAwyMkAwAAAAAAgOERkgEAAAAAAMDwCMkAAAAAAABgeIRkAAAAAAAAMDxCMgAAAAAAABgeIRkAAAAAAAAMj5AMAAAAAADAwGJjY+Xn5ycXFxcFBgZq69at16zfsmWLAgMD5eLiosaNG2vx4sWFatauXauAgAA5OzsrICBAH330kc3xGTNmyGQy2Ww+Pj5lel8lRUgGAAAAAABgUAkJCRo3bpyio6OVkpKikJAQhYWFyWw2F1l/5MgR9erVSyEhIUpJSdHTTz+tsWPHau3atdaa5ORkhYeHa9CgQdq3b58GDRqkhx9+WN9++63NXC1btlRaWpp1O3DgQLne6/UQkgEAAAAAABjUggULNGLECI0cOVItWrRQTEyMGjRooLi4uCLrFy9eLF9fX8XExKhFixYaOXKkhg8frvnz51trYmJidO+992rq1Kny9/fX1KlTdc899ygmJsZmLkdHR/n4+Fi32rVrl+etXhchGQAAAAAAQCWSnZ2tM2fOWLe8vLwi6/Lz87V7926FhobajIeGhmrHjh1FnpOcnFyovkePHtq1a5cuXLhwzZor5zx8+LDq1q0rPz8/PfLII/r1119LdJ9ljZAMAAAAAACgEgkICJCHh4d1mzNnTpF1mZmZunTpkry9vW3Gvb29lZ6eXuQ56enpRdZfvHhRmZmZ16z5+5wdOnTQypUrtWHDBi1dulTp6enq1KmTsrKySny/ZcXRblcGAAAAAABAmTt06JDq1atn3Xd2dr5mvclkstm3WCyFxq5Xf+X49eYMCwuz/rl169YKDg5WkyZNtGLFCkVFRV2z3/Ji95VkJfmCQlpamh599FE1b95cVapU0bhx425eowAAAAAAABVA9erVVaNGDet2tZDMy8tLDg4OhVaNZWRkFFoJdpmPj0+R9Y6OjvL09LxmzdXmlCQ3Nze1bt1ahw8fvu79lRe7hmQl/YJCXl6eateurejoaN1+++03uVsAAAAAAIDKw8nJSYGBgUpKSrIZT0pKUqdOnYo8Jzg4uFD9xo0bFRQUpKpVq16z5mpzSn9lPqmpqapTp05pbqVM2DUkK+kXFBo1aqSFCxdq8ODB8vDwuMndAgAAAAAAVC5RUVF68803tXz5cqWmpmr8+PEym82KiIiQJE2dOlWDBw+21kdEROjYsWOKiopSamqqli9frmXLlmnChAnWmieffFIbN27USy+9pB9++EEvvfSSvvjiC5snAidMmKAtW7boyJEj+vbbb9W/f3+dOXNGQ4YMuWn3fiW7vZPs8hcUpkyZYjN+rS8olEZeXp7NVxyys7PLbG4AAAAAAICKLDw8XFlZWZo1a5bS0tLUqlUrJSYmqmHDhpL+evXV35/48/PzU2JiosaPH6/XX39ddevW1aJFi9SvXz9rTadOnbRmzRo988wzmjZtmpo0aaKEhAR16NDBWnP8+HENGDBAmZmZql27tjp27KhvvvnGel17sFtIVpovKJTGnDlzNHPmzDKbDwAAAAAAoDKJjIxUZGRkkcfi4+MLjXXp0kV79uy55pz9+/dX//79r3p8zZo1JerxZrD7i/tL+gWFkpo6dapOnz5t3Q4dOlRmcwMAAAAAAKBysNtKstJ8QaE0nJ2dbb7icObMmTKbGwAAAAAAAJWD3VaSleYLCgAAAAAAAEB5sNtKMumvLygMGjRIQUFBCg4O1pIlSwp9QeHEiRNauXKl9Zy9e/dKks6ePavff/9de/fulZOTkwICAuxxCwAAAAAAAKgE7BqSlfQLCpLUtm1b6593796td999Vw0bNtTRo0dvZusAAAAAAACoROwakkkl/4KCxWIp544AAAAAAABgNHb/uiUAAAAAAABgb4RkAAAAAAAAMDxCMgAAAAAAABgeIRkAAAAAAAAMj5AMAAAAAAAAhkdIBgAAAAAAAMMjJAMAAAAAAIDhEZIBAAAAAP6/9u48ropy8eP494iyyOICAmosoqmguFJe9AqYKamZlrlV3MjUzHLJLf25YG6p14VcM4q00qjEyEwLl9RSwiWwRbJcuGiCmpqpNzVhfn/4Yq5HcEvxqOfzfr3m9XJmnpnnmaOPM+d7npkBALtHSAYAAAAAAAC7R0gGAAAAAAAAu0dIBgAAAAAAALtHSAYAAAAAAAC7R0gGAAAAAAAAu0dIBgAAAAAAALtHSAYAAAAAAAC7R0gGAAAAAAAAu0dIBgAAAAAAALtHSAYAAAAAAAC7R0gGAAAAAAAAu0dIBgAAAAAAALtHSAYAAAAAAAC7R0gGAAAAAAAAu0dIBgAAAAAAALtHSAYAAAAAAAC7R0gGAAAAAAAAu0dIBgAAAAAAALtHSAYAAAAAAAC7R0gGAAAAAAAAu0dIBgAAAAAAALtHSAYAAAAAAAC7R0gGAAAAAAAAu0dIBgAAAAAAALtHSAYAAAAAAAC7R0gGAAAAAAAAu0dIBgAAAAAAALtHSAYAAAAAAAC7R0gGAAAAAAAAu0dIBgAAAAAAALtHSAYAAAAAAAC7R0gGAAAAAAAAu0dIBgAAAAAAALtHSAYAAAAAAAC7R0gGAAAAAAAAu0dIBgAAAAAAALtHSAYAAAAAAAC7R0gGAAAAAAAAu0dIBgAAAAAAALtHSAYAAAAAAAC7R0gGAAAAAAAAu0dIBgAAAAAAALtHSAYAAAAAAAC7R0gGAAAAAAAAu0dIBgAAAAAAALtHSAYAAAAAAAC7R0gGAAAAAAAAu0dIBgAAAAAAALtHSAYAAAAAAAC7R0gGAAAAAAAAu0dIBgAAAAAAALtHSAYAAAAAAAC7R0gGAAAAAAAAu0dIBgAAAAAAALtHSAYAAAAAAAC7R0gGAAAAAAAAu0dIBgAAAAAAALtHSAYAAAAAAAC7R0gGAAAAAAAAu0dIBgAAAAAAALtn85Bs3rx5qlatmpydndW4cWN99dVXVyy/YcMGNW7cWM7OzgoKCtLrr79+i1oKAAAAAABw9ymJbCY5OVkhISFycnJSSEiIPv744xuut6TZNCT74IMPNHDgQI0cOVIZGRlq3ry52rRpo5ycnGLL79u3T23btlXz5s2VkZGh//u//1P//v2VnJx8i1sOAAAAAABw5yuJbCYtLU1du3ZVTEyMduzYoZiYGHXp0kXp6el/u95bwaYh2YwZM/Tss8+qZ8+eCg4OVnx8vPz8/DR//vxiy7/++uvy9/dXfHy8goOD1bNnT/Xo0UPTpk27xS0HAAAAAAC485VENhMfH69WrVppxIgRql27tkaMGKGWLVsqPj7+b9d7K5S2VcXnzp3T9u3bNXz4cKvlrVu31ubNm4vdJi0tTa1bt7ZaFh0drbfeekt//fWXypQpU2Sbs2fP6uzZs+b8iRMnJEm5ubk3eghWDh06JEk6duKAzpw7fVP3fTs5cerCcR4/9B+d+/OUjVtTck4eu/Dv49SBHJ07ffce57njxyRd+Pd74MABG7fmyuhjdxf62O2HPnZ3oY/dfuhjdxf62O2nsI+dOJmn8/nnbNyaknPyv0clSX8cPaj8v+7e4zx94ogk6b95uco/d/ce59ljF/4+b2YfK8w6Tpw4IQ8PD3O5k5OTnJycipQvqWwmLS1NL730UpEyhSHZ36n3ljBs5NdffzUkGZs2bbJaPnHiRKNmzZrFbnPvvfcaEydOtFq2adMmQ5Jx8ODBYreJi4szJDExMTExMTExMTExMTExMTHZ5RQXF3dLs5kyZcoYixcvtiqzePFiw9HR8W/XeyvYbCRZIYvFYjVvGEaRZVcrX9zyQiNGjNCgQYPM+fPnzysrK0t+fn4qVcrm7y3ANTh58qRCQkK0c+dOubu727o5wF2HPgaULPoYULLoY0DJoo/dWQoKCpSTk6OQkBCVLv2/yKe4UWQXK4ls5lr2eb31ljSbhWReXl5ycHBQXl6e1fLDhw/Lx8en2G18fX2LLV+6dGl5enoWu01xQwqbNWt2Ay3HrfbHH39IkqpWrWo1XBTAzUEfA0oWfQwoWfQxoGTRx+48/v7+11y2pLKZy5Up3OffqfdWsNlQKkdHRzVu3FirV6+2Wr569Wo1bdq02G3Cw8OLlE9NTVVYWFixzyMDAAAAAABA8Uoqm7lcmcJ9/p16bwWb3m45aNAgxcTEKCwsTOHh4XrjjTeUk5OjPn36SLpwq+Svv/6qd955R5LUp08fzZkzR4MGDVKvXr2Ulpamt956S++//74tDwMAAAAAAOCOVBLZzIABAxQREaEpU6aoQ4cO+uSTT7RmzRp9/fXX11yvLdg0JOvatauOHj2qcePGKTc3V3Xr1tXKlSsVEBAg6cJbGXJycszy1apV08qVK/XSSy9p7ty5qlKlimbNmqVOnTrZ6hBwCzg5OSkuLu6q91AD+HvoY0DJoo8BJYs+BpQs+tjdrySymaZNmyopKUmjRo3S6NGjVb16dX3wwQdq0qTJNddrCxaj8OlqAAAAAAAAgJ3i9Y4AAAAAAACwe4RkAAAAAAAAsHuEZAAAAAAAALB7hGQAYOcCAwMVHx9/08sCuDGX9jeLxaKUlBSbtQf2bf369bJYLPr9999t3RQAt0BUVJQGDhxo62YAtxwhGW4bsbGxslgsRabdu3dr48aNat++vapUqcKXBNzVLu4HZcqUUVBQkIYMGaLTp0+XWJ1bt25V7969b3pZ4E52cV8sXbq0/P399fzzz+v48eO2bhpw3Q4fPqznnntO/v7+cnJykq+vr6Kjo5WWlmbrplm53i/lS5YskYODg/r06VNyjQJuUGxsrDp27Fhk+e0ePC9btkzjx48v8XoI43C7ISTDbeWhhx5Sbm6u1VStWjWdPn1a9evX15w5c2zdRKDEFfaDvXv3asKECZo3b56GDBlSpNxff/11U+qrVKmSypYte9PLAne6wr6YnZ2tN998U59++qn69u1r62YB161Tp07asWOHFi1apJ9//lnLly9XVFSUjh07Zuum3ZDExEQNGzZMSUlJ+u9//2vTtpw7d86m9QM3W8WKFeXu7m7rZgC3HCEZbiuFv25ePDk4OKhNmzaaMGGCHnvsMVs3EShxhf3Az89PTzzxhJ588kmlpKRo7NixatCggRITExUUFCQnJycZhqETJ06od+/e8vb2loeHhx544AHt2LHDap/Lly9XWFiYnJ2d5eXlZdWXLr2la+zYseZogypVqqh///6XLZuTk6MOHTrIzc1NHh4e6tKliw4dOmS1rwYNGujdd99VYGCgypUrp27duunkyZM3/4MDbrLCvnjPPfeodevW6tq1q1JTU831b7/9toKDg+Xs7KzatWtr3rx5VtsfOHBA3bp1U8WKFeXq6qqwsDClp6dLkvbs2aMOHTrIx8dHbm5uuu+++7RmzZpbenywD7///ru+/vprTZkyRS1atFBAQIDuv/9+jRgxQu3atZMkZWdny2KxKDMz02o7i8Wi9evXW+1v06ZNql+/vpydndWkSRN9//335rr//Oc/at++vSpUqCBXV1fVqVNHK1euNNfv3LlTbdu2lZubm3x8fBQTE6PffvtN0oXRNhs2bNBrr71mjuLMzs6+7HFlZ2dr8+bNGj58uGrXrq2lS5cWKZOYmKg6derIyclJlStX1osvvmh1fL1795aPj4+cnZ1Vt25drVixQtL/zl0Xi4+PV2BgoDlfODro1VdfVZUqVVSzZk1J0nvvvaewsDC5u7vL19dXTzzxhA4fPmy1rx9//FHt2rWTh4eH3N3d1bx5c+3Zs0cbN25UmTJllJeXZ1V+8ODBioiIuOxngbvD0aNH1b17d91zzz0qW7asQkND9f7771uViYqKUr9+/TRw4EBVqFBBPj4+euONN3T69Gk988wzcnd3V/Xq1bVq1Spzm8IRa1988YUaNmwoFxcXPfDAAzp8+LBWrVql4OBgeXh4qHv37lZh86UjvAIDAzVp0iT16NFD7u7u8vf31xtvvGHVvs2bN6tBgwZydnZWWFiYUlJSivzfcr2Sk5PNfhwYGKjp06dbrZ83b57uvfdeOTs7y8fHR48//ri5bunSpQoNDZWLi4s8PT314IMPlujdGbg7EJIBwG3OxcXFHDW2e/duffjhh0pOTjYvONq1a6e8vDytXLlS27dvV6NGjdSyZUtzhMBnn32mxx57TO3atVNGRobWrl2rsLCwYutaunSpZs6cqQULFuiXX35RSkqKQkNDiy1rGIY6duyoY8eOacOGDVq9erX27Nmjrl27WpXbs2ePUlJStGLFCq1YsUIbNmzQ5MmTb9KnA9wae/fu1eeff64yZcpIkhISEjRy5EhNnDhRWVlZmjRpkkaPHq1FixZJkk6dOqXIyEgdPHhQy5cv144dOzRs2DAVFBSY69u2bas1a9YoIyND0dHRat++vXJycmx2jLg7ubm5yc3NTSkpKTp79uwN72/o0KGaNm2atm7dKm9vbz3yyCPmOeqFF17Q2bNntXHjRn3//feaMmWK3NzcJEm5ubmKjIxUgwYNtG3bNn3++ec6dOiQunTpIkl67bXXFB4erl69epl3E/j5+V22HYmJiWrXrp3KlSunp556Sm+99ZbV+vnz5+uFF15Q79699f3332v58uWqUaOGJKmgoEBt2rTR5s2b9d5772nnzp2aPHmyHBwcruuzWLt2rbKysrR69WozYDt37pzGjx+vHTt2KCUlRfv27VNsbKy5za+//qqIiAg5Oztr3bp12r59u3r06KHz588rIiJCQUFBevfdd83y58+f13vvvadnnnnmutqGO8+ZM2fUuHFjrVixQj/88IN69+6tmJgY88eVQosWLZKXl5e2bNmifv366fnnn1fnzp3VtGlTffvtt4qOjlZMTEyR0ZVjx47VnDlztHnzZu3fv19dunRRfHy8lixZos8++0yrV6/W7Nmzr9jG6dOnKywsTBkZGerbt6+ef/55/fTTT5KkkydPqn379goNDdW3336r8ePH6+WXX76hz2T79u3q0qWLunXrpu+//15jx47V6NGjtXDhQknStm3b1L9/f40bN067du3S559/bgbKubm56t69u3r06KGsrCytX79ejz32mAzDuKE2wQ4YwG3i6aefNhwcHAxXV1dzevzxx4uUk2R8/PHHt76BwC3w9NNPGx06dDDn09PTDU9PT6NLly5GXFycUaZMGePw4cPm+rVr1xoeHh7GmTNnrPZTvXp1Y8GCBYZhGEZ4eLjx5JNPXrbOgIAAY+bMmYZhGMb06dONmjVrGufOnbtq2dTUVMPBwcHIyckx1//444+GJGPLli2GYRhGXFycUbZsWeOPP/4wywwdOtRo0qTJ1T8MwIYuPic5OzsbkgxJxowZMwzDMAw/Pz9jyZIlVtuMHz/eCA8PNwzDMBYsWGC4u7sbR48eveY6Q0JCjNmzZ5vzF/c3w+D8h79v6dKlRoUKFQxnZ2ejadOmxogRI4wdO3aY6/ft22dIMjIyMsxlx48fNyQZX375pWEYhvHll18akoykpCSzzNGjRw0XFxfjgw8+MAzDMEJDQ42xY8cW24bRo0cbrVu3tlq2f/9+Q5Kxa9cuwzAMIzIy0hgwYMBVjyc/P9/w8/MzUlJSDMMwjCNHjhhlypQxfvnlF7NMlSpVjJEjRxa7/RdffGGUKlXKrPdScXFxRv369a2WzZw50wgICDDnn376acPHx8c4e/bsFdu6ZcsWQ5Jx8uRJwzAMY8SIEUa1atUue56dMmWKERwcbM6npKQYbm5uxqlTp65YD25fxX3Hufjccvz48ctu27ZtW2Pw4MHmfGRkpPHPf/7TnD9//rzh6upqxMTEmMtyc3MNSUZaWpphGP/ru2vWrDHLvPrqq4YkY8+ePeay5557zoiOjraq6+L+GBAQYDz11FPmfEFBgeHt7W3Mnz/fMAzDmD9/vuHp6Wn8+eefZpmEhIQi/7dc6kr9/oknnjBatWpltWzo0KFGSEiIYRiGkZycbHh4eFhdZxbavn27IcnIzs6+bN1AcRhJhttKixYtlJmZaU6zZs2ydZOAW27FihVyc3OTs7OzwsPDFRERYf6yFxAQoEqVKpllt2/frlOnTsnT09McLeDm5qZ9+/Zpz549kqTMzEy1bNnymuru3Lmz/vzzTwUFBalXr176+OOPdf78+WLLZmVlyc/Pz+qX/pCQEJUvX15ZWVnmssDAQKtnWlSuXLnIrSfA7ajwnJSenq5+/fopOjpa/fr105EjR7R//349++yzVv1uwoQJVv2uYcOGqlixYrH7Pn36tIYNG2b2GTc3N/3000+MJEOJ6NSpkzmqMTo6WuvXr1ejRo3M0RjXIzw83PxzxYoVVatWLfP//P79+2vChAlq1qyZ4uLi9N1335llt2/fri+//NKqz9SuXVuSzH5zrVJTU3X69Gm1adNGkuTl5aXWrVsrMTFR0oUXFRw8ePCy577MzEzdc8895i2Sf1doaKgcHR2tlmVkZKhDhw4KCAiQu7u7oqKiJMns25mZmWrevLk5KvVSsbGx2r17t7755htJF0bMdenSRa6urjfUVtjWpd9xMjMz9eabb1qVyc/P18SJE1WvXj3zui41NbXIeaFevXrmnx0cHOTp6Wk16t/Hx0eSilxrXbydj4+PypYtq6CgIKtlV7s+u3gfFotFvr6+5ja7du1SvXr15OzsbJa5//77r7i/q8nKylKzZs2sljVr1ky//PKL8vPz1apVKwUEBCgoKEgxMTFavHixOYKufv36atmypUJDQ9W5c2clJCTw8h1ck9K2bgBwMVdXV3MoPGCvWrRoofnz56tMmTKqUqWK1YX0pRfJBQUFqly5cpFnxkhS+fLlJV24XfNa+fn5adeuXVq9erXWrFmjvn376t///rc2bNhQ5ILeMAxZLJYi+7h0+aXbWSwW85Yz4HZ28Tlp1qxZatGihV555RXzuUYJCQlq0qSJ1TaFt2tdrd8NHTpUX3zxhaZNm6YaNWrIxcVFjz/+OA//RolxdnZWq1at1KpVK40ZM0Y9e/ZUXFycYmNjVarUhd/NjYtuQ7qel8MU/p/fs2dPRUdH67PPPlNqaqpeffVVTZ8+Xf369VNBQYHat2+vKVOmFNm+cuXK13UsiYmJOnbsmNWLZAoKCpSRkaHx48dftf9dbX2pUqWK3JJV3Odx6Tn59OnTat26tVq3bq333ntPlSpVUk5OjqKjo82+fbW6vb291b59e7399tsKCgrSypUriz3H485S3HecAwcOWM1Pnz5dM2fOVHx8vEJDQ+Xq6qqBAwcWOS8Ud1118bLC/njptdalZf7O9dmVtinuuvDSfnS9rrZPd3d3ffvtt1q/fr1SU1M1ZswYjR07Vlu3blX58uW1evVqbd68WampqZo9e7ZGjhyp9PR0VatW7YbahbsbI8kA4DZTeCEVEBBw2V+aCzVq1Eh5eXkqXbq0atSoYTV5eXlJuvCr39q1a6+5fhcXFz3yyCOaNWuW1q9fr7S0NKsHMxcKCQlRTk6O9u/fby7buXOnTpw4oeDg4GuuD7hTxMXFadq0acrPz1fVqlW1d+/eIv2u8MK7Xr16yszMvOzbA7/66ivFxsbq0UcfVWhoqHx9fa/4kHLgZgsJCTEfYF04Qjk3N9dcf7kHbReOcJKk48eP6+effzZHhEkXfmzp06ePli1bpsGDByshIUHShfPVjz/+qMDAwCL9pjBscnR0VH5+/hXbffToUX3yySdKSkoqMjLn1KlTWrVqldzd3RUYGHjZc1+9evV04MAB/fzzz8Wur1SpkvLy8qy+jF/Lg8d/+ukn/fbbb5o8ebKaN2+u2rVrFzua56uvvrpiCNmzZ08lJSVpwYIFql69epGRNLg7ffXVV+rQoYOeeuop1a9fX0FBQfrll19s3axrVrt2bX333XdWzz7ctm3bDe0zJCREX3/9tdWyzZs3q2bNmuaPUqVLl9aDDz6oqVOn6rvvvlN2drbWrVsn6UKI16xZM73yyivKyMiQo6OjPv744xtqE+5+jCTDHeHUqVPavXu3Ob9v3z5lZmaqYsWK8vf3t2HLANt68MEHFR4ero4dO2rKlCmqVauWDh48qJUrV6pjx44KCwtTXFycWrZsqerVq6tbt246f/68Vq1apWHDhhXZ38KFC5Wfn68mTZqobNmyevfdd+Xi4qKAgIBi665Xr56efPJJxcfH6/z58+rbt68iIyMv+2IA4E4WFRWlOnXqaNKkSRo7dqz69+8vDw8PtWnTRmfPntW2bdt0/PhxDRo0SN27d9ekSZPMt99VrlxZGRkZqlKlisLDw1WjRg0tW7ZM7du3l8Vi0ejRoxlhiRJx9OhRde7cWT169FC9evXk7u6ubdu2aerUqerQoYOkCz+O/OMf/9DkyZMVGBio3377TaNGjSp2f+PGjZOnp6d8fHw0cuRIeXl5qWPHjpKkgQMHqk2bNqpZs6aOHz+udevWmT+avPDCC0pISFD37t01dOhQeXl5affu3UpKSlJCQoIcHBwUGBio9PR0ZWdny83NTRUrVjRHuRV699135enpqc6dOxdZ9/DDD+utt97Sww8/rLFjx6pPnz7y9vZWmzZtdPLkSW3atEn9+vVTZGSkIiIi1KlTJ82YMUM1atTQTz/9JIvFooceekhRUVE6cuSIpk6dqscff1yff/65Vq1aJQ8Pjyt+1v7+/nJ0dNTs2bPVp08f/fDDDxo/frxVmRdffFGzZ89Wt27dNGLECJUrV07ffPON7r//ftWqVUuSFB0drXLlymnChAkaN27ctf1F445Xo0YNJScna/PmzapQoYJmzJihvLy8O+aHxyeeeEIjR45U7969NXz4cOXk5GjatGmSVOydBxc7cuRIkSDa19dXgwcP1n333afx48era9euSktL05w5c8y3Sa9YsUJ79+5VRESEKlSooJUrV6qgoEC1atVSenq61q5dq9atW8vb21vp6ek6cuTIHfN5wnYYSYY7wrZt29SwYUM1bNhQkjRo0CA1bNhQY8aMsXHLANuyWCxauXKlIiIi1KNHD9WsWVPdunVTdna2+UyKqKgoffTRR1q+fLkaNGigBx54oMibkgqVL19eCQkJatasmTkC7dNPP5Wnp2exdaekpKhChQqKiIjQgw8+qKCgIH3wwQclesyALQ0aNEgJCQmKjo7Wm2++qYULFyo0NFSRkZFauHChOZLM0dFRqamp8vb2Vtu2bRUaGmr19ryZM2eqQoUKatq0qdq3b6/o6Gg1atTIloeGu5Sbm5uaNGmimTNnKiIiQnXr1tXo0aPVq1cvzZkzxyyXmJiov/76S2FhYRowYIAmTJhQ7P4mT56sAQMGqHHjxsrNzdXy5cvN53Ll5+frhRdeUHBwsB566CHVqlXL/DJbpUoVbdq0Sfn5+YqOjlbdunU1YMAAlStXzgy7hgwZIgcHB4WEhJi3Kl4qMTFRjz76aJGATLrw7LUVK1bo0KFDevrppxUfH6958+apTp06evjhh61G5SQnJ+u+++5T9+7dFRISomHDhpmj2IKDgzVv3jzNnTtX9evX15YtWzRkyJCrftaVKlXSwoUL9dFHHykkJESTJ082Q4JCnp6eWrdunfkG3MaNGyshIcFq5HipUqUUGxur/Px8/etf/7pqvbg7jB49Wo0aNVJ0dLSioqLk6+trBtB3Ag8PD3366afKzMxUgwYNNHLkSPO72sXPKSvOkiVLzO96hdPrr7+uRo0a6cMPP1RSUpLq1q2rMWPGaNy4ceYbY8uXL69ly5bpgQceUHBwsF5//XW9//77qlOnjjw8PLRx40a1bdtWNWvW1KhRozR9+nTzWYbA5ViMG71RGAAAAABw0/Tq1UuHDh3S8uXLbd0U4G9bvHixnnnmGZ04ceK6npEL2BK3WwIAAADAbeDEiRPaunWrFi9erE8++cTWzQGuyzvvvKOgoCBVrVpVO3bs0Msvv6wuXboQkOGOQkgGAAAAALeBDh06aMuWLXruuefUqlUrWzcHuC55eXkaM2aM8vLyVLlyZXXu3FkTJ060dbOA68LtlgAAAAAAALB7PLgfAAAAAAAAdo+QDAAAAAAAAHaPkAwAAAAAAAB2j5AMAAAAAAAAdo+QDAAAAAAAAHaPkAwAAOA2YrFYlJKSYutmAAAA2B1CMgAAgEvExsbKYrGoT58+Rdb17dtXFotFsbGx17Sv9evXy2Kx6Pfff7+m8rm5uWrTps11tBYAAAA3AyEZAABAMfz8/JSUlKQ///zTXHbmzBm9//778vf3v+n1nTt3TpLk6+srJyenm75/AAAAXBkhGQAAQDEaNWokf39/LVu2zFy2bNky+fn5qWHDhuYywzA0depUBQUFycXFRfXr19fSpUslSdnZ2WrRooUkqUKFClYj0KKiovTiiy9q0KBB8vLyUqtWrSQVvd3ywIED6tatmypWrChXV1eFhYUpPT29hI8eAADA/pS2dQMAAABuV88884zefvttPfnkk5KkxMRE9ejRQ+vXrzfLjBo1SsuWLdP8+fN17733auPGjXrqqadUqVIl/fOf/1RycrI6deqkXbt2ycPDQy4uLua2ixYt0vPPP69NmzbJMIwi9Z86dUqRkZGqWrWqli9fLl9fX3377bcqKCgo8WMHAACwN4RkAAAAlxETE6MRI0YoOztbFotFmzZtUlJSkhmSnT59WjNmzNC6desUHh4uSQoKCtLXX3+tBQsWKDIyUhUrVpQkeXt7q3z58lb7r1GjhqZOnXrZ+pcsWaIjR45o69at5n5q1Khx8w8UAAAAhGQAAACX4+XlpXbt2mnRokUyDEPt2rWTl5eXuX7nzp06c+aMeatkoXPnzlndknk5YWFhV1yfmZmphg0bmgEZAAAASg4hGQAAwBX06NFDL774oiRp7ty5VusKb3v87LPPVLVqVat11/LwfVdX1yuuv/jWTAAAAJQsQjIAAIAreOihh8w3T0ZHR1utCwkJkZOTk3JychQZGVns9o6OjpKk/Pz86667Xr16evPNN3Xs2DFGkwEAAJQw3m4JAABwBQ4ODsrKylJWVpYcHBys1rm7u2vIkCF66aWXtGjRIu3Zs0cZGRmaO3euFi1aJEkKCAiQxWLRihUrdOTIEZ06deqa6+7evbt8fX3VsWNHbdq0SXv37lVycrLS0tJu6jECAACAkAwAAOCqPDw85OHhUey68ePHa8yYMXr11VcVHBys6Ohoffrpp6pWrZokqWrVqnrllVc0fPhw+fj4mLduXgtHR0elpqbK29tbbdu2VWhoqCZPnlwkrAMAAMCNsxjFvW8cAAAAAAAAsCOMJAMAAAAAAIDdIyQDAAAAAACA3SMkAwAAAAAAgN0jJAMAAAAAAIDdIyQDAAAAAACA3SMkAwAAAAAAgN0jJAMAAAAAAIDdIyQDAAAAAACA3SMkAwAAAAAAgN0jJAMAAAAAAIDdIyQDAAAAAACA3ft/8c7DCTE0+MYAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 1400x800 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Graph results\n",
    "data = {\n",
    "    'Metric': ['Hamming Loss', 'F1', 'Precision', 'Recall', 'Subset Accuracy'],\n",
    "    'Final': [final_results['hamming'], final_results['f1'], final_results['precision'], final_results['recall'], final_results['subset_acc']],\n",
    "    'Base (Google)': [base_google_results['hamming'], base_google_results['f1'], base_google_results['precision'], base_google_results['recall'], base_google_results['subset_acc']],\n",
    "    'Base (Custom)': [base_custom_results['hamming'], base_custom_results['f1'], base_custom_results['precision'], base_custom_results['recall'], base_custom_results['subset_acc']]\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Set the figure size and color palette\n",
    "plt.figure(figsize=(14, 8))\n",
    "palette = sns.color_palette(\"viridis\")\n",
    "\n",
    "# Create the first bar plot for f1, precision, recall, and subset_acc\n",
    "ax = sns.barplot(x='Metric', y='value', hue='Model', data=pd.melt(df[df['Metric'] != 'Hamming Loss'], id_vars='Metric', var_name='Model', value_name='value'), palette=palette)\n",
    "ax.set_ylabel('Score')\n",
    "ax.set_title('Model Performance Metrics')\n",
    "\n",
    "# Create a secondary y-axis for the hamming loss\n",
    "ax2 = ax.twinx()\n",
    "ax2 = sns.barplot(x='Metric', y='value', hue='Model', data=pd.melt(df[df['Metric'] == 'Hamming Loss'], id_vars='Metric', var_name='Model', value_name='value'), ax=ax2, dodge=True, palette=palette)\n",
    "ax2.set_ylabel('Hamming Loss')\n",
    "\n",
    "# Remove extra legend\n",
    "ax.legend_.remove()\n",
    "\n",
    "# Add outline to bars\n",
    "for patch in ax.patches:\n",
    "    patch.set_edgecolor('black')\n",
    "    patch.set_linewidth(1)\n",
    "for patch in ax2.patches:\n",
    "    patch.set_edgecolor('black')\n",
    "    patch.set_linewidth(1)\n",
    "\n",
    "# Show the plot\n",
    "plt.savefig('../../figs/recurrent_nn/final_performance.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4c7ae6b-22e2-4fbd-b1d3-797b89879523",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
