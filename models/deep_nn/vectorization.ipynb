{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "157f478c-acc2-4ada-aa18-6d97b926ac91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import re\n",
    "import os\n",
    "import scipy.sparse as sp\n",
    "from pylatexenc.latexwalker import LatexWalker, LatexEnvironmentNode, LatexMacroNode, LatexCharsNode, LatexGroupNode, LatexMathNode\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad309807-5d95-478e-858d-bca0083a11d3",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9446b16d-f90f-4d6f-9f38-5f3dc9da95af",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fine_tokenize_expression(expr):\n",
    "    '''\n",
    "    Tokenizes an expression into each of its component words/symbols. Essentially combines \n",
    "    all plaintext next to one another into a single token and any symbol into a single token.\n",
    "\n",
    "    Parameters:\n",
    "    expr (str): An expression to tokenize.\n",
    "\n",
    "    Returns:\n",
    "    tokens (list[str]): A list of tokens.\n",
    "    '''\n",
    "    tokens = re.findall(r'\\w+|[^\\s\\w]', expr)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ceddd724-a27b-4aff-98d0-275abd7f3412",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fine_tokenize_latex(latex_str):\n",
    "    '''\n",
    "    Tokenizes a LaTeX string finely. This means every symbol and every word are separate. \n",
    "\n",
    "    Parameters:\n",
    "    latex_str (str): A LaTeX string to tokenize.\n",
    "\n",
    "    Returns:\n",
    "    tokens (list[str]): A list of tokens.\n",
    "    '''\n",
    "    # Create a LatexWalker to get the tree of nodes from the LaTeX code.\n",
    "    walker = LatexWalker(latex_str)\n",
    "    nodes, _, __ = walker.get_latex_nodes()\n",
    "    tokens = []\n",
    "\n",
    "    # Inner function to parse each individual node\n",
    "    def extract_tokens(node):\n",
    "\n",
    "        # For macro, environment, and group nodes, use the name of the node \n",
    "        # as a token, then recursively parse its contents.\n",
    "        if isinstance(node, LatexMacroNode):\n",
    "            tokens.append(node.macroname)\n",
    "            if node.nodeargd is None:\n",
    "                return\n",
    "            for arg in node.nodeargd.argnlist:\n",
    "                extract_tokens(arg)\n",
    "        elif isinstance(node, LatexEnvironmentNode):\n",
    "            tokens.append(node.environmentname)\n",
    "            for n in node.nodelist:\n",
    "                extract_tokens(n)\n",
    "        elif isinstance(node, LatexGroupNode):\n",
    "            for child_node in node.nodelist:\n",
    "                extract_tokens(child_node)\n",
    "\n",
    "        # For char nodes, tokenize the text using the above regex.\n",
    "        elif isinstance(node, LatexCharsNode):\n",
    "            tokens.extend(fine_tokenize_expression(node.chars))\n",
    "\n",
    "    for node in nodes:\n",
    "        extract_tokens(node)\n",
    "    \n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f99142bd-b0c3-4d44-baca-775ac56739df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def coarse_tokenize_expression(expr):\n",
    "    '''\n",
    "    Tokenizes an expression into commands, arguments, and parenthetical expressions, each of \n",
    "    which becomes a separate token.\n",
    "\n",
    "    Parameters:\n",
    "    expr (str): An expression to tokenize.\n",
    "\n",
    "    Returns:\n",
    "    tokens (list[str]): A list of tokens.\n",
    "    '''\n",
    "    token_pattern = re.compile(r'\\\\[a-zA-Z]+|{[^{}]+}|[()\\[\\]]|\\S+')\n",
    "    tokens = token_pattern.findall(expr)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9ce7e29d-25b3-434a-911a-b8d004a82066",
   "metadata": {},
   "outputs": [],
   "source": [
    "def latex_to_string(node, include_braces=False):\n",
    "    '''\n",
    "    Converts LaTeX nodes to strings for use by the coarse tokenizer.\n",
    "\n",
    "    Parameters:\n",
    "    node (LatexNode): A node to extract text from.\n",
    "    include_braces (bool): Whether or not to include the braces in the tokens. Defaults to False.\n",
    "\n",
    "    Returns:\n",
    "    return_str (str): A string with the LaTeX text properly parsed.\n",
    "    '''\n",
    "    return_str = ''\n",
    "\n",
    "    # If chars node, simply get the characters\n",
    "    if isinstance(node, LatexCharsNode):\n",
    "        return_str = node.chars\n",
    "        \n",
    "    # If macro, group, or environment node, recursively get all text from \n",
    "    # child nodes and join it all together with the node name.\n",
    "    elif isinstance(node, LatexMacroNode):\n",
    "        if node.nodeargd is None:\n",
    "            return return_str\n",
    "        macro_content = f\"\\\\{node.macroname}\" + ''.join([latex_to_string(arg, include_braces=True) for arg in node.nodeargd.argnlist])\n",
    "        return_str = macro_content\n",
    "    elif isinstance(node, LatexGroupNode):\n",
    "        content = ''.join([latex_to_string(child, include_braces=True) for child in node.nodelist])\n",
    "        if include_braces:\n",
    "            return_str = f\"{{{content}}}\"\n",
    "        else:\n",
    "            return_str = content\n",
    "    elif isinstance(node, LatexEnvironmentNode):\n",
    "        env_content = f\"\\\\begin{{{node.environmentname}}}\" + ''.join([latex_to_string(n, include_braces=True) for n in node.nodelist]) + f\"\\\\end{{{node.environmentname}}}\"\n",
    "        return_str = env_content\n",
    "    return return_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "49401af4-7f39-4b09-ab69-2a31b3c0e3e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def coarse_tokenize_latex(latex_str):\n",
    "    '''\n",
    "    Tokenizes a LaTeX string finely. This means that arguments and parenthetical expressions \n",
    "    are kept together as one token.\n",
    "\n",
    "    Parameters:\n",
    "    latex_str (str): A LaTeX string to tokenize.\n",
    "\n",
    "    Returns:\n",
    "    tokens (list[str]): A list of tokens.\n",
    "    '''\n",
    "    # Create a LatexWalker to get the tree of nodes from the LaTeX code.\n",
    "    walker = LatexWalker(latex_str)\n",
    "    nodes, _, __ = walker.get_latex_nodes()\n",
    "    tokens = []\n",
    "\n",
    "    # Inner function to parse each individual node\n",
    "    def extract_tokens(node):\n",
    "\n",
    "        # For macros nodes, get arguments and insert the macro name and arguments together as one token\n",
    "        if isinstance(node, LatexMacroNode):\n",
    "            if node.nodeargd is None:\n",
    "                return\n",
    "            macro_name = f\"\\\\{node.macroname}\"\n",
    "            args = ''.join([latex_to_string(arg, include_braces=True) for arg in node.nodeargd.argnlist])\n",
    "            tokens.append(macro_name + args)\n",
    "\n",
    "        # For environment nodes, place the entire environment as one token\n",
    "        elif isinstance(node, LatexEnvironmentNode):\n",
    "            env_name = f\"\\\\begin{{{node.environmentname}}}\" + ''.join([latex_to_string(n) for n in node.nodelist]) + f\"\\\\end{{{node.environmentname}}}\"\n",
    "            tokens.append(env_name)\n",
    "\n",
    "        # For char nodes, just coarsely tokenize the expression\n",
    "        elif isinstance(node, LatexCharsNode):\n",
    "            tokens.extend(coarse_tokenize_expression(node.chars))\n",
    "\n",
    "        # For group nodes, just get the content and join it all together\n",
    "        elif isinstance(node, LatexGroupNode):\n",
    "            group_content = ''.join([latex_to_string(child_node, include_braces=True) for child_node in node.nodelist])\n",
    "            tokens.append(f\"{{{group_content}}}\")\n",
    "\n",
    "        # For math nodes, apply recursion and get contents\n",
    "        elif isinstance(node, LatexMathNode):\n",
    "            for n in node.nodelist:\n",
    "                extract_tokens(n)\n",
    "    \n",
    "    for node in nodes:\n",
    "        extract_tokens(node)\n",
    "\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b2b5ee88-2b45-4058-b234-8ab10efa1d36",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_tokens(tokens):\n",
    "    '''\n",
    "    Merges tokens that are related to each other by a subscript or superscript.\n",
    "\n",
    "    Parameters:\n",
    "    tokens (list[str]): A list of tokens.\n",
    "\n",
    "    Returns:\n",
    "    merged_tokens (list[str]): A list of merged tokens.\n",
    "    '''\n",
    "    merged_tokens = []\n",
    "    skip_next = False\n",
    "    \n",
    "    for i, token in enumerate(tokens):\n",
    "        if skip_next:\n",
    "            skip_next = False\n",
    "            continue\n",
    "\n",
    "        if token in ('^', '_'):\n",
    "            if i > 0 and i < len(tokens) - 1:\n",
    "                # Merge previous token, current ^/_ token, and next token\n",
    "                merged_tokens[-1] += token + tokens[i + 1]\n",
    "                skip_next = True\n",
    "            elif i == 0 and i < len(tokens) - 1:\n",
    "                # Merge current ^/_ token and next token\n",
    "                merged_tokens.append(token + tokens[i + 1])\n",
    "                skip_next = True\n",
    "            elif i > 0:\n",
    "                # Merge previous token and current ^/_ token\n",
    "                merged_tokens[-1] += token\n",
    "        elif token.endswith('^') or token.endswith('_'):\n",
    "            # Merge current token with next token\n",
    "            if i < len(tokens) - 1:\n",
    "                merged_tokens.append(token + tokens[i + 1])\n",
    "                skip_next = True\n",
    "            else:\n",
    "                merged_tokens.append(token)\n",
    "        elif token.startswith('^') or token.startswith('_'):\n",
    "            # Merge previous token with current token\n",
    "            if i > 0:\n",
    "                merged_tokens[-1] += token\n",
    "            else:\n",
    "                merged_tokens.append(token)\n",
    "        else:\n",
    "            merged_tokens.append(token)\n",
    "    \n",
    "    return merged_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa12744e-bd6d-483d-a76c-addd4464334e",
   "metadata": {},
   "source": [
    "## Main Code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "096ff57e-be0a-4c2b-8a70-0cf5c64f6581",
   "metadata": {},
   "source": [
    "At one point I tried to use TF-IDF vectorization and a standard neural network to perform this classification task. I abandoned this method, however, after learning more advanced NLP techniques such as recurrent NNs. I'm leaving this notebook in, though, just so that my process is documented."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eccce900-f67d-41b0-98a6-eb74f660705f",
   "metadata": {},
   "source": [
    "I'll be vectorizing the body text using 4 different methods:\n",
    "- Combined raw vectorization: This will take the entirety of the body text and vectorize it using TF-IDF vectorization. I don't expect this to work very well since this will have both text and LaTeX in it.\n",
    "- Separate raw vectorization: Separates the text from the LaTeX in the body of the question, but still uses regular TF-IDF vectorization on both portions. I expect this to work much better than with the LaTeX and the text combined.\n",
    "- Separate vectorization with fine tokenization: Treats the text with regular TF-IDF but uses custom tokenization to tokenize and vectorize the LaTeX. This will treat each symbol and word in the LaTeX as its own token to apply vectorization to.\n",
    "- Separate vectorization with coarse tokenization: Treats the text with regular TF-IDF but uses custom tokenization to tokenize and vectorize the LaTeX. This time, though, tokens are much larger, being composed of individual macros along with their arguments.\n",
    "\n",
    "The thinking behind the last two is that a lot of relevant features will lie within the types of environments, macros, and even characters used within LaTeX code. For instance, if we see a lot of $\\int$ symbols, the question is probably related to calculus in some way. However, if we keep seeing $\\zeta$ or $\\textnormal{mod}$, the question might be more number theoretical. Even the presence of certain mathematical symbols like $e$ can be used to narrow down the field of math a question is in. For this reason, it makes sense to tokenize expressions in a way that preserves the structure of the underlying problem. See above for documentation on how this is explicitly done.\n",
    "\n",
    "For each of these 4 methods, I'll be using 5 different dimensionalities: 1,000, 2,000, 5,000, 10,000, and 15,000. It's entirely possible that the larger of these values are computationally infeasible for me to train a neural network on, but that's a later problem. Might as well gather the data here anyways.\n",
    "\n",
    "Also, when I save these vectorized datasets, the filesizes are going to be quite large. Because of that, if you're looking at this repo and want to recreate this analysis somehow, just use the raw data that is in the repo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9a221963-59f1-4154-81d9-2a024a520f5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in all data and combine into a single dataframe\n",
    "parsed_data_df = pd.read_csv('../../data/parsed_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "24675b38-b760-4b14-972d-0c1b526b6ee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "dim_list = [1000, 2000, 5000, 10000, 15000]\n",
    "for dim in dim_list:\n",
    "\n",
    "    # Vectorize just raw body text\n",
    "    vectorizer_raw = TfidfVectorizer(max_features=dim)\n",
    "    X_raw = vectorizer_raw.fit_transform(parsed_data_df['body'])\n",
    "    with open(f'vectorized_data/vectorizers/vectorizer_raw_{dim}.pkl', 'wb') as file:\n",
    "        pickle.dump(vectorizer_raw, file)\n",
    "\n",
    "    # Vectorize text and LaTeX code separately, treating both as raw text\n",
    "    vectorizer_text = TfidfVectorizer(max_features=dim)\n",
    "    vectorizer_latex_raw = TfidfVectorizer(max_features=dim)\n",
    "    X_text = vectorizer_text.fit_transform(parsed_data_df['body_text'])\n",
    "    X_latex_raw = vectorizer_latex_raw.fit_transform(parsed_data_df['body_latex'])\n",
    "    with open(f'vectorized_data/vectorizers/vectorizer_text_{dim}.pkl', 'wb') as file:\n",
    "        pickle.dump(vectorizer_text, file)\n",
    "    with open(f'vectorized_data/vectorizers/vectorizer_latex_raw_{dim}.pkl', 'wb') as file:\n",
    "        pickle.dump(vectorizer_latex_raw, file)\n",
    "\n",
    "    # Vectorize text and LaTeX code separately with fine tokenizer\n",
    "    parsed_data_df['body_latex_fine'] = parsed_data_df['body_latex'].apply(lambda x: ' '.join(fine_tokenize_latex(remove_newlines(x))))\n",
    "    vectorizer_latex_fine = TfidfVectorizer(max_features=dim)\n",
    "    X_latex_fine = vectorizer_latex_fine.fit_transform(parsed_data_df['body_latex_fine'])\n",
    "    with open(f'vectorized_data/vectorizers/vectorizer_latex_fine_{dim}.pkl', 'wb') as file:\n",
    "        pickle.dump(vectorizer_latex_fine, file)\n",
    "\n",
    "    # Vectorize text and LaTeX code separately with coarse tokenizer\n",
    "    parsed_data_df['body_latex_coarse'] = parsed_data_df['body_latex'].apply(lambda x: ' '.join(merge_tokens(coarse_tokenize_latex(remove_newlines(x)))))\n",
    "    vectorizer_latex_coarse = TfidfVectorizer(max_features=dim)\n",
    "    X_latex_coarse = vectorizer_latex_coarse.fit_transform(parsed_data_df['body_latex_coarse'])\n",
    "    with open(f'vectorized_data/vectorizers/vectorizer_latex_coarse_{dim}.pkl', 'wb') as file:\n",
    "        pickle.dump(vectorizer_latex_coarse, file)\n",
    "\n",
    "    # Combine text and LaTeX features\n",
    "    X_text_latex_raw = sp.hstack([X_text, X_latex_raw])\n",
    "    X_text_latex_fine = sp.hstack([X_text, X_latex_fine])\n",
    "    X_text_latex_coarse = sp.hstack([X_text, X_latex_coarse])\n",
    "\n",
    "    # Store the final datasets as NPZs\n",
    "    sp.save_npz(f'vectorized_data/sparse_matrices/raw_{dim}.npz', X_raw)\n",
    "    sp.save_npz(f'vectorized_data/sparse_matrices/text_latex_raw_{dim}.npz', X_text_latex_raw)\n",
    "    sp.save_npz(f'vectorized_data/sparse_matrices/text_latex_fine_{dim}.npz', X_text_latex_fine)\n",
    "    sp.save_npz(f'vectorized_data/sparse_matrices/text_latex_coarse_{dim}.npz', X_text_latex_coarse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f7aa399b-8995-4215-902e-c6695662bd06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the tags separately as a CSV file\n",
    "parsed_data_df[['tags']].to_csv('vectorized_data/tags.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
